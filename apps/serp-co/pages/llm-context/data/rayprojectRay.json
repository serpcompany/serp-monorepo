[
  {
    "owner": "ray-project",
    "repo": "ray",
    "content": "TITLE: Serving a Scikit-learn Model with Ray Serve (Python)\nDESCRIPTION: Serves a scikit-learn gradient boosting classifier using Ray Serve. It defines a deployment and exposes an endpoint for making predictions. Placeholders `__serve_example_begin__` and `__serve_example_end__` mark the code block.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} ../serve/doc_code/sklearn_quickstart.py\n:language: python\n:start-after: __serve_example_begin__\n:end-before: __serve_example_end__\n```\"\n```\n\n----------------------------------------\n\nTITLE: Transformer Tuning with Ray Tune\nDESCRIPTION: This code snippet demonstrates how to use Ray Tune to optimize the hyperparameters of a Hugging Face Transformer model. It sets up the data, model, and training arguments, then uses the `hyperparameter_search` function of the `Trainer` class to perform the optimization using the PopulationBasedTraining scheduler.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_transformers.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nThis example is uses the official\nhuggingface transformers `hyperparameter_search` API.\n\"\"\"\nimport os\n\nimport ray\nfrom ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.examples.pbt_transformers.utils import (\n    download_data,\n    build_compute_metrics_fn,\n)\nfrom ray.tune.schedulers import PopulationBasedTraining\nfrom transformers import (\n    glue_tasks_num_labels,\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    GlueDataset,\n    GlueDataTrainingArguments,\n    TrainingArguments,\n)\n\n\ndef tune_transformer(num_samples=8, gpus_per_trial=0, smoke_test=False):\n    data_dir_name = \"./data\" if not smoke_test else \"./test_data\"\n    data_dir = os.path.abspath(os.path.join(os.getcwd(), data_dir_name))\n    if not os.path.exists(data_dir):\n        os.mkdir(data_dir, 0o755)\n\n    # Change these as needed.\n    model_name = (\n        \"bert-base-uncased\" if not smoke_test else \"sshleifer/tiny-distilroberta-base\"\n    )\n    task_name = \"rte\"\n\n    task_data_dir = os.path.join(data_dir, task_name.upper())\n\n    num_labels = glue_tasks_num_labels[task_name]\n\n    config = AutoConfig.from_pretrained(\n        model_name, num_labels=num_labels, finetuning_task=task_name\n    )\n\n    # Download and cache tokenizer, model, and features\n    print(\"Downloading and caching Tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Triggers tokenizer download to cache\n    print(\"Downloading and caching pre-trained model\")\n    AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        config=config,\n    )\n\n    def get_model():\n        return AutoModelForSequenceClassification.from_pretrained(\n            model_name,\n            config=config,\n        )\n\n    # Download data.\n    download_data(task_name, data_dir)\n\n    data_args = GlueDataTrainingArguments(task_name=task_name, data_dir=task_data_dir)\n\n    train_dataset = GlueDataset(\n        data_args, tokenizer=tokenizer, mode=\"train\", cache_dir=task_data_dir\n    )\n    eval_dataset = GlueDataset(\n        data_args, tokenizer=tokenizer, mode=\"dev\", cache_dir=task_data_dir\n    )\n\n    training_args = TrainingArguments(\n        output_dir=\".\",\n        learning_rate=1e-5,  # config\n        do_train=True,\n        do_eval=True,\n        no_cuda=gpus_per_trial <= 0,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        num_train_epochs=2,  # config\n        max_steps=-1,\n        per_device_train_batch_size=16,  # config\n        per_device_eval_batch_size=16,  # config\n        warmup_steps=0,\n        weight_decay=0.1,  # config\n        logging_dir=\"./logs\",\n        skip_memory_metrics=True,\n        report_to=\"none\",\n    )\n\n    trainer = Trainer(\n        model_init=get_model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=build_compute_metrics_fn(task_name),\n    )\n\n    tune_config = {\n        \"per_device_train_batch_size\": 32,\n        \"per_device_eval_batch_size\": 32,\n        \"num_train_epochs\": tune.choice([2, 3, 4, 5]),\n        \"max_steps\": 1 if smoke_test else -1,  # Used for smoke test.\n    }\n\n    scheduler = PopulationBasedTraining(\n        time_attr=\"training_iteration\",\n        metric=\"eval_acc\",\n        mode=\"max\",\n        perturbation_interval=1,\n        hyperparam_mutations={\n            \"weight_decay\": tune.uniform(0.0, 0.3),\n            \"learning_rate\": tune.uniform(1e-5, 5e-5),\n            \"per_device_train_batch_size\": [16, 32, 64],\n        },\n    )\n\n    reporter = CLIReporter(\n        parameter_columns={\n            \"weight_decay\": \"w_decay\",\n            \"learning_rate\": \"lr\",\n            \"per_device_train_batch_size\": \"train_bs/gpu\",\n            \"num_train_epochs\": \"num_epochs\",\n        },\n        metric_columns=[\"eval_acc\", \"eval_loss\", \"epoch\", \"training_iteration\"],\n    )\n\n    trainer.hyperparameter_search(\n        hp_space=lambda _: tune_config,\n        backend=\"ray\",\n        n_trials=num_samples,\n        resources_per_trial={\"cpu\": 1, \"gpu\": gpus_per_trial},\n        scheduler=scheduler,\n        keep_checkpoints_num=1,\n        checkpoint_score_attr=\"training_iteration\",\n        stop={\"training_iteration\": 1} if smoke_test else None,\n        progress_reporter=reporter,\n        local_dir=\"~/ray_results/\",\n        name=\"tune_transformer_pbt\",\n        log_to_file=True,\n    )\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--smoke-test\",\n        default=True,\n        action=\"store_true\",\n        help=\"Finish quickly for testing\",\n    )\n    args, _ = parser.parse_known_args()\n\n    ray.init()\n\n    if args.smoke_test:\n        tune_transformer(num_samples=1, gpus_per_trial=0, smoke_test=True)\n    else:\n        # You can change the number of GPUs here:\n        tune_transformer(num_samples=8, gpus_per_trial=1)\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Ray Serve \"Hello World\" Application\nDESCRIPTION: This code snippet demonstrates how to define a simple Python application using Ray Serve. The application is run locally and can be queried over HTTP. The code showcases defining, deploying, and serving a basic application.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/index.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n{literalinclude} doc_code/quickstart.py\n:language: python\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: LLM Deployment using Bind Pattern with Single Model\nDESCRIPTION: Configure and deploy a single LLM model using Ray Serve's bind pattern with custom configuration options\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.llm import LLMConfig, LLMServer, LLMRouter\n\nllm_config = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"qwen-0.5b\",\n        model_source=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1, max_replicas=2,\n        )\n    ),\n    accelerator_type=\"A10G\",\n    engine_kwargs=dict(\n        tensor_parallel_size=2,\n    ),\n)\n\ndeployment = LLMServer.as_deployment(llm_config.get_serve_options(name_prefix=\"vLLM:\")).bind(llm_config)\nllm_app = LLMRouter.as_deployment().bind([deployment])\nserve.run(llm_app, blocking=True)\n```\n\n----------------------------------------\n\nTITLE: Training Cifar10 with Keras and Ray Tune\nDESCRIPTION: This code snippet demonstrates how to train a Cifar10 model using Keras and Ray Tune.  It configures and runs a PBT (Population Based Training) experiment to optimize hyperparameters. The snippet relies on Keras for model definition and training, and Ray Tune for hyperparameter tuning and distributed execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/pbt_tune_cifar10_with_keras.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"/../../python/ray/tune/examples/pbt_tune_cifar10_with_keras.py\"\n```\n\n----------------------------------------\n\nTITLE: Task-Based Prediction Function with Ray\nDESCRIPTION: Implements a Ray remote task for making predictions on a data shard. Takes a model and shard path as input, processes the data, and returns prediction results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/batch_prediction.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.parquet as pq\nimport ray\n\n@ray.remote\ndef make_prediction(model, shard_path):\n    df = pq.read_table(shard_path).to_pandas()\n    result = model(df)\n\n    # Write out the prediction result.\n    # NOTE: unless the driver will have to further process the\n    # result (other than simply writing out to storage system),\n    # writing out at remote task is recommended, as it can avoid\n    # congesting or overloading the driver.\n    # ...\n\n    # Here we just return the size about the result in this example.\n    return len(result)\n```\n\n----------------------------------------\n\nTITLE: Creating PyTorch Lightning Module for Dolly-v2-7b Fine-tuning\nDESCRIPTION: Defines a PyTorch Lightning module for the dolly-v2-7b model with training step and optimizer configuration. The module loads the pre-trained model from Hugging Face and sets up the forward pass to compute the language modeling loss.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport lightning.pytorch as pl\n\nclass DollyV2Model(pl.LightningModule):\n    def __init__(self, lr=2e-5, eps=1e-8):\n        super().__init__()\n        self.save_hyperparameters()\n        self.lr = lr\n        self.eps = eps\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n    def forward(self, batch):\n        outputs = self.model(\n            batch[\"input_ids\"], \n            attention_mask=batch[\"attention_mask\"], \n            labels=batch[\"labels\"]\n        )\n        return outputs.loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self.forward(batch)\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=True)\n        return loss\n\n    def configure_optimizers(self):\n        if self.global_rank == 0:\n            print(self.trainer.model)\n        return torch.optim.AdamW(self.trainer.model.parameters(), lr=self.lr, eps=self.eps)\n```\n\n----------------------------------------\n\nTITLE: Prefetching Batches in Ray Train Loop\nDESCRIPTION: This snippet shows how to use prefetching in a Ray Train loop to improve performance. It demonstrates setting up a TorchTrainer with a dataset and using iter_batches with prefetch_batches set to 10.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import train\nfrom ray.train import ScalingConfig\nfrom ray.train.torch import TorchTrainer\n\nds = ray.data.read_text(\n    \"s3://anonymous@ray-example-data/sms_spam_collection_subset.txt\"\n)\n\ndef train_loop_per_worker():\n    # Get an iterator to the dataset we passed in below.\n    it = train.get_dataset_shard(\"train\")\n    for _ in range(2):\n        # Prefetch 10 batches at a time.\n        for batch in it.iter_batches(batch_size=128, prefetch_batches=10):\n            print(\"Do some training on batch\", batch)\n\nmy_trainer = TorchTrainer(\n    train_loop_per_worker,\n    scaling_config=ScalingConfig(num_workers=2),\n    datasets={\"train\": ds},\n)\nmy_trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Querying Deployed LLM using cURL\nDESCRIPTION: Example of making a chat completion request to the deployed LLM model using cURL\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n     -H \"Content-Type: application/json\" \\\n     -H \"Authorization: Bearer fake-key\" \\\n     -d '{\n           \"model\": \"qwen-0.5b\",\n           \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n         }'\n```\n\n----------------------------------------\n\nTITLE: Defining Training Function for GPT-J Fine-tuning with Ray Train and DeepSpeed\nDESCRIPTION: This function sets up the training environment, configures DeepSpeed, initializes the model and tokenizer, and defines the training loop using Hugging Face's Trainer. It includes custom metrics computation and Ray Train reporting callbacks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport evaluate\nimport torch\nfrom transformers import (\n    Trainer,\n    TrainingArguments,\n    GPTJForCausalLM,\n    AutoTokenizer,\n    default_data_collator,\n)\nfrom transformers.utils.logging import disable_progress_bar, enable_progress_bar\n\nfrom ray import train\nfrom ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n\n\ndef train_func(config):\n    # Use the actual number of CPUs assigned by Ray\n    os.environ[\"OMP_NUM_THREADS\"] = str(\n        train.get_context().get_trial_resources().bundles[-1].get(\"CPU\", 1)\n    )\n    # Enable tf32 for better performance\n    torch.backends.cuda.matmul.allow_tf32 = True\n\n    batch_size = config.get(\"batch_size\", 4)\n    epochs = config.get(\"epochs\", 2)\n    warmup_steps = config.get(\"warmup_steps\", 0)\n    learning_rate = config.get(\"learning_rate\", 0.00002)\n    weight_decay = config.get(\"weight_decay\", 0.01)\n    steps_per_epoch = config.get(\"steps_per_epoch\")\n\n    deepspeed = {\n        \"fp16\": {\n            \"enabled\": \"auto\",\n            \"initial_scale_power\": 8,\n            \"hysteresis\": 4,\n            \"consecutive_hysteresis\": True,\n        },\n        \"bf16\": {\"enabled\": \"auto\"},\n        \"optimizer\": {\n            \"type\": \"AdamW\",\n            \"params\": {\n                \"lr\": \"auto\",\n                \"betas\": \"auto\",\n                \"eps\": \"auto\",\n            },\n        },\n        \"zero_optimization\": {\n            \"stage\": 3,\n            \"offload_optimizer\": {\n                \"device\": \"cpu\",\n                \"pin_memory\": True,\n            },\n            \"overlap_comm\": True,\n            \"contiguous_gradients\": True,\n            \"reduce_bucket_size\": \"auto\",\n            \"stage3_prefetch_bucket_size\": \"auto\",\n            \"stage3_param_persistence_threshold\": \"auto\",\n            \"gather_16bit_weights_on_model_save\": True,\n            \"round_robin_gradients\": True,\n        },\n        \"gradient_accumulation_steps\": \"auto\",\n        \"gradient_clipping\": \"auto\",\n        \"steps_per_print\": 10,\n        \"train_batch_size\": \"auto\",\n        \"train_micro_batch_size_per_gpu\": \"auto\",\n        \"wall_clock_breakdown\": False,\n    }\n\n    print(\"Preparing training arguments\")\n    training_args = TrainingArguments(\n        \"output\",\n        logging_steps=1,\n        save_strategy=\"steps\",\n        save_steps=steps_per_epoch,\n        max_steps=steps_per_epoch * epochs,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=1,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        label_names=[\"input_ids\", \"attention_mask\"],\n        push_to_hub=False,\n        report_to=\"none\",\n        disable_tqdm=True,  # declutter the output a little\n        fp16=True,\n        gradient_checkpointing=True,\n        deepspeed=deepspeed,\n    )\n    disable_progress_bar()\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    print(\"Loading model\")\n\n    model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\n    model.resize_token_embeddings(len(tokenizer))\n\n    print(\"Model loaded\")\n\n    enable_progress_bar()\n\n    metric = evaluate.load(\"accuracy\")\n\n    train_ds = train.get_dataset_shard(\"train\")\n    eval_ds = train.get_dataset_shard(\"validation\")\n\n    train_ds_iterable = train_ds.iter_torch_batches(\n        batch_size=batch_size,\n        local_shuffle_buffer_size=train.get_context().get_world_size() * batch_size,\n    )\n    eval_ds_iterable = eval_ds.iter_torch_batches(batch_size=batch_size)\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds_iterable,\n        eval_dataset=eval_ds_iterable,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n        data_collator=default_data_collator,\n    )\n\n    # Add callback to report checkpoints to Ray Train\n    trainer.add_callback(RayTrainReportCallback())\n    trainer = prepare_trainer(trainer)\n    trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Basic Trial Scheduler Usage with ASHA in Ray Tune\nDESCRIPTION: This snippet demonstrates the basic usage of trial schedulers in Ray Tune using ASHAScheduler. It shows how to set up a simple training function, configure the scheduler with a metric to minimize, and run the tuning process with 10 samples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\n\ndef train_fn(config):\n    # This objective function is just for demonstration purposes\n    tune.report({\"loss\": config[\"param\"]})\n\ntuner = tune.Tuner(\n    train_fn,\n    tune_config=tune.TuneConfig(\n        scheduler=ASHAScheduler(),\n        metric=\"loss\",\n        mode=\"min\",\n        num_samples=10,\n    ),\n    param_space={\"param\": tune.uniform(0, 1)},\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Implementing Efficient Data Loading for Distributed PyTorch Training with Ray Train\nDESCRIPTION: This code demonstrates how to properly structure a PyTorch training function with Ray Train by moving data loading inside the function and using FileLock to prevent data corruption. It loads FashionMNIST data, creates data loaders, trains a neural network model, and saves checkpoints throughout training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom filelock import FileLock\n\nfrom ray.train import Checkpoint\n\ndef load_data():\n    # Download training data from open datasets.\n    with FileLock(\"./data.lock\"):\n        training_data = datasets.FashionMNIST(\n            root=\"data\",\n            train=True,\n            download=True,\n            transform=ToTensor(),\n        )\n\n        # Download test data from open datasets.\n        test_data = datasets.FashionMNIST(\n            root=\"data\",\n            train=False,\n            download=True,\n            transform=ToTensor(),\n        )\n    return training_data, test_data\n\n\ndef train_func(config: dict):\n    batch_size = config[\"batch_size\"]\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n\n    batch_size_per_worker = batch_size // train.get_context().get_world_size()\n\n    training_data, test_data = load_data()  # <- this is new!\n\n    # Create data loaders.\n    train_dataloader = DataLoader(training_data, batch_size=batch_size_per_worker)\n    test_dataloader = DataLoader(test_data, batch_size=batch_size_per_worker)\n\n    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n    test_dataloader = train.torch.prepare_data_loader(test_dataloader)\n\n    model = NeuralNetwork()\n    model = train.torch.prepare_model(model)\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\n    for epoch in range(epochs):\n        train_epoch(epoch, train_dataloader, model, loss_fn, optimizer)\n        test_loss = test_epoch(test_dataloader, model, loss_fn)\n        with TemporaryDirectory() as tmpdir:\n            torch.save(\n                {\n                    \"epoch\": epoch,\n                    \"model\": model.module.state_dict()\n                },\n                os.path.join(tmpdir, \"checkpoint.pt\")\n            )\n            train.report(dict(loss=test_loss), checkpoint=Checkpoint.from_directory(tmpdir))\n\n    print(\"Done!\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Better Approach with Actors in Python\nDESCRIPTION: This snippet provides a superior solution for state management in Ray by using actor instances. It emphasizes the importance of encapsulating state within an actor and sharing the actor's handle, thus ensuring that each actor operates within its own process while having the ability to access and mutate shared state appropriately.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/global-variables.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/anti_pattern_global_variables.py\n    :language: python\n    :start-after: __better_approach_start__\n    :end-before: __better_approach_end__\n```\n\n----------------------------------------\n\nTITLE: Creating and Executing a Ray Remote Task\nDESCRIPTION: Shows how to define a Python function as a Ray task using the @ray.remote decorator, execute it asynchronously with .remote(), and retrieve its result using ray.get().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/walkthrough.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Define a remote function\n@ray.remote\ndef add(x, y):\n    return x + y\n\n# Call the remote function\nfuture = add.remote(1, 2)  # Returns an object reference\n\n# Get the result\nresult = ray.get(future)  # Blocks until the result is ready\nprint(result)  # 3\n```\n\n----------------------------------------\n\nTITLE: Full Ray Serve Application Script\nDESCRIPTION: This complete script sets up and runs the Ray Serve application, providing the necessary commands for executing the application and instructions for testing it via HTTP. It allows users to run the application and verify its functionality through defined endpoints.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/getting_started.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom ray import serve\n\n@serve.deployment\nclass Translator:\n    ...  # existing implementation\n\napp = serve.run(Translator.bind())\n```\n\n```\n\n----------------------------------------\n\nTITLE: Implementing ASHA Scheduler with Ray Tune for PyTorch Lightning\nDESCRIPTION: Sets up a complete Ray Tune experiment using the ASHA scheduler to tune hyperparameters for a PyTorch Lightning MNIST classifier. The code defines the tuning function and executes it with a specified number of samples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef tune_mnist_asha(num_samples=10):\n    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n\n    tuner = tune.Tuner(\n        ray_trainer,\n        param_space={\"train_loop_config\": search_space},\n        tune_config=tune.TuneConfig(\n            metric=\"ptl/val_accuracy\",\n            mode=\"max\",\n            num_samples=num_samples,\n            scheduler=scheduler,\n        ),\n    )\n    return tuner.fit()\n\n\nresults = tune_mnist_asha(num_samples=num_samples)\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor Supervision Pattern in Python using Ray\nDESCRIPTION: This code demonstrates the actor supervision pattern in Ray. It defines a SupervisorActor that manages WorkerActors, distributes tasks, and handles worker failures. The example shows how to create a supervisor, spawn workers, and execute tasks in parallel.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/tree-of-actors.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\n@ray.remote\nclass WorkerActor:\n    def __init__(self, rank):\n        self.rank = rank\n\n    def do_task(self, task_id):\n        print(f\"Worker {self.rank} executing task {task_id}\")\n        # Simulate work\n        time.sleep(1)\n        return f\"Result from worker {self.rank} for task {task_id}\"\n\n@ray.remote\nclass SupervisorActor:\n    def __init__(self, num_workers):\n        self.num_workers = num_workers\n        self.workers = [WorkerActor.remote(i) for i in range(num_workers)]\n\n    def execute_tasks(self, num_tasks):\n        tasks = []\n        for i in range(num_tasks):\n            worker = self.workers[i % self.num_workers]\n            tasks.append(worker.do_task.remote(i))\n        return ray.get(tasks)\n\n# Initialize Ray\nray.init()\n\n# Create a supervisor with 4 workers\nsupervisor = SupervisorActor.remote(4)\n\n# Execute 10 tasks\nresults = ray.get(supervisor.execute_tasks.remote(10))\n\nfor result in results:\n    print(result)\n\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Batch Inference with Ray\nDESCRIPTION: Implementation of a PyTorch predictor class using a simple neural network with GPU support. Demonstrates data transfer between CPU and GPU, with configurable batch processing using Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/batch_inference.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport ray\n\nds = ray.data.from_numpy(np.ones((1, 100)))\n\nclass TorchPredictor:\n    def __init__(self):\n        # Move the neural network to GPU device by specifying \"cuda\".\n        self.model = nn.Sequential(\n            nn.Linear(in_features=100, out_features=1),\n            nn.Sigmoid(),\n        ).cuda()\n        self.model.eval()\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        # Move the input batch to GPU device by specifying \"cuda\".\n        tensor = torch.as_tensor(batch[\"data\"], dtype=torch.float32, device=\"cuda\")\n        with torch.inference_mode():\n            # Move the prediction output back to CPU before returning.\n            return {\"output\": self.model(tensor).cpu().numpy()}\n\n# Use 2 actors, each actor using 1 GPU. 2 GPUs total.\npredictions = ds.map_batches(\n    TorchPredictor,\n    num_gpus=1,\n    # Specify the batch size for inference.\n    # Increase this for larger datasets.\n    batch_size=1,\n    # Set the concurrency to the number of GPUs in your cluster.\n    concurrency=2,\n    )\npredictions.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating ray.get() Anti-Pattern in Python\nDESCRIPTION: This code snippet shows an anti-pattern where ray.get() is called in a loop, blocking parallelism. It includes both the problematic pattern and the correct approach for comparison.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/ray-get-loop.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n__anti_pattern_start__\n# Anti-pattern: Calling ray.get() in a loop\nresults = []\nfor i in range(10):\n    result = ray.get(some_function.remote(i))  # Blocks until result is available\n    results.append(result)\n\n# Correct pattern: Separate remote calls from ray.get()\nrefs = [some_function.remote(i) for i in range(10)]  # Schedule all tasks\nresults = ray.get(refs)  # Wait for all results at once\n__anti_pattern_end__\n```\n\n----------------------------------------\n\nTITLE: LLM Batch Inference with vLLM\nDESCRIPTION: Implementation of batch inference using vLLM for large language models. Configures a vLLM engine processor and defines preprocessing and postprocessing steps for haiku generation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/batch_inference.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\nimport numpy as np\n\nconfig = vLLMEngineProcessorConfig(\n    model=\"unsloth/Llama-3.1-8B-Instruct\",\n    engine_kwargs={\n        \"enable_chunked_prefill\": True,\n        \"max_num_batched_tokens\": 4096,\n        \"max_model_len\": 16384,\n    },\n    concurrency=1,\n    batch_size=64,\n)\nprocessor = build_llm_processor(\n    config,\n    preprocess=lambda row: dict(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a bot that responds with haikus.\"},\n            {\"role\": \"user\", \"content\": row[\"item\"]}\n        ],\n        sampling_params=dict(\n            temperature=0.3,\n            max_tokens=250,\n        )\n    ),\n    postprocess=lambda row: dict(\n        answer=row[\"generated_text\"]\n    ),\n)\n\nds = ray.data.from_items([\"Start of the haiku is: Complete this for me...\"])\n\nds = processor(ds)\nds.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Implementing XGBoost Training with Ray Tune's Early Stopping in Python\nDESCRIPTION: A complete implementation of XGBoost model training with Ray Tune integration using ASHAScheduler for early stopping. The code includes functions for training on the breast cancer dataset, retrieving the best model checkpoint, and configuring the hyperparameter search space.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-xgboost.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport sklearn.datasets\nimport sklearn.metrics\nfrom ray.tune.schedulers import ASHAScheduler\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\nfrom ray import tune\nfrom ray.tune.integration.xgboost import TuneReportCheckpointCallback\n\n\ndef train_breast_cancer(config: dict):\n    # This is a simple training function to be passed into Tune\n    # Load dataset\n    data, labels = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    # Split into train and test set\n    train_x, test_x, train_y, test_y = train_test_split(data, labels, test_size=0.25)\n    # Build input matrices for XGBoost\n    train_set = xgb.DMatrix(train_x, label=train_y)\n    test_set = xgb.DMatrix(test_x, label=test_y)\n    # Train the classifier, using the Tune callback\n    xgb.train(\n        config,\n        train_set,\n        evals=[(test_set, \"eval\")],\n        verbose_eval=False,\n        # `TuneReportCheckpointCallback` defines the checkpointing frequency and format.\n        callbacks=[TuneReportCheckpointCallback(frequency=1)],\n    )\n\n\ndef get_best_model_checkpoint(results):\n    best_result = results.get_best_result()\n\n    # `TuneReportCheckpointCallback` provides a helper method to retrieve the\n    # model from a checkpoint.\n    best_bst = TuneReportCheckpointCallback.get_model(best_result.checkpoint)\n\n    accuracy = 1.0 - best_result.metrics[\"eval-error\"]\n    print(f\"Best model parameters: {best_result.config}\")\n    print(f\"Best model total accuracy: {accuracy:.4f}\")\n    return best_bst\n\n\ndef tune_xgboost(smoke_test=False):\n    search_space = {\n        # You can mix constants with search space objects.\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": [\"logloss\", \"error\"],\n        \"max_depth\": tune.randint(1, 9),\n        \"min_child_weight\": tune.choice([1, 2, 3]),\n        \"subsample\": tune.uniform(0.5, 1.0),\n        \"eta\": tune.loguniform(1e-4, 1e-1),\n    }\n    # This will enable aggressive early stopping of bad trials.\n    scheduler = ASHAScheduler(\n        max_t=10, grace_period=1, reduction_factor=2  # 10 training iterations\n    )\n\n    tuner = tune.Tuner(\n        train_breast_cancer,\n        tune_config=tune.TuneConfig(\n            metric=\"eval-logloss\",\n            mode=\"min\",\n            scheduler=scheduler,\n            num_samples=1 if smoke_test else 10,\n        ),\n        param_space=search_space,\n    )\n    results = tuner.fit()\n    return results\n\n\nresults = tune_xgboost(smoke_test=SMOKE_TEST)\n\n# Load the best model checkpoint.\nbest_bst = get_best_model_checkpoint(results)\n\n# You could now do further predictions with\n# best_bst.predict(...)\n```\n\n----------------------------------------\n\nTITLE: Efficient Parallelization by Batching Tiny Tasks\nDESCRIPTION: Demonstrates how to effectively parallelize small tasks by batching them into larger units. This approach amortizes the overhead of remote invocation and achieves better performance than both sequential execution and individual tiny tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport ray\n\ndef tiny_work(x):\n    time.sleep(0.0001) # replace this is with work you need to do\n    return x\n\n@ray.remote\ndef mega_work(start, end):\n    return [tiny_work(x) for x in range(start, end)]\n\nstart = time.time()\nresult_ids = []\n[result_ids.append(mega_work.remote(x*1000, (x+1)*1000)) for x in range(100)]\nresults = ray.get(result_ids)\nprint(\"duration =\", time.time() - start)\n```\n\n----------------------------------------\n\nTITLE: Building and Training PPO Algorithm with RLlib\nDESCRIPTION: This snippet builds and trains an algorithm using the configured PPO setup. It iterates five times over the training process, performing sample collection, loss calculation, and model updates. Dependencies are managed through RLlib.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\n\n# Build the algorithm.\nalgo = config.build_algo()\n\n# Train it for 5 iterations ...\nfor _ in range(5):\n    pprint(algo.train())\n```\n\n----------------------------------------\n\nTITLE: Memory-Aware Scheduling with Static Memory Requirements in Ray\nDESCRIPTION: Example demonstrating how to specify memory requirements for tasks and actors using the memory parameter in the ray.remote decorator, enabling memory-aware scheduling for resource allocation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/memory-management.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# reserve 500MiB of available memory to place this task\n@ray.remote(memory=500 * 1024 * 1024)\ndef some_function(x):\n    pass\n\n# reserve 2.5GiB of available memory to place this actor\n@ray.remote(memory=2500 * 1024 * 1024)\nclass SomeActor:\n    def __init__(self, a, b):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Implementing MNIST Classifier with PyTorch Lightning\nDESCRIPTION: Implementation of a PyTorch Lightning module for MNIST classification. The model includes configurable layer sizes and learning rate, along with methods for training, validation, and loss calculation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MNISTClassifier(pl.LightningModule):\n    def __init__(self, config):\n        super(MNISTClassifier, self).__init__()\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10, top_k=1)\n        self.layer_1_size = config[\"layer_1_size\"]\n        self.layer_2_size = config[\"layer_2_size\"]\n        self.lr = config[\"lr\"]\n\n        # mnist images are (1, 28, 28) (channels, width, height)\n        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n        self.eval_loss = []\n        self.eval_accuracy = []\n\n    def cross_entropy_loss(self, logits, labels):\n        return F.nll_loss(logits, labels)\n\n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n\n        x = self.layer_1(x)\n        x = torch.relu(x)\n\n        x = self.layer_2(x)\n        x = torch.relu(x)\n\n        x = self.layer_3(x)\n        x = torch.log_softmax(x, dim=1)\n\n        return x\n\n    def training_step(self, train_batch, batch_idx):\n        x, y = train_batch\n        logits = self.forward(x)\n        loss = self.cross_entropy_loss(logits, y)\n        accuracy = self.accuracy(logits, y)\n\n        self.log(\"ptl/train_loss\", loss)\n        self.log(\"ptl/train_accuracy\", accuracy)\n        return loss\n\n    def validation_step(self, val_batch, batch_idx):\n        x, y = val_batch\n        logits = self.forward(x)\n        loss = self.cross_entropy_loss(logits, y)\n        accuracy = self.accuracy(logits, y)\n        self.eval_loss.append(loss)\n        self.eval_accuracy.append(accuracy)\n        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n\n    def on_validation_epoch_end(self):\n        avg_loss = torch.stack(self.eval_loss).mean()\n        avg_acc = torch.stack(self.eval_accuracy).mean()\n        self.log(\"ptl/val_loss\", avg_loss, sync_dist=True)\n        self.log(\"ptl/val_accuracy\", avg_acc, sync_dist=True)\n        self.eval_loss.clear()\n        self.eval_accuracy.clear()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n\n\nclass MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=128):\n        super().__init__()\n        self.data_dir = tempfile.mkdtemp()\n        self.batch_size = batch_size\n        self.transform = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        )\n\n    def setup(self, stage=None):\n        with FileLock(f\"{self.data_dir}.lock\"):\n            mnist = MNIST(\n                self.data_dir, train=True, download=True, transform=self.transform\n            )\n            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n\n            self.mnist_test = MNIST(\n                self.data_dir, train=False, download=True, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\n```\n\n----------------------------------------\n\nTITLE: Implementing HuggingFace Batch Inference with Ray\nDESCRIPTION: Implementation of a HuggingFace predictor class for text generation using GPT-2 model with GPU support. Uses Ray's data parallel processing with configurable batch size and GPU concurrency.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/batch_inference.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport ray\n\nds = ray.data.from_numpy(np.asarray([\"Complete this\", \"for me\"]))\n\nclass HuggingFacePredictor:\n    def __init__(self):\n        from transformers import pipeline\n        # Set \"cuda:0\" as the device so the Huggingface pipeline uses GPU.\n        self.model = pipeline(\"text-generation\", model=\"gpt2\", device=\"cuda:0\")\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\n        predictions = self.model(list(batch[\"data\"]), max_length=20, num_return_sequences=1)\n        batch[\"output\"] = [sequences[0][\"generated_text\"] for sequences in predictions]\n        return batch\n\n# Use 2 actors, each actor using 1 GPU. 2 GPUs total.\npredictions = ds.map_batches(\n    HuggingFacePredictor,\n    num_gpus=1,\n    # Specify the batch size for inference.\n    # Increase this for larger datasets.\n    batch_size=1,\n    # Set the concurrency to the number of GPUs in your cluster.\n    concurrency=2,\n    )\npredictions.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Training Worker Function for HPU LoRA Fine-tuning in Python\nDESCRIPTION: This worker function handles the complete training process for each worker. It adapts transformers to Gaudi, prepares training arguments, loads and preprocesses datasets, configures the tokenizer, prepares the model, sets up the Gaudi-specific configurations, and initializes the GaudiTrainer to train the model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef train_func_per_worker(config: Dict):\n    # adapt transformers to gaudi\n    adapt_transformers_to_gaudi()\n\n    # prepare training arguments\n    training_args = prepare_training_args(config)\n\n    # prepare datasets\n    # here we use dataset \"tatsu-lab/alpaca\" from huggingface\n    raw_datasets = datasets.DatasetDict({\"train\": datasets.load_dataset(\"tatsu-lab/alpaca\", split='train[0:4096]')})\n    preprocess_dataset(raw_datasets)\n\n    # prepare tokenizer\n    tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model\"])\n    tokenized_datasets = preprocess_dataset_to_tokenizer(raw_datasets, tokenizer)\n\n    # prepare model\n    model = prepare_model(config, training_args.device)\n\n    # prepare data collator\n    data_collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", mlm=False)\n\n    # prepare gaudi config\n    gaudi_config = GaudiConfig()\n    gaudi_config.use_fused_adam = True\n    gaudi_config.use_fused_clip_norm = True\n\n    # instance GaudiTrainer\n    trainer = GaudiTrainer(\n        model=model,\n        gaudi_config=gaudi_config,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=None,\n        preprocess_logits_for_metrics=None,\n    )\n\n    train_result = trainer.train()\n    print(f\"train_result = {train_result}\")\n    trainer.save_model()\n\n    return train_result\n```\n\n----------------------------------------\n\nTITLE: Creating and Transforming a Dataset in Ray Data (Python)\nDESCRIPTION: This snippet demonstrates how to create a Dataset from a range, add a column, and select columns. It showcases the lazy evaluation nature of Ray Data operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/key-concepts.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\ndataset = ray.data.range(100)\ndataset = dataset.add_column(\"test\", lambda x: x[\"id\"] + 1)\ndataset = dataset.select_columns(\"test\")\n```\n\n----------------------------------------\n\nTITLE: Implementing TorchTrainer for PyTorch Lightning\nDESCRIPTION: This code snippet demonstrates the new TorchTrainer API for PyTorch Lightning integration. It shows how to create a training function with a Lightning model, configure training with callbacks, and use Ray's scaling capabilities to train across multiple workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport lightning.pytorch as pl\n\nimport ray.train\nfrom ray.train.torch import TorchTrainer\nfrom ray.train.lightning import (\n    RayDDPStrategy,\n    RayLightningEnvironment,\n    RayTrainReportCallback,\n    prepare_trainer\n)\n\ndef train_func():\n    # [1] Create a Lightning model\n    model = MyLightningModule(lr=1e-3, feature_dim=128)\n\n    # [2] Report Checkpoint with callback\n    ckpt_report_callback = RayTrainReportCallback()\n\n    # [3] Create a Lighting Trainer\n    trainer = pl.Trainer(\n        max_epochs=10,\n        log_every_n_steps=100,\n        # New configurations below\n        devices=\"auto\",\n        accelerator=\"auto\",\n        strategy=RayDDPStrategy(),\n        plugins=[RayLightningEnvironment()],\n        callbacks=[ckpt_report_callback],\n    )\n\n    # Validate your Lightning trainer configuration\n    trainer = prepare_trainer(trainer)\n\n    # [4] Build your datasets on each worker\n    datamodule = MyLightningDataModule(batch_size=32)\n    trainer.fit(model, datamodule=datamodule)\n\n# [5] Explicitly define and run the training function\nray_trainer = TorchTrainer(\n    train_func,\n    scaling_config=ray.train.ScalingConfig(num_workers=4, use_gpu=True),\n    run_config=ray.train.RunConfig(\n        checkpoint_config=ray.train.CheckpointConfig(\n            num_to_keep=3,\n            checkpoint_score_attribute=\"val_accuracy\",\n            checkpoint_score_order=\"max\",\n        ),\n    )\n)\nresult = ray_trainer.fit()\n\n# [6] Load the trained model from a simplified checkpoint interface.\ncheckpoint: ray.train.Checkpoint = result.checkpoint\nwith checkpoint.as_directory() as checkpoint_dir:\n    print(\"Checkpoint contents:\", os.listdir(checkpoint_dir))\n    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.ckpt\")\n    model = MyLightningModule.load_from_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Remote Function Invocation with Ray - Improved Version\nDESCRIPTION: This snippet demonstrates an optimized approach to invoking remote tasks by explicitly storing a large object in the Ray object store using `ray.put()`, then passing its ID to the remote function. This approach significantly reduces the duration of invoking the remote function multiple times.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.shutdown()\n\nimport time\nimport numpy as np\nimport ray\n\nray.init(num_cpus=4)\n\n@ray.remote\ndef no_work(a):\n    return\n\nstart = time.time()\na_id = ray.put(np.zeros((5000, 5000)))\nresult_ids = [no_work.remote(a_id) for x in range(10)]\nresults = ray.get(result_ids)\nprint(\"duration =\", time.time() - start)\n```\n\n----------------------------------------\n\nTITLE: Sending Image Generation Request to Stable Diffusion API\nDESCRIPTION: Python script to send an HTTP GET request to the deployed Stable Diffusion service. Takes a text prompt as input and saves the generated image as a PNG file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/stable-diffusion.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nprompt = \"a cute cat is dancing on the grass.\"\ninput = \"%20\".join(prompt.split(\" \"))\nresp = requests.get(f\"http://127.0.0.1:8000/imagine?prompt={input}\")\nwith open(\"output.png\", 'wb') as f:\n    f.write(resp.content)\n```\n\n----------------------------------------\n\nTITLE: Setting Up PBT with RLlib using Python\nDESCRIPTION: This code snippet configures a population-based training (PBT) experiment using Ray Tune with the RLlib library. It sets up hyperparameter mutations, a custom exploration function, and stopping criteria before initiating the training of multiple PPO algorithm trials concurrently. Dependencies include Ray Tune and RLlib, with parameters such as the number of workers, CPUs, and GPUs configurable based on available resources. Inputs include specifications for training, while outputs are the resulting model configurations and performance metrics from the best trials.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_ppo_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nfrom ray import tune\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\nfrom ray.tune.schedulers import PopulationBasedTraining\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\"\n    )\n    args, _ = parser.parse_known_args()\n\n    # Postprocess the perturbed config to ensure it's still valid\n    def explore(config):\n        # ensure we collect enough timesteps to do sgd\n        if config[\"train_batch_size\"] < config[\"sgd_minibatch_size\"] * 2:\n            config[\"train_batch_size\"] = config[\"sgd_minibatch_size\"] * 2\n        # ensure we run at least one sgd iter\n        if config[\"num_sgd_iter\"] < 1:\n            config[\"num_sgd_iter\"] = 1\n        return config\n\n    hyperparam_mutations = {\n        \"clip_param\": lambda: random.uniform(0.01, 0.5),\n        \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n        \"num_epochs\": lambda: random.randint(1, 30),\n        \"minibatch_size\": lambda: random.randint(128, 16384),\n        \"train_batch_size_per_learner\": lambda: random.randint(2000, 160000),\n    }\n\n    pbt = PopulationBasedTraining(\n        time_attr=\"time_total_s\",\n        perturbation_interval=120,\n        resample_probability=0.25,\n        # Specifies the mutations of these hyperparams\n        hyperparam_mutations=hyperparam_mutations,\n        custom_explore_fn=explore,\n    )\n\n    # Stop when we've either reached 100 training iterations or reward=300\n    stopping_criteria = {\"training_iteration\": 100, \"episode_reward_mean\": 300}\n\n    config = (\n        PPOConfig()\n        .environment(\"Humanoid-v2\")\n        .env_runners(num_env_runners=4)\n        .training(\n            # These params are tuned from a fixed starting value.\n            kl_coeff=1.0,\n            lambda_=0.95,\n            clip_param=0.2,\n            lr=1e-4,\n            # These params start off randomly drawn from a set.\n            num_epochs=tune.choice([10, 20, 30]),\n            minibatch_size=tune.choice([128, 512, 2048]),\n            train_batch_size_per_learner=tune.choice([10000, 20000, 40000]),\n        )\n        .rl_module(\n            model_config=DefaultModelConfig(free_log_std=True),\n        )\n    )\n\n    tuner = tune.Tuner(\n        \"PPO\",\n        tune_config=tune.TuneConfig(\n            metric=\"env_runners/episode_return_mean\",\n            mode=\"max\",\n            scheduler=pbt,\n            num_samples=1 if args.smoke_test else 2,\n        ),\n        param_space=config,\n        run_config=tune.RunConfig(stop=stopping_criteria),\n    )\n    results = tuner.fit()\n\n```\n\n----------------------------------------\n\nTITLE: Development Workflow with Serve Run - Console\nDESCRIPTION: Console command for deploying and testing Ray Serve applications in a local or remote setup using 'serve run'. Ideal for development environments to observe logs directly in the console. Requires a multi-application config file as input.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/multi-app.md#2025-04-12_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ serve run config.yaml\n> 2023-04-04 11:00:05,901 INFO scripts.py:327 -- Deploying from config file: \"config.yaml\".\n> 2023-04-04 11:00:07,505 INFO worker.py:1613 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265\n> 2023-04-04 11:00:09,012 SUCC scripts.py:393 -- Submitted deploy config successfully.\n```\n\n----------------------------------------\n\nTITLE: Defining Text Translation Model\nDESCRIPTION: This snippet defines a text translation model using the Translator class. The model utilizes the t5-small transformer model to translate text from English to French and returns the output in a specific dictionary format. The example demonstrates the basic functional structure of reading English input and outputting its French translation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/getting_started.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nclass Translator:\n    def __init__(self):\n        self.model = ...  # Loads the t5-small model\n    \n    def translate(self, text):\n        return self.model(text)\n```\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Class API Trainable in Ray Tune\nDESCRIPTION: Example demonstrating how to define a trainable using the class-based API for optimizing hyperparameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Trainable(tune.Trainable):\n    def setup(self, config):\n        self.a = config[\"a\"]\n        self.b = config[\"b\"]\n        self.x = 0\n\n    def step(self):  # This is called iteratively.\n        score = self.a * (self.x ** 2) + self.b\n        self.x += 1\n        return {\"score\": score}\n```\n\n----------------------------------------\n\nTITLE: Implementing Worker Fault Tolerance in Ray Train with PyTorch (Python)\nDESCRIPTION: This code snippet shows a complete example of implementing worker fault tolerance in a PyTorch training script using Ray Train. It includes checkpoint saving and loading logic to resume training after failures.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/fault-tolerance.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom ray.train import RunConfig, FailureConfig, ScalingConfig, Checkpoint\nfrom ray.train.torch import TorchTrainer\n\ndef train_func():\n    model = torch.nn.Linear(1, 1)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n    checkpoint = ray.train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as dir:\n            state_dict = torch.load(os.path.join(dir, \"checkpoint.pt\"))\n            model.load_state_dict(state_dict[\"model\"])\n            optimizer.load_state_dict(state_dict[\"optimizer\"])\n            start_epoch = state_dict[\"epoch\"]\n    else:\n        start_epoch = 0\n\n    for epoch in range(start_epoch, 10):\n        loss = torch.nn.MSELoss()(model(torch.ones(1)), torch.zeros(1))\n        loss.backward()\n        optimizer.step()\n\n        checkpoint_data = {\n            \"epoch\": epoch,\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict()\n        }\n        checkpoint = Checkpoint.from_dict(checkpoint_data)\n        ray.train.report({\"loss\": loss.item()}, checkpoint=checkpoint)\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(num_workers=4),\n    run_config=RunConfig(\n        failure_config=FailureConfig(max_failures=3),\n        storage_path=\"s3://bucket/\"\n    )\n)\n\nresults = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Creating a Training Function with Ray Tune Integration\nDESCRIPTION: Function that trains a PyTorch model with hyperparameters supplied by Ray Tune. It reports the validation accuracy back to Tune using tune.report() to enable optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_cifar(config, checkpoint_dir=None):\n    # Data preparation\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(\n        root=\"./data\", train=True, download=True, transform=transform)\n\n    testset = torchvision.datasets.CIFAR10(\n        root=\"./data\", train=False, download=True, transform=transform)\n\n    # Split training data\n    test_abs = int(len(trainset) * 0.8)\n    train_subset, val_subset = random_split(\n        trainset, [test_abs, len(trainset) - test_abs])\n\n    trainloader = torch.utils.data.DataLoader(\n        train_subset,\n        batch_size=int(config[\"batch_size\"]),\n        shuffle=True,\n        num_workers=8)\n    valloader = torch.utils.data.DataLoader(\n        val_subset,\n        batch_size=int(config[\"batch_size\"]),\n        shuffle=True,\n        num_workers=8)\n\n    # Build network based on the configured hyperparameters\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if torch.cuda.device_count() > 1:\n            net = nn.DataParallel(ConvNet())\n    net = ConvNet().to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n\n    # Load checkpoint if one exists\n    if checkpoint_dir:\n        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n        model_state, optimizer_state = torch.load(checkpoint)\n        net.load_state_dict(model_state)\n        optimizer.load_state_dict(optimizer_state)\n\n    # Training loop\n    for epoch in range(10):  # loop over the dataset multiple times\n        running_loss = 0.0\n        epoch_steps = 0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            epoch_steps += 1\n            if i % 2000 == 1999:  # print every 2000 mini-batches\n                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n                                                 running_loss / epoch_steps))\n                running_loss = 0.0\n\n        # Validation loss\n        val_loss = 0.0\n        val_steps = 0\n        total = 0\n        correct = 0\n        for i, data in enumerate(valloader, 0):\n            with torch.no_grad():\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = net(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                loss = criterion(outputs, labels)\n                val_loss += loss.cpu().numpy()\n                val_steps += 1\n\n        # Store checkpoint for ASHA scheduler\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            checkpoint = os.path.join(temp_checkpoint_dir, \"checkpoint\")\n            torch.save(\n                (net.state_dict(), optimizer.state_dict()), checkpoint)\n            checkpoint_dir = temp_checkpoint_dir\n\n        # Report metrics to Tune\n        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n    print(\"Finished Training\")\n```\n\n----------------------------------------\n\nTITLE: Basic Ray Train Setup with Experiment Tracking\nDESCRIPTION: Demonstrates the basic structure for using experiment tracking libraries with Ray Train. It shows how to define a training function and set up a TorchTrainer with a scaling configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig\n\ndef train_func():\n    # Training code and native experiment tracking library calls go here.\n\nscaling_config = ScalingConfig(num_workers=2, use_gpu=True)\ntrainer = TorchTrainer(train_func, scaling_config=scaling_config)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Tune for Population Based Training in Python\nDESCRIPTION: This snippet demonstrates how to install Ray Tune, a necessary dependency for utilizing Population Based Training (PBT). It ensures that the environment is set up for subsequent examples and implementations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"ray[tune]\"\n```\n\n----------------------------------------\n\nTITLE: Converting ObjectRefs to asyncio.Futures in Ray\nDESCRIPTION: Shows how to convert Ray ObjectRefs to asyncio.Futures, allowing the use of 'await' on Ray futures in concurrent applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/async_api.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport asyncio\n\n@ray.remote\ndef some_task():\n    return 1\n\nasync def await_obj_ref():\n    await some_task.remote()\n    await asyncio.wait([some_task.remote()])\n\nasyncio.run(await_obj_ref())\n```\n\n----------------------------------------\n\nTITLE: Plotting Results from a Tune Experiment\nDESCRIPTION: Code to plot the performance of a trial using the ResultGrid object returned by Tuner.fit(). It extracts data for visualization and analysis.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Plot with Pandas\nimport matplotlib.pyplot as plt\n\ndfs = result_grid.get_dataframe()\n\n# Plot by epoch\nfig, ax = plt.subplots(figsize=(12, 8))\ndfs.loc[:, [\"training_iteration\", \"accuracy\"]].plot(x=\"training_iteration\", y=\"accuracy\", ax=ax)\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Training Iteration\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Fine-tuning Configuration for Ray Train\nDESCRIPTION: This code sets up the configuration for fine-tuning the model using Ray Train, including model initialization, metric loading, and naming conventions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\n\nfrom datasets import load_metric\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nimport ray.train\nfrom ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n\nnum_labels = 3 if task.startswith(\"mnli\") else 1 if task == \"stsb\" else 2\nmetric_name = (\n    \"pearson\"\n    if task == \"stsb\"\n    else \"matthews_correlation\"\n    if task == \"cola\"\n    else \"accuracy\"\n)\nmodel_name = model_checkpoint.split(\"/\")[-1]\nvalidation_key = (\n    \"validation_mismatched\"\n    if task == \"mnli-mm\"\n    else \"validation_matched\"\n    if task == \"mnli\"\n    else \"validation\"\n)\nname = f\"{model_name}-finetuned-{task}\"\n```\n\n----------------------------------------\n\nTITLE: Installing RLlib and Dependencies with pip\nDESCRIPTION: Command to install Ray RLlib along with PyTorch and Farama Gymnasium, which includes Atari, ROM license, and MuJoCo environments for reinforcement learning experiments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[rllib]\" torch \"gymnasium[atari,accept-rom-license,mujoco]\"\n```\n\n----------------------------------------\n\nTITLE: Starting Ray via CLI - Bash\nDESCRIPTION: This Bash command is used to start a Ray runtime on a single machine by opening a head node. It's required that the user runs the specified command in their terminal to activate the Ray environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\n$ ray start --head --port=6379\n\nLocal node IP: 192.123.1.123\n2020-09-20 10:38:54,193 INFO services.py:1166 -- View the Ray dashboard at http://localhost:8265\n\n--------------------\nRay runtime started.\n--------------------\n```\n\n----------------------------------------\n\nTITLE: Converting DeepSpeed ZeRO-3 Checkpoint to FP32 for Vicuna-13B in Python\nDESCRIPTION: This snippet defines a function to convert a DeepSpeed ZeRO-3 checkpoint to a single FP32 checkpoint file. It removes optimizer states and adjusts the state dict keys to be compatible with the HuggingFace Vicuna model.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nfrom deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\ndef extract_fp32_ckpt_from_zero(zero_ckpt_dir):\n    state_dict = get_fp32_state_dict_from_zero_checkpoint(zero_ckpt_dir)\n    vicuna_state_dict = {\n        k.replace(\"_forward_module.model.\", \"\"): v for k, v in state_dict.items()\n    }\n    torch.save(vicuna_state_dict, os.path.join(zero_ckpt_dir, \"full_model.pt\"))\n\n\nfull_model_ckpt_path = \"/mnt/local_storage/checkpoint.ckpt/full_model.pt\"\nextract_fp32_ckpt_from_zero(\"/mnt/local_storage/checkpoint.ckpt\")\n```\n\n----------------------------------------\n\nTITLE: Text Summarization and Translation Application\nDESCRIPTION: This Python script defines a Ray Serve application that takes English text as input, summarizes it, and translates it into a specified language (defaulting to French). It uses the `transformers` and `torch` libraries for NLP tasks.  The script showcases a basic Ray Serve deployment configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/index.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Text summarization and translation application.\n\"\"\"\nimport ray\nfrom ray import serve\nimport torch\n\ntry:\n    from transformers import pipeline\nexcept ImportError:\n    raise ImportError(\n        \"Please install transformers with: pip install transformers\"\n    )\n\n@serve.deployment\nclass Translator:\n    def __init__(self, language: str = \"french\"):\n        self.model = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n\n    def translate(self, text: str) -> str:\n        return self.model(text)[0][\"translation_text\"]\n\n    async def __call__(self, request):\n        text = await request.body()\n        return self.translate(text.decode(\"utf-8\"))\n\n\n@serve.deployment\nclass Summarizer:\n    def __init__(self):\n        self.model = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n    def summarize(self, text: str) -> str:\n        return self.model(text, min_length=5, max_length=15)[0][\"summary_text\"]\n\n    async def __call__(self, request):\n        text = await request.body()\n        return self.summarize(text.decode(\"utf-8\"))\n\n\n@serve.deployment\nclass TextML:\n    def __init__(self, translator: serve.deployment, summarizer: serve.deployment):\n        self.translator = translator.get_handle()\n        self.summarizer = summarizer.get_handle()\n\n    async def __call__(self, request):\n        text = await request.body()\n        text = text.decode(\"utf-8\")\n        summary = await self.summarizer.remote(text)\n        translation = await self.translator.remote(summary)\n        return translation\n\n\ntranslator = Translator.bind()\nsummarizer = Summarizer.bind()\napp = TextML.bind(translator, summarizer)\n\nif __name__ == \"__main__\":\n    # Run with:\n    # serve run text_ml:app\n\n    ray.init(ignore_reinit_error=True)\n    serve.start(detached=True)\n\n    print(serve.run(app, _blocking=True))\n\n    # Send a request to the endpoint:\n    # curl -X POST -d 'This is a string of text to summarize and translate.' http://localhost:8000/\n\n```\n\n----------------------------------------\n\nTITLE: Installing Ray and Dependencies\nDESCRIPTION: Commands to install Ray with data and train modules, along with other required libraries like PyTorch and torchvision.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_detection.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install 'ray[data,train]'\n!pip install torch torchmetrics torchvision xmltodict\n```\n\n----------------------------------------\n\nTITLE: Using Accelerator Types in Ray Tasks (Python)\nDESCRIPTION: This code shows how to use the `accelerator_type` option in `ray.remote` to specify that a task should be executed on a node with a specific accelerator, such as an NVIDIA Tesla V100. The example defines a simple `train` task and executes it, ensuring that it runs on a node with the required accelerator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    from ray.util.accelerators import NVIDIA_TESLA_V100\n\n    @ray.remote(num_gpus=1, accelerator_type=NVIDIA_TESLA_V100)\n    def train(data):\n        return \"This function was run on a node with a Tesla V100 GPU\"\n\n    ray.get(train.remote(1))\n\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch DataLoader for Distributed Training with Ray Train\nDESCRIPTION: Shows how to use ray.train.torch.prepare_data_loader() to set up a PyTorch DataLoader for distributed training. This function adds a DistributedSampler and moves batches to the correct device.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n from torch.utils.data import DataLoader\n+import ray.train.torch\n\n def train_func():\n\n     ...\n\n     dataset = ...\n\n     data_loader = DataLoader(dataset, batch_size=worker_batch_size, shuffle=True)\n+    data_loader = ray.train.torch.prepare_data_loader(data_loader)\n\n     for epoch in range(10):\n+        if ray.train.get_context().get_world_size() > 1:\n+            data_loader.sampler.set_epoch(epoch)\n\n         for X, y in data_loader:\n-            X = X.to_device(device)\n-            y = y.to_device(device)\n\n     ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Request Handling and Batch Processing Logic in Python\nDESCRIPTION: This code snippet shows the main logic for handling requests, including the handle_request, run_model, generate_text, and consume_streamer methods. It demonstrates how to batch requests, generate text, and stream the output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def handle_request(self, prompt: str):\n    return StreamingResponse(self.run_model(prompt))\n\n@serve.batch(max_batch_size=4)\nasync def run_model(self, prompts: list):\n    streamer = RawStreamer()\n    Thread(target=self.generate_text, args=(prompts, streamer)).start()\n    async for batch in self.consume_streamer(streamer, len(prompts)):\n        yield batch\n\ndef generate_text(self, prompts, streamer):\n    inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True)\n    self.model.generate(\n        **inputs,\n        streamer=streamer,\n        max_new_tokens=50,\n    )\n    streamer.end()\n\nasync def consume_streamer(self, streamer, batch_size):\n    decoded = [\"\"] * batch_size\n    async for output in streamer.text_generator:\n        batch = output.tolist()\n        for i, tokens in enumerate(batch):\n            if tokens == []:\n                decoded[i] = StopIteration\n            else:\n                decoded[i] += self.tokenizer.decode(tokens)\n        yield decoded\n```\n\n----------------------------------------\n\nTITLE: Installing Ray with machine learning dependencies\nDESCRIPTION: Installs Ray with data, train, tune, and serve components for machine learning applications. Alternatively, it shows how to install RLlib for reinforcement learning support.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -U \"ray[data,train,tune,serve]\"\n\n# For reinforcement learning support, install RLlib instead.\n# pip install -U \"ray[rllib]\"\n```\n\n----------------------------------------\n\nTITLE: Complete ASHA-based Hyperparameter Tuning Function for MNIST\nDESCRIPTION: A comprehensive function for hyperparameter tuning of an MNIST classifier using Ray Tune with ASHA scheduler. It defines the search space, configures the scheduler and reporter, sets up resource allocation, and runs the tuning process with proper reporting.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef tune_mnist_asha(num_samples=10, num_epochs=10, gpus_per_trial=0, data_dir=\"~/data\"):\n    config = {\n        \"layer_1_size\": tune.choice([32, 64, 128]),\n        \"layer_2_size\": tune.choice([64, 128, 256]),\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"batch_size\": tune.choice([32, 64, 128]),\n    }\n\n    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n\n    reporter = CLIReporter(\n        parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"],\n    )\n\n    train_fn_with_parameters = tune.with_parameters(\n        train_mnist_tune,\n        num_epochs=num_epochs,\n        num_gpus=gpus_per_trial,\n        data_dir=data_dir,\n    )\n    resources_per_trial = {\"cpu\": 1, \"gpu\": gpus_per_trial}\n\n    tuner = tune.Tuner(\n        tune.with_resources(train_fn_with_parameters, resources=resources_per_trial),\n        tune_config=tune.TuneConfig(\n            metric=\"loss\",\n            mode=\"min\",\n            scheduler=scheduler,\n            num_samples=num_samples,\n        ),\n        run_config=tune.RunConfig(\n            name=\"tune_mnist_asha\",\n            progress_reporter=reporter,\n        ),\n        param_space=config,\n    )\n    results = tuner.fit()\n\n    print(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Inline YAML Configuration for LLM Deployment\nDESCRIPTION: Define multiple LLM models and their configurations directly in a single YAML file for production deployment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# config.yaml\napplications:\n- args:\n    llm_configs:\n        - model_loading_config:\n            model_id: qwen-0.5b\n            model_source: Qwen/Qwen2.5-0.5B-Instruct\n          accelerator_type: A10G\n          deployment_config:\n            autoscaling_config:\n                min_replicas: 1\n                max_replicas: 2\n        - model_loading_config:\n            model_id: qwen-1.5b\n            model_source: Qwen/Qwen2.5-1.5B-Instruct\n          accelerator_type: A10G\n          deployment_config:\n            autoscaling_config:\n                min_replicas: 1\n                max_replicas: 2\n  import_path: ray.serve.llm:build_openai_app\n  name: llm_app\n  route_prefix: \"/\"\n```\n\n----------------------------------------\n\nTITLE: Checking RayService Application Status\nDESCRIPTION: Commands to retrieve and describe the status of the RayService, showing deployment and component health details\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/kubernetes.md#2025-04-12_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get rayservices\nNAME                AGE\nrayservice-sample   7s\n\n$ kubectl describe rayservice rayservice-sample\n```\n\n----------------------------------------\n\nTITLE: Running a Basic Tuning Experiment\nDESCRIPTION: Function to evaluate a single trial with Ray Tune, sampling hyperparameters randomly from a uniform distribution. It uses Tuner.fit() to run the experiment and returns a ResultGrid object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef test_best_model(best_trial):\n    best_trained_model = ConvNet()\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if torch.cuda.device_count() > 1:\n            best_trained_model = nn.DataParallel(best_trained_model)\n    best_trained_model.to(device)\n\n    checkpoint_path = os.path.join(best_trial.checkpoint.path, \"checkpoint\")\n\n    model_state, optimizer_state = torch.load(checkpoint_path)\n    best_trained_model.load_state_dict(model_state)\n\n    # Prepare test data\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n    testset = torchvision.datasets.CIFAR10(\n        root=\"./data\", train=False, download=True, transform=transform)\n\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=4, shuffle=False, num_workers=2)\n\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = best_trained_model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total\n```\n\n----------------------------------------\n\nTITLE: Transforming Images with Ray Data\nDESCRIPTION: Example of image transformation using map_batches to increase image brightness.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-images.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\nimport numpy as np\nimport ray\n\ndef increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\n    return batch\n\nds = (\n    ray.data.read_images(\"s3://anonymous@ray-example-data/batoidea/JPEGImages\")\n    .map_batches(increase_brightness)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Function for Distributed PyTorch Model\nDESCRIPTION: This function implements the training loop for the PyTorch model. It handles data loading, model initialization, and training iterations. The function is designed to work with Ray Train for distributed training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/tune_cifar_torch_pbt_example.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef train_cifar(config):\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    train_loader, test_loader = load_data(config[\"data_dir\"])\n\n    model = Net().to(device)\n\n    optimizer = torch.optim.SGD(\n        model.parameters(),\n        lr=config[\"lr\"],\n        momentum=config[\"momentum\"],\n        weight_decay=config[\"decay\"],\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    model = train.torch.prepare_model(model)\n    train_loader = train.torch.prepare_data_loader(train_loader)\n    test_loader = train.torch.prepare_data_loader(test_loader)\n\n    results = []\n    for epoch in range(50):\n        train_acc, train_loss = train_epoch(model, optimizer, train_loader, criterion)\n        results.append(train_acc)\n        session.report(\n            {\n                \"mean_accuracy\": sum(results[-5:]) / len(results[-5:]),\n                \"training_iteration\": epoch,\n            }\n        )\n```\n\n----------------------------------------\n\nTITLE: Defining Configurable Neural Network Architecture\nDESCRIPTION: PyTorch neural network class with configurable layer sizes for the fully connected layers, allowing hyperparameter tuning of network architecture.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-cifar.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Data Ingestion with PyTorch Lightning and Ray Train\nDESCRIPTION: Python script showing how to use Ray Data and Ray Train with PyTorch Lightning. It demonstrates creating datasets, accessing them in the training function, and using them with PyTorch Lightning's Trainer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import train\n\n# Create the train and validation datasets.\ntrain_data = ray.data.read_csv(\"./train.csv\")\nval_data = ray.data.read_csv(\"./validation.csv\")\n\ndef train_func_per_worker():\n    # Access Ray datsets in your train_func via ``get_dataset_shard``.\n    # Ray Data shards all datasets across workers by default.\n    train_ds = train.get_dataset_shard(\"train\")\n    val_ds = train.get_dataset_shard(\"validation\")\n\n    # Create Ray dataset iterables via ``iter_torch_batches``.\n    train_dataloader = train_ds.iter_torch_batches(batch_size=16)\n    val_dataloader = val_ds.iter_torch_batches(batch_size=16)\n\n    ...\n\n    trainer = pl.Trainer(\n        # ...\n    )\n\n    # Feed the Ray dataset iterables to ``pl.Trainer.fit``.\n    trainer.fit(\n        model,\n        train_dataloaders=train_dataloader,\n        val_dataloaders=val_dataloader\n    )\n\ntrainer = TorchTrainer(\n    train_func,\n    # You can pass in multiple datasets to the Trainer.\n    datasets={\"train\": train_data, \"validation\": val_data},\n    scaling_config=ScalingConfig(num_workers=4),\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Configuring ASHA Scheduler for Efficient Hyperparameter Search\nDESCRIPTION: Creates an ASHAScheduler that will intelligently terminate poorly-performing trials early to save computational resources during hyperparameter tuning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nscheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray and Parallelizing Functions with Python Tasks\nDESCRIPTION: Demonstrates how to use Ray to convert a simple function into a distributed task that can be executed in parallel across multiple workers\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init()\n\n@ray.remote\ndef f(x):\n    return x * x\n\nfutures = [f.remote(i) for i in range(4)]\nprint(ray.get(futures)) # [0, 1, 4, 9]\n```\n\n----------------------------------------\n\nTITLE: Implementing Checkpoint Reporting with TensorFlow\nDESCRIPTION: Example showing how to implement checkpoint saving and reporting in a distributed TensorFlow training function using Ray Train's checkpoint system.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/distributed-tensorflow-keras.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nimport tempfile\n\nfrom ray import train\nfrom ray.train import Checkpoint, ScalingConfig\nfrom ray.train.tensorflow import TensorflowTrainer\n\nimport numpy as np\n\ndef train_func(config):\n    import tensorflow as tf\n    n = 100\n    # create a toy dataset\n    # data   : X - dim = (n, 4)\n    # target : Y - dim = (n, 1)\n    X = np.random.normal(0, 1, size=(n, 4))\n    Y = np.random.uniform(0, 1, size=(n, 1))\n\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    with strategy.scope():\n        # toy neural network : 1-layer\n        model = tf.keras.Sequential([tf.keras.layers.Dense(1, activation=\"linear\", input_shape=(4,))])\n        model.compile(optimizer=\"Adam\", loss=\"mean_squared_error\", metrics=[\"mse\"])\n\n    for epoch in range(config[\"num_epochs\"]):\n        history = model.fit(X, Y, batch_size=20)\n\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            model.save(os.path.join(temp_checkpoint_dir, \"model.keras\"))\n            checkpoint_dict = os.path.join(temp_checkpoint_dir, \"checkpoint.json\")\n            with open(checkpoint_dict, \"w\") as f:\n                json.dump({\"epoch\": epoch}, f)\n            checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n\n            train.report({\"loss\": history.history[\"loss\"][0]}, checkpoint=checkpoint)\n\ntrainer = TensorflowTrainer(\n    train_func,\n    train_loop_config={\"num_epochs\": 5},\n    scaling_config=ScalingConfig(num_workers=2),\n)\nresult = trainer.fit()\nprint(result.checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Transformers with Ray Train Integration\nDESCRIPTION: A complete implementation showing how to integrate Ray Train with Hugging Face Transformers for distributed training, including reporting metrics, preparing the trainer, and loading trained models.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-transformers.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport numpy as np\nimport evaluate\nfrom datasets import load_dataset\nfrom transformers import (\n    Trainer,\n    TrainingArguments,\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n)\n\nimport ray.train.huggingface.transformers\nfrom ray.train import ScalingConfig\nfrom ray.train.torch import TorchTrainer\n\n\n# [1] Encapsulate data preprocessing, training, and evaluation\n# logic in a training function\n# ============================================================\ndef train_func():\n    # Datasets\n    dataset = load_dataset(\"yelp_review_full\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n    small_train_dataset = (\n        dataset[\"train\"].select(range(100)).map(tokenize_function, batched=True)\n    )\n    small_eval_dataset = (\n        dataset[\"test\"].select(range(100)).map(tokenize_function, batched=True)\n    )\n\n    # Model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-cased\", num_labels=5\n    )\n\n    # Evaluation Metrics\n    metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    # Hugging Face Trainer\n    training_args = TrainingArguments(\n        output_dir=\"test_trainer\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        report_to=\"none\",\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=small_train_dataset,\n        eval_dataset=small_eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    # [2] Report Metrics and Checkpoints to Ray Train\n    # ===============================================\n    callback = ray.train.huggingface.transformers.RayTrainReportCallback()\n    trainer.add_callback(callback)\n\n    # [3] Prepare Transformers Trainer\n    # ================================\n    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)\n\n    # Start Training\n    trainer.train()\n\n\n# [4] Define a Ray TorchTrainer to launch `train_func` on all workers\n# ===================================================================\nray_trainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(num_workers=2, use_gpu=True),\n    # [4a] For multi-node clusters, configure persistent storage that is\n    # accessible across all worker nodes\n    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n)\nresult: ray.train.Result = ray_trainer.fit()\n\n# [5] Load the trained model\nwith result.checkpoint.as_directory() as checkpoint_dir:\n    checkpoint_path = os.path.join(\n        checkpoint_dir,\n        ray.train.huggingface.transformers.RayTrainReportCallback.CHECKPOINT_NAME,\n    )\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Defining Ray Train Function for Dolly-v2 Model Training in Python\nDESCRIPTION: This code defines the training function for each worker in Ray Train. It sets up the Dolly-v2 model, configures data ingestion using Ray Data, and initializes a PyTorch Lightning Trainer with FSDP strategy and mixed precision.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import Checkpoint\nfrom ray.train.lightning import RayLightningEnvironment, RayTrainReportCallback, prepare_trainer\n\n# Training function for each worker\ndef train_func(config):\n    lr = config[\"lr\"]\n    eps = config[\"eps\"]\n    strategy = config[\"strategy\"]\n    batch_size_per_worker = config[\"batch_size_per_worker\"]\n\n    # Model\n    model = DollyV2Model(lr=lr, eps=eps)\n\n    # Ray Data Ingestion\n    train_ds = ray.train.get_dataset_shard(\"train\")\n    train_dataloader = train_ds.iter_torch_batches(batch_size=batch_size_per_worker)\n\n    # Lightning Trainer\n    trainer = pl.Trainer(\n        max_epochs=1, \n        devices=\"auto\",\n        accelerator=\"auto\", \n        precision=\"16-mixed\",\n        strategy=strategy,\n        plugins=[RayLightningEnvironment()],\n        callbacks=[RayTrainReportCallback()],\n        enable_checkpointing=False,\n    )\n\n    trainer = prepare_trainer(trainer)\n\n    trainer.fit(model, train_dataloaders=train_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Setting Up PyTorch Training with Ray\nDESCRIPTION: This snippet defines a training function compatible with Ray, where data shards are processed in batches across multiple epochs. It demonstrates setting up a trainer with PyTorch as the backend, configuring dataset and workers, converting the training function into a tunable trainable, and executing it using a tuner. The setup uses Ray's intermediate reporting API for reporting within epochs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/reproducibility.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import tune\n\ndef training_func(config):\n    dataloader = ray.train.get_dataset()\\\n        .get_shard(torch.rank())\\\n        .iter_torch_batches(batch_size=config[\"batch_size\"])\n\n    for i in config[\"epochs\"]:\n        ray.train.report(...)  # use same intermediate reporting API\n\n# Declare the specification for training.\ntrainer = Trainer(backend=\"torch\", num_workers=12, use_gpu=True)\ndataset = ray.dataset.window()\n\n# Convert this to a trainable.\ntrainable = trainer.to_tune_trainable(training_func, dataset=dataset)\n\ntuner = tune.Tuner(trainable,\n    param_space={\"lr\": tune.uniform(), \"batch_size\": tune.randint(1, 2, 3)},\n    tune_config=tune.TuneConfig(num_samples=12))\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Distributed HPU Training with Ray in Python\nDESCRIPTION: This function configures and initializes distributed training for Llama models on HPU devices using Ray. It supports both DDP and DeepSpeed training methods with different execution modes (lazy, eager, eager.compile). The function sets up the configuration, initializes Ray, creates a TorchTrainer, and starts the training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef train_llama(num_workers, execution_mode, training_method):\n    import ray\n    from ray.train import ScalingConfig\n    from ray.train.torch import TorchTrainer, TorchConfig\n\n    # deepspeed config, can also place it to config file\n    deepspeed_config = {\n        \"steps_per_print\": 64,\n        \"train_batch_size\": \"auto\",\n        \"train_micro_batch_size_per_gpu\": \"auto\",\n        \"gradient_accumulation_steps\": \"auto\",\n        \"bf16\": {\n            \"enabled\": True\n        },\n        \"gradient_clipping\": 1.0,\n        \"zero_optimization\": {\n            \"stage\": 3,\n            \"overlap_comm\": False,\n            \"contiguous_gradients\": False,\n            \"stage3_gather_16bit_weights_on_model_save\": True\n        }\n    }\n\n    # Preparing train configurations\n    train_config = {\n        \"execution_mode\": execution_mode,\n        \"model\": \"/root/models/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080/\",\n        \"model_config\": {\"torch_dtype\": torch.bfloat16, \"trust_remote_code\": False, \"use_auth_token\": None},\n        \"lora_config\": {\"task_type\": \"CAUSAL_LM\", \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1, \"target_modules\": [\"q_proj\", \"v_proj\"]},\n        \"lr\": 1e-4,\n        \"epochs\": 2,\n        \"batch_size_per_worker\": 8,\n        \"output\": \"/tmp/ray/\",\n        \"deepspeed\": deepspeed_config if training_method == \"deepspeed\" else None,\n    }\n\n    # Configure computation resources\n    # In ScalingConfig, require an HPU for each worker\n    scaling_config = ScalingConfig(num_workers=num_workers, resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n    # Set backend to hccl in TorchConfig\n    torch_config = TorchConfig(backend = \"hccl\")\n\n    # start your ray cluster\n    ray.init()\n\n    # Initialize a Ray TorchTrainer\n    trainer = TorchTrainer(\n        train_loop_per_worker=train_func_per_worker,\n        train_loop_config=train_config,\n        torch_config=torch_config,\n        scaling_config=scaling_config,\n    )\n\n    result = trainer.fit()\n    print(f\"Training result: {result}\")\n```\n\n----------------------------------------\n\nTITLE: Stateful Transforms with Python Classes in Ray Datasets\nDESCRIPTION: Demonstrates how to use a Python class for stateful transforms in Ray Datasets, allowing for expensive setup operations to be performed once per worker.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport ray\n\nclass TorchPredictor:\n\n    def __init__(self):\n        self.model = torch.nn.Identity()\n        self.model.eval()\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        inputs = torch.as_tensor(batch[\"data\"], dtype=torch.float32)\n        with torch.inference_mode():\n            batch[\"output\"] = self.model(inputs).detach().numpy()\n        return batch\n\nds = (\n    ray.data.from_numpy(np.ones((32, 100)))\n    .map_batches(TorchPredictor, concurrency=2)\n)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Training Script with Ray Train Integration\nDESCRIPTION: Modified version of the PyTorch training script using Ray Train for distributed training. This script demonstrates how to prepare the model, data loader, and training loop for distributed execution, as well as how to report metrics and save checkpoints using Ray Train utilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import resnet18\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Normalize, Compose\n\nimport ray.train.torch\n\ndef train_func():\n    # Model, Loss, Optimizer\n    model = resnet18(num_classes=10)\n    model.conv1 = torch.nn.Conv2d(\n        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n    )\n    # [1] Prepare model.\n    model = ray.train.torch.prepare_model(model)\n    # model.to(\"cuda\")  # This is done by `prepare_model`\n    criterion = CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=0.001)\n\n    # Data\n    transform = Compose([ToTensor(), Normalize((0.28604,), (0.32025,))])\n    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n    train_data = FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n    # [2] Prepare dataloader.\n    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n\n    # Training\n    for epoch in range(10):\n        if ray.train.get_context().get_world_size() > 1:\n            train_loader.sampler.set_epoch(epoch)\n\n        for images, labels in train_loader:\n            # This is done by `prepare_data_loader`!\n            # images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # [3] Report metrics and checkpoint.\n        metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            torch.save(\n                model.module.state_dict(),\n                os.path.join(temp_checkpoint_dir, \"model.pt\")\n            )\n            ray.train.report(\n                metrics,\n                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n            )\n        if ray.train.get_context().get_world_rank() == 0:\n            print(metrics)\n\n# [4] Configure scaling and resource requirements.\nscaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=True)\n\n# [5] Launch distributed training job.\ntrainer = ray.train.torch.TorchTrainer(\n    train_func,\n    scaling_config=scaling_config,\n    # [5a] If running in a multi-node cluster, this is where you\n    # should configure the run's persistent storage that is accessible\n    # across all worker nodes.\n    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n)\nresult = trainer.fit()\n\n# [6] Load the trained model.\nwith result.checkpoint.as_directory() as checkpoint_dir:\n    model_state_dict = torch.load(os.path.join(checkpoint_dir, \"model.pt\"))\n    model = resnet18(num_classes=10)\n    model.conv1 = torch.nn.Conv2d(\n        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n    )\n    model.load_state_dict(model_state_dict)\n```\n\n----------------------------------------\n\nTITLE: Data Ingestion with PyTorch and Ray Train\nDESCRIPTION: Python script demonstrating the setup of data ingestion using Ray Data and Ray Train with PyTorch. It covers creating a dataset, preprocessing, inputting to the Trainer, and consuming in the training function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport ray\nfrom ray import train\nfrom ray.train import Checkpoint, ScalingConfig\nfrom ray.train.torch import TorchTrainer\n\n# Set this to True to use GPU.\n# If False, do CPU training instead of GPU training.\nuse_gpu = False\n\n# Step 1: Create a Ray Dataset from in-memory Python lists.\n# You can also create a Ray Dataset from many other sources and file\n# formats.\ntrain_dataset = ray.data.from_items([{\"x\": [x], \"y\": [2 * x]} for x in range(200)])\n\n# Step 2: Preprocess your Ray Dataset.\ndef increment(batch):\n    batch[\"y\"] = batch[\"y\"] + 1\n    return batch\n\ntrain_dataset = train_dataset.map_batches(increment)\n\n\ndef train_func():\n    batch_size = 16\n\n    # Step 4: Access the dataset shard for the training worker via\n    # ``get_dataset_shard``.\n    train_data_shard = train.get_dataset_shard(\"train\")\n    # `iter_torch_batches` returns an iterable object that\n    # yield tensor batches. Ray Data automatically moves the Tensor batches\n    # to GPU if you enable GPU training.\n    train_dataloader = train_data_shard.iter_torch_batches(\n        batch_size=batch_size, dtypes=torch.float32\n    )\n\n    for epoch_idx in range(1):\n        for batch in train_dataloader:\n            inputs, labels = batch[\"x\"], batch[\"y\"]\n            assert type(inputs) == torch.Tensor\n            assert type(labels) == torch.Tensor\n            assert inputs.shape[0] == batch_size\n            assert labels.shape[0] == batch_size\n            # Only check one batch for demo purposes.\n            # Replace the above with your actual model training code.\n            break\n\n# Step 3: Create a TorchTrainer. Specify the number of training workers and\n# pass in your Ray Dataset.\n# The Ray Dataset is automatically split across all training workers.\ntrainer = TorchTrainer(\n    train_func,\n    datasets={\"train\": train_dataset},\n    scaling_config=ScalingConfig(num_workers=2, use_gpu=use_gpu)\n)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Inception Score During PBT Training\nDESCRIPTION: Plots the inception scores of all trials during training to visualize the improvement in generated image quality over time when using PBT.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\n# Uncomment to apply plotting styles\n# !pip install seaborn\n# import seaborn as sns\n# sns.set_style(\"darkgrid\")\n\nresult_dfs = [result.metrics_dataframe for result in results_grid]\nbest_result = results_grid.get_best_result(metric=\"is_score\", mode=\"max\")\n\nplt.figure(figsize=(7, 4))\nfor i, df in enumerate(result_dfs):\n    plt.plot(df[\"is_score\"], label=i)\nplt.legend()\nplt.title(\"Inception Score During Training\")\nplt.xlabel(\"Training Iterations\")\nplt.ylabel(\"Inception Score\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining a Ray Serve deployment\nDESCRIPTION: This code snippet demonstrates how to define a Ray Serve deployment using the `@serve.deployment` decorator. It shows how to specify parameters like `num_replicas`, `max_ongoing_requests`, `user_config`, `health_check_period_s`, and `health_check_timeout_s` directly within the decorator. This is one way to configure deployment settings within the application code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/configure-serve-deployment.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example of deploying a model with multiple replicas and a user config.\n\"\"\"\nimport time\nimport ray\n\nfrom ray import serve\n\n\n@serve.deployment(num_replicas=2, max_ongoing_requests=100,\n                  user_config={\"hello\": \"world\"}, health_check_period_s=5,\n                  health_check_timeout_s=10)\nclass MyModel:\n    def __init__(self, user_config):\n        self.config = user_config\n        self.ready = False\n\n    def reconfigure(self, user_config):\n        self.config = user_config\n        print(\"Reconfiguring to: {self.config}\")\n\n    def __call__(self, request):\n        if not self.ready:\n            time.sleep(2)\n            self.ready = True\n        return {\"config\": self.config, \"request\": request}\n\n\nmodel = MyModel.bind()\n\n# __deployment_start__\n@serve.deployment(num_replicas=2, max_ongoing_requests=100,\n                  user_config={\"hello\": \"world\"}, health_check_period_s=5,\n                  health_check_timeout_s=10)\nclass MyModel:\n    def __init__(self, user_config):\n        self.config = user_config\n        self.ready = False\n\n    def reconfigure(self, user_config):\n        self.config = user_config\n        print(\"Reconfiguring to: {self.config}\")\n\n    def __call__(self, request):\n        if not self.ready:\n            time.sleep(2)\n            self.ready = True\n        return {\"config\": self.config, \"request\": request}\n\n\nmodel = MyModel.bind()\n# __deployment_end__\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing ObjectRefs in Ray\nDESCRIPTION: Demonstrates how to serialize an ObjectRef, store it externally, deserialize and use it, and free its object. This example uses ray.cloudpickle for serialization and a private API for freeing the object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/serialization.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: /ray-core/doc_code/object_ref_serialization.py\n```\n\n----------------------------------------\n\nTITLE: Reading Text Files with Ray Data\nDESCRIPTION: Demonstrates reading plain text files using ray.data.read_text(). Creates a dataset where each line becomes a row.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-text.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\n\nds.show(3)\n```\n\n----------------------------------------\n\nTITLE: Implementing Random Search Space in Ray Tune\nDESCRIPTION: Example demonstrating how to define a random search space using uniform sampling for hyperparameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparam_space = {\n    \"a\": tune.uniform(0, 1),\n    \"b\": tune.uniform(0, 1),\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Lightning Environment Plugin\nDESCRIPTION: Shows how to configure the environment plugin for PyTorch Lightning with Ray Train. RayLightningEnvironment handles rank and world size configuration in a distributed setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_5\n\nLANGUAGE: diff\nCODE:\n```\n import lightning.pytorch as pl\n-from pl.plugins.environments import LightningEnvironment\n+import ray.train.lightning\n\n def train_func():\n     ...\n     trainer = pl.Trainer(\n         ...\n-        plugins=[LightningEnvironment()],\n+        plugins=[ray.train.lightning.RayLightningEnvironment()],\n         ...\n     )\n     ...\n```\n\n----------------------------------------\n\nTITLE: Parameter Server Implementation\nDESCRIPTION: Ray remote actor implementation of the parameter server that manages model weights and applies gradients.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_parameter_server.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass ParameterServer(object):\n    def __init__(self, lr):\n        self.model = ConvNet()\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n\n    def apply_gradients(self, *gradients):\n        summed_gradients = [\n            np.stack(gradient_zip).sum(axis=0) for gradient_zip in zip(*gradients)\n        ]\n        self.optimizer.zero_grad()\n        self.model.set_gradients(summed_gradients)\n        self.optimizer.step()\n        return self.model.get_weights()\n\n    def get_weights(self):\n        return self.model.get_weights()\n```\n\n----------------------------------------\n\nTITLE: Creating and Retrieving Named Actors in Python\nDESCRIPTION: Demonstrates how to create an actor with a name and retrieve it later using Ray in Python. This allows accessing the actor from any job in the Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/named-actors.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\nclass Counter:\n    pass\n\n# Create an actor with a name\ncounter = Counter.options(name=\"some_name\").remote()\n\n# Retrieve the actor later somewhere\ncounter = ray.get_actor(\"some_name\")\n```\n\n----------------------------------------\n\nTITLE: Resource Assignment Functions\nDESCRIPTION: Documentation listing for Tune's resource management utilities including placement groups and GPU handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_8\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n    :toctree: doc/\n\n    tune.with_resources\n    ~tune.execution.placement_groups.PlacementGroupFactory\n    tune.utils.wait_for_gpu\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray for Distributed Computing\nDESCRIPTION: Initializes the Ray framework with logging disabled for cleaner output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nray.init(configure_logging=False)\n```\n\n----------------------------------------\n\nTITLE: FastAPI Integration with Ray Serve\nDESCRIPTION: An example of integrating FastAPI with Ray Serve to handle HTTP requests and perform validations effectively. This demonstrates how to combine HTTP serving capabilities with ML model deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/index.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n{literalinclude} doc_code/fastapi_example.py\n:language: python\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Ray Serve Model Composition\nDESCRIPTION: Shows how to chain multiple models together in Ray Serve by combining a Summarizer and Translator. The code demonstrates asynchronous communication between deployments using remote calls.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/getting_started.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom transformers import pipeline\n\n@serve.deployment\nclass Translator:\n    def __init__(self):\n        self.model = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n\n    def translate(self, text: str) -> str:\n        return self.model(text)[0][\"translation_text\"]\n\n@serve.deployment\nclass Summarizer:\n    def __init__(self, translator):\n        self.model = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n        self.translator = translator\n\n    async def __call__(self, text: str) -> str:\n        summary = self.model(text, max_length=20, min_length=5, do_sample=False)[0][\"summary_text\"]\n        translation = await self.translator.translate.remote(summary)\n        return translation\n\napp = Summarizer.bind(Translator.bind())\n```\n\n----------------------------------------\n\nTITLE: Setting Up Ray Train Configuration for Dolly-v2 Model Fine-tuning in Python\nDESCRIPTION: This code configures Ray Train for fine-tuning the Dolly-v2-7b model. It sets up the run configuration, scaling configuration for distributed training, and the training configuration including learning rate and batch size.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import RunConfig, ScalingConfig, CheckpointConfig\n\n# Save Ray Train checkpoints according to the performance on validation set\nrun_config = RunConfig(\n    name=\"finetune_dolly-v2-7b\",\n    storage_path=storage_path,\n    checkpoint_config=CheckpointConfig(num_to_keep=1),\n)\n\n# Scale the FSDP training workload across 16 GPUs\n# You can change this config based on your compute resources.\nscaling_config = ScalingConfig(\n    num_workers=num_workers, use_gpu=True, trainer_resources={\"memory\": 100 * 1024 ** 3}\n)\n\n# Configuration to pass into train_func\ntrain_config = {\n    \"lr\": 2e-5,\n    \"eps\": 1e-8,\n    \"strategy\": fsdp_strategy,\n    \"batch_size_per_worker\": 10\n}\n\n# Define a TorchTrainer and launch you training workload\nray_trainer = TorchTrainer(\n    train_func,\n    train_loop_config=train_config,\n    run_config=run_config,\n    scaling_config=scaling_config,\n    datasets={\"train\": train_ds},\n)\nresult = ray_trainer.fit()\n\nresult\n```\n\n----------------------------------------\n\nTITLE: Setting Up Training Function with Ray Lightning Integration\nDESCRIPTION: Defines a training function that configures the DollyV2 model, Ray Data ingestion, and PyTorch Lightning trainer with Ray integration. The function sets up mixed precision training, distributed strategy, and callback integration for Ray Train reports.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import Checkpoint\nfrom ray.train.lightning import RayLightningEnvironment, RayTrainReportCallback, prepare_trainer\n\n# Training function for each worker\ndef train_func(config):\n    lr = config[\"lr\"]\n    eps = config[\"eps\"]\n    strategy = config[\"strategy\"]\n    batch_size_per_worker = config[\"batch_size_per_worker\"]\n\n    # Model\n    model = DollyV2Model(lr=lr, eps=eps)\n\n    # Ray Data Ingestion\n    train_ds = ray.train.get_dataset_shard(\"train\")\n    train_dataloader = train_ds.iter_torch_batches(batch_size=batch_size_per_worker)\n\n    # Lightning Trainer\n    trainer = pl.Trainer(\n        max_epochs=1, \n        devices=\"auto\",\n        accelerator=\"auto\", \n        precision=\"16-mixed\",\n        strategy=strategy,\n        plugins=[RayLightningEnvironment()],\n        callbacks=[RayTrainReportCallback()],\n        enable_checkpointing=False,\n    )\n\n    trainer = prepare_trainer(trainer)\n\n    trainer.fit(model, train_dataloaders=train_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Serving Hugging Face Transformers with Ray Serve\nDESCRIPTION: Describes how to deploy a pre-trained Hugging Face Transformers model using Ray Serve for sentiment analysis. The example requires the 'transformers' library and showcases serving complex models with Ray Serve.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/index.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n{literalinclude} doc_code/transformers_example.py\n:language: python\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Ray Data LLM Batch Inference with LoRA\nDESCRIPTION: Complete implementation of batch inference using Ray Data LLM with LoRA adapter. Includes processor configuration, data preprocessing, batch processing setup, and result handling. Uses the unsloth/Llama-3.2-1B-Instruct model with EdBergJr/Llama32_Baha_3 LoRA adapter.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/llm/examples/batch/vllm-with-lora.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray.data.llm import build_llm_processor, vLLMEngineProcessorConfig\n\n# 1. Construct a vLLM processor config.\nprocessor_config = vLLMEngineProcessorConfig(\n    # The base model.\n    model_source=\"unsloth/Llama-3.2-1B-Instruct\",\n    # vLLM engine config.\n    engine_kwargs=dict(\n        # Enable LoRA in the vLLM engine; otherwise you won't be able to\n        # process requests with LoRA adapters.\n        enable_lora=True,\n        # You need to set the LoRA rank for the adapter.\n        # The LoRA rank is the value of \"r\" in the LoRA config.\n        # If you want to use multiple LoRA adapters in this pipeline,\n        # please specify the maximum LoRA rank among all of them.\n        max_lora_rank=32,\n        # The maximum number of LoRA adapters vLLM cached. \"1\" means\n        # vLLM only caches one LoRA adapter at a time, so if your dataset\n        # needs more than one LoRA adapters, then there would be context\n        # switching. On the other hand, while increasing max_loras reduces\n        # the context switching, it increases the memory footprint.\n        max_loras=1,\n        # Older GPUs (e.g. T4) don't support bfloat16. You should remove\n        # this line if you're using later GPUs.\n        dtype=\"half\",\n        # Reduce the model length to fit small GPUs. You should remove\n        # this line if you're using large GPUs.\n        max_model_len=1024,\n    ),\n    # The batch size used in Ray Data.\n    batch_size=16,\n    # Use one GPU in this example.\n    concurrency=1,\n    # If you save the LoRA adapter in S3, you can set the following path.\n    # dynamic_lora_loading_path=\"s3://your-lora-bucket/\",\n)\n\n# 2. Construct a processor using the processor config.\nprocessor = build_llm_processor(\n    processor_config,\n    # Convert the input data to the OpenAI chat form.\n    preprocess=lambda row: dict(\n        # If you specify \"model\" in a request, and the model is different\n        # from the model you specify in the processor config, then this\n        # is the LoRA adapter. The \"model\" here can be a LoRA adapter\n        # available in the HuggingFace Hub or a local path.\n        #\n        # If you set dynamic_lora_loading_path, then only specify the LoRA\n        # path under dynamic_lora_loading_path.\n        model=\"EdBergJr/Llama32_Baha_3\",\n        messages=[\n            {\"role\": \"system\",\n             \"content\": \"You are a calculator. Please only output the answer \"\n                \"of the given equation.\"},\n            {\"role\": \"user\", \"content\": f\"{row['id']} ** 3 = ?\"},\n        ],\n        sampling_params=dict(\n            temperature=0.3,\n            max_tokens=20,\n            detokenize=False,\n        ),\n    ),\n    # Only keep the generated text in the output dataset.\n    postprocess=lambda row: {\n        \"resp\": row[\"generated_text\"],\n    },\n)\n\n# 3. Synthesize a dataset with 30 rows.\nds = ray.data.range(30)\n# 4. Apply the processor to the dataset. Note that this line won't kick off\n# anything because processor is execution lazily.\nds = processor(ds)\n# Materialization kicks off the pipeline execution.\nds = ds.materialize()\n\n# 5. Print all outputs.\nfor out in ds.take_all():\n    print(out)\n    print(\"==========\")\n\n# 6. Shutdown Ray to release resources.\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running PPO Algorithm Instance Directly in Python\nDESCRIPTION: This code snippet demonstrates how to configure a PPO (Proximal Policy Optimization) algorithm instance directly through the Python API in RLlib. It shows the setup of the environment and training parameters, constructing the algorithm instance, and executing the training process for one iteration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/key-concepts.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\n# Configure.\nconfig = (\n    PPOConfig()\n    .environment(\"CartPole-v1\")\n    .training(\n        train_batch_size_per_learner=2000,\n        lr=0.0004,\n    )\n)\n\n# Build the Algorithm.\nalgo = config.build()\n\n# Train for one iteration, which is 2000 timesteps (1 train batch).\nprint(algo.train())\n\nalgo.stop()\n```\n\n----------------------------------------\n\nTITLE: Preparing and Tokenizing Training Data\nDESCRIPTION: Loads the tiny_shakespeare dataset, splits text into sentences, and tokenizes the data for training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef split_text(batch: pd.DataFrame) -> pd.DataFrame:\n    text = list(batch[\"text\"])\n    flat_text = \"\".join(text)\n    split_text = [\n        x.strip()\n        for x in flat_text.split(\"\\n\")\n        if x.strip() and not x.strip()[-1] == \":\"\n    ]\n    return pd.DataFrame(split_text, columns=[\"text\"])\n\n\ndef tokenize(batch: pd.DataFrame) -> dict:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n    tokenizer.pad_token = tokenizer.eos_token\n    ret = tokenizer(\n        list(batch[\"text\"]),\n        truncation=True,\n        max_length=256,\n        padding=\"max_length\",\n        return_tensors=\"np\",\n    )\n    ret[\"labels\"] = ret[\"input_ids\"].copy()\n    return dict(ret)\n\nhf_dataset = load_dataset(\"tiny_shakespeare\")\ntrain_ds = ray.data.from_huggingface(hf_dataset[\"train\"])\n\n# First split the dataset into multiple sentences.\ntrain_ds = train_ds.map_batches(split_text, batch_format=\"pandas\")\n\n# Then tokenize the dataset.\ntrain_ds = train_ds.map_batches(tokenize, batch_format=\"pandas\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop for PyTorch Model\nDESCRIPTION: This function implements the training loop for a single epoch, including forward pass, loss calculation, backpropagation, and optimization steps.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef train_epoch(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Dummy Model Loader in Python\nDESCRIPTION: Defines a simple dummy model function that takes pandas DataFrames as input and returns binary predictions based on passenger count. Includes a payload to simulate memory usage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/batch_prediction.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\ndef load_model():\n    # A dummy model.\n    def model(batch: pd.DataFrame) -> pd.DataFrame:\n        # Dummy payload so copying the model will actually copy some data\n        # across nodes.\n        model.payload = np.zeros(100_000_000)\n        return pd.DataFrame({\"score\": batch[\"passenger_count\"] % 2 == 0})\n    \n    return model\n```\n\n----------------------------------------\n\nTITLE: Load model from checkpoint\nDESCRIPTION: Loads the model from the best-performing checkpoint identified during the tuning process.  It retrieves the checkpoint directory and loads the model using `AutoModelForSequenceClassification.from_pretrained`.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom ray.train import Checkpoint\n\ncheckpoint: Checkpoint = best_result.checkpoint\n\nwith checkpoint.as_directory() as checkpoint_dir:\n    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Creating a Stateful Actor in Ray (Python)\nDESCRIPTION: Shows how to create a stateful actor in Ray using a decorated Python class. The actor maintains a count of database queries.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass DataTracker:\n    def __init__(self):\n        self._counts = 0\n\n    def increment(self):\n        self._counts += 1\n\n    def counts(self):\n        return self._counts\n```\n\n----------------------------------------\n\nTITLE: Initializing TorchTrainer for Distributed PyTorch Training\nDESCRIPTION: Sets up a basic TorchTrainer configuration with a training function and scaling config. This snippet shows the core components needed to launch a distributed PyTorch training job with Ray Train.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig\n\ndef train_func():\n    # Your PyTorch training code here.\n    ...\n\nscaling_config = ScalingConfig(num_workers=2, use_gpu=True)\ntrainer = TorchTrainer(train_func, scaling_config=scaling_config)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Counter Actor in Python\nDESCRIPTION: Demonstrates creating a basic Counter actor class using the ray.remote decorator. The actor maintains an internal counter value that can be incremented and retrieved.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\n    def get_counter(self):\n        return self.value\n\n# Create an actor from this class.\ncounter = Counter.remote()\n```\n\n----------------------------------------\n\nTITLE: Advanced Configuration for Train Driver Resources in Python\nDESCRIPTION: Demonstrates assigning custom resources to Ray Tune functions to ensure the Train driver process runs on safe nodes, minimizing downtime risks in longer training tasks. The configuration allows the driver to manage fault tolerance and worker failures gracefully.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/hyperparameter-optimization.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/train_tune_interop.py\n    :language: python\n    :start-after: __trainable_resources_start__\n    :end-before: __trainable_resources_end__\n```\n\n----------------------------------------\n\nTITLE: Building Custom Algorithm Classes - Python\nDESCRIPTION: This snippet explains how to create a custom algorithm by subclassing the Algorithm class in RLlib. Users are encouraged to override specific methods such as setup and training_step to implement custom behavior. It also cautions against using the build_trainer utility for this purpose.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/algorithm.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass CustomAlgorithm(Algorithm):\n    def setup(self, config):\n        super().setup(config)\n        # Custom setup logic\n\n    def training_step(self):\n        # Custom training logic\n        pass\n```\n\n----------------------------------------\n\nTITLE: Correct Ray Parallelization: Delayed ray.get()\nDESCRIPTION: Shows the proper way to use Ray for parallelization by first submitting all tasks and then calling ray.get() once to retrieve all results. This allows tasks to execute in parallel, reducing execution time to approximately 1 second.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresults = ray.get([do_some_work.remote(x) for x in range(4)])\n```\n\n----------------------------------------\n\nTITLE: Run training with Ray Train\nDESCRIPTION: Calls the `fit` method of the TorchTrainer to start the training process. The returned Result object contains metrics and the Ray Train Checkpoint associated with the last iteration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Training Function with DeepSpeed\nDESCRIPTION: Main training function that configures DeepSpeed optimization, training arguments, model loading, and training loop. Uses Hugging Face Transformers with Ray Train integration for distributed training. Includes DeepSpeed configuration for memory optimization and distributed data parallelism.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport evaluate\nimport torch\nfrom transformers import (\n    Trainer,\n    TrainingArguments,\n    GPTJForCausalLM,\n    AutoTokenizer,\n    default_data_collator,\n)\nfrom transformers.utils.logging import disable_progress_bar, enable_progress_bar\n\nfrom ray import train\nfrom ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n\n\ndef train_func(config):\n    os.environ[\"OMP_NUM_THREADS\"] = str(\n        train.get_context().get_trial_resources().bundles[-1].get(\"CPU\", 1)\n    )\n    torch.backends.cuda.matmul.allow_tf32 = True\n\n    batch_size = config.get(\"batch_size\", 4)\n    epochs = config.get(\"epochs\", 2)\n    warmup_steps = config.get(\"warmup_steps\", 0)\n    learning_rate = config.get(\"learning_rate\", 0.00002)\n    weight_decay = config.get(\"weight_decay\", 0.01)\n    steps_per_epoch = config.get(\"steps_per_epoch\")\n\n    deepspeed = {\n        \"fp16\": {\n            \"enabled\": \"auto\",\n            \"initial_scale_power\": 8,\n            \"hysteresis\": 4,\n            \"consecutive_hysteresis\": True,\n        },\n        \"bf16\": {\"enabled\": \"auto\"},\n        \"optimizer\": {\n            \"type\": \"AdamW\",\n            \"params\": {\n                \"lr\": \"auto\",\n                \"betas\": \"auto\",\n                \"eps\": \"auto\",\n            },\n        },\n        \"zero_optimization\": {\n            \"stage\": 3,\n            \"offload_optimizer\": {\n                \"device\": \"cpu\",\n                \"pin_memory\": True,\n            },\n            \"overlap_comm\": True,\n            \"contiguous_gradients\": True,\n            \"reduce_bucket_size\": \"auto\",\n            \"stage3_prefetch_bucket_size\": \"auto\",\n            \"stage3_param_persistence_threshold\": \"auto\",\n            \"gather_16bit_weights_on_model_save\": True,\n            \"round_robin_gradients\": True,\n        },\n        \"gradient_accumulation_steps\": \"auto\",\n        \"gradient_clipping\": \"auto\",\n        \"steps_per_print\": 10,\n        \"train_batch_size\": \"auto\",\n        \"train_micro_batch_size_per_gpu\": \"auto\",\n        \"wall_clock_breakdown\": False,\n    }\n\n    training_args = TrainingArguments(\n        \"output\",\n        logging_steps=1,\n        save_strategy=\"steps\",\n        save_steps=steps_per_epoch,\n        max_steps=steps_per_epoch * epochs,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=1,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_steps,\n        label_names=[\"input_ids\", \"attention_mask\"],\n        push_to_hub=False,\n        report_to=\"none\",\n        disable_tqdm=True,\n        fp16=True,\n        gradient_checkpointing=True,\n        deepspeed=deepspeed,\n    )\n    disable_progress_bar()\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\n    model.resize_token_embeddings(len(tokenizer))\n\n    enable_progress_bar()\n\n    metric = evaluate.load(\"accuracy\")\n\n    train_ds = train.get_dataset_shard(\"train\")\n    eval_ds = train.get_dataset_shard(\"validation\")\n\n    train_ds_iterable = train_ds.iter_torch_batches(\n        batch_size=batch_size,\n        local_shuffle_buffer_size=train.get_context().get_world_size() * batch_size,\n    )\n    eval_ds_iterable = eval_ds.iter_torch_batches(batch_size=batch_size)\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds_iterable,\n        eval_dataset=eval_ds_iterable,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n        data_collator=default_data_collator,\n    )\n\n    trainer.add_callback(RayTrainReportCallback())\n    trainer = prepare_trainer(trainer)\n    trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Scaling XGBoost Training with Multiple GPUs\nDESCRIPTION: Configures XGBoost training to utilize multiple GPUs on a single node. This example demonstrates how to set up distributed training with 4 workers, each utilizing a GPU for acceleration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nscaling_config = ScalingConfig(\n    num_workers=4,\n    use_gpu=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Trained Model\nDESCRIPTION: Executes the inference function with the trained model result to generate predictions on the test dataset. This demonstrates how to use the trained model to make predictions on new data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npredict_xgboost(result)\n```\n\n----------------------------------------\n\nTITLE: Setting Model Replica Configuration\nDESCRIPTION: Defines the number of model replicas for parallel processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/start.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNUM_REPLICAS: int = 4\n```\n\n----------------------------------------\n\nTITLE: Transforming Tensor Data with Element-wise Operations\nDESCRIPTION: Shows how to transform tensor data using map operations on individual records, implementing a brightness increase function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-tensors.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\n\nimport ray\nimport numpy as np\n\nds = ray.data.read_images(\"s3://anonymous@air-example-data/AnimalDetection\")\n\ndef increase_brightness(row: Dict[str, Any]) -> Dict[str, Any]:\n    row[\"image\"] = np.clip(row[\"image\"] + 4, 0, 255)\n    return row\n\n# Increase the brightness, record at a time.\nds.map(increase_brightness)\n\ndef batch_increase_brightness(batch: Dict[str, np.ndarray]) -> Dict:\n    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\n    return batch\n\n# Increase the brightness, batch at a time.\nds.map_batches(batch_increase_brightness)\n```\n\n----------------------------------------\n\nTITLE: Launching Ray Jobs\nDESCRIPTION: Commands to submit data preparation and fine-tuning jobs to the Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/aws-trainium/llama3.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f 2-llama3-finetune-trn1-rayjob-create-data.yaml\n\nkubectl apply -f 3-llama3-finetune-trn1-rayjob-submit-finetuning-job.yaml\n```\n\n----------------------------------------\n\nTITLE: Standard PyTorch Training Script\nDESCRIPTION: A typical PyTorch training script without Ray Train integration. This script sets up a model, data loader, and training loop for the Fashion MNIST dataset using a ResNet18 model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import resnet18\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Normalize, Compose\n\n# Model, Loss, Optimizer\nmodel = resnet18(num_classes=10)\nmodel.conv1 = torch.nn.Conv2d(\n    1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n)\nmodel.to(\"cuda\")\ncriterion = CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=0.001)\n\n# Data\ntransform = Compose([ToTensor(), Normalize((0.28604,), (0.32025,))])\ntrain_data = FashionMNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n\n# Training\nfor epoch in range(10):\n    for images, labels in train_loader:\n        images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n    checkpoint_dir = tempfile.mkdtemp()\n    checkpoint_path = os.path.join(checkpoint_dir, \"model.pt\")\n    torch.save(model.state_dict(), checkpoint_path)\n    print(metrics)\n```\n\n----------------------------------------\n\nTITLE: Implementing MNIST Classifier with PyTorch Lightning\nDESCRIPTION: This snippet defines a PyTorch Lightning module for an MNIST classifier. It includes the model architecture, training and validation steps, data preparation, and optimizer configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LightningMNISTClassifier(pl.LightningModule):\n    \"\"\"\n    This has been adapted from\n    https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09\n    \"\"\"\n\n    def __init__(self, config, data_dir=None):\n        super(LightningMNISTClassifier, self).__init__()\n\n        self.data_dir = data_dir or os.getcwd()\n\n        self.layer_1_size = config[\"layer_1_size\"]\n        self.layer_2_size = config[\"layer_2_size\"]\n        self.lr = config[\"lr\"]\n        self.batch_size = config[\"batch_size\"]\n\n        # mnist images are (1, 28, 28) (channels, width, height)\n        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n\n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n        x = x.view(batch_size, -1)\n\n        x = self.layer_1(x)\n        x = torch.relu(x)\n\n        x = self.layer_2(x)\n        x = torch.relu(x)\n\n        x = self.layer_3(x)\n        x = torch.log_softmax(x, dim=1)\n\n        return x\n\n    def cross_entropy_loss(self, logits, labels):\n        return F.nll_loss(logits, labels)\n\n    def accuracy(self, logits, labels):\n        _, predicted = torch.max(logits.data, 1)\n        correct = (predicted == labels).sum().item()\n        accuracy = correct / len(labels)\n        return torch.tensor(accuracy)\n\n    def training_step(self, train_batch, batch_idx):\n        x, y = train_batch\n        logits = self.forward(x)\n        loss = self.cross_entropy_loss(logits, y)\n        accuracy = self.accuracy(logits, y)\n\n        self.log(\"ptl/train_loss\", loss)\n        self.log(\"ptl/train_accuracy\", accuracy)\n        return loss\n\n    def validation_step(self, val_batch, batch_idx):\n        x, y = val_batch\n        logits = self.forward(x)\n        loss = self.cross_entropy_loss(logits, y)\n        accuracy = self.accuracy(logits, y)\n        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n        self.log(\"ptl/val_loss\", avg_loss)\n        self.log(\"ptl/val_accuracy\", avg_acc)\n\n    @staticmethod\n    def download_data(data_dir):\n        transform = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        )\n        with FileLock(os.path.expanduser(\"~/.data.lock\")):\n            return MNIST(data_dir, train=True, download=True, transform=transform)\n\n    def prepare_data(self):\n        mnist_train = self.download_data(self.data_dir)\n\n        self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=int(self.batch_size))\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=int(self.batch_size))\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n\n\ndef train_mnist(config):\n    model = LightningMNISTClassifier(config)\n    trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False)\n\n    trainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hyperparameters with a Dictionary in Ray Train\nDESCRIPTION: Defining a train function that accepts a configuration dictionary containing hyperparameters like batch size, learning rate, and number of epochs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef train_func(config: dict):\n    batch_size = config[\"batch_size\"]\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Worker Training Function for LLaMa Pretraining on Habana Gaudi\nDESCRIPTION: This function is executed by each worker during training. It loads datasets and tokenizer from Hugging Face, prepares the model, creates a GaudiTrainer instance, runs the training process, and saves the model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef pretrain_llama(config: Dict[str, Any]):\n\n    training_args = GaudiTrainingArguments(**config[\"training_args\"])\n    set_seed(training_args.seed)\n\n    raw_datasets = load_datasets(config[\"datasets\"])\n\n    tokenizer = load_tokenizer(config[\"tokenizer\"])\n\n    tokenized_datasets = tokenize_dataset(raw_datasets, tokenizer)\n\n    tokenized_datasets = group_dataset(config[\"model\"], tokenized_datasets, tokenizer)\n\n    model = load_model(config[\"model\"])\n\n    trainer = get_trainer(training_args, tokenized_datasets, tokenizer, model)\n\n    result = trainer.train()\n    trainer.save_model()\n    print(result)\n```\n\n----------------------------------------\n\nTITLE: Training with TensorFlow using Ray in Python\nDESCRIPTION: The code defines a function for training a simple neural network model using TensorFlow and Ray's train module. It includes the setup for distributed training with a multi-worker strategy. Checkpoints are managed to save and restore model states, allowing continuation of training from a specific state. Key parameters like 'num_epochs' are used to define the training loop, and results including checkpoints are reported back.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/distributed-tensorflow-keras.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nfrom ray import train\nfrom ray.train import Checkpoint, ScalingConfig\nfrom ray.train.tensorflow import TensorflowTrainer\n\nimport numpy as np\n\ndef train_func(config):\n    import tensorflow as tf\n    n = 100\n    # create a toy dataset\n    # data   : X - dim = (n, 4)\n    # target : Y - dim = (n, 1)\n    X = np.random.normal(0, 1, size=(n, 4))\n    Y = np.random.uniform(0, 1, size=(n, 1))\n\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    with strategy.scope():\n        # toy neural network : 1-layer\n        checkpoint = train.get_checkpoint()\n        if checkpoint:\n            with checkpoint.as_directory() as checkpoint_dir:\n                model = tf.keras.models.load_model(\n                    os.path.join(checkpoint_dir, \"model.keras\")\n                )\n        else:\n            model = tf.keras.Sequential(\n                [tf.keras.layers.Dense(1, activation=\"linear\", input_shape=(4,))]\n            )\n        model.compile(optimizer=\"Adam\", loss=\"mean_squared_error\", metrics=[\"mse\"])\n\n    for epoch in range(config[\"num_epochs\"]):\n        history = model.fit(X, Y, batch_size=20)\n\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            model.save(os.path.join(temp_checkpoint_dir, \"model.keras\"))\n            extra_json = os.path.join(temp_checkpoint_dir, \"checkpoint.json\")\n            with open(extra_json, \"w\") as f:\n                json.dump({\"epoch\": epoch}, f)\n            checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n\n            train.report({\"loss\": history.history[\"loss\"][0]}, checkpoint=checkpoint)\n\ntrainer = TensorflowTrainer(\n    train_func,\n    train_loop_config={\"num_epochs\": 5},\n    scaling_config=ScalingConfig(num_workers=2),\n)\nresult = trainer.fit()\nprint(result.checkpoint)\n\n# Start a new run from a loaded checkpoint\ntrainer = TensorflowTrainer(\n    train_func,\n    train_loop_config={\"num_epochs\": 5},\n    scaling_config=ScalingConfig(num_workers=2),\n    resume_from_checkpoint=result.checkpoint,\n)\nresult = trainer.fit()\n\n```\n\n----------------------------------------\n\nTITLE: Defining Search Space Configuration in Ray Tune\nDESCRIPTION: Comprehensive example showing various methods to define search spaces for hyperparameter tuning including random, uniform, and choice sampling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"a1\": tune.randint(5, 10),  # Integer between 5 and 10\n    \"a2\": tune.choice([\"a\", \"b\", \"c\"]),  # Choose from list of values\n    \"a3\": tune.grid_search([1, 2, 3]),  # Grid search over values\n    \"a4\": tune.uniform(0, 10),  # Uniform distribution between 0 and 10\n    \"a5\": tune.quniform(0, 10, 2), # Uniform with quantization\n    \"a6\": tune.qrandint(0, 10, 2),  # Uniform integer with quantization\n    \"a7\": tune.randn(10, 2),  # Normal distribution\n    \"a8\": tune.qrandn(10, 2, 2),  # Normal distribution with quantization\n    \"a9\": tune.loguniform(1e-4, 1e-1)  # Log-uniform distribution\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching Object Data in Ray (Python)\nDESCRIPTION: Shows how to retrieve object data from object references using ray.get() in Python. It includes examples of getting single and multiple object references, as well as setting a timeout for long-running operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects.rst#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\nimport time\n\n# Get the value of one object ref.\nobj_ref = ray.put(1)\nassert ray.get(obj_ref) == 1\n\n# Get the values of multiple object refs in parallel.\nassert ray.get([ray.put(i) for i in range(3)]) == [0, 1, 2]\n\n# You can also set a timeout to return early from a ``get``\n# that's blocking for too long.\nfrom ray.exceptions import GetTimeoutError\n# ``GetTimeoutError`` is a subclass of ``TimeoutError``.\n\n@ray.remote\ndef long_running_function():\n    time.sleep(8)\n\nobj_ref = long_running_function.remote()\ntry:\n    ray.get(obj_ref, timeout=4)\nexcept GetTimeoutError:  # You can capture the standard \"TimeoutError\" instead\n    print(\"`get` timed out.\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Reset Method in MultiAgentEnv\nDESCRIPTION: Example of implementing the reset method in a multi-agent environment, returning observation dictionaries for active agents\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef reset(self, *, seed=None, options=None):\n    ...\n    return {\n        \"agent_1\": np.array([0.0, 1.0, 0.0, 0.0]),\n        \"agent_2\": np.array([0.0, 0.0, 1.0]),\n    }, {}  # <- empty info dict\n```\n\n----------------------------------------\n\nTITLE: Replaying a PBT Run with Ray Tune\nDESCRIPTION: Demonstrates how to replay a Population Based Training run using a previously generated policy log file. This is useful for replicating the hyperparameter schedule from a successful PBT run.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport glob\n\nfrom ray import tune\nfrom ray.tune.schedulers import PopulationBasedTrainingReplay\n\n# Get a random replay policy from the experiment we just ran\nsample_pbt_trial_log = glob.glob(\n    os.path.expanduser(\"/tmp/ray_results/pbt_test/pbt_policy*.txt\")\n)[0]\nreplay = PopulationBasedTrainingReplay(sample_pbt_trial_log)\n\ntuner = tune.Tuner(\n    train_convnet,\n    tune_config=tune.TuneConfig(scheduler=replay),\n    run_config=tune.RunConfig(stop={\"training_iteration\": 50}),\n)\nresults_grid = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Implementing DeepSpeed Training Function with Ray Train\nDESCRIPTION: Example showing how to set up a basic training function using DeepSpeed with Ray Train. Includes model initialization, dataset preparation, and training loop setup with distributed training support.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/deepspeed.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport deepspeed\nfrom deepspeed.accelerator import get_accelerator\n\ndef train_func():\n    # Instantiate your model and dataset\n    model = ...\n    train_dataset = ...\n    eval_dataset = ...\n    deepspeed_config = {...} # Your DeepSpeed config\n\n    # Prepare everything for distributed training\n    model, optimizer, train_dataloader, lr_scheduler = deepspeed.initialize(\n        model=model,\n        model_parameters=model.parameters(),\n        training_data=tokenized_datasets[\"train\"],\n        collate_fn=collate_fn,\n        config=deepspeed_config,\n    )\n\n    # Define the GPU device for the current worker\n    device = get_accelerator().device_name(model.local_rank)\n\n    # Start training\n    for epoch in range(num_epochs):\n        # Training logic\n        ...\n        \n        # Report metrics to Ray Train\n        ray.train.report(metrics={\"loss\": loss})\n\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(...),\n    # If running in a multi-node cluster, this is where you\n    # should configure the run's persistent storage that is accessible\n    # across all worker nodes.\n    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n    ...\n)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining the objective function\nDESCRIPTION: Defines the objective function that Ray Tune will optimize.  It takes a Tune config, evaluates the score of the experiment in a training loop, and uses tune.report to report the score back to Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"def objective(config):\\n    for step in range(config[\\\"steps\\\"]):\\n        score = evaluate(step, config[\\\"width\\\"], config[\\\"height\\\"], config[\\\"activation\\\"])  # Fixed: Corrected argument order to match evaluate function signature\n        tune.report({\\\"iterations\\\": step, \\\"mean_loss\\\": score})\"\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost Hyperparameters in Python\nDESCRIPTION: This snippet demonstrates how to set up a configuration dictionary for XGBoost, including parameters like objective, eval_metric, max_depth, min_child_weight, subsample, and eta. It then trains a model on the breast cancer dataset and prints the accuracy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-xgboost.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": [\"logloss\", \"error\"],\n    \"max_depth\": 2,\n    \"min_child_weight\": 0,\n    \"subsample\": 0.8,\n    \"eta\": 0.2,\n}\nresults = train_breast_cancer(config)\naccuracy = 1.0 - results[\"eval\"][\"error\"][-1]\nprint(f\"Accuracy: {accuracy:.4f}\")\n```\n\n----------------------------------------\n\nTITLE: Load model from Hugging Face Hub\nDESCRIPTION: Demonstrates how to load a model from the Hugging Face Hub using the model identifier.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"sgugger/my-awesome-model\")\n```\n\n----------------------------------------\n\nTITLE: Defining Random Sampling Search Space in Ray Tune\nDESCRIPTION: Shows how to specify a search space using various random sampling primitives for different parameter types.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-search-spaces.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    trainable,\n    param_space={\n        \"param1\": tune.choice([True, False]),\n        \"bar\": tune.uniform(0, 10),\n        \"alpha\": tune.sample_from(lambda _: np.random.uniform(100) ** 2),\n        \"const\": \"hello\"  # It is also ok to specify constant values.\n    })\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Starting Ray in Python\nDESCRIPTION: Imports necessary modules and initializes a local Ray cluster using ray.init().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/monte_carlo_pi.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport random\nimport time\n\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple PyTorch CNN Model\nDESCRIPTION: Implementation of a PyTorch neural network model as a nn.Module with a 2D convolutional layer, a fully connected layer, and a softmax function for classification.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Implementing Initial Tune Experiment with Checkpointing\nDESCRIPTION: An example of configuring an initial Tune experiment with checkpointing capabilities to enable fault tolerance. The code demonstrates setting up trainable functions with checkpoint saving and loading, and configuring the experiment path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-fault-tolerance.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef trainable(config):\n    # A simple training function that supports checkpointing\n    for i in range(100):\n        time.sleep(0.1)  # This represents work being done\n\n        # Report metrics periodically\n        intermediate_score = i + config[\"value\"]\n        tune.report(score=intermediate_score)\n\n        # Save checkpoint\n        with tune.checkpoint_dir(step=i) as checkpoint_dir:\n            path = os.path.join(checkpoint_dir, \"checkpoint.pkl\")\n            with open(path, \"wb\") as f:\n                checkpoint_data = {\n                    \"iteration\": i,\n                    \"score\": intermediate_score,\n                }\n                pickle.dump(checkpoint_data, f)\n\nparam_space = {\"value\": tune.grid_search([1, 2, 3])}\n\ntuner = tune.Tuner(\n    trainable,\n    param_space=param_space,\n    run_config=RunConfig(\n        name=\"tune_fault_tolerance_guide\",\n        storage_path=\"~/ray_results\",\n    ),\n)\n\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Reading from MySQL Database with Ray Data\nDESCRIPTION: This snippet shows how to read data from a MySQL database using Ray Data's `read_sql` function. It defines a function to establish a connection to the MySQL database using `mysql.connector` and then uses `ray.data.read_sql` to execute SQL queries and create Ray Datasets.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport mysql.connector\n\nimport ray\n\ndef create_connection():\n    return mysql.connector.connect(\n        user=\"admin\",\n        password=...,\n        host=\"example-mysql-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com\",\n        connection_timeout=30,\n        database=\"example\",\n    )\n\n# Get all movies\ndataset = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n# Get movies after the year 1980\ndataset = ray.data.read_sql(\n    \"SELECT title, score FROM movie WHERE year >= 1980\", create_connection\n)\n# Get the number of movies per year\ndataset = ray.data.read_sql(\n    \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipelining in Ray with Python\nDESCRIPTION: This code snippet demonstrates how to implement pipelining in Ray to improve performance by overlapping computation and communication. It shows two worker actor implementations: one without pipelining and one with pipelining, highlighting the difference in CPU utilization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/pipelining.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\n@ray.remote\nclass Queue:\n    def __init__(self):\n        self.items = []\n\n    def add_item(self, item):\n        self.items.append(item)\n\n    def get_item(self):\n        if not self.items:\n            return None\n        return self.items.pop(0)\n\n@ray.remote\nclass WorkerWithoutPipelining:\n    def __init__(self, queue):\n        self.queue = queue\n\n    def run(self):\n        while True:\n            item = ray.get(self.queue.get_item.remote())\n            if item is None:\n                break\n            # Do some computation.\n            time.sleep(0.5)\n\n@ray.remote\nclass WorkerWithPipelining:\n    def __init__(self, queue):\n        self.queue = queue\n\n    def run(self):\n        next_item = self.queue.get_item.remote()\n        while True:\n            curr_item = next_item\n            next_item = self.queue.get_item.remote()\n            curr_item = ray.get(curr_item)\n            if curr_item is None:\n                break\n            # Do some computation.\n            time.sleep(0.5)\n\nqueue = Queue.remote()\nfor i in range(100):\n    ray.get(queue.add_item.remote(i))\n\nworker_without_pipelining = WorkerWithoutPipelining.remote(queue)\nstart = time.time()\nray.get(worker_without_pipelining.run.remote())\nend = time.time()\nprint(f\"Time taken without pipelining: {end - start}\")\n\nqueue = Queue.remote()\nfor i in range(100):\n    ray.get(queue.add_item.remote(i))\n\nworker_with_pipelining = WorkerWithPipelining.remote(queue)\nstart = time.time()\nray.get(worker_with_pipelining.run.remote())\nend = time.time()\nprint(f\"Time taken with pipelining: {end - start}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Ray Serve Deployment for DeepSpeed Llama Model\nDESCRIPTION: This code snippet defines a Ray Serve deployment for the DeepSpeed Llama model. It sets up the deployment configuration, including the number of replicas and resources per replica.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\n    \"meta-llama/Llama-2-70b-chat-hf\",\n    # Replace the path if necessary.\n    cache_dir=os.getenv(\"TRANSFORMERS_CACHE\", None),\n    # Specify your Hugging Face token.\n    token=\"\"\n)\n```\n\n----------------------------------------\n\nTITLE: Client-side Structured Output Interaction (JSON Schema)\nDESCRIPTION: Python code for client-side interaction with a Ray Serve LLM deployment to request structured JSON output with a specific schema. Uses Pydantic to define the expected response structure.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom typing import List, Literal\nfrom pydantic import BaseModel\n\n# Initialize client\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n\n# Define a pydantic model of a preset of allowed colors\nclass Color(BaseModel):\n    colors: List[Literal[\"cyan\", \"magenta\", \"yellow\"]]\n\n# Request structured JSON output\nresponse = client.chat.completions.create(\n    model=\"qwen-0.5b\",\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": Color.model_json_schema()\n        \n    },\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that outputs JSON.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"List three colors in JSON format\"\n        }\n    ],\n    stream=True,\n)\n\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n# Example response:\n# {\n#   \"colors\": [\n#     \"cyan\",\n#     \"magenta\",\n#     \"yellow\"\n#   ]\n# }\n```\n\n----------------------------------------\n\nTITLE: Implementing Validation Loop for PyTorch Model\nDESCRIPTION: This function implements the validation loop, evaluating the model's performance on the test dataset and calculating accuracy and average loss.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef test_epoch(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n    return test_loss\n```\n\n----------------------------------------\n\nTITLE: Converting ML Model to Ray Serve with FastAPI in Python\nDESCRIPTION: The code snippet demonstrates how to convert a text-translation model written in Python into a Ray Serve application using FastAPI. Steps include importing Ray Serve and FastAPI, adding Serve deployment decorators, and binding the deployment configuration to the model. It requires the Ray Serve library and FastAPI framework. The 'Translator' deployment configuration includes parameters like 'num_replicas' and 'ray_actor_options', specifying resources used by the application replicas.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/develop-and-deploy.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"../serve/doc_code/develop_and_deploy.py\\n:start-after: __deployment_start__\\n:end-before: __deployment_end__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Lightning Model for Text Classification\nDESCRIPTION: This code snippet defines a PyTorch Lightning module for text classification using a pre-trained BERT model. It includes methods for forward pass, training step, validation step, and optimizer configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_cola_advanced.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass SentimentModel(pl.LightningModule):\n    def __init__(self, lr=2e-5, eps=1e-8):\n        super().__init__()\n        self.lr = lr\n        self.eps = eps\n        self.num_classes = 2\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            \"bert-base-cased\", num_labels=self.num_classes\n        )\n        self.metric = load_metric(\"glue\", \"cola\")\n        self.predictions = []\n        self.references = []\n\n    def forward(self, batch):\n        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n        outputs = self.model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        return logits\n\n    def training_step(self, batch, batch_idx):\n        labels = batch[\"label\"]\n        logits = self.forward(batch)\n        loss = F.cross_entropy(logits.view(-1, self.num_classes), labels)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        labels = batch[\"label\"]\n        logits = self.forward(batch)\n        preds = torch.argmax(logits, dim=1)\n        self.predictions.append(preds)\n        self.references.append(labels)\n\n    def on_validation_epoch_end(self):\n        predictions = torch.concat(self.predictions).view(-1)\n        references = torch.concat(self.references).view(-1)\n        matthews_correlation = self.metric.compute(\n            predictions=predictions, references=references\n        )\n\n        # self.metric.compute() returns a dictionary:\n        # e.g. {\"matthews_correlation\": 0.53}\n        self.log_dict(matthews_correlation, sync_dist=True)\n        self.predictions.clear()\n        self.references.clear()\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.lr, eps=self.eps)\n```\n\n----------------------------------------\n\nTITLE: Configuring Population Based Training with Ray Tune\nDESCRIPTION: Implements Population Based Training for MNIST model using Ray Tune. Sets up configuration parameters, scheduler with perturbation settings, and CLI reporter for tracking metrics. Uses tune.Tuner for experiment management with specified resources and parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef tune_mnist_pbt(num_samples=10, num_epochs=10, gpus_per_trial=0, data_dir=\"~/data\"):\n    config = {\n        \"layer_1_size\": tune.choice([32, 64, 128]),\n        \"layer_2_size\": tune.choice([64, 128, 256]),\n        \"lr\": 1e-3,\n        \"batch_size\": 64,\n    }\n\n    scheduler = PopulationBasedTraining(\n        perturbation_interval=4,\n        hyperparam_mutations={\n            \"lr\": tune.loguniform(1e-4, 1e-1),\n            \"batch_size\": [32, 64, 128],\n        },\n    )\n\n    reporter = CLIReporter(\n        parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"],\n    )\n\n    tuner = tune.Tuner(\n        tune.with_resources(\n            tune.with_parameters(\n                train_mnist_tune_checkpoint,\n                num_epochs=num_epochs,\n                num_gpus=gpus_per_trial,\n                data_dir=data_dir,\n            ),\n            resources={\"cpu\": 1, \"gpu\": gpus_per_trial},\n        ),\n        tune_config=tune.TuneConfig(\n            metric=\"loss\",\n            mode=\"min\",\n            scheduler=scheduler,\n            num_samples=num_samples,\n        ),\n        run_config=tune.RunConfig(\n            name=\"tune_mnist_asha\",\n            progress_reporter=reporter,\n        ),\n        param_space=config,\n    )\n    results = tuner.fit()\n\n    print(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Ray Data Equivalent of Custom PyTorch Dataset\nDESCRIPTION: This code demonstrates how to achieve the same functionality as the custom PyTorch Dataset using Ray Data. It reads images from S3, extracts labels from file paths, and applies image transformations using map operations on the dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-pytorch.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/batoidea/JPEGImages\", include_paths=True)\n\n# Extract the label from the file path.\ndef extract_label(row: dict):\n    filepath = row[\"path\"]\n    last_slash_idx = filepath.rfind(\"/\")\n    dot_idx = filepath.rfind('.')\n    label = int(filepath[last_slash_idx+1:dot_idx])\n    row[\"label\"] = label\n    return row\n\ntransform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Resize((128, 128)),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n# Preprocess the images.\ndef transform_image(row: dict):\n    row[\"transformed_image\"] = transform(row[\"image\"])\n    return row\n\n# Map the transformations over the dataset.\nds = ds.map(extract_label).map(transform_image)\n```\n\n----------------------------------------\n\nTITLE: Launching Ray Cluster on vSphere\nDESCRIPTION: Complete sequence of commands to download configuration, set credentials, and manage a Ray cluster on vSphere including creation, connection, and teardown.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/vsphere.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Download the example-full.yaml\nwget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/vsphere/example-full.yaml\n\n# Setup vSphere credentials using environment variables\nexport VSPHERE_SERVER=vcenter-address.example.com\nexport VSPHERE_USER=foo\nexport VSPHERE_PASSWORD=bar\n\n# Edit the example-full.yaml to update the frozen VM related configs under vsphere_config. there are 3 options:\n# 1. If you have a single frozen VM, set the \"name\" under \"frozen_vm\".\n# 2. If you have a set of frozen VMs in a resource pool (one VM on each ESXi host), set the \"resource_pool\" under \"frozen_vm\".\n# 3. If you don't have any existing frozen VM in the vSphere cluster, but you have an OVF template of a frozen VM, set the \"library_item\" under \"frozen_vm\". After that, you need to either set the \"name\" of the to-be-deployed frozen VM, or set the \"resource_pool\" to point to an existing resource pool for the to-be-deployed frozen VMs for all the ESXi hosts in the vSphere cluster. Also, the \"datastore\" must be specified.\n# Optionally configure the head and worker node resource pool and datastore placement.\n# If not configured via environment variables, the vSphere credentials can alternatively be configured in this file.\n\n# vi example-full.yaml\n\n# Create or update the cluster. When the command finishes, it will print\n# out the command that can be used to SSH into the cluster head node.\nray up example-full.yaml\n\n# Get a remote screen on the head node.\nray attach example-full.yaml\n\n# Try running a Ray program.\npython -c 'import ray; ray.init()'\nexit\n\n# Tear down the cluster.\nray down example-full.yaml\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Runtime - Python\nDESCRIPTION: This snippet demonstrates how to initialize a local Ray instance on a machine using Python. The `ray.init()` function must be called before using any other Ray APIs, setting up the environment for parallel and distributed computing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\nray.shutdown()\n\nimport ray\n# Other Ray APIs will not work until `ray.init()` is called.\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Defining Single-Worker PyTorch Training Function (Python)\nDESCRIPTION: This code defines a single-worker PyTorch training function. It's a standard PyTorch training loop that processes the data and updates the model's weights using an optimizer. Placeholders `__torch_single_begin__` and `__torch_single_end__` mark the beginning and end of the code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/pytorch/torch_quick_start.py\n:language: python\n:start-after: __torch_single_begin__\n:end-before: __torch_single_end__\n```\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installing the necessary Python packages including Ray Data, PyTorch, and torchvision for running the image classification example.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/pytorch_resnet_batch_prediction.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"ray[data]\" torch torchvision\n```\n\n----------------------------------------\n\nTITLE: Training MNIST with Keras and Ray Tune\nDESCRIPTION: This snippet demonstrates how to train a Keras model on the MNIST dataset while using Ray Tune for hyperparameter optimization. It includes functions for training the model and setting up the tuner with an AsyncHyperBandScheduler. The code uses a callback to report metrics to Ray Tune during training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune_mnist_keras.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom filelock import FileLock\nfrom tensorflow.keras.datasets import mnist\n\nfrom ray import tune\nfrom ray.tune.schedulers import AsyncHyperBandScheduler\nfrom ray.air.integrations.keras import ReportCheckpointCallback\n\n\ndef train_mnist(config):\n    # https://github.com/tensorflow/tensorflow/issues/32159\n    import tensorflow as tf\n\n    batch_size = 128\n    num_classes = 10\n    epochs = 12\n\n    with FileLock(os.path.expanduser(\"~/.data.lock\")):\n        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    x_train, x_test = x_train / 255.0, x_test / 255.0\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.layers.Flatten(input_shape=(28, 28)),\n            tf.keras.layers.Dense(config[\"hidden\"], activation=\"relu\"),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n        ]\n    )\n\n    model.compile(\n        loss=\"sparse_categorical_crossentropy\",\n        optimizer=tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"], momentum=config[\"momentum\"]),\n        metrics=[\"accuracy\"],\n    )\n\n    model.fit(\n        x_train,\n        y_train,\n        batch_size=batch_size,\n        epochs=epochs,\n        verbose=0,\n        validation_data=(x_test, y_test),\n        callbacks=[ReportCheckpointCallback(metrics={\"accuracy\": \"accuracy\"})],\n    )\n\n\ndef tune_mnist():\n    sched = AsyncHyperBandScheduler(\n        time_attr=\"training_iteration\", max_t=400, grace_period=20\n    )\n\n    tuner = tune.Tuner(\n        tune.with_resources(train_mnist, resources={\"cpu\": 2, \"gpu\": 0}),\n        tune_config=tune.TuneConfig(\n            metric=\"accuracy\",\n            mode=\"max\",\n            scheduler=sched,\n            num_samples=10,\n        ),\n        run_config=tune.RunConfig(\n            name=\"exp\",\n            stop={\"accuracy\": 0.99},\n        ),\n        param_space={\n            \"threads\": 2,\n            \"learning_rate\": tune.uniform(0.001, 0.1),\n            \"momentum\": tune.uniform(0.1, 0.9),\n            \"hidden\": tune.randint(32, 512),\n        },\n    )\n    results = tuner.fit()\n    return results\n\n    \n\nresults = tune_mnist()\nprint(f\"Best hyperparameters found were: {results.get_best_result().config} | Accuracy: {results.get_best_result().metrics['accuracy']}\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Multiplexed Deployment using Ray Serve\nDESCRIPTION: This snippet demonstrates how to create a multiplexed deployment with multiple models stored in an S3 bucket. It utilizes the APIs `serve.multiplexed` and `serve.get_multiplexed_model_id`. The configuration allows efficient routing of requests to corresponding models based on request headers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/model-multiplexing.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# Define your multiplexed deployment here based on the APIs mentioned.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up OptunaSearch with Conditional Space in Python\nDESCRIPTION: This snippet initializes an Optuna search with a define-by-run function for constructing a conditional search space, specifying metrics and optimization mode.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsearcher = OptunaSearch(space=define_by_run_func, metric=\"mean_loss\", mode=\"min\")\nalgo = ConcurrencyLimiter(searcher, max_concurrent=4)\n```\n\n----------------------------------------\n\nTITLE: Defining a Conditional Search Space in Python\nDESCRIPTION: This snippet demonstrates a define-by-run function which constructs a more complex search space depending on the values selected for other hyperparameters. It includes suggestions for conditional hyperparameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef define_by_run_func(trial) -> Optional[Dict[str, Any]]:\n    \"\"\"Define-by-run function to construct a conditional search space.\n\n    Ensure no actual computation takes place here. That should go into\n    the trainable passed to ``Tuner()`` (in this example, that's\n    ``objective``).\n\n    For more information, see https://optuna.readthedocs.io/en/stable\\\n    /tutorial/10_key_features/002_configurations.html\n\n    Args:\n        trial: Optuna Trial object\n        \n    Returns:\n        Dict containing constant parameters or None\n    \"\"\"\n\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n\n    # Define-by-run allows for conditional search spaces.\n    if activation == \"relu\":\n        trial.suggest_float(\"width\", 0, 20)\n        trial.suggest_float(\"height\", -100, 100)\n    else:\n        trial.suggest_float(\"width\", -1, 21)\n        trial.suggest_float(\"height\", -101, 101)\n        \n    # Return all constants in a dictionary.\n    return {\"steps\": 100}\n```\n\n----------------------------------------\n\nTITLE: Quickstart for Hyperparameter Tuning with Ray Tune in Python\nDESCRIPTION: This code snippet demonstrates how to launch a hyperparameter tuning job using Ray Tune, where different configurations are evaluated using a defined train function. It uses a TorchTrainer to distribute training jobs, and specifies a ScalingConfig for resource allocation per worker.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/hyperparameter-optimization.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/train_tune_interop.py\n    :language: python\n    :start-after: __quickstart_start__\n    :end-before: __quickstart_end__\n```\n\n----------------------------------------\n\nTITLE: Actor Pool Implementation for Batch Prediction\nDESCRIPTION: Shows how to use Ray's ActorPool to manage multiple prediction actors efficiently for parallel processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/batch_prediction.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.actor_pool import ActorPool\n\nmodel = load_model()\nmodel_ref = ray.put(model)\nnum_actors = 4\nactors = [BatchPredictor.remote(model_ref) for _ in range(num_actors)]\npool = ActorPool(actors)\ninput_files = [\n        f\"s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_full_year_data.parquet\"\n        f\"/fe41422b01c04169af2a65a83b753e0f_{i:06d}.parquet\"\n        for i in range(12)\n]\nfor file in input_files:\n    pool.submit(lambda a, v: a.predict.remote(v), file)\nwhile pool.has_next():\n    print(\"Prediction output size:\", pool.get_next())\n```\n\n----------------------------------------\n\nTITLE: Running PPO benchmark script\nDESCRIPTION: This command executes the PPO benchmark script which trains PPO algorithm for the Atari-Breakout game with and without compiled RLModules and compares their speed. The script requires a Ray cluster with at least 129 CPUs and 2 GPUs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-torch2x.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ./run_ppo_with_inference_bm.py --backend <backend> --mode <mode>\n```\n\n----------------------------------------\n\nTITLE: Wrapping PyTorch Training Logic in a Function\nDESCRIPTION: This function encapsulates the entire training process, including data loading, model initialization, and training/validation loops, preparing it for integration with Ray Train.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef train_func():\n    batch_size = 64\n    lr = 1e-3\n    epochs = 5\n    \n    # Create data loaders.\n    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n    \n    # Get cpu or gpu device for training.\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using {device} device\")\n    \n    model = NeuralNetwork().to(device)\n    print(model)\n    \n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    \n    for t in range(epochs):\n        print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_epoch(train_dataloader, model, loss_fn, optimizer)\n        test_epoch(test_dataloader, model, loss_fn)\n\n    print(\"Done!\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Dataset and Model with PyTorch and Ray Train (Python)\nDESCRIPTION: This code initializes a dataset and a model using PyTorch, preparing them for distributed training with Ray Train. It uses placeholders `__torch_setup_begin__` and `__torch_setup_end__` to indicate the start and end of the relevant code block.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/pytorch/torch_quick_start.py\n:language: python\n:start-after: __torch_setup_begin__\n:end-before: __torch_setup_end__\n```\"\n```\n\n----------------------------------------\n\nTITLE: Importing Ray and Defining Deployment\nDESCRIPTION: This code snippet demonstrates importing necessary libraries and defining a Ray Serve deployment for the Translator class. It highlights how to integrate Ray Serve's functionalities while allowing configuration through the decorator's parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/getting_started.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport ray\nfrom ray import serve\n\n@serve.deployment\nclass Translator:\n    ...  # existing implementation\n```\n\n```\n\n----------------------------------------\n\nTITLE: Training via Behavioral Cloning and PPO Fine-Tuning\nDESCRIPTION: Combines behavioral cloning for pre-training agents with PPO for fine-tuning, facilitating a two-phase training strategy in offline imitation learning followed by online reinforcement learning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Train with behavioral cloning (BC), Finetune with PPO\n# This script implements a two-phase training strategy.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Training Function in Ray Train\nDESCRIPTION: This code shows how to define a training function that will run on each distributed worker process. The function typically contains logic for loading the model, dataset, training the model, saving checkpoints, and logging metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/overview.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef train_func():\n    \"\"\"User-defined training function that runs on each distributed worker process.\n\n    This function typically contains logic for loading the model,\n    loading the dataset, training the model, saving checkpoints,\n    and logging metrics.\n    \"\"\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Serve LLM Dependencies\nDESCRIPTION: Install required dependencies for Ray Serve LLM deployment, including Ray and vLLM\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ray[serve,llm]>=2.43.0 vllm>=0.7.2\n\n# Suggested dependencies when using vllm 0.7.2:\npip install xgrammar==0.1.11 pynvml==12.0.0\n```\n\n----------------------------------------\n\nTITLE: Training a TensorFlow Model with Spark DataFrame using RayDP\nDESCRIPTION: Example of creating a DataFrame in Spark, splitting it into training/testing sets, and training a TensorFlow model using RayDP's TFEstimator API for distributed training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/raydp.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col\ndf = spark.range(1, 1000)\n# calculate z = x + 2y + 1000\ndf = df.withColumn(\"x\", col(\"id\")*2)\\\n  .withColumn(\"y\", col(\"id\") + 200)\\\n  .withColumn(\"z\", col(\"x\") + 2*col(\"y\") + 1000)\n\nfrom raydp.utils import random_split\ntrain_df, test_df = random_split(df, [0.7, 0.3])\n\n# TensorFlow code\nfrom tensorflow import keras\ninput_1 = keras.Input(shape=(1,))\ninput_2 = keras.Input(shape=(1,))\n\nconcatenated = keras.layers.concatenate([input_1, input_2])\noutput = keras.layers.Dense(1, activation='sigmoid')(concatenated)\nmodel = keras.Model(inputs=[input_1, input_2],\n                    outputs=output)\n\noptimizer = keras.optimizers.Adam(0.01)\nloss = keras.losses.MeanSquaredError()\n\nfrom raydp.tf import TFEstimator\nestimator = TFEstimator(\n  num_workers=2,\n  model=model,\n  optimizer=optimizer,\n  loss=loss,\n  metrics=[\"accuracy\", \"mse\"],\n  feature_columns=[\"x\", \"y\"],\n  label_column=\"z\",\n  batch_size=1000,\n  num_epochs=2,\n  use_gpu=False,\n  config={\"fit_config\": {\"steps_per_epoch\": 2}})\n\nestimator.fit_on_spark(train_df, test_df)\n\ntensorflow_model = estimator.get_model()\n\nestimator.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Running Stable Diffusion Fine-Tuning Script with Ray\nDESCRIPTION: This bash command demonstrates how to run the Stable Diffusion fine-tuning script using Ray. It includes various parameters for the training process, such as model path, data directory, and HPU-specific configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/sd.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython /root/optimum-habana/examples/stable-diffusion/training/textual_inversion.py \\\n  --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \\\n  --train_data_dir \"/root/cat\" \\\n  --learnable_property object \\\n  --placeholder_token \"<cat-toy>\" \\\n  --initializer_token toy \\\n  --resolution 512 \\\n  --train_batch_size 4 \\\n  --max_train_steps 3000 \\\n  --learning_rate 5.0e-04 \\\n  --scale_lr \\\n  --lr_scheduler constant \\\n  --lr_warmup_steps 0 \\\n  --output_dir /tmp/textual_inversion_cat \\\n  --save_as_full_pipeline \\\n  --gaudi_config_name Habana/stable-diffusion \\\n  --throughput_warmup_steps 3\n```\n\n----------------------------------------\n\nTITLE: Training a LightGBM Model with Ray Tune in Python\nDESCRIPTION: This Python script uses Ray Tune to optimize hyperparameters for a LightGBM model on the breast cancer dataset. It defines the training function, sets up the dataset, configures hyperparameter search spaces, and applies the ASHA Scheduler. Dependencies include lightgbm, numpy, scikit-learn, ray, and ray[tune]. It outputs the best hyperparameters found after the tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/lightgbm_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightgbm as lgb\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.tune.integration.lightgbm import TuneReportCheckpointCallback\n\ndef train_breast_cancer(config):\n\n    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.25)\n    train_set = lgb.Dataset(train_x, label=train_y)\n    test_set = lgb.Dataset(test_x, label=test_y)\n    gbm = lgb.train(\n        config,\n        train_set,\n        valid_sets=[test_set],\n        valid_names=[\"eval\"],\n        callbacks=[\n            TuneReportCheckpointCallback(\n                {\n                    \"binary_error\": \"eval-binary_error\",\n                    \"binary_logloss\": \"eval-binary_logloss\",\n                }\n            )\n        ],\n    )\n    preds = gbm.predict(test_x)\n    pred_labels = np.rint(preds)\n    tune.report(\n        {\n            \"mean_accuracy\": sklearn.metrics.accuracy_score(test_y, pred_labels),\n            \"done\": True,\n        }\n    )\n\n\nif __name__ == \"__main__\":\n    config = {\n        \"objective\": \"binary\",\n        \"metric\": [\"binary_error\", \"binary_logloss\"],\n        \"verbose\": -1,\n        \"boosting_type\": tune.grid_search([\"gbdt\", \"dart\"]),\n        \"num_leaves\": tune.randint(10, 1000),\n        \"learning_rate\": tune.loguniform(1e-8, 1e-1),\n    }\n\n    tuner = tune.Tuner(\n        train_breast_cancer,\n        tune_config=tune.TuneConfig(\n            metric=\"binary_error\",\n            mode=\"min\",\n            scheduler=ASHAScheduler(),\n            num_samples=2,\n        ),\n        param_space=config,\n    )\n    results = tuner.fit()\n\n    print(f\"Best hyperparameters found were: {results.get_best_result().config}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running PBT for DCGAN Training\nDESCRIPTION: Sets up the PBT scheduler and Tune configuration for training a DCGAN on MNIST data. It specifies hyperparameter mutations for the Generator and Discriminator learning rates and uses inception score as the optimization metric.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import PopulationBasedTraining\n\nfrom ray.tune.examples.pbt_dcgan_mnist.common import Net\nfrom ray.tune.examples.pbt_dcgan_mnist.pbt_dcgan_mnist_func import (\n    dcgan_train,\n    download_mnist_cnn,\n)\n\n# Load the pretrained mnist classification model for inception_score\nmnist_cnn = Net()\nmodel_path = download_mnist_cnn()\nmnist_cnn.load_state_dict(torch.load(model_path))\nmnist_cnn.eval()\n# Put the model in Ray object store.\nmnist_model_ref = ray.put(mnist_cnn)\n\nperturbation_interval = 5\nscheduler = PopulationBasedTraining(\n    perturbation_interval=perturbation_interval,\n    hyperparam_mutations={\n        # Distribution for resampling\n        \"netG_lr\": tune.uniform(1e-2, 1e-5),\n        \"netD_lr\": tune.uniform(1e-2, 1e-5),\n    },\n)\n\nsmoke_test = True  # For testing purposes: set this to False to run the full experiment\ntuner = tune.Tuner(\n    dcgan_train,\n    run_config=tune.RunConfig(\n        name=\"pbt_dcgan_mnist_tutorial\",\n        stop={\"training_iteration\": 5 if smoke_test else 150},\n    ),\n    tune_config=tune.TuneConfig(\n        metric=\"is_score\",\n        mode=\"max\",\n        num_samples=2 if smoke_test else 8,\n        scheduler=scheduler,\n    ),\n    param_space={\n        # Define how initial values of the learning rates should be chosen.\n        \"netG_lr\": tune.choice([0.0001, 0.0002, 0.0005]),\n        \"netD_lr\": tune.choice([0.0001, 0.0002, 0.0005]),\n        \"mnist_model_ref\": mnist_model_ref,\n        \"checkpoint_interval\": perturbation_interval,\n    },\n)\nresults_grid = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TensorFlow Deployment\nDESCRIPTION: Command to install TensorFlow 2 and required packages for Ray Serve deployment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/serve-ml-models.md#2025-04-12_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ pip install \"tensorflow>=2.0\" requests \"ray[serve]\"\n```\n\n----------------------------------------\n\nTITLE: Creating a RayCluster with kubectl-ray\nDESCRIPTION: Commands to create a RayCluster using the kubectl-ray plugin. It shows the default creation and how to override default values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kubectl-plugin.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl ray create cluster raycluster-sample\nkubectl ray create cluster raycluster-sample-2 --worker-replicas 2\n```\n\n----------------------------------------\n\nTITLE: Installing Ray with Data, Train, XGBoost and LightGBM Dependencies\nDESCRIPTION: Installs the required dependencies for running distributed training and inference with XGBoost and LightGBM on Ray. This includes Ray with data and train extensions, as well as the XGBoost and LightGBM libraries.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -qU \"ray[data,train]\" xgboost lightgbm\n```\n\n----------------------------------------\n\nTITLE: Connecting to Ray Cluster in Python\nDESCRIPTION: Code snippet demonstrating how to connect to an existing Ray cluster from Python. This is the basic initialization step required before using any Ray functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_start.txt#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray in Python\nDESCRIPTION: Demonstrates how to import and initialize Ray for distributed computing. Ray.init() is automatically called in recent versions (>=1.5) on first use of a Ray remote API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/walkthrough.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Start Ray\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Multi-Model LLM Deployment using Builder Pattern\nDESCRIPTION: Configure and deploy multiple LLM models using Ray Serve's builder pattern\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.llm import LLMConfig, build_openai_app\n\nllm_config1 = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"qwen-0.5b\",\n        model_source=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1, max_replicas=2,\n        )\n    ),\n    accelerator_type=\"A10G\",\n)\n\nllm_config2 = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"qwen-1.5b\",\n        model_source=\"Qwen/Qwen2.5-1.5B-Instruct\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1, max_replicas=2,\n        )\n    ),\n    accelerator_type=\"A10G\",\n)\n\napp = build_openai_app({\"llm_configs\": [llm_config1, llm_config2]})\nserve.run(app, blocking=True)\n```\n\n----------------------------------------\n\nTITLE: Actor State Inspection Command\nDESCRIPTION: Command to inspect actor state details including death causes using Ray's State API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/terminating-actors.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nray list actors --detail\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Inference Implementation\nDESCRIPTION: Implementation of batch inference using a PyTorch neural network. Creates a Dataset from numpy arrays and defines a Predictor class with a simple linear model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/batch_inference.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport ray\n\nds = ray.data.from_numpy(np.ones((1, 100)))\n\nclass TorchPredictor:\n    def __init__(self):\n        self.model = nn.Sequential(\n            nn.Linear(in_features=100, out_features=1),\n            nn.Sigmoid(),\n        )\n        self.model.eval()\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        tensor = torch.as_tensor(batch[\"data\"], dtype=torch.float32)\n        with torch.inference_mode():\n            return {\"output\": self.model(tensor).numpy()}\n\npredictions = ds.map_batches(TorchPredictor, concurrency=2)\npredictions.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: LLM Deployment using Builder Pattern with Single Model\nDESCRIPTION: Configure and deploy a single LLM model using Ray Serve's builder pattern with custom configuration options\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.llm import LLMConfig, build_openai_app\n\nllm_config = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"qwen-0.5b\",\n        model_source=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1, max_replicas=2,\n        )\n    ),\n    accelerator_type=\"A10G\",\n    engine_kwargs=dict(\n        tensor_parallel_size=2,\n    ),\n)\n\napp = build_openai_app({\"llm_configs\": [llm_config]})\nserve.run(app, blocking=True)\n```\n\n----------------------------------------\n\nTITLE: Loading and Generating Images with Fine-tuned Stable Diffusion on Habana Gaudi\nDESCRIPTION: Loads a fine-tuned Stable Diffusion model from a local directory using Habana Gaudi hardware acceleration. The code initializes a GaudiStableDiffusionPipeline with bfloat16 precision and HPU graphs, then generates an image based on a prompt containing the trained token.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/sd.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom optimum.habana.diffusers import GaudiStableDiffusionPipeline\nmodel_id = \"/tmp/textual_inversion_cat/\"\npipe = GaudiStableDiffusionPipeline.from_pretrained(\n  model_id,\n  torch_dtype=torch.bfloat16,\n  use_habana=True,\n  use_hpu_graphs=True,\n  gaudi_config=\"Habana/stable-diffusion\",\n)\nprompt = \"a <cat-toy> is dancing on the grass.\"\nimage = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\nimage.save(\"cat-backpack.png\")\n```\n\n----------------------------------------\n\nTITLE: Describing Datasets with Schema in Ray\nDESCRIPTION: This snippet demonstrates how to view a dataset's column names and types using the schema() method on a Ray Dataset created from a CSV file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/inspecting-data.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Creating and Scaling GPU Actors in Ray with Python\nDESCRIPTION: This Python example demonstrates how to use Ray's GPU-aware scheduling by creating GPU actors with @ray.remote(num_gpus=1) annotation. When executed, this code will trigger the autoscaling of GPU workers in a properly configured RayCluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gpu.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\n\n@ray.remote(num_gpus=1)\nclass GPUActor:\n    def say_hello(self):\n        print(\"I live in a pod with GPU access.\")\n\n# Request actor placement.\ngpu_actors = [GPUActor.remote() for _ in range(2)]\n# The following command will block until two Ray pods with GPU access are scaled\n# up and the actors are placed.\nray.get([actor.say_hello.remote() for actor in gpu_actors])\n```\n\n----------------------------------------\n\nTITLE: Defining the Model Training Function in Python\nDESCRIPTION: This code defines a training function that takes a configuration dictionary and returns results. This function represents the core training logic that will be executed by Tune with different parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-run.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef train_model(config):\n    # config: Dictionary of trial parameters\n    model_id = config[\"model_id\"]\n    \n    # Train model, simulate with a fast function\n    final_score = model_id\n    \n    # Return results\n    return {\n        \"score\": final_score,\n        \"other_data\": ...,  # can return any Python object\n    }\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Ray Train with Transformers\nDESCRIPTION: Command to install the necessary Python packages for running Ray Train with Hugging Face Transformers, including Ray with train support, PyTorch, Transformers, datasets, evaluate, and scikit-learn.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-transformers.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[train]\" torch \"transformers[torch]\" datasets evaluate numpy scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Evaluating Search Space with Cross-Validation\nDESCRIPTION: This function serves as the main entry point for the AutoML system, coordinating the evaluation of all model configurations using cross-validation and returning the best model based on the specified metric.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/automl_for_time_series.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_search_space_with_cv(\n    search_space: Dict[Type[_TS], Dict[str, list]],\n    df: pd.DataFrame,\n    label_column: str,\n    metrics: Dict[str, Callable[[pd.Series, pd.Series], float]],\n    eval_metric: str,\n    mode: str = \"min\",\n    freq: str = \"D\",\n    cv: Union[int, TimeSeriesSplit] = 5,\n) -> List[Tuple[_TS, Dict[str, float]]]:\n    assert eval_metric in metrics\n    assert mode in (\"min\", \"max\")\n\n    configurations = list(generate_configurations(search_space))\n    print(\n        f\"Evaluating {len(configurations)} configurations with {cv.get_n_splits()} splits each, \"\n        f\"totalling {len(configurations)*cv.get_n_splits()} tasks...\"\n    )\n    ret = evaluate_models_with_cv(\n        configurations, df, label_column, metrics, freq=freq, cv=cv\n    )\n\n    # Sort the results by eval_metric\n    ret = sorted(ret.items(), key=lambda x: x[1][eval_metric], reverse=(mode == \"max\"))\n    print(\"Evaluation complete!\")\n    return ret\n```\n\n----------------------------------------\n\nTITLE: Preserving Order in Ray Datasets Transformations\nDESCRIPTION: Shows how to preserve the order of blocks when transforming data in Ray Datasets by setting the preserve_order execution option.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nctx = ray.data.DataContext().get_current()\n\n# By default, this is set to False.\nctx.execution_options.preserve_order = True\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Example of Seeding for Reproducible Ray Tune Experiments\nDESCRIPTION: This example shows a comprehensive approach to seeding random number generators for reproducible experiments in Ray Tune, including search algorithms, schedulers, and training functions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport numpy as np\nimport torch\n\nfrom ray import tune\nfrom ray.tune.search.bayesopt import BayesOptSearch\nfrom ray.tune.schedulers import ASHAScheduler\n\ndef set_seeds(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef train_fn(config):\n    set_seeds(config[\"seed\"])\n    # Your training code here\n\nsearch_alg = BayesOptSearch(random_state=1234)\nscheduler = ASHAScheduler(random_state=5678)\n\ntuner = tune.Tuner(\n    train_fn,\n    tune_config=tune.TuneConfig(\n        search_alg=search_alg,\n        scheduler=scheduler,\n    ),\n    param_space={\n        # ...\n        \"seed\": tune.randint(0, 1000000),\n    },\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Initializing Model for Pytorch ResNet Finetuning\nDESCRIPTION: Provides two options for initializing the ResNet model: one using pre-trained weights and another using a checkpoint. Includes functions to load and modify the model architecture for fine-tuning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nfrom ray.train import Checkpoint\n\n# Option 1: Initialize model with pretrained weights\ndef initialize_model():\n    # Load pretrained model params\n    model = models.resnet50(pretrained=True)\n\n    # Replace the original classifier with a new Linear layer\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, 2)\n\n    # Ensure all params get updated during finetuning\n    for param in model.parameters():\n        param.requires_grad = True\n    return model\n\n\n# Option 2: Initialize model with an Train checkpoint\n# Replace this with your own uri\nCHECKPOINT_FROM_S3 = Checkpoint(\n    path=\"s3://air-example-data/finetune-resnet-checkpoint/TorchTrainer_4f69f_00000_0_2023-02-14_14-04-09/checkpoint_000001/\"\n)\n\n\ndef initialize_model_from_checkpoint(checkpoint: Checkpoint):\n    with checkpoint.as_directory() as tmpdir:\n        state_dict = torch.load(os.path.join(tmpdir, \"checkpoint.pt\"))\n    resnet50 = initialize_model()\n    resnet50.load_state_dict(state_dict[\"model\"])\n    return resnet50\n```\n\n----------------------------------------\n\nTITLE: Illustrating Task Execution Order from Different Submitters in Python\nDESCRIPTION: This code demonstrates that a synchronous, single-threaded actor does not guarantee execution order for tasks from different submitters. It shows how a task with a delayed dependency can be executed after a later submitted task from a different worker.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/task-orders.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport ray\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def add(self, addition):\n        self.value += addition\n        return self.value\n\ncounter = Counter.remote()\n\n# Submit task from a worker\n@ray.remote\ndef submitter(value):\n    return ray.get(counter.add.remote(value))\n\n# Simulate delayed result resolution.\n@ray.remote\ndef delayed_resolution(value):\n    time.sleep(5)\n    return value\n\n# Submit tasks from different workers, with\n# the first submitted task waiting for\n# dependency resolution.\nvalue0 = submitter.remote(delayed_resolution.remote(1))\nvalue1 = submitter.remote(2)\n\n# Output: 3. The first submitted task is executed later.\nprint(ray.get(value0))\n# Output: 2. The later submitted task is executed first.\nprint(ray.get(value1))\n```\n\n----------------------------------------\n\nTITLE: Defining Ray Train Function for Vicuna-13B Fine-tuning in Python\nDESCRIPTION: This snippet defines the training function for Ray Train, setting up the PyTorch Lightning Trainer with Ray-specific components like RayDeepSpeedStrategy and RayLightningEnvironment. It also configures Ray Data ingestion for the training dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray.train\nfrom ray.train import CheckpointConfig, RunConfig, ScalingConfig\nfrom ray.train.torch import TorchTrainer\nfrom ray.train.lightning import (\n    prepare_trainer,\n    RayDeepSpeedStrategy, \n    RayLightningEnvironment, \n    RayTrainReportCallback\n)\n\n\ndef train_func(config):\n    \"\"\"Training function for each worker.\"\"\"\n\n    # Unpack the `train_loop_config`\n    max_epochs = config[\"max_epochs\"]\n    batch_size = config[\"batch_size\"]\n    accumulate_grad_batches = config[\"accumulate_grad_batches\"]\n\n    model = Vicuna13BModel()\n    \n    # Prepare Ray Data Ingestion\n    train_ds = ray.train.get_dataset_shard(\"train\")\n    train_dataloader = train_ds.iter_torch_batches(batch_size=batch_size)\n    \n    pl_trainer = pl.Trainer(\n        devices=\"auto\",\n        accelerator=\"auto\",\n        strategy=RayDeepSpeedStrategy(config=deepspeed_configs),\n        plugins=[RayLightningEnvironment()],\n        callbacks=[RayTrainReportCallback()],\n        enable_checkpointing=False, # RayTrainReportCallback will save the checkpoints\n        max_epochs=max_epochs,\n        precision=\"bf16-mixed\",\n        accumulate_grad_batches=accumulate_grad_batches,\n    )\n    pl_trainer = prepare_trainer(pl_trainer)\n\n    pl_trainer.fit(model, train_dataloaders=train_dataloader)\n    \n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func,\n    train_loop_config={\n        \"max_epochs\": 1,\n        \"batch_size\": BATCH_SIZE_PER_WORKER,\n        \"accumulate_grad_batches\": 2\n    },\n    run_config=RunConfig(\n        name=\"vicuna-13b-finetune\",\n        storage_path=\"s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/air-release-tests\",\n        checkpoint_config=CheckpointConfig(num_to_keep=1),\n    ),\n    scaling_config=ScalingConfig(\n        num_workers=NUM_WORKERS,\n        use_gpu=True,\n        resources_per_worker={\"CPU\": 15, \"GPU\": 1},\n    ),\n    datasets={\"train\": processed_ds},\n)\n```\n\n----------------------------------------\n\nTITLE: Reading from Databricks with Ray Data\nDESCRIPTION: This snippet demonstrates how to read data from Databricks using Ray Data's `read_databricks_tables` function. It specifies the warehouse ID, catalog, schema, and a SQL query to retrieve data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\ndataset = ray.data.read_databricks_tables(\n    warehouse_id='...',  # Databricks SQL warehouse ID\n    catalog='catalog_1',  # Unity catalog name\n    schema='db_1',  # Schema name\n    query=\"SELECT title, score FROM movie WHERE year >= 1980\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Remote EnvRunner Actors in RLlib with Python\nDESCRIPTION: This snippet demonstrates how to create remote EnvRunner actors in RLlib using Ray and configure them for parallel experience gathering. It showcases the use of SingleAgentEnvRunner to collect episodes from the environment and reports the returns of collected episodes. The code also illustrates configuring the environment and the number of EnvRunners.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/key-concepts.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport tree  # pip install dm_tree\nimport ray\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.env.single_agent_env_runner import SingleAgentEnvRunner\n\n# Configure the EnvRunners.\nconfig = (\n    PPOConfig()\n    .environment(\"Acrobot-v1\")\n    .env_runners(num_env_runners=2, num_envs_per_env_runner=1)\n)\n# Create the EnvRunner actors.\nenv_runners = [\n    ray.remote(SingleAgentEnvRunner).remote(config=config)\n    for _ in range(config.num_env_runners)\n]\n\n# Gather lists of `SingleAgentEpisode`s (each EnvRunner actor returns one\n# such list with exactly two episodes in it).\nepisodes = ray.get([\n    er.sample.remote(num_episodes=3)\n    for er in env_runners\n])\n# Two remote EnvRunners used.\nassert len(episodes) == 2\n# Each EnvRunner returns three episodes\nassert all(len(eps_list) == 3 for eps_list in episodes)\n\n# Report the returns of all episodes collected\nfor episode in tree.flatten(episodes):\n    print(\"R=\", episode.get_return())\n\n```\n\nLANGUAGE: python\nCODE:\n```\nfor er in env_runners:\n    er.stop.remote()\n```\n\n----------------------------------------\n\nTITLE: Basic Ray Multiprocessing Pool Example in Python\nDESCRIPTION: A simple example demonstrating how to use Ray's multiprocessing Pool implementation to distribute tasks. It creates a Pool instance and uses the map function to apply a function to a range of inputs in parallel.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/multiprocessing.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.multiprocessing import Pool\n\ndef f(index):\n    return index\n\npool = Pool()\nfor result in pool.map(f, range(100)):\n    print(result)\n```\n\n----------------------------------------\n\nTITLE: Client-side Vision Language Model Interaction\nDESCRIPTION: Python code for client-side interaction with a vision language model deployed using Ray Serve LLM. Demonstrates how to send a request with both text and image content.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Initialize client\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n\n# Create and send a request with an image\nresponse = client.chat.completions.create(\n    model=\"pixtral-12b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"What's in this image?\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://example.com/image.jpg\"\n                    }\n                }\n            ]\n        }\n    ],\n```\n\n----------------------------------------\n\nTITLE: Integrating RLlib with Ray Tune for Hyperparameter Optimization\nDESCRIPTION: Demonstrates how to use Ray Tune to perform hyperparameter sweeps on an RLlib algorithm, creating multiple trials with different learning rates and running them in parallel.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import train, tune\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\nconfig = (\n    PPOConfig()\n    .environment(\"Pendulum-v1\")\n    # Specify a simple tune hyperparameter sweep.\n    .training(\n        lr=tune.grid_search([0.001, 0.0005, 0.0001]),\n    )\n)\n\n# Create a Tuner instance to manage the trials.\ntuner = tune.Tuner(\n    config.algo_class,\n    param_space=config,\n    # Specify a stopping criterion. Note that the criterion has to match one of the\n    # pretty printed result metrics from the results returned previously by\n    # \".train()\". Also note that -1100 is not a good episode return for\n    # Pendulum-v1, we are using it here to shorten the experiment time.\n    run_config=train.RunConfig(\n        stop={\"env_runners/episode_return_mean\": -1100.0},\n    ),\n)\n# Run the Tuner and capture the results.\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Processing Datasets with Ray Data\nDESCRIPTION: Example showing how to read, transform, and process data using Ray Data. The code demonstrates reading a CSV from S3, applying a parallel transformation to compute petal area, and iterating through batches of the transformed dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport ray\n\n# Create datasets from on-disk files, Python objects, and cloud storage like S3.\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\n# Apply functions to transform data. Ray Data executes transformations in parallel.\ndef compute_area(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    length = batch[\"petal length (cm)\"]\n    width = batch[\"petal width (cm)\"]\n    batch[\"petal area (cm^2)\"] = length * width\n    return batch\n\ntransformed_ds = ds.map_batches(compute_area)\n\n# Iterate over batches of data.\nfor batch in transformed_ds.iter_batches(batch_size=4):\n    print(batch)\n```\n\n----------------------------------------\n\nTITLE: Basic Ingress Deployment for HTTP Handling in Ray Serve\nDESCRIPTION: This code snippet shows a basic ingress deployment in Ray Serve that handles HTTP requests.  The `BasicIngress` deployment receives a `Starlette` request object, extracts the query parameters from the URL, and returns a JSON response. This demonstrates how to handle HTTP requests and return responses within a Ray Serve deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/key-concepts.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example showing a basic ingress deployment for HTTP handling.\n\"\"\"\nfrom starlette.requests import Request\n\nimport ray\nfrom ray import serve\n\n@serve.deployment\nclass BasicIngress:\n    async def __call__(self, request: Request) -> dict:\n        name = request.query_params[\"name\"]\n        return {f\"Hello\", name: f\"You sent a GET request to root.\"\n                f\"Your name is {name}.\"}\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Ray Generator Task in Python\nDESCRIPTION: This snippet demonstrates how to create a Ray generator task by decorating a Python generator function with @ray.remote. It shows the difference between using a normal Python generator and a Ray generator task.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-generator.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n+import ray\nimport time\n\n# Takes 25 seconds to finish.\n+@ray.remote\ndef f():\n    for i in range(5):\n        time.sleep(5)\n        yield i\n\n-for obj in f():\n+for obj_ref in f.remote():\n    # Prints every 5 seconds and stops after 25 seconds.\n-    print(obj)\n+    print(ray.get(obj_ref))\n```\n\n----------------------------------------\n\nTITLE: Using HyperBand Scheduler in Ray Tune\nDESCRIPTION: Example showing how to configure and use the HyperBand scheduler with Tune for hyperparameter optimization. The scheduler takes a metric to optimize and a mode (min/max) parameter.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef trainable(config):\n    # ...\n    pass\n\ntuner = Tuner(\n    trainable,\n    tune_config=tune.TuneConfig(\n        scheduler=tune.schedulers.HyperBandScheduler(\n            metric=\"loss\",\n            mode=\"min\"\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing Test Data for Model Evaluation\nDESCRIPTION: Loads and prepares test data for the selected location. It reads the raw data, applies transformations, splits it into train/test sets, and extracts features and target values for model evaluation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Create some test data\ndf_list = [read_data(f, sample_location_id) for f in s3_files[:1]]\ndf_raw = pd.concat(df_list, ignore_index=True)\ndf = transform_df(df_raw)\n_, test_df = train_test_split(df, test_size=0.2, shuffle=True)\ntest_X = test_df[[\"passenger_count\", \"trip_distance\"]]\ntest_y = np.array(test_df.trip_duration)  # actual values\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Better Approach: Defining Remote Function Outside Loop (Python)\nDESCRIPTION: This code snippet demonstrates the recommended approach of defining the remote function outside the loop, ensuring it's pickled and uploaded only once, thus improving performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/redefine-task-actor-loop.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# __better_approach_start__\n@ray.remote\ndef f(i):\n    return i\n\nfor i in range(10):\n    ray.get([f.remote(i) for _ in range(1000)])\n# __better_approach_end__\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Processor Configuration\nDESCRIPTION: Example of configuring and using a vLLM processor for batch inference with the Llama model. Includes preprocessing and postprocessing logic for handling haiku generation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-llms.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\nimport numpy as np\n\nconfig = vLLMEngineProcessorConfig(\n    model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n    engine_kwargs={\n        \"enable_chunked_prefill\": True,\n        \"max_num_batched_tokens\": 4096,\n        \"max_model_len\": 16384,\n    },\n    concurrency=1,\n    batch_size=64,\n)\nprocessor = build_llm_processor(\n    config,\n    preprocess=lambda row: dict(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a bot that responds with haikus.\"},\n            {\"role\": \"user\", \"content\": row[\"item\"]}\n        ],\n        sampling_params=dict(\n            temperature=0.3,\n            max_tokens=250,\n        )\n    ),\n    postprocess=lambda row: dict(\n        answer=row[\"generated_text\"],\n        **row\n    ),\n)\n\nds = ray.data.from_items([\"Start of the haiku is: Complete this for me...\"])\n\nds = processor(ds)\nds.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Writing Replay Buffer Contents to Disk via Callback in Python\nDESCRIPTION: This example illustrates how to implement a custom callback function to write the replay buffer contents to disk during training. It uses the ormsgpack library for efficient serialization. The callback function is added to the DQNConfig and is triggered every other training iteration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-callback.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ormsgpack\nfrom ray.rllib.algorithms.dqn import DQNConfig\n\ndef _write_buffer_if_necessary(algorithm, metrics_logger, result):\n    # Write the buffer contents only every ith iteration.\n    if algorithm.training_iteration % 2 == 0:\n        # python dict\n        buffer_contents = algorithm.local_replay_buffer.get_state()\n\n        # binary\n        msgpacked = ormsgpack.packb(\n           buffer_contents,\n           option=ormsgpack.OPT_SERIALIZE_NUMPY,\n        )\n\n        # Open some file and write the buffer contents into it using `ormsgpack`.\n        with open(\"replay_buffer_contents.msgpack\", \"wb\") as f:\n           f.write(msgpacked)\n\nconfig = (\n    DQNConfig()\n    .environment(\"CartPole-v1\")\n    .callbacks(\n       on_train_result=_write_buffer_if_necessary,\n    )\n)\ndqn = config.build()\n\n# Train n times. Expect RLlib to write buffer every ith iteration.\nfor _ in range(4):\n    print(dqn.train())\n```\n\n----------------------------------------\n\nTITLE: Custom Callbacks in RLlib Python\nDESCRIPTION: This code snippet illustrates the implementation of custom callbacks in Ray RLlib's new API stack. It demonstrates subclassing the RLlibCallback and adapting the method signatures to match the new environment runner context.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.callbacks.callbacks import RLlibCallback\n\nclass YourCallbacksClass(RLlibCallback):\n\n    def on_episode_start(\n        self,\n        *,\n        episode,\n        env_runner,\n        metrics_logger,\n        env,\n        env_index,\n        rl_module,\n\n        # Old API stack args; don't use or access these inside your method code.\n        worker=None,\n        base_env=None,\n        policies=None,\n        **kwargs,\n    ):\n        # The `SingleAgentEpisode` or `MultiAgentEpisode` that RLlib has just started.\n        print(episode)\n        # The `EnvRunner` class that collects the episode in question.\n        print(env_runner)\n        # The MetricsLogger object on the EnvRunner.\n        print(metrics_logger.peek(\"episode_return_mean\", default=0.0))\n        # The gymnasium env for sample collection.\n        print(env)\n        # The env index in case of a vector env.\n        print(env_index)\n        # The RL Module that this EnvRunner uses.\n        print(rl_module)\n```\n\n----------------------------------------\n\nTITLE: Loading and Initializing Pre-trained PyTorch Object Detection Model\nDESCRIPTION: Loads a pre-trained FasterRCNN ResNet50 model from torchvision and sets it to evaluation mode.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n\nweights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\nmodel = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\nmodel.eval();\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Dataset Splitting in Ray Train\nDESCRIPTION: Demonstrates how to use DataConfig to split only the training dataset while keeping the validation dataset unsharded across workers. This allows for distributed training with sharded training data but full validation data for each worker.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import train\nfrom ray.train import ScalingConfig\nfrom ray.train.torch import TorchTrainer\n\nds = ray.data.read_text(\n    \"s3://anonymous@ray-example-data/sms_spam_collection_subset.txt\"\n)\ntrain_ds, val_ds = ds.train_test_split(0.3)\n\ndef train_loop_per_worker():\n    # Get the sharded training dataset\n    train_ds = train.get_dataset_shard(\"train\")\n    for _ in range(2):\n        for batch in train_ds.iter_batches(batch_size=128):\n            print(\"Do some training on batch\", batch)\n\n    # Get the unsharded full validation dataset\n    val_ds = train.get_dataset_shard(\"val\")\n    for _ in range(2):\n        for batch in val_ds.iter_batches(batch_size=128):\n            print(\"Do some evaluation on batch\", batch)\n\nmy_trainer = TorchTrainer(\n    train_loop_per_worker,\n    scaling_config=ScalingConfig(num_workers=2),\n    datasets={\"train\": train_ds, \"val\": val_ds},\n    dataset_config=ray.train.DataConfig(\n        datasets_to_split=[\"train\"],\n    ),\n)\nmy_trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Accessing Ray Dashboard\nDESCRIPTION: Sets up port forwarding to access the Ray dashboard on localhost port 8265. This allows monitoring the status of Ray Serve applications through the web UI.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# (1) Forward the dashboard port to localhost.\n# (2) Check the Serve page in the Ray dashboard at http://localhost:8265/#/serve.\nkubectl port-forward svc/rayservice-sample-head-svc 8265:8265\n```\n\n----------------------------------------\n\nTITLE: Defining a Ray Generator Function in Python\nDESCRIPTION: This code snippet shows how to define a Python generator function and decorate it with @ray.remote to create a Ray generator. It demonstrates the basic structure of a Ray generator task.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-generator.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\n@ray.remote\ndef streaming_generator():\n    for i in range(4):\n        time.sleep(1)\n        yield i\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Data with Ray Data\nDESCRIPTION: Defines preprocessing steps to prepare the dataset for training. This includes splitting text into individual lines, removing extraneous characters, and tokenizing the text. The processed data is suitable for feeding into the model for training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nblock_size = 512\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\n\ndef split_text(batch: pd.DataFrame) -> pd.DataFrame:\n    text = list(batch[\"text\"])\n    flat_text = \"\".join(text)\n    split_text = [\n        x.strip()\n        for x in flat_text.split(\"\\n\")\n        if x.strip() and not x.strip()[-1] == \":\"\n    ]\n    return pd.DataFrame(split_text, columns=[\"text\"])\n\ndef tokenize(batch: pd.DataFrame) -> dict:\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n    tokenizer.pad_token = tokenizer.eos_token\n    ret = tokenizer(\n        list(batch[\"text\"]),\n        truncation=True,\n        max_length=block_size,\n        padding=\"max_length\",\n        return_tensors=\"np\",\n    )\n    ret[\"labels\"] = ret[\"input_ids\"].copy()\n    return dict(ret)\n\n\nprocessed_datasets = {\n    key: (\n        ds.map_batches(split_text, batch_format=\"pandas\")\n        .map_batches(tokenize, batch_format=\"pandas\")\n    )\n    for key, ds in ray_datasets.items()\n}\nprocessed_datasets\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom DataConfig for Advanced Dataset Distribution\nDESCRIPTION: Shows how to create a custom DataConfig subclass that handles dataset distribution across workers. This example demonstrates the same functionality as the default DataConfig implementation but can be extended for more complex use cases.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Note that this example class is doing the same thing as the basic DataConfig\n# implementation included with Ray Train.\nfrom typing import Optional, Dict, List\n\nimport ray\nfrom ray import train\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import DataConfig, ScalingConfig\nfrom ray.data import Dataset, DataIterator, NodeIdStr\nfrom ray.actor import ActorHandle\n\nds = ray.data.read_text(\n    \"s3://anonymous@ray-example-data/sms_spam_collection_subset.txt\"\n)\n\ndef train_loop_per_worker():\n    # Get an iterator to the dataset we passed in below.\n    it = train.get_dataset_shard(\"train\")\n    for _ in range(2):\n        for batch in it.iter_batches(batch_size=128):\n            print(\"Do some training on batch\", batch)\n\n\nclass MyCustomDataConfig(DataConfig):\n    def configure(\n        self,\n        datasets: Dict[str, Dataset],\n        world_size: int,\n        worker_handles: Optional[List[ActorHandle]],\n        worker_node_ids: Optional[List[NodeIdStr]],\n        **kwargs,\n    ) -> List[Dict[str, DataIterator]]:\n        assert len(datasets) == 1, \"This example only handles the simple case\"\n\n        # Configure Ray Data for ingest.\n        ctx = ray.data.DataContext.get_current()\n        ctx.execution_options = DataConfig.default_ingest_options()\n\n        # Split the stream into shards.\n        iterator_shards = datasets[\"train\"].streaming_split(\n            world_size, equal=True, locality_hints=worker_node_ids\n        )\n\n        # Return the assigned iterators for each worker.\n        return [{\"train\": it} for it in iterator_shards]\n\n\nmy_trainer = TorchTrainer(\n    train_loop_per_worker,\n    scaling_config=ScalingConfig(num_workers=2),\n    datasets={\"train\": ds},\n    dataset_config=MyCustomDataConfig(),\n)\nmy_trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining the PBT Scheduler in Python\nDESCRIPTION: This snippet sets up a Population Based Training (PBT) scheduler which defines how the hyperparameters will be mutated during the training process. It includes important parameters such as 'hyperparam_mutations', allowing exploration of various hyperparameter values to optimize performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nperturbation_interval = 5\nscheduler = PopulationBasedTraining(\n    time_attr=\"training_iteration\",\n    perturbation_interval=perturbation_interval,\n    metric=\"mean_accuracy\",\n    mode=\"max\",\n    hyperparam_mutations={\n        # distribution for resampling\n        \"lr\": tune.uniform(0.0001, 1),\n        # allow perturbations within this set of categorical values\n        \"momentum\": [0.8, 0.9, 0.99],\n    },\n)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Stable Diffusion with Ray Serve\nDESCRIPTION: Command to install required Python packages including Ray Serve, PyTorch, Diffusers, and Transformers libraries needed for running the Stable Diffusion model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/stable-diffusion.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[serve]\" requests torch diffusers==0.12.1 transformers\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running Distributed Training with Ray Tune\nDESCRIPTION: This code sets up the Ray Tune experiment for hyperparameter optimization using Population Based Training (PBT). It defines the search space, configures the PBT scheduler, and launches the tuning process with distributed training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/tune_cifar_torch_pbt_example.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef tune_cifar(num_workers=2, use_gpu=False):\n    data_dir = os.path.abspath(\"./data\")\n    load_data(data_dir)  # We do this first to download the data.\n\n    config = {\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"momentum\": tune.uniform(0.1, 0.9),\n        \"decay\": tune.loguniform(1e-5, 1e-3),\n        \"data_dir\": data_dir,\n    }\n\n    pbt_scheduler = PopulationBasedTraining(\n        time_attr=\"training_iteration\",\n        perturbation_interval=5,\n        hyperparam_mutations={\n            \"lr\": tune.loguniform(1e-4, 1e-1),\n            \"momentum\": tune.uniform(0.1, 0.9),\n        },\n    )\n\n    tuner = tune.Tuner(\n        tune.with_resources(\n            tune.with_parameters(TorchTrainer.as_trainable(train_cifar)),\n            resources={\"cpu\": 2, \"gpu\": int(use_gpu)},\n        ),\n        tune_config=tune.TuneConfig(\n            metric=\"mean_accuracy\",\n            mode=\"max\",\n            scheduler=pbt_scheduler,\n            num_samples=4,\n        ),\n        run_config=train.RunConfig(\n            name=\"pbt_test\",\n            local_dir=\"~/ray_results\",\n            checkpoint_config=train.CheckpointConfig(\n                checkpoint_frequency=5, checkpoint_at_end=True\n            ),\n        ),\n        param_space=config,\n    )\n    results = tuner.fit()\n\n    print(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Handling Multiple Routes with FastAPI - Python\nDESCRIPTION: This snippet shows how to define multiple HTTP routes using FastAPI within a Ray Serve deployment. It allows various HTTP methods to be declared efficiently.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"{literalinclude} doc_code/http_guide/http_guide.py\\n:start-after: __begin_fastapi_multi_routes__\\n:end-before: __end_fastapi_multi_routes__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Checkpoint Saving in Ray Train\nDESCRIPTION: This snippet demonstrates how to configure checkpoint saving using CheckpointConfig in Ray Train. It shows how to keep only the top K checkpoints based on a specified metric.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import CheckpointConfig\n\ncheckpoint_config = CheckpointConfig(\n    num_to_keep=5,\n    checkpoint_score_attribute=\"accuracy\",\n    checkpoint_score_order=\"max\"\n)\n```\n\n----------------------------------------\n\nTITLE: Reporting Aggregated Metrics with TorchMetrics in Python\nDESCRIPTION: This snippet shows how to report aggregated R2 score and mean loss from all workers using the TorchMetrics library. It leverages the built-in capabilities of Ray Train and TorchMetrics for distributed metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/monitoring-logging.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n.. literalinclude:: ../doc_code/metric_logging.py\n    :language: python\n    :start-after: __torchmetrics_start__\n    :end-before: __torchmetrics_end__\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Training with Ray Train - Python\nDESCRIPTION: This code example demonstrates how to run distributed training of a TensorFlow model on the MNIST dataset using the Ray Train framework. It illustrates the integration of TensorFlow with Ray for efficient model training across multiple resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/tf/tensorflow_mnist_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: /../../python/ray/train/examples/tf/tensorflow_mnist_example.py\n```\n\n----------------------------------------\n\nTITLE: Loading Image Dataset from S3\nDESCRIPTION: Loading the Imagenette dataset from an S3 bucket using Ray Data's read_images function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/pytorch_resnet_batch_prediction.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\ns3_uri = \"s3://anonymous@air-example-data-2/imagenette2/train/\"\n\nds = ray.data.read_images(s3_uri, mode=\"RGB\")\nds\n```\n\n----------------------------------------\n\nTITLE: Initializing SingleAgentEnvRunner - Python\nDESCRIPTION: This snippet shows the initialization of the SingleAgentEnvRunner class, which sets up the necessary parameters and environment for single-agent reinforcement learning tasks. Key parameters include the environment configuration and any additional settings required for the agent's operation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_env_runner.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass SingleAgentEnvRunner:\n    def __init__(self):\n        # Initialize the environment and agent\n        pass\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running a TorchTrainer\nDESCRIPTION: This code shows how to create a TorchTrainer instance and start the distributed training process. The trainer combines the training function and scaling configuration, then executes the training job with the fit() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/overview.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\n\ntrainer = TorchTrainer(train_func, scaling_config=scaling_config)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Querying PyTorch Model via HTTP\nDESCRIPTION: Python code to send an image request to the deployed PyTorch ResNet model\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/serve-ml-models.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nray_logo_bytes = requests.get(\n    \"https://raw.githubusercontent.com/ray-project/\"\n    \"ray/master/doc/source/images/ray_header_logo.png\"\n).content\n\nresp = requests.post(\"http://localhost:8000/\", data=ray_logo_bytes)\nprint(resp.json())\n```\n\n----------------------------------------\n\nTITLE: Creating GKE Cluster with GCSFuse and GPU Support\nDESCRIPTION: This command creates a GKE cluster with the GCSFuse CSI driver, Workload Identity, and a GPU node pool with 4 L4 GPUs. It sets up the necessary infrastructure for distributed training with Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PROJECT_ID=<your project id>\ngcloud container clusters create kuberay-with-gcsfuse \\\n    --addons GcsFuseCsiDriver \\\n    --cluster-version=1.29.4 \\\n    --location=us-east4-c \\\n    --machine-type=g2-standard-8 \\\n    --release-channel=rapid \\\n    --num-nodes=4 \\\n    --accelerator type=nvidia-l4,count=1,gpu-driver-version=latest \\\n    --workload-pool=${PROJECT_ID}.svc.id.goog\n```\n\n----------------------------------------\n\nTITLE: Implementing Rock-Paper-Scissors Multi-Agent Environment using RLlib in Python\nDESCRIPTION: This Python code sample outlines the structure of a Rock-Paper-Scissors multi-agent environment using the RLlib framework. Dependencies include RLlib and Python libraries for handling environment setups. The example demonstrates simultaneous step actions for agents and tracks moves until a defined episode end.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nThis is a scaffold for the Rock-Paper-Scissors multi-agent environment in RLlib.\n\"\"\"\n# Define your class here\ndef class_definition():\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nConstructor for Rock-Paper-Scissors multi-agent environment.\n\"\"\"\n# Define the constructor here\ndef constructor():\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nReset logic for the multi-agent environment, returning player observations and resetting move counter.\n\"\"\"\n# Define the reset function here\ndef reset():\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nStep function handling observations, rewards, and episode termination logic after 10 moves.\n\"\"\"\n# Define the step function here\ndef step():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Transforming Dataset with User-Defined Function in Ray Data\nDESCRIPTION: Illustrates how to apply a user-defined function to transform a dataset in Ray Data. The transformation computes a new 'petal area' attribute based on existing columns. It also demonstrates how to view the updated schema after transformation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/quickstart.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\n\n# Define a transformation to compute a \"petal area\" attribute\ndef transform_batch(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    vec_a = batch[\"petal length (cm)\"]\n    vec_b = batch[\"petal width (cm)\"]\n    batch[\"petal area (cm^2)\"] = vec_a * vec_b\n    return batch\n\n# Apply the transformation to our dataset\ntransformed_ds = ds.map_batches(transform_batch)\n\n# View the updated schema with the new column\n# .materialize() will execute all the lazy transformations and\n# materialize the dataset into object store memory\nprint(transformed_ds.materialize())\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Agent Settings in RLlib\nDESCRIPTION: Demonstrates configuration of multi-agent reinforcement learning scenarios using the multi_agent method of AlgorithmConfig.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/algorithm-config.rst#2025-04-12_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nconfig.multi_agent(policies={\"policy1\": PolicySpec()})\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate with FSDP Plugin\nDESCRIPTION: Example showing how to configure Hugging Face Accelerate with PyTorch's Fully Sharded Data Parallel (FSDP) for distributed training. Creates a FullyShardedDataParallelPlugin and passes it to the Accelerator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/huggingface-accelerate.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\nfrom accelerate import Accelerator, FullyShardedDataParallelPlugin\n\ndef train_func():\n    fsdp_plugin = FullyShardedDataParallelPlugin(\n        state_dict_config=FullStateDictConfig(\n            offload_to_cpu=False,\n            rank0_only=False\n        ),\n        optim_state_dict_config=FullOptimStateDictConfig(\n            offload_to_cpu=False,\n            rank0_only=False\n        )\n    )\n\n    # Initialize accelerator\n    accelerator = Accelerator(\n        ...,\n        fsdp_plugin=fsdp_plugin,\n    )\n\n    # Start training\n    ...\n\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(...),\n    run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n    ...\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Chatbot WebSocket Implementation\nDESCRIPTION: WebSocket-based chatbot implementation for bidirectional streaming communication.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment\nclass Chatbot:\n    async def handle_request(self, ws: WebSocket) -> None:\n        await ws.accept()\n        conversation = \"\"\n        try:\n            while True:\n                prompt = await ws.receive_text()\n                conversation += f\"Human: {prompt}\\n\"\n                streamer = TextIteratorStreamer(self.tokenizer)\n                self.loop.run_in_executor(\n                    None, self.generate_text, conversation, streamer\n                )\n                response = \"\"\n                for token in await self.consume_streamer(streamer):\n                    await ws.send_text(token)\n                    response += token\n                await ws.send_text(\"<<Response Finished>>\")\n                conversation += f\"Assistant: {response}\\n\"\n        except WebSocketDisconnect:\n            return\n```\n\n----------------------------------------\n\nTITLE: Configuring FSDP Strategy for Distributed LLM Training\nDESCRIPTION: Sets up the Fully Sharded Data Parallel (FSDP) strategy to distribute the large language model across multiple GPUs. This includes defining the model sharding policy to wrap each transformer layer and configuring prefetching to optimize performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport lightning.pytorch as pl \n\nfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\nfrom torch.distributed.fsdp import ShardingStrategy, BackwardPrefetch\nfrom transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXLayer\n\nfrom ray.train.lightning import RayFSDPStrategy\n\n\n# Define the model sharding policy:\n# Wrap every GPTNeoXLayer as its own FSDP instance\nauto_wrap_policy = functools.partial(\n    transformer_auto_wrap_policy,\n    transformer_layer_cls = {GPTNeoXLayer}\n)\n\nfsdp_strategy = RayFSDPStrategy(\n    sharding_strategy=ShardingStrategy.FULL_SHARD,\n    backward_prefetch=BackwardPrefetch.BACKWARD_PRE,\n    forward_prefetch=True,\n    auto_wrap_policy=auto_wrap_policy,\n    limit_all_gathers=True,\n    activation_checkpointing=[GPTNeoXLayer],\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Ray Serve Deployment\nDESCRIPTION: This code snippet demonstrates how to define a simple Ray Serve deployment using the `@serve.deployment` decorator on a Python class.  The deployment class `MyDeployment` has an `__init__` method that takes a name argument and a `__call__` method to handle requests. The `bind` method is used to create an application instance with the specified arguments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/key-concepts.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example showing a simple deployment.\n\"\"\"\nimport ray\nfrom ray import serve\n\n@serve.deployment\nclass MyDeployment:\n    def __init__(self, name: str):\n        self.name = name\n\n    def __call__(self, request: str) -> str:\n        return f\"{self.name} received request: {request}\"\n\n\n# Example instantiation:\n# my_deployment = MyDeployment.bind(name=\"hi\")\n```\n\n----------------------------------------\n\nTITLE: Defining Tokenization Function for Data Preprocessing\nDESCRIPTION: This code defines a function to tokenize input sentences using the Hugging Face tokenizer, handling both single and paired sentence tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom typing import Dict\n\n\n# Tokenize input sentences\ndef collate_fn(examples: Dict[str, np.array]):\n    sentence1_key, sentence2_key = task_to_keys[task]\n    if sentence2_key is None:\n        outputs = tokenizer(\n            list(examples[sentence1_key]),\n            truncation=True,\n            padding=\"longest\",\n            return_tensors=\"pt\",\n        )\n    else:\n        outputs = tokenizer(\n            list(examples[sentence1_key]),\n            list(examples[sentence2_key]),\n            truncation=True,\n            padding=\"longest\",\n            return_tensors=\"pt\",\n        )\n\n    outputs[\"labels\"] = torch.LongTensor(examples[\"label\"])\n\n    # Move all input tensors to GPU\n    for key, value in outputs.items():\n        outputs[key] = value.cuda()\n\n    return outputs\n```\n\n----------------------------------------\n\nTITLE: Defining DeepSpeed Worker for Llama2 Model\nDESCRIPTION: This code defines a DeepSpeed worker for the Llama2 model. It sets up the model configuration, tokenizer, and inference engine using DeepSpeed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\n    \"meta-llama/Llama-2-70b-chat-hf\",\n    # Replace the path if necessary.\n    cache_dir=os.getenv(\"TRANSFORMERS_CACHE\", None),\n    # Specify your Hugging Face token.\n    token=\"\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Inference with Ray Data LLM and Structural Outputs\nDESCRIPTION: This Python script demonstrates how to perform batch inference using Ray Data LLM with structural outputs in JSON format. It defines a schema, configures a vLLM processor, applies it to a dataset of math problems, and outputs structured responses.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/llm/examples/batch/vllm-with-structural-output.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nimport ray\nfrom ray.data.llm import build_llm_processor, vLLMEngineProcessorConfig\n\n# 1. Construct a guided decoding schema. It can be:\n# choice: List[str]\n# json: str\n# grammar: str\n# See https://docs.vllm.ai/en/latest/getting_started/examples/structured_outputs.html\n# for more details about how to construct the schema. Here we use JSON as an example.\nclass AnswerWithExplain(BaseModel):\n    problem: str\n    answer: int\n    explain: str\n\njson_schema = AnswerWithExplain.model_json_schema()\n\n# 2. construct a vLLM processor config.\nprocessor_config = vLLMEngineProcessorConfig(\n    # The base model.\n    model_source=\"unsloth/Llama-3.2-1B-Instruct\",\n    # vLLM engine config.\n    engine_kwargs=dict(\n        # Specify the guided decoding library to use. The default is \"xgrammar\".\n        # See https://docs.vllm.ai/en/latest/serving/engine_args.html\n        # for other available libraries.\n        guided_decoding_backend=\"xgrammar\",\n        # Older GPUs (e.g. T4) don't support bfloat16. You should remove\n        # this line if you're using later GPUs.\n        dtype=\"half\",\n        # Reduce the model length to fit small GPUs. You should remove\n        # this line if you're using large GPUs.\n        max_model_len=1024,\n    ),\n    # The batch size used in Ray Data.\n    batch_size=16,\n    # Use one GPU in this example.\n    concurrency=1,\n)\n\n# 3. construct a processor using the processor config.\nprocessor = build_llm_processor(\n    processor_config,\n    # Convert the input data to the OpenAI chat form.\n    preprocess=lambda row: dict(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a math teacher. Give the answer to \"\n                \"the equation and explain it. Output the problem, answer and \"\n                \"explanation in JSON\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"3 * {row['id']} + 5 = ?\",\n            },\n        ],\n        sampling_params=dict(\n            temperature=0.3,\n            max_tokens=150,\n            detokenize=False,\n            # Specify the guided decoding schema.\n            guided_decoding=dict(json=json_schema),\n        ),\n    ),\n    # Only keep the generated text in the output dataset.\n    postprocess=lambda row: {\n        \"resp\": row[\"generated_text\"],\n    },\n)\n\n# 4. Synthesize a dataset with 30 rows.\n# Each row has a single column \"id\" ranging from 0 to 29.\nds = ray.data.range(30)\n# 5. Apply the processor to the dataset. Note that this line won't kick off\n# anything because processor is execution lazily.\nds = processor(ds)\n# Materialization kicks off the pipeline execution.\nds = ds.materialize()\n\n# 6. Print all outputs.\n# Example output:\n# {\n#     \"problem\": \"3 * 6 + 5 = ?\",\n#     \"answer\": 23,\n#     \"explain\": \"To solve this equation, we need to follow the order of\n#       operations (PEMDAS): Parentheses, Exponents, Multiplication and Division,\n#       and Addition and Subtraction. In this case, we first multiply 3 and 6,\n#       which equals 18. Then we add 5 to 18, which equals 23.\"\n# }\nfor out in ds.take_all():\n    print(out[\"resp\"])\n    print(\"==========\")\n\n# 7. Shutdown Ray to release resources.\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Defining Search Spaces with Ray Tune\nDESCRIPTION: This example demonstrates how to define a comprehensive search space configuration for Ray Tune, including various distribution types such as uniform, normal, integer, and categorical distributions with different sampling methods.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/search_space.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    # Sample a float uniformly between -5.0 and -1.0\n    \"uniform\": tune.uniform(-5, -1),\n\n    # Sample a float uniformly between 3.2 and 5.4,\n    # rounding to multiples of 0.2\n    \"quniform\": tune.quniform(3.2, 5.4, 0.2),\n\n    # Sample a float uniformly between 0.0001 and 0.01, while\n    # sampling in log space\n    \"loguniform\": tune.loguniform(1e-4, 1e-2),\n\n    # Sample a float uniformly between 0.0001 and 0.1, while\n    # sampling in log space and rounding to multiples of 0.00005\n    \"qloguniform\": tune.qloguniform(1e-4, 1e-1, 5e-5),\n\n    # Sample a random float from a normal distribution with\n    # mean=10 and sd=2\n    \"randn\": tune.randn(10, 2),\n\n    # Sample a random float from a normal distribution with\n    # mean=10 and sd=2, rounding to multiples of 0.2\n    \"qrandn\": tune.qrandn(10, 2, 0.2),\n\n    # Sample a integer uniformly between -9 (inclusive) and 15 (exclusive)\n    \"randint\": tune.randint(-9, 15),\n\n    # Sample a random uniformly between -21 (inclusive) and 12 (inclusive (!))\n    # rounding to multiples of 3 (includes 12)\n    # if q is 1, then randint is called instead with the upper bound exclusive\n    \"qrandint\": tune.qrandint(-21, 12, 3),\n\n    # Sample a integer uniformly between 1 (inclusive) and 10 (exclusive),\n    # while sampling in log space\n    \"lograndint\": tune.lograndint(1, 10),\n\n    # Sample a integer uniformly between 1 (inclusive) and 10 (inclusive (!)),\n    # while sampling in log space and rounding to multiples of 2\n    # if q is 1, then lograndint is called instead with the upper bound exclusive\n    \"qlograndint\": tune.qlograndint(1, 10, 2),\n\n    # Sample an option uniformly from the specified choices\n    \"choice\": tune.choice([\"a\", \"b\", \"c\"]),\n\n    # Sample from a random function, in this case one that\n    # depends on another value from the search space\n    \"func\": tune.sample_from(lambda spec: spec.config.uniform * 0.01),\n\n    # Do a grid search over these values. Every value will be sampled\n    # ``num_samples`` times (``num_samples`` is the parameter you pass to ``tune.TuneConfig``,\n    # which is taken in by ``Tuner``)\n    \"grid\": tune.grid_search([32, 64, 128])\n}\n```\n\n----------------------------------------\n\nTITLE: Converting TensorFlow Dataset to Ray Dataset\nDESCRIPTION: This snippet shows how to convert a TensorFlow dataset (CIFAR10) to a Ray Dataset using `ray.data.from_tf`. It imports necessary libraries, loads the CIFAR10 dataset using `tensorflow_datasets`, and then converts it into a Ray Dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport tensorflow_datasets as tfds\n\ntf_ds, _ = tfds.load(\"cifar10\", split=[\"train\", \"test\"])\nds = ray.data.from_tf(tf_ds)\n\nprint(ds)\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Event Synchronization with Ray Actors in Python\nDESCRIPTION: This code snippet demonstrates how to create a distributed Event using a Ray actor. It allows multiple tasks to wait on a condition and be notified when the condition is met. The example includes creating an Event actor, waiting on the event in multiple tasks, and setting the event to release waiting tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/actor-sync.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\n@ray.remote\nclass Event:\n    def __init__(self):\n        self._is_set = False\n        self._waiters = []\n\n    def wait(self):\n        if self._is_set:\n            return\n        promise = ray.promise()\n        self._waiters.append(promise)\n        return promise.get()\n\n    def set(self):\n        self._is_set = True\n        for promise in self._waiters:\n            promise.set()\n\n@ray.remote\ndef wait_and_print(event, msg):\n    print(f\"waiting on event: {msg}\")\n    ray.get(event.wait.remote())\n    print(f\"event set: {msg}\")\n\nray.init()\n\nevent = Event.remote()\n\n# Start 3 tasks that wait on the event\nray.get([\n    wait_and_print.remote(event, \"one\"),\n    wait_and_print.remote(event, \"two\"),\n    wait_and_print.remote(event, \"three\"),\n])\n\n# Sleep for a bit and then set the event\ntime.sleep(3)\nevent.set.remote()\n\ntime.sleep(1)  # Wait for a moment to let all tasks finish\n\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Placement Group Functions in Python\nDESCRIPTION: This snippet lists the import paths for Ray's placement group related functions and classes. It includes functions for creating, getting, and removing placement groups, as well as accessing the placement group table.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/scheduling.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nray.util.placement_group\nray.util.placement_group.get_placement_group\nray.util.placement_group.PlacementGroup\nray.util.placement_group_table\nray.util.remove_placement_group\nray.util.get_current_placement_group\n```\n\n----------------------------------------\n\nTITLE: Import MLflow and Ray Tune\nDESCRIPTION: Imports necessary libraries for MLflow integration with Ray Tune, including os, tempfile, time, mlflow, ray, and specific modules for MLflow logging and setup within Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-mlflow.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\nimport time\n\nimport mlflow\n\nfrom ray import tune\nfrom ray.air.integrations.mlflow import MLflowLoggerCallback, setup_mlflow\n```\n\n----------------------------------------\n\nTITLE: Preparing PyTorch Model for Distributed Training in Ray Train\nDESCRIPTION: Sets up the model for distributed training by wrapping it with Ray Train's prepare_model function, which handles device placement and gradient synchronization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n    model = train.torch.prepare_model(model)\n```\n\n----------------------------------------\n\nTITLE: Running Tune with Early Stopping using ASHAScheduler\nDESCRIPTION: Configures Ray Tune to use the ASHAScheduler for early stopping of underperforming trials. This allows for more efficient hyperparameter optimization by terminating poor trials early.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsearch_space = {\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"momentum\": tune.uniform(0.1, 0.9),\n    \"batch_size\": tune.choice([2, 4, 8, 16])\n}\n\nsched = ASHAScheduler(metric=\"loss\", mode=\"min\")\n\ntune_config = tune.TuneConfig(\n    scheduler=sched,\n    num_samples=5\n)\n\ntuner = tune.Tuner(\n    tune.with_resources(\n        tune.with_parameters(train_cifar),\n        resources={\"cpu\": 2, \"gpu\": 0}\n    ),\n    tune_config=tune_config,\n    param_space=search_space,\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Timing Code Blocks with MetricsLogger in Python\nDESCRIPTION: Demonstrates how to use MetricsLogger for timing code blocks using the context manager API. The example calculates the time taken for code execution and logs it with exponential mean averaging (EMA).\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom ray.rllib.utils.metrics.metrics_logger import MetricsLogger\n\nlogger = MetricsLogger()\n\n# First delta measurement:\nwith logger.log_time(\"my_block_to_be_timed\", reduce=\"mean\", ema_coeff=0.1):\n    time.sleep(1.0)\n\n# EMA should be ~1sec.\nassert 1.1 > logger.peek(\"my_block_to_be_timed\") > 0.9\n\n# Second delta measurement:\nwith logger.log_time(\"my_block_to_be_timed\"):\n    time.sleep(2.0)\n\n# EMA should be ~1.1sec.\nassert 1.15 > logger.peek(\"my_block_to_be_timed\") > 1.05\n```\n\n----------------------------------------\n\nTITLE: Defining the evaluation function\nDESCRIPTION: This snippet defines a simple evaluation function that simulates a long-running ML experiment. It takes the step, width, and height as inputs, sleeps for 0.1 seconds, and returns a score based on these parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(step, width, height):\n    time.sleep(0.1)\n    return (0.1 + width * step / 100) ** (-1) + height * 0.1\n\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running TorchTrainer for Model Fine-Tuning\nDESCRIPTION: This snippet creates a TorchTrainer instance with the configured training loop and settings, then executes the training process using the fit() method. The result contains the training outcome and final checkpoint information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_loop_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Training Function for MNIST ConvNet using Ray Tune\nDESCRIPTION: This code defines a training function for a convolutional neural network (ConvNet) for the MNIST dataset. It includes logic for setting up data loaders, the model, and optimizer, as well as state checkpointing to support the PBT process. The function reports mean accuracy metrics at regular intervals for tuning purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nimport torch\nimport torch.optim as optim\n\nimport ray\nfrom ray import tune\nfrom ray.tune.examples.mnist_pytorch import ConvNet, get_data_loaders, test_func\nfrom ray.tune.schedulers import PopulationBasedTraining\n\ndef train_convnet(config):\n    # Create our data loaders, model, and optmizer.\n    step = 1\n    train_loader, test_loader = get_data_loaders()\n    model = ConvNet()\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=config.get(\"lr\", 0.01),\n        momentum=config.get(\"momentum\", 0.9),\n    )\n\n    # Myabe resume from a checkpoint.\n    checkpoint = tune.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            checkpoint_dict = torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n\n        # Load model state and iteration step from checkpoint.\n        model.load_state_dict(checkpoint_dict[\"model_state_dict\"])\n        # Load optimizer state (needed since we're using momentum),\n        # then set the `lr` and `momentum` according to the config.\n        optimizer.load_state_dict(checkpoint_dict[\"optimizer_state_dict\"])\n        for param_group in optimizer.param_groups:\n            if \"lr\" in config:\n                param_group[\"lr\"] = config[\"lr\"]\n            if \"momentum\" in config:\n                param_group[\"momentum\"] = config[\"momentum\"]\n\n        # Note: Make sure to increment the checkpointed step by 1 to get the current step.\n        last_step = checkpoint_dict[\"step\"]\n        step = last_step + 1\n\n    while True:\n        ray.tune.examples.mnist_pytorch.train_func(model, optimizer, train_loader)\n        acc = test_func(model, test_loader)\n        metrics = {\"mean_accuracy\": acc, \"lr\": config[\"lr\"]}\n\n        # Every `checkpoint_interval` steps, checkpoint our current state.\n        if step % config[\"checkpoint_interval\"] == 0:\n            with tempfile.TemporaryDirectory() as tmpdir:\n                torch.save(\n                    {\n                        \"step\": step,\n                        \"model_state_dict\": model.state_dict(),\n                        \"optimizer_state_dict\": optimizer.state_dict(),\n                    },\n                    os.path.join(tmpdir, \"checkpoint.pt\"),\n                )\n                tune.report(metrics, checkpoint=tune.Checkpoint.from_directory(tmpdir))\n        else:\n            tune.report(metrics)\n\n        step += 1\n```\n\n----------------------------------------\n\nTITLE: Better Approach #1: Using Ray Object Store for Large Objects\nDESCRIPTION: This snippet shows a better approach where the large object is stored in the Ray object store using ray.put(), and its reference is passed to the remote function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/closure-capture-large-objects.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport numpy as np\n\nray.init()\n\n# Large object (e.g., large numpy array)\nlarge_data = np.random.random((10000, 10000))\n\n# Put the large object in object store\nlarge_data_ref = ray.put(large_data)\n\n@ray.remote\ndef f(large_data_ref):\n    return ray.get(large_data_ref)[0]\n\nfuture = f.remote(large_data_ref)\nray.get(future)\n```\n\n----------------------------------------\n\nTITLE: Starting the Tuning Process with Ray Tune\nDESCRIPTION: This snippet initializes Ray, creates a Tuner instance with the defined training function, and sets up configurations for stopping conditions and checkpointing. It ultimately begins the tuning process aimed at optimizing the neural network's performance based on specified metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif ray.is_initialized():\n    ray.shutdown()\nray.init()\n\ntuner = tune.Tuner(\n    train_convnet,\n    run_config=tune.RunConfig(\n        name=\"pbt_test\",\n        # Stop when we've reached a threshold accuracy, or a maximum\n        # training_iteration, whichever comes first\n        stop={\"mean_accuracy\": 0.96, \"training_iteration\": 50},\n        checkpoint_config=tune.CheckpointConfig(\n            checkpoint_score_attribute=\"mean_accuracy\",\n            num_to_keep=4,\n        ),\n        storage_path=\"/tmp/ray_results\",\n    ),\n    tune_config=tune.TuneConfig(\n        scheduler=scheduler,\n        num_samples=4,\n    ),\n    param_space={\n        \"lr\": tune.uniform(0.001, 1),\n        \"momentum\": tune.uniform(0.001, 1),\n        \"checkpoint_interval\": perturbation_interval,\n    },\n)\n\nresults_grid = tuner.fit()\n\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning a Text Classifier with Hugging Face Transformers and Ray Train\nDESCRIPTION: This comprehensive example demonstrates distributed training of a text classifier using Ray Train and Hugging Face Transformers on the Yelp review dataset. The code imports necessary libraries, configures the dataset and model, sets up the training function with a HuggingFace Trainer, and executes distributed training using Ray's TorchTrainer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/transformers_torch_trainer_basic.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# File: /../../python/ray/train/examples/transformers/transformers_torch_trainer_basic.py\n```\n\n----------------------------------------\n\nTITLE: Custom Checkpoint Saving with Hugging Face Transformers\nDESCRIPTION: Shows how to implement a custom callback for Hugging Face Transformers to save checkpoints with Ray Train. This example collects the latest metrics and reports on checkpoint save.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CustomCallback(TrainerCallback):\n    def on_save(self, args, state, control, **kwargs):\n        if state.is_world_process_zero:\n            metrics = state.metrics.copy()\n            with tempfile.TemporaryDirectory() as tmpdir:\n                state.save_to_json(os.path.join(tmpdir, \"trainer_state.json\"))\n                checkpoint = train.Checkpoint.from_directory(tmpdir)\n                train.report(metrics=metrics, checkpoint=checkpoint)\n\ndef train_func():\n    # ... other setup code ...\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=16,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_steps=500,\n        save_total_limit=3,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n        callbacks=[CustomCallback()],\n    )\n\n    trainer.train()\n\ntrainer = train.torch.TorchTrainer(\n    train_func,\n    # ... other TorchTrainer args ...\n)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Loading FashionMNIST Dataset with PyTorch\nDESCRIPTION: This code snippet demonstrates how to download and prepare the FashionMNIST dataset using PyTorch's datasets module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Manual Actor Checkpointing in Python\nDESCRIPTION: This code demonstrates how to implement manual checkpointing for actors in Ray. It shows creating an actor, saving its state, and recovering from a checkpoint upon failure.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/actors.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# __actor_checkpointing_manual_restart_begin__\n# __actor_checkpointing_manual_restart_end__\n```\n\n----------------------------------------\n\nTITLE: Initializing Hugging Face Tokenizer\nDESCRIPTION: This code initializes a tokenizer from the Hugging Face Transformers library based on the specified model checkpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n```\n\n----------------------------------------\n\nTITLE: Sequential Execution of Many Tiny Tasks\nDESCRIPTION: Demonstrates sequential execution of 100,000 very small tasks (0.1ms each) in Python. This establishes a baseline for comparing with Ray's parallelization of tiny tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\ndef tiny_work(x):\n    time.sleep(0.0001) # Replace this with work you need to do.\n    return x\n\nstart = time.time()\nresults = [tiny_work(x) for x in range(100000)]\nprint(\"duration =\", time.time() - start)\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoostTrainer for Multi-GPU Training\nDESCRIPTION: Sets up an XGBoostTrainer for 2 workers with GPU acceleration, including XGBoost-specific parameters for GPU-based training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntrainer = XGBoostTrainer(\n    scaling_config=ScalingConfig(\n        # Number of workers to use for data parallelism.\n        num_workers=2,\n        # Whether to use GPU acceleration.\n        use_gpu=True,\n    ),\n    params={\n        # XGBoost specific params\n        \"tree_method\": \"gpu_hist\",\n        \"eval_metric\": [\"logloss\", \"error\"],\n    },\n    ...\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Experiment-level Analysis with ResultGrid\nDESCRIPTION: Shows how to check for errors, access individual results, and retrieve metrics for all trials using the ResultGrid object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune_analyze_results.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Check if there have been errors\nif result_grid.errors:\n    print(\"One of the trials failed!\")\nelse:\n    print(\"No errors!\")\n\nnum_results = len(result_grid)\nprint(\"Number of results:\", num_results)\n\n# Iterate over results\nfor i, result in enumerate(result_grid):\n    if result.error:\n        print(f\"Trial #{i} had an error:\", result.error)\n        continue\n\n    print(\n        f\"Trial #{i} finished successfully with a mean accuracy metric of:\",\n        result.metrics[\"mean_accuracy\"]\n    )\n\nresults_df = result_grid.get_dataframe()\nresults_df[[\"training_iteration\", \"mean_accuracy\"]]\n\nprint(\"Shortest training time:\", results_df[\"time_total_s\"].min())\nprint(\"Longest training time:\", results_df[\"time_total_s\"].max())\n\nbest_result_df = result_grid.get_dataframe(\n    filter_metric=\"mean_accuracy\", filter_mode=\"max\"\n)\nbest_result_df[[\"training_iteration\", \"mean_accuracy\"]]\n```\n\n----------------------------------------\n\nTITLE: HuggingFace Batch Inference Implementation\nDESCRIPTION: Implementation of batch inference using HuggingFace GPT-2 model. Creates a Dataset from numpy arrays and defines a Predictor class for text generation inference.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/batch_inference.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport ray\n\nds = ray.data.from_numpy(np.asarray([\"Complete this\", \"for me\"]))\n\nclass HuggingFacePredictor:\n    def __init__(self):\n        from transformers import pipeline\n        self.model = pipeline(\"text-generation\", model=\"gpt2\")\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\n        predictions = self.model(list(batch[\"data\"]), max_length=20, num_return_sequences=1)\n        batch[\"output\"] = [sequences[0][\"generated_text\"] for sequences in predictions]\n        return batch\n\npredictions = ds.map_batches(HuggingFacePredictor, concurrency=2)\npredictions.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Defining Training Function with Ray Train and PyTorch Lightning\nDESCRIPTION: This code snippet defines a training function that sets up the PyTorch Lightning Trainer with Ray-specific configurations. It uses Ray Train to handle distributed training and Ray Data for data loading.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_cola_advanced.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray.train\nfrom ray.train.lightning import (\n    prepare_trainer,\n    RayDDPStrategy,\n    RayLightningEnvironment,\n    RayTrainReportCallback,\n)\n\ntrain_func_config = {\n    \"lr\": 1e-5,\n    \"eps\": 1e-8,\n    \"batch_size\": 16,\n    \"max_epochs\": 5,\n}\n\ndef train_func(config):\n    # Unpack the input configs passed from `TorchTrainer(train_loop_config)`\n    lr = config[\"lr\"]\n    eps = config[\"eps\"]\n    batch_size = config[\"batch_size\"]\n    max_epochs = config[\"max_epochs\"]\n\n    # Fetch the Dataset shards\n    train_ds = ray.train.get_dataset_shard(\"train\")\n    val_ds = ray.train.get_dataset_shard(\"validation\")\n\n    # Create a dataloader for Ray Datasets\n    train_ds_loader = train_ds.iter_torch_batches(batch_size=batch_size)\n    val_ds_loader = val_ds.iter_torch_batches(batch_size=batch_size)\n\n    # Model\n    model = SentimentModel(lr=lr, eps=eps)\n\n    trainer = pl.Trainer(\n        max_epochs=max_epochs,\n        accelerator=\"auto\",\n        devices=\"auto\",\n        strategy=RayDDPStrategy(),\n        plugins=[RayLightningEnvironment()],\n        callbacks=[RayTrainReportCallback()],\n        enable_progress_bar=False,\n    )\n\n    trainer = prepare_trainer(trainer)\n\n    trainer.fit(model, train_dataloaders=train_ds_loader, val_dataloaders=val_ds_loader)\n```\n\n----------------------------------------\n\nTITLE: Flappy Bird DreamerV3 Implementation - Python\nDESCRIPTION: Complete implementation example showing how to set up and run DreamerV3 on a custom Flappy Bird environment, including environment registration and configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamerv3/README.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.rllib.algorithms.dreamerv3.dreamerv3 import DreamerV3Config\n\ndef _env_creator(ctx):\n    import flappy_bird_gymnasium\n    import gymnasium as gym\n    from supersuit.generic_wrappers import resize_v1\n    from ray.rllib.algorithms.dreamerv3.utils.env_runner import NormalizedImageEnv\n\n    return NormalizedImageEnv(\n        resize_v1(\n            gym.make(\"FlappyBird-rgb-v0\", audio_on=False), x_size=64, y_size=64\n        )\n    )\n\ntune.register_env(\"flappy-bird\", _env_creator)\n\nconfig = (\n    DreamerV3Config()\n    .environment(\"flappy-bird\")\n    .training(\n        model_size=\"S\",\n        training_ratio=1024,\n    )\n)\n\nresults = tune.Tuner(trainable=\"DreamerV3\", param_space=config).fit()\n```\n\n----------------------------------------\n\nTITLE: Configuring Trial-level Fault Tolerance in Tune\nDESCRIPTION: Example showing how to configure trial-level fault tolerance using FailureConfig to handle individual trial failures due to preemptible instances, network issues, or resource constraints.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-fault-tolerance.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Configure fault tolerance at trial level\nfailure_config = tune.FailureConfig(\n    max_failures=3,  # Allow each trial to fail up to 3 times\n)\n\n# Create the Tuner with the failure_config\ntuner = tune.Tuner(\n    trainable,\n    param_space=param_space,\n    run_config=RunConfig(\n        name=\"tune_fault_tolerance_guide\",\n        storage_path=\"~/ray_results\",\n        failure_config=failure_config,\n    ),\n)\n\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Runtime Environment for LLM Fine-tuning\nDESCRIPTION: Sets up Ray with a runtime environment that installs necessary Python libraries on each node. This is required for distributed fine-tuning of the dolly-v2-7b model using PyTorch Lightning and FSDP.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(\n    runtime_env={\n        \"pip\": [\n            \"datasets\",\n            \"evaluate\",\n            \"transformers>=4.26.0\",\n            \"torch>=1.12.0\",\n            \"lightning>=2.0\",\n        ]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing GPU-to-GPU Communication with NCCL in Ray Compiled Graphs\nDESCRIPTION: This code snippet shows how to use the with_tensor_transport API hint to enable GPU-to-GPU communication with NCCL in Ray Compiled Graphs. It creates a DAG for sending a tensor from a sender to a receiver using NCCL-based transfer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/quickstart.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef create_and_execute_dag():\n    with InputNode() as inputs:\n        send_node = sender.send.bind()\n        with with_tensor_transport():\n            receive_node = receiver.receive.bind(send_node)\n        output = OutputNode(receive_node)\n\n    dag = DAG(inputs)\n    compiled = dag.compile()\n    results = compiled.execute()\n    return results\n\nresults = ray.get(create_and_execute_dag.remote())\n```\n\n----------------------------------------\n\nTITLE: Serve Configuration File\nDESCRIPTION: This YAML file configures a Ray Serve application named 'default' with a route prefix '/'. It defines two deployments: 'Translator' and 'Summarizer', each with one replica. It specifies the `import_path` to the application code and includes `torch` and `transformers` in the runtime environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/index.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nproxy_location: EveryNode\n\nhttp_options:\n  host: 0.0.0.0\n  port: 8000\n\napplications:\n- name: default\n  route_prefix: /\n  import_path: text_ml:app\n  runtime_env:\n    pip:\n      - torch\n      - transformers\n  deployments:\n  - name: Translator\n    num_replicas: 1\n    user_config:\n      language: french\n  - name: Summarizer\n    num_replicas: 1\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running TorchTrainer with Ray Train\nDESCRIPTION: This code snippet shows how to initialize a TorchTrainer with the optimized train_func and configuration. It sets up distributed training with 2 workers and specifies learning rate, batch size, and number of epochs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func,\n    train_loop_config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n    scaling_config=ScalingConfig(num_workers=2, use_gpu=False),\n)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Quickstart Example for Ray Tune in Python\nDESCRIPTION: A simple example that demonstrates how to use Ray Tune to minimize a function f(x) = a**2 + b by defining an objective function, a search space, and starting a Tune run.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef objective(config):\n    a = config[\"a\"]\n    b = config[\"b\"]\n    score = a**2 + b\n    return {\"score\": score}\n\nsearch_space = {\"a\": tune.uniform(-5, 5), \"b\": tune.uniform(0, 5)}\n\nresult = tune.run(\n    objective,\n    config=search_space,\n    metric=\"score\",\n    mode=\"min\"\n)\n\nprint(\"Best hyperparameters found were:\", result.best_config)\n```\n\n----------------------------------------\n\nTITLE: Streaming Execution Pipeline in Ray Data (Python)\nDESCRIPTION: This example illustrates the streaming execution model in Ray Data. It creates a dataset from a CSV file, applies multiple map transformations, filters the data, and then triggers execution with the show() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/key-concepts.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Create a dataset with 1K rows\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\n# Define a pipeline of operations\nds = ds.map(lambda x: {\"target1\": x[\"target\"] * 2})\nds = ds.map(lambda x: {\"target2\": x[\"target1\"] * 2})\nds = ds.map(lambda x: {\"target3\": x[\"target2\"] * 2})\nds = ds.filter(lambda x: x[\"target3\"] % 4 == 0)\n\n# Data starts flowing when you call a method like show()\nds.show(5)\n```\n\n----------------------------------------\n\nTITLE: Converting Ray Data to TensorFlow Dataset\nDESCRIPTION: Shows how to convert Ray Data shards into TensorFlow datasets for distributed training. Includes helper function to build TensorFlow dataset and disable automatic sharding.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/distributed-tensorflow-keras.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nfrom ray import train\nfrom ray.train.tensorflow import prepare_dataset_shard\n\ndef train_func(config: dict):\n    # ...\n\n    # Get dataset shard from Ray Train\n    dataset_shard = train.get_context().get_dataset_shard(\"train\")\n\n    # Define a helper function to build a TensorFlow dataset\n    def to_tf_dataset(dataset, batch_size):\n        def to_tensor_iterator():\n            for batch in dataset.iter_tf_batches(\n                batch_size=batch_size, dtypes=tf.float32\n            ):\n                yield batch[\"image\"], batch[\"label\"]\n\n        output_signature = (\n            tf.TensorSpec(shape=(None, 784), dtype=tf.float32),\n            tf.TensorSpec(shape=(None, 784), dtype=tf.float32),\n        )\n        tf_dataset = tf.data.Dataset.from_generator(\n            to_tensor_iterator, output_signature=output_signature\n        )\n        # Call prepare_dataset_shard to disable automatic sharding\n        # (since the dataset is already sharded)\n        return prepare_dataset_shard(tf_dataset)\n\n    for epoch in range(epochs):\n        # Call our helper function to build the dataset\n        tf_dataset = to_tf_dataset(\n            dataset=dataset_shard,\n            batch_size=64,\n        )\n        history = multi_worker_model.fit(tf_dataset)\n```\n\n----------------------------------------\n\nTITLE: Standard Hugging Face Transformers Training Implementation\nDESCRIPTION: A standalone Hugging Face Transformers implementation for text classification using the BERT model on the Yelp Review dataset, without Ray integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-transformers.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Adapted from Hugging Face tutorial: https://huggingface.co/docs/transformers/training\n\nimport numpy as np\nimport evaluate\nfrom datasets import load_dataset\nfrom transformers import (\n    Trainer,\n    TrainingArguments,\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n)\n\n# Datasets\ndataset = load_dataset(\"yelp_review_full\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\nsmall_train_dataset = dataset[\"train\"].select(range(100)).map(tokenize_function, batched=True)\nsmall_eval_dataset = dataset[\"test\"].select(range(100)).map(tokenize_function, batched=True)\n\n# Model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-cased\", num_labels=5\n)\n\n# Metrics\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Hugging Face Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Start Training\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Deploying vLLM LLM with RayService on Kubernetes\nDESCRIPTION: Applies the RayService custom resource to deploy a Ray Serve application that uses vLLM to serve the Llama 3 8B Instruct model. This creates the necessary resources in the Kubernetes cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/vllm-rayservice.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/vllm/ray-service.vllm.yaml\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-blocking Task Processing\nDESCRIPTION: Advanced implementation using ray.wait for non-blocking task processing with timeout\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\nobject_references = [\n    retrieve_task.remote(item, db_object_ref) for item in range(8)\n]\nall_data = []\n\nwhile len(object_references) > 0:\n    finished, object_references = ray.wait(\n        object_references, timeout=7.0\n    )\n    data = ray.get(finished)\n    print_runtime(data, start)\n    all_data.extend(data)\n\nprint_runtime(all_data, start)\n```\n\n----------------------------------------\n\nTITLE: Preparing LoRA Fine-tuning Model for HPU in Python\nDESCRIPTION: This function prepares a pre-trained language model for LoRA fine-tuning. It loads a model from a specified source, optionally configures it for DeepSpeed, converts it to a PEFT model with LoRA configuration, and moves it to the appropriate device with the correct data type.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_model(config: Dict, device):\n    # prepare from pretrained model\n    deepspeed = config[\"deepspeed\"] if \"deepspeed\" in config else None\n    if deepspeed is not None:\n        auto_config = transformers.AutoConfig.from_pretrained(config[\"model\"], use_cache=False, revision=\"main\", use_auth_token=None, trust_remote_code=None)\n        model = transformers.AutoModelForCausalLM.from_pretrained(config[\"model\"], config=auto_config, **config[\"model_config\"])\n        model.generation_config.attn_softmax_bf16 = True\n        model.generation_config.use_flash_attention = True\n    else:\n        model = transformers.AutoModelForCausalLM.from_pretrained(config[\"model\"], **config[\"model_config\"])\n    model.enable_input_require_grads()\n\n    # convert to peft model for lora training\n    peft_config = peft.LoraConfig(**config[\"lora_config\"])\n    model = peft.get_peft_model(model, peft_config)\n\n    model.to(dtype=config[\"model_config\"][\"torch_dtype\"], device=device)\n\n    return model\n```\n\n----------------------------------------\n\nTITLE: Implementing TensorFlow Batch Inference with Ray\nDESCRIPTION: Implementation of a TensorFlow predictor class with GPU support using Keras Sequential model. Shows GPU device placement and batch processing configuration using Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/batch_inference.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\n\nimport ray\n\nds = ray.data.from_numpy(np.ones((1, 100)))\n\nclass TFPredictor:\n    def __init__(self):\n        import tensorflow as tf\n        from tensorflow import keras\n\n        # Move the neural network to GPU by specifying the GPU device.\n        with tf.device(\"GPU:0\"):\n            input_layer = keras.Input(shape=(100,))\n            output_layer = keras.layers.Dense(1, activation=\"sigmoid\")\n            self.model = keras.Sequential([input_layer, output_layer])\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        import tensorflow as tf\n\n        # Move the input batch to GPU by specifying GPU device.\n        with tf.device(\"GPU:0\"):\n            return {\"output\": self.model(batch[\"data\"]).numpy()}\n\n# Use 2 actors, each actor using 1 GPU. 2 GPUs total.\npredictions = ds.map_batches(\n    TFPredictor,\n    num_gpus=1,\n    # Specify the batch size for inference.\n    # Increase this for larger datasets.\n    batch_size=1,\n    # Set the concurrency to the number of GPUs in your cluster.\n    concurrency=2,\n)\npredictions.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Stopping Ray Tune Experiment with User-defined Function\nDESCRIPTION: Defines a custom stopping function that checks the number of iterations and mean accuracy. The experiment stops when the function returns True for any trial.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-stopping.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef stop_fn(trial_id, result):\n    return result[\"training_iteration\"] >= 10 or result[\"mean_accuracy\"] >= 0.8\n\ntuner = tune.Tuner(\n    train_function,\n    tune_config=tune.TuneConfig(\n        num_samples=5,\n        stop=stop_fn\n    ),\n    param_space={\"base_accuracy\": 0.3}\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Recursive Ray Task with Debugging Support\nDESCRIPTION: Demonstrates how to debug recursive Ray tasks by stepping between them. This example implements a recursive factorial function as a Ray task and shows how to use the debugger to navigate through the execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/ray-debugging.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(runtime_env={\"env_vars\": {\"RAY_DEBUG\": \"legacy\"}})\n\n@ray.remote\ndef fact(n):\n    if n == 1:\n        return n\n    else:\n        n_ref = fact.remote(n - 1)\n        return n * ray.get(n_ref)\n\n@ray.remote\ndef compute():\n    breakpoint()\n    result_ref = fact.remote(5)\n    result = ray.get(result_ref)\n\nray.get(compute.remote())\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained Stable Diffusion Model in Bash\nDESCRIPTION: This bash script downloads and caches a pre-trained Stable Diffusion model for fine-tuning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/dreambooth_finetuning.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython dreambooth/cache_model.py \\\n    --pretrained_model_name_or_path=\"$PRETRAINED_MODEL\" \\\n    --cache_dir=\"$CACHE_DIR\"\n```\n\n----------------------------------------\n\nTITLE: Using Scheduler API for Value Interpolation in RLlib\nDESCRIPTION: Illustrates creating a PiecewiseSchedule to manage scheduled values across timesteps. Allows dynamic value interpolation and updates based on integer timestep inputs, commonly used for hyperparameter scheduling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/utils.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.utils.schedules.scheduler import Scheduler\n\nscheduler = Scheduler([[0, 0.1], [50, 0.05], [60, 0.001]])\nprint(scheduler.get_current_value())  # <- expect 0.1\n\n# Up the timestep.\nscheduler.update(timestep=45)\nprint(scheduler.get_current_value())  # <- expect 0.055\n\n# Up the timestep.\nscheduler.update(timestep=100)\nprint(scheduler.get_current_value())  # <- expect 0.001 (keep final value)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Applications with Serve Status API in Python\nDESCRIPTION: This Python code snippet demonstrates how to retrieve Serve application details using the `serve.status()` API. It returns the same information as the `serve status` CLI command inside a dataclass. The example highlights how to use this method within a deployment or Ray driver script to get live information about Serve applications on the Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"__monitor_start__\\n# monitoring_app.py\\nimport ray\\nfrom ray import serve\\n\n@serve.deployment\\nclass StatusMonitor:\\n    def __call__(self):\\n        status = serve.status()\\n        running_apps = {\\n            name: app\\n            for name, app in status.applications.items()\\n            if app.status == serve.ApplicationStatus.RUNNING\\n        }\\n        return running_apps\\n\napp = StatusMonitor.bind()\\n\\n__monitor_end__\"\n```\n\n----------------------------------------\n\nTITLE: Using Nvidia GPUs in Ray Tasks and Actors\nDESCRIPTION: This snippet demonstrates how to use Nvidia GPUs in Ray tasks and actors. It initializes Ray with 2 GPUs, defines a GPU actor and task, and shows how Ray assigns GPUs and sets the CUDA_VISIBLE_DEVICES environment variable.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\n\nray.init(num_gpus=2)\n\n@ray.remote(num_gpus=1)\nclass GPUActor:\n    def ping(self):\n        print(\"GPU IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"GPU\"]))\n        print(\"CUDA_VISIBLE_DEVICES: {}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n\n@ray.remote(num_gpus=1)\ndef gpu_task():\n    print(\"GPU IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"GPU\"]))\n    print(\"CUDA_VISIBLE_DEVICES: {}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n\ngpu_actor = GPUActor.remote()\nray.get(gpu_actor.ping.remote())\n# The actor uses the first GPU so the task uses the second one.\nray.get(gpu_task.remote())\n```\n\n----------------------------------------\n\nTITLE: Implementing Tune Training Function with PyTorch Lightning\nDESCRIPTION: This snippet defines a training function that integrates Ray Tune with PyTorch Lightning, allowing for hyperparameter optimization of the MNIST classifier.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef train_mnist_tune(config, num_epochs=10, num_gpus=0, data_dir=\"~/data\"):\n    data_dir = os.path.expanduser(data_dir)\n    model = LightningMNISTClassifier(config, data_dir)\n    trainer = pl.Trainer(\n        max_epochs=num_epochs,\n        # If fractional GPUs passed in, convert to int.\n        gpus=math.ceil(num_gpus),\n        logger=TensorBoardLogger(save_dir=os.getcwd(), name=\"\", version=\".\"),\n        enable_progress_bar=False,\n        callbacks=[\n            TuneReportCallback(\n                {\"loss\": \"ptl/val_loss\", \"mean_accuracy\": \"ptl/val_accuracy\"},\n                on=\"validation_end\",\n            )\n        ],\n    )\n    trainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Getting Spaces from SingleAgentEnvRunner - Python\nDESCRIPTION: This function provides the action and observation spaces of the environment, allowing the agent to understand what actions it can take and what observations it can expect during its interaction.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_env_runner.rst#2025-04-12_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef get_spaces(self):\n    # Get the action and observation spaces\n    pass\n```\n\n----------------------------------------\n\nTITLE: Incorrect Ray Parallelization: Missing ray.get()\nDESCRIPTION: Shows a common mistake when first using Ray where remote function calls are made but the results are not retrieved with ray.get(). This code returns ObjectRefs instead of actual results and doesn't wait for task completion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport ray\n\nray.init(num_cpus=4) # Specify this system has 4 CPUs.\n\n@ray.remote\ndef do_some_work(x):\n    time.sleep(1) # Replace this with work you need to do.\n    return x\n\nstart = time.time()\nresults = [do_some_work.remote(x) for x in range(4)]\nprint(\"duration =\", time.time() - start)\nprint(\"results =\", results)\n```\n\n----------------------------------------\n\nTITLE: Access training results\nDESCRIPTION: Displays the result object which contains metrics and the Ray Train Checkpoint associated with the last iteration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nresult\n```\n\n----------------------------------------\n\nTITLE: Defining Vicuna13BModel with PyTorch Lightning and DeepSpeed\nDESCRIPTION: This snippet defines a PyTorch Lightning module for the Vicuna-13B model, incorporating DeepSpeed optimizations and efficient model initialization techniques. It includes setup for DeepSpeed ZeRO-3 sharding and CPU Adam optimizer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport transformers\nimport lightning.pytorch as pl\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom deepspeed.ops.adam import DeepSpeedCPUAdam\n\n\nclass ZeRO3Config:\n    def __init__(self, pl_module):\n        self.config = pl_module.trainer.strategy.config\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def is_zero3(self) -> bool:\n        return True\n\n\ndef enable_transformers_pretrained_deepspeed_sharding(\n    pl_module: \"pl.LightningModule\",\n) -> None:\n    transformers.deepspeed._hf_deepspeed_config_weak_ref = ZeRO3Config(pl_module)\n\n\nclass Vicuna13BModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Enable tf32 for better performance\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n    def setup(self, stage) -> None:\n        # Defer model initialization to inject deepspeed configs to HF.\n        # During initialization, HF transformers can immediately partition \n        # the model across all gpus avoid the overhead in time and memory \n        # copying it on CPU or each GPU first.\n        enable_transformers_pretrained_deepspeed_sharding(self)\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        if self.global_rank == 0:\n            print(\"DeepSpeed Configs: \", self.trainer.strategy.config)\n            print(\"Model Archetecture: \", self.model)\n\n    def forward(self, batch):\n        outputs = self.model(\n            batch[\"input_ids\"],\n            labels=batch[\"labels\"],\n            attention_mask=batch[\"attention_mask\"],\n        )\n        return outputs.loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self.forward(batch)\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, sync_dist=True)\n        return loss\n\n    def configure_optimizers(self):\n        return DeepSpeedCPUAdam(self.parameters(), lr=2e-5, weight_decay=0.01)\n```\n\n----------------------------------------\n\nTITLE: Processing Streaming LLM Response in Python\nDESCRIPTION: Code snippet showing how to consume a streaming response from an LLM model, outputting each content chunk as it arrives.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Saving Inference Results with Ray Data\nDESCRIPTION: Saves the processed dataset with inference results to local storage in Parquet format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nds.write_parquet(\"local://tmp/inference_results\")\n```\n\n----------------------------------------\n\nTITLE: Specifying Actor Resources\nDESCRIPTION: Examples of specifying CPU and GPU resource requirements for actors across different languages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(num_cpus=2, num_gpus=0.5)\nclass Actor:\n    pass\n```\n\nLANGUAGE: java\nCODE:\n```\nRay.actor(Counter::new).setResource(\"CPU\", 2.0).setResource(\"GPU\", 0.5).remote();\n```\n\nLANGUAGE: c++\nCODE:\n```\nray::Actor(CreateCounter).SetResource(\"CPU\", 2.0).SetResource(\"GPU\", 0.5).Remote();\n```\n\n----------------------------------------\n\nTITLE: Converting to Distributed Multi-Worker Training in PyTorch with Ray Train (Python)\nDESCRIPTION: This code shows how to convert a single-worker PyTorch training function to a distributed multi-worker function using `ray.train.torch.prepare_model` and `ray.train.torch.prepare_data_loader`. The `prepare_model` function wraps the model with `DistributedDataParallel`, and `prepare_data_loader` adds `DistributedSampler` to the DataLoaders.  Placeholders `__torch_distributed_begin__` and `__torch_distributed_end__` define start and end.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/pytorch/torch_quick_start.py\n:language: python\n:start-after: __torch_distributed_begin__\n:end-before: __torch_distributed_end__\n```\"\n```\n\n----------------------------------------\n\nTITLE: Training LightGBM Model with Ray\nDESCRIPTION: Defines a function to train a LightGBM model using Ray. It preprocesses the data, configures the trainer, and returns the training result.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightgbm/lightgbm_example.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_lightgbm(num_workers: int, use_gpu: bool = False) -> Result:\n    train_dataset, valid_dataset, _ = prepare_data()\n\n    # Scale some random columns, and categorify the categorical_column,\n    # allowing LightGBM to use its built-in categorical feature support\n    scaler = StandardScaler(columns=[\"mean radius\", \"mean texture\"])\n    categorizer = Categorizer([\"categorical_column\"])\n\n    train_dataset = categorizer.fit_transform(scaler.fit_transform(train_dataset))\n    valid_dataset = categorizer.transform(scaler.transform(valid_dataset))\n\n    # LightGBM specific params\n    params = {\n        \"objective\": \"binary\",\n        \"metric\": [\"binary_logloss\", \"binary_error\"],\n    }\n\n    trainer = LightGBMTrainer(\n        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n        label_column=\"target\",\n        params=params,\n        datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n        num_boost_round=100,\n        metadata = {\"scaler_pkl\": scaler.serialize(), \"categorizer_pkl\": categorizer.serialize()}\n    )\n    result = trainer.fit()\n    print(result.metrics)\n\n    return result\n```\n\n----------------------------------------\n\nTITLE: Local Deployment Commands for Ray Serve\nDESCRIPTION: Commands for deploying and querying the Stable Diffusion service locally using Ray Serve and FastAPI.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/README.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nserve run app:entrypoint\n```\n\nLANGUAGE: bash\nCODE:\n```\npython query.py\n```\n\n----------------------------------------\n\nTITLE: Text Transformation with Ray Data\nDESCRIPTION: Shows how to transform text data using map operations, implementing a simple lowercase conversion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-text.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\nimport ray\n\ndef to_lower(row: Dict[str, Any]) -> Dict[str, Any]:\n    row[\"text\"] = row[\"text\"].lower()\n    return row\n\nds = (\n    ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\n    .map(to_lower)\n)\n\nds.show(3)\n```\n\n----------------------------------------\n\nTITLE: Checkpointing a Learner in Python\nDESCRIPTION: The snippet covers checkpointing for a single learner, allowing for the preservation and restoration of its neural network weights and optimizer states. Dependencies include the `tempfile` module for creating checkpoint directories, while inputs/outputs revolve around path-based storage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-learner.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlearner.save_to_path(LEARNER_CKPT_DIR)\nlearner.restore_from_path(LEARNER_CKPT_DIR)\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Dataset for Llama Pre-training\nDESCRIPTION: Defines a function to tokenize the loaded dataset using the provided tokenizer. Handles potential warnings for long input sequences and applies tokenization in batches for efficiency.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef tokenize_dataset(datasets, tokenizer):\n    column_names = list(datasets[\"train\"].features)\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n\n    def tokenize_function(examples):\n        with CaptureLogger(tok_logger) as cl:\n            output = tokenizer(examples[text_column_name])\n        # clm input could be much much longer than block_size\n        if \"Token indices sequence length is longer than the\" in cl.out:\n            tok_logger.warning(\n                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n                \" before being passed to the model.\"\n            )\n        return output\n\n    tokenized_datasets = datasets.map(\n        tokenize_function,\n        batched=True,\n        num_proc=None,\n        remove_columns=column_names,\n        load_from_cache_file=True,\n        desc=\"Running tokenizer on dataset\",\n    )\n\n    return tokenized_datasets\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving a Single-Agent PPO Algorithm Checkpoint in Python\nDESCRIPTION: This code snippet demonstrates how to configure a PPO algorithm for a single-agent environment (Pendulum-v1), train it for one iteration, and save its state to a checkpoint directory. The save_to_path() method creates a checkpoint that captures the algorithm's complete state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/checkpoints.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\n# Configure and build an initial algorithm.\nconfig = (\n    PPOConfig()\n    .environment(\"Pendulum-v1\")\n)\nppo = config.build()\n\n# Train for one iteration, then save to a checkpoint.\nprint(ppo.train())\ncheckpoint_dir = ppo.save_to_path()\nprint(f\"saved algo to {checkpoint_dir}\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Best Model After Tuning\nDESCRIPTION: Code to retrieve the best performing model from a Tune experiment and evaluate it on a test dataset. It loads the best checkpoint and computes the test accuracy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nbest_trial = results.get_best_result(\"loss\", \"min\")\nprint(\"Best trial config: {}\".format(best_trial.config))\nprint(\"Best trial final validation loss: {}\".format(\n    best_trial.metrics[\"loss\"]))\nprint(\"Best trial final validation accuracy: {}\".format(\n    best_trial.metrics[\"accuracy\"]))\n\ntest_acc = test_best_model(best_trial)\nprint(\"Best trial test set accuracy: {}\".format(test_acc))\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop with Ray Tune Integration\nDESCRIPTION: Main training function with Ray Tune integration for hyperparameter tuning, including checkpoint handling, metrics reporting, and device management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-cifar.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef train_cifar(config):\n    net = Net(config[\"l1\"], config[\"l2\"])\n    device = config[\"device\"]\n    if device == \"cuda\":\n        net = nn.DataParallel(net)\n    net.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9, weight_decay=5e-5)\n\n    if tune.get_checkpoint():\n        loaded_checkpoint = tune.get_checkpoint()\n        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n            model_state, optimizer_state = torch.load(\n                os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\")\n            )\n            net.load_state_dict(model_state)\n            optimizer.load_state_dict(optimizer_state)\n\n    if config[\"smoke_test\"]:\n        trainset, _ = load_test_data()\n    else:\n        trainset, _ = load_data()\n    train_loader, val_loader = create_dataloaders(\n        trainset, \n        config[\"batch_size\"],\n        num_workers=0 if config[\"smoke_test\"] else 8\n    )\n\n    for epoch in range(config[\"max_num_epochs\"]):\n        net.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        net.eval()\n        val_loss = 0.0\n        correct = total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = net(inputs)\n                val_loss += criterion(outputs, labels).item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n        metrics = {\n            \"loss\": val_loss / len(val_loader),\n            \"accuracy\": correct / total,\n        }\n\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n            torch.save(\n                (net.state_dict(), optimizer.state_dict()), path\n            )\n            checkpoint = tune.Checkpoint.from_directory(temp_checkpoint_dir)\n            tune.report(metrics, checkpoint=checkpoint)\n    print(\"Finished Training!\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Data with Ray Dataset\nDESCRIPTION: Defines a function to load breast cancer data from an S3 bucket and split it into train, validation, and test datasets. The test dataset drops the target column for inference scenarios.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_data() -> Tuple[Dataset, Dataset, Dataset]:\n    \"\"\"Load and split the dataset into train, validation, and test sets.\"\"\"\n    dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n    train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n    test_dataset = valid_dataset.drop_columns([\"target\"])\n    return train_dataset, valid_dataset, test_dataset\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Ray\nDESCRIPTION: This code snippet computes gradients from multiple rollouts in parallel using Ray actors. It puts the model into the Ray object store, then launches tasks on Ray actors to compute gradients.  The gradients are then accumulated and the model updated.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_pong_example.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"for i in range(1, 1 + iterations):\n    model_id = ray.put(model)\n    gradient_ids = []\n    # Launch tasks to compute gradients from multiple rollouts in parallel.\n    start_time = time.time()\n    gradient_ids = [actor.compute_gradient.remote(model_id) for actor in actors]\n    for batch in range(batch_size):\n        [grad_id], gradient_ids = ray.wait(gradient_ids)\n        grad, reward_sum = ray.get(grad_id)\n        # Accumulate the gradient over batch.\n        for k in model.weights:\n            grad_buffer[k] += grad[k]\n        running_reward = (\n            reward_sum\n            if running_reward is None\n            else running_reward * 0.99 + reward_sum * 0.01\n        )\n    end_time = time.time()\n    print(\n        \\\"Batch {} computed {} rollouts in {} seconds, \\\n        running mean is {}\\\".format(\n            i, batch_size, end_time - start_time, running_reward\n        )\n    )\n    model.update(grad_buffer, rmsprop_cache, learning_rate, decay_rate)\n    zero_grads(grad_buffer)\"\n```\n\n----------------------------------------\n\nTITLE: Defining GLUE Tasks and Setting Task Parameters\nDESCRIPTION: This code defines the list of GLUE benchmark tasks and sets parameters for the specific task, model checkpoint, and batch size to use in training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nGLUE_TASKS = [\n    \"cola\",\n    \"mnli\",\n    \"mnli-mm\",\n    \"mrpc\",\n    \"qnli\",\n    \"qqp\",\n    \"rte\",\n    \"sst2\",\n    \"stsb\",\n    \"wnli\",\n]\n\ntask = \"cola\"\nmodel_checkpoint = \"distilbert-base-uncased\"\nbatch_size = 16\n```\n\n----------------------------------------\n\nTITLE: Generating Text with GPT-J Pipeline\nDESCRIPTION: Demonstrates text generation using the configured pipeline with multiple input prompts and sampling parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Generate from prompts!\nfor sentence in pipe(\n    [\"Romeo and Juliet\", \"Romeo\", \"Juliet\"], do_sample=True, min_length=20\n):\n    print(sentence)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing and Training with Ray Data and Ray Train\nDESCRIPTION: This snippet demonstrates how to use Ray Data preprocessors (StandardScaler and Concatenator) with a Ray Train TorchTrainer. It shows data loading, preprocessing, and a basic training loop implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom tempfile import TemporaryDirectory\n\nimport ray\nfrom ray import train\nfrom ray.train import Checkpoint, ScalingConfig\nfrom ray.train.torch import TorchTrainer\nfrom ray.data.preprocessors import Concatenator, StandardScaler\n\ndataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n\n# Create preprocessors to scale some columns and concatenate the results.\nscaler = StandardScaler(columns=[\"mean radius\", \"mean texture\"])\ncolumns_to_concatenate = dataset.columns()\ncolumns_to_concatenate.remove(\"target\")\nconcatenator = Concatenator(columns=columns_to_concatenate, dtype=np.float32)\n\n# Compute dataset statistics and get transformed datasets. Note that the\n# fit call is executed immediately, but the transformation is lazy.\ndataset = scaler.fit_transform(dataset)\ndataset = concatenator.fit_transform(dataset)\n\ndef train_loop_per_worker():\n    context = train.get_context()\n    print(context.get_metadata())  # prints {\"preprocessor_pkl\": ...}\n\n    # Get an iterator to the dataset we passed in below.\n    it = train.get_dataset_shard(\"train\")\n    for _ in range(2):\n        # Prefetch 10 batches at a time.\n        for batch in it.iter_batches(batch_size=128, prefetch_batches=10):\n            print(\"Do some training on batch\", batch)\n\n    # Save a checkpoint.\n    with TemporaryDirectory() as temp_dir:\n        train.report(\n            {\"score\": 2.0},\n            checkpoint=Checkpoint.from_directory(temp_dir),\n        )\n\nmy_trainer = TorchTrainer(\n    train_loop_per_worker,\n    scaling_config=ScalingConfig(num_workers=2),\n    datasets={\"train\": dataset},\n    metadata={\"preprocessor_pkl\": scaler.serialize()},\n)\n\n# Get the fitted preprocessor back from the result metadata.\nmetadata = my_trainer.fit().checkpoint.get_metadata()\nprint(StandardScaler.deserialize(metadata[\"preprocessor_pkl\"]))\n```\n\n----------------------------------------\n\nTITLE: Fault-Tolerant Object Handling in Ray (Python)\nDESCRIPTION: Demonstrates a fault-tolerant way of returning objects from tasks in Ray. By returning the object directly instead of its ObjectRef, it becomes owned by the driver and can be automatically recovered if lost.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault-tolerance.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef a():\n    return 1\n\nx = ray.get(a.remote())\nprint(x)\n```\n\n----------------------------------------\n\nTITLE: Using New TorchTrainer API for Hugging Face Integration in Python\nDESCRIPTION: Example demonstrating the new TorchTrainer API introduced in Ray 2.7 for training Hugging Face Transformers models. This approach offers better transparency, flexibility, and alignment with standard Hugging Face scripts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-transformers.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM\nfrom datasets import load_dataset\n\nimport ray\nfrom ray.train.torch import TorchTrainer\nfrom ray.train.huggingface.transformers import (\n    RayTrainReportCallback,\n    prepare_trainer,\n)\nfrom ray.train import ScalingConfig\n\n\nhf_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n# optional: preprocess the dataset\n# hf_datasets = hf_datasets.map(preprocess, ...)\n\nray_train_ds = ray.data.from_huggingface(hf_datasets[\"train\"])\nray_eval_ds = ray.data.from_huggingface(hf_datasets[\"validation\"])\n\n# [1] Define the full training function\n# =====================================\ndef train_func():\n    MODEL_NAME = \"gpt2\"\n    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n    model = AutoModelForCausalLM.from_config(model_config)\n\n    # [2] Build Ray Data iterables\n    # ============================\n    train_dataset = ray.train.get_dataset_shard(\"train\")\n    eval_dataset = ray.train.get_dataset_shard(\"validation\")\n\n    train_iterable_ds = train_dataset.iter_torch_batches(batch_size=8)\n    eval_iterable_ds = eval_dataset.iter_torch_batches(batch_size=8)\n\n    args = transformers.TrainingArguments(\n        output_dir=f\"{MODEL_NAME}-wikitext2\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        learning_rate=2e-5,\n        weight_decay=0.01,\n        max_steps=100,\n    )\n\n    trainer = transformers.Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_iterable_ds,\n        eval_dataset=eval_iterable_ds,\n    )\n\n    # [3] Add Ray Train Report Callback\n    # =================================\n    trainer.add_callback(RayTrainReportCallback())\n\n    # [4] Prepare your trainer\n    # ========================\n    trainer = prepare_trainer(trainer)\n    trainer.train()\n\n# Build a Ray TorchTrainer\nscaling_config = ScalingConfig(num_workers=4, use_gpu=True)\nray_trainer = TorchTrainer(\n    train_func,\n    scaling_config=scaling_config,\n    datasets={\"train\": ray_train_ds, \"validation\": ray_eval_ds},\n)\nresult = ray_trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Ray Core API Example for Remote Execution\nDESCRIPTION: Demonstrates the traditional Ray Core API approach for remote execution with approximately 1ms overhead.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/ray-compiled-graph.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Ray Core API for remote execution.\n# ~1ms overhead to invoke `recv`.\nref = receiver.recv.remote(data)\nray.get(ref)\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Ray Tasks\nDESCRIPTION: Implementation of parallel data retrieval using Ray tasks with object references\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\nobject_references = [\n    retrieve_task.remote(item) for item in range(8)\n]\ndata = ray.get(object_references)\nprint_runtime(data, start)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for PyTorch Deployment\nDESCRIPTION: Command to install PyTorch, Torchvision and required packages for Ray Serve deployment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/serve-ml-models.md#2025-04-12_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ pip install torch torchvision requests  \"ray[serve]\"\n```\n\n----------------------------------------\n\nTITLE: Data Worker Implementation\nDESCRIPTION: Ray remote actor implementation of workers that compute gradients on data batches.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_parameter_server.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass DataWorker(object):\n    def __init__(self):\n        self.model = ConvNet()\n        self.data_iterator = iter(get_data_loader()[0])\n\n    def compute_gradients(self, weights):\n        self.model.set_weights(weights)\n        try:\n            data, target = next(self.data_iterator)\n        except StopIteration:  # When the epoch ends, start a new epoch.\n            self.data_iterator = iter(get_data_loader()[0])\n            data, target = next(self.data_iterator)\n        self.model.zero_grad()\n        output = self.model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        return self.model.get_gradients()\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Cluster for 13B Language Model Training (YAML)\nDESCRIPTION: This YAML configuration defines the cluster setup for training a 13B parameter language model. It specifies a head node and GPU worker nodes with their instance types and scaling limits.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nhead_node_type:\n  name: head_node_type\n  instance_type: m5.xlarge\n\nworker_node_types:\n- name: gpu_worker\n  instance_type: g5.12xlarge\n  min_workers: 0\n  max_workers: 4\n  use_spot: false\n```\n\n----------------------------------------\n\nTITLE: Importing Machine Learning Libraries\nDESCRIPTION: Imports scikit-learn and XGBoost libraries for model training, and Ray Tune for hyperparameter tuning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# import standard sklearn libraries\nimport sklearn\nfrom sklearn.base import BaseEstimator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nprint(f\"sklearn: {sklearn.__version__}\")\nimport xgboost as xgb\n\nprint(f\"xgboost: {xgb.__version__}\")\n# import ray libraries\nfrom ray import tune\nfrom ray.tune import Checkpoint\n\n# set global random seed for sklearn models\nnp.random.seed(415)\n```\n\n----------------------------------------\n\nTITLE: Defining the Optuna Search Algorithm in Python\nDESCRIPTION: This snippet initializes the Optuna search algorithm which will be used for hyperparameter optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nalgo = OptunaSearch()\n```\n\n----------------------------------------\n\nTITLE: Running Experiment with Conditional Hyperparameter Space in Python\nDESCRIPTION: Executes a Ray Tune experiment using an objective function designed for conditional search spaces, utilizing an empty config since the space is provided directly to HyperOptSearch.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    objective_two,\n    tune_config=tune.TuneConfig(\n        metric=\"mean_loss\",\n        mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Request Cancellation Handling - Python\nDESCRIPTION: This snippet outlines the mechanism for handling request cancellations in Ray Serve. It shows how to manage the asyncio.CancelledError exception to customize deployment behavior when a request is canceled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"{literalinclude} doc_code/http_guide/disconnects.py\\n:start-after: __start_basic_disconnect__\\n:end-before: __end_basic_disconnect__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Serve LLM with Remote S3 Storage in YAML\nDESCRIPTION: YAML configuration for Ray Serve LLM application that uses a model stored in S3 bucket instead of downloading from Hugging Face, specifying GPU type and model parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\n# config.yaml\napplications:\n- args:\n    llm_configs:\n        - accelerator_type: A10G\n          engine_kwargs:\n            max_model_len: 8192\n          model_loading_config:\n            model_id: my_llama\n            model_source:\n              bucket_uri: s3://anonymous@air-example-data/rayllm-ossci/meta-Llama-3.2-1B-Instruct\n  import_path: ray.serve.llm:build_openai_app\n  name: llm_app\n  route_prefix: \"/\"\n```\n\n----------------------------------------\n\nTITLE: Submitting XGBoost Workload using Ray Job Python SDK\nDESCRIPTION: Python script to submit the XGBoost workload to the Ray cluster. It configures the XGBoostTrainer, sets up the dataset, and submits the job using the Ray Job SDK.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/examples/ml-example.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# XGBoost workload submission script\n:language: python\n```\n\n----------------------------------------\n\nTITLE: Model Composition with Ray Serve\nDESCRIPTION: Illustrates using Ray Serve's model composition API to combine multiple model deployments into a single application. This enhances functionality by allowing multiple models to work together seamlessly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/index.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n{literalinclude} doc_code/quickstart_composed.py\n:language: python\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining the search space\nDESCRIPTION: This snippet defines the search space for the hyperparameters. It specifies the range of values for 'width' and 'height' using tune.uniform. It also sets a fixed value for steps.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsearch_space = {\n    \"steps\": 100,\n    \"width\": tune.uniform(0, 20),\n    \"height\": tune.uniform(-100, 100),\n}\n\n```\n\n----------------------------------------\n\nTITLE: Transforming Rows with Map in Ray Datasets\nDESCRIPTION: Example of using the map method to transform individual rows in a Ray Dataset. The function parses filenames from file paths.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import Any, Dict\nimport ray\n\ndef parse_filename(row: Dict[str, Any]) -> Dict[str, Any]:\n    row[\"filename\"] = os.path.basename(row[\"path\"])\n    return row\n\nds = (\n    ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\", include_paths=True)\n    .map(parse_filename)\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Transformers Pipeline for Text Generation Python\nDESCRIPTION: Initializes the Transformers pipeline for text generation using a fine-tuned model and tokenizer. It specifies the task as text generation and uses device_map to manage hardware placement.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline, AutoTokenizer, GPTJForCausalLM\n\nmodel = GPTJForCausalLM.from_pretrained(\"/mnt/local_storage/checkpoint\")\ntokenizer = AutoTokenizer.from_pretrained(\"/mnt/local_storage/checkpoint\")\n\npipe = pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n```\n\n----------------------------------------\n\nTITLE: Sending Inference Request to Deployed LLM\nDESCRIPTION: cURL command that sends a chat completion request to the model using the OpenAI-compatible API exposed by vLLM. This request asks for a description of the Ray project with system and user messages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/vllm-rayservice.md#2025-04-12_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n$ curl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n      \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n      \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Provide a brief sentence describing the Ray open-source project.\"}\n      ],\n      \"temperature\": 0.7\n    }'\n```\n\n----------------------------------------\n\nTITLE: Translate action_sampler_fn with Custom RLModule\nDESCRIPTION: This snippet illustrates how to translate the `action_sampler_fn` functionality to the new RLModule API in RLlib. It shows how to define custom exploration and inference logic within an RLModule using a custom distribution class `YOUR_DIST_CLASS`. This involves creating an RLModule and overriding its `_forward_exploration` and `_forward_inference` methods to sample actions from the custom distribution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n    \"from ray.rllib.models.torch.torch_distributions import YOUR_DIST_CLASS\\n\\n\\n    class MyRLModule(TorchRLModule):\\n\\n        def _forward_exploration(self, batch):\\n            computation_results = ...\\n            my_dist = YOUR_DIST_CLASS(computation_results)\\n            actions = my_dist.sample()\\n            return {Columns.ACTIONS: actions}\\n\\n        # Maybe for inference, you would like to sample from the deterministic version\\n        # of your distribution:\\n        def _forward_inference(self, batch):\\n            computation_results = ...\\n            my_dist = YOUR_DIST_CLASS(computation_results)\\n            greedy_actions = my_dist.to_deterministic().sample()\\n            return {Columns.ACTIONS: greedy_actions}\"\n```\n\n----------------------------------------\n\nTITLE: FastAPI Integration for HTTP Handling in Ray Serve\nDESCRIPTION: This code snippet demonstrates how to integrate Ray Serve with FastAPI for more expressive HTTP handling.  It creates a FastAPI app and mounts it to the Ray Serve deployment using the `http_adapter` argument. The FastAPI app defines an endpoint `/hello/{name}` that returns a personalized greeting. This shows how to leverage FastAPI's features for defining complex APIs within a Ray Serve deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/key-concepts.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example showing FastAPI integration for HTTP handling.\n\"\"\"\nfrom fastapi import FastAPI\n\nimport ray\nfrom ray import serve\nfrom ray.serve.http_adapters import FastAPIAdapter\n\napp = FastAPI()\nadapter = FastAPIAdapter(app=app)\n\n@serve.deployment(http_adapter=adapter)\nclass FastAPIHandler:\n    @app.get(\"/hello/{name}\")\n    async def say_hello(self, name: str):\n        return {\"message\": f\"Hello {name}!\"}\n\n# You can send requests to:\n# - /hello/foo\n# - /\n# - /docs\n# - /openapi.json\n```\n\n----------------------------------------\n\nTITLE: Tune with MLflowLoggerCallback\nDESCRIPTION: This function demonstrates how to use the MLflowLoggerCallback to log metrics to MLflow during a Tune run. It configures a Tune run with a callback that automatically logs metrics reported to Tune to the specified MLflow tracking URI.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-mlflow.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef tune_with_callback(mlflow_tracking_uri, finish_fast=False):\n    tuner = tune.Tuner(\n        train_function,\n        tune_config=tune.TuneConfig(num_samples=5),\n        run_config=tune.RunConfig(\n            name=\"mlflow\",\n            callbacks=[\n                MLflowLoggerCallback(\n                    tracking_uri=mlflow_tracking_uri,\n                    experiment_name=\"mlflow_callback_example\",\n                    save_artifact=True,\n                )\n            ],\n        ),\n        param_space={\n            \"width\": tune.randint(10, 100),\n            \"height\": tune.randint(0, 100),\n            \"steps\": 5 if finish_fast else 100,\n        },\n    )\n    results = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Implementing MNIST Classifier as PyTorch Lightning Module\nDESCRIPTION: Defines a simple multi-layer perceptron neural network for MNIST classification as a PyTorch Lightning Module, including training, validation, and test steps.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_mnist_example.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MNISTClassifier(pl.LightningModule):\n    def __init__(self, lr=1e-3, feature_dim=128):\n        torch.manual_seed(421)\n        super(MNISTClassifier, self).__init__()\n        self.save_hyperparameters()\n\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, feature_dim),\n            nn.ReLU(),\n            nn.Linear(feature_dim, 10),\n            nn.ReLU(),\n        )\n        self.lr = lr\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10, top_k=1)\n        self.eval_loss = []\n        self.eval_accuracy = []\n        self.test_accuracy = []\n        pl.seed_everything(888)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n        x = self.linear_relu_stack(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = torch.nn.functional.cross_entropy(y_hat, y)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, val_batch, batch_idx):\n        loss, acc = self._shared_eval(val_batch)\n        self.log(\"val_accuracy\", acc)\n        self.eval_loss.append(loss)\n        self.eval_accuracy.append(acc)\n        return {\"val_loss\": loss, \"val_accuracy\": acc}\n\n    def test_step(self, test_batch, batch_idx):\n        loss, acc = self._shared_eval(test_batch)\n        self.test_accuracy.append(acc)\n        self.log(\"test_accuracy\", acc, sync_dist=True, on_epoch=True)\n        return {\"test_loss\": loss, \"test_accuracy\": acc}\n\n    def _shared_eval(self, batch):\n        x, y = batch\n        logits = self.forward(x)\n        loss = F.nll_loss(logits, y)\n        acc = self.accuracy(logits, y)\n        return loss, acc\n\n    def on_validation_epoch_end(self):\n        avg_loss = torch.stack(self.eval_loss).mean()\n        avg_acc = torch.stack(self.eval_accuracy).mean()\n        self.log(\"val_loss\", avg_loss, sync_dist=True)\n        self.log(\"val_accuracy\", avg_acc, sync_dist=True)\n        self.eval_loss.clear()\n        self.eval_accuracy.clear()\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom RLlib Callback (LogAcrobotAngle)\nDESCRIPTION: This code defines a custom RLlib callback class, `LogAcrobotAngle`, that overrides the `on_episode_step` and `on_episode_end` methods. The `on_episode_step` method extracts the angle of the first joint from the Acrobot-v1 environment and logs it temporarily. The `on_episode_end` method calculates the average angle over the episode and logs it to the MetricsLogger with a smoothing window of 50.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-callback.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport numpy as np\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.callbacks.callbacks import RLlibCallback\n\nclass LogAcrobotAngle(RLlibCallback):\n    def on_episode_step(self, *, episode, env, **kwargs):\n        # First get the angle from the env (note that `env` is a VectorEnv).\n        # See https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/acrobot.py\n        # for the env source code.\n        cos_theta1, sin_theta1 = env.envs[0].unwrapped.state[0], env.envs[0].unwrapped.state[1]\n        # Convert cos/sin/tan into degree.\n        deg_theta1 = math.degrees(math.atan2(sin_theta1, cos_theta1))\n\n        # Log the theta1 degree value in the episode object, temporarily.\n        episode.add_temporary_timestep_data(\"theta1\", deg_theta1)\n\n    def on_episode_end(self, *, episode, metrics_logger, **kwargs):\n        # Get all the logged theta1 degree values and average them.\n        theta1s = episode.get_temporary_timestep_data(\"theta1\")\n        avg_theta1 = np.mean(theta1s)\n\n        # Log the final result - per episode - to the MetricsLogger.\n        # Report with a sliding/smoothing window of 50.\n        metrics_logger.log_value(\"theta1_mean\", avg_theta1, reduce=\"mean\", window=50)\n```\n\n----------------------------------------\n\nTITLE: Creating a SingleAgentEpisode in RLlib\nDESCRIPTION: This snippet demonstrates how to manually create and populate a SingleAgentEpisode with dummy data. While RLlib typically handles Episode creation, this shows the underlying structure and data population process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/single-agent-episode.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example of how to create and populate a SingleAgentEpisode.\n\"\"\"\n\nimport numpy as np\n\nfrom ray.rllib.env.single_agent_episode import SingleAgentEpisode\n\n# Create an empty episode object.\nepisode = SingleAgentEpisode()\n\n# The reset observation (necessary).\nepisode.reset(obs=1.0)\n\n# Add some data to it.\nepisode.add_action(action=0)\nepisode.add_reward(reward=1.0)\nepisode.add_observation(obs=2.0)\n\nepisode.add_action(action=1)\nepisode.add_reward(reward=-0.5)\nepisode.add_observation(obs=3.0)\n\n# Finalize the episode by flagging it as terminated (or truncated).\nepisode.set_terminated(truncated=False)\n\n# Convert the data in the Episode to numpy (so we can pass it into a model).\nepisode.numpyize()\n\n# The data is now available in numpy format (for tf/torch models).\nassert episode.observations.dtype == np.float32\nassert episode.observations.shape == (3,)\n\nprint(episode)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Ray Actor for Concurrent Task Execution\nDESCRIPTION: This snippet demonstrates an asynchronous Ray actor implementation using asyncio. It allows concurrent execution of methods by yielding control with 'await', enabling the 'get_num_tasks_executed' method to run while the 'run' method is active.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/concurrent-operations-async-actor.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass AsyncTaskExecutor:\n    def __init__(self):\n        self.num_tasks_executed = 0\n\n    async def run(self):\n        while True:\n            task = await fetch_next_task()\n            await self.execute_task(task)\n            # Yield control to allow other coroutines to run\n            await asyncio.sleep(0)\n\n    async def execute_task(self, task):\n        # Execute the task\n        self.num_tasks_executed += 1\n\n    async def get_num_tasks_executed(self):\n        return self.num_tasks_executed\n\n# Usage\nexecutor = AsyncTaskExecutor.remote()\nexecutor.run.remote()\n# This call will not be blocked\nnum_tasks = ray.get(executor.get_num_tasks_executed.remote())\n```\n\n----------------------------------------\n\nTITLE: Implementing Cross-Validation for Model Evaluation\nDESCRIPTION: This function coordinates the cross-validation process, distributing tasks across the Ray cluster and aggregating results. It handles data partitioning, task submission, and result processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/automl_for_time_series.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_models_with_cv(\n    models: List[_TS],\n    df: pd.DataFrame,\n    label_column: str,\n    metrics: Dict[str, Callable[[pd.Series, pd.Series], float]],\n    freq: str = \"D\",\n    cv: Union[int, TimeSeriesSplit] = 5,\n) -> Dict[_TS, Dict[str, float]]:\n    # Obtain CV train-test indices for each fold.\n    if isinstance(cv, int):\n        cv = TimeSeriesSplit(cv)\n    train_test_indices = list(cv.split(df))\n\n    # Put df into Ray object store for better performance.\n    df_ref = ray.put(df)\n\n    # Add tasks to be executed for each fold.\n    fold_refs = []\n    for model in models:\n        fold_refs.extend(\n            [\n                train_and_evaluate_fold.remote(\n                    model,\n                    df_ref,\n                    train_indices,\n                    test_indices,\n                    label_column,\n                    metrics,\n                    freq=freq,\n                )\n                for train_indices, test_indices in train_test_indices\n            ]\n        )\n\n    fold_results = ray.get(fold_refs)\n\n    # Split fold results into a list of CV splits-sized chunks.\n    # Ray guarantees that order is preserved.\n    fold_results_per_model = [\n        fold_results[i : i + len(train_test_indices)]\n        for i in range(0, len(fold_results), len(train_test_indices))\n    ]\n\n    # Aggregate and average results from all folds per model.\n    # We go from a list of dicts to a dict of lists and then\n    # get a mean of those lists.\n    mean_results_per_model = []\n    for model_results in fold_results_per_model:\n        aggregated_results = defaultdict(list)\n        for fold_result in model_results:\n            for metric, value in fold_result.items():\n                aggregated_results[metric].append(value)\n        mean_results = {\n            metric: np.mean(values) for metric, values in aggregated_results.items()\n        }\n        mean_results_per_model.append(mean_results)\n\n    # Join models and their metrics together.\n    mean_results_per_model = {\n        models[i]: mean_results_per_model[i] for i in range(len(mean_results_per_model))\n    }\n    return mean_results_per_model\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Scikit-learn Deployment\nDESCRIPTION: Command to install Scikit-learn and required packages for Ray Serve deployment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/serve-ml-models.md#2025-04-12_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ pip install scikit-learn requests \"ray[serve]\"\n```\n\n----------------------------------------\n\nTITLE: Saving Ray Dataset with PyTorch Tensor to NumPy\nDESCRIPTION: This code shows how to create a Ray Dataset with a PyTorch tensor and save it as a NumPy file. It imports torch and ray, creates a tensor, wraps it in a dataset, and writes it to a local NumPy file, specifying the column to save.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-pytorch.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport ray\n\ntensor = torch.Tensor(1)\nds = ray.data.from_items([{\"tensor\": tensor}])\n\nds.write_numpy(\"local:///tmp/tensor\", column=\"tensor\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Worker Training Function\nDESCRIPTION: This Python snippet defines the training function, which handles data loading, tokenization, model initialization, and the training loop for each worker during the BERT training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/bert.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_func_per_worker(config: Dict):\n    # Datasets\n    dataset = load_dataset(\"yelp_review_full\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n    \n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n    batch_size = config[\"batch_size_per_worker\"]\n\n    train_dataset = dataset[\"train\"].select(range(1000)).map(tokenize_function, batched=True)\n    eval_dataset = dataset[\"test\"].select(range(1000)).map(tokenize_function, batched=True)\n\n    # Prepare dataloader for each worker\n    dataloaders = {}\n    dataloaders[\"train\"] = torch.utils.data.DataLoader(\n        train_dataset, \n        shuffle=True, \n        collate_fn=transformers.default_data_collator, \n        batch_size=batch_size\n    )\n    dataloaders[\"test\"] = torch.utils.data.DataLoader(\n        eval_dataset, \n        shuffle=True, \n        collate_fn=transformers.default_data_collator, \n        batch_size=batch_size\n    )\n\n    # Obtain HPU device automatically\n    device = ray.train.torch.get_device()\n\n    # Prepare model and optimizer\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-cased\", num_labels=5\n    )\n    model = model.to(device)\n    \n    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\n    # Start training loops\n    for epoch in range(epochs):\n        # Each epoch has a training and validation phase\n        for phase in [\"train\", \"test\"]:\n            if phase == \"train\":\n                model.train()  # Set model to training mode\n            else:\n                model.eval()  # Set model to evaluate mode\n\n            for batch  in dataloaders[phase]:\n                batch = {k: v.to(device) for k, v in batch.items()}\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                with torch.set_grad_enabled(phase == \"train\"):\n                    # Get model outputs and calculate loss\n                    outputs = model(**batch)\n                    loss = outputs.loss\n\n                    # backward + optimize only if in training phase\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n                        print(f\"train epoch:[{epoch}]\\tloss:{loss:.6f}\")\n```\n\n----------------------------------------\n\nTITLE: Complete Ray Train Training Function\nDESCRIPTION: Full implementation of the training function that integrates Ray Train for distributed training, including data preparation, model setup, training loop, and checkpointing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport ray.train as train\nfrom ray.train import Checkpoint\n\ndef train_func(config: dict):\n    batch_size = config[\"batch_size\"]\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n\n    batch_size_per_worker = batch_size // train.get_context().get_world_size()\n\n    # Create data loaders.\n    train_dataloader = DataLoader(\n        training_data, batch_size=batch_size_per_worker, shuffle=True\n    )\n    test_dataloader = DataLoader(test_data, batch_size=batch_size_per_worker)\n\n    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n    test_dataloader = train.torch.prepare_data_loader(test_dataloader)\n\n    model = NeuralNetwork()\n    model = train.torch.prepare_model(model)\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\n    for epoch in range(epochs):\n        train_epoch(epoch, train_dataloader, model, loss_fn, optimizer)\n        test_loss = test_epoch(test_dataloader, model, loss_fn)\n\n        with TemporaryDirectory() as tmpdir:\n            if train.get_context().get_world_rank() == 0:\n                state_dict = dict(epoch=epoch, model=model.state_dict())\n                torch.save(state_dict, os.path.join(tmpdir, \"checkpoint.bin\"))\n                checkpoint = Checkpoint.from_directory(tmpdir)\n            else:\n                checkpoint = None\n            train.report(dict(loss=test_loss), checkpoint=checkpoint)\n\n    print(\"Done!\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Convolutional Neural Network and Training Functions in PyTorch\nDESCRIPTION: This snippet defines a simple two-layer Convolutional Neural Network class and functions for training and testing the model. It includes methods to optimize the model and check its validation accuracy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_hyperparameter.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ConvNet(nn.Module):\n    \"\"\"Simple two layer Convolutional Neural Network.\"\"\"\n\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n        self.fc = nn.Linear(192, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n        x = x.view(-1, 192)\n        x = self.fc(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef train(model, optimizer, train_loader, device=torch.device(\"cpu\")):\n    \"\"\"Optimize the model with one pass over the data.\n\n    Cuts off at 1024 samples to simplify training.\n    \"\"\"\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if batch_idx * len(data) > 1024:\n            return\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n\n\ndef test(model, test_loader, device=torch.device(\"cpu\")):\n    \"\"\"Checks the validation accuracy of the model.\n\n    Cuts off at 512 samples for simplicity.\n    \"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(test_loader):\n            if batch_idx * len(data) > 512:\n                break\n            data, target = data.to(device), target.to(device)\n            outputs = model(data)\n            _, predicted = torch.max(outputs.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n    return correct / total\n```\n\n----------------------------------------\n\nTITLE: Reading from PostgreSQL Database with Ray Data\nDESCRIPTION: This code demonstrates how to read data from a PostgreSQL database using Ray Data. It defines a connection function using `psycopg2` and uses `ray.data.read_sql` to execute SQL queries and create Ray Datasets.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport psycopg2\n\nimport ray\n\ndef create_connection():\n    return psycopg2.connect(\n        user=\"postgres\",\n        password=...,\n        host=\"example-postgres-database.c2c2k1yfll7o.us-west-2.rds.amazonaws.com\",\n        dbname=\"example\",\n    )\n\n# Get all movies\ndataset = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n# Get movies after the year 1980\ndataset = ray.data.read_sql(\n    \"SELECT title, score FROM movie WHERE year >= 1980\", create_connection\n)\n# Get the number of movies per year\ndataset = ray.data.read_sql(\n    \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Serve Application - Python\nDESCRIPTION: Python code defining a Serve application, typically used for serving machine learning models or business logic that handles incoming requests. Requires Ray Serve and Python packages specified in the script. Key component: deployment definitions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/multi-app.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Python code omitted for brevity.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Checkpointing in Ray Tune Training Function\nDESCRIPTION: This code snippet demonstrates how to implement checkpointing in a Ray Tune training function. It shows loading from a checkpoint, simulating training, creating new checkpoints, and reporting results with checkpoints.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune_get_data_in_and_out.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pickle\nimport tempfile\n\nfrom ray import tune\n\ndef training_function(config, data):\n    model = {\n        \"hyperparameter_a\": config[\"hyperparameter_a\"],\n        \"hyperparameter_b\": config[\"hyperparameter_b\"],\n    }\n    epochs = config[\"epochs\"]\n\n    # Load the checkpoint, if there is any.\n    checkpoint = tune.get_checkpoint()\n    start_epoch = 0\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            with open(os.path.join(checkpoint_dir, \"model.pkl\"), \"w\") as f:\n                checkpoint_dict = pickle.load(f)\n        start_epoch = checkpoint_dict[\"epoch\"] + 1\n        model = checkpoint_dict[\"state\"]\n\n    # Simulate training & evaluation - we obtain back a \"metric\" and a \"trained_model\".\n    for epoch in range(start_epoch, epochs):\n        # Simulate doing something expensive.\n        time.sleep(1)\n        metric = (0.1 + model[\"hyperparameter_a\"] * epoch / 100) ** (\n            -1\n        ) + model[\"hyperparameter_b\"] * 0.1 * data[\"A\"].sum()\n\n        checkpoint_dict = {\"state\": model, \"epoch\": epoch}\n\n        # Create the checkpoint.\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            with open(os.path.join(temp_checkpoint_dir, \"model.pkl\"), \"w\") as f:\n                pickle.dump(checkpoint_dict, f)\n            tune.report(\n                {\"metric\": metric},\n                checkpoint=tune.Checkpoint.from_directory(temp_checkpoint_dir),\n            )\n\n\ntuner = tune.Tuner(\n    tune.with_parameters(training_function, data=data),\n    param_space={\n        \"hyperparameter_a\": tune.uniform(0, 20),\n        \"hyperparameter_b\": tune.uniform(-100, 100),\n        \"epochs\": 10,\n    },\n    tune_config=tune.TuneConfig(num_samples=4, metric=\"metric\", mode=\"max\"),\n    run_config=tune.RunConfig(\n        callbacks=[MLflowLoggerCallback(experiment_name=\"example\")]\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining BayesOpt Search Algorithm\nDESCRIPTION: This snippet defines the BayesOpt search algorithm with a concurrency limiter. It configures the BayesOptSearch algorithm with specific utility kwargs and constrains it to a maximum of 4 concurrent trials using ConcurrencyLimiter.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nalgo = BayesOptSearch(utility_kwargs={\"kind\": \"ucb\", \"kappa\": 2.5, \"xi\": 0.0})\nalgo = ConcurrencyLimiter(algo, max_concurrent=4)\n\n```\n\n----------------------------------------\n\nTITLE: Callback Resolution Order Example in Python\nDESCRIPTION: This example demonstrates the resolution order of chained callbacks in RLlib. It defines two RLlibCallback subclasses and two lambda functions for the on_train_result event.  The configuration specifies the order of the subclasses, and then the lambda functions, demonstrating the order in which they are called during training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-callback.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyCallbacks(RLlibCallback):\n    def on_train_result(self, *, algorithm, metrics_logger, result, **kwargs):\n        print(\"RLlibCallback subclass\")\n\nclass MyDebugCallbacks(RLlibCallback):\n    def on_train_result(self, *, algorithm, metrics_logger, result, **kwargs):\n        print(\"debug subclass\")\n\n# Define the callbacks order through the config.\n# Subclasses first, then individual `on_train_result` (or other events) callables:\nconfig.callbacks(\n    callbacks_class=[MyDebugCallbacks, MyCallbacks],  # <- note: debug class first\n    on_train_result=[\n        lambda algorithm, **kw: print('lambda 1'),\n        lambda algorithm, **kw: print('lambda 2'),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Requirements for Tasks in Java\nDESCRIPTION: This Java code snippet specifies the required resources for a task and an actor using Ray's API. It shows how to set the CPU and GPU requirements explicitly to ensure proper scheduling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/resources.rst#2025-04-12_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n\"\"\"\n// Specify required resources.\nRay.task(MyRayApp::myFunction).setResource(\"CPU\", 1.0).setResource(\"GPU\", 1.0).setResource(\"special_hardware\", 1.0).remote();\n\nRay.actor(Counter::new).setResource(\"CPU\", 2.0).setResource(\"GPU\", 1.0).remote();\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Integrating Existing FastAPI Apps with Ray Serve - Python\nDESCRIPTION: This code snippet explains how to integrate an existing FastAPI app into a Ray Serve deployment without modifications. It retains existing middlewares and supports automatic OpenAPI documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"{literalinclude} doc_code/http_guide/http_guide.py\\n:start-after: __begin_byo_fastapi__\\n:end-before: __end_byo_fastapi__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Actor-Based Batch Predictor Class\nDESCRIPTION: Implements a Ray actor class for batch prediction that caches the model and processes multiple prediction requests efficiently.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/batch_prediction.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport ray\n\n@ray.remote\nclass BatchPredictor:\n    def __init__(self, model):\n        self.model = model\n        \n    def predict(self, shard_path):\n        df = pq.read_table(shard_path).to_pandas()\n        result =self.model(df)\n\n        # Write out the prediction result.\n        # NOTE: unless the driver will have to further process the\n        # result (other than simply writing out to storage system),\n        # writing out at remote task is recommended, as it can avoid\n        # congesting or overloading the driver.\n        # ...\n\n        # Here we just return the size about the result in this example.\n        return len(result)\n```\n\n----------------------------------------\n\nTITLE: Customizing Evaluation Config in RLlib - Python\nDESCRIPTION: This snippet shows how to override default configuration settings for evaluation workers in RLlib by turning off exploration during evaluation to ensure deterministic policy behavior.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-advanced-api.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Disable explorer during evaluation\n'evaluation_config': {\n    'explore': False\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Counter Actor in Java\nDESCRIPTION: Shows how to create a Counter actor in Java using Ray.actor(). The implementation provides basic counter functionality with an increment method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors.rst#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class Counter {\n\n  private int value = 0;\n\n  public int increment() {\n    this.value += 1;\n    return this.value;\n  }\n}\n\n// Create an actor from this class.\n// `Ray.actor` takes a factory method that can produce\n// a `Counter` object. Here, we pass `Counter`'s constructor\n// as the argument.\nActorHandle<Counter> counter = Ray.actor(Counter::new).remote();\n```\n\n----------------------------------------\n\nTITLE: Streaming DeploymentHandle Calls\nDESCRIPTION: This Python code defines a streaming deployment, Streamer, which yields a sequence of greetings. The example shows how to call this deployment from a client using a DeploymentHandle and iterate over the results as an asynchronous generator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/model_composition.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nimport ray\nfrom ray import serve\n\n@serve.deployment\nclass Streamer:\n    async def gen(self):\n        yield \"Hello\"\n        await asyncio.sleep(0.1)\n        yield \"World\"\n        await asyncio.sleep(0.1)\n        yield \"!\"\n\n    async def __call__(self, handle):\n        handle = handle.options(stream=True)\n        generator = handle.gen.remote()\n        final = []\n        async for item in generator:\n            final.append(await item)\n        return \"\".join(final)\n\n@serve.deployment\nclass Caller:\n    def __init__(self, streamer):\n        self.streamer = streamer\n\n    async def __call__(self):\n        return await self.streamer(\"handle\")\n\nif __name__ == \"__main__\":\n    ray.init(ignore_reinit_error=True)\n\n    streamer = Streamer.bind()\n    caller = Caller.bind(streamer)\n\n    serve.run(caller)\n\n    # Example:\n    # curl \"http://localhost:8000/\"\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Checkpointing in Ray Train\nDESCRIPTION: Creates and saves model checkpoints during training, then reports them to Ray Train to enable model recovery and continued training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n    from ray import train\n    from ray.train import Checkpoint\n\n    with TemporaryDirectory() as tmpdir:\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model\": model.module.state_dict()\n            },\n            os.path.join(tmpdir, \"checkpoint.pt\")\n        )\n        train.report(dict(loss=test_loss), checkpoint=Checkpoint.from_directory(tmpdir))\n```\n\n----------------------------------------\n\nTITLE: Chaining DeploymentHandle Calls\nDESCRIPTION: This Python code defines three deployments: Adder, Multiplier, and Ingress. The Ingress deployment chains calls to the Adder and Multiplier deployments together, passing the result of the Adder call directly to the Multiplier call without explicitly awaiting it.  This demonstrates how Ray Serve manages the awaiting behavior under the hood.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/model_composition.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import serve\n\n@serve.deployment\nclass Adder:\n    def __init__(self, increment: int):\n        self.increment = increment\n\n    def add(self, val: int) -> int:\n        return val + self.increment\n\n\n@serve.deployment\nclass Multiplier:\n    def __init__(self, multiple: int):\n        self.multiple = multiple\n\n    def multiply(self, val: int) -> int:\n        return val * self.multiple\n\n\n@serve.deployment\nclass Ingress:\n    def __init__(self, adder, multiplier):\n        self.adder = adder\n        self.multiplier = multiplier\n\n    async def __call__(self, request):\n        val = int(request.query_params[\"val\"])\n        added_val = self.adder.add.remote(val)\n        multiplied_val = self.multiplier.multiply.remote(added_val)\n        return await multiplied_val\n\n\nif __name__ == \"__main__\":\n    ray.init(ignore_reinit_error=True)\n\n    adder = Adder.bind(increment=5)\n    multiplier = Multiplier.bind(multiple=3)\n    ingress = Ingress.bind(adder, multiplier)\n\n    serve.run(ingress)\n\n    # Example:\n    # curl \"http://localhost:8000/?val=1\"\n\n```\n\n----------------------------------------\n\nTITLE: PyTorch Lightning with Ray Train Integration\nDESCRIPTION: The same training script integrated with Ray Train. Key additions include Ray Lightning strategy, environment plugins, callbacks for metrics reporting, and distributed configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import resnet18\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Normalize, Compose\nimport lightning.pytorch as pl\n\nimport ray.train.lightning\nfrom ray.train.torch import TorchTrainer\n\n# Model, Loss, Optimizer\nclass ImageClassifier(pl.LightningModule):\n    def __init__(self):\n        super(ImageClassifier, self).__init__()\n        self.model = resnet18(num_classes=10)\n        self.model.conv1 = torch.nn.Conv2d(\n            1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        outputs = self.forward(x)\n        loss = self.criterion(outputs, y)\n        self.log(\"loss\", loss, on_step=True, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=0.001)\n\n\ndef train_func():\n    # Data\n    transform = Compose([ToTensor(), Normalize((0.28604,), (0.32025,))])\n    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n    train_data = FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n    train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n\n    # Training\n    model = ImageClassifier()\n    # [1] Configure PyTorch Lightning Trainer.\n    trainer = pl.Trainer(\n        max_epochs=10,\n        devices=\"auto\",\n        accelerator=\"auto\",\n        strategy=ray.train.lightning.RayDDPStrategy(),\n        plugins=[ray.train.lightning.RayLightningEnvironment()],\n        callbacks=[ray.train.lightning.RayTrainReportCallback()],\n        # [1a] Optionally, disable the default checkpointing behavior\n        # in favor of the `RayTrainReportCallback` above.\n        enable_checkpointing=False,\n    )\n    trainer = ray.train.lightning.prepare_trainer(trainer)\n    trainer.fit(model, train_dataloaders=train_dataloader)\n\n# [2] Configure scaling and resource requirements.\nscaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=True)\n\n# [3] Launch distributed training job.\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=scaling_config,\n    # [3a] If running in a multi-node cluster, this is where you\n    # should configure the run's persistent storage that is accessible\n    # across all worker nodes.\n    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n)\nresult: ray.train.Result = trainer.fit()\n\n# [4] Load the trained model.\nwith result.checkpoint.as_directory() as checkpoint_dir:\n    model = ImageClassifier.load_from_checkpoint(\n        os.path.join(\n            checkpoint_dir,\n            ray.train.lightning.RayTrainReportCallback.CHECKPOINT_NAME,\n        ),\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Loss Function and Optimizer in PyTorch\nDESCRIPTION: This snippet defines the loss function (CrossEntropyLoss) and optimizer (SGD) used for training the neural network.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Model Training with Ray Train\nDESCRIPTION: Shows integration between Ray Data and Ray Train for distributed model training using PyTorch, including dataset loading and training loop implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-pytorch.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import nn\nimport ray\nfrom ray import train\nfrom ray.train import ScalingConfig\nfrom ray.train.torch import TorchTrainer\n\ndef train_func():\n    model = nn.Sequential(nn.Linear(30, 1), nn.Sigmoid())\n    loss_fn = torch.nn.BCELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\n    train_data_shard = train.get_dataset_shard(\"train\")\n\n    for epoch_idx in range(2):\n        for batch in train_data_shard.iter_torch_batches(batch_size=128, dtypes=torch.float32):\n            features = torch.stack([batch[col_name] for col_name in batch.keys() if col_name != \"target\"], axis=1)\n            predictions = model(features)\n            train_loss = loss_fn(predictions, batch[\"target\"].unsqueeze(1))\n            train_loss.backward()\n            optimizer.step()\n\ntrain_dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n\ntrainer = TorchTrainer(\n    train_func,\n    datasets={\"train\": train_dataset},\n    scaling_config=ScalingConfig(num_workers=2)\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: PPO Algorithm Configuration in RLlib\nDESCRIPTION: Configuration options for Proximal Policy Optimization algorithm, supporting single and multi-agent training with continuous and discrete action spaces\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-algorithms.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nray.rllib.algorithms.ppo.ppo.PPOConfig.training()\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Serialization with inspect_serializability\nDESCRIPTION: Shows how to use ray.util.inspect_serializability to identify non-serializable objects within functions, classes, or object instances. This example demonstrates detecting a non-serializable threading.Lock object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/serialization.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util import inspect_serializability\nimport threading\n\nlock = threading.Lock()\n\ndef test():\n    print(lock)\n\ninspect_serializability(test, name=\"test\")\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Tracing in Ray Tasks\nDESCRIPTION: Example code demonstrating how to add custom tracing spans within Ray remote functions. This allows for more detailed tracing information within the execution of Ray tasks by creating custom spans with specific names.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/ray-tracing.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\n\n@ray.remote\ndef my_func():\n    tracer = trace.get_tracer(__name__)\n\n    with tracer.start_as_current_span(\"foo\"):\n        print(\"Hello world from OpenTelemetry Python!\")\n```\n\n----------------------------------------\n\nTITLE: Installing Data-Juicer Dependencies\nDESCRIPTION: Shell commands for installing Data-Juicer with distributed processing requirements via pip.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/data_juicer_distributed_data_processing.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -v -e .  # Install the minimal requirements of Data-Juicer\npip install -v -e \".[dist]\"  # Include dependencies on Ray and other distributed libraries\n```\n\n----------------------------------------\n\nTITLE: Generating Ray Serve LLM Configuration Files\nDESCRIPTION: Bash command to run the Ray Serve LLM configuration generator, which creates model and deployment config files interactively.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython -m ray.serve.llm.gen_config\n```\n\n----------------------------------------\n\nTITLE: Text Classification with Transformers\nDESCRIPTION: Demonstrates batch inference using the Transformers library for text classification with parallel processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-text.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\n\nimport numpy as np\nfrom transformers import pipeline\n\nimport ray\n\nclass TextClassifier:\n    def __init__(self):\n        self.model = pipeline(\"text-classification\")\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\n        predictions = self.model(list(batch[\"text\"]))\n        batch[\"label\"] = [prediction[\"label\"] for prediction in predictions]\n        return batch\n\nds = (\n    ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\n    .map_batches(TextClassifier, concurrency=2)\n)\n\nds.show(3)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Anti-pattern of Global Variables in Python\nDESCRIPTION: This code snippet illustrates the misuse of global variables in Ray, highlighting how changes to global state are not reflected across different processes. It's an introductory example showcasing the pitfalls of sharing state using global variables instead of actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/global-variables.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/anti_pattern_global_variables.py\n    :language: python\n    :start-after: __anti_pattern_start__\n    :end-before: __anti_pattern_end__\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Fine-tuned Dolly-v2 Model Using HuggingFace Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use the HuggingFace Pipeline to generate text with the fine-tuned Dolly-v2 model. It provides example prompts and generates text using the pipeline with specified parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfor prompt in [\"This is\", \"I am\", \"Once more\"]:\n    print(nlp_pipeline(prompt, max_new_tokens=20, do_sample=True, pad_token_id=tokenizer.eos_token_id))\n```\n\n----------------------------------------\n\nTITLE: Configuring Structured Output for Ray Serve LLM\nDESCRIPTION: Python code for server-side configuration of a Ray Serve LLM deployment supporting structured output. Sets up model loading and deployment parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.llm import LLMConfig, build_openai_app\n\nllm_config = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"qwen-0.5b\",\n        model_source=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1,\n            max_replicas=2,\n        )\n    ),\n    accelerator_type=\"A10G\",\n)\n\n# Build and deploy the model\napp = build_openai_app({\"llm_configs\": [llm_config]})\nserve.run(app, blocking=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Ray Actor Class for Stateful Processing\nDESCRIPTION: Demonstrates how to create a stateful worker using Ray actors. The example shows a Counter class that maintains its state between method calls and executes methods serially.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/walkthrough.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Define an actor class\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.count = 0\n\n    def increment(self):\n        self.count += 1\n        return self.count\n\n# Create an actor instance\ncounter = Counter.remote()\n\n# Call actor methods\nfuture1 = counter.increment.remote()  # Returns an object reference\nfuture2 = counter.increment.remote()\n\n# Get the results\nprint(ray.get(future1))  # 1\nprint(ray.get(future2))  # 2\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing CoNaLa Dataset for LLM Fine-tuning\nDESCRIPTION: Loads the CoNaLa dataset, combines curated and mined data, converts it to a Ray Dataset, and applies preprocessing steps including prompt formatting and tokenization for fine-tuning Vicuna-13B.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport re\nimport ray\nimport json\nfrom transformers import AutoTokenizer\nfrom datasets import concatenate_datasets, load_dataset\n\n# Combine the curated dataset and automatically-mined dataset\nhf_dataset_curated = load_dataset(\"neulab/conala\")\nhf_dataset_mined = load_dataset(\"neulab/conala\", \"mined\", split=\"train[:5000]\")\nhf_dataset_merged = concatenate_datasets(\n    [hf_dataset_curated[\"train\"], hf_dataset_mined]\n)\nprint(hf_dataset_merged)\n\n# Convert it into Ray Dataset\nray_ds = ray.data.from_huggingface(hf_dataset_merged)\n\n# Build a prompt template for Vicuna-13b model\nPROMPT_TEMPLATE = \"Intent: {intent}\\nOne-line code snippet: {snippet}\"\n\n\ndef fill_prompt(batch):\n    batch[\"input_sentence\"] = batch.apply(\n        lambda row: PROMPT_TEMPLATE.format(\n            intent=row[\"rewritten_intent\"]\n            if row[\"rewritten_intent\"]\n            else row[\"intent\"],\n            snippet=f\"`{row['snippet']}`\",\n        )\n        + \"</s>\",\n        axis=1,\n    )\n    return batch[[\"input_sentence\"]]\n\n\n# Tokenize input sentences to tensors\ndef tokenize(batch):\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_NAME, padding_side=\"left\", use_fast=False\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    ret = tokenizer(\n        list(batch[\"input_sentence\"]),\n        truncation=True,\n        max_length=128,\n        padding=\"max_length\",\n        return_tensors=\"np\",\n    )\n    ret[\"labels\"] = ret[\"input_ids\"].copy()\n    return dict(ret)\n\n# Preprocess train dataset\nprocessed_ds = ray_ds.map_batches(fill_prompt, batch_format=\"pandas\").map_batches(tokenize, batch_format=\"pandas\")\n\n# To accelerate release tests\nprocessed_ds = processed_ds.limit(16 * 8 * 16)  # each worker has 16 batches\n```\n\n----------------------------------------\n\nTITLE: Using the Deprecated LightningTrainer API\nDESCRIPTION: This code snippet shows the now-deprecated LightningTrainer API that uses a LightningConfigBuilder to define configurations for the Lightning module and trainer. This API is more constraining and limits control over the training functionality compared to the new TorchTrainer API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.lightning import LightningConfigBuilder, LightningTrainer\n\nconfig_builder = LightningConfigBuilder()\n# [1] Collect model configs\nconfig_builder.module(cls=MyLightningModule, lr=1e-3, feature_dim=128)\n\n# [2] Collect checkpointing configs\nconfig_builder.checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n\n# [3] Collect pl.Trainer configs\nconfig_builder.trainer(\n    max_epochs=10,\n    accelerator=\"gpu\",\n    log_every_n_steps=100,\n)\n\n# [4] Build datasets on the head node\ndatamodule = MyLightningDataModule(batch_size=32)\nconfig_builder.fit_params(datamodule=datamodule)\n\n# [5] Execute the internal training function in a black box\nray_trainer = LightningTrainer(\n    lightning_config=config_builder.build(),\n    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),\n    run_config=RunConfig(\n        checkpoint_config=CheckpointConfig(\n            num_to_keep=3,\n            checkpoint_score_attribute=\"val_accuracy\",\n            checkpoint_score_order=\"max\",\n        ),\n    )\n)\nresult = ray_trainer.fit()\n\n# [6] Load the trained model from an opaque Lightning-specific checkpoint.\nlightning_checkpoint = result.checkpoint\nmodel = lightning_checkpoint.get_model(MyLightningModule)\n```\n\n----------------------------------------\n\nTITLE: Implementing Synchronous Ray Actor for Task Execution\nDESCRIPTION: This snippet shows a synchronous Ray actor implementation for task execution. It demonstrates the limitation where the long-running 'run' method blocks other method calls.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/concurrent-operations-async-actor.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass TaskExecutor:\n    def __init__(self):\n        self.num_tasks_executed = 0\n\n    def run(self):\n        while True:\n            task = fetch_next_task()\n            self.execute_task(task)\n\n    def execute_task(self, task):\n        # Execute the task\n        self.num_tasks_executed += 1\n\n    def get_num_tasks_executed(self):\n        return self.num_tasks_executed\n\n# Usage\nexecutor = TaskExecutor.remote()\nexecutor.run.remote()\n# This call will be blocked forever\nray.get(executor.get_num_tasks_executed.remote())\n```\n\n----------------------------------------\n\nTITLE: Extracting and Inspecting a Single Batch of Images\nDESCRIPTION: Takes a batch of 10 images from the dataset and prints information about the batch structure and image shape.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsingle_batch = ds.take_batch(10)\nprint(f\"Num columns: {len(single_batch['image'])}\")\nprint(f\"Image shape: {single_batch['image'][0].shape}\")\n```\n\n----------------------------------------\n\nTITLE: Passing Constants to Training Functions in Ray Tune\nDESCRIPTION: Uses tune.with_parameters to wrap a training function with constant values that shouldn't be part of the hyperparameter search space. This passes data directory location, number of epochs, and GPU allocation as fixed parameters to the training function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ngpus_per_trial = 0\ndata_dir = \"~/data\"\n\ntrain_fn_with_parameters = tune.with_parameters(\n    train_mnist_tune, num_epochs=num_epochs, num_gpus=gpus_per_trial, data_dir=data_dir\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tuner with ConfigSpace - Python\nDESCRIPTION: This snippet updates the algorithm and scheduler setup to accommodate passing the ConfigSpace directly to the TuneBOHB algorithm, ensuring compatible metric and mode definitions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# As we are passing config space directly to the searcher,\\n# we need to define metric and mode in it as well, in addition\\n# to Tuner()\\nalgo = TuneBOHB(\\n    space=config_space,\\n    metric=\"mean_loss\",\\n    mode=\"max\",\\n)\\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\\nscheduler = HyperBandForBOHB(\\n    time_attr=\"training_iteration\",\\n    max_t=100,\\n    reduction_factor=4,\\n    stop_last_trials=False,\\n)\n```\n\n----------------------------------------\n\nTITLE: Using NodeAffinitySchedulingStrategy in Ray\nDESCRIPTION: This snippet illustrates the use of NodeAffinitySchedulingStrategy in Ray. It creates a remote function with a specific node affinity and executes it, demonstrating how to schedule tasks on particular nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/index.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnode_id = ray.get_runtime_context().node_id\n\n@ray.remote(\n    scheduling_strategy=NodeAffinitySchedulingStrategy(\n        node_id, soft=False))\ndef f():\n    return 1\n\nray.get(f.remote())\n```\n\n----------------------------------------\n\nTITLE: Data Dependencies in Ray Compiled Graph\nDESCRIPTION: Demonstration of specifying data dependencies between actors in a Ray Compiled Graph using DAGNodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/quickstart.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith ray.dag.InputNode() as input_node:\n    # a.echo returns a DAGNode\n    output1 = a.echo.bind(input_node)\n    # Pass the DAGNode as an argument\n    output2 = b.echo.bind(output1)\n    dag = output2.experimental_compile()\n\ndag.execute(\"hello\")\nprint(ray.get(dag))\n```\n\n----------------------------------------\n\nTITLE: Calculating Steps per Epoch for GPT-J Fine-tuning\nDESCRIPTION: This code calculates the number of steps per epoch based on the dataset size, batch size, and number of workers. This is used to configure the training loop and checkpoint saving frequency.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 16\ntrain_ds_size = processed_datasets[\"train\"].count()\nsteps_per_epoch = train_ds_size // (batch_size * num_workers)\n```\n\n----------------------------------------\n\nTITLE: Executing Single-Worker PyTorch Training Function (Python)\nDESCRIPTION: This code snippet shows how to execute the single-worker PyTorch training function defined earlier. Placeholders `__torch_single_run_begin__` and `__torch_single_run_end__` define start and end, and the `:dedent: 4` parameter removes indentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/pytorch/torch_quick_start.py\n:language: python\n:start-after: __torch_single_run_begin__\n:end-before: __torch_single_run_end__\n:dedent: 4\n```\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Restore Options for Tune Experiments\nDESCRIPTION: Example showing how to customize the restoration behavior of a Tune experiment with different options for handling trials based on their previous states, including whether to resume ERRORED trials or restart them.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-fault-tolerance.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner.restore(\n    path=\"~/ray_results/tune_fault_tolerance_guide\",\n    trainable=trainable,\n    # Only resume RUNNING trials\n    resume_errored=False,\n    # Or, resume RUNNING trials and restart ERRORED trials from scratch\n    # resume_errored=False,\n    # restart_errored=True,\n)\n\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Hyperparameter Evaluation with Ray in Python\nDESCRIPTION: This snippet defines a Ray remote function for evaluating hyperparameters. It creates and trains a neural network with given hyperparameters, then returns the model's accuracy. This function will be executed in parallel for different sets of hyperparameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_hyperparameter.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef evaluate_hyperparameters(config):\n    model = ConvNet()\n    train_loader, test_loader = get_data_loaders(config[\"batch_size\"])\n    optimizer = optim.SGD(\n        model.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"]\n    )\n    train(model, optimizer, train_loader)\n    return test(model, test_loader)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Distributed PyTorch Training with Ray\nDESCRIPTION: This snippet imports necessary libraries and modules for distributed PyTorch training using Ray Train and Tune. It includes imports for PyTorch, Ray, and custom utility functions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/tune_cifar_torch_pbt_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom filelock import FileLock\nfrom torch.utils.data import DataLoader, Subset\n\nimport ray\nfrom ray import train, tune\nfrom ray.air import session\nfrom ray.train import ScalingConfig\nfrom ray.train.torch import TorchTrainer\nfrom ray.tune.schedulers import PopulationBasedTraining\n```\n\n----------------------------------------\n\nTITLE: Basic DeploymentHandle Example\nDESCRIPTION: This Python code defines three Ray Serve deployments: SpanishResponder, FrenchResponder, and LanguageClassifier. The LanguageClassifier deployment takes the other two as constructor arguments, which Ray Serve converts to DeploymentHandles. It uses these handles to call the say_hello methods of the other deployments based on the input request.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/model_composition.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example showing DeploymentHandles.\n\"\"\"\n\nimport ray\nfrom ray import serve\n\n@serve.deployment\nclass SpanishResponder:\n    def __init__(self, greeting: str):\n        self.greeting = greeting\n\n    def say_hello(self, name: str) -> str:\n        return f\"{self.greeting}, {name}!\"\n\n@serve.deployment\nclass FrenchResponder:\n    def __init__(self, greeting: str):\n        self.greeting = greeting\n\n    def say_hello(self, name: str) -> str:\n        return f\"{self.greeting}, {name}!\"\n\n@serve.deployment\nclass LanguageClassifier:\n    def __init__(self, spanish_responder, french_responder):\n        self.spanish_responder = spanish_responder\n        self.french_responder = french_responder\n\n    async def __call__(self, request):\n        name = request.query_params[\"name\"]\n        language = request.query_params[\"language\"]\n\n        if language == \"spanish\":\n            response = self.spanish_responder.say_hello.remote(name)\n        elif language == \"french\":\n            response = self.french_responder.say_hello.remote(name)\n        else:\n            return \"Please specify 'language' as either 'spanish' or 'french'.\"\n\n        return await response\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@serve.deployment\ndef endpoint(request):\n    return \"/\"\n\n\n\nif __name__ == \"__main__\":\n    ray.init(ignore_reinit_error=True)\n\n    spanish_responder = SpanishResponder.bind(greeting=\"Hola\")\n    french_responder = FrenchResponder.bind(greeting=\"Bonjour\")\n    language_classifier = LanguageClassifier.bind(spanish_responder, french_responder)\n\n    serve.run(language_classifier)\n\n    # Example Usage:\n    # curl \"http://localhost:8000/?language=spanish&name=Dora\"\n    # curl \"http://localhost:8000/?language=french&name=Dora\"\n\n```\n\n----------------------------------------\n\nTITLE: Modified Test Epoch Function for Ray Train\nDESCRIPTION: Updated test_epoch function that adjusts the dataset size calculation for distributed training by dividing by the world size.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef test_epoch(dataloader, model, loss_fn):\n    # Divide the dataset size by the world size to get the per-worker dataset size.\n    size = len(dataloader.dataset) // ray.train.get_context().get_world_size()\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n    return test_loss\n```\n\n----------------------------------------\n\nTITLE: Streaming Worker Logs with Python SDK\nDESCRIPTION: Shows how to stream logs from a specific worker process using the Ray Python SDK. It retrieves the node IP and worker PID, then uses the get_log function to continuously print log lines.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import get_log\n\n# Retrieve the node IP from list_nodes() or ray.nodes()\n# get the PID of the worker running the Actor easily when output\n# of worker is directed to the driver (default)\n# The loop blocks with `follow=True`\nfor line in get_log(pid=<PID>, node_ip=<NODE_IP>, follow=True):\n    print(line)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Parallel Hyperparameter Evaluation and Result Processing in Python\nDESCRIPTION: This snippet demonstrates the main hyperparameter tuning process. It launches multiple parallel tasks for evaluating different hyperparameters, then processes the results as they complete to find the best performing set of hyperparameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_hyperparameter.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Keep track of the best hyperparameters and the best accuracy.\nbest_hyperparameters = None\nbest_accuracy = 0\n# A list holding the object refs for all of the experiments that we have\n# launched but have not yet been processed.\nremaining_ids = []\n# A dictionary mapping an experiment's object ref to its hyperparameters.\n# hyerparameters used for that experiment.\nhyperparameters_mapping = {}\n\n# Randomly generate sets of hyperparameters and launch a task to evaluate it.\nfor i in range(num_evaluations):\n    hyperparameters = generate_hyperparameters()\n    accuracy_id = evaluate_hyperparameters.remote(hyperparameters)\n    remaining_ids.append(accuracy_id)\n    hyperparameters_mapping[accuracy_id] = hyperparameters\n\n# Fetch and print the results of the tasks in the order that they complete.\nwhile remaining_ids:\n    # Use ray.wait to get the object ref of the first task that completes.\n    done_ids, remaining_ids = ray.wait(remaining_ids)\n    # There is only one return result by default.\n    result_id = done_ids[0]\n\n    hyperparameters = hyperparameters_mapping[result_id]\n    accuracy = ray.get(result_id)\n    print(\n        \"\"\"We achieve accuracy {:.3}% with\n        learning_rate: {:.2}\n        batch_size: {}\n        momentum: {:.2}\n      \"\"\".format(\n            100 * accuracy,\n            hyperparameters[\"learning_rate\"],\n            hyperparameters[\"batch_size\"],\n            hyperparameters[\"momentum\"],\n        )\n    )\n    if accuracy > best_accuracy:\n        best_hyperparameters = hyperparameters\n        best_accuracy = accuracy\n\n# Record the best performing set of hyperparameters.\nprint(\n    \"\"\"Best accuracy over {} trials was {:.3} with\n      learning_rate: {:.2}\n      batch_size: {}\n      momentum: {:.2}\n      \"\"\".format(\n        num_evaluations,\n        100 * best_accuracy,\n        best_hyperparameters[\"learning_rate\"],\n        best_hyperparameters[\"batch_size\"],\n        best_hyperparameters[\"momentum\"],\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Image Classification with ResNet Model\nDESCRIPTION: Implementation of image classification using a pre-trained ResNet18 model in Ray Data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-images.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchvision import models\n\nclass ImageClassifier:\n    def __init__(self):\n        weights = models.ResNet18_Weights.DEFAULT\n        self.model = models.resnet18(weights=weights)\n        self.model.eval()\n\n    def __call__(self, batch):\n        inputs = torch.from_numpy(batch[\"image\"])\n        with torch.inference_mode():\n            outputs = self.model(inputs)\n        return {\"class\": outputs.argmax(dim=1)}\n\npredictions = ds.map_batches(\n    ImageClassifier,\n    concurrency=2,\n    batch_size=4\n)\npredictions.show(3)\n```\n\n----------------------------------------\n\nTITLE: Migrating from Standard PyTorch Lightning to Ray Train\nDESCRIPTION: A diff showing the changes needed to adapt a standard PyTorch Lightning script to use Ray Train. Key changes involve replacing the distributed strategy and environment plugins with Ray equivalents.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n import lightning.pytorch as pl\n-from pl.strategies import DDPStrategy\n-from pl.plugins.environments import LightningEnvironment\n+import ray.train.lightning\n\n def train_func():\n     ...\n     model = MyLightningModule(...)\n     datamodule = MyLightningDataModule(...)\n\n     trainer = pl.Trainer(\n-        devices=[0, 1, 2, 3],\n-        strategy=DDPStrategy(),\n-        plugins=[LightningEnvironment()],\n+        devices=\"auto\",\n+        accelerator=\"auto\",\n+        strategy=ray.train.lightning.RayDDPStrategy(),\n+        plugins=[ray.train.lightning.RayLightningEnvironment()]\n     )\n+    trainer = ray.train.lightning.prepare_trainer(trainer)\n\n     trainer.fit(model, datamodule=datamodule)\n```\n\n----------------------------------------\n\nTITLE: Creating PyTorch DataLoaders\nDESCRIPTION: This snippet shows how to create DataLoader objects for both training and test datasets, which will be used to iterate over the data during training and evaluation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 64\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n```\n\n----------------------------------------\n\nTITLE: Massively Parallel Pi Calculation\nDESCRIPTION: Launches multiple parallel tasks for improved pi estimation accuracy, demonstrating Ray's ability to distribute computation across a cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/highly_parallel.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nFULL_SAMPLE_COUNT = 100 * 1000 * 1000 * 1000 # 100 billion samples! \nBATCHES = int(FULL_SAMPLE_COUNT / SAMPLE_COUNT)\nprint(f'Doing {BATCHES} batches')\nresults = []\nfor _ in range(BATCHES):\n    results.append(pi4_sample.remote(sample_count = SAMPLE_COUNT))\noutput = ray.get(results)\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU Usage in Ray Train\nDESCRIPTION: Configures the ScalingConfig to use GPUs by setting use_gpu=True, which requests one GPU per training worker. In this example, 8 GPUs will be used in total.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\n\nscaling_config = ScalingConfig(\n    num_workers=8,\n    use_gpu=True\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Huggingface Image Classification Pipeline\nDESCRIPTION: Sets up a Huggingface Image Classification pipeline using a pre-trained Vision Transformer model, and performs inference on a batch of images.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import pipeline\nfrom PIL import Image\n\n# Note, you must have GPUs on your head node in order to do this with GPUs.\n# If doing CPU inference, set DEVICE=\"cpu\" instead.\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclassifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\", device=DEVICE)\noutputs = classifier([Image.fromarray(image_array) for image_array in single_batch[\"image\"]], top_k=1, batch_size=10)\ndel classifier # Delete the classifier to free up GPU memory.\noutputs\n```\n\n----------------------------------------\n\nTITLE: Initializing Modin with Ray for Pandas Operations\nDESCRIPTION: Shows how to import Modin's pandas implementation and initialize a Ray instance for distributed dataframe processing. This basic example demonstrates loading a Parquet file from S3 using Modin's pandas replacement.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/modin/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport modin.pandas as pd\nimport ray\n\nray.init()\ndf = pd.read_parquet(\"s3://my-bucket/big.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Defining Python module for cross-language use\nDESCRIPTION: Defines a Python module with a remote function and a remote class that will be called from Java in a cross-language context.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\ndef add(a, b):\n    return a + b\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Serve LLM Deployment for Qwen-1.5B Model\nDESCRIPTION: YAML configuration for deploying the Qwen-1.5B model using Ray Serve LLM. Specifies model loading details, accelerator type, and autoscaling parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_loading_config:\n  model_id: qwen-1.5b\n  model_source: Qwen/Qwen2.5-1.5B-Instruct\naccelerator_type: A10G\ndeployment_config:\n  autoscaling_config:\n    min_replicas: 1\n    max_replicas: 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray DDP Strategy for PyTorch Lightning\nDESCRIPTION: Shows how to configure the distributed strategy for PyTorch Lightning with Ray Train. Ray Train offers specialized strategy classes like RayDDPStrategy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\n import lightning.pytorch as pl\n-from pl.strategies import DDPStrategy\n+import ray.train.lightning\n\n def train_func():\n     ...\n     trainer = pl.Trainer(\n         ...\n-        strategy=DDPStrategy(),\n+        strategy=ray.train.lightning.RayDDPStrategy(),\n         ...\n     )\n     ...\n```\n\n----------------------------------------\n\nTITLE: Tuning PyTorch Models with Optuna in Ray Tune\nDESCRIPTION: Example showing how to tune a PyTorch model using Optuna search algorithm within Ray Tune, focusing on optimizing momentum and learning rate parameters of the model's optimizer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/index.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef objective(config):\n    model = ConvNet()\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=config[\"lr\"],\n        momentum=config[\"momentum\"]\n    )\n    for i in range(10):\n        train(model, optimizer)\n        acc = test(model)\n        tune.report(mean_accuracy=acc)\n\nsearch_space = {\n    \"lr\": tune.loguniform(1e-5, 1e-1),\n    \"momentum\": tune.uniform(0.1, 0.9)\n}\n\noptuna_search = OptunaSearch(\n    metric=\"mean_accuracy\",\n    mode=\"max\"\n)\n\nresults = tune.run(\n    objective,\n    search_alg=optuna_search,\n    config=search_space,\n    num_samples=20,\n    metric=\"mean_accuracy\",\n    mode=\"max\",\n    stop={\"training_iteration\": 5}\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Resource Allocation for Trials\nDESCRIPTION: Shows how to specify custom CPU resources per trial using tune.with_resources(), including examples of full CPU, fractional CPU, and dynamic resource allocation based on config.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-resources.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# If you have 4 CPUs on your machine, this will run 2 concurrent trials at a time.\ntrainable_with_resources = tune.with_resources(trainable, {\"cpu\": 2})\ntuner = tune.Tuner(\n    trainable_with_resources,\n    tune_config=tune.TuneConfig(num_samples=10)\n)\nresults = tuner.fit()\n\n# If you have 4 CPUs on your machine, this will run 1 trial at a time.\ntrainable_with_resources = tune.with_resources(trainable, {\"cpu\": 4})\ntuner = tune.Tuner(\n    trainable_with_resources,\n    tune_config=tune.TuneConfig(num_samples=10)\n)\nresults = tuner.fit()\n\n# Fractional values are also supported, (i.e., {\"cpu\": 0.5}).\n# If you have 4 CPUs on your machine, this will run 8 concurrent trials at a time.\ntrainable_with_resources = tune.with_resources(trainable, {\"cpu\": 0.5})\ntuner = tune.Tuner(\n    trainable_with_resources,\n    tune_config=tune.TuneConfig(num_samples=10)\n)\nresults = tuner.fit()\n\n# Custom resource allocation via lambda functions are also supported.\n# If you want to allocate gpu resources to trials based on a setting in your config\ntrainable_with_resources = tune.with_resources(trainable,\n    resources=lambda spec: {\"gpu\": 1} if spec.config.use_gpu else {\"gpu\": 0})\ntuner = tune.Tuner(\n    trainable_with_resources,\n    tune_config=tune.TuneConfig(num_samples=10)\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining FastAPI HTTP Deployments - Python\nDESCRIPTION: This code snippet illustrates how to define a Serve deployment using the @serve.ingress decorator to integrate FastAPI. It highlights the routing capabilities and respects the route_prefix set through Serve.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"{literalinclude} doc_code/http_guide/http_guide.py\\n:start-after: __begin_fastapi__\\n:end-before: __end_fastapi__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Hyperparameter Search Space\nDESCRIPTION: Defines the search space for hyperparameters, including layer dimensions, learning rate, and batch size. Uses Ray Tune's sampling functions to create an effective search space.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsearch_space = {\n    \"layer_1_size\": tune.choice([32, 64, 128]),\n    \"layer_2_size\": tune.choice([64, 128, 256]),\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"batch_size\": tune.choice([32, 64]),\n}\n```\n\n----------------------------------------\n\nTITLE: Synchronous Training Setup\nDESCRIPTION: Initialization code for synchronous parameter server training with Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_parameter_server.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\niterations = 200\nnum_workers = 2\n\nray.init(ignore_reinit_error=True)\nps = ParameterServer.remote(1e-2)\nworkers = [DataWorker.remote() for i in range(num_workers)]\n\nmodel = ConvNet()\ntest_loader = get_data_loader()[1]\n```\n\n----------------------------------------\n\nTITLE: Custom PyTorch RLModule Implementation\nDESCRIPTION: Example of implementing a custom PyTorch-based RLModule for use with RLlib. Implements a simple policy network with configurable hidden dimensions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom ray.rllib.core.columns import Columns\nfrom ray.rlib.core.rl_module.torch import TorchRLModule\n\n# Define your custom env class by subclassing `TorchRLModule`:\nclass CustomTorchRLModule(TorchRLModule):\n    def setup(self):\n        # You have access here to the following already set attributes:\n        # self.observation_space\n        # self.action_space\n        # self.inference_only\n        # self.model_config  # <- a dict with custom settings\n        input_dim = self.observation_space.shape[0]\n        hidden_dim = self.model_config[\"hidden_dim\"]\n        output_dim = self.action_space.n\n\n        # Define and assign your torch subcomponents.\n        self._policy_net = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, output_dim),\n        )\n\n    def _forward(self, batch, **kwargs):\n        # Push the observations from the batch through our `self._policy_net`.\n        action_logits = self._policy_net(batch[Columns.OBS])\n        # Return parameters for the default action distribution, which is\n        # `TorchCategorical` (due to our action space being `gym.spaces.Discrete`).\n        return {Columns.ACTION_DIST_INPUTS: action_logits}\n```\n\n----------------------------------------\n\nTITLE: Creating and Submitting a Production Job for Language Model Training (Bash)\nDESCRIPTION: These bash commands show how to create a job YAML file for a 7B language model and submit it as a production job on the Anyscale platform.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython create_job_yaml.py --size=7b --output-path=./job.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nanyscale job submit job.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a Search Space for Hyperparameters in Python\nDESCRIPTION: This snippet creates a search space for the hyperparameters with defined ranges and choices. It sets the number of steps and specifies the ranges for width and height along with activation function choices.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsearch_space = {\n    \"steps\": 100,\n    \"width\": tune.uniform(0, 20),\n    \"height\": tune.uniform(-100, 100),\n    \"activation\": tune.choice([\"relu\", \"tanh\"]),\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Ray Train Function for Vicuna-13B Fine-tuning\nDESCRIPTION: This snippet defines the training function used by Ray Train to fine-tune the Vicuna-13B model. It sets up the PyTorch Lightning trainer with Ray-specific components and prepares the data ingestion using Ray Data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray.train\nfrom ray.train import CheckpointConfig, RunConfig, ScalingConfig\nfrom ray.train.torch import TorchTrainer\nfrom ray.train.lightning import (\n    prepare_trainer,\n    RayDeepSpeedStrategy, \n    RayLightningEnvironment, \n    RayTrainReportCallback\n)\n\n\ndef train_func(config):\n    \"\"\"Training function for each worker.\"\"\"\n\n    # Unpack the `train_loop_config`\n    max_epochs = config[\"max_epochs\"]\n    batch_size = config[\"batch_size\"]\n    accumulate_grad_batches = config[\"accumulate_grad_batches\"]\n\n    model = Vicuna13BModel()\n    \n    # Prepare Ray Data Ingestion\n    train_ds = ray.train.get_dataset_shard(\"train\")\n    train_dataloader = train_ds.iter_torch_batches(batch_size=batch_size)\n    \n    pl_trainer = pl.Trainer(\n        devices=\"auto\",\n        accelerator=\"auto\",\n        strategy=RayDeepSpeedStrategy(config=deepspeed_configs),\n        plugins=[RayLightningEnvironment()],\n        callbacks=[RayTrainReportCallback()],\n        enable_checkpointing=False, # RayTrainReportCallback will save the checkpoints\n        max_epochs=max_epochs,\n        precision=\"bf16-mixed\",\n        accumulate_grad_batches=accumulate_grad_batches,\n    )\n    pl_trainer = prepare_trainer(pl_trainer)\n\n    pl_trainer.fit(model, train_dataloaders=train_dataloader)\n    \n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func,\n    train_loop_config={\n        \"max_epochs\": 1,\n        \"batch_size\": BATCH_SIZE_PER_WORKER,\n        \"accumulate_grad_batches\": 2\n    },\n    run_config=RunConfig(\n        name=\"vicuna-13b-finetune\",\n        storage_path=\"s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/air-release-tests\",\n        checkpoint_config=CheckpointConfig(num_to_keep=1),\n    ),\n    scaling_config=ScalingConfig(\n        num_workers=NUM_WORKERS,\n        use_gpu=True,\n        resources_per_worker={\"CPU\": 15, \"GPU\": 1},\n    ),\n    datasets={\"train\": processed_ds},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining the evaluation function\nDESCRIPTION: Defines a simple evaluation function that simulates a long-running ML experiment.  It takes hyperparameters (width, height, activation) as input and returns a score. A delay is added to simulate experiment duration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"def evaluate(step, width, height, activation):\\n    time.sleep(0.1)\\n    activation_boost = 10 if activation==\\\"relu\\\" else 1\\n    return (0.1 + width * step / 100) ** (-1) + height * 0.1 + activation_boost\"\n```\n\n----------------------------------------\n\nTITLE: Inspecting Ray Train Checkpoints After Training\nDESCRIPTION: This code shows how to access and inspect checkpoints after training using the Checkpoint.as_directory and Checkpoint.to_directory APIs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Get the latest checkpoint\ncheckpoint = result.checkpoint\n\n# Access checkpoint as a read-only directory\nwith checkpoint.as_directory() as ckpt_dir:\n    print(os.listdir(ckpt_dir))\n\n# Copy checkpoint to a writable directory\ncheckpoint.to_directory(\"/tmp/checkpoint\")\nprint(os.listdir(\"/tmp/checkpoint\"))\n```\n\n----------------------------------------\n\nTITLE: Splitting Datasets for Distributed Parallel Training in Ray\nDESCRIPTION: Demonstrates how to split a dataset into disjoint shards for distributed data parallel training using Ray Data's streaming_split() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/iterating-over-data.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\nclass Worker:\n\n    def train(self, data_iterator):\n        for batch in data_iterator.iter_batches(batch_size=8):\n            pass\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\nworkers = [Worker.remote() for _ in range(4)]\nshards = ds.streaming_split(n=4, equal=True)\nray.get([w.train.remote(s) for w, s in zip(workers, shards)])\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Training Function for Forecasting Models\nDESCRIPTION: Implements a custom training function that fits forecasting models to dataset partitions, computes evaluation metrics, and reports results to Ray Tune. It includes helper functions for data fetching and cross-validation evaluation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/start.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nn_cv_windows = 1\n\nmodels = [\n    AutoETS(season_length=24),\n    MSTL(season_length=24, trend_forecaster=AutoARIMA()),\n]\n\ncpus_per_trial = len(models) * n_cv_windows\n\n\ndef train_fn(config: dict):\n    def get_m5_partition(unique_id: str) -> pd.DataFrame:\n        df = pd.read_parquet(\n            \"https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet\"\n        )\n        df = df[df[\"unique_id\"] == unique_id]\n        return df.dropna()\n\n    def evaluate_cross_validation(df: pd.DataFrame) -> pd.DataFrame:\n        from sklearn.metrics import mean_squared_error\n\n        models = df.drop(columns=[\"ds\", \"cutoff\", \"y\"]).columns.tolist()\n        evals = []\n        for model in models:\n            eval_ = (\n                df.groupby([\"unique_id\", \"cutoff\"])\n                .apply(\n                    lambda x: mean_squared_error(\n                        x[\"y\"].values, x[model].values, squared=False\n                    )\n                ).to_frame()\n            )\n            eval_.columns = [model]\n            evals.append(eval_)\n        evals = pd.concat(evals, axis=1)\n        evals = evals.groupby([\"unique_id\"]).mean(numeric_only=True)\n        evals[\"best_model\"] = evals.idxmin(axis=1)\n        return evals\n\n    data_partition_id = config[\"data_partition_id\"]\n    train_df = get_m5_partition(data_partition_id)\n\n    forecast_horizon = 24\n\n    sf = StatsForecast(\n        df=train_df,\n        models=models,\n        freq=\"H\",\n        n_jobs=cpus_per_trial,\n    )\n    cv_df = sf.cross_validation(\n        h=forecast_horizon,\n        step_size=forecast_horizon,\n        n_windows=n_cv_windows,\n    )\n\n    eval_df = evaluate_cross_validation(df=cv_df)\n    best_model = eval_df[\"best_model\"][data_partition_id]\n    forecast_mse = eval_df[best_model][data_partition_id]\n\n    if data_partition_id == \"H1\":\n        forecast_df = sf.forecast(h=forecast_horizon)\n        fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n        plot_df = pd.concat([train_df, forecast_df]).set_index(\"ds\")\n        plot_df[[\"y\", best_model]].plot(ax=ax)\n        ax.set_title(f\"Forecast for data partition: {data_partition_id}\")\n        ax.set_xlabel(f\"Timestamp [ds]\")\n        ax.set_ylabel(f\"Target [y]\")\n        ax.get_figure().savefig(\"prediction.png\")\n\n    train.report({\"forecast_mse\": forecast_mse, \"best_model\": best_model})\n\n\ntrainable = tune.with_resources(train_fn, resources={\"CPU\": cpus_per_trial})\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Samples - Python\nDESCRIPTION: This snippet defines the number of samples for hyperparameter tuning, indicating how many different configurations will be evaluated during the tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 1000\n```\n\n----------------------------------------\n\nTITLE: Stopping Ray Tune Experiment by Time\nDESCRIPTION: Uses the TuneConfig time_budget_s parameter to stop the entire experiment after a specified amount of time.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-stopping.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    train_function,\n    tune_config=tune.TuneConfig(\n        num_samples=5,\n        time_budget_s=30\n    ),\n    param_space={\"base_accuracy\": 0.3}\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Applying Batch Inference to Full Dataset using Ray Data\nDESCRIPTION: Uses Ray Data's map_batches API to apply the ImageClassifier to the entire dataset in parallel across multiple GPUs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictions = ds.map_batches(\n    ImageClassifier,\n    concurrency=4, # Use 4 model replicas. Change this number based on the number of GPUs in your cluster.\n    num_gpus=1 if torch.cuda.is_available() else 0,  # Specify GPUs per model replica (use 0 for CPU inference)\n    batch_size=BATCH_SIZE # Use batch size from above.\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Trip Booking Workflow with Ray\nDESCRIPTION: This example demonstrates a more complex workflow for booking a trip, including parallel execution of flight and hotel bookings. It shows how to create nested workflows and manage task dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef book_flight(...) -> Flight: ...\n\n@ray.remote\ndef book_hotel(...) -> Hotel: ...\n\n@ray.remote\ndef finalize_or_cancel(\n    flights: List[Flight],\n    hotels: List[Hotel]) -> Receipt: ...\n\n@ray.remote\ndef book_trip(origin: str, dest: str, dates) -> Receipt:\n    # Note that the workflow engine will not begin executing\n    # child workflows until the parent task returns.\n    # This avoids task overlap and ensures recoverability.\n    f1 = book_flight.bind(origin, dest, dates[0])\n    f2 = book_flight.bind(dest, origin, dates[1])\n    hotel = book_hotel.bind(dest, dates)\n    return workflow.continuation(finalize_or_cancel.bind([f1, f2], [hotel]))\n\nreceipt: Receipt = workflow.run(book_trip.bind(\"OAK\", \"SAN\", [\"6/12\", \"7/5\"]))\n```\n\n----------------------------------------\n\nTITLE: Migrating ModelV2 to RLModule by Policy Checkpoint\nDESCRIPTION: Illustrates migrating a ModelV2 setup to the new RLModule API stack by loading a policy checkpoint, ensuring continuity of learned parameters during migration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Migrate ModelV2 to RLModule by Policy Checkpoint\n# This script completes migration using policy checkpoints.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Better Approach: Batch Processing with ray.get and ray.wait in Python\nDESCRIPTION: This code snippet shows a better approach to fetching and processing multiple objects in Ray. It uses batching with ray.get() and ray.wait() to process results in the order of completion, reducing memory usage and improving efficiency.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/ray-get-too-many-objects.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nobject_ids = [long_running_task.remote() for _ in range(10000)]\nwhile object_ids:\n    done, object_ids = ray.wait(object_ids)\n    results = ray.get(done)\n    for result in results:\n        process_result(result)\n```\n\n----------------------------------------\n\nTITLE: Defining Training Loop for Object Detection Model\nDESCRIPTION: Implements the training loop for fine-tuning the fasterrcnn_resnet50_fpn model, including data preparation, loss calculation, and optimization steps.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_detection.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nfrom torchvision import models\nfrom tempfile import TemporaryDirectory\nfrom ray import train\n\ndef train_one_epoch(*, model, optimizer, batch_size, epoch):\n    model.train()\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1.0 / 1000\n        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n            optimizer, start_factor=warmup_factor, total_iters=250\n        )\n\n    device = ray.train.torch.get_device()\n    train_dataset_shard = train.get_dataset_shard(\"train\")\n\n    batches = train_dataset_shard.iter_batches(batch_size=batch_size)\n    for batch in batches:\n        inputs = [torch.as_tensor(image).to(device) for image in batch[\"image\"]]\n\n        targets = []\n        for i in range(len(batch[\"boxes\"])):\n            # `boxes` is a (B, 4) tensor, where B is the number of boxes in the image.\n            boxes = torch.as_tensor([box for box in batch[\"boxes\"][i]]).to(device)\n            # `labels` is a (B,) tensor, where B is the number of boxes in the image.\n            labels = torch.as_tensor(batch[\"labels\"][i]).to(device)\n            targets.append({\"boxes\": boxes, \"labels\": labels})\n\n        loss_dict = model(inputs, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        train.report(\n            {\n                \"losses\": losses.item(),\n                \"epoch\": epoch,\n                \"lr\": optimizer.param_groups[0][\"lr\"],\n                **{key: value.item() for key, value in loss_dict.items()},\n            }\n        )\n\ndef train_loop_per_worker(config):\n    # By default, `fasterrcnn_resnet50_fpn`'s backbone is pre-trained on ImageNet.\n    model = models.detection.fasterrcnn_resnet50_fpn(num_classes=3)\n    model = ray.train.torch.prepare_model(model)\n    parameters = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(\n        parameters,\n        lr=config[\"lr\"],\n        momentum=config[\"momentum\"],\n        weight_decay=config[\"weight_decay\"],\n    )\n    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n        optimizer, milestones=config[\"lr_steps\"], gamma=config[\"lr_gamma\"]\n    )\n\n    for epoch in range(0, config[\"epochs\"]):\n        train_one_epoch(\n            model=model,\n            optimizer=optimizer,\n            batch_size=config[\"batch_size\"],\n            epoch=epoch,\n        )\n        lr_scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Setting up Sender and Receiver Actors for GPU Communication in Ray\nDESCRIPTION: This code snippet demonstrates how to create sender and receiver actors for GPU-to-GPU communication in Ray. It requires at least 2 GPUs and uses Ray's remote decorators to specify GPU resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/quickstart.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(num_gpus=1)\nclass Sender:\n    def send(self):\n        return torch.ones(10, device=\"cuda\")\n\n@ray.remote(num_gpus=1)\nclass Receiver:\n    def receive(self, x):\n        return x.to(\"cuda\")\n\nsender = Sender.remote()\nreceiver = Receiver.remote()\n```\n\n----------------------------------------\n\nTITLE: Running AutoML for Time Series Forecasting\nDESCRIPTION: This code snippet demonstrates how to use the implemented AutoML system to evaluate multiple time series models on a dataset, optimizing for mean squared error.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/automl_for_time_series.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = get_m5_partition(\"FOODS_1_001_CA_1\")\ndf\n\ntuning_results = evaluate_search_space_with_cv(\n    {AutoARIMA: {}, ETS: {\"season_length\": [6, 7], \"model\": [\"ZNA\", \"ZZZ\"]}},\n    df,\n    \"y\",\n    {\"mse\": mean_squared_error, \"mae\": mean_absolute_error},\n    \"mse\",\n    cv=TimeSeriesSplit(test_size=1),\n)\n\nprint(tuning_results[0])\n\n# Print arguments of the model:\nprint(tuning_results[0][0].__dict__)\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Device Plugin and Verifying GPU Nodes\nDESCRIPTION: Commands to install the NVIDIA device plugin DaemonSet and verify GPU allocation on cluster nodes. Includes verification of node GPU status using kubectl custom columns output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Install the DaemonSet\nkubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.9.0/nvidia-device-plugin.yml\n\n# Verify that your nodes have allocatable GPUs. If the GPU node fails to detect GPUs,\n# please verify whether the DaemonSet schedules the Pod on the GPU node.\nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n\n# Example output:\n# NAME                                GPU\n# ip-....us-west-2.compute.internal   4\n# ip-....us-west-2.compute.internal   <none>\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataset from Python Objects with Ray Data\nDESCRIPTION: Shows how to create a Ray Data Dataset from a list of Python dictionaries using the from_items function. Each dictionary is treated as a row in the resulting dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.from_items([\n    {\"food\": \"spam\", \"price\": 9.34},\n    {\"food\": \"ham\", \"price\": 5.37},\n    {\"food\": \"eggs\", \"price\": 0.94}\n])\n\nprint(ds)\n```\n\n----------------------------------------\n\nTITLE: Creating GKE Cluster with GPU Nodes for LLM Deployment\nDESCRIPTION: Creates a Google Kubernetes Engine (GKE) cluster with L4 GPUs for tensor parallelism in vLLM. The cluster includes g2-standard-24 instances with 2 NVIDIA L4 GPUs each across two nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/vllm-rayservice.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container clusters create kuberay-gpu-cluster \\\n    --machine-type=g2-standard-24 \\\n    --location=us-east4-c \\\n    --num-nodes=2 \\\n    --accelerator=type=nvidia-l4,count=2,gpu-driver-version=latest\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate with DeepSpeed Plugin\nDESCRIPTION: Example showing how to configure Hugging Face Accelerate with DeepSpeed for distributed training. Creates a DeepSpeedPlugin from a configuration dictionary and passes it to the Accelerator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/huggingface-accelerate.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator, DeepSpeedPlugin\n\nDEEPSPEED_CONFIG = {\n    \"fp16\": {\n        \"enabled\": True\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": False\n        },\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"gather_16bit_weights_on_model_save\": True,\n        \"round_robin_gradients\": True\n    },\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 10,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": False\n}\n\ndef train_func():\n    # Create a DeepSpeedPlugin from config dict\n    ds_plugin = DeepSpeedPlugin(hf_ds_config=DEEPSPEED_CONFIG)\n\n    # Initialize Accelerator\n    accelerator = Accelerator(\n        ...,\n        deepspeed_plugin=ds_plugin,\n    )\n\n    # Start training\n    ...\n\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(...),\n    run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n    ...\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray TorchTrainer for Stable Diffusion Fine-Tuning on HPUs\nDESCRIPTION: This snippet sets up Ray for distributed training of Stable Diffusion on Habana HPUs. It configures the scaling and torch settings, and initializes the TorchTrainer with the main training loop.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/sd.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    import ray\n    from ray import train\n    from ray.train import ScalingConfig, Checkpoint, CheckpointConfig, RunConfig\n    from ray.train.torch import TorchTrainer, TorchConfig\n\n    ray.init()\n\n    # Configure computation resources\n    # In ScalingConfig, require an HPU for each worker\n    scaling_config = ScalingConfig(num_workers=1, resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n    # Set backend to hccl in TorchConfig\n    torch_config = TorchConfig(backend = \"hccl\")\n    # Initialize a Ray TorchTrainer\n    trainer = TorchTrainer(\n        train_loop_per_worker=main,\n\t\ttrain_loop_config={\"args\": parse_args()},\n        torch_config=torch_config,\n        scaling_config=scaling_config,\n    )\n\n    result = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Logging with Comet in PyTorch Lightning\nDESCRIPTION: This snippet shows how to integrate Comet for logging experiments in PyTorch Lightning with Ray Train's TorchTrainer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. dropdown:: Comet\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking/lightning_exp_tracking_model_dl.py\n        :language: python\n        :start-after: __model_dl_start__\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking/lightning_exp_tracking_comet.py\n        :language: python\n        :start-after: __lightning_experiment_tracking_comet_start__\n```\n\n----------------------------------------\n\nTITLE: Retrieving Workflow Metadata in Python\nDESCRIPTION: Example demonstrating how to retrieve metadata for a workflow using the workflow.get_metadata() function. The example runs a simple addition workflow and then retrieves and verifies its metadata including status and timing information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/metadata.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import workflow\n\n@ray.remote\ndef add(left: int, right: int) -> int:\n    return left + right\n\nworkflow.run(add.bind(10, 20), workflow_id=\"add_example\")\n\nworkflow_metadata = workflow.get_metadata(\"add_example\")\n\nassert workflow_metadata[\"status\"] == \"SUCCESSFUL\"\nassert \"start_time\" in workflow_metadata[\"stats\"]\nassert \"end_time\" in workflow_metadata[\"stats\"]\n```\n\n----------------------------------------\n\nTITLE: Generating TensorBoard HParams Output with TensorFlow 2.x\nDESCRIPTION: Code example showing how to configure a Tune experiment to automatically generate TensorBoard HParams output when using TensorFlow 2.x, using grid search for hyperparameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    ...,\n    param_space={\n        \"lr\": tune.grid_search([1e-5, 1e-4]),\n        \"momentum\": tune.grid_search([0, 0.9])\n    }\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Configure PopulationBasedTraining scheduler\nDESCRIPTION: This configures the PopulationBasedTraining (PBT) scheduler with specific parameters such as the perturbation interval, metric, mode, quantile fraction, resample probability, and hyperparameter mutations. The `synch=True` parameter is set for synchronous PBT, which is helpful for visualization but can slow down the optimization process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"perturbation_interval = 4\n\npbt_scheduler = PopulationBasedTraining(\n    time_attr=\\\"training_iteration\\\",\n    perturbation_interval=perturbation_interval,\n    metric=\\\"Q\\\",\n    mode=\\\"max\\\",\n    quantile_fraction=0.5,\n    resample_probability=0.5,\n    hyperparam_mutations={\n        \\\"lr\\\": tune.qloguniform(5e-3, 1e-1, 5e-4),\n        \\\"h0\\\": tune.uniform(0.0, 1.0),\n        \\\"h1\\\": tune.uniform(0.0, 1.0),\n    },\n    synch=True,\n)\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Stable Diffusion Model Deployment\nDESCRIPTION: Defines a Ray Serve deployment class that loads and serves the Stable Diffusion model with GPU acceleration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/start.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(\n    ray_actor_options={\"num_gpus\": 1},\n    num_replicas=NUM_REPLICAS,\n)\nclass StableDiffusionV2:\n    def __init__(self):\n        # <Replace with your own model loading logic>\n        import torch\n        from diffusers import EulerDiscreteScheduler, StableDiffusionPipeline\n\n        model_id = \"stabilityai/stable-diffusion-2\"\n        scheduler = EulerDiscreteScheduler.from_pretrained(\n            model_id, subfolder=\"scheduler\"\n        )\n        self.pipe = StableDiffusionPipeline.from_pretrained(\n            model_id, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16\n        )\n        self.pipe = self.pipe.to(\"cuda\")\n\n    def generate(self, prompt: str, img_size: int = 776):\n        # <Replace with your own model inference logic>\n        assert len(prompt), \"prompt parameter cannot be empty\"\n        image = self.pipe(prompt, height=img_size, width=img_size).images[0]\n        return image\n```\n\n----------------------------------------\n\nTITLE: Configuring ASHA Scheduler for Hyperparameter Tuning in Python\nDESCRIPTION: Sets up an Asynchronous Hyperband Scheduler (ASHA) that automatically terminates poorly performing trials during hyperparameter optimization to save resources. The scheduler runs for a maximum of 10 epochs with an initial grace period of 1 epoch.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnum_epochs = 10\n\nscheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n```\n\n----------------------------------------\n\nTITLE: Defining an Actor Class for Profiling\nDESCRIPTION: Example of creating a Ray actor class named 'Sleeper' with a method that introduces an artificial delay. This is used to demonstrate how to profile actor performance with cProfile.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/optimize-performance.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Our actor\n@ray.remote\nclass Sleeper:\n    def __init__(self):\n        self.sleepValue = 0.5\n\n    # Equivalent to func(), but defined within an actor\n    def actor_func(self):\n        time.sleep(self.sleepValue)\n```\n\n----------------------------------------\n\nTITLE: Defining the objective function\nDESCRIPTION: This snippet defines the objective function that will be optimized by Ray Tune. It takes a Tune config, evaluates the score of the experiment in a training loop, and reports the score back to Tune using tune.report.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef objective(config):\n    for step in range(config[\"steps\"]):\n        score = evaluate(step, config[\"width\"], config[\"height\"])\n        tune.report({\"iterations\": step, \"mean_loss\": score})\n\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Tuning with Ray Tune for XGBoost\nDESCRIPTION: This snippet shows how to use Ray Tune to perform hyperparameter optimization for XGBoost. It defines a training function, sets up a search space for hyperparameters, and runs the tuning process with 10 trials. The code includes loading the breast cancer dataset, splitting it into train and test sets, and training the XGBoost model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-xgboost.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport sklearn.datasets\nimport sklearn.metrics\n\nfrom ray import tune\n\n\ndef train_breast_cancer(config):\n    # Load dataset\n    data, labels = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    # Split into train and test set\n    train_x, test_x, train_y, test_y = train_test_split(data, labels, test_size=0.25)\n    # Build input matrices for XGBoost\n    train_set = xgb.DMatrix(train_x, label=train_y)\n    test_set = xgb.DMatrix(test_x, label=test_y)\n    # Train the classifier\n    results = {}\n    xgb.train(\n        config,\n        train_set,\n        evals=[(test_set, \"eval\")],\n        evals_result=results,\n        verbose_eval=False,\n    )\n    # Return prediction accuracy\n    accuracy = 1.0 - results[\"eval\"][\"error\"][-1]\n    tune.report({\"mean_accuracy\": accuracy, \"done\": True})\n\n\nconfig = {\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": [\"logloss\", \"error\"],\n    \"max_depth\": tune.randint(1, 9),\n    \"min_child_weight\": tune.choice([1, 2, 3]),\n    \"subsample\": tune.uniform(0.5, 1.0),\n    \"eta\": tune.loguniform(1e-4, 1e-1),\n}\ntuner = tune.Tuner(\n    train_breast_cancer,\n    tune_config=tune.TuneConfig(num_samples=10),\n    param_space=config,\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Preparing PyTorch Lightning Trainer for Ray Train\nDESCRIPTION: This diff shows how to prepare a PyTorch Lightning Trainer for use with Ray Train by validating its configuration using the ray.train.lightning.prepare_trainer method. This step ensures compatibility between Lightning and Ray Train.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_8\n\nLANGUAGE: diff\nCODE:\n```\nimport lightning.pytorch as pl\nimport ray.train.lightning\n\ndef train_func():\n    ...\n    trainer = pl.Trainer(...)\n+    trainer = ray.train.lightning.prepare_trainer(trainer)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Threaded Actors in Ray\nDESCRIPTION: Shows how to use the max_concurrency Actor option without async methods to achieve threaded concurrency in Ray actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/async_api.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass ThreadedActor:\n    def task_1(self): print(\"I'm running in a thread!\")\n    def task_2(self): print(\"I'm running in another thread!\")\n\na = ThreadedActor.options(max_concurrency=2).remote()\nray.get([a.task_1.remote(), a.task_2.remote()])\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Image Datasets with Ray Data in Python\nDESCRIPTION: This snippet demonstrates how to load image datasets using Ray Data's read_images function, tokenize prompts, and apply image preprocessing using torchvision.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/dreambooth_finetuning.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninstance_dataset = read_images(instance_data_dir, mode=\"RGB\")\nclass_dataset = read_images(class_data_dir, mode=\"RGB\")\n\ntokenizer = AutoTokenizer.from_pretrained(\n    pretrained_model_name_or_path,\n    subfolder=\"tokenizer\",\n    revision=revision,\n)\ninstance_prompt_ids = _tokenize(tokenizer, instance_prompt)\n```\n\nLANGUAGE: python\nCODE:\n```\npreprocess = transforms.Compose(\n    [\n        transforms.Resize(\n            resize_dim, interpolation=transforms.InterpolationMode.BILINEAR\n        ),\n        transforms.CenterCrop(crop_dim),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ninstance_dataset = instance_dataset.map_batches(\n    lambda x: {\"instance_images\": preprocess(x[\"image\"])},\n    batch_format=\"pandas\",\n)\nclass_dataset = class_dataset.map_batches(\n    lambda x: {\"class_images\": preprocess(x[\"image\"])},\n    batch_format=\"pandas\",\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Ray Train Structure for Distributed Transformers Training\nDESCRIPTION: Minimal code structure demonstrating how to set up a TorchTrainer with a training function and scaling configuration for distributed Transformers training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-transformers.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig\n\ndef train_func():\n    # Your Transformers training code here\n    ...\n\nscaling_config = ScalingConfig(num_workers=2, use_gpu=True)\ntrainer = TorchTrainer(train_func, scaling_config=scaling_config)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining Remote Pi Calculation Task\nDESCRIPTION: Creates a Ray remote task that performs Monte Carlo estimation of pi/4 by randomly sampling points and checking if they fall within a quarter circle.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/highly_parallel.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef pi4_sample(sample_count):\n    \"\"\"pi4_sample runs sample_count experiments, and returns the \n    fraction of time it was inside the circle. \n    \"\"\"\n    in_count = 0\n    for i in range(sample_count):\n        x = random.random()\n        y = random.random()\n        if x*x + y*y <= 1:\n            in_count += 1\n    return Fraction(in_count, sample_count)\n```\n\n----------------------------------------\n\nTITLE: Using Search Algorithms with Ray Tune\nDESCRIPTION: Demonstrates how to use a search algorithm (OptunaSearch) with Ray Tune to optimize hyperparameters. The example defines a simple training function that reports a loss metric, and sets up a tuner with OptunaSearch to minimize this metric over 100 samples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/suggestion.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.tune.search.optuna import OptunaSearch\n\ndef train_fn(config):\n    # This objective function is just for demonstration purposes\n    tune.report({\"loss\": config[\"param\"]})\n\ntuner = tune.Tuner(\n    train_fn,\n    tune_config=tune.TuneConfig(\n        search_alg=OptunaSearch(),\n        num_samples=100,\n        metric=\"loss\",\n        mode=\"min\",\n    ),\n    param_space={\"param\": tune.uniform(0, 1)},\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: GPU Packing Strategy in Ray with Fractional Resources\nDESCRIPTION: Demonstrates Ray's GPU packing strategy with fractional resources. When assigning accelerators to tasks with fractional requirements, Ray packs one accelerator before moving to the next to avoid fragmentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nray.init(num_gpus=3)\n\n@ray.remote(num_gpus=0.5)\nclass FractionalGPUActor:\n    def ping(self):\n        print(\"GPU id: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"GPU\"]))\n\nfractional_gpu_actors = [FractionalGPUActor.remote() for _ in range(3)]\n# Ray tries to pack GPUs if possible.\n[ray.get(fractional_gpu_actors[i].ping.remote()) for i in range(3)]\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing Function\nDESCRIPTION: Defining a preprocessing function that transforms images for model input while preserving the original image.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/pytorch_resnet_batch_prediction.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom typing import Dict\n\ndef preprocess_image(row: Dict[str, np.ndarray]):\n    return {\n        \"original_image\": row[\"image\"],\n        \"transformed_image\": transform(row[\"image\"]),\n    }\n```\n\n----------------------------------------\n\nTITLE: Nevergrad Example\nDESCRIPTION: This Python script demonstrates how to use Nevergrad with Ray Tune for hyperparameter optimization. It uses Nevergrad's optimizers for searching the hyperparameter space of a simple training function, leveraging Ray Tune's distributed execution for faster experimentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/nevergrad_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"/../../python/ray/tune/examples/nevergrad_example.py\"\n```\n\n----------------------------------------\n\nTITLE: Implementing the Objective Function for Ray Tune\nDESCRIPTION: Defines the objective function that will be optimized by Ray Tune. It takes a configuration, evaluates the landscape function, and reports metrics back to Tune at each iteration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef objective(config):\n    for i in range(config[\"iterations\"]):\n        x = np.array([config.get(\"x{}\".format(i + 1)) for i in range(6)])\n        tune.report(\n            {\"timesteps_total\": i, \"landscape\": landscape(x), \"l2norm\": np.sqrt((x ** 2).sum())}\n        )\n        time.sleep(0.02)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Ray and LightGBM\nDESCRIPTION: Imports necessary modules from Ray for data processing, training, and LightGBM integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightgbm/lightgbm_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple\n\nimport ray\nfrom ray.data import Dataset, Preprocessor\nfrom ray.data.preprocessors import Categorizer, StandardScaler\nfrom ray.train.lightgbm import LightGBMTrainer\nfrom ray.train import Result, ScalingConfig\n```\n\n----------------------------------------\n\nTITLE: Defining the Ray Tune Trainable Function\nDESCRIPTION: Creates a trainable function for Ray Tune that loads data for a specific location, trains a model, evaluates it, and checkpoints the result.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n############\n# STEP 1.  Define Python functions to\n#          b) train and evaluate a model on a segment of data.\n############\ndef train_model(config: dict) -> None:\n\n    algorithm = config[\"algorithm\"]\n    sample_location_id = config[\"location\"]\n\n    # Load data.\n    df_list = [read_data(f, sample_location_id) for f in s3_files]\n    df_raw = pd.concat(df_list, ignore_index=True)\n\n    # Transform data.\n    df = transform_df(df_raw)\n\n    # We need at least 10 rows to create a train / test split.\n    if df.shape[0] < 10:\n        print_time(f\"Location {sample_location_id} has only {df.shape[0]} rows.\")\n        tune.report(dict(error=None))\n        return None\n\n    # Train/valid split.\n    train_df, valid_df = train_test_split(df, test_size=0.2, shuffle=True)\n    train_X = train_df[[\"passenger_count\", \"trip_distance\"]]\n    train_y = train_df[TARGET]\n    valid_X = valid_df[[\"passenger_count\", \"trip_distance\"]]\n    valid_y = valid_df[TARGET]\n\n    # Train model.\n    model = algorithm.fit(train_X, train_y)\n    pred_y = model.predict(valid_X)\n\n    # Evaluate.\n    error = sklearn.metrics.mean_absolute_error(valid_y, pred_y)\n\n    # Define a model checkpoint using Ray Train API.\n    state_dict = {\"model\": algorithm, \"location_id\": sample_location_id}\n    \n    with TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"ckpt.pkl\"), 'wb') as file:\n            pickle.dump(state_dict, file)\n    \n        checkpoint = Checkpoint.from_directory(tmpdir)\n\n        # Save checkpoint and report back metrics, using ray.tune.report()\n        # The metrics you specify here will appear in Tune summary table.\n        # They will also be recorded in Tune results under `metrics`.\n        metrics = dict(error=error)\n        tune.report(metrics, checkpoint=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Defining a Deployment with Load Shedding in Ray Serve\nDESCRIPTION: This Python code defines a Ray Serve deployment with load shedding configured using the `max_ongoing_requests` and `max_queued_requests` parameters.  It demonstrates how to limit the number of concurrent and queued requests to prevent system overload and maintain responsiveness.  When the queue is full, new requests will be rejected with a 503 status code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/best-practices.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport ray\nfrom ray import serve\n\n\n@serve.deployment(max_ongoing_requests=2, max_queued_requests=2)\nclass SlowDeployment:\n    async def __call__(self) -> str:\n        await time.sleep(2)\n        return \"success!\"\n\n\n@serve.deployment\nclass Frontend:\n    def __init__(self, backend: serve.handle.DeploymentHandle):\n        self.backend = backend\n\n    async def __call__(self) -> str:\n        return await self.backend.remote()\n\n\napp = Frontend.bind(SlowDeployment.bind())\n\nif __name__ == \"__main__\":\n    serve.run(app)\n\n```\n\n----------------------------------------\n\nTITLE: Restoring a Tune Experiment After Interruption\nDESCRIPTION: Code example showing how to restore a previously interrupted Tune experiment using Tuner.restore. This enables experiment-level fault tolerance by resuming the experiment from where it left off.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-fault-tolerance.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner.restore(\n    path=\"~/ray_results/tune_fault_tolerance_guide\", trainable=trainable\n)\n\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Launching Distributed Training with Ray TorchTrainer\nDESCRIPTION: This code snippet sets up the Ray TorchTrainer with the defined training function, scaling configuration, and dataset. It then launches the distributed training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_cola_advanced.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import RunConfig, ScalingConfig, CheckpointConfig, DataConfig\n\n\n# Save the top-2 checkpoints according to the evaluation metric\n# The checkpoints and metrics are reported by `RayTrainReportCallback`\nrun_config = RunConfig(\n    name=\"ptl-sent-classification\",\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=2,\n        checkpoint_score_attribute=\"matthews_correlation\",\n        checkpoint_score_order=\"max\",\n    ),\n)\n\n# Schedule four workers for DDP training (1 GPU/worker by default)\nscaling_config = ScalingConfig(num_workers=4, use_gpu=True)\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func,\n    train_loop_config=train_func_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n    datasets={\"train\": train_dataset, \"validation\": validation_dataset}, # <- Feed the Ray Datasets here\n)\n\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Initializing and Managing Single Workflow in Ray\nDESCRIPTION: Demonstrates basic workflow operations including running a task, getting status, resuming, canceling and deleting a workflow. Shows how to use the workflow_id parameter for unique identification.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/management.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import workflow\n\n@ray.remote\ndef task():\n    return 3\n\nworkflow.run(task.bind(), workflow_id=\"workflow_id\")\n\n# Get the status of a workflow.\ntry:\n    status = workflow.get_status(workflow_id=\"workflow_id\")\n    assert status in {\n        \"RUNNING\", \"RESUMABLE\", \"FAILED\",\n        \"CANCELED\", \"SUCCESSFUL\"}\nexcept workflow.exceptions.WorkflowNotFoundError:\n    print(\"Workflow doesn't exist.\")\n\n# Resume a workflow.\nprint(workflow.resume(workflow_id=\"workflow_id\"))\n# return is an ObjectRef which is the result of this workflow\n\n# Cancel a workflow.\nworkflow.cancel(workflow_id=\"workflow_id\")\n\n# Delete the workflow.\nworkflow.delete(workflow_id=\"workflow_id\")\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset from Pandas DataFrame - Ray\nDESCRIPTION: This snippet explains how to create a Ray dataset from a Pandas DataFrame using the `ray.data.from_pandas` function. It shows how to construct a DataFrame and convert it into a dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport ray\n\n\ndf = pd.DataFrame({\n    \"food\": [\"spam\", \"ham\", \"eggs\"],\n    \"price\": [9.34, 5.37, 0.94]\n})\nds = ray.data.from_pandas(df)\n\nprint(ds)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hierarchical Multi-Agent Environment with PPO\nDESCRIPTION: Example configuration for a hierarchical multi-agent setup using PPO. It defines top-level and low-level policies with a policy mapping function that routes agents to their respective policies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/hierarchical-envs.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\nconfig = (\n    PPOConfig()\n    .multi_agent(\n        policies={\"top_level\", \"low_level\"},\n        policy_mapping_fn=(\n            lambda aid, eps, **kw: \"low_level\" if aid.startswith(\"low_level\") else \"top_level\"\n        ),\n        policies_to_train=[\"top_level\"],\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Managing Workflow Execution and Results\nDESCRIPTION: Illustrates how to initialize Ray with storage, run a workflow with a specific ID, check its status, and retrieve its output both synchronously and asynchronously.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/key-concepts.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nray.init(storage=\"/tmp/data\")\nassert workflow.run(dag, workflow_id=\"run_1\") == 101\nassert workflow.get_status(\"run_1\") == workflow.WorkflowStatus.SUCCESSFUL\nassert workflow.get_output(\"run_1\") == 101\n# workflow.get_output_async returns an ObjectRef.\nassert ray.get(workflow.get_output_async(\"run_1\")) == 101\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Ray Tune and PyTorch\nDESCRIPTION: Imports necessary dependencies for PyTorch model definition and Ray Tune optimization. Includes imports for the ASHAScheduler used for early stopping and other utilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import random_split\nimport torchvision\nimport torchvision.transforms as transforms\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\n```\n\n----------------------------------------\n\nTITLE: ResNet Model Inference Class\nDESCRIPTION: Implementation of a ResNet model class for batch inference that handles model initialization and prediction logic.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/pytorch_resnet_batch_prediction.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport torch\n\n\nclass ResnetModel:\n    def __init__(self):\n        self.weights = ResNet152_Weights.IMAGENET1K_V1\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = models.resnet152(weights=self.weights).to(self.device)\n        self.model.eval()\n\n    def __call__(self, batch: Dict[str, np.ndarray]):\n        torch_batch = torch.from_numpy(batch[\"transformed_image\"]).to(self.device)\n        with torch.inference_mode():\n            prediction = self.model(torch_batch)\n            predicted_classes = prediction.argmax(dim=1).detach().cpu()\n            predicted_labels = [\n                self.weights.meta[\"categories\"][i] for i in predicted_classes\n            ]\n            return {\n                \"predicted_label\": predicted_labels,\n                \"original_image\": batch[\"original_image\"],\n            }\n```\n\n----------------------------------------\n\nTITLE: Creating Checkpoints with Function API in Ray Tune\nDESCRIPTION: Demonstrates how to save checkpoints during training by using the tune.report method with a checkpoint object created from a directory, and how to load checkpoints using tune.get_checkpoint(). This is the recommended approach for Function API users.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-trial-checkpoints.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef train_function(config):\n    checkpoint = tune.get_checkpoint()\n    start_epoch = 0\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            state_dict = torch.load(os.path.join(checkpoint_dir, \"model.pt\"))\n            model.load_state_dict(state_dict)\n            start_epoch = state_dict[\"epoch\"]\n\n    for epoch in range(start_epoch, 100):\n        # ... train for one epoch ...\n        torch.save({\"epoch\": epoch, **model.state_dict()}, \"model.pt\")\n        \n        # Create a checkpoint from the current working directory\n        checkpoint = ray.train.Checkpoint.from_directory(\".\")\n        tune.report(loss=loss, checkpoint=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Creating an Algorithm from a Checkpoint in Python\nDESCRIPTION: Recreate an entire Algorithm instance from a checkpoint using the from_checkpoint method. This allows users to load a previously saved model and continue training it.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/checkpoints.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm import Algorithm\n\nnew_ppo = Algorithm.from_checkpoint(checkpoint_dir)\nassert new_ppo.config.env == \"Pendulum-v1\"\nnew_ppo.train()\n```\n\n----------------------------------------\n\nTITLE: Training MLP Policy with PPO in Python\nDESCRIPTION: This example shows how to configure an MLP policy with custom hidden layers and activations for training with PPO using default RLModule. This setup uses a 32,32-stack with ReLU activations for dense layers, providing flexibility in neural network design.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n\nconfig = (\n    PPOConfig()\n    .environment(\"CartPole-v1\")\n    .rl_module(\n        model_config=DefaultModelConfig(\n            fcnet_hiddens=[32, 32],\n            fcnet_activation=\"relu\",\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Torch Dataset to Ray Dataset\nDESCRIPTION: This snippet demonstrates how to convert a PyTorch Dataset (CIFAR10 in this case) to a Ray Dataset using `ray.data.from_torch`.  It imports necessary libraries, downloads and transforms the CIFAR10 dataset, and then converts it into a Ray Dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ntds = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=ToTensor())\nds = ray.data.from_torch(tds)\n\nprint(ds)\n```\n\n----------------------------------------\n\nTITLE: Defining Map Function for Word Counting\nDESCRIPTION: Implements the map function that processes each document and yields word-count pairs. Converts text to lowercase and splits into words.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/map_reduce.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef map_function(document):\n    for word in document.lower().split():\n        yield word, 1\n```\n\n----------------------------------------\n\nTITLE: Defining and Executing a Simple Workflow DAG in Python\nDESCRIPTION: This snippet demonstrates how to define a simple 3-node workflow DAG using Ray remote functions and execute it using the workflow.run() method. It also shows how to run the workflow asynchronously.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport ray\n\n@ray.remote\ndef read_data(num: int):\n    return [i for i in range(num)]\n\n@ray.remote\ndef preprocessing(data: List[float]) -> List[float]:\n    return [d**2 for d in data]\n\n@ray.remote\ndef aggregate(data: List[float]) -> float:\n    return sum(data)\n\n# Build the DAG:\n# data -> preprocessed_data -> aggregate\ndata = read_data.bind(10)\npreprocessed_data = preprocessing.bind(data)\noutput = aggregate.bind(preprocessed_data)\n\nfrom ray import workflow\n\n# Execute the workflow and print the result.\nprint(workflow.run(output))\n\n# You can also run the workflow asynchronously and fetch the output via\n# 'ray.get'\noutput_ref = workflow.run_async(output)\nprint(ray.get(output_ref))\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Map Operation with Ray\nDESCRIPTION: Creates a Ray remote function to apply the map operation across distributed partitions. Includes partitioning logic based on word first letters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/map_reduce.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\ndef apply_map(corpus, num_partitions=3):\n    map_results = [list() for _ in range(num_partitions)]\n    for document in corpus:\n        for result in map_function(document):\n            first_letter = result[0].decode(\"utf-8\")[0]\n            word_index = ord(first_letter) % num_partitions\n            map_results[word_index].append(result)\n    return map_results\n```\n\n----------------------------------------\n\nTITLE: Managing Intel Gaudi (HPU) Resources in Ray\nDESCRIPTION: Shows how to initialize Ray with Intel Gaudi HPU resources, create HPU-aware actors and tasks, and access HPU IDs and environment variables. Demonstrates Ray's automatic allocation of HPU resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\n\nray.init(resources={\"HPU\": 2})\n\n@ray.remote(resources={\"HPU\": 1})\nclass HPUActor:\n    def ping(self):\n        print(\"HPU IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"HPU\"]))\n        print(\"HABANA_VISIBLE_MODULES: {}\".format(os.environ[\"HABANA_VISIBLE_MODULES\"]))\n\n@ray.remote(resources={\"HPU\": 1})\ndef hpu_task():\n    print(\"HPU IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"HPU\"]))\n    print(\"HABANA_VISIBLE_MODULES: {}\".format(os.environ[\"HABANA_VISIBLE_MODULES\"]))\n\nhpu_actor = HPUActor.remote()\nray.get(hpu_actor.ping.remote())\n# The actor uses the first HPU so the task uses the second one.\nray.get(hpu_task.remote())\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Web Crawler with Ray Tasks\nDESCRIPTION: Converting the sequential crawler to a parallel version using Ray Tasks by decorating the find_links function with @ray.remote.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/web-crawler.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\ndef find_links_task(start_url, base_url, depth=2):\n    return find_links(start_url, base_url, depth)\n```\n\n----------------------------------------\n\nTITLE: Creating a Placement Group in Python\nDESCRIPTION: This snippet demonstrates how to create a placement group using ray.util.placement_group in Python. It specifies a bundle of resources and a placement strategy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Reserve a placement group with a single bundle\npg = ray.util.placement_group(\n    bundles=[{\"CPU\": 1, \"GPU\": 1}],\n    strategy=\"STRICT_PACK\"\n)\n```\n\n----------------------------------------\n\nTITLE: Requesting GPU Resources Directly via Ray Autoscaler SDK\nDESCRIPTION: This Python code demonstrates how to make a direct request to the Ray autoscaler using the SDK to scale up GPU resources. It requests two bundles, each with one GPU resource.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gpu.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\nray.autoscaler.sdk.request_resources(bundles=[{\"GPU\": 1}] * 2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Tune Search Space for MNIST Classifier\nDESCRIPTION: This snippet defines the hyperparameter search space for Ray Tune to optimize the MNIST classifier, including layer sizes, learning rate, and batch size.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"layer_1_size\": tune.choice([32, 64, 128]),\n    \"layer_2_size\": tune.choice([64, 128, 256]),\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"batch_size\": tune.choice([32, 64, 128]),\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Legacy Debugger and Setting Breakpoints in Python\nDESCRIPTION: Sets up a Ray task with a breakpoint for debugging. This example initializes Ray with the legacy debugger enabled and defines a simple remote function that includes a breakpoint before returning the square of its input.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/ray-debugging.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(runtime_env={\"env_vars\": {\"RAY_DEBUG\": \"legacy\"}})\n\n@ray.remote\ndef f(x):\n    breakpoint()\n    return x * x\n\nfutures = [f.remote(i) for i in range(2)]\nprint(ray.get(futures))\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Tune with WandbLoggerCallback\nDESCRIPTION: This function sets up a Tune experiment using the WandbLoggerCallback to log metrics to Wandb.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-wandb.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef tune_with_callback():\n    \"\"\"Example for using a WandbLoggerCallback with the function API\"\"\"\n    tuner = tune.Tuner(\n        train_function,\n        tune_config=tune.TuneConfig(\n            metric=\"loss\",\n            mode=\"min\",\n        ),\n        run_config=tune.RunConfig(\n            callbacks=[WandbLoggerCallback(project=\"Wandb_example\")]\n        ),\n        param_space={\n            \"mean\": tune.grid_search([1, 2, 3, 4, 5]),\n            \"sd\": tune.uniform(0.2, 0.8),\n        },\n    )\n    tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Configuring Nevergrad Search Algorithm\nDESCRIPTION: Configures the Nevergrad search algorithm with a OnePlusOne optimizer and limits the concurrency to a maximum of 4 trials.  This controls the hyperparameter search process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"algo = NevergradSearch(\\n    optimizer=ng.optimizers.OnePlusOne,\\n)\\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\"\n```\n\n----------------------------------------\n\nTITLE: Defining Object Detection Model Class for Ray Data Batch Processing\nDESCRIPTION: Creates a class to handle batch inference for object detection, compatible with Ray Data's map_batches operation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass ObjectDetectionModel:\n    def __init__(self):\n        # Define the model loading and initialization code in `__init__`.\n        self.weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n        self.model = fasterrcnn_resnet50_fpn_v2(\n            weights=self.weights,\n            box_score_thresh=0.9,\n        )\n        if torch.cuda.is_available():\n            # Move the model to GPU if it's available.\n            self.model = self.model.cuda()\n        self.model.eval()\n\n    def __call__(self, input_batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        # Define the per-batch inference code in `__call__`.\n        batch = [torch.from_numpy(image) for image in input_batch[\"transformed\"]]\n        if torch.cuda.is_available():\n            # Move the data to GPU if it's available.\n            batch = [image.cuda() for image in batch]\n        predictions = self.model(batch)\n        # keep the original image for visualization purposes\n        return {\n            \"image\": input_batch[\"image\"],\n            \"labels\": [pred[\"labels\"].detach().cpu().numpy() for pred in predictions],\n            \"boxes\": [pred[\"boxes\"].detach().cpu().numpy() for pred in predictions],\n        }\n```\n\n----------------------------------------\n\nTITLE: Executing Fit Method on TorchTrainer Python\nDESCRIPTION: Executes the fit method on the TorchTrainer instance to initiate the training process. The results object stores metrics and checkpoints obtained from the training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresults = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining Ray Task for Model Training and Evaluation\nDESCRIPTION: This function is a Ray remote task that trains and evaluates a time series model on a single fold of data. It handles model fitting, forecasting, and metric calculation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/automl_for_time_series.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef train_and_evaluate_fold(\n    model: _TS,\n    df: pd.DataFrame,\n    train_indices: np.ndarray,\n    test_indices: np.ndarray,\n    label_column: str,\n    metrics: Dict[str, Callable[[pd.Series, pd.Series], float]],\n    freq: str = \"D\",\n) -> Dict[str, float]:\n    try:\n        # Create the StatsForecast object with train data & model.\n        statsforecast = StatsForecast(\n            df=df.iloc[train_indices], models=[model], freq=freq\n        )\n        # Make a forecast and calculate metrics on test data.\n        # This will fit the model first automatically.\n        forecast = statsforecast.forecast(len(test_indices))\n        return {\n            metric_name: metric(\n                df.iloc[test_indices][label_column], forecast[model.__class__.__name__]\n            )\n            for metric_name, metric in metrics.items()\n        }\n    except Exception:\n        # In case the model fit or eval fails, return None for all metrics.\n        return {metric_name: None for metric_name, metric in metrics.items()}\n```\n\n----------------------------------------\n\nTITLE: Passing Dynamic Object Reference Generators Between Tasks\nDESCRIPTION: This snippet illustrates how to pass an ObjectRef returned by a task using 'num_returns=\"dynamic\"' to another task. The receiving task can then utilize the DynamicObjectRefGenerator to manage return values from the original task.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tasks/generators.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/generator.py\n    :language: python\n    :start-after: __dynamic_generator_pass_start__\n    :end-before: __dynamic_generator_pass_end__\n```\n\n----------------------------------------\n\nTITLE: Converting Tabular Data to RLlib Episodes\nDESCRIPTION: Demonstrates how to convert columnar format expert data into RLlib's SingleAgentEpisode objects for training on full expert trajectories.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport msgpack\nimport msgpack_numpy as mnp\n\nfrom collections import defaultdict\n\nfrom ray import data\nfrom ray.rllib.env.single_agent_episode import SingleAgentEpisode\n\n# Load the dataset with the tabular data.\nds = data.read_parquet(tabular_data_path)\n\n# Build the environment from which the data was sampled to get the\n# spaces.\nenv = gym.make(\"CartPole-v1\")\n# Define buffers for episode data.\neps_obs = []\neps_actions = []\neps_rewards = []\n# Note, extra-model-outputs needs to be a dictionary with list\n# values.\neps_extra_model_outputs = defaultdict(list)\n# Define a buffer for unwritten episodes.\nepisodes = []\n\n# Start iterating over the rows of your experience data.\nfor i, row in enumerate(ds.iter_rows(prefetch_batches=10)):\n    # If the episode isn't terminated nor truncated, buffer the data.\n    if not row[\"terminateds\"] and not row[\"truncateds\"]:\n        eps_obs.append(row[\"obs\"])\n        eps_actions.append(row[\"actions\"])\n        eps_rewards.append(row[\"rewards\"])\n        eps_extra_model_outputs[\"action_dist_inputs\"].append(row[\"action_dist_inputs\"])\n        eps_extra_model_outputs[\"action_logp\"].append(row[\"action_logp\"])\n    # Otherwise, build the episode.\n    else:\n        eps_obs.append(row[\"new_obs\"])\n        episode = SingleAgentEpisode(\n            id_=row[\"eps_id\"],\n            agent_id=row[\"agent_id\"],\n            module_id=row[\"module_id\"],\n            observations=eps_obs,\n            # Use the spaces from the environment.\n            observation_space=env.observation_space,\n            action_space=env.action_space,\n            actions=eps_actions,\n            rewards=eps_rewards,\n            # Set the starting timestep to zero.\n            t_started=0,\n            # You don't want to have a lookback buffer.\n            len_lookback_buffer=0,\n            terminated=row[\"terminateds\"],\n            truncated=row[\"truncateds\"],\n            extra_model_outputs=eps_extra_model_outputs,\n        )\n        # Store the ready-to-write episode to the episode buffer.\n        episodes.append(msgpack.packb(episode.get_state(), default=mnp.encode))\n        # Clear all episode data buffers.\n        eps_obs.clear()\n        eps_actions.clear()\n        eps_rewards.clear()\n        eps_extra_model_outputs = defaultdict(list)\n\n    # Write episodes to disk when the episode buffer holds 50 episodes.\n    if len(episodes) > 49:\n        # Generate a Ray dataset from episodes.\n        episodes_ds = data.from_items(episodes)\n        # Write the Parquet data and compress it.\n        episodes_ds.write_parquet(\n            f\"/tmp/test_converting/file-{i}\".zfill(6),\n            compression=\"gzip\",\n        )\n        # Delete the dataset in memory and clear the episode buffer.\n        del episodes_ds\n        episodes.clear()\n\n# If we are finished and have unwritten episodes, write them now.\nif len(episodes) > 0:\n    episodes_ds = data.from_items(episodes)\n    episodes_ds.write_parquet(\n        f\"/tmp/test_converting/file-{i}\".zfill(6),\n        compression=\"gzip\",\n    )\n    del episodes_ds\n    episodes.clear()\n```\n\n----------------------------------------\n\nTITLE: Running MNIST Tuning Examples\nDESCRIPTION: Example code showing how to run both ASHA and PBT tuning experiments for MNIST with specified parameters including data directory, number of samples, epochs, and GPU allocation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndata_dir = \"~/data/\"\n\ntune_mnist_asha(num_samples=1, num_epochs=6, gpus_per_trial=0, data_dir=data_dir)\ntune_mnist_pbt(num_samples=1, num_epochs=6, gpus_per_trial=0, data_dir=data_dir)\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing to BigQuery with Ray Data\nDESCRIPTION: This code demonstrates how to read data from and write data to Google BigQuery using Ray Data. It utilizes the `ray.data.read_bigquery` and `ds.write_bigquery` functions to interact with BigQuery datasets.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Read the entire dataset. Do not specify query.\nds = ray.data.read_bigquery(\n    project_id=\"my_gcloud_project_id\",\n    dataset=\"bigquery-public-data.ml_datasets.iris\",\n)\n\n# Read from a SQL query of the dataset. Do not specify dataset.\nds = ray.data.read_bigquery(\n    project_id=\"my_gcloud_project_id\",\n    query = \"SELECT * FROM `bigquery-public-data.ml_datasets.iris` LIMIT 50\",\n)\n\n# Write back to BigQuery\nds.write_bigquery(\n    project_id=\"my_gcloud_project_id\",\n    dataset=\"destination_dataset.destination_table\",\n    overwrite_table=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Objective Function for Conditional Search Space in Python\nDESCRIPTION: Adjusts the objective function to accommodate conditional dependencies in the search space. Evaluates the performance considering these dependencies and reports via Ray Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef evaluation_fn(step, width, height, mult=1):\n    return (0.1 + width * step / 100) ** (-1) + height * 0.1 * mult\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Ray Workflows\nDESCRIPTION: This snippet shows how to handle errors in workflows using max_retries, retry_exceptions, and catch_exceptions options. It demonstrates both automatic retries and manual exception handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple\nimport random\n\nimport ray\nfrom ray import workflow\n\n@ray.remote\ndef faulty_function() -> str:\n    if random.random() > 0.5:\n        raise RuntimeError(\"oops\")\n    return \"OK\"\n\n# Tries up to five times before giving up.\nr1 = faulty_function.options(max_retries=5).bind()\ntry:\n    workflow.run(r1)\nexcept ray.exceptions.RayTaskError:\n    pass\n\n@ray.remote\ndef handle_errors(result: Tuple[str, Exception]):\n    # The exception field will be None on success.\n    err = result[1]\n    if err:\n        return \"There was an error: {}\".format(err)\n    else:\n        return \"OK\"\n\n# `handle_errors` receives a tuple of (result, exception).\nr2 = faulty_function.options(**workflow.options(catch_exceptions=True)).bind()\nworkflow.run(handle_errors.bind(r2))\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Train for PyTorch Lightning\nDESCRIPTION: Sets up the training function for Ray Train, including distributed strategy, cluster environment, and parallel devices configuration for PyTorch Lightning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_mnist_example.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nuse_gpu = True # Set to False if you want to run without GPUs\nnum_workers = 4\n\nimport pytorch_lightning as pl\nfrom ray.train import RunConfig, ScalingConfig, CheckpointConfig\nfrom ray.train.torch import TorchTrainer\nfrom ray.train.lightning import (\n    RayDDPStrategy,\n    RayLightningEnvironment,\n    RayTrainReportCallback,\n    prepare_trainer,\n)\n\ndef train_func_per_worker():\n    model = MNISTClassifier(lr=1e-3, feature_dim=128)\n    datamodule = MNISTDataModule(batch_size=128)\n\n    trainer = pl.Trainer(\n        devices=\"auto\",\n        strategy=RayDDPStrategy(),\n        plugins=[RayLightningEnvironment()],\n        callbacks=[RayTrainReportCallback()],\n        max_epochs=10,\n        accelerator=\"gpu\" if use_gpu else \"cpu\",\n        log_every_n_steps=100,\n        logger=CSVLogger(\"logs\"),\n    )\n    \n    trainer = prepare_trainer(trainer)\n    \n    # Train model\n    trainer.fit(model, datamodule=datamodule)\n\n    # Evaluation on the test dataset\n    trainer.test(model, datamodule=datamodule)\n```\n\n----------------------------------------\n\nTITLE: MLflow Integration with Ray Train\nDESCRIPTION: Demonstrates how to integrate MLflow with Ray Train. It includes setting up the Databricks configuration, initializing MLflow, logging metrics, and ending the run.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import train\nimport mlflow\n\n# Run the following on the head node:\n# $ databricks configure --token\n# mv ~/.databrickscfg YOUR_SHARED_STORAGE_PATH\n# This function assumes `databricks_config_file` is specified in the Trainer's `train_loop_config`.\ndef train_func(config):\n    # Step 1 and 2\n    os.environ[\"DATABRICKS_CONFIG_FILE\"] = config[\"databricks_config_file\"]\n    mlflow.set_tracking_uri(\"databricks\")\n    mlflow.set_experiment_id(...)\n    mlflow.start_run()\n\n    # ...\n\n    loss = optimize()\n\n    metrics = {\"loss\": loss}\n    # Only report the results from the first worker to MLflow\n    to avoid duplication\n\n    # Step 3\n    if train.get_context().get_world_rank() == 0:\n        mlflow.log_metrics(metrics)\n```\n\n----------------------------------------\n\nTITLE: Defining ImageClassifier Class for Batch Inference\nDESCRIPTION: Creates a custom ImageClassifier class that encapsulates the Huggingface pipeline for efficient batch inference across multiple GPUs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\n\nfrom transformers import pipeline\nfrom PIL import Image\n\n# Pick the largest batch size that can fit on our GPUs.\n# If doing CPU inference you might need to lower considerably (e.g. to 10).\nBATCH_SIZE = 1024\n\nclass ImageClassifier:\n    def __init__(self):\n        self.classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\", device=DEVICE)\n\n    def __call__(self, batch: Dict[str, np.ndarray]):\n        # Convert the numpy array of images into a list of PIL images which is the format the HF pipeline expects.\n        outputs = self.classifier(\n            [Image.fromarray(image_array) for image_array in batch[\"image\"]], \n            top_k=1, \n            batch_size=BATCH_SIZE)\n        \n        # `outputs` is a list of length-one lists. For example:\n        # [[{'score': '...', 'label': '...'}], ..., [{'score': '...', 'label': '...'}]]\n        batch[\"score\"] = [output[0][\"score\"] for output in outputs]\n        batch[\"label\"] = [output[0][\"label\"] for output in outputs]\n        # note: we keep the original image column in the result so that we can display the images later\n        return batch\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet with Column Pruning in Python\nDESCRIPTION: This example demonstrates how to read a Parquet file using Ray Data with column pruning (projection pushdown) to efficiently load only the required columns.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Read just two of the five columns of the Iris dataset.\nray.data.read_parquet(\n    \"s3://anonymous@ray-example-data/iris.parquet\",\n    columns=[\"sepal.length\", \"variety\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Synchronous Training Loop\nDESCRIPTION: Implementation of synchronous training loop where all workers compute gradients before weights are updated.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_parameter_server.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Running synchronous parameter server training.\")\ncurrent_weights = ps.get_weights.remote()\nfor i in range(iterations):\n    gradients = [worker.compute_gradients.remote(current_weights) for worker in workers]\n    # Calculate update after all gradients are available.\n    current_weights = ps.apply_gradients.remote(*gradients)\n\n    if i % 10 == 0:\n        # Evaluate the current model.\n        model.set_weights(ray.get(current_weights))\n        accuracy = evaluate(model, test_loader)\n        print(\"Iter {}: \\taccuracy is {:.1f}\".format(i, accuracy))\n\nprint(\"Final accuracy is {:.1f}.\".format(accuracy))\n# Clean up Ray resources and processes before the next example.\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Processing and Tokenizing Shakespeare Dataset for LLM Fine-tuning\nDESCRIPTION: Loads and preprocesses the tiny_shakespeare dataset for fine-tuning the language model. The function splits the text into sentences and tokenizes them using the model's tokenizer with appropriate padding and truncation.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef split_text(batch: pd.DataFrame) -> pd.DataFrame:\n    text = list(batch[\"text\"])\n    flat_text = \"\".join(text)\n    split_text = [\n        x.strip()\n        for x in flat_text.split(\"\\n\")\n        if x.strip() and not x.strip()[-1] == \":\"\n    ]\n    return pd.DataFrame(split_text, columns=[\"text\"])\n\n\ndef tokenize(batch: pd.DataFrame) -> dict:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n    tokenizer.pad_token = tokenizer.eos_token\n    ret = tokenizer(\n        list(batch[\"text\"]),\n        truncation=True,\n        max_length=256,\n        padding=\"max_length\",\n        return_tensors=\"np\",\n    )\n    ret[\"labels\"] = ret[\"input_ids\"].copy()\n    return dict(ret)\n\nhf_dataset = load_dataset(\"tiny_shakespeare\")\ntrain_ds = ray.data.from_huggingface(hf_dataset[\"train\"])\n```\n\n----------------------------------------\n\nTITLE: Comparing Normal Functions and Generators for Large Value Returns in Ray\nDESCRIPTION: This code example demonstrates the difference between using normal functions and generators for returning large values in Ray. It shows how generators can prevent out-of-memory errors when dealing with multiple large return values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/generators.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport numpy as np\nimport os\n\nray.init()\n\n@ray.remote\ndef large_values(num_returns):\n    return tuple(\n        np.ones((25000, 1000), dtype=np.float32)\n        for _ in range(num_returns)\n    )\n\n@ray.remote\ndef large_values_generator(num_returns):\n    for i in range(num_returns):\n        yield np.ones((25000, 1000), dtype=np.float32)\n        print(f\"({large_values_generator.name} pid={os.getpid()}) yielded return value {i}\")\n\nif __name__ == \"__main__\":\n    import sys\n\n    num_returns = int(sys.argv[1])\n\n    print(\"Using normal functions...\")\n    try:\n        ray.get(large_values.remote(num_returns))\n        print(\"Success!\")\n    except Exception:\n        print(\"Worker failed\")\n\n    print(\"Using generators...\")\n    try:\n        ray.get(large_values_generator.remote(num_returns))\n        print(\"Success!\")\n    except Exception:\n        print(\"Worker failed\")\n```\n\n----------------------------------------\n\nTITLE: Discovering Ray Metrics Endpoints with Python\nDESCRIPTION: Python script demonstrating how to programmatically discover metrics endpoints in a Ray cluster using ray.nodes() API. The script combines NodeManagerAddress with MetricsExportPort to find metrics agents' URLs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/metrics.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# On a cluster node:\nimport ray\nray.init()\nfrom pprint import pprint\npprint(ray.nodes())\n\n\"\"\"\nPass the <NodeManagerAddress>:<MetricsExportPort> from each of these entries\nto Prometheus.\n[{'Alive': True,\n  'MetricsExportPort': 8080,\n  'NodeID': '2f480984702a22556b90566bdac818a4a771e69a',\n  'NodeManagerAddress': '192.168.1.82',\n  'NodeManagerHostname': 'host2.attlocal.net',\n  'NodeManagerPort': 61760,\n  'ObjectManagerPort': 61454,\n  'ObjectStoreSocketName': '/tmp/ray/session_2020-08-04_18-18-16_481195_34255/sockets/plasma_store',\n  'RayletSocketName': '/tmp/ray/session_2020-08-04_18-18-16_481195_34255/sockets/raylet',\n  'Resources': {'CPU': 1.0,\n                'memory': 123.0,\n                'node:192.168.1.82': 1.0,\n                'object_store_memory': 2.0},\n  'alive': True},\n{'Alive': True,\n  'MetricsExportPort': 8080,\n  'NodeID': 'ce6f30a7e2ef58c8a6893b3df171bcd464b33c77',\n  'NodeManagerAddress': '192.168.1.82',\n  'NodeManagerHostname': 'host1.attlocal.net',\n  'NodeManagerPort': 62052,\n  'ObjectManagerPort': 61468,\n  'ObjectStoreSocketName': '/tmp/ray/session_2020-08-04_18-18-16_481195_34255/sockets/plasma_store.1',\n  'RayletSocketName': '/tmp/ray/session_2020-08-04_18-18-16_481195_34255/sockets/raylet.1',\n  'Resources': {'CPU': 1.0,\n                'memory': 134.0,\n                'node:192.168.1.82': 1.0,\n                'object_store_memory': 2.0},\n  'alive': True}]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Reproducible Data Execution in Ray Train\nDESCRIPTION: Shows how to configure Ray Datasets for reproducible execution by preserving order. This is important for ensuring consistent results when developing or tuning models, as it eliminates variability from data ingestion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Preserve ordering in Ray Datasets for reproducibility.\nctx = ray.data.DataContext.get_current()\nctx.execution_options.preserve_order = True\n\nds = ray.data.read_text(\n    \"s3://anonymous@ray-example-data/sms_spam_collection_subset.txt\"\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving a Multi-Agent PPO Algorithm Checkpoint in Python\nDESCRIPTION: This example shows how to configure a PPO algorithm for a multi-agent environment with two agents, train it for one iteration, and save its state to a checkpoint. It registers a custom multi-agent environment and sets up a policy mapping function to associate agents with policies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/checkpoints.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.examples.envs.classes.multi_agent import MultiAgentPendulum\nfrom ray.tune import register_env\n\nregister_env(\"multi-pendulum\", lambda cfg: MultiAgentPendulum({\"num_agents\": 2}))\n\n# Configure and build an initial algorithm.\nmulti_agent_config = (\n    PPOConfig()\n    .environment(\"multi-pendulum\")\n    .multi_agent(\n        policies={\"p0\", \"p1\"},\n        # Agent IDs are 0 and 1 -> map to p0 and p1, respectively.\n        policy_mapping_fn=lambda aid, eps, **kw: f\"p{aid}\"\n    )\n)\nppo = multi_agent_config.build()\n\n# Train for one iteration, then save to a checkpoint.\nprint(ppo.train())\nmulti_agent_checkpoint_dir = ppo.save_to_path()\nprint(f\"saved multi-agent algo to {multi_agent_checkpoint_dir}\")\n```\n\n----------------------------------------\n\nTITLE: Running the Tune experiment\nDESCRIPTION: This snippet runs the Tune experiment by defining the objective function, search space, and search algorithm. It configures the Tuner with the objective, search algorithm, metric, and number of samples, and then executes the experiment using tuner.fit().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n        metric=\"mean_loss\",\n        mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space=search_space,\n)\nresults = tuner.fit()\n\n```\n\n----------------------------------------\n\nTITLE: Initializing ResNet-50 Model for Binary Classification\nDESCRIPTION: Function to load a pre-trained ResNet-50 model and modify its final layer for binary classification of ants and bees.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef initialize_model():\n    # Load pretrained model params\n    model = models.resnet50(pretrained=True)\n\n    # Replace the original classifier with a new Linear layer\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, 2)\n\n    # Ensure all params get updated during finetuning\n    for param in model.parameters():\n        param.requires_grad = True\n    return model\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Runners in Ray RLlib\nDESCRIPTION: This code snippet shows how to configure the number of environment runners and environments per runner in Ray RLlib. It also demonstrates how to use multiprocessing to step environments in parallel and how to allocate resources for expensive environments. Dependencies include Ray RLlib and potentially gymnasium for multiprocessing support. Key parameters include 'num_env_runners', 'num_envs_per_env_runner', and 'remote_worker_envs'.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-env.rst#2025-04-12_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n\"config.env_runners(num_env_runners=.., num_envs_per_env_runner=.., remote_worker_envs=True)\"\n```\n\nLANGUAGE: Python\nCODE:\n```\n\"config.env_runners(num_cpus_per_env_runner=.., num_gpus_per_env_runner=..)\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed BERT Training with HF Accelerate and Ray\nDESCRIPTION: Complete implementation demonstrating how to set up distributed training of a BERT model using Hugging Face Accelerate with Ray Train and Ray Data. The code includes dataset preparation, model configuration, training loop setup, and distributed training execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/accelerate/accelerate_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nimport ray.train as train\nfrom ray.train import Checkpoint\nfrom ray.train.torch import prepare_data_loader, TorchTrainer\n\n\ndef train_func(config):\n    # Initialize accelerator\n    accelerator = Accelerator()\n\n    # Load dataset\n    dataset = load_dataset(\"glue\", \"mrpc\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n    # Preprocess dataset\n    def tokenize_function(examples):\n        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n\n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n    train_dataset = tokenized_datasets[\"train\"]\n\n    # Create dataloaders\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=8\n    )\n\n    # Prepare for distributed training\n    train_dataloader = prepare_data_loader(train_dataloader)\n\n    # Initialize model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-cased\", num_labels=2\n    )\n\n    # Prepare for training\n    model, optimizer, train_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader\n    )\n\n    # Training loop\n    model.train()\n    for epoch in range(3):\n        for batch in train_dataloader:\n            outputs = model(**batch)\n            loss = outputs.loss\n            accelerator.backward(loss)\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Report metrics\n            train.report({\"loss\": loss.item()})\n\n\n# Initialize and run training\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config={\"num_workers\": 2},\n    torch_config={\"backend\": \"gloo\"},\n)\nresults = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Reusing Ray Actors in DAGs\nDESCRIPTION: Demonstrates how to reuse Ray Actors in DAGs by creating them with Actor.remote() instead of Actor.bind(). This prevents actors from being killed when the DAG finishes execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-dag.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\n# Create a reusable actor\ncounter = Counter.remote()\n\n# Build DAG using the reusable actor\ndag = counter.increment.bind()\n\n# Execute DAG multiple times\nfor _ in range(3):\n    result = ray.get(dag.execute())\n    print(result)  # Output: 1, 2, 3\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Spilling Directory via CLI\nDESCRIPTION: This command-line example demonstrates how to specify a custom object spilling directory when starting Ray using the ray start command.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/object-spilling.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nray start --object-spilling-directory=/path/to/spill/dir\n```\n\n----------------------------------------\n\nTITLE: Implementing Automatic Actor Checkpointing in Python\nDESCRIPTION: This code shows how to implement automatic checkpointing for actors in Ray using Ray's automatic actor restart feature. It demonstrates checkpointing in the actor and restoring from a checkpoint in the constructor.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/actors.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# __actor_checkpointing_auto_restart_begin__\n# __actor_checkpointing_auto_restart_end__\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Core API Components in Python\nDESCRIPTION: This code snippet demonstrates how to import various components of Ray's core API. It includes initialization, task management, actor handling, object operations, runtime context, and cross-language functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/core.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import init, shutdown, is_initialized, job_config, LoggingConfig\nfrom ray import remote, cancel\nfrom ray.actor import ActorClass, ActorMethod, ActorHandle, ActorClassInheritanceException, exit_actor\nfrom ray import method, get_actor, kill\nfrom ray import get, wait, put\nfrom ray.runtime_context import get_runtime_context, RuntimeContext\nfrom ray import get_gpu_ids\nfrom ray.cross_language import java_function, java_actor_class\n```\n\n----------------------------------------\n\nTITLE: Streaming Batched Requests in Ray Serve with Python\nDESCRIPTION: This example shows how to employ an async generator to stream outputs from batched requests using the ray.serve.batch decorator in Python. The function processes input lists and yields outputs iteratively. It handles cases where certain inputs may yield fewer outputs by using the StopIteration object, which ensures proper termination of the generator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dyn-req-batch.md#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\n  Example of streaming batched requests using an async generator\n  \n  async def example_batch_stream(self, arg1: List[int], arg2: List[str]) -> AsyncGenerator[List[int], None]:\n      # Method implementation using yield\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Results with Increased PBT Population\nDESCRIPTION: Creates visualization plots for the experiment with increased population size, showing parameter history and Q value history for all four trials to demonstrate more complex exploitation patterns.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw=dict(width_ratios=[1.5, 1]))\n\ncolors = [\"red\", \"black\", \"blue\", \"green\"]\nlabels = [\"h = [1, 0]\", \"h = [0, 1]\", \"h = [0.01, 0.99]\", \"h = [0.99, 0.01]\"]\n\nplot_parameter_history(\n    pbt_4_results,\n    colors,\n    labels,\n    perturbation_interval=perturbation_interval,\n    fig=fig,\n    ax=axs[0],\n)\nplot_Q_history(pbt_4_results, colors, labels, ax=axs[1])\n```\n\n----------------------------------------\n\nTITLE: Defining Pip Dependencies in Runtime Environment\nDESCRIPTION: This snippet provides an example of how to specify Pip dependencies in a runtime environment configuration in Ray. It shows the correct structure for including both pip and conda dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\"dependencies\": [\"pytorch\", \"torchvision\", \"pip\", {\"pip\": [\"pendulum\"]}]}\n```\n\n----------------------------------------\n\nTITLE: Actor Teardown in Ray Compiled Graph\nDESCRIPTION: Shows proper teardown procedure when reusing actors in Compiled Graph to avoid resource conflicts and potential segfaults. Demonstrates explicit teardown before actor reuse.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/troubleshooting.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a compiled graph with some actors\\ndag1 = CompiledDAG()\\n# Execute the compiled graph\\nray.get(dag1.execute())\\n# Explicitly teardown the compiled graph\\ndag1.teardown()\\n\\n# Create another compiled graph with the same actors\\ndag2 = CompiledDAG()\n```\n\n----------------------------------------\n\nTITLE: Configuring ASHA Scheduler with Custom Parameters\nDESCRIPTION: This snippet shows how to configure the ASHA scheduler with custom parameters such as time attribute, metric, mode, maximum time steps, grace period, reduction factor, and number of brackets. It demonstrates detailed scheduler configuration for more control over the early stopping behavior.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom tune.schedulers import ASHAScheduler\n\nasha_scheduler = ASHAScheduler(\n    time_attr='training_iteration',\n    metric='loss',\n    mode='min',\n    max_t=100,\n    grace_period=10,\n    reduction_factor=3,\n    brackets=1,\n)\ntuner = tune.Tuner(\n    train_fn,\n    tune_config=tune.TuneConfig(scheduler=asha_scheduler),\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Best Hyperparameter Configuration from Ray Tune Results\nDESCRIPTION: Gets the best trial result from the Ray Tune experiment, based on validation accuracy. The best performing configuration used specific values for batch_size, layer sizes, and learning rate.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresults.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies with pip\nDESCRIPTION: Command to install the required Python packages beautifulsoup4 and ray for the web crawler implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/web-crawler.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"beautifulsoup4==4.11.1\" \"ray>=2.2.0\"\n```\n\n----------------------------------------\n\nTITLE: Using Persist with Dask-on-Ray in Python\nDESCRIPTION: This example shows how to use dask.persist() with Dask-on-Ray to submit tasks to the Ray cluster and return Ray futures inlined in the Dask collection, facilitating faster downstream computations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/dask-on-ray.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport dask.array as da\nfrom ray.util.dask import ray_dask_get, enable_dask_on_ray\n\n# Start Ray.\nray.init()\n\n# Tell Dask to use Ray as the scheduler.\nenable_dask_on_ray()\n\n# Create a large array.\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\n\n# Trigger computation of the Array and store the\n# result in the Ray object store. This allows future\n# operations on this array to be faster.\nx = x.persist()\n\n# Compute the mean. This is faster because the Array is already\n# in the Ray object store.\nmean = x.mean().compute()\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Training Implementation\nDESCRIPTION: Implementation of asynchronous parameter server training where workers independently compute and apply gradients.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_parameter_server.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Running Asynchronous Parameter Server Training.\")\n\nray.init(ignore_reinit_error=True)\nps = ParameterServer.remote(1e-2)\nworkers = [DataWorker.remote() for i in range(num_workers)]\n\ncurrent_weights = ps.get_weights.remote()\n\ngradients = {}\nfor worker in workers:\n    gradients[worker.compute_gradients.remote(current_weights)] = worker\n\nfor i in range(iterations * num_workers):\n    ready_gradient_list, _ = ray.wait(list(gradients))\n    ready_gradient_id = ready_gradient_list[0]\n    worker = gradients.pop(ready_gradient_id)\n\n    # Compute and apply gradients.\n    current_weights = ps.apply_gradients.remote(*[ready_gradient_id])\n    gradients[worker.compute_gradients.remote(current_weights)] = worker\n\n    if i % 10 == 0:\n        # Evaluate the current model after every 10 updates.\n        model.set_weights(ray.get(current_weights))\n        accuracy = evaluate(model, test_loader)\n        print(\"Iter {}: \\taccuracy is {:.1f}\".format(i, accuracy))\n\nprint(\"Final accuracy is {:.1f}.\".format(accuracy))\n```\n\n----------------------------------------\n\nTITLE: PB2 PPO Example in Ray Tune\nDESCRIPTION: This snippet includes the complete Python code for the PB2 PPO example in Ray Tune. It defines the configuration, search space, and training loop for the experiment. It leverages the `ray.tune` library for distributed hyperparameter optimization and experiment management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/pb2_ppo_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"/../../python/ray/tune/examples/pb2_ppo_example.py\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up XGBoostTrainer with Cloud Storage for Multi-node Clusters\nDESCRIPTION: Configures an XGBoostTrainer with a shared storage location, which is required for multi-node clusters to enable checkpointing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntrainer = XGBoostTrainer(\n    ..., run_config=ray.train.RunConfig(storage_path=\"s3://...\")\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Task or Actor Resource Requirements in Python\nDESCRIPTION: This snippet shows how to explicitly set resource requirements for tasks and actors in Ray using the @ray.remote decorator. This is crucial for ensuring that tasks are scheduled properly based on the resources they require.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/resources.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n@ray.remote(num_cpus=1, num_gpus=1)\ndef my_task():\n    return 'Task Executed'\n\n@ray.remote\nclass MyActor:\n    def __init__(self):\n        pass\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Training Function for DCGAN with PBT\nDESCRIPTION: Implements the training function for a DCGAN with support for PBT. It includes checkpoint saving/loading and handles perturbed learning rates correctly when exploiting other trials.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n   :language: python\n   :start-after: __Train_begin__\n   :end-before: __Train_end__\n```\n\n----------------------------------------\n\nTITLE: Implementing ActorPool for Task Scheduling in Python\nDESCRIPTION: This snippet demonstrates the usage of Ray's ActorPool utility class for scheduling tasks over a fixed pool of actors. It shows how to create an ActorPool, submit tasks, and retrieve results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/actor-utils.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/actor-pool.py\n```\n\n----------------------------------------\n\nTITLE: Reading Image Files with Ray Data in Python\nDESCRIPTION: Shows how to read image files using Ray Data's read_images function. The example reads from an S3 bucket and prints the schema of the resulting dataset, which represents images as NumPy ndarrays.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/batoidea/JPEGImages/\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Quick Sort with Nested Tasks in Python using Ray\nDESCRIPTION: This code snippet demonstrates how to implement a distributed quick sort algorithm using nested tasks in Ray. It includes a non-distributed version for comparison and shows how to maximize parallelism using ray.get().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/nested-tasks.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport random\nimport time\n\n@ray.remote\ndef quick_sort_distributed(arr):\n    if len(arr) <= 1:\n        return arr\n    else:\n        pivot = arr[0]\n        left = [x for x in arr[1:] if x < pivot]\n        right = [x for x in arr[1:] if x >= pivot]\n        \n        # Recursively sort the sub-arrays in parallel\n        left_sorted = quick_sort_distributed.remote(left)\n        right_sorted = quick_sort_distributed.remote(right)\n        \n        # Wait for both sub-arrays to be sorted\n        left_sorted, right_sorted = ray.get([left_sorted, right_sorted])\n        \n        return left_sorted + [pivot] + right_sorted\n\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    else:\n        pivot = arr[0]\n        left = [x for x in arr[1:] if x < pivot]\n        right = [x for x in arr[1:] if x >= pivot]\n        return quick_sort(left) + [pivot] + quick_sort(right)\n\n# Initialize Ray\nray.init()\n\n# Generate a large list of random numbers\ndata = [random.randint(1, 1000) for _ in range(100000)]\n\n# Time the distributed version\nstart_time = time.time()\nsorted_data_distributed = ray.get(quick_sort_distributed.remote(data))\nend_time = time.time()\nprint(f\"Distributed version took {end_time - start_time:.2f} seconds\")\n\n# Time the non-distributed version\nstart_time = time.time()\nsorted_data = quick_sort(data)\nend_time = time.time()\nprint(f\"Non-distributed version took {end_time - start_time:.2f} seconds\")\n\n# Verify that both versions produce the same result\nassert sorted_data_distributed == sorted_data\n\n# Shut down Ray\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment using RLlib and Gymnasium - Subclass\nDESCRIPTION: Demonstrates how to use a custom subclass of gymnasium.Env in RLlib. The example shows creating a custom environment with observation and action spaces, and configuring it in RLlib using PPOConfig.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-env.rst#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\nclass MyDummyEnv(gym.Env):\n    # Write the constructor and provide a single `config` arg,\n    # which may be set to None by default.\n    def __init__(self, config=None):\n        # As per gymnasium standard, provide observation and action spaces in your\n        # constructor.\n        self.observation_space = gym.spaces.Box(-1.0, 1.0, (1,), np.float32)\n        self.action_space = gym.spaces.Discrete(2)\n\n    def reset(self, seed=None, options=None):\n        # Return (reset) observation and info dict.\n        return np.array([1.0]), {}\n\n    def step(self, action):\n        # Return next observation, reward, terminated, truncated, and info dict.\n        return np.array([1.0]), 1.0, False, False, {}\n\nconfig = (\n    PPOConfig()\n    .environment(\n        MyDummyEnv,\n        env_config={},  # `config` to pass to your env class\n    )\n)\nalgo = config.build()\nprint(algo.train())\n```\n\nLANGUAGE: Python\nCODE:\n```\nalgo.stop()\n```\n\n----------------------------------------\n\nTITLE: GPU-based Batch Processing with TorchPredictor in Ray\nDESCRIPTION: This snippet demonstrates how to use Ray for GPU-based batch processing with a TorchPredictor. It creates a dataset from numpy arrays and applies a map_batches operation using a TorchPredictor with GPU acceleration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninputs = torch.as_tensor(batch[\"data\"], dtype=torch.float32).cuda()\nwith torch.inference_mode():\n    batch[\"output\"] = self.model(inputs).detach().cpu().numpy()\nreturn batch\n\nds = (\n    ray.data.from_numpy(np.ones((32, 100)))\n    .map_batches(\n        TorchPredictor,\n        # Two workers with one GPU each\n        concurrency=2,\n        # Batch size is required if you're using GPUs.\n        batch_size=4,\n        num_gpus=1\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Actor Restart Behavior in Python\nDESCRIPTION: This code snippet demonstrates how to configure and test actor restart behavior in Ray. It shows setting max_restarts and max_task_retries, and handling RayActorError exceptions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/actors.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# __actor_restart_begin__\n# __actor_restart_end__\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset from HuggingFace Dataset - Ray\nDESCRIPTION: This snippet demonstrates how to convert a HuggingFace Dataset to a Ray Dataset using the `ray.data.from_huggingface` function. It shows how to load a dataset and convert it while noting parallel read limitations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nimport ray.data\nfrom datasets import load_dataset\n\nhf_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\nray_ds = ray.data.from_huggingface(hf_ds[\"train\"])\nray_ds.take(2)\n```\n\n----------------------------------------\n\nTITLE: Loading Llama Model for Pre-training on Intel Gaudi\nDESCRIPTION: Defines a function to load the Llama model configuration and initialize the model for causal language modeling. Uses the AutoConfig and AutoModelForCausalLM classes from the transformers library.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef load_model(config):\n    name = config[\"name\"]\n    model_config = config.get(\"config\", {})\n    auto_config = transformers.AutoConfig.from_pretrained(\n        pretrained_model_name_or_path=name, **model_config\n    )\n    model = transformers.AutoModelForCausalLM.from_config(auto_config, trust_remote_code=False)\n\n    return model\n```\n\n----------------------------------------\n\nTITLE: Configuring Vision Language Model for Ray Serve LLM\nDESCRIPTION: Python code for server-side configuration of a vision language model deployment using Ray Serve LLM. Sets up model loading, deployment, and engine parameters for a multimodal model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.llm import LLMConfig, build_openai_app\n\n\n# Configure a vision model\nllm_config = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"pixtral-12b\",\n        model_source=\"mistral-community/pixtral-12b\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1,\n            max_replicas=2,\n        )\n    ),\n    accelerator_type=\"L40S\",\n    engine_kwargs=dict(\n        tensor_parallel_size=1,\n        max_model_len=8192,\n    ),\n)\n\n# Build and deploy the model\napp = build_openai_app({\"llm_configs\": [llm_config]})\nserve.run(app, blocking=True)\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Collective Operations in Python\nDESCRIPTION: Example showing how to use multi-GPU collective operations in Ray. Demonstrates initializing workers with multiple GPUs and performing allreduce and point-to-point communication across multiple GPUs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/ray-collective.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport ray.util.collective as collective\n\nimport cupy as cp\nfrom cupy.cuda import Device\n\n\n@ray.remote(num_gpus=2)\nclass Worker:\n   def __init__(self):\n       with Device(0):\n           self.send1 = cp.ones((4, ), dtype=cp.float32)\n       with Device(1):\n           self.send2 = cp.ones((4, ), dtype=cp.float32) * 2\n       with Device(0):\n           self.recv1 = cp.ones((4, ), dtype=cp.float32)\n       with Device(1):\n           self.recv2 = cp.ones((4, ), dtype=cp.float32) * 2\n\n   def setup(self, world_size, rank):\n       self.rank = rank\n       collective.init_collective_group(world_size, rank, \"nccl\", \"177\")\n       return True\n\n   def allreduce_call(self):\n       collective.allreduce_multigpu([self.send1, self.send2], \"177\")\n       return [self.send1, self.send2]\n\n   def p2p_call(self):\n       if self.rank == 0:\n          collective.send_multigpu(self.send1 * 2, 1, 1, \"8\")\n       else:\n```\n\n----------------------------------------\n\nTITLE: Custom Binary File Processing with BeautifulSoup\nDESCRIPTION: Example of reading binary files and custom parsing using BeautifulSoup to process HTML content.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-text.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\nfrom bs4 import BeautifulSoup\nimport ray\n\ndef parse_html(row: Dict[str, Any]) -> Dict[str, Any]:\n    html = row[\"bytes\"].decode(\"utf-8\")\n    soup = BeautifulSoup(html, features=\"html.parser\")\n    return {\"text\": soup.get_text().strip()}\n\nds = (\n    ray.data.read_binary_files(\"s3://anonymous@ray-example-data/index.html\")\n    .map(parse_html)\n)\n\nds.show()\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray with Custom Resources in Python\nDESCRIPTION: Initializing Ray with specific CPU and GPU resource allocations and custom resource definitions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/configure.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# If not connecting to an existing cluster, you can specify resources overrides:\nray.init(num_cpus=8, num_gpus=1)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Specifying custom resources\nray.init(num_gpus=1, resources={'Resource1': 4, 'Resource2': 16})\n```\n\n----------------------------------------\n\nTITLE: Composing Functions into a DAG using Ray Workflows\nDESCRIPTION: Demonstrates how to create a simple DAG using Ray's remote functions and the bind() method. This example shows the creation of two functions and their composition into a DAG.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/key-concepts.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\ndef one() -> int:\n    return 1\n\n@ray.remote\ndef add(a: int, b: int) -> int:\n    return a + b\n\ndag = add.bind(100, one.bind())\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom Namespace for Actors\nDESCRIPTION: Examples showing how to create actors with explicitly specified namespaces, independent of the current job's namespace.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/namespaces.rst#2025-04-12_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nSystem.setProperty(\"ray.address\", \"localhost:10001\");\ntry {\n    Ray.init();\n    // Create an actor with specified namespace.\n    Ray.actor(Actor::new).setName(\"my_actor\", \"actor_namespace\").remote();\n    // It is accessible in its namespace.\n    Ray.getActor(\"my_actor\", \"actor_namespace\").isPresent(); // return true\n} finally {\n    Ray.shutdown();\n}\n```\n\nLANGUAGE: c++\nCODE:\n```\nray::RayConfig config;\nray::Init(config);\n// Create an actor with specified namespace.\nray::Actor(RAY_FUNC(Counter::FactoryCreate)).SetName(\"my_actor\", \"actor_namespace\").Remote();\n// It is accessible in its namespace.\nray::GetActor<Counter>(\"orange\");\nray::Shutdown();\n```\n\n----------------------------------------\n\nTITLE: Shared Encoder RLModule Implementation\nDESCRIPTION: This code implements a shared encoder module that processes raw observations into embeddings. The encoder uses a simple neural network to transform observations into a fixed-size embedding vector that can be used by different policy networks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass VPGSharedEncoderRLModule(TorchRLModule):\n    \"\"\"Encoder RLModule to produce embeddings for policy RLModules.\n\n    This module will be shared by all policy RLModules in the multi-agent setup.\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        model_config_dict: dict,\n    ):\n        super().__init__(observation_space, action_space, model_config_dict)\n\n        # Our shared encoder module is just a simple MLP.\n        model_config = ModelConfig.from_dict(model_config_dict)\n        input_dim = get_observation_dim(self.observation_space)\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 32),\n            nn.ReLU(),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n        )\n\n    def encode(self, observations, *, timestep=None):\n        # Convert the raw observations to embeddings using our encoder.\n        return self.encoder(observations)\n\n    def forward(\n        self,\n        observations,\n        *,\n        mode=RLModule.Forward.INFERENCE,\n        timestep=None,\n        explore=None,\n        feature_callbacks=None,\n    ):\n        # Only get the encoder's output.\n        embedding = self.encode(observations, timestep=timestep)\n        return embedding\n```\n\n----------------------------------------\n\nTITLE: Custom Checkpoint Saving with PyTorch Lightning\nDESCRIPTION: Shows how to implement a custom callback for PyTorch Lightning to save checkpoints with Ray Train. This example reports a checkpoint every 3 epochs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass CustomCallback(pl.Callback):\n    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n        if trainer.current_epoch % 3 == 0:\n            with tempfile.TemporaryDirectory() as tmpdir:\n                trainer.save_checkpoint(f\"{tmpdir}/model.ckpt\")\n                checkpoint = train.Checkpoint.from_directory(tmpdir)\n                train.report(\n                    metrics=trainer.callback_metrics,\n                    checkpoint=checkpoint,\n                )\n\ndef train_func():\n    model = LightningModule()\n    trainer = pl.Trainer(\n        # ... other trainer args ...\n        callbacks=[CustomCallback()],\n    )\n    trainer.fit(model)\n\ntrainer = train.torch.TorchTrainer(\n    train_func,\n    # ... other TorchTrainer args ...\n)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Training Function for Ray Tune\nDESCRIPTION: This function defines a basic training loop that reports a random loss to Tune. It's used to demonstrate Wandb integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-wandb.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_function(config):\n    for i in range(30):\n        loss = config[\"mean\"] + config[\"sd\"] * np.random.randn()\n        tune.report({\"loss\": loss})\n```\n\n----------------------------------------\n\nTITLE: Basic Accelerate Training Setup with TorchTrainer\nDESCRIPTION: Sample code showing the basic pattern for using Hugging Face Accelerate with Ray Train. It demonstrates how to create an Accelerator instance and prepare components for distributed training within a training function that's passed to TorchTrainer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/huggingface-accelerate.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\n\ndef train_func():\n    # Instantiate the accelerator\n    accelerator = Accelerator(...)\n\n    model = ...\n    optimizer = ...\n    train_dataloader = ...\n    eval_dataloader = ...\n    lr_scheduler = ...\n\n    # Prepare everything for distributed training\n    (\n        model,\n        optimizer,\n        train_dataloader,\n        eval_dataloader,\n        lr_scheduler,\n    ) = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # Start training\n    ...\n\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(...),\n    # If running in a multi-node cluster, this is where you\n    # should configure the run's persistent storage that is accessible\n    # across all worker nodes.\n    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n    ...\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Node with Custom Resources in Shell\nDESCRIPTION: This snippet illustrates the command to start a Ray node with specified CPU, GPU, and custom resources. It is useful for setting up the environment before running Ray tasks or actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/resources.rst#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n\"\"\"\nray start --head --num-cpus=3 --num-gpus=4 --resources='{\\\"special_hardware\\\": 1, \\\"custom_label\\\": 1}'\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoint with Native PyTorch\nDESCRIPTION: Shows how to save a checkpoint using native PyTorch in a Ray Train context. It includes saving model state, optimizer state, and epoch number.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_func():\n    model = MyModel()\n    model = train.torch.prepare_model(model)\n    optimizer = optim.SGD(model.parameters(), lr=0.001)\n\n    for epoch in range(num_epochs):\n        # ... training loop ...\n\n        if should_checkpoint and train.get_context().get_world_rank() == 0:\n            with tempfile.TemporaryDirectory() as tmpdir:\n                torch.save(\n                    {\n                        \"epoch\": epoch,\n                        \"model_state_dict\": model.module.state_dict(),\n                        \"optimizer_state_dict\": optimizer.state_dict(),\n                    },\n                    f\"{tmpdir}/checkpoint.pt\",\n                )\n                checkpoint = train.Checkpoint.from_directory(tmpdir)\n                train.report({\"loss\": loss}, checkpoint=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Setting Training Parameters for Hyperparameter Tuning\nDESCRIPTION: Configures the number of epochs and samples for the hyperparameter search, with separate settings for smoke tests and full runs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# The maximum training epochs\nnum_epochs = 5\n\n# Number of sampls from parameter space\nnum_samples = 10\n```\n\n----------------------------------------\n\nTITLE: Distributed Batch Inference\nDESCRIPTION: Executing distributed batch inference using Ray Data's map_batches API with GPU acceleration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/pytorch_resnet_batch_prediction.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictions = transformed_ds.map_batches(\n    ResnetModel,\n    concurrency=4,  # Use 4 GPUs. Change this number based on the number of GPUs in your cluster.\n    num_gpus=1,  # Specify 1 GPU per model replica.\n    batch_size=720,  # Use the largest batch size that can fit on our GPUs.\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Cluster with Runtime Environment\nDESCRIPTION: Initializes a Ray cluster and specifies a runtime environment that includes the necessary Python packages. This ensures that all the nodes in the cluster will have access to these packages during execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(\n    runtime_env={\n        \"pip\": [\n            \"datasets\",\n            \"evaluate\",\n            # The latest combination accelerate==0.25.0, transformers==4.36.0, deepspeed==0.12.4\n            # has issues with DeepSpeed process group initialization,\n            # and will result in a batch_size validation problem.\n            # TODO(ml-team): get rid of the pins once the issue is fixed.\n            \"accelerate==0.18.0\",\n            \"transformers==4.26.0\",\n            \"torch>=1.12.0\",\n            \"deepspeed==0.12.3\",\n        ],\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet Files with Ray Data (Python)\nDESCRIPTION: This snippet demonstrates how to save a Ray Dataset to Parquet files on disk or cloud storage. It uses the `write_parquet` function of a Ray Dataset object, writing data to the specified location.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"transformed_ds.write_parquet(\"local:///tmp/iris/\")\"\n```\n\n----------------------------------------\n\nTITLE: Defining Vicuna-13B Lightning Module in Python\nDESCRIPTION: This snippet defines a PyTorch Lightning Module for the Vicuna-13B model. It includes model initialization with DeepSpeed sharding, forward pass, training step, and optimizer configuration using DeepSpeedCPUAdam.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport transformers\nimport lightning.pytorch as pl\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom deepspeed.ops.adam import DeepSpeedCPUAdam\n\n\nclass ZeRO3Config:\n    def __init__(self, pl_module):\n        self.config = pl_module.trainer.strategy.config\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def is_zero3(self) -> bool:\n        return True\n\n\ndef enable_transformers_pretrained_deepspeed_sharding(\n    pl_module: \"pl.LightningModule\",\n) -> None:\n    transformers.deepspeed._hf_deepspeed_config_weak_ref = ZeRO3Config(pl_module)\n\n\nclass Vicuna13BModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Enable tf32 for better performance\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n    def setup(self, stage) -> None:\n        # Defer model initialization to inject deepspeed configs to HF.\n        # During initialization, HF transformers can immediately partition \n        # the model across all gpus avoid the overhead in time and memory \n        # copying it on CPU or each GPU first.\n        enable_transformers_pretrained_deepspeed_sharding(self)\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        if self.global_rank == 0:\n            print(\"DeepSpeed Configs: \", self.trainer.strategy.config)\n            print(\"Model Archetecture: \", self.model)\n\n    def forward(self, batch):\n        outputs = self.model(\n            batch[\"input_ids\"],\n            labels=batch[\"labels\"],\n            attention_mask=batch[\"attention_mask\"],\n        )\n        return outputs.loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self.forward(batch)\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, sync_dist=True)\n        return loss\n\n    def configure_optimizers(self):\n        return DeepSpeedCPUAdam(self.parameters(), lr=2e-5, weight_decay=0.01)\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Data Package\nDESCRIPTION: Command to install Ray Data package using pip\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/batch_inference.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"ray[data]\"\n```\n\n----------------------------------------\n\nTITLE: Processing Data With Concurrency Limit in Ray\nDESCRIPTION: This snippet shows how to limit concurrency in Ray tasks by specifying memory requirements. It defines a remote function with a memory resource requirement and applies it to input files, effectively limiting the number of concurrent tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/limit-running-tasks.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(memory=100 * 1024 * 1024)  # 100MB\ndef process_data(file):\n    # Load data from file into memory and process it\n    return result\n\nresults = ray.get([process_data.remote(file) for file in input_files])\n```\n\n----------------------------------------\n\nTITLE: Plotting Metrics for Tune Experiment Results\nDESCRIPTION: Demonstrates how to create plots of learning curves for individual trials and combined plots for all trials using matplotlib.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune_analyze_results.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbest_result.metrics_dataframe.plot(\"training_iteration\", \"mean_accuracy\")\n\nax = None\nfor result in result_grid:\n    label = f\"lr={result.config['lr']:.3f}, momentum={result.config['momentum']}\"\n    if ax is None:\n        ax = result.metrics_dataframe.plot(\"training_iteration\", \"mean_accuracy\", label=label)\n    else:\n        result.metrics_dataframe.plot(\"training_iteration\", \"mean_accuracy\", ax=ax, label=label)\nax.set_title(\"Mean Accuracy vs. Training Iteration for All Trials\")\nax.set_ylabel(\"Mean Test Accuracy\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Ray Tracing via ray.init()\nDESCRIPTION: Example code for initializing Ray with tracing enabled using the ray.init() method. This approach allows you to programmatically enable tracing when connecting to a Ray cluster from within Python code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/ray-tracing.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nray.init(_tracing_startup_hook=\"ray.util.tracing.setup_local_tmp_tracing:setup_tracing\")\n\n@ray.remote\ndef my_function():\n    return 1\n\nobj_ref = my_function.remote()\n```\n\n----------------------------------------\n\nTITLE: Output Best Hyperparameters from Conditional Search with Ray Tune in Python\nDESCRIPTION: Displays the best hyperparameters found that achieve minimum mean loss, processed from a conditional hyperparameter search space.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tune with a Bayesian Optimization Search Algorithm\nDESCRIPTION: Integrates a Bayesian Optimization search algorithm (HyperOpt) with Ray Tune to intelligently navigate the hyperparameter space, resulting in more efficient optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.search.hyperopt import HyperOptSearch\n\nsearch_space = {\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"momentum\": tune.uniform(0.1, 0.9),\n    \"batch_size\": tune.choice([2, 4, 8, 16])\n}\n\nsearch_alg = HyperOptSearch()\nsched = ASHAScheduler(metric=\"loss\", mode=\"min\")\n\ntune_config = tune.TuneConfig(\n    search_alg=search_alg,\n    scheduler=sched,\n    num_samples=4\n)\n\ntuner = tune.Tuner(\n    tune.with_resources(\n        tune.with_parameters(train_cifar),\n        resources={\"cpu\": 2, \"gpu\": 0}\n    ),\n    param_space=search_space,\n    tune_config=tune_config,\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Example Tuning Results Output in Bash\nDESCRIPTION: Example output showing the results of the XGBoost hyperparameter tuning process. The output displays all trials with their parameters and evaluation metrics, highlighting how early stopping terminated underperforming trials after only a few iterations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-xgboost.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n Number of trials: 10/10 (10 TERMINATED)\n +---------------------------------+------------+-------+-------------+-------------+--------------------+-------------+--------+------------------+----------------+--------------+\n | Trial name                      | status     | loc   |         eta |   max_depth |   min_child_weight |   subsample |   iter |   total time (s) |   eval-logloss |   eval-error |\n |---------------------------------+------------+-------+-------------+-------------+--------------------+-------------+--------+------------------+----------------+--------------|\n | train_breast_cancer_ba275_00000 | TERMINATED |       | 0.00205087  |           2 |                  1 |    0.898391 |     10 |        0.380619  |       0.678039 |     0.090909 |\n | train_breast_cancer_ba275_00001 | TERMINATED |       | 0.000183834 |           4 |                  3 |    0.924939 |      1 |        0.0228798 |       0.693009 |     0.111888 |\n | train_breast_cancer_ba275_00002 | TERMINATED |       | 0.0242721   |           7 |                  2 |    0.501551 |     10 |        0.376154  |       0.54472  |     0.06993  |\n | train_breast_cancer_ba275_00003 | TERMINATED |       | 0.000449692 |           5 |                  3 |    0.890212 |      1 |        0.0234981 |       0.692811 |     0.090909 |\n | train_breast_cancer_ba275_00004 | TERMINATED |       | 0.000376393 |           7 |                  2 |    0.883609 |      1 |        0.0231569 |       0.692847 |     0.062937 |\n | train_breast_cancer_ba275_00005 | TERMINATED |       | 0.00231942  |           3 |                  3 |    0.877464 |      2 |        0.104867  |       0.689541 |     0.083916 |\n | train_breast_cancer_ba275_00006 | TERMINATED |       | 0.000542326 |           1 |                  2 |    0.578584 |      1 |        0.0213971 |       0.692765 |     0.083916 |\n | train_breast_cancer_ba275_00007 | TERMINATED |       | 0.0016801   |           1 |                  2 |    0.975302 |      1 |        0.02226   |       0.691999 |     0.083916 |\n | train_breast_cancer_ba275_00008 | TERMINATED |       | 0.000595756 |           8 |                  3 |    0.58429  |      1 |        0.0221152 |       0.692657 |     0.06993  |\n | train_breast_cancer_ba275_00009 | TERMINATED |       | 0.000357845 |           8 |                  1 |    0.637776 |      1 |        0.022635  |       0.692859 |     0.090909 |\n +---------------------------------+------------+-------+-------------+-------------+--------------------+-------------+--------+------------------+----------------+--------------+\n\n\n Best model parameters: {'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.5015513240240503, 'eta': 0.024272050872920895}\n Best model total accuracy: 0.9301\n```\n\n----------------------------------------\n\nTITLE: Weights & Biases Integration with Ray Train\nDESCRIPTION: Shows how to integrate Weights & Biases (W&B) with Ray Train. It includes setting up the W&B API key, initializing W&B, logging metrics, and finishing the run.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import train\nimport wandb\n\n# Step 1\n# This ensures that all ray worker processes have `WANDB_API_KEY` set.\nray.init(runtime_env={\"env_vars\": {\"WANDB_API_KEY\": \"your_api_key\"}})\n\ndef train_func():\n    # Step 1 and 2\n    if train.get_context().get_world_rank() == 0:\n        wandb.init(\n            name=...,\n            project=...,\n            # ...\n        )\n\n    # ...\n    loss = optimize()\n    metrics = {\"loss\": loss}\n\n    # Step 3\n    if train.get_context().get_world_rank() == 0:\n        wandb.log(metrics)\n\n    # ...\n\n    # Step 4\n    # Make sure that all loggings are uploaded to the W&B backend.\n    if train.get_context().get_world_rank() == 0:\n        wandb.finish()\n```\n\n----------------------------------------\n\nTITLE: Setting Workflow Options in Python\nDESCRIPTION: This snippet shows how to set Ray options and workflow-specific options for workflow tasks using decorators and the options() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import workflow\n\n@workflow.options(checkpoint=True)\n@ray.remote(num_cpus=2, num_gpus=3, max_retries=5)\ndef read_data(num: int):\n    return [i for i in range(num)]\n\nread_data_with_options = read_data.options(\n    num_cpus=1, num_gpus=1, **workflow.options(checkpoint=True))\n```\n\n----------------------------------------\n\nTITLE: Full GCP Configuration (Autoscaler)\nDESCRIPTION: This YAML configuration represents a comprehensive Ray autoscaler setup for GCP, showcasing a detailed configuration with various options. It defines node types, resource allocation, and advanced settings for managing a Ray cluster on Google Cloud Platform.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/gcp/example-full.yaml\n            :language: yaml\n\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Tune and LightGBM with Dependencies\nDESCRIPTION: This Bash command installs the necessary packages for running the Ray Tune and LightGBM example, including relevant machine learning libraries. Required dependencies are lightgbm, numpy, scikit-learn, and ray with tune support. This sets up the environment before executing the Python script.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/lightgbm_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[tune]\" lightgbm scikit-learn numpy\n```\n\n----------------------------------------\n\nTITLE: Implementing Bayesian Optimization in Ray Tune\nDESCRIPTION: Example showing how to use Bayesian optimization for hyperparameter tuning with the bayesian-optimization package.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.search.bayesopt import BayesOptSearch\n\nalgo = BayesOptSearch(metric=\"score\", mode=\"min\")\ntuner = tune.Tuner(\n    trainable,\n    tune_config=tune.TuneConfig(\n        search_alg=algo,\n        num_samples=20\n    )\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining an Async Actor in Ray\nDESCRIPTION: Shows how to define an async actor in Ray using async method definitions. Ray automatically detects whether an actor supports async calls.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/async_api.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\n@ray.remote\nclass AsyncActor:\n    async def run_task(self):\n        print(\"started\")\n        await asyncio.sleep(2) # Network, I/O task here\n        print(\"ended\")\n\nactor = AsyncActor.remote()\n# All 5 tasks should start at once. After 2 second they should all finish.\n# they should finish at the same time\nray.get([actor.run_task.remote() for _ in range(5)])\n```\n\n----------------------------------------\n\nTITLE: Streaming HTTP Responses - Python\nDESCRIPTION: This code snippet defines a Serve application that streams results incrementally to the client, suitable for applications that require long-running processes, like text generation or video processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"{literalinclude} doc_code/http_guide/streaming_example.py\\n:start-after: __begin_example__\\n:end-before: __end_example__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Dataset from S3 using Ray Data in Python\nDESCRIPTION: Demonstrates how to load a CSV dataset directly from S3 using Ray Data and preview the first record. This showcases Ray Data's ability to seamlessly integrate with various file systems.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/quickstart.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Load a CSV dataset directly from S3\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\n# Preview the first record\nds.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Fine-Tuned Model in Python\nDESCRIPTION: This Python command shows how to generate images using the fine-tuned Stable Diffusion model. It specifies the model directory, output directory, prompts, and the number of samples to generate per prompt.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/README.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython generate.py \\\n  --model_dir=$TUNED_MODEL_DIR \\\n  --output_dir=$IMAGES_NEW_DIR \\\n  --prompts=\"photo of a $UNIQUE_TOKEN $CLASS_NAME\" \\\n  --num_samples_per_prompt=5\n```\n\n----------------------------------------\n\nTITLE: Using Ray as Joblib Backend for Distributed Scikit-learn\nDESCRIPTION: This code demonstrates how to register Ray as a joblib backend for scikit-learn and run a RandomizedSearchCV operation in a distributed manner using Ray actors. It includes hyperparameter search configuration for an SVC model on the digits dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/joblib.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVC\ndigits = load_digits()\nparam_space = {\n    'C': np.logspace(-6, 6, 30),\n    'gamma': np.logspace(-8, 8, 30),\n    'tol': np.logspace(-4, -1, 30),\n    'class_weight': [None, 'balanced'],\n}\nmodel = SVC(kernel='rbf')\nsearch = RandomizedSearchCV(model, param_space, cv=5, n_iter=300, verbose=10)\n\nimport joblib\nfrom ray.util.joblib import register_ray\nregister_ray()\nwith joblib.parallel_backend('ray'):\n    search.fit(digits.data, digits.target)\n```\n\n----------------------------------------\n\nTITLE: Configuring Worker Fault Tolerance in RLlib with Python\nDESCRIPTION: Enables worker fault tolerance by setting `fault_tolerance` with the parameter `restart_failed_env_runners` to true. This supports self-recovery of interrupted workers and elasticity, allowing training to continue despite worker removal. Dependencies include RLlib and Ray Core's actor fault tolerance capabilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-fault-tolerance.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"config.fault_tolerance(restart_failed_env_runners=True)\"\n```\n\n----------------------------------------\n\nTITLE: Client Script for DeploymentHandle Example\nDESCRIPTION: This Python script defines a client to interact with the LanguageClassifier deployment.  It sends HTTP requests to the deployment with language and name parameters and prints the response.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/model_composition.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresponse = requests.get(\"http://localhost:8000/?language=spanish&name=Dora\").text\nprint(response)\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring an AlgorithmConfig Instance with Method Chaining in Python\nDESCRIPTION: Demonstrates creating an AlgorithmConfig instance and chaining multiple method calls to configure learning rate and number of Learner actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    # Create an `AlgorithmConfig` instance.\n    AlgorithmConfig()\n    # Change the learning rate.\n    .training(lr=0.0005)\n    # Change the number of Learner actors.\n    .learners(num_learners=2)\n)\n```\n\n----------------------------------------\n\nTITLE: Example Tuning Results Output\nDESCRIPTION: Sample output showing the results of a tuning run, displaying trial names, status, hyperparameters, and performance metrics in a tabulated format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n+-----------------------------------------+------------+-------+----------------+----------------+-----------+--------------+-----------+-----------------+----------------------+\n| Trial name                              | status     | loc   |   layer_1_size |   layer_2_size |        lr |   batch_size |      loss |   mean_accuracy |   training_iteration |\n|-----------------------------------------+------------+-------+----------------+----------------+-----------+--------------+-----------+-----------------+----------------------|\n| train_mnist_tune_checkpoint_85489_00000 | TERMINATED |       |            128 |            128 | 0.001     |           64 | 0.108734  |        0.973101 |                   10 |\n| train_mnist_tune_checkpoint_85489_00001 | TERMINATED |       |            128 |            128 | 0.001     |           64 | 0.093577  |        0.978639 |                   10 |\n| train_mnist_tune_checkpoint_85489_00002 | TERMINATED |       |            128 |            256 | 0.0008    |           32 | 0.0922348 |        0.979299 |                   10 |\n| train_mnist_tune_checkpoint_85489_00003 | TERMINATED |       |             64 |            256 | 0.001     |           64 | 0.124648  |        0.973892 |                   10 |\n| train_mnist_tune_checkpoint_85489_00004 | TERMINATED |       |            128 |             64 | 0.001     |           64 | 0.101717  |        0.975079 |                   10 |\n| train_mnist_tune_checkpoint_85489_00005 | TERMINATED |       |             64 |             64 | 0.001     |           64 | 0.121467  |        0.969146 |                   10 |\n| train_mnist_tune_checkpoint_85489_00006 | TERMINATED |       |            128 |            256 | 0.00064   |           32 | 0.053446  |        0.987062 |                   10 |\n| train_mnist_tune_checkpoint_85489_00007 | TERMINATED |       |            128 |            256 | 0.001     |           64 | 0.129804  |        0.973497 |                   10 |\n| train_mnist_tune_checkpoint_85489_00008 | TERMINATED |       |             64 |            256 | 0.0285125 |          128 | 0.363236  |        0.913867 |                   10 |\n| train_mnist_tune_checkpoint_85489_00009 | TERMINATED |       |             32 |            256 | 0.001     |           64 | 0.150946  |        0.964201 |                   10 |\n+-----------------------------------------+------------+-------+----------------+----------------+-----------+--------------+-----------+-----------------+----------------------+\n```\n\n----------------------------------------\n\nTITLE: Fractional GPU Allocation in Ray\nDESCRIPTION: Demonstrates how to use fractional GPU resources in Ray, allowing multiple tasks to share the same GPU. This example initializes Ray with one GPU and creates four tasks each using 25% of a GPU.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nray.init(num_cpus=4, num_gpus=1)\n\n@ray.remote(num_gpus=0.25)\ndef f():\n    import time\n\n    time.sleep(1)\n\n# The four tasks created here can execute concurrently\n# and share the same GPU.\nray.get([f.remote() for _ in range(4)])\n```\n\n----------------------------------------\n\nTITLE: Training a PPO Algorithm for Multiple Iterations\nDESCRIPTION: Executes the training loop for the PPO algorithm over multiple iterations and prints the result dictionary after each training step, which contains metrics about training progress.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\n\nfor _ in range(4):\n    pprint(ppo.train())\n```\n\n----------------------------------------\n\nTITLE: Custom PyTorch Dataset for Image Processing\nDESCRIPTION: This snippet defines a custom PyTorch Dataset class for loading and preprocessing images from an S3 bucket. It includes methods for initialization, length calculation, and item retrieval with image transformation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-pytorch.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\nimport boto3\nfrom botocore import UNSIGNED\nfrom botocore.config import Config\n\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass ImageDataset(Dataset):\n    def __init__(self, bucket_name: str, dir_path: str):\n        self.s3 = boto3.resource(\"s3\", config=Config(signature_version=UNSIGNED))\n        self.bucket = self.s3.Bucket(bucket_name)\n        self.files = [obj.key for obj in self.bucket.objects.filter(Prefix=dir_path)]\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((128, 128)),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        img_name = self.files[idx]\n\n        # Infer the label from the file name.\n        last_slash_idx = img_name.rfind(\"/\")\n        dot_idx = img_name.rfind(\".\")\n        label = int(img_name[last_slash_idx+1:dot_idx])\n\n        # Download the S3 file locally.\n        obj = self.bucket.Object(img_name)\n        tmp = tempfile.NamedTemporaryFile()\n        tmp_name = \"{}.jpg\".format(tmp.name)\n\n        with open(tmp_name, \"wb\") as f:\n            obj.download_fileobj(f)\n            f.flush()\n            f.close()\n            image = Image.open(tmp_name)\n\n        # Preprocess the image.\n        image = self.transform(image)\n\n        return image, label\n\ndataset = ImageDataset(bucket_name=\"ray-example-data\", dir_path=\"batoidea/JPEGImages/\")\n```\n\n----------------------------------------\n\nTITLE: Initializing RunConfig for Ray Train in Python\nDESCRIPTION: This code snippet demonstrates how to create and configure a RunConfig object for use in Ray Train. It shows various settings including experiment name, storage path, stopping criteria, callbacks, checkpoint config, and failure configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/configuration-overview.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import train\nfrom ray.train import RunConfig, CheckpointConfig\nfrom ray.air import FailureConfig\n\nrun_config = RunConfig(\n    name=\"experiment_1\",\n    storage_path=\"/tmp/ray_results\",\n    stop={\n        \"training_iteration\": 100,\n        \"time_total_s\": 600,\n    },\n    callbacks=[YourCustomCallback()],\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=5,\n        checkpoint_score_attribute=\"accuracy\",\n        checkpoint_score_order=\"max\",\n    ),\n    failure_config=FailureConfig(max_failures=3),\n    verbose=2,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop for ResNet on HPU\nDESCRIPTION: Main training loop function for distributed ResNet training on HPUs. It handles dataset preparation, model initialization, and training/validation phases with Ray Train support.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef train_loop_per_worker(configs):\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    # Calculate the batch size for a single worker\n    worker_batch_size = configs[\"batch_size\"] // train.get_context().get_world_size()\n\n    # Download dataset once on local rank 0 worker\n    if train.get_context().get_local_rank() == 0:\n        download_datasets()\n    torch.distributed.barrier()\n\n    # Build datasets on each worker\n    torch_datasets = build_datasets()\n\n    # Prepare dataloader for each worker\n    dataloaders = dict()\n    dataloaders[\"train\"] = DataLoader(\n        torch_datasets[\"train\"], batch_size=worker_batch_size, shuffle=True\n    )\n    dataloaders[\"val\"] = DataLoader(\n        torch_datasets[\"val\"], batch_size=worker_batch_size, shuffle=False\n    )\n\n    # Distribute\n    dataloaders[\"train\"] = train.torch.prepare_data_loader(dataloaders[\"train\"])\n    dataloaders[\"val\"] = train.torch.prepare_data_loader(dataloaders[\"val\"])\n\n    # Obtain HPU device automatically\n    device = train.torch.get_device()\n\n    # Prepare DDP Model, optimizer, and loss function\n    model = initialize_model()\n    model = model.to(device)\n\n    optimizer = optim.SGD(\n        model.parameters(), lr=configs[\"lr\"], momentum=configs[\"momentum\"]\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # Start training loops\n    for epoch in range(configs[\"num_epochs\"]):\n        # Each epoch has a training and validation phase\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                model.train()  # Set model to training mode\n            else:\n                model.eval()  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                with torch.set_grad_enabled(phase == \"train\"):\n                    # Get model outputs and calculate loss\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n\n                # calculate statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += evaluate(outputs, labels)\n\n            size = len(torch_datasets[phase]) // train.get_context().get_world_size()\n            epoch_loss = running_loss / size\n            epoch_acc = running_corrects / size\n\n            if train.get_context().get_world_rank() == 0:\n                print(\n                    \"Epoch {}-{} Loss: {:.4f} Acc: {:.4f}\".format(\n                        epoch, phase, epoch_loss, epoch_acc\n                    )\n                )\n\n            # Report metrics and checkpoint every epoch\n            if phase == \"val\":\n                train.report(\n                    metrics={\"loss\": epoch_loss, \"acc\": epoch_acc},\n                )\n```\n\n----------------------------------------\n\nTITLE: Get-Or-Create a Named Actor in Python\nDESCRIPTION: Demonstrates how to use the get_if_exists option to either get an existing named actor or create a new one if it doesn't exist in Python. This is useful for creating actors only if they don't already exist.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/named-actors.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# This feature is not yet available in Java.\n```\n\n----------------------------------------\n\nTITLE: Handling Model Requests via DeploymentHandle API\nDESCRIPTION: This snippet illustrates how to send requests from an upstream deployment to a multiplexed deployment using the Handle API. It shows the necessary configuration to specify the `multiplexed_model_id` and route requests correctly to the desired models.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/model-multiplexing.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# Use the handle API to send requests to a multiplexed deployment.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Client Implementation\nDESCRIPTION: Client code that demonstrates how to make requests to the composed Ray Serve application. It sends text to be summarized and translated.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/getting_started.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport requests\n\ntext = \"\"\"\nIt was the best of times, it was the worst of times, it was the age of\nwisdom, it was the age of foolishness, it was the epoch of belief, it\nwas the epoch of incredulity, it was the season of Light, it was the\nseason of Darkness, it was the spring of hope, it was the winter of\ndespair.\n\"\"\"\n\nresponse = requests.post(\"http://localhost:8000/\", json=text)\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Environment Based on Worker Index Using RLlib\nDESCRIPTION: Shows how to dynamically choose an environment based on worker and vector indexes in RLlib. The environment changes its behavior depending on these indexes using a custom Python class.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-env.rst#2025-04-12_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass EnvDependingOnWorkerAndVectorIndex(gym.Env):\n    def __init__(self, config):\n        # Pick actual env based on worker and env indexes.\n        self.env = gym.make(\n            choose_env_for(config.worker_index, config.vector_index)\n        )\n        self.action_space = self.env.action_space\n        self.observation_space = self.env.observation_space\n\n    def reset(self, seed, options):\n        return self.env.reset(seed, options)\n\n    def step(self, action):\n        return self.env.step(action)\n\nregister_env(\"multi_env\", lambda config: MultiEnv(config))\n```\n\n----------------------------------------\n\nTITLE: Reporting Training Metrics and Checkpoints with Ray Train in PyTorch\nDESCRIPTION: This code demonstrates how to report metrics and save checkpoints during PyTorch training using Ray Train. It shows how to create a temporary directory for the model checkpoint, save metrics, and report both to Ray Train for monitoring and persistence.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nimport ray.train\n\ndef train_func():\n\n    ...\n\n    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n       torch.save(\n           model.state_dict(), os.path.join(temp_checkpoint_dir, \"model.pt\")\n       )\n\n       metrics = {\"loss\": loss.item()}  # Training/validation metrics.\n\n       # Build a Ray Train checkpoint from a directory\n       checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n\n       # Ray Train will automatically save the checkpoint to persistent storage,\n       # so the local `temp_checkpoint_dir` can be safely cleaned up after.\n       ray.train.report(metrics=metrics, checkpoint=checkpoint)\n\n    ...\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Shakespeare Dataset for GPT-J Fine-Tuning\nDESCRIPTION: Defines preprocessing functions to split text and tokenize the dataset, then applies these transformations using Ray Data's map_batches API.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nblock_size = 512\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\n\n\ndef split_text(batch: pd.DataFrame) -> pd.DataFrame:\n    text = list(batch[\"text\"])\n    flat_text = \"\".join(text)\n    split_text = [\n        x.strip()\n        for x in flat_text.split(\"\\n\")\n        if x.strip() and not x.strip()[-1] == \":\"\n    ]\n    return pd.DataFrame(split_text, columns=[\"text\"])\n\n\ndef tokenize(batch: pd.DataFrame) -> dict:\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n    tokenizer.pad_token = tokenizer.eos_token\n    ret = tokenizer(\n        list(batch[\"text\"]),\n        truncation=True,\n        max_length=block_size,\n        padding=\"max_length\",\n        return_tensors=\"np\",\n    )\n    ret[\"labels\"] = ret[\"input_ids\"].copy()\n    return dict(ret)\n\n\nprocessed_datasets = {\n    key: (\n        ds.map_batches(split_text, batch_format=\"pandas\")\n        .map_batches(tokenize, batch_format=\"pandas\")\n    )\n    for key, ds in ray_datasets.items()\n}\nprocessed_datasets\n```\n\n----------------------------------------\n\nTITLE: Policy RLModule Implementation for MultiRLModule Architecture\nDESCRIPTION: This code defines a VPGPolicyRLModule class that handles action computation based on encoded observations. The module implements both a forward method and a specialized forward_given_embeddings method that processes pre-computed embeddings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass VPGPolicyRLModule(TorchRLModule):\n    \"\"\"Policy RLModule for VPG that recieves embeddings from a shared encoder.\n\n    This module has a forward method to implement the standard RLModule API,\n    but it also has a special `forward_given_embeddings` method that will be\n    called by our MultiRLModule (in the `forward` implementation of\n    VPGUsingSharedEncoderRLModule).\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        model_config_dict: dict,\n    ):\n        super().__init__(observation_space, action_space, model_config_dict)\n\n        # We only have two modules: One to compute logits, one for the value function.\n        model_config = ModelConfig.from_dict(model_config_dict)\n        self.logits_module = nn.Linear(16, get_action_dim(self.action_space))\n        self.value_module = nn.Linear(16, 1)\n```\n\n----------------------------------------\n\nTITLE: Manual Actor Termination via Handle in Python\nDESCRIPTION: Demonstrates how to forcefully terminate a Ray actor using ray.kill() method. This approach bypasses normal Python exit handlers and can optionally support automatic actor restart.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/terminating-actors.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\nclass Actor:\n    pass\n\nactor_handle = Actor.remote()\n\nray.kill(actor_handle)\n```\n\n----------------------------------------\n\nTITLE: Defining Java class for cross-language exception handling\nDESCRIPTION: Defines a Java class that calls a Python function which raises an exception, used to demonstrate cross-language exception handling in Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_10\n\nLANGUAGE: java\nCODE:\n```\npackage io.ray.demo;\n\nimport io.ray.api.ObjectRef;\nimport io.ray.api.Ray;\nimport io.ray.api.function.PyFunction;\n\npublic class MyRayClass {\n\n  public static int raiseExceptionFromPython() {\n    PyFunction<Integer> raiseException = PyFunction.of(\n        \"ray_exception\", \"raise_exception\", Integer.class);\n    ObjectRef<Integer> refObj = Ray.task(raiseException).remote();\n    return refObj.get();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Accelerator (HPU) in Ray Serve Deployment\nDESCRIPTION: Example demonstrating allocation of a custom accelerator (HPU) resource in a Ray Serve deployment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/resource-allocation.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(ray_actor_options={\"resources\": {\"HPU\": 1}})\ndef func(*args):\n    return do_something_with_my_hpu()\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed ZeRO-3 for Vicuna-13B in Python\nDESCRIPTION: This snippet sets up the DeepSpeed ZeRO-3 configuration for training the Vicuna-13B model. It includes settings for mixed precision, optimizer offloading, and other ZeRO-3 specific parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(MODEL_NAME)\nHIDDEN_SIZE = config.hidden_size\n\ndeepspeed_configs = {\n    \"zero_allow_untested_optimizer\": True,\n    \"bf16\": {\"enabled\": True},\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"reduce_bucket_size\": HIDDEN_SIZE * HIDDEN_SIZE,\n        \"stage3_prefetch_bucket_size\": 0.9 * HIDDEN_SIZE * HIDDEN_SIZE,\n        \"stage3_param_persistence_threshold\": 10 * HIDDEN_SIZE,\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Trainable with Actor Reuse in Python\nDESCRIPTION: Advanced example demonstrating how to implement a Trainable class with actor reuse capability to avoid expensive setup operations. Includes reset_config implementation and tuner configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom time import sleep\nimport ray\nfrom ray import tune\nfrom ray.tune.tuner import Tuner\n\n\ndef expensive_setup():\n    print(\"EXPENSIVE SETUP\")\n    sleep(1)\n\n\nclass QuadraticTrainable(tune.Trainable):\n    def setup(self, config):\n        self.config = config\n        expensive_setup()  # use reuse_actors=True to only run this once\n        self.max_steps = 5\n        self.step_count = 0\n\n    def step(self):\n        # Extract hyperparameters from the config\n        h1 = self.config[\"hparam1\"]\n        h2 = self.config[\"hparam2\"]\n\n        # Compute a simple quadratic objective where the optimum is at hparam1=3 and hparam2=5\n        loss = (h1 - 3) ** 2 + (h2 - 5) ** 2\n\n        metrics = {\"loss\": loss}\n\n        self.step_count += 1\n        if self.step_count > self.max_steps:\n            metrics[\"done\"] = True\n\n        # Return the computed loss as the metric\n        return metrics\n\n    def reset_config(self, new_config):\n        # Update the configuration for a new trial while reusing the actor\n        self.config = new_config\n        return True\n\n\nray.init()\n\n\ntuner_with_reuse = Tuner(\n    QuadraticTrainable,\n    param_space={\n        \"hparam1\": tune.uniform(-10, 10),\n        \"hparam2\": tune.uniform(-10, 10),\n    },\n    tune_config=tune.TuneConfig(\n        num_samples=10,\n        max_concurrent_trials=1,\n        reuse_actors=True,  # Enable actor reuse and avoid expensive setup\n    ),\n    run_config=ray.tune.RunConfig(\n        verbose=0,\n        checkpoint_config=ray.tune.CheckpointConfig(checkpoint_at_end=False),\n    ),\n)\ntuner_with_reuse.fit()\n```\n\n----------------------------------------\n\nTITLE: Analyzing Training Results and Loading Checkpoints\nDESCRIPTION: Examines the training results, including validation accuracy and saved checkpoints. Demonstrates how to load the best model from the saved checkpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_mnist_example.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Validation Accuracy: \", result.metrics[\"val_accuracy\"])\nprint(\"Trial Directory: \", result.path)\nprint(sorted(os.listdir(result.path)))\n\ncheckpoint = result.checkpoint\n\nwith checkpoint.as_directory() as ckpt_dir:\n    best_model = MNISTClassifier.load_from_checkpoint(f\"{ckpt_dir}/checkpoint.ckpt\")\n\nbest_model\n```\n\n----------------------------------------\n\nTITLE: Modified Train Epoch Function for Ray Train\nDESCRIPTION: Updated train_epoch function that sets the dataloader's epoch for distributed sampling and removes manual device placement since Ray Train handles this automatically.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport ray.train\n\ndef train_epoch(epoch, dataloader, model, loss_fn, optimizer):\n    if ray.train.get_context().get_world_size() > 1:\n        dataloader.sampler.set_epoch(epoch)\n\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # We don't need this anymore! Ray Train does this automatically:\n        # X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n```\n\n----------------------------------------\n\nTITLE: Running the Experiment to Minimize the Mean Loss in Python\nDESCRIPTION: This snippet sets up and runs the tuning experiment using the defined objective function, search space, and tuning configuration. It executes the tuning process and captures the results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n        metric=\"mean_loss\",\n        mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space=search_space,\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Passing Object References Between Tasks in Ray Workflow\nDESCRIPTION: This snippet demonstrates how Ray object references can be passed into and returned from workflow tasks. The contents are logged to durable storage before execution to ensure recoverability.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef do_add(a, b):\n    return a + b\n\n@ray.remote\ndef add(a, b):\n    return do_add.remote(a, b)\n\nworkflow.run(add.bind(ray.put(10), ray.put(20))) == 30\n```\n\n----------------------------------------\n\nTITLE: Defining gRPC Context Application in Ray Serve\nDESCRIPTION: Shows how to define a Ray Serve application that utilizes gRPC context for setting custom status codes, details, and trailing metadata. Uses RayServegRPCContext for request metadata handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/grpc-guide.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:start-after: __begin_grpc_context_define_app__\n:end-before: __end_grpc_context_define_app__\n:language: python\n```\n\n----------------------------------------\n\nTITLE: Two Actors Example\nDESCRIPTION: This Python code demonstrates the memory monitor's preference for killing retriable tasks/actors. It submits two actors: one retriable and one non-retriable. When the node runs out of memory, the memory monitor will kill the retriable actor first.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/ray-oom-prevention.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\n@ray.remote(max_restarts=1)\nclass Actor1:\n    def __init__(self):\n        pass\n    def ping(self):\n        return \"pong1\"\n\n@ray.remote\nclass Actor2:\n    def __init__(self):\n        pass\n    def ping(self):\n        return \"pong2\"\n\nif __name__ == \"__main__\":\n    ray.init()\n\n    # Give actor1 higher priority.\n    a1 = Actor1.remote()\n    # Give actor2 low priority.\n    a2 = Actor2.remote()\n\n    # Wait some time for the actors to start up.\n    time.sleep(3)\n\n    # Kill first.\n    try:\n        ray.get(a2.ping.remote(), timeout=1)\n    except ray.exceptions.RayTaskError as e:\n        print(\"First started actor, which is retriable, was killed by the memory monitor.\")\n\n    # Kill second.\n    print(\"Second started actor, which is not-retriable, finished.\")\n    ray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Actor Task Retries in Python\nDESCRIPTION: This snippet demonstrates different ways to configure task retries for Ray actors. It shows how to set max_task_retries at the actor creation level, class definition level, and the default value. It also explains the interaction between max_task_retries, retry_exceptions, and max_restarts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/actors.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nActor.options(max_task_retries=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(max_task_retries=2)\n```\n\nLANGUAGE: python\nCODE:\n```\nmax_task_retries=5\n```\n\nLANGUAGE: python\nCODE:\n```\nretry_exceptions=True\n```\n\nLANGUAGE: python\nCODE:\n```\nmax_restarts=2\n```\n\n----------------------------------------\n\nTITLE: Implementing Reduce Operation with Ray\nDESCRIPTION: Creates a Ray remote function to implement the reduce phase, which combines word counts from different partitions into a final count dictionary.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/map_reduce.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef apply_reduce(*results):\n    reduce_results = dict()\n    for res in results:\n        for key, value in res:\n            if key not in reduce_results:\n                reduce_results[key] = 0\n            reduce_results[key] += value\n\n    return reduce_results\n```\n\n----------------------------------------\n\nTITLE: Implementing MultiRLModule with Shared Encoder in PyTorch\nDESCRIPTION: This code snippet demonstrates the implementation of a MultiRLModule class that uses a shared encoder with two separate policy networks. The module processes observations through a single encoder and uses the resulting embeddings as input for the two policy heads.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass VPGUsingSharedEncoderRLModule(MultiRLModule):\n    \"\"\"MultiRLModule for two agent VPG that shares a encoder RLModule instance.\n\n    This MultiRLModule creates its own (child) RLModules: One shared encoder module\n    and two policy modules, which both use this encoder to compute action logits and\n    state values.\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        model_config_dict: dict,\n        *,\n        # We'll use these spaces to create our encoder and policy sub-modules.\n        obs_space_p1: gym.spaces.Space,\n        act_space_p1: gym.spaces.Space,\n        obs_space_p2: gym.spaces.Space,\n        act_space_p2: gym.spaces.Space,\n    ):\n        # Create our three child RLModules: One encoder + two policy nets.\n        self.encoder_module = VPGSharedEncoderRLModule(\n            observation_space=obs_space_p1,  # Both p1 and p2 have same obs space.\n            action_space=act_space_p1,  # Not used by Encoder.\n            model_config_dict=model_config_dict,\n        )\n        self.policy_p1 = VPGPolicyRLModule(\n            observation_space=obs_space_p1,\n            action_space=act_space_p1,\n            model_config_dict=model_config_dict,\n        )\n        self.policy_p2 = VPGPolicyRLModule(\n            observation_space=obs_space_p2,\n            action_space=act_space_p2,\n            model_config_dict=model_config_dict,\n        )\n\n        # Let's tag each child module (just so we know which one was called).\n        self.encoder_module._tag = \"encoder\"\n        self.policy_p1._tag = \"policy1\"\n        self.policy_p2._tag = \"policy2\"\n\n        # Maps RLModule.forward signature to the respective modules' forward methods.\n        modules = {\n            \"p1\": self.policy_p1,\n            \"p2\": self.policy_p2,\n        }\n\n        # Maps forward modes (RLModule.Forward.{INFERENCE, EXPLORATION, PREDICTION})\n        # to the corresponding sub-modules' forward methods.\n        # E.g. maps tuple(\"p1\", RLModule.Forward.INFERENCE) to\n        # self.policy_p1.forward(... mode=RLModule.Forward.INFERENCE, ...).\n        mapping = {\n            (\"p1\", RLModule.Forward.INFERENCE): (\"p1\", RLModule.Forward.INFERENCE),\n            (\"p1\", RLModule.Forward.EXPLORATION): (\"p1\", RLModule.Forward.EXPLORATION),\n            (\"p1\", RLModule.Forward.PREDICTION): (\"p1\", RLModule.Forward.PREDICTION),\n            (\"p2\", RLModule.Forward.INFERENCE): (\"p2\", RLModule.Forward.INFERENCE),\n            (\"p2\", RLModule.Forward.EXPLORATION): (\"p2\", RLModule.Forward.EXPLORATION),\n            (\"p2\", RLModule.Forward.PREDICTION): (\"p2\", RLModule.Forward.PREDICTION),\n        }\n\n        super().__init__(\n            observation_space={\n                \"p1\": obs_space_p1,\n                \"p2\": obs_space_p2,\n            },\n            action_space={\n                \"p1\": act_space_p1,\n                \"p2\": act_space_p2,\n            },\n            model_config_dict=model_config_dict,\n            modules=modules,\n            mapping=mapping,\n        )\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Model for Distributed Training with Ray Train\nDESCRIPTION: Demonstrates how to use ray.train.torch.prepare_model() to configure a PyTorch model for distributed training. This function moves the model to the correct device and wraps it in DistributedDataParallel.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n-from torch.nn.parallel import DistributedDataParallel\n+import ray.train.torch\n\n def train_func():\n\n     ...\n\n     # Create model.\n     model = ...\n\n     # Set up distributed training and device placement.\n-    device_id = ... # Your logic to get the right device.\n-    model = model.to(device_id or \"cpu\")\n-    model = DistributedDataParallel(model, device_ids=[device_id])\n+    model = ray.train.torch.prepare_model(model)\n\n     ...\n```\n\n----------------------------------------\n\nTITLE: Running Tune Experiment with MNIST PyTorch Example\nDESCRIPTION: Sets up and runs a Tune experiment using the MNIST PyTorch example, configuring hyperparameters, run settings, and checkpoint configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune_analyze_results.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom ray import tune\nfrom ray.tune import ResultGrid\nfrom ray.tune.examples.mnist_pytorch import train_mnist\n\nstorage_path = \"/tmp/ray_results\"\nexp_name = \"tune_analyzing_results\"\ntuner = tune.Tuner(\n    train_mnist,\n    param_space={\n        \"lr\": tune.loguniform(0.001, 0.1),\n        \"momentum\": tune.grid_search([0.8, 0.9, 0.99]),\n        \"should_checkpoint\": True,\n    },\n    run_config=tune.RunConfig(\n        name=exp_name,\n        stop={\"training_iteration\": 100},\n        checkpoint_config=tune.CheckpointConfig(\n            checkpoint_score_attribute=\"mean_accuracy\",\n            num_to_keep=5,\n        ),\n        storage_path=storage_path,\n    ),\n    tune_config=tune.TuneConfig(mode=\"max\", metric=\"mean_accuracy\", num_samples=3),\n)\nresult_grid: ResultGrid = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Implementing MultiOutputNode in Ray DAG\nDESCRIPTION: Shows how to use MultiOutputNode when a DAG has more than one output. The dag_node.execute() returns a list of Ray object references passed to MultiOutputNode.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-dag.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray.dag import MultiOutputNode\n\n@ray.remote\ndef multiple_output(x):\n    return x, x * 2, x * 3\n\n@ray.remote\ndef add(x, y):\n    return x + y\n\ndag = multiple_output.bind(1)\nmod = MultiOutputNode(dag, num_outputs=3)\n\nresult_1 = add.bind(mod[0], mod[1])\nresult_2 = add.bind(mod[1], mod[2])\n\nresults = ray.get([result_1.execute(), result_2.execute()])\nprint(results)  # Output: [3, 5]\n```\n\n----------------------------------------\n\nTITLE: Caching Llama2-70b Model from Hugging Face Hub\nDESCRIPTION: This code snippet downloads and caches the Llama2-70b-chat-hf model from the Hugging Face Hub. It requires a Hugging Face token for authentication.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\nsnapshotdownload(\n    \"meta-llama/Llama-2-70b-chat-hf\",\n    # Replace the path if necessary.\n    cache_dir=os.getenv(\"TRANSFORMERS_CACHE\", None),\n    # Specify your Hugging Face token.\n    token=\"\"\n)\n```\n\n----------------------------------------\n\nTITLE: Passing Objects as Arguments in Ray (Python)\nDESCRIPTION: Illustrates how to pass objects as arguments to Ray tasks and actor methods in Python. It shows the difference between passing objects as top-level arguments (by value) and nested arguments (by reference).\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects.rst#2025-04-12_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n@ray.remote\nclass Actor:\n  def __init__(self, arg):\n    pass\n\n  def method(self, arg):\n    pass\n\nobj = ray.put(2)\n\n# Examples of passing objects to actor constructors.\nactor_handle = Actor.remote(obj)  # by-value\nactor_handle = Actor.remote([obj])  # by-reference\n\n# Examples of passing objects to actor method calls.\nactor_handle.method.remote(obj)  # by-value\nactor_handle.method.remote([obj])  # by-reference\n```\n\n----------------------------------------\n\nTITLE: Creating a Tuner with PBT Scheduler in Ray Tune\nDESCRIPTION: Configures a Ray Tune Tuner with Population Based Training scheduler. The setup includes parameter space definition with grid search and sample_from parameters, and configuration to match checkpoint interval with perturbation interval.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntuner = Tuner(\n    train_func,\n    param_space={\n        \"lr\": 0.05,\n        \"h0\": tune.grid_search([0.0, 1.0]),\n        \"h1\": tune.sample_from(lambda spec: 1.0 - spec.config[\"h0\"]),\n        \"num_training_iterations\": 100,\n        # Match `checkpoint_interval` with `perturbation_interval`\n        \"checkpoint_interval\": perturbation_interval,\n    },\n    tune_config=TuneConfig(\n        num_samples=1,\n        # Set the PBT scheduler in this config\n        scheduler=pbt_scheduler,\n    ),\n    run_config=tune.RunConfig(\n        stop={\"training_iteration\": 100},\n        failure_config=tune.FailureConfig(max_failures=3),\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Multi-Application Config - YAML\nDESCRIPTION: A YAML configuration outlining the setup for deploying multiple Ray Serve applications, specifying HTTP and gRPC options, logging configurations, and application details. Required for Serve deployment via CLI. Inputs include application names, routes, and deployment settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/multi-app.md#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nproxy_location: EveryNode\n\nhttp_options:\n  host: 0.0.0.0\n  port: 8000\n\ngrpc_options:\n  port: 9000\n  grpc_servicer_functions: []\n\nlogging_config:\n  encoding: JSON\n  log_level: INFO\n  logs_dir: null\n  enable_access_log: true\n\napplications:\n  - name: app1\n    route_prefix: /classify\n    import_path: image_classifier:app\n    runtime_env: {}\n    deployments:\n      - name: downloader\n      - name: ImageClassifier\n\n  - name: app2\n    route_prefix: /translate\n    import_path: text_translator:app\n    runtime_env: {}\n    deployments:\n      - name: Translator\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Batch Inference Implementation\nDESCRIPTION: Implementation of batch inference using a TensorFlow/Keras neural network. Creates a Dataset from numpy arrays and defines a Predictor class with a simple sequential model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/batch_inference.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport ray\n\nds = ray.data.from_numpy(np.ones((1, 100)))\n\nclass TFPredictor:\n    def __init__(self):\n        from tensorflow import keras\n        input_layer = keras.Input(shape=(100,))\n        output_layer = keras.layers.Dense(1, activation=\"sigmoid\")\n        self.model = keras.Sequential([input_layer, output_layer])\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        return {\"output\": self.model(batch[\"data\"]).numpy()}\n\npredictions = ds.map_batches(TFPredictor, concurrency=2)\npredictions.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Preventing GPU Resource Leakage in Ray Tasks (Python)\nDESCRIPTION: This code demonstrates how Ray, by default, disables worker process reuse for GPU tasks to prevent GPU resource leakage. It shows a task that allocates memory on the GPU but never releases it. Re-enabling worker reuse can be done by setting `max_calls=0` in the `ray.remote` decorator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\n    # By default, ray does not reuse workers for GPU tasks to prevent\n    # GPU resource leakage.\n    @ray.remote(num_gpus=1)\n    def leak_gpus():\n        import tensorflow as tf\n\n        # This task allocates memory on the GPU and then never release it.\n        tf.Session()\n\n```\n\n----------------------------------------\n\nTITLE: Avoiding Node-Specific Custom Resources in Ray (Python)\nDESCRIPTION: Shows an example of using custom resources that are specific to a particular node, which can lead to issues if that node fails. This practice is not recommended for fault-tolerant applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault-tolerance.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\nnode_id = ray.get_runtime_context().node_id\n\n@ray.remote(resources={f\"node:{node_id}\": 0.01})\ndef f():\n    return 1\n\nfuture = f.remote()\nprint(ray.get(future))\n```\n\n----------------------------------------\n\nTITLE: Multi-Model LLM Deployment using Bind Pattern\nDESCRIPTION: Configure and deploy multiple LLM models using Ray Serve's bind pattern\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.llm import LLMConfig, LLMServer, LLMRouter\n\nllm_config1 = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"qwen-0.5b\",\n        model_source=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1, max_replicas=2,\n        )\n    ),\n    accelerator_type=\"A10G\",\n)\n\nllm_config2 = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"qwen-1.5b\",\n        model_source=\"Qwen/Qwen2.5-1.5B-Instruct\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1, max_replicas=2,\n        )\n    ),\n    accelerator_type=\"A10G\",\n)\n\ndeployment1 = LLMServer.as_deployment(llm_config1.get_serve_options(name_prefix=\"vLLM:\")).bind(llm_config1)\ndeployment2 = LLMServer.as_deployment(llm_config2.get_serve_options(name_prefix=\"vLLM:\")).bind(llm_config2)\nllm_app = LLMRouter.as_deployment().bind([deployment1, deployment2])\nserve.run(llm_app, blocking=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring PPO with a Lambda Callback in Python\nDESCRIPTION: This snippet demonstrates how to configure a PPO algorithm with a simple lambda callback that prints the episode return after each episode terminates. It defines a PPOConfig, sets the environment, adds the callback, builds the algorithm, and then trains it.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-callback.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\nppo = config = (\n    PPOConfig()\n    .environment(\"CartPole-v1\")\n    .callbacks(\n        on_episode_end=(\n            lambda episode, **kw: print(f\"Episode done. R={episode.get_return()}\")\n        )\n    )\n    .build()\n)\nppo.train()\n```\n\nLANGUAGE: python\nCODE:\n```\nppo.stop()\n```\n\n----------------------------------------\n\nTITLE: vLLM Ray Serve Configuration for LLM Deployment\nDESCRIPTION: YAML configuration for the Ray Serve deployment that specifies the model, resource requirements, and runtime environment. It configures tensor parallelism across multiple GPUs and uses vLLM as the serving engine.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/vllm-rayservice.md#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n  serveConfigV2: |\n    applications:\n    - name: llm\n      route_prefix: /\n      import_path:  ray-operator.config.samples.vllm.serve:model\n      deployments:\n      - name: VLLMDeployment\n        num_replicas: 1\n        ray_actor_options:\n          num_cpus: 8\n          # NOTE: num_gpus is set automatically based on TENSOR_PARALLELISM\n      runtime_env:\n        working_dir: \"https://github.com/ray-project/kuberay/archive/master.zip\"\n        pip: [\"vllm==0.5.4\"]\n        env_vars:\n          MODEL_ID: \"meta-llama/Meta-Llama-3-8B-Instruct\"\n          TENSOR_PARALLELISM: \"2\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating TorchTrainer for Distributed Training with Ray Train (Python)\nDESCRIPTION: This code snippet instantiates a `TorchTrainer` with 4 workers and uses it to run the distributed training function. `TorchTrainer` simplifies distributed PyTorch training with Ray Train, handling worker management and communication. Placeholders `__torch_trainer_begin__` and `__torch_trainer_end__` and the `:dedent: 4` parameter denote the code block.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/pytorch/torch_quick_start.py\n:language: python\n:start-after: __torch_trainer_begin__\n:end-before: __torch_trainer_end__\n:dedent: 4\n```\"\n```\n\n----------------------------------------\n\nTITLE: Wrapping Async Functions for Remote Tasks in Ray\nDESCRIPTION: Demonstrates how to wrap an async function with a synchronous wrapper to use it as a remote task in Ray, since Ray doesn't directly support asyncio for remote tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/async_api.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync def f():\n    pass\n\n@ray.remote\ndef wrapper():\n    import asyncio\n    asyncio.run(f())\n```\n\n----------------------------------------\n\nTITLE: Creating HorovodTrainer for Distributed Training in Python\nDESCRIPTION: This snippet demonstrates how to create a HorovodTrainer object for distributed training using Ray Train. It sets up the trainer with a training function and scaling configuration, including options for GPU usage and number of workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/horovod.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\nfrom ray.train.horovod import HorovodTrainer\n# For GPU Training, set `use_gpu` to True.\nuse_gpu = False\ntrainer = HorovodTrainer(\n    train_func,\n    scaling_config=ScalingConfig(use_gpu=use_gpu, num_workers=2)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing AsyncIO in Ray Actors\nDESCRIPTION: Demonstrates how to create an async actor in Ray and execute concurrent tasks using asyncio. The example shows both regular and async ray.get operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/async_api.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport asyncio\n\n@ray.remote\nclass AsyncActor:\n    # multiple invocation of this method can be running in\n    # the event loop at the same time\n    async def run_concurrent(self):\n        print(\"started\")\n        await asyncio.sleep(2) # concurrent workload here\n        print(\"finished\")\n\nactor = AsyncActor.remote()\n\n# regular ray.get\nray.get([actor.run_concurrent.remote() for _ in range(4)])\n\n# async ray.get\nasync def async_get():\n    await actor.run_concurrent.remote()\nasyncio.run(async_get())\n```\n\n----------------------------------------\n\nTITLE: Example of Deployment Configuration Priority\nDESCRIPTION: This example illustrates the priority order for deployment parameters. It shows that Serve uses `num_replicas` from the config file and `graceful_shutdown_timeout_s` from the application code. Other deployment settings use Serve defaults.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/configure-serve-deployment.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(num_replicas=2, graceful_shutdown_timeout_s=6)\nclass ExampleDeployment:\n    ...\n\nexample_app = ExampleDeployment.bind()\n```\n\n----------------------------------------\n\nTITLE: Point-to-Point Communication with send/recv in Python\nDESCRIPTION: Example demonstrating point-to-point communication between Ray actors using send and recv operations. Shows how to set up blocking communication between two workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/ray-collective.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport cupy\nimport ray.util.collective as col\n\n\n@ray.remote(num_gpus=1)\nclass Worker:\n    def __init__(self):\n        self.buffer = cupy.ones((10,), dtype=cupy.float32)\n\n    def get_buffer(self):\n        return self.buffer\n\n    def do_send(self, target_rank=0):\n        # this call is blocking\n        col.send(target_rank)\n\n    def do_recv(self, src_rank=0):\n        # this call is blocking\n        col.recv(src_rank)\n\n    def do_allreduce(self):\n        # this call is blocking as well\n        col.allreduce(self.buffer)\n        return self.buffer\n\n# Create two actors\nA = Worker.remote()\nB = Worker.remote()\n\n# Put A and B in a collective group\ncol.create_collective_group([A, B], options={rank=[0, 1], ...})\n\n# let A to send a message to B; a send/recv has to be specified once at each worker\nray.get([A.do_send.remote(target_rank=1), B.do_recv.remote(src_rank=0)])\n\n# An anti-pattern: the following code will hang, because it doesn't instantiate the recv side call\nray.get([A.do_send.remote(target_rank=1)])\n```\n\n----------------------------------------\n\nTITLE: Using Fractional GPUs in Ray Train\nDESCRIPTION: Demonstrates how to configure fractional GPU usage, allowing multiple workers to share the same GPU by specifying a fractional GPU resource requirement.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\n\nscaling_config = ScalingConfig(\n    num_workers=8,\n    resources_per_worker={\n        \"CPU\": 4,\n        \"GPU\": 0.5,\n    },\n    use_gpu=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Batchbot with FastAPI and Hugging Face Model in Python\nDESCRIPTION: This snippet shows the constructor for the Batchbot class, which initializes the FastAPI app, loads the Hugging Face model and tokenizer, and sets up necessary configurations for batch processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(route_prefix=\"/batchbot\")\n@serve.ingress(app)\nclass Batchbot:\n    def __init__(self):\n        self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.model.eval()\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Map Operations\nDESCRIPTION: Executes the map operations in parallel using Ray's remote execution capabilities. Demonstrates how to collect and process results from multiple mappers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/map_reduce.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmap_results = [\n    apply_map.options(num_returns=num_partitions)\n    .remote(data, num_partitions)\n    for data in partitions\n]\n\nfor i in range(num_partitions):\n    mapper_results = ray.get(map_results[i])\n    for j, result in enumerate(mapper_results):\n        print(f\"Mapper {i}, return value {j}: {result[:2]}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Callback in Ray Tune\nDESCRIPTION: Creates a custom callback class that prints a metric each time a result is received from a trial. The example shows how to define the callback class, implement the on_trial_result method, and pass it to a Tuner via RunConfig.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-metrics.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.tune import Callback\n\n\nclass MyCallback(Callback):\n    def on_trial_result(self, iteration, trials, trial, result, **info):\n        print(f\"Got result: {result['metric']}\")\n\n\ndef train_fn(config):\n    for i in range(10):\n        tune.report({\"metric\": i})\n\n\ntuner = tune.Tuner(\n    train_fn,\n    run_config=tune.RunConfig(callbacks=[MyCallback()]))\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Serve via pip\nDESCRIPTION: This snippet shows how to install Ray Serve and its dependencies using pip. It is the first step before deploying models using Ray Serve.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/index.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[serve]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Periodic Checkpointing with Class API in Ray Tune\nDESCRIPTION: Demonstrates how to enable periodic checkpointing for Class API Trainables by setting the checkpoint_frequency parameter in the run configuration. This will automatically checkpoint trials every N iterations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-trial-checkpoints.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    MyTrainable,\n    run_config=air.RunConfig(\n        checkpoint_config=CheckpointConfig(\n            checkpoint_frequency=5,  # checkpoint every 5 iterations\n            num_to_keep=2,  # keep the 2 most recent checkpoints\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Annotations with Ray Data\nDESCRIPTION: Uses Ray Data to read and parse annotation files in parallel from an S3 bucket.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_detection.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\n\npath = \"s3://anonymous@air-example-data/AnimalDetection/Annotations\"\nannotations: ray.data.Dataset = (\n    ray.data.read_binary_files(path)\n    .map(decode_annotation)\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Population Based Training (PBT) Scheduler\nDESCRIPTION: This snippet demonstrates how to configure and use Population Based Training for hyperparameter optimization. It shows how to set up the scheduler with perturbation intervals and hyperparameter mutation spaces, including both discrete values for learning rate and continuous values for alpha using tune.uniform.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.tune.schedulers import PopulationBasedTraining\n\npbt_scheduler = PopulationBasedTraining(\n    time_attr='training_iteration',\n    metric='loss',\n    mode='min',\n    perturbation_interval=1,\n    hyperparam_mutations={\n        \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n        \"alpha\": tune.uniform(0.0, 1.0),\n    }\n)\ntuner = tune.Tuner(\n    train_fn,\n    tune_config=tune.TuneConfig(\n        num_samples=4,\n        scheduler=pbt_scheduler,\n    ),\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Restoring Model from Ray Tune Checkpoint\nDESCRIPTION: Retrieves the algorithm type and checkpoint for a specific location, then restores the trained model from the checkpoint. This demonstrates how to load a serialized model that was saved during the Tune experiment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Get the algorithm used\nsample_algorithm = final_df.loc[[sample_location_id]].algorithm.values[0]\nprint(f\"algorithm type:: {type(sample_algorithm)}\")\n\n# Get a checkpoint directly from the pandas dataframe of Tune results\ncheckpoint = final_df.checkpoint[sample_location_id]\nprint(f\"checkpoint type:: {type(checkpoint)}\")\n\n# Restore a model from checkpoint\nwith checkpoint.as_directory() as tmpdir:\n    with open(os.path.join(tmpdir, \"ckpt.pkl\"), \"rb\") as fin:\n        state_dict = pickle.load(fin)\nsample_model = state_dict[\"model\"]\n```\n\n----------------------------------------\n\nTITLE: Transforming Batches in Ray Datasets\nDESCRIPTION: Shows how to use map_batches to apply vectorized transformations on batches of data in a Ray Dataset. The example increases the brightness of images.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport ray\n\ndef increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\n    return batch\n\nds = (\n    ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\n    .map_batches(increase_brightness)\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Transformed Dataset as Parquet Files using Ray Data\nDESCRIPTION: Demonstrates how to export a processed dataset to Parquet format using Ray Data's write_parquet method. It also verifies the creation of the output files, showcasing Ray Data's capability to save data in various formats.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/quickstart.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Save the transformed dataset as Parquet files\ntransformed_ds.write_parquet(\"/tmp/iris\")\n\n# Verify the files were created\nprint(os.listdir(\"/tmp/iris\"))\n```\n\n----------------------------------------\n\nTITLE: Install Ray Tune and Matplotlib\nDESCRIPTION: This snippet installs Ray Tune and Matplotlib using pip. Ray Tune is used for hyperparameter optimization, and Matplotlib is used for visualization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"!pip install -q -U \\\"ray[tune]\\\" matplotlib\"\n```\n\n----------------------------------------\n\nTITLE: Iterating PyTorch Tensors with Ray Data\nDESCRIPTION: Demonstrates how to iterate over image data as PyTorch tensors using Ray Data's iter_torch_batches method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-pytorch.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport torch\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\n\nfor batch in ds.iter_torch_batches(batch_size=2):\n    print(batch)\n```\n\n----------------------------------------\n\nTITLE: Configuring Runtime Environment Timeout\nDESCRIPTION: This snippet details how to set a timeout for creating the runtime environment in Ray. It showcases both dictionary-based and class-based configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\"setup_timeout_seconds\": 10}\n```\n\n----------------------------------------\n\nTITLE: Initializing TorchTrainer with PyTorch Lightning\nDESCRIPTION: Basic structure for setting up Ray's TorchTrainer with PyTorch Lightning. Defines a training function and scaling configuration for a distributed training job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig\n\ndef train_func():\n    # Your PyTorch Lightning training code here.\n\nscaling_config = ScalingConfig(num_workers=2, use_gpu=True)\ntrainer = TorchTrainer(train_func, scaling_config=scaling_config)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Task-Based Prediction Driver Code\nDESCRIPTION: Main driver code that launches task-based predictions across multiple files. Demonstrates proper model distribution using ray.put() for efficiency.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/batch_prediction.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninput_files = [\n        f\"s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_full_year_data.parquet\"\n        f\"/fe41422b01c04169af2a65a83b753e0f_{i:06d}.parquet\"\n        for i in range(12)\n]\n\nmodel = load_model()\nmodel_ref = ray.put(model)\n\nresult_refs = []\n\nfor file in input_files:\n    result_refs.append(make_prediction.remote(model_ref, file))\n\nresults = ray.get(result_refs)\n\nfor r in results:\n    print(\"Prediction output size:\", r)\n```\n\n----------------------------------------\n\nTITLE: Hugging Face notebook login\nDESCRIPTION: Logs into the Hugging Face Hub using the `notebook_login` function, prompting the user for their username and password. This is required to push the model to the Hugging Face Hub.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Groupby and Group Transformations with Ray Data (NumPy)\nDESCRIPTION: This snippet demonstrates how to use Ray Data to perform groupby operations and apply transformations to groups. It creates a dataset of images, groups them by label, and applies a normalization function to each group.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport ray\n\nitems = [\n    {\"image\": np.zeros((32, 32, 3)), \"label\": label}\n    for _ in range(10) for label in range(100)\n]\n\ndef normalize_images(group: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    group[\"image\"] = (group[\"image\"] - group[\"image\"].mean()) / group[\"image\"].std()\n    return group\n\nds = (\n    ray.data.from_items(items)\n    .groupby(\"label\")\n    .map_groups(normalize_images)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring EnvRunners in RLlib Python\nDESCRIPTION: This snippet shows how to configure the number of EnvRunners in the Ray RLlib framework. It replaces the older RolloutWorkers with more efficient EnvRunners, allowing for improved separation of concerns and cleaner code design.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nconfig.env_runners(\n    num_env_runners=2,  # use this instead of `num_workers`\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nconfig.env_runners(\n    create_env_on_local_worker=False,\n    sample_collector=None,\n    enable_connectors=True,\n    remote_worker_envs=False,\n    remote_env_batch_wait_ms=0,\n    preprocessor_pref=\"deepmind\",\n    enable_tf1_exec_eagerly=False,\n    sampler_perf_stats_ema_coef=None,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nconfig.env_runners(explore=True)  # <- or False\n```\n\n----------------------------------------\n\nTITLE: Configuring Fractional GPU Resources in Ray Serve Deployments\nDESCRIPTION: Example showing how to allocate fractional GPU resources (0.5 GPU each) across two deployments for efficient resource sharing\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/resource-allocation.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(ray_actor_options={\"num_gpus\": 0.5})\ndef func_1(*args):\n    return do_something_with_my_gpu()\n\n@serve.deployment(ray_actor_options={\"num_gpus\": 0.5})\ndef func_2(*args):\n    return do_something_with_my_gpu()\n```\n\n----------------------------------------\n\nTITLE: WebSocket Support in FastAPI - Python\nDESCRIPTION: This snippet illustrates how to handle WebSocket requests with Ray Serve. It includes a FastAPI example to manage WebSocket communications effectively.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"{literalinclude} doc_code/http_guide/websockets_example.py\\n:start-after: __websocket_serve_app_start__\\n:end-before: __websocket_serve_app_end__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: OpenAI Compatible API Integration\nDESCRIPTION: Example of configuring a processor to work with OpenAI-compatible API endpoints for batch inference\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-llms.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport os\nfrom ray.data.llm import HttpRequestProcessorConfig, build_llm_processor\n\nOPENAI_KEY = os.environ[\"OPENAI_API_KEY\"]\nds = ray.data.from_items([\"Hand me a haiku.\"])\n\n\nconfig = HttpRequestProcessorConfig(\n    url=\"https://api.openai.com/v1/chat/completions\",\n    headers={\"Authorization\": f\"Bearer {OPENAI_KEY}\"},\n    qps=1,\n)\n\nprocessor = build_llm_processor(\n    config,\n    preprocess=lambda row: dict(\n        payload=dict(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a bot that responds with haikus.\"},\n                {\"role\": \"user\", \"content\": row[\"item\"]}\n            ],\n            temperature=0.0,\n            max_tokens=150,\n        ),\n    ),\n    postprocess=lambda row: dict(response=row[\"http_response\"][\"choices\"][0][\"message\"][\"content\"]),\n)\n\nds = processor(ds)\nprint(ds.take_all())\n```\n\n----------------------------------------\n\nTITLE: Managing Data with Ray's Distributed Object Store\nDESCRIPTION: Illustrates different ways to work with Ray objects, including implicit creation from task returns, explicit creation with ray.put(), and passing object references to avoid data copying.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/walkthrough.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport numpy as np\n\n# Implicit object creation - result of a task\n@ray.remote\ndef create_array():\n    return np.random.random(1000)\n\nfuture = create_array.remote()  # Returns an object reference\n\n# Explicit object creation\nlarge_array = np.random.random(1000)\narray_ref = ray.put(large_array)  # Put object in store, returns a reference\n\n# Pass references to tasks (no data copying)\n@ray.remote\ndef process_array(array_ref):\n    # The array is fetched from the object store\n    return np.mean(array_ref)\n\nmean_ref = process_array.remote(array_ref)\nprint(ray.get(mean_ref))  # Gets the result\n```\n\n----------------------------------------\n\nTITLE: Implementing Function API Trainable in Ray Tune\nDESCRIPTION: Example showing how to define a trainable function that computes and reports optimization scores for tuning hyperparameters a and b.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef trainable(config):\n    a, b = config[\"a\"], config[\"b\"]\n    for x in range(10):\n        score = a * (x ** 2) + b\n        session.report({\"score\": score})\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Dataset for Pytorch ResNet Finetuning\nDESCRIPTION: Defines data transformations, functions to download and build datasets for training and validation. Includes a separate implementation for SMOKE_TEST to use a subset of data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, models, transforms\nimport numpy as np\n\n# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    \"train\": transforms.Compose(\n        [\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    ),\n    \"val\": transforms.Compose(\n        [\n            transforms.Resize(224),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    ),\n}\n\ndef download_datasets():\n    os.system(\n        \"wget https://download.pytorch.org/tutorial/hymenoptera_data.zip >/dev/null 2>&1\"\n    )\n    os.system(\"unzip hymenoptera_data.zip >/dev/null 2>&1\")\n\n# Download and build torch datasets\ndef build_datasets():\n    torch_datasets = {}\n    for split in [\"train\", \"val\"]:\n        torch_datasets[split] = datasets.ImageFolder(\n            os.path.join(\"./hymenoptera_data\", split), data_transforms[split]\n        )\n    return torch_datasets\n```\n\nLANGUAGE: python\nCODE:\n```\nif SMOKE_TEST:\n    from torch.utils.data import Subset\n\n    def build_datasets():\n        torch_datasets = {}\n        for split in [\"train\", \"val\"]:\n            torch_datasets[split] = datasets.ImageFolder(\n                os.path.join(\"./hymenoptera_data\", split), data_transforms[split]\n            )\n            \n        # Only take a subset for smoke test\n        for split in [\"train\", \"val\"]:\n            indices = list(range(100))\n            torch_datasets[split] = Subset(torch_datasets[split], indices)\n        return torch_datasets\n```\n\n----------------------------------------\n\nTITLE: Calling Java from Python in Ray\nDESCRIPTION: Demonstrates how to call Java static methods and create Java actors from Python using Ray's cross-language feature.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Call a Java static method.\nadd = ray.java_function(\"io.ray.demo.Math\", \"add\")\nresult = ray.get(add.remote(1, 2))\nprint(result)  # 3\n\n# Create a Java actor.\nCounter = ray.java_actor_class(\"io.ray.demo.Counter\")\ncounter = Counter.remote()\nresult = ray.get(counter.increment.remote())\nprint(result)  # 1\nresult = ray.get(counter.increment.remote())\nprint(result)  # 2\n```\n\n----------------------------------------\n\nTITLE: Runtime Environment Error Handling - Python\nDESCRIPTION: Demonstrates how to handle runtime environment setup failures for both tasks and actors in Ray. Shows error catching for RuntimeEnvSetupError when using invalid configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\n@ray.remote\ndef f():\n    pass\n\n@ray.remote\nclass A:\n    def f(self):\n        pass\n\nstart = time.time()\nbad_env = {\"conda\": {\"dependencies\": [\"this_doesnt_exist\"]}}\n\n# [Tasks] will raise `RuntimeEnvSetupError`.\ntry:\n  ray.get(f.options(runtime_env=bad_env).remote())\nexcept ray.exceptions.RuntimeEnvSetupError:\n  print(\"Task fails with RuntimeEnvSetupError\")\n\n# [Actors] will raise `RuntimeEnvSetupError`.\na = A.options(runtime_env=bad_env).remote()\ntry:\n  ray.get(a.f.remote())\nexcept ray.exceptions.RuntimeEnvSetupError:\n  print(\"Actor fails with RuntimeEnvSetupError\")\n```\n\n----------------------------------------\n\nTITLE: Image Generation Client Implementation\nDESCRIPTION: Implements client-side functionality for making requests to the Stable Diffusion endpoint and displaying results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/start.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nendpoint = \"http://localhost:8000/imagine\"\n\n\n@ray.remote(num_cpus=0)\ndef generate_image(prompt, image_size):\n    req = {\"prompt\": prompt, \"img_size\": image_size}\n    resp = requests.get(endpoint, params=req)\n    return resp.content\n\n\ndef show_images(filenames):\n    fig, axs = plt.subplots(1, len(filenames), figsize=(4 * len(filenames), 4))\n    for i, filename in enumerate(filenames):\n        ax = axs if len(filenames) == 1 else axs[i]\n        ax.imshow(plt.imread(filename))\n        ax.axis(\"off\")\n    plt.show()\n\n\ndef main(\n    interactive: bool = False,\n    prompt: str = \"twin peaks sf in basquiat painting style\",\n    num_images: int = 4,\n    image_size: int = 640,\n):\n    try:\n        requests.get(endpoint, timeout=0.1)\n    except Exception as e:\n        raise RuntimeWarning(\n            \"Did you setup the Ray Serve model replicas with `serve.run` \"\n            \"in a previous cell?\"\n        ) from e\n\n    generation_times = []\n    while True:\n        prompt = (\n            prompt\n            if not interactive\n            else input(f\"\\nEnter a prompt (or 'q' to quit):  \")\n        )\n        if prompt.lower() == 'q':\n            break\n\n        print(\"\\nGenerating image(s)...\\n\")\n        start = time.time()\n\n        # Make `num_images` requests to the endpoint at once!\n        images = ray.get(\n            [generate_image.remote(prompt, image_size) for _ in range(num_images)]\n        )\n\n        dirname = f\"{uuid.uuid4().hex[:8]}\"\n        os.makedirs(dirname)\n        filenames = []\n        for i, image in enumerate(images):\n            filename = os.path.join(dirname, f\"{i}.png\")\n            with open(filename, \"wb\") as f:\n                f.write(image)\n            filenames.append(filename)\n\n        elapsed = time.time() - start\n        generation_times.append(elapsed)\n        print(\n            f\"\\nGenerated {len(images)} image(s) in {elapsed:.2f} seconds to \"\n            f\"the directory: {dirname}\\n\"\n        )\n        show_images(filenames)\n        if not interactive:\n            break\n    return np.mean(generation_times) if generation_times else -1\n```\n\n----------------------------------------\n\nTITLE: Distributed Training Loop Configuration in Python\nDESCRIPTION: This snippet defines the 'train_loop_per_worker' function which performs the training steps for each worker, handling parameter broadcasting and loss calculation using Horovod for distributed optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/horovod_simple.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_loop_per_worker(config):\n    import torch\n    import horovod.torch as hvd\n\n    hvd.init()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    mode = config[\"mode\"]\n    net = Net(mode).to(device)\n    optimizer = torch.optim.SGD(\n        net.parameters(),\n        lr=config[\"lr\"],\n    )\n    optimizer = hvd.DistributedOptimizer(optimizer)\n\n    num_steps = 5\n    print(hvd.size())\n    np.random.seed(1 + hvd.rank())\n    torch.manual_seed(1234)\n    hvd.broadcast_parameters(net.state_dict(), root_rank=0)\n    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n\n    start = time.time()\n    x_max = config[\"x_max\"]\n    for step in range(1, num_steps + 1):\n        features = torch.Tensor(np.random.rand(1) * 2 * x_max - x_max).to(device)\n        if mode == \"square\":\n            labels = sq(features)\n        else:\n            labels = qu(features)\n        optimizer.zero_grad()\n        outputs = net(features)\n        loss = torch.nn.MSELoss()(outputs, labels)\n        loss.backward()\n\n        optimizer.step()\n        time.sleep(0.1)\n        tune.report(dict(loss=loss.item()))\n    total = time.time() - start\n    print(f\"Took {total:0.3f} s. Avg: {total / num_steps:0.3f} s.\")\n```\n\n----------------------------------------\n\nTITLE: Running RLlib on Ray (Python)\nDESCRIPTION: This code snippet shows how to run RLlib on Ray for reinforcement learning tasks. It initializes Ray and runs a simple RL algorithm. Placeholders `__quick_start_begin__` and `__quick_start_end__` mark the code block.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} ../rllib/doc_code/rllib_on_ray_readme.py\n:end-before: __quick_start_end__\n:language: python\n:start-after: __quick_start_begin__\n```\"\n```\n\n----------------------------------------\n\nTITLE: Exporting Stable Diffusion Models to ONNX Format\nDESCRIPTION: Script to export VAE and CLIP text encoder models to ONNX format for use with Triton Server. Uses PyTorch's ONNX export functionality with dynamic axis configuration and constant folding optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/triton-server-integration.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import AutoencoderKL\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nprompt = \"Draw a dog\"\nvae = AutoencoderKL.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=True\n)\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nvae.forward = vae.decode\ntorch.onnx.export(\n    vae,\n    (torch.randn(1, 4, 64, 64), False),\n    \"vae.onnx\",\n    input_names=[\"latent_sample\", \"return_dict\"],\n    output_names=[\"sample\"],\n    dynamic_axes={\n        \"latent_sample\": {0: \"batch\", 1: \"channels\", 2: \"height\", 3: \"width\"},\n    },\n    do_constant_folding=True,\n    opset_version=14,\n)\n\ntext_input = tokenizer(\n    prompt,\n    padding=\"max_length\",\n    max_length=tokenizer.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n)\n\ntorch.onnx.export(\n    text_encoder,\n    (text_input.input_ids.to(torch.int32)),\n    \"encoder.onnx\",\n    input_names=[\"input_ids\"],\n    output_names=[\"last_hidden_state\", \"pooler_output\"],\n    dynamic_axes={\n        \"input_ids\": {0: \"batch\", 1: \"sequence\"},\n    },\n    opset_version=14,\n    do_constant_folding=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Define the training function\nDESCRIPTION: Defines the `train_func` which encapsulates the training logic. It initializes the model, tokenizer, datasets, and Trainer from Hugging Face and prepares the trainer for Ray. It sets up the training arguments and the compute metrics function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef train_func(config):\n    print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n\n    metric = load_metric(\"glue\", actual_task)\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_checkpoint, num_labels=num_labels\n    )\n\n    train_ds = ray.train.get_dataset_shard(\"train\")\n    eval_ds = ray.train.get_dataset_shard(\"eval\")\n\n    train_ds_iterable = train_ds.iter_torch_batches(\n        batch_size=batch_size, collate_fn=collate_fn\n    )\n    eval_ds_iterable = eval_ds.iter_torch_batches(\n        batch_size=batch_size, collate_fn=collate_fn\n    )\n\n    print(\"max_steps_per_epoch: \", max_steps_per_epoch)\n\n    args = TrainingArguments(\n        name,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=config.get(\"learning_rate\", 2e-5),\n        num_train_epochs=config.get(\"epochs\", 2),\n        weight_decay=config.get(\"weight_decay\", 0.01),\n        push_to_hub=False,\n        max_steps=max_steps_per_epoch * config.get(\"epochs\", 2),\n        disable_tqdm=True,  # declutter the output a little\n        no_cuda=not use_gpu,  # you need to explicitly set no_cuda if you want CPUs\n        report_to=\"none\",\n    )\n\n    def compute_metrics(eval_pred):\n        predictions, labels = eval_pred\n        if task != \"stsb\":\n            predictions = np.argmax(predictions, axis=1)\n        else:\n            predictions = predictions[:, 0]\n        return metric.compute(predictions=predictions, references=labels)\n\n    trainer = Trainer(\n        model,\n        args,\n        train_dataset=train_ds_iterable,\n        eval_dataset=eval_ds_iterable,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.add_callback(RayTrainReportCallback())\n\n    trainer = prepare_trainer(trainer)\n\n    print(\"Starting training\")\n    trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tune Results with ResultGrid\nDESCRIPTION: Demonstrates how to access various metrics from a ResultGrid object including best trials and hyperparameter configurations after training completion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresults = tuner.fit()\n\n# Get the best result\nbest_result = results.get_best_result()\n\n# Get the best result's metrics\nprint(\"Best result metrics:\", best_result.metrics)\n\n# Get the best result's hyperparameter configuration\nprint(\"Best hyperparameters:\", best_result.config)\n```\n\n----------------------------------------\n\nTITLE: Loading Trained Model Checkpoint for Inference\nDESCRIPTION: This code loads the best checkpoint from the training result into a model for inference. It initializes the model and sets the appropriate device (GPU) for evaluation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = initialize_model_from_checkpoint(result.checkpoint)\ndevice = torch.device(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Breast Cancer Dataset\nDESCRIPTION: Defines a function to load the breast cancer dataset from S3 and split it into train, validation, and test sets.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightgbm/lightgbm_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_data() -> Tuple[Dataset, Dataset, Dataset]:\n    dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer_with_categorical.csv\")\n    train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n    test_dataset = valid_dataset.drop_columns(cols=[\"target\"])\n    return train_dataset, valid_dataset, test_dataset\n```\n\n----------------------------------------\n\nTITLE: Navigating to Project Directory\nDESCRIPTION: Command to change directory to the Llama3.1 fine-tuning project folder.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/aws-trainium/llama3.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd aws-neuron-eks-samples/llama3.1_8B_finetune_ray_ptl_neuron\n```\n\n----------------------------------------\n\nTITLE: Reading Azure Blob Storage Files with Ray Data and ADLFS in Python\nDESCRIPTION: Shows how to read files from Azure Blob Storage using Ray Data with the ADLFS library. It creates an AzureBlobFileSystem object and uses it with the read_parquet function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport adlfs\nimport ray\n\nds = ray.data.read_parquet(\n    \"az://ray-example-data/iris.parquet\",\n    adlfs.AzureBlobFileSystem(account_name=\"azureopendatastorage\")\n)\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Loading Image Dataset with Ray Data\nDESCRIPTION: Uses Ray Data to load a prepared image dataset from S3 and displays its schema.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@air-example-data/AnimalDetection/JPEGImages\")\ndisplay(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Tune hyperparameters with Ray Tune\nDESCRIPTION: Configures and runs hyperparameter tuning using Ray Tune. It defines a search space for the learning rate and epochs, and uses an ASHAScheduler to terminate poorly performing trials. Includes checkpoint configuration for the tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.tune import Tuner\nfrom ray.tune.schedulers.async_hyperband import ASHAScheduler\n\ntune_epochs = 4\ntuner = Tuner(\n    trainer,\n    param_space={\n        \"train_loop_config\": {\n            \"learning_rate\": tune.grid_search([2e-5, 2e-4, 2e-3, 2e-2]),\n            \"epochs\": tune_epochs,\n        }\n    },\n    tune_config=tune.TuneConfig(\n        metric=\"eval_loss\",\n        mode=\"min\",\n        num_samples=1,\n        scheduler=ASHAScheduler(\n            max_t=tune_epochs,\n        ),\n    ),\n    run_config=RunConfig(\n        name=\"tune_transformers\",\n        checkpoint_config=CheckpointConfig(\n            num_to_keep=1,\n            checkpoint_score_attribute=\"eval_loss\",\n            checkpoint_score_order=\"min\",\n        ),\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying Gradio App with GradioServer\nDESCRIPTION: Code to wrap and deploy the Gradio app using Ray Serve's GradioServer\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/gradio-integration.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp = GradioServer.bind(create_app)\n```\n\n----------------------------------------\n\nTITLE: Running the Experiment with Conditional Search Space in Python\nDESCRIPTION: This snippet executes the tuning process using the defined objective function and conditional search space, accommodating dynamic hyperparameter dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model on Ray\nDESCRIPTION: Sets up and trains an XGBoost classifier on Ray with distributed processing. This example uses binary logistic regression for a classification task, configures evaluation metrics, and includes the preprocessor in the checkpoint for later inference.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Set up the XGBoost trainer with the specified configuration\ntrainer = XGBoostTrainer(\n    # see \"How to scale out training?\" for more details\n    scaling_config=ScalingConfig(\n        # Number of workers to use for data parallelism.\n        num_workers=2,\n        # Whether to use GPU acceleration. Set to True to schedule GPU workers.\n        use_gpu=False,\n    ),\n    label_column=\"target\",\n    num_boost_round=20,\n    # XGBoost specific params (see the `xgboost.train` API reference)\n    params={\n        \"objective\": \"binary:logistic\",\n        # uncomment this and set `use_gpu=True` to use GPU for training\n        # \"tree_method\": \"gpu_hist\",\n        \"eval_metric\": [\"logloss\", \"error\"],\n    },\n    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n    # store the preprocessor in the checkpoint for inference later\n    metadata={\"preprocessor_pkl\": preprocessor.serialize()},\n    run_config=run_config,\n)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Limiting Concurrent Ray Train Runs in Python\nDESCRIPTION: Illustrates how to limit the number of concurrent Ray Train runs to manage resource constraints, such as limited GPU availability. By setting the max_concurrent_trials parameter, users can control the number of fully resource-equipped trials running simultaneously, minimizing resource starvation and managing logging verbosity.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/hyperparameter-optimization.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/train_tune_interop.py\n    :language: python\n    :start-after: __max_concurrent_trials_start__\n    :end-before: __max_concurrent_trials_end__\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Crawler Functions\nDESCRIPTION: Core implementation of the web crawler including helper function extract_links to process URLs and main function find_links to recursively crawl pages to a specified depth.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/web-crawler.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef extract_links(elements, base_url, max_results=100):\n    links = []\n    for e in elements:\n        url = e[\"href\"]\n        if \"https://\" not in url:\n            url = base_url + url\n        if base_url in url:\n            links.append(url)\n    return set(links[:max_results])\n\n\ndef find_links(start_url, base_url, depth=2):\n    if depth == 0:\n        return set()\n\n    page = requests.get(start_url)\n    soup = BeautifulSoup(page.content, \"html.parser\")\n    elements = soup.find_all(\"a\", href=True)\n    links = extract_links(elements, base_url)\n\n    for url in links:\n        new_links = find_links(url, base_url, depth-1)\n        links = links.union(new_links)\n    return links\n```\n\n----------------------------------------\n\nTITLE: Production-Ready HTTP Requests with Retries and Backoff\nDESCRIPTION: This snippet shows how to implement production-ready HTTP requests using the `requests` library, including retries with exponential backoff and timeouts.  It enhances the basic request pattern by adding robustness against transient errors and network issues.  Dependencies include the `requests` library and `tenacity` for implementing retry logic.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/best-practices.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Production-ready HTTP requests with retry.\n\"\"\"\nimport requests\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(5),\n    wait=wait_exponential(multiplier=1, min=0.1, max=10),\n    reraise=True,\n)\ndef send_request(host: str, route: str = \"/\") -> None:\n    response = requests.get(f\"http://{host}{route}\", timeout=10)\n    response.raise_for_status()\n    print(f\"Request finished with status code {response.status_code}.\\n\")\n\n\nif __name__ == \"__main__\":\n    host = \"localhost:8000\"\n    send_request(host)\n    send_request(host, route=\"/salutation\")\n\n```\n\n----------------------------------------\n\nTITLE: Querying and Polling Job Status using Ray Jobs REST API in Python\nDESCRIPTION: This code snippet shows how to query and poll for a job's status using the Ray Jobs REST API. It sends GET requests to the job's endpoint and checks the status, breaking the loop if the job reaches a final state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/rest.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\nwhile time.time() - start <= 10:\n    resp = requests.get(\n        f\"http://127.0.0.1:8265/api/jobs/{job_id}\"\n    )\n    rst = json.loads(resp.text)\n    status = rst[\"status\"]\n    print(f\"status: {status}\")\n    if status in {JobStatus.SUCCEEDED, JobStatus.STOPPED, JobStatus.FAILED}:\n        break\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Defining Class Labels for Object Detection\nDESCRIPTION: Defines a dictionary mapping class names to numeric labels for the object detection task.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_detection.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCLASS_TO_LABEL = {\n    \"background\": 0,\n    \"cat\": 1,\n    \"dog\": 2,\n}\n```\n\n----------------------------------------\n\nTITLE: Restoring State from a Checkpoint in Python\nDESCRIPTION: This code snippet demonstrates how to restore a previously saved state into an active Algorithm instance, which enables users to continue training with the restored state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/checkpoints.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnew_ppo = config.build()\nnew_ppo.restore_from_path(checkpoint_dir)\nnew_ppo.train()\n```\n\n----------------------------------------\n\nTITLE: Setting up PPOConfig for Learners - Python\nDESCRIPTION: This snippet illustrates how to configure the PPO algorithm's Learners in RLlib, specifying the resources allocated for training and preparing the algorithm for execution. The configuration includes the number of learners as well as CPU and GPU allocation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-learner.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nconfig = (\n    PPOConfig()\n    .learners(\n        num_learners=0,  # Set this to greater than 1 to allow for DDP style updates.\n        num_gpus_per_learner=0,  # Set this to 1 to enable GPU training.\n        num_cpus_per_learner=1,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Ray Train TorchTrainer\nDESCRIPTION: Creates and configures the Ray Train TorchTrainer with scaling and run configurations, then fits the trainer to start the distributed training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_mnist_example.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nscaling_config = ScalingConfig(num_workers=num_workers, use_gpu=use_gpu)\n\nrun_config = RunConfig(\n    name=\"ptl-mnist-example\",\n    storage_path=\"/tmp/ray_results\",\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=3,\n        checkpoint_score_attribute=\"val_accuracy\",\n        checkpoint_score_order=\"max\",\n    ),\n)\n\ntrainer = TorchTrainer(\n    train_func_per_worker,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Saving Tensor Data in Parquet Format\nDESCRIPTION: Example of saving tensor data to Parquet file format using Ray Data's write_parquet method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-tensors.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\nds.write_parquet(\"/tmp/simple\")\n```\n\n----------------------------------------\n\nTITLE: Calling Deployments via HTTP - Python\nDESCRIPTION: This snippet demonstrates how to configure the ingress deployment for HTTP requests in a Ray Serve application. It explains how the requests are routed to the deployment's __call__ method with a Starlette Request object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"{literalinclude} doc_code/http_guide/http_guide.py\\n:start-after: __begin_starlette__\\n:end-before: __end_starlette__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Turn-Based Tic-Tac-Toe Environment with RLlib in Python\nDESCRIPTION: The code provides an implementation scaffold for a Tic-Tac-Toe turn-based game using RLlib. This Python script outlines the setup of agent IDs, observation and action spaces, and handling game logic with alternating turns. Dependencies include RLlib and the logic for maintaining board states and calculating rewards.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nScaffold for the Tic-Tac-Toe multi-agent environment in RLlib.\n\"\"\"\n# Define your class here\ndef class_definition():\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nConstructor for Tic-Tac-Toe environment, defining agents and spaces.\n\"\"\"\n# Define the constructor here\ndef constructor():\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nReset method for Tic-Tac-Toe, clearing the board and setting the starting player.\n\"\"\"\n# Define the reset function here\ndef reset():\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nStep method for the Tic-Tac-Toe game, handling alternation of player actions and reward calculation.\n\"\"\"\n# Define the step function here\ndef step():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Performing Object Detection Inference with PyTorch\nDESCRIPTION: Uses the pre-trained model to perform object detection on the preprocessed image.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprediction = model(batch)[0]\n```\n\n----------------------------------------\n\nTITLE: Specifying Node Resources with ray.init() in Python\nDESCRIPTION: This snippet shows how to specify node resources manually when starting a single node Ray cluster using ray.init(). It allows developers to control the logical resources allocated to the Ray cluster, overriding default values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/resources.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nray.init(num_cpus=3, num_gpus=4, resources={'special_hardware': 1, 'custom_label': 1})\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Distributed ResNet Inference Class\nDESCRIPTION: Implementation of a ResNet model class for distributed batch inference with GPU support.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/01_batch_inference/start.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport torch\n\n\nclass ResnetModel:\n    def __init__(self):\n        self.weights = ResNet152_Weights.IMAGENET1K_V1\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = models.resnet152(weights=self.weights).to(self.device)\n        self.model.eval()\n\n    def __call__(self, batch: Dict[str, np.ndarray]):\n        torch_batch = torch.from_numpy(batch[\"transformed_image\"]).to(self.device)\n        with torch.inference_mode():\n            prediction = self.model(torch_batch)\n            predicted_classes = prediction.argmax(dim=1).detach().cpu()\n            predicted_labels = [\n                self.weights.meta[\"categories\"][i] for i in predicted_classes\n            ]\n            return {\n                \"predicted_label\": predicted_labels,\n                \"original_image\": batch[\"original_image\"],\n            }\n```\n\n----------------------------------------\n\nTITLE: Evaluating Trained Model on Validation Dataset\nDESCRIPTION: This code implements an evaluation loop to test the trained model on the validation dataset. It calculates and prints the overall accuracy of the model's predictions on the validation data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel = model.to(device)\nmodel.eval()\n\ndownload_datasets()\ntorch_datasets = build_datasets()\ndataloader = DataLoader(torch_datasets[\"val\"], batch_size=32, num_workers=4)\ncorrects = 0\nfor inputs, labels in dataloader:\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    preds = model(inputs)\n    corrects += evaluate(preds, labels)\n\nprint(\"Accuracy: \", corrects / len(dataloader.dataset))\n```\n\n----------------------------------------\n\nTITLE: Block Coalescing Using Repartition and MapBatches in Ray Data\nDESCRIPTION: Demonstrates two approaches to combine small blocks into larger ones: using repartition() for exact block count control and map_batches() for streaming execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Pretend there are two CPUs.\nray.init(num_cpus=2)\n\n# 1. Use ds.repartition().\nds = ray.data.range(10, override_num_blocks=10).repartition(1)\nprint(ds.materialize().stats())\n\n# 2. Use ds.map_batches().\nds = ray.data.range(10, override_num_blocks=10).map_batches(lambda batch: batch, batch_size=10)\nprint(ds.materialize().stats())\n```\n\n----------------------------------------\n\nTITLE: Ensuring Task Scheduling with Ray Placement Groups in Python\nDESCRIPTION: Addresses issues related to resource allocation failures when scheduling tasks within Ray Placement Groups. The solution involves modifying the remote task declaration to exclude the current placement group scheduling strategy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/general-debugging.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import tune\nfrom ray.util.placement_group import PlacementGroupSchedulingStrategy\n\ndef create_task_that_uses_resources():\n    @ray.remote(num_cpus=10, scheduling_strategy=PlacementGroupSchedulingStrategy(placement_group=None))\n    def sample_task():\n        print(\"Hello\")\n        return\n\n    return ray.get([sample_task.remote() for _ in range(10)])\n\ndef objective(config):\n    create_task_that_uses_resources()\n\ntuner = tune.Tuner(objective, param_space={\"a\": 1})\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Nested Parameters with Conditional Sampling in Ray Tune\nDESCRIPTION: Illustrates a complex search space with nested parameters, grid search, and conditional sampling based on other parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-search-spaces.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    my_trainable,\n    run_config=RunConfig(name=\"my_trainable\"),\n    param_space={\n        \"alpha\": tune.sample_from(lambda spec: np.random.uniform(100)),\n        \"beta\": tune.sample_from(lambda spec: spec.config.alpha * np.random.normal()),\n        \"nn_layers\": [\n            tune.grid_search([16, 64, 256]),\n            tune.grid_search([16, 64, 256]),\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing CoNaLa Dataset for Vicuna Fine-tuning\nDESCRIPTION: This snippet loads the CoNaLa dataset, combines curated and mined data, converts it to a Ray Dataset, and applies preprocessing steps including prompt template filling and tokenization. It prepares the data for fine-tuning the Vicuna model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport re\nimport ray\nimport json\nfrom transformers import AutoTokenizer\nfrom datasets import concatenate_datasets, load_dataset\n\n# Combine the curated dataset and automatically-mined dataset\nhf_dataset_curated = load_dataset(\"neulab/conala\")\nhf_dataset_mined = load_dataset(\"neulab/conala\", \"mined\", split=\"train[:5000]\")\nhf_dataset_merged = concatenate_datasets(\n    [hf_dataset_curated[\"train\"], hf_dataset_mined]\n)\nprint(hf_dataset_merged)\n\n# Convert it into Ray Dataset\nray_ds = ray.data.from_huggingface(hf_dataset_merged)\n\n# Build a prompt template for Vicuna-13b model\nPROMPT_TEMPLATE = \"Intent: {intent}\\nOne-line code snippet: {snippet}\"\n\n\ndef fill_prompt(batch):\n    batch[\"input_sentence\"] = batch.apply(\n        lambda row: PROMPT_TEMPLATE.format(\n            intent=row[\"rewritten_intent\"]\n            if row[\"rewritten_intent\"]\n            else row[\"intent\"],\n            snippet=f\"`{row['snippet']}`\",\n        )\n        + \"</s>\",\n        axis=1,\n    )\n    return batch[[\"input_sentence\"]]\n\n\n# Tokenize input sentences to tensors\ndef tokenize(batch):\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_NAME, padding_side=\"left\", use_fast=False\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    ret = tokenizer(\n        list(batch[\"input_sentence\"]),\n        truncation=True,\n        max_length=128,\n        padding=\"max_length\",\n        return_tensors=\"np\",\n    )\n    ret[\"labels\"] = ret[\"input_ids\"].copy()\n    return dict(ret)\n\n# Preprocess train dataset\nprocessed_ds = ray_ds.map_batches(fill_prompt, batch_format=\"pandas\").map_batches(tokenize, batch_format=\"pandas\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Data Loaders for Distributed Training in Ray Train\nDESCRIPTION: Prepares PyTorch data loaders for distributed training by applying Ray Train's data sharding functionality to both training and testing data loaders.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n    test_dataloader = train.torch.prepare_data_loader(test_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Llama Pre-training on Intel Gaudi\nDESCRIPTION: Installs required Python packages including Ray, transformers, datasets, and optimum-habana for pre-training on Intel Gaudi HPUs. Also installs a custom DeepSpeed version compatible with Habana AI processors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ray[train] notebook transformers datasets evaluate peft accelerate scikit-learn optimum-habana\n\npip install git+https://github.com/HabanaAI/DeepSpeed.git@1.20.0\n```\n\n----------------------------------------\n\nTITLE: Anti-pattern: Closure Capturing Large Objects in Ray Remote Functions\nDESCRIPTION: This code snippet demonstrates the anti-pattern of capturing a large object (large_data) in a remote function definition, which can lead to performance issues.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/closure-capture-large-objects.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport numpy as np\n\nray.init()\n\n# Large object (e.g., large numpy array)\nlarge_data = np.random.random((10000, 10000))\n\n@ray.remote\ndef f():\n    return large_data[0]\n\nfuture = f.remote()\nray.get(future)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Agent Algorithm with Multiple RLModuleSpecs\nDESCRIPTION: Illustrates how to configure a multi-agent environment using MultiRLModuleSpec where different RLModules are assigned to each agent, detailing how to map policies to agents for a specific use case.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.core.rl_module.rl_module import RLModuleSpec\nfrom ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\nfrom ray.rllib.examples.envs.classes.multi_agent import MultiAgentCartPole\n\nconfig = (\n    PPOConfig()\n    .environment(MultiAgentCartPole, env_config={\"num_agents\": 2})\n    .multi_agent(\n        policies={\"p0\", \"p1\"},\n        # Agent IDs of `MultiAgentCartPole` are 0 and 1, mapping to\n        # \"p0\" and \"p1\", respectively.\n        policy_mapping_fn=lambda agent_id, episode, **kw: f\"p{agent_id}\"\n    )\n    .rl_module(\n        rl_module_spec=MultiRLModuleSpec(\n            # Agents (0 and 1) use different (single) RLModules.\n            rl_module_specs={\n                \"p0\": RLModuleSpec(\n                    module_class=MyRLModuleClass,\n                    # Small network.\n                    model_config={\"fcnet_hiddens\": [32, 32]},\n                ),\n                \"p1\": RLModuleSpec(\n                    module_class=MyRLModuleClass,\n                    # Large network.\n                    model_config={\"fcnet_hiddens\": [128, 128]},\n                ),\n            },\n        ),\n    )\n)\nppo = config.build()\nprint(ppo.get_module())\n```\n\n----------------------------------------\n\nTITLE: Writing Ray Dataset to Local Parquet Files\nDESCRIPTION: Demonstrates saving a Ray Dataset to local disk in Parquet format using the local:// scheme. Reads CSV data from S3 and writes it locally.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/saving-data.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\nds.write_parquet(\"local:///tmp/iris/\")\n```\n\n----------------------------------------\n\nTITLE: Specifying Path to Environment YAML\nDESCRIPTION: This snippet demonstrates how to specify the path to an environment YAML file used for setting up the runtime environment. The path can either be absolute or relative to the current working directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n\"./environment.yml\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-LoRA Deployment for Ray Serve LLM\nDESCRIPTION: Python code for server-side configuration of a multi-LoRA deployment using Ray Serve LLM. Sets up LoRA config, model loading, and deployment parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.llm import LLMConfig, build_openai_app\n\n# Configure the model with LoRA\nllm_config = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"qwen-0.5b\",\n        model_source=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    ),\n    lora_config=dict(\n        # Let's pretend this is where LoRA weights are stored on S3.\n        # For example\n        # s3://my_dynamic_lora_path/lora_model_1_ckpt\n        # s3://my_dynamic_lora_path/lora_model_2_ckpt\n        # are two of the LoRA checkpoints\n        dynamic_lora_loading_path=\"s3://my_dynamic_lora_path\",\n        max_num_adapters_per_replica=16,\n    ),\n    engine_kwargs=dict(\n        enable_lora=True,\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1,\n            max_replicas=2,\n        )\n    ),\n    accelerator_type=\"A10G\",\n)\n\n# Build and deploy the model\napp = build_openai_app({\"llm_configs\": [llm_config]})\nserve.run(app, blocking=True)\n```\n\n----------------------------------------\n\nTITLE: Logging Scalar Values in MetricsLogger - Python\nDESCRIPTION: This snippet demonstrates how to log a scalar float value using the MetricsLogger API under a specified key. The example shows how values are averaged upon calling the reduce method, allowing for easy tracking of important metrics over time.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.utils.metrics.metrics_logger import MetricsLogger\n\nlogger = MetricsLogger()\n\n# Log a scalar float value under the `loss` key. By default, all logged\n# values under that key are averaged, once `reduce()` is called.\nlogger.log_value(\"loss\", 0.01, reduce=\"mean\", window=2)\n```\n\n----------------------------------------\n\nTITLE: Adding RayTrainReportCallback to PyTorch Lightning Trainer\nDESCRIPTION: This diff demonstrates how to add the RayTrainReportCallback to a PyTorch Lightning Trainer's callbacks list. This callback enables reporting metrics and checkpoints to Ray Train, which supports fault-tolerant training and hyperparameter optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_7\n\nLANGUAGE: diff\nCODE:\n```\nimport lightning.pytorch as pl\nfrom ray.train.lightning import RayTrainReportCallback\n\ndef train_func():\n    ...\n    trainer = pl.Trainer(\n        ...\n-        callbacks=[...],\n+        callbacks=[..., RayTrainReportCallback()],\n    )\n    ...\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with Fused Map Operation in Python\nDESCRIPTION: This snippet shows how to read a CSV file and apply a map operation with fused execution by manually setting the number of blocks to match the number of files.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Pretend there are two CPUs.\nray.init(num_cpus=2)\n\nds = ray.data.read_csv(\"example://iris.csv\", override_num_blocks=1).map(lambda row: row)\nprint(ds.materialize().stats())\n```\n\n----------------------------------------\n\nTITLE: Configuring Worker Node Ray Commands for AWS\nDESCRIPTION: YAML configuration for starting Ray on worker nodes of an AWS cluster. It stops any existing Ray processes, sets file descriptor limits, and starts Ray in worker mode, connecting to the head node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nworker_start_ray_commands:\n  - ray stop\n  - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\n```\n\n----------------------------------------\n\nTITLE: Ray DAG Node Creation and Execution\nDESCRIPTION: Example showing how to create and execute a Ray DAG node with input handling, task binding, and execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/quickstart.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith ray.dag.InputNode() as input_node:\n    # Lazily execute the task\n    output = a.echo.bind(input_node)\n    dag = output.experimental_compile()\n\n# Execute the DAG\ndag.execute(\"hello\")\nray.get(dag)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Locality-Aware Scheduling in Ray\nDESCRIPTION: This code shows how Ray's locality-aware scheduling works. It creates a large object, defines a remote function that uses this object, and calls the function, allowing Ray to schedule the task on a node where the object is local.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/index.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n@ray.remote\ndef create_large_object():\n    return np.zeros(1024 * 1024 * 10, dtype=np.uint8)\n\n@ray.remote\ndef f(large_object):\n    return large_object.nbytes\n\nlarge_object = create_large_object.remote()\nray.get(f.remote(large_object))\n```\n\n----------------------------------------\n\nTITLE: Optimizing Large Data Object Handling in Ray\nDESCRIPTION: Code diff showing the recommended approach for handling large data objects in Ray training functions, moving from passing through config to direct initialization within the training function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/common/torch-configure-train_func.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef load_dataset():\n    # Return a large in-memory dataset\n    ...\n\ndef load_model():\n    # Return a large in-memory model instance\n    ...\n\n-config = {\"data\": load_dataset(), \"model\": load_model()}\n\ndef train_func(config):\n-    data = config[\"data\"]\n-    model = config[\"model\"]\n\n+    data = load_dataset()\n+    model = load_model()\n    ...\n\ntrainer = ray.train.torch.TorchTrainer(train_func, train_loop_config=config, ...)\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Tune (Bash)\nDESCRIPTION: This command installs Ray Tune and its dependencies using pip. `-U` upgrades to the latest version, and `ray[tune]` includes specific dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install -U \\\"ray[tune]\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Serve Logging in Python\nDESCRIPTION: Example of creating a simple Ray Serve deployment that logs custom messages when queried. Shows how to use the standard Python logging module with Ray Serve.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment\nclass SayHello:\n    def __init__(self):\n        pass\n    \n    def __call__(self):\n        logger = logging.getLogger(\"ray.serve\")\n        logger.info(\"Hello world!\")\n        return \"Hello!\"\n```\n\n----------------------------------------\n\nTITLE: Setting Resources for EnvRunner Actors in RLlib\nDESCRIPTION: Illustrates how to allocate CPU and GPU resources per EnvRunner actor in the RLlib configuration. Utilizes Ray's resource allocation features. Configurations like `num_cpus_per_env_runner` and `num_gpus_per_env_runner` allow customized resource distribution. Supports fractional GPU usage for efficient resource management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/scaling-guide.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig.env_runners(\n    num_cpus_per_env_runner=..,\n    num_gpus_per_env_runner=..,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Behavior Cloning Training with RLlib\nDESCRIPTION: Sets up behavior cloning configuration for training on previously recorded CartPole-v1 data. Includes environment setup, offline data configuration, evaluation settings, and tuning parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.rllib.algorithms.bc import BCConfig\n\n# Setup the config for behavior cloning.\nconfig = (\n    BCConfig()\n    .environment(\n        env=\"CartPole-v1\",\n    )\n    .learners(\n        num_learners=0,\n    )\n    .training(\n        train_batch_size_per_learner=1024,\n    )\n    .offline_data(\n        input_=[data_path],\n        input_read_episodes=True,\n        input_read_batch_size=512,\n        map_batches_kwargs={\n            \"concurrency\": 2,\n            \"num_cpus\": 1,\n        },\n        iter_batches_kwargs={\n            \"prefetch_batches\": 2,\n            \"local_shuffle_buffer_size\": None,\n        },\n        dataset_num_iters_per_learner=1,\n    )\n    .evaluation(\n        evaluation_interval=3,\n        evaluation_num_env_runners=1,\n        evaluation_duration=5,\n        evaluation_parallel_to_training=True,\n    )\n)\n\nmetric = f\"{EVALUATION_RESULTS}/{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\"\n\ntuner = tune.Tuner(\n    \"BC\",\n    param_space=config,\n    run_config=tune.RunConfig(\n        name=\"docs_rllib_offline_bc\",\n        stop={metric: 450.0},\n        checkpoint_config=tune.CheckpointConfig(\n            checkpoint_frequency=0,\n            checkpoint_at_end=True,\n        ),\n        verbose=2,\n    )\n)\nanalysis = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for GPT-J Fine-Tuning\nDESCRIPTION: Installs the required Python packages for fine-tuning GPT-J, including datasets, evaluate, accelerate, transformers, torch, and deepspeed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install -q \"datasets\" \"evaluate\" \"accelerate==0.18.0\" \"transformers==4.26.0\" \"torch>=1.12.0\" \"deepspeed==0.12.3\"\n```\n\n----------------------------------------\n\nTITLE: Consuming Data from Ray Dataset using take_batch Method\nDESCRIPTION: Shows how to access dataset contents using the take_batch method in Ray Data. This example extracts the first 3 rows as a batch for processing, demonstrating how to work with subsets of the dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/quickstart.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Extract the first 3 rows as a batch for processing\nprint(transformed_ds.take_batch(batch_size=3))\n```\n\n----------------------------------------\n\nTITLE: Pipeline Data Processing with Ray\nDESCRIPTION: This example illustrates the inefficiency of using `ray.get()` to retrieve results from multiple tasks concurrently. It demonstrates an approach using `ray.wait()` to process results incrementally as they become available, optimizing the total execution duration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport random\nimport ray\n\n@ray.remote\ndef do_some_work(x):\n    time.sleep(random.uniform(0, 4)) # Replace this with work you need to do.\n    return x\n\ndef process_results(results):\n    sum = 0\n    for x in results:\n        time.sleep(1) # Replace this with some processing code.\n        sum += x\n    return sum\n\nstart = time.time()\ndata_list = ray.get([do_some_work.remote(x) for x in range(4)])\nsum = process_results(data_list)\nprint(\"duration =\", time.time() - start, \"\\nresult = \", sum)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport random\nimport ray\n\n@ray.remote\ndef do_some_work(x):\n    time.sleep(random.uniform(0, 4)) # Replace this with work you need to do.\n    return x\n\ndef process_incremental(sum, result):\n    time.sleep(1) # Replace this with some processing code.\n    return sum + result\n\nstart = time.time()\nresult_ids = [do_some_work.remote(x) for x in range(4)]\nsum = 0\nwhile len(result_ids):\n    done_id, result_ids = ray.wait(result_ids)\n    sum = process_incremental(sum, ray.get(done_id[0]))\nprint(\"duration =\", time.time() - start, \"\\nresult = \", sum)\n```\n\n----------------------------------------\n\nTITLE: Run Two Actors Example (Bash)\nDESCRIPTION: This bash command executes the `two_actors.py` script. The output will show that the retriable actor was killed by the memory monitor, while the non-retriable actor finished successfully. This demonstrates that the memory monitor prefers to kill retriable tasks or actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/ray-oom-prevention.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ python two_actors.py\n```\n\n----------------------------------------\n\nTITLE: Creating Ray Train-compatible Training Function\nDESCRIPTION: A training function that integrates Ray Train with PyTorch Lightning. It configures the model, datamodule, and trainer with Ray's distributed training utilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.lightning import (\n    RayDDPStrategy,\n    RayLightningEnvironment,\n    RayTrainReportCallback,\n    prepare_trainer,\n)\n\n\ndef train_func(config):\n    dm = MNISTDataModule(batch_size=config[\"batch_size\"])\n    model = MNISTClassifier(config)\n\n    trainer = pl.Trainer(\n        devices=\"auto\",\n        accelerator=\"auto\",\n        strategy=RayDDPStrategy(),\n        callbacks=[RayTrainReportCallback()],\n        plugins=[RayLightningEnvironment()],\n        enable_progress_bar=False,\n    )\n    trainer = prepare_trainer(trainer)\n    trainer.fit(model, datamodule=dm)\n```\n\n----------------------------------------\n\nTITLE: Saving Ray Dataset with PyTorch Tensor to Parquet\nDESCRIPTION: This snippet demonstrates how to create a Ray Dataset containing a PyTorch tensor and save it to a Parquet file. It imports torch and ray, creates a tensor, wraps it in a dataset, and writes it to a local Parquet file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-pytorch.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport ray\n\ntensor = torch.Tensor(1)\nds = ray.data.from_items([{\"tensor\": tensor}])\n\nds.write_parquet(\"local:///tmp/tensor\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Single-Module Algorithm with RLModuleSpec\nDESCRIPTION: Shows how to configure a single-agent algorithm (PPO) with an RLModuleSpec, specifying the model configuration details and environment settings in the setup process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.core.rl_module.rl_module import RLModuleSpec\n\nconfig = (\n    PPOConfig()\n    .environment(\"CartPole-v1\")\n    .rl_module(\n        rl_module_spec=RLModuleSpec(\n            module_class=MyRLModuleClass,\n            model_config={\"some_key\": \"some_setting\"},\n        ),\n    )\n)\nppo = config.build()\nprint(ppo.get_module())\n```\n\n----------------------------------------\n\nTITLE: Using Deprecated TransformersTrainer API for Hugging Face Integration in Python\nDESCRIPTION: Example showing how to use the older TransformersTrainer API to train a Hugging Face Transformers model with Ray. This approach uses a trainer_init_per_worker function to define a transformers.Trainer and is being deprecated.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-transformers.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM\nfrom datasets import load_dataset\n\nimport ray\nfrom ray.train.huggingface import TransformersTrainer\nfrom ray.train import ScalingConfig\n\n\nhf_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n# optional: preprocess the dataset\n# hf_datasets = hf_datasets.map(preprocess, ...)\n\nray_train_ds = ray.data.from_huggingface(hf_datasets[\"train\"])\nray_eval_ds = ray.data.from_huggingface(hf_datasets[\"validation\"])\n\n# Define the Trainer generation function\ndef trainer_init_per_worker(train_dataset, eval_dataset, **config):\n    MODEL_NAME = \"gpt2\"\n    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n    model = AutoModelForCausalLM.from_config(model_config)\n    args = transformers.TrainingArguments(\n        output_dir=f\"{MODEL_NAME}-wikitext2\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        learning_rate=2e-5,\n        weight_decay=0.01,\n        max_steps=100,\n    )\n    return transformers.Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n    )\n\n# Build a Ray TransformersTrainer\nscaling_config = ScalingConfig(num_workers=4, use_gpu=True)\nray_trainer = TransformersTrainer(\n    trainer_init_per_worker=trainer_init_per_worker,\n    scaling_config=scaling_config,\n    datasets={\"train\": ray_train_ds, \"validation\": ray_eval_ds},\n)\nresult = ray_trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Building DAG with Ray Remote Functions\nDESCRIPTION: Demonstrates how to create a chain of functions using Ray DAG API. Each node can be executed as the root or used as input for other functions to form complex DAGs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-dag.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\ndef add(x, y):\n    return x + y\n\n@ray.remote\ndef double(x):\n    return x * 2\n\n# Build DAG\ndag = double.bind(add.bind(1, 2))\n\n# Execute DAG\nresult = ray.get(dag.execute())\nprint(result)  # Output: 6\n\n# Execute from an intermediate node\nintermediate = add.bind(1, 2)\nresult = ray.get(intermediate.execute())\nprint(result)  # Output: 3\n```\n\n----------------------------------------\n\nTITLE: Configuring and Setting up a Learner in RLlib with Python\nDESCRIPTION: This snippet shows how to configure and set up a Learner in RLlib, focusing on the configuration aspects. It demonstrates how to use PPOConfig to set environment, training parameters like learning rate, and RL module configurations including the model's hidden layers. This configuration is essential for defining the behavior of the learning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/key-concepts.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport ray\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n\n# Configure the Learner.\nconfig = (\n    PPOConfig()\n    .environment(\"Acrobot-v1\")\n    .training(lr=0.0001)\n    .rl_module(model_config=DefaultModelConfig(fcnet_hiddens=[64, 32]))\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Cluster for XGBoost Benchmark on AWS\nDESCRIPTION: YAML configuration for setting up a Ray cluster on AWS with 10 nodes, each having 16 CPU, 64 Gi memory, and 1000 GB disk space. This setup is optimized for running the XGBoost training benchmark.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/examples/ml-example.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Cluster configuration file content\n:language: yaml\n```\n\n----------------------------------------\n\nTITLE: Writing Custom Data Using write_datasink in Python\nDESCRIPTION: This code snippet demonstrates how to use the write_datasink method to write images from a Dataset to files using a custom ImageDatasink.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/custom-datasource-example.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nds.write_datasink(ImageDatasink(\"/path/to/output/directory\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Logger Callback for Ray Tune in Python\nDESCRIPTION: This snippet defines a custom logger callback for Ray Tune by inheriting from the LoggerCallback interface. It implements methods for logging trial start, results, and completion, writing to a specified file in the trial's log directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, List\n\nimport json\nimport os\n\nfrom ray.tune.logger import LoggerCallback\n\n\nclass CustomLoggerCallback(LoggerCallback):\n    \"\"\"Custom logger interface\"\"\"\n\n    def __init__(self, filename: str = \"log.txt\"):\n        self._trial_files = {}\n        self._filename = filename\n\n    def log_trial_start(self, trial: \"Trial\"):\n        trial_logfile = os.path.join(trial.logdir, self._filename)\n        self._trial_files[trial] = open(trial_logfile, \"at\")\n\n    def log_trial_result(self, iteration: int, trial: \"Trial\", result: Dict):\n        if trial in self._trial_files:\n            self._trial_files[trial].write(json.dumps(result))\n\n    def on_trial_complete(self, iteration: int, trials: List[\"Trial\"],\n                          trial: \"Trial\", **info):\n        if trial in self._trial_files:\n            self._trial_files[trial].close()\n            del self._trial_files[trial]\n```\n\n----------------------------------------\n\nTITLE: Configuring Trainer Resources in Ray Train (Deprecated)\nDESCRIPTION: Shows how to adjust the resources used by the Trainer actor itself. This is useful when you need to allocate all available resources to workers and leave none for the coordinator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\n\nscaling_config = ScalingConfig(\n    num_workers=4,\n    resources_per_worker={\n        \"CPU\": 2,\n    },\n    trainer_resources={\n        \"CPU\": 0,\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Complex Search Space Configuration in Ray Tune\nDESCRIPTION: Shows a more complex example of search space configuration, including random sampling, conditional parameters, and grid search for neural network layers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-search-spaces.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    my_trainable,\n    run_config=RunConfig(name=\"my_trainable\"),\n    # num_samples will repeat the entire config 10 times.\n    tune_config=tune.TuneConfig(num_samples=10),\n    param_space={\n        # ``sample_from`` creates a generator to call the lambda once per trial.\n        \"alpha\": tune.sample_from(lambda spec: np.random.uniform(100)),\n        # ``sample_from`` also supports \"conditional search spaces\"\n        \"beta\": tune.sample_from(lambda spec: spec.config.alpha * np.random.normal()),\n        \"nn_layers\": [\n            # tune.grid_search will make it so that all values are evaluated.\n            tune.grid_search([16, 64, 256]),\n            tune.grid_search([16, 64, 256]),\n        ],\n    },\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Caching Actor with RayDaskCallbacks in Python\nDESCRIPTION: This snippet illustrates using a Ray actor in conjunction with Dask callbacks to implement result caching based on task execution time. If a task's execution time exceeds a defined threshold, its result is cached within the actor for future use.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/dask-on-ray.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example combining Dask callbacks with an actor to cache results.\"\"\"\n\nimport time\n\nimport ray\n\nfrom ray.util.dask import RayDaskCallback\n\nCACHE_THRESHOLD = 0.1\n\n\n@ray.remote\nclass CachingActor:\n    def __init__(self):\n        self.cache = {}\n        self.task_durations = {}\n\n    def record_duration(self, task, duration):\n        self.task_durations[task] = duration\n\n    def get_result(self, task):\n        return self.cache.get(task)\n\n    def store_result(self, task, result):\n        self.cache[task] = result\n\n\nclass CachingCallback(RayDaskCallback):\n    def __init__(self, caching_actor):\n        super().__init__()\n        self.caching_actor = caching_actor\n\n    def _ray_pretask(self, dsk, state, task):\n        state[\"t\"] = time.time()\n        result = ray.get(self.caching_actor.get_result.remote(task))\n        if result is not None:\n            state[\"result\"] = result\n        else:\n            state[\"result\"] = None\n\n    def _ray_posttask(self, dsk, state, task, result):\n        duration = time.time() - state[\"t\"]\n        ray.get(self.caching_actor.record_duration.remote(task, duration))\n        if duration > CACHE_THRESHOLD:\n            if state[\"result\"] is None:\n                ray.get(self.caching_actor.store_result.remote(task, result))\n\n```\n\n----------------------------------------\n\nTITLE: Defining Image Preprocessing Function for Ray Data\nDESCRIPTION: Creates a function to preprocess images for the object detection model, compatible with Ray Data's map operation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom torchvision.models.detection import (FasterRCNN_ResNet50_FPN_V2_Weights,\n                                          fasterrcnn_resnet50_fpn_v2)\nfrom typing import Dict\n\n\ndef preprocess_image(data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n    preprocessor = transforms.Compose(\n        [transforms.ToTensor(), weights.transforms()]\n    )\n    return {\n        \"image\": data[\"image\"],\n        \"transformed\": preprocessor(data[\"image\"]),\n    }\n```\n\n----------------------------------------\n\nTITLE: Initializing Collective Groups with Worker Actors in Python\nDESCRIPTION: Example demonstrating how to initialize collective groups with Ray actors using both imperative and declarative approaches. Shows setting up a collective group with multiple GPU workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/ray-collective.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport ray.util.collective as collective\n\nimport cupy as cp\n\n\n@ray.remote(num_gpus=1)\nclass Worker:\n   def __init__(self):\n       self.send = cp.ones((4, ), dtype=cp.float32)\n       self.recv = cp.zeros((4, ), dtype=cp.float32)\n\n   def setup(self, world_size, rank):\n       collective.init_collective_group(world_size, rank, \"nccl\", \"default\")\n       return True\n\n   def compute(self):\n       collective.allreduce(self.send, \"default\")\n       return self.send\n\n   def destroy(self):\n       collective.destroy_group()\n\n# imperative\nnum_workers = 2\nworkers = []\ninit_rets = []\nfor i in range(num_workers):\n   w = Worker.remote()\n   workers.append(w)\n   init_rets.append(w.setup.remote(num_workers, i))\n_ = ray.get(init_rets)\nresults = ray.get([w.compute.remote() for w in workers])\n\n\n# declarative\nfor i in range(num_workers):\n   w = Worker.remote()\n   workers.append(w)\n_options = {\n   \"group_name\": \"177\",\n   \"world_size\": 2,\n   \"ranks\": [0, 1],\n   \"backend\": \"nccl\"\n}\ncollective.create_collective_group(workers, **_options)\nresults = ray.get([w.compute.remote() for w in workers])\n```\n\n----------------------------------------\n\nTITLE: Running a Small Grid Search with Ray Tune (Python)\nDESCRIPTION: Runs a hyperparameter grid search using Ray Tune with an iterative training function.  Placeholders `__quick_start_begin__` and `__quick_start_end__` mark the relevant code block.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} ../../../python/ray/tune/tests/example.py\n:end-before: __quick_start_end__\n:language: python\n:start-after: __quick_start_begin__\n```\"\n```\n\n----------------------------------------\n\nTITLE: Optimizing Memory Usage with Smaller Batch Size in Ray Data\nDESCRIPTION: Example demonstrating how reducing batch size to 32 can significantly lower peak heap memory usage when processing large tensors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Pretend there are two CPUs.\nray.init(num_cpus=2)\n\nds = ray.data.range_tensor(1000, shape=(125_000, ), override_num_blocks=1)\nds = ds.map_batches(lambda batch: batch, batch_size=32)\nprint(ds.materialize().stats())\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for Ray Serve Components\nDESCRIPTION: This snippet shows how to configure logging for Ray Serve controller and proxies by passing logging_config to serve.start.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nserve.start(\n    logging_config={\n        \"root\": {\"level\": \"INFO\"},\n        \"ray\": {\"level\": \"WARNING\"},\n        \"grpc\": {\"level\": \"ERROR\"},\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Reading from Snowflake Database with Ray Data\nDESCRIPTION: This snippet demonstrates reading data from a Snowflake database using Ray Data. It defines a connection function using `snowflake.connector` and uses `ray.data.read_sql` to query the database and create a Ray Dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport snowflake.connector\n\nimport ray\n\ndef create_connection():\n    return snowflake.connector.connect(\n        user=...,\n        password=...\n        account=\"ZZKXUVH-IPB52023\",\n        database=\"example\",\n    )\n\n# Get all movies\ndataset = ray.data.read_sql(\"SELECT * FROM movie\", create_connection)\n# Get movies after the year 1980\ndataset = ray.data.read_sql(\n    \"SELECT title, score FROM movie WHERE year >= 1980\", create_connection\n)\n# Get the number of movies per year\ndataset = ray.data.read_sql(\n    \"SELECT year, COUNT(*) FROM movie GROUP BY year\", create_connection\n)\n```\n\n----------------------------------------\n\nTITLE: Using Async Methods in Ray Serve Handlers\nDESCRIPTION: Explains the use of `async def` for handling asynchronous requests in Ray Serve to prevent overloading and manage concurrency more effectively. Adjust the `max_ongoing_requests` in the serve deployment decorator to control backpressure and workload performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/performance.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(max_ongoing_requests=1000)\nasync def async_handler(request):\n    # implementation details\n    pass\n```\n\n----------------------------------------\n\nTITLE: Executing Ray Train Model Training\nDESCRIPTION: Initiates the training process using the trainer.fit() method and stores results.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresults = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Reserving Resources for Deployment in Ray Serve\nDESCRIPTION: Demonstrates how to reserve computational resources (GPUs and CPUs) for deployment replicas using the `ray_actor_options` dictionary within Ray Serve to ensure adequate performance. This example uses YAML configuration for defining resource reservations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/performance.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nray_actor_options={\"num_gpus\": 1, \"num_cpus\": 2}\n```\n\n----------------------------------------\n\nTITLE: Defining MNIST DataModule for PyTorch Lightning\nDESCRIPTION: Creates a PyTorch Lightning DataModule for the MNIST dataset, handling data loading, preprocessing, and splitting into train, validation, and test sets.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_mnist_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=100):\n        super().__init__()\n        self.data_dir = os.getcwd()\n        self.batch_size = batch_size\n        self.transform = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        )\n\n    def setup(self, stage=None):\n        with FileLock(f\"{self.data_dir}.lock\"):\n            mnist = MNIST(\n                self.data_dir, train=True, download=True, transform=self.transform\n            )\n\n            # Split data into train and val sets\n            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n\n    def test_dataloader(self):\n        with FileLock(f\"{self.data_dir}.lock\"):\n            self.mnist_test = MNIST(\n                self.data_dir, train=False, download=True, transform=self.transform\n            )\n        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\n```\n\n----------------------------------------\n\nTITLE: Setting Global Variables for Ray Cluster Configuration\nDESCRIPTION: Defines global variables for the GPT-J model, GPU usage, number of workers, and CPUs per worker for the Ray cluster setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"EleutherAI/gpt-j-6B\"\nuse_gpu = True\nnum_workers = 16\ncpus_per_worker = 8\n```\n\n----------------------------------------\n\nTITLE: Groupby and Group Transformations with Ray Data (pandas)\nDESCRIPTION: This snippet shows how to use Ray Data with pandas DataFrames to perform groupby operations and apply transformations. It reads a CSV file, groups the data by the 'target' column, and applies a normalization function to each group.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport ray\n\ndef normalize_features(group: pd.DataFrame) -> pd.DataFrame:\n    target = group.drop(\"target\")\n    group = (group - group.min()) / group.std()\n    group[\"target\"] = target\n    return group\n\nds = (\n    ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n    .groupby(\"target\")\n    .map_groups(normalize_features)\n)\n```\n\n----------------------------------------\n\nTITLE: Grouping Dataset for Llama Pre-training\nDESCRIPTION: Defines a function to group the tokenized dataset into chunks of a specified block size. This preprocessing step concatenates all texts and generates chunks to optimize pre-training speed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef group_dataset(config, datasets, tokenizer):\n    config_name = config[\"name\"]\n    auto_config = transformers.AutoConfig.from_pretrained(config_name)\n    max_pos_embeddings = auto_config.max_position_embeddings\n    block_size = tokenizer.model_max_length\n    if block_size > max_pos_embeddings:\n        print(\n            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n            f\"Using block_size={min(1024, max_pos_embeddings)} instead. You can change that default value by passing --block_size xxx.\"\n        )\n        if max_pos_embeddings > 0:\n            block_size = min(1024, max_pos_embeddings)\n        else:\n            block_size = 1024\n\n    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n    def group_texts(examples):\n        # Concatenate all texts.\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n        total_length = (total_length // block_size) * block_size\n        # Split by chunks of max_len.\n        result = {\n            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    lm_datasets = datasets.map(\n        group_texts,\n        batched=True,\n        num_proc=None,\n        load_from_cache_file=True,\n        desc=f\"Grouping texts in chunks of {block_size}\",\n    )\n    return lm_datasets\n```\n\n----------------------------------------\n\nTITLE: Building an Algorithm Instance from Config in Python\nDESCRIPTION: Demonstrates how to build an algorithm instance directly from a configuration object using the build_algo() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Build the algorithm instance.\nimpala = config.build_algo()\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Custom Runtime Environment for Vicuna Fine-tuning\nDESCRIPTION: This snippet initializes Ray with a custom runtime environment that includes specific package versions for datasets, torch, deepspeed, accelerate, transformers, and lightning. It also sets constants for the number of workers, batch size, and model name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nNUM_WORKERS = 16\nBATCH_SIZE_PER_WORKER = 8\nMODEL_NAME = \"lmsys/vicuna-13b-v1.3\"\n\nray.init(\n    runtime_env={\n        \"pip\": [\n            \"datasets==2.13.1\",\n            \"torch>=1.13.0\",\n            \"deepspeed==0.12.3\",\n            \"accelerate==0.20.3\",\n            \"transformers==4.30.2\",\n            \"lightning==2.0.3\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: End-to-End Workflow for PyTorch GPU Training on Kubernetes\nDESCRIPTION: A shell script that summarizes the complete workflow for GPU training on GCP, including setting up a Kubernetes cluster with GPU nodes, deploying a Ray cluster with KubeRay, and running a PyTorch training benchmark.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/gpu-training-example.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Step 1: Set up a Kubernetes cluster on GCP\n# Create a node-pool for a CPU-only head node\n# e2-standard-8 => 8 vCPU; 32 GB RAM\ngcloud container clusters create gpu-cluster-1 \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-central1-c --machine-type e2-standard-8\n\n# Create a node-pool for GPU. The node is for a GPU Ray worker node.\n# n1-standard-8 => 8 vCPU; 30 GB RAM\ngcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-tesla-t4,count=1 \\\n  --zone us-central1-c --cluster gpu-cluster-1 \\\n  --num-nodes 1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n  --machine-type n1-standard-8\n\n# Install NVIDIA GPU device driver\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n\n# Step 2: Deploy a Ray cluster on Kubernetes with the KubeRay operator.\n# Please make sure you are connected to your Kubernetes cluster. For GCP, you can do so by:\n#   (Method 1) Copy the connection command from the GKE console\n#   (Method 2) \"gcloud container clusters get-credentials <your-cluster-name> --region <your-region> --project <your-project>\"\n#   (Method 3) \"kubectl config use-context ...\"\n\n# Install both CRDs and KubeRay operator.\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.3.0\n\n# Create a Ray cluster\nkubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/ray-cluster.gpu.yaml\n\n# Set up port-forwarding\nkubectl port-forward services/raycluster-head-svc 8265:8265\n\n# Step 3: Run the PyTorch image training benchmark.\n# Install Ray if needed\npip3 install -U \"ray[default]\"\n\n# Download the Python script\ncurl https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/doc_code/pytorch_training_e2e_submit.py -o pytorch_training_e2e_submit.py\n\n# Submit the training job to your ray cluster\npython3 pytorch_training_e2e_submit.py\n\n# Use the following command to follow this Job's logs:\n# Substitute the Ray Job's submission id.\nray job logs 'raysubmit_xxxxxxxxxxxxxxxx' --address http://127.0.0.1:8265 --follow\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Input and Output Nodes\nDESCRIPTION: References to Ray's InputNode and MultiOutputNode classes for handling input and output in the compiled graph.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/compiled-graph-api.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nray.dag.input_node.InputNode\nray.dag.output_node.MultiOutputNode\n```\n\n----------------------------------------\n\nTITLE: Configuring Reproducible Tune Experiments in Python\nDESCRIPTION: Example of setting up a reproducible Tune experiment by configuring random seeds for both the driver program and the trainable function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport ray\nfrom ray import tune\n\ndef trainable(config):\n    # Set seed for trainable\n    np.random.seed(config[\"seed\"])\n    return {\"score\": np.random.random()}\n\nray.init()\n\n# Set seed for driver\nnp.random.seed(1234)\n\nresult = tune.run(\n    trainable,\n    config={\n        # Make sure to include the seed in config\n        \"seed\": tune.randint(0, 1000000),\n    },\n    num_samples=100,\n    search_alg=tune.search.BasicVariantGenerator(random_state=1234),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Communication Overlap in Ray DAG\nDESCRIPTION: Example demonstrating how to enable GPU communication and computation overlap using the _overlap_gpu_communication parameter in Ray's DAG compilation. Shows performance comparison between enabled and disabled states.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/overlap.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noverlap_gpu_communication=False, duration=1.0670117866247892\\noverlap_gpu_communication=True, duration=0.9211348341777921\n```\n\n----------------------------------------\n\nTITLE: Returning Large Values Using Normal Python Functions in Ray\nDESCRIPTION: This snippet demonstrates a Ray task that returns multiple large numpy arrays using a normal Python function. This approach can lead to high heap memory usage as all arrays are held in memory simultaneously.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/generators.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef large_values(num_returns):\n    return tuple(\n        np.ones((25000, 1000), dtype=np.float32)\n        for _ in range(num_returns)\n    )\n```\n\n----------------------------------------\n\nTITLE: Querying TensorFlow Model via HTTP\nDESCRIPTION: Python code to send a test request to the deployed TensorFlow MNIST model\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/serve-ml-models.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport numpy as np\n\nresp = requests.get(\n    \"http://localhost:8000/\", json={\"array\": np.random.randn(28 * 28).tolist()}\n)\nprint(resp.json())\n```\n\n----------------------------------------\n\nTITLE: Training ResNet on HPU with Ray TorchTrainer\nDESCRIPTION: Function that sets up and runs distributed ResNet training on HPU accelerators. It configures batch size, training epochs, learning parameters, and HPU-specific settings for the Ray distributed training framework.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef train_resnet(num_workers=2):\n    global_batch_size = 16\n\n    train_loop_config = {\n        \"input_size\": 224,  # Input image size (224 x 224)\n        \"batch_size\": 32,  # Batch size for training\n        \"num_epochs\": 10,  # Number of epochs to train for\n        \"lr\": 0.001,  # Learning Rate\n        \"momentum\": 0.9,  # SGD optimizer momentum\n    }\n    # Configure computation resources\n    # In ScalingConfig, require an HPU for each worker\n    scaling_config = ScalingConfig(num_workers=num_workers, resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n    # Set backend to hccl in TorchConfig\n    torch_config = TorchConfig(backend = \"hccl\")\n    \n    ray.init()\n    \n    # Initialize a Ray TorchTrainer\n    trainer = TorchTrainer(\n        train_loop_per_worker=train_loop_per_worker,\n        train_loop_config=train_loop_config,\n        torch_config=torch_config,\n        scaling_config=scaling_config,\n    )\n\n    result = trainer.fit()\n    print(f\"Training result: {result}\")\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Hyperparameter Tuning with Ray Tune for PyTorch\nDESCRIPTION: Defines the main function that sets up a Ray Tune experiment for a PyTorch CIFAR model. It configures the ASHA scheduler for early stopping, creates a tuner with specified resources per trial, runs the tuning process, and reports the best results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-cifar.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef main(config, gpus_per_trial=1):\n    scheduler = ASHAScheduler(\n        time_attr=\"training_iteration\",\n        max_t=config[\"max_num_epochs\"],\n        grace_period=1,\n        reduction_factor=2)\n    \n    tuner = tune.Tuner(\n        tune.with_resources(\n            tune.with_parameters(train_cifar),\n            resources={\"cpu\": 2, \"gpu\": gpus_per_trial}\n        ),\n        tune_config=tune.TuneConfig(\n            metric=\"loss\",\n            mode=\"min\",\n            scheduler=scheduler,\n            num_samples=config[\"num_trials\"],\n        ),\n        param_space=config,\n    )\n    results = tuner.fit()\n    \n    best_result = results.get_best_result(\"loss\", \"min\")\n\n    print(f\"Best trial config: {best_result.config}\")\n    print(f\"Best trial final validation loss: {best_result.metrics['loss']}\")\n    print(f\"Best trial final validation accuracy: {best_result.metrics['accuracy']}\")\n\n    test_best_model(best_result, smoke_test=config[\"smoke_test\"])\n\nmain(config, gpus_per_trial=1 if torch.cuda.is_available() else 0)\n```\n\n----------------------------------------\n\nTITLE: Using PettingZoo API with RLlib for Multi-Agent Environments in Python\nDESCRIPTION: Showcases the integration of PettingZoo multi-agent environments with RLlib using the `PettingZooEnv` wrapper. Example includes setting up the 'pistonball' environment for RL with specific configuration through Ray's registry. Requires RLlib and PettingZoo library.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pettingzoo.butterfly import pistonball_v6\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\nfrom ray.tune.registry import register_env\n\nregister_env(\n    \"pistonball\",\n    lambda cfg: PettingZooEnv(pistonball_v6.env(num_floors=cfg.get(\"n_pistons\", 20))),\n)\n\nconfig = (\n    PPOConfig()\n    .environment(\"pistonball\", env_config={\"n_pistons\": 30})\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Trials in Ray Tune\nDESCRIPTION: Illustrates how to use tune.RunConfig to specify the number of trials and how it interacts with grid search and random sampling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-search-spaces.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 13 different configs.\ntuner = tune.Tuner(trainable, tune_config=tune.TuneConfig(num_samples=13), param_space={\n    \"x\": tune.choice([0, 1, 2]),\n    }\n)\ntuner.fit()\n\n# 13 different configs.\ntuner = tune.Tuner(trainable, tune_config=tune.TuneConfig(num_samples=13), param_space={\n    \"x\": tune.choice([0, 1, 2]),\n    \"y\": tune.randn([0, 1, 2]),\n    }\n)\ntuner.fit()\n\n# 4 different configs.\ntuner = tune.Tuner(trainable, tune_config=tune.TuneConfig(num_samples=1), param_space={\"x\": tune.grid_search([1, 2, 3, 4])})\ntuner.fit()\n\n# 3 different configs.\ntuner = tune.Tuner(trainable, tune_config=tune.TuneConfig(num_samples=1), param_space={\"x\": grid_search([1, 2, 3])})\ntuner.fit()\n\n# 6 different configs.\ntuner = tune.Tuner(trainable, tune_config=tune.TuneConfig(num_samples=2), param_space={\"x\": tune.grid_search([1, 2, 3])})\ntuner.fit()\n\n# 9 different configs.\ntuner = tune.Tuner(trainable, tune_config=tune.TuneConfig(num_samples=1), param_space={\n    \"x\": tune.grid_search([1, 2, 3]),\n    \"y\": tune.grid_search([a, b, c])}\n)\ntuner.fit()\n\n# 18 different configs.\ntuner = tune.Tuner(trainable, tune_config=tune.TuneConfig(num_samples=2), param_space={\n    \"x\": tune.grid_search([1, 2, 3]),\n    \"y\": tune.grid_search([a, b, c])}\n)\ntuner.fit()\n\n# 45 different configs.\ntuner = tune.Tuner(trainable, tune_config=tune.TuneConfig(num_samples=5), param_space={\n    \"x\": tune.grid_search([1, 2, 3]),\n    \"y\": tune.grid_search([a, b, c])}\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Sampling from SingleAgentEnvRunner - Python\nDESCRIPTION: This method samples data from the environment, allowing the agent to interact with it and gather observations for learning. It manages the agent's actions and the corresponding environment feedback, including rewards and state transitions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_env_runner.rst#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef sample(self):\n    # Sample data from the environment\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running MultiRLModule with Algorithm\nDESCRIPTION: This code demonstrates how to configure and run a multi-agent algorithm using the custom MultiRLModule with shared encoder. It shows how to create MultiRLModuleSpec and RLModuleSpec objects for the different components and plug them into an algorithm configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef train_vpg_ma_shared_encoder():\n    from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n    from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n    from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n    from ray.rllib.algorithms.vpg.torch.vpg_torch_learner import VPGTorchLearner\n\n    # Simple test environment with two agent slots (ids \"p1\" and \"p2\").\n    env = MultiAgentBlackjackEnv()\n\n    # We need to define RLModuleSpecs for our (MultiRLModule's) sub-modules.\n    # Although these specs are later NOT visible from the outside, we need them here,\n    # because we need to specify the different obsand action spaces\n    # for our child RLModules (encoder and two policy nets).\n    encoder_module_spec = RLModuleSpec(\n        module_class=VPGSharedEncoderRLModule,\n        observation_space=env.observation_space[\"p1\"],\n        action_space=env.action_space[\"p1\"],\n        model_config_dict={},\n    )\n    policy1_module_spec = RLModuleSpec(\n        module_class=VPGPolicyRLModule,\n        observation_space=env.observation_space[\"p1\"],\n        action_space=env.action_space[\"p1\"],\n        model_config_dict={},\n    )\n    policy2_module_spec = RLModuleSpec(\n        module_class=VPGPolicyRLModule,\n        observation_space=env.observation_space[\"p2\"],\n        action_space=env.action_space[\"p2\"],\n        model_config_dict={},\n    )\n\n    # Create a spec for our MultiRLModule, which has one encoder and two policy modules.\n    multi_rl_module_spec = MultiRLModuleSpec(\n        module_class=VPGUsingSharedEncoderRLModule,\n        # Override the observation_space slot with a dict here (mapping PolicyIDs to\n        # individual policy observation spaces).\n        observation_space={\n            \"p1\": env.observation_space[\"p1\"],\n            \"p2\": env.observation_space[\"p2\"],\n        },\n        # Override the action_space slot with a dict here (mapping PolicyIDs to\n        # individual policy action spaces).\n        action_space={\n            \"p1\": env.action_space[\"p1\"],\n            \"p2\": env.action_space[\"p2\"],\n        },\n        model_config_dict={},\n        obs_space_p1=env.observation_space[\"p1\"],\n        act_space_p1=env.action_space[\"p1\"],\n        obs_space_p2=env.observation_space[\"p2\"],\n        act_space_p2=env.action_space[\"p2\"],\n    )\n\n    # Create an algorithm using our MultiRLModuleSpec that can run\n    # our multi-agent setup with shared encoder.\n    config = (\n        AlgorithmConfig()\n        .environment(env)\n        .multi_agent(policies=[\"p1\", \"p2\"])\n        .experimental(_enable_new_api_stack=True)\n        .rl_module(rl_module_spec=multi_rl_module_spec)\n    )\n    algo = config.build()\n\n    # Train for n iterations.\n    result = algo.train()\n    result = algo.train()\n\n    policy_a_logits = algo.get_policy(\"p1\").compute_actions([1, 0, 0])[2][\"action_dist\"].logits\n    policy_b_logits = algo.get_policy(\"p2\").compute_actions([1, 0, 0])[2][\"action_dist\"].logits\n\n    print(f\"P1 logits: {policy_a_logits}\")\n    print(f\"P2 logits: {policy_b_logits}\")\n```\n\n----------------------------------------\n\nTITLE: Executing Workflows with Ray\nDESCRIPTION: Shows how to execute a workflow DAG using Ray's workflow.run and workflow.run_async methods. It demonstrates both synchronous and asynchronous execution of workflows.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/key-concepts.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import workflow\n\n# Run the workflow until it completes and returns the output\nassert workflow.run(dag) == 101\n\n# Or you can run it asynchronously and fetch the output via 'ray.get'\noutput_ref = workflow.run_async(dag)\nassert ray.get(output_ref) == 101\n```\n\n----------------------------------------\n\nTITLE: Textbot Constructor Implementation\nDESCRIPTION: Constructor code for the Textbot class that initializes the model, tokenizer and async loop.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment\nclass Textbot:\n    def __init__(self, model_id: str):\n        self.model = AutoModelForCausalLM.from_pretrained(model_id)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n        self.loop = asyncio.get_event_loop()\n        self.fastapi_app = FastAPI()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Ray Tracing Startup Hook\nDESCRIPTION: Example implementation of a tracing startup hook function that configures a TracerProvider and exports spans to files in '/tmp/spans'. The function sets up the OpenTelemetry tracing provider and adds a SimpleSpanProcessor with a ConsoleSpanExporter to write trace data to files.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/ray-tracing.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport os\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import (\n    ConsoleSpanExporter,\n    SimpleSpanProcessor,\n)\n\n\ndef setup_tracing() -> None:\n    # Creates /tmp/spans folder\n    os.makedirs(\"/tmp/spans\", exist_ok=True)\n    # Sets the tracer_provider. This is only allowed once per execution\n    # context and will log a warning if attempted multiple times.\n    trace.set_tracer_provider(TracerProvider())\n    trace.get_tracer_provider().add_span_processor(\n        SimpleSpanProcessor(\n            ConsoleSpanExporter(\n                out=open(f\"/tmp/spans/{os.getpid()}.json\", \"a\")\n                )\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Storage Path for Ray Train Checkpoints\nDESCRIPTION: This code snippet sets up the storage path for Ray Train checkpoints, using either a cloud storage bucket or an NFS path. It includes a fallback option using environment variables for Anyscale users.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstorage_path = \"s3://your-bucket-here\"  # TODO: Set up cloud storage\n# storage_path=\"/mnt/path/to/nfs\"     # TODO: Alternatively, set up NFS\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os, re\n\nartifact_storage = os.environ.get(\"ANYSCALE_ARTIFACT_STORAGE\", \"artifact_storage\")\nuser_name = re.sub(r\"\\s+\", \"__\", os.environ.get(\"ANYSCALE_USERNAME\", \"user\"))\nstorage_path = f\"{artifact_storage}/{user_name}/gptj-deepspeed-finetune\"\n```\n\n----------------------------------------\n\nTITLE: Loading and Configuring ResNet Model\nDESCRIPTION: Setting up a pre-trained ResNet152 model with appropriate transforms and device configuration for GPU usage if available.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/pytorch_resnet_batch_prediction.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchvision.models import ResNet152_Weights\nfrom torchvision import transforms\nfrom torchvision import models\n\nweights = ResNet152_Weights.IMAGENET1K_V1\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = models.resnet152(weights=weights).to(device)\nmodel.eval()\n\nimagenet_transforms = weights.transforms\ntransform = transforms.Compose([transforms.ToTensor(), imagenet_transforms()])\n```\n\n----------------------------------------\n\nTITLE: Configuring PPO Training Parameters\nDESCRIPTION: Sets important training hyperparameters for the PPO algorithm, including learning rate, batch size per learner, and the number of epochs to train on each batch of data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconfig.training(\n    lr=0.0002,\n    train_batch_size_per_learner=2000,\n    num_epochs=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Concurrency Groups in Python Ray Actor\nDESCRIPTION: Demonstrates how to create an AsyncIO actor with two concurrency groups ('io' and 'compute') having different concurrency limits. Shows method assignment to groups and remote execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/concurrency_group_api.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(concurrency_groups={\"io\": 2, \"compute\": 4})\nclass AsyncIOActor:\n    def __init__(self):\n        pass\n\n    @ray.method(concurrency_group=\"io\")\n    async def f1(self):\n        pass\n\n    @ray.method(concurrency_group=\"io\")\n    async def f2(self):\n        pass\n\n    @ray.method(concurrency_group=\"compute\")\n    async def f3(self):\n        pass\n\n    @ray.method(concurrency_group=\"compute\")\n    async def f4(self):\n        pass\n\n    async def f5(self):\n        pass\n\na = AsyncIOActor.remote()\na.f1.remote()  # executed in the \"io\" group.\na.f2.remote()  # executed in the \"io\" group.\na.f3.remote()  # executed in the \"compute\" group.\na.f4.remote()  # executed in the \"compute\" group.\na.f5.remote()  # executed in the default group.\n```\n\n----------------------------------------\n\nTITLE: Excluding Policies from Updates in Multi-Agent Training\nDESCRIPTION: This snippet demonstrates how to configure the multi-agent environment to exclude certain policies from updates during training. It shows the definition of a policy mapping function that alternates between a learning policy and a random policy based on the episode ID.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef policy_mapping_fn(agent_id, episode, **kwargs):\n    agent_idx = int(agent_id[-1])  # 0 (player1) or 1 (player2)\n    return \"learning_policy\" if episode.id_ % 2 == agent_idx else \"random_policy\"\n\nconfig = (\n    PPOConfig()\n    .environment(env=\"two_player_game\")\n    .multi_agent(\n        policy_mapping_fn=policy_mapping_fn,\n        policies_to_train=[\"learning_policy\"],\n    )\n    .rl_module(\n        rl_module_spec=MultiRLModuleSpec(rl_module_specs={\n            \"learning_policy\": RLModuleSpec(),\n            \"random_policy\": RLModuleSpec(rl_module_class=RandomRLModule),\n        }),\n    )\n)\n\nalgo = config.build()\nprint(algo.train())\n```\n\n----------------------------------------\n\nTITLE: Image Generation Function\nDESCRIPTION: Defines a function to generate images from text prompts using the loaded Stable Diffusion pipeline. Supports configurable image size and number of samples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/playground.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef generate(\n    pipeline: DiffusionPipeline,\n    prompt: str,\n    img_size: int = 512,\n    num_samples: int = 1,\n) -> list:\n    return pipeline([prompt] * num_samples, height=img_size, width=img_size).images\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Ray - Java\nDESCRIPTION: This Java snippet demonstrates how to shut down a Ray instance using `Ray.shutdown()`. It must be invoked to prevent resource leaks after finishing the application.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nimport io.ray.api.Ray;\n\npublic class MyRayApp {\n\n  public static void main(String[] args) {\n    Ray.init();\n    ... // ray program\n    Ray.shutdown();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Over-Parallelization in Ray (Python)\nDESCRIPTION: This code snippet shows an anti-pattern where tasks are parallelized at too fine a granularity, leading to poor performance due to overhead.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/too-fine-grained-tasks.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# __anti_pattern_start__\n@ray.remote\ndef increment(x):\n    return x + 1\n\ndata = list(range(1000000))\nresults = ray.get([increment.remote(x) for x in data])\n# __anti_pattern_end__\n```\n\n----------------------------------------\n\nTITLE: Configuring EnvRunner Actors in RLlib with Ray\nDESCRIPTION: This snippet demonstrates how to increase the number of EnvRunner actors in the RLlib configuration for improved sampling parallelism. Requires Ray and RLlib libraries. Key configuration is setting the number of EnvRunner actors using `env_runners(num_env_runners=n)`. Ideal for scenarios where higher parallel sampling throughput is needed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/scaling-guide.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\nconfig = (\n    PPOConfig()\n    # Use 4 EnvRunner actors (default is 2).\n    .env_runners(num_env_runners=4)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Runtime Environment with GitHub URL - Python\nDESCRIPTION: Example showing how to configure a runtime environment using a specific GitHub repository commit. Demonstrates the recommended approach of using commit-specific URLs for consistency across cluster nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nruntime_env = {\"working_dir\": (\"https://github.com\"\n                               \"/example_user/example_repository/archive/abcdefg.zip\")}\n```\n\n----------------------------------------\n\nTITLE: Loading and Partitioning Text Data with Python\nDESCRIPTION: Loads the Zen of Python text and splits it into three partitions for distributed processing. Uses subprocess to get the text and array slicing for partitioning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/map_reduce.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport subprocess\nzen_of_python = subprocess.check_output([\"python\", \"-c\", \"import this\"])\ncorpus = zen_of_python.split()\n\nnum_partitions = 3\nchunk = len(corpus) // num_partitions\npartitions = [\n    corpus[i * chunk: (i + 1) * chunk] for i in range(num_partitions)\n]\n```\n\n----------------------------------------\n\nTITLE: Task Retry Implementation in Ray\nDESCRIPTION: Example code demonstrating task retry functionality with configurable retry attempts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/tasks.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Example showing task retry configuration and behavior\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PyTorch Lightning and Ray Train\nDESCRIPTION: Imports necessary Python libraries for data handling, neural network operations, and integration with PyTorch Lightning and Ray Train.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_mnist_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom filelock import FileLock\nfrom torch.utils.data import DataLoader, random_split, Subset\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import trainer\nfrom pytorch_lightning.loggers.csv_logs import CSVLogger\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared Filesystem (NFS, HDFS) in Ray Train\nDESCRIPTION: Shows how to set up persistent storage using a shared filesystem path with TorchTrainer and RunConfig\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/persistent-storage.rst#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom ray import train\nfrom ray.train.torch import TorchTrainer\n\ntrainer = TorchTrainer(\n    ...,\n    run_config=train.RunConfig(\n        storage_path=\"/mnt/cluster_storage\",\n        # HDFS example:\n        # storage_path=f\"hdfs://{hostname}:{port}/subpath\",\n        name=\"experiment_name\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Unnecessary ray.get() Anti-pattern in Python\nDESCRIPTION: This code snippet shows an anti-pattern where ray.get() is called unnecessarily, forcing data transfer to the driver and then to the reduce worker. This approach can harm performance due to unnecessary data movement.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/unnecessary-ray-get.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef generate_rollout():\n    # Generate a large rollout\n    return large_rollout\n\n@ray.remote\ndef reduce(rollouts):\n    # Aggregate rollouts\n    return aggregated_result\n\nrollouts = [generate_rollout.remote() for _ in range(10)]\nrollouts = ray.get(rollouts)  # Unnecessary ray.get\nfinal_result = reduce.remote(rollouts)\nprint(ray.get(final_result))\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom RawStreamer for Batch Processing in Python\nDESCRIPTION: This code defines a custom RawStreamer class that supports processing batches of tokens, which is necessary for handling batched requests with Hugging Face models.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass RawStreamer:\n    def __init__(self):\n        self.queue = Queue()\n        self.stop_signal = object()\n        self.text_generator = self.generate()\n\n    def put(self, value):\n        self.queue.put(value)\n\n    def end(self):\n        self.queue.put(self.stop_signal)\n\n    def generate(self):\n        while True:\n            value = self.queue.get()\n            if value == self.stop_signal:\n                break\n            yield value\n```\n\n----------------------------------------\n\nTITLE: Vanilla Gradio Implementation\nDESCRIPTION: Standard Gradio implementation for text generation using multiple models without parallelization\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/gradio-integration.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport gradio as gr\nfrom transformers import pipeline\n\npipe1 = pipeline(\"text-generation\", model=\"gpt2\")\npipe2 = pipeline(\"text-generation\", model=\"distilgpt2\")\n\ndef generate(text):\n    result1 = pipe1(text, min_length=20, max_length=100)\n    result2 = pipe2(text, min_length=20, max_length=100)\n    return result1[0][\"generated_text\"], result2[0][\"generated_text\"]\n\ndemo = gr.Interface(\n    fn=generate,\n    inputs=gr.Textbox(lines=3, placeholder=\"Text to complete\"),\n    outputs=[\"text\", \"text\"],\n    title=\"Text Generation with Multiple Models\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing TensorBoard for Ray Tune Visualization\nDESCRIPTION: Shows how to install tensorboardX which is required for visualizing Ray Tune experiments with TensorBoard.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install tensorboardX\n```\n\n----------------------------------------\n\nTITLE: Reading Raw Images with Ray Data\nDESCRIPTION: Demonstrates loading raw image files (like JPEG) using ray.data.read_images(). Uses PIL library for image processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-images.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/batoidea/JPEGImages\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Using Ray's Logger and Configuring Log Levels\nDESCRIPTION: Code examples for customizing Ray's default logger settings, such as changing log levels for different Ray components and adding handlers. Dependencies include the logging module. Configures Ray's logger for more verbose output or file logging, influencing both internal Ray processes and library-specific logging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogger = logging.getLogger(\"ray\")\nlogger.setLevel(logging.WARNING) # Modify the Ray logging config\n\nray_data_logger = logging.getLogger(\"ray.data\")\nray_tune_logger = logging.getLogger(\"ray.tune\")\nray_rllib_logger = logging.getLogger(\"ray.rllib\")\nray_train_logger = logging.getLogger(\"ray.train\")\nray_serve_logger = logging.getLogger(\"ray.serve\")\n\nray_data_logger.setLevel(logging.WARNING)\n\nray_tune_logger.addHandler(logging.FileHandler(\"extra_ray_tune_log.log\"))\n```\n\n----------------------------------------\n\nTITLE: Setting Up the Tune Experiment with Aim Logger - Python\nDESCRIPTION: This snippet configures the Ray Tune experiment with the AimLoggerCallback. It sets up a grid search across specified hyperparameters and logs results to a specified path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-aim.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    train_function,\n    run_config=tune.RunConfig(\n        callbacks=[AimLoggerCallback()],\n        storage_path=\"/tmp/ray_results\",\n        name=\"aim_example\",\n    ),\n    param_space={\n        \"mean\": tune.grid_search([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n        \"sd\": tune.uniform(0.1, 0.9),\n    },\n    tune_config=tune.TuneConfig(\n        metric=\"loss\",\n        mode=\"min\",\n    ),\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Function in Python\nDESCRIPTION: Defines a simple evaluation function simulating a long-running machine learning task by introducing a delay, and calculates a score using hyperparameters 'width', 'height', and 'step'.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(step, width, height):\n    time.sleep(0.1)\n    return (0.1 + width * step / 100) ** (-1) + height * 0.1\n```\n\n----------------------------------------\n\nTITLE: Standard PyTorch Lightning Training Script\nDESCRIPTION: A reference implementation of a PyTorch Lightning training script without Ray Train integration. Includes model definition, data loading, and training setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchvision.models import resnet18\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Normalize, Compose\nfrom torch.utils.data import DataLoader\nimport lightning.pytorch as pl\n\n# Model, Loss, Optimizer\nclass ImageClassifier(pl.LightningModule):\n    def __init__(self):\n        super(ImageClassifier, self).__init__()\n        self.model = resnet18(num_classes=10)\n        self.model.conv1 = torch.nn.Conv2d(\n            1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        outputs = self.forward(x)\n        loss = self.criterion(outputs, y)\n        self.log(\"loss\", loss, on_step=True, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=0.001)\n\n# Data\ntransform = Compose([ToTensor(), Normalize((0.28604,), (0.32025,))])\ntrain_data = FashionMNIST(root='./data', train=True, download=True, transform=transform)\ntrain_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n\n# Training\nmodel = ImageClassifier()\ntrainer = pl.Trainer(max_epochs=10)\ntrainer.fit(model, train_dataloaders=train_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating Trained Policies using Python\nDESCRIPTION: This code shows how to load a pretrained PPO policy from a checkpoint using Ray RLlib. It initializes the Algorithm with the best result checkpoint and retrieves the policy for further evaluation. This snippet assumes a successful training session has completed and does not require extra dependencies beyond Ray RLlib.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_ppo_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm import Algorithm\n\nloaded_ppo = Algorithm.from_checkpoint(best_result.checkpoint)\nloaded_policy = loaded_ppo.get_policy()\n\n# See your trained policy in action\n# loaded_policy.compute_single_action(...)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Requirements for Tasks in C++\nDESCRIPTION: This C++ code snippet demonstrates how to specify resource requirements for tasks and actors using Ray's API. It highlights how to align resource utilization with system capabilities effectively.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/resources.rst#2025-04-12_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\n\"\"\"\n// Specify required resources.\nray::Task(MyFunction).SetResource(\"CPU\", 1.0).SetResource(\"GPU\", 1.0).SetResource(\"special_hardware\", 1.0).Remote();\n\nray::Actor(CreateCounter).SetResource(\"CPU\", 2.0).SetResource(\"GPU\", 1.0).Remote();\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Data Ingestion with HuggingFace Transformers and Ray Train\nDESCRIPTION: Python script illustrating the use of Ray Data and Ray Train with HuggingFace Transformers. It shows how to create datasets from HuggingFace datasets, access them in the training function, and use them with the Transformers Trainer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport ray.train\n\n...\n\n# Create the train and evaluation datasets.\ntrain_data = ray.data.from_huggingface(hf_train_ds)\neval_data = ray.data.from_huggingface(hf_eval_ds)\n\ndef train_func():\n    # Access Ray datsets in your train_func via ``get_dataset_shard``.\n    # Ray Data shards all datasets across workers by default.\n    train_ds = ray.train.get_dataset_shard(\"train\")\n    eval_ds = ray.train.get_dataset_shard(\"evaluation\")\n\n    # Create Ray dataset iterables via ``iter_torch_batches``.\n    train_iterable_ds = train_ds.iter_torch_batches(batch_size=16)\n    eval_iterable_ds = eval_ds.iter_torch_batches(batch_size=16)\n\n    ...\n\n    args = transformers.TrainingArguments(\n        ...,\n        max_steps=max_steps # Required for iterable datasets\n    )\n\n    trainer = transformers.Trainer(\n        ...,\n        model=model,\n        train_dataset=train_iterable_ds,\n        eval_dataset=eval_iterable_ds,\n    )\n\n    # Prepare your Transformers Trainer\n    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)\n    trainer.train()\n\ntrainer = TorchTrainer(\n    train_func,\n    # You can pass in multiple datasets to the Trainer.\n    datasets={\"train\": train_data, \"evaluation\": val_data},\n    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Running the Tune Experiment\nDESCRIPTION: This code executes the Tune experiment with the defined training function, configuration space, and resource requirements. It runs all trials and returns the results upon completion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-run.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Run the experiment\nresults = tune.run(\n    train_model,         # The function to execute\n    config=config,       # Config space\n    resources_per_trial=resources_per_trial,  # Resource request\n    name=\"train_model\",  # Name of experiment\n)\n\n# List all trials\nprint(\"Trials:\", list(results.results.values()))\n```\n\n----------------------------------------\n\nTITLE: Logging Training Metrics in Ray Train Worker\nDESCRIPTION: This snippet demonstrates the logging of training metrics during a distributed training job using Ray. It includes loss, gradient norm, learning rate, epoch, and memory usage information for each training step.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n(RayTrainWorker pid=36561) {'loss': 4.1052, 'grad_norm': 2.225008249282837, 'learning_rate': 8.26086956521739e-06, 'epoch': 2.5, 'memory_allocated (GB)': 28.87, 'max_memory_allocated (GB)': 94.26, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=36561) {'loss': 4.0472, 'grad_norm': 2.0701019763946533, 'learning_rate': 8.212560386473431e-06, 'epoch': 2.51, 'memory_allocated (GB)': 28.87, 'max_memory_allocated (GB)': 94.26, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=36561) {'loss': 4.097, 'grad_norm': 2.119075059890747, 'learning_rate': 8.164251207729469e-06, 'epoch': 2.51, 'memory_allocated (GB)': 28.87, 'max_memory_allocated (GB)': 94.26, 'total_memory_available (GB)': 94.62}\n```\n\n----------------------------------------\n\nTITLE: Textbot Request Handling Logic\nDESCRIPTION: Implementation of request handling methods including streaming response generation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment\nclass Textbot:\n    async def handle_request(self, prompt: str) -> StreamingResponse:\n        from transformers import TextIteratorStreamer\n        streamer = TextIteratorStreamer(self.tokenizer)\n        self.loop.run_in_executor(None, self.generate_text, prompt, streamer)\n        return StreamingResponse(self.consume_streamer(streamer))\n\n    def generate_text(self, prompt: str, streamer) -> None:\n        inputs = self.tokenizer([prompt], return_tensors=\"pt\")\n        self.model.generate(\n            inputs[\"input_ids\"],\n            max_new_tokens=100,\n            streamer=streamer,\n        )\n\n    async def consume_streamer(\n        self, streamer\n    ) -> AsyncGenerator[str, None]:\n        for token in streamer:\n            yield token\n            if not streamer.is_generating:\n                break\n            await asyncio.sleep(0.1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Data Execution Resources\nDESCRIPTION: Example showing how to configure CPU, GPU, and memory limits for Ray Data execution using DataContext.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nctx = ray.data.DataContext.get_current()\nctx.execution_options.resource_limits.cpu = 10\nctx.execution_options.resource_limits.gpu = 5\nctx.execution_options.resource_limits.object_store_memory = 10e9\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network Model in PyTorch\nDESCRIPTION: This code defines a simple neural network class using PyTorch's nn.Module, and instantiates the model on the appropriate device (CPU or GPU).\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Behavior Cloning Learner in Python\nDESCRIPTION: This code defines the initialization and loss computation for a behavior cloning learner using PyTorch under Ray RLlib. Dependencies include various RLlib modules for configuration and sample batch handling. Loss calculation is based on action distribution log-probabilities from input tensors, focusing on supervised learning from demonstrations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-learner.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass BCTorchLearner(TorchLearner):\n\n    @override(Learner)\n    def compute_loss_for_module(\n        self,\n        *,\n        module_id: ModuleID,\n        config: AlgorithmConfig = None,\n        batch: Dict[str, Any],\n        fwd_out: Dict[str, TensorType],\n    ) -> TensorType:\n\n        # standard behavior cloning loss\n        action_dist_inputs = fwd_out[SampleBatch.ACTION_DIST_INPUTS]\n        action_dist_class = self._module[module_id].get_train_action_dist_cls()\n        action_dist = action_dist_class.from_logits(action_dist_inputs)\n        loss = -torch.mean(action_dist.logp(batch[SampleBatch.ACTIONS]))\n\n        return loss\n```\n\n----------------------------------------\n\nTITLE: Start Ray with Memory Threshold (Bash)\nDESCRIPTION: This bash command starts Ray with a specified memory usage threshold. The `RAY_memory_usage_threshold` environment variable is set to 0.4, which means that Ray will start killing processes when memory usage reaches 40% of capacity. This can be used for testing and demonstration purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/ray-oom-prevention.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nRAY_memory_usage_threshold=0.4 ray start --head\n```\n\n----------------------------------------\n\nTITLE: Composing Ray Serve Deployments with DeploymentHandle\nDESCRIPTION: This code snippet illustrates how to compose multiple Ray Serve deployments using `DeploymentHandle`. The `Model` deployment defines a simple model. The `Ingress` deployment takes two `Model` deployments as arguments and uses their handles to make predictions and combine the results. This demonstrates how deployments can call into each other within a Ray Serve application.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/key-concepts.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example showing how to compose deployments.\n\"\"\"\nfrom typing import List\n\nimport ray\nfrom ray import serve\n\n@serve.deployment\nclass Model:\n    def __init__(self, weight: int):\n        self.weight = weight\n\n    def predict(self, input: int) -> int:\n        return self.weight * input\n\n    async def __call__(self, request: int) -> int:\n        return self.predict(request)\n\n@serve.deployment\nclass Ingress:\n    def __init__(self, model1: Model, model2: Model):\n        self.model1 = model1\n        self.model2 = model2\n\n    async def __call__(self, request: int) -> List[int]:\n        ref1 = self.model1.predict.remote(request)\n        ref2 = self.model2.predict.remote(request)\n        return await ray.get([ref1, ref2])\n\n# Example instantiation:\n# model1 = Model.bind(weight=1)\n# model2 = Model.bind(weight=2)\n# ingress = Ingress.bind(model1, model2)\n```\n\n----------------------------------------\n\nTITLE: Checkpointing a LearnerGroup in Python\nDESCRIPTION: This example illustrates saving and restoring the state of all learners in a LearnerGroup using checkpoint paths. All network weights and optimizer states are included in these operations. This approach helps ensure learner states are preserved and can be restored to achieve identical configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-learner.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlearner_group.save_to_path(LEARNER_GROUP_CKPT_DIR)\nlearner_group.restore_from_path(LEARNER_GROUP_CKPT_DIR)\n```\n\n----------------------------------------\n\nTITLE: Setting Up HuggingFace Pipeline for Text Generation\nDESCRIPTION: Initializes a HuggingFace text generation pipeline using the fine-tuned Dolly-v2-7b model. Loads the checkpoint from the Ray Train output directory, configures the tokenizer, and prepares the model for inference.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n\nckpt_path = os.path.join(result.checkpoint.path, \"checkpoint.ckpt\")\n\ndolly = DollyV2Model.load_from_checkpoint(ckpt_path, map_location=torch.device(\"cpu\"))\n\nnlp_pipeline = pipeline(\n    task=\"text-generation\", \n    model=dolly.model, \n    tokenizer=tokenizer, \n    device_map=\"auto\"\n)\n```\n\n----------------------------------------\n\nTITLE: Combining Grid Search and Random Sampling in Ray Tune\nDESCRIPTION: Demonstrates how to combine grid search and random sampling primitives in a single search space configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-search-spaces.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# 6 different configs.\ntuner = tune.Tuner(trainable, tune_config=tune.TuneConfig(num_samples=2), param_space={\n    \"x\": tune.sample_from(...),\n    \"y\": tune.grid_search([a, b, c])\n    }\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: RayDaskCallback Subclass Implementation in Python\nDESCRIPTION: This snippet showcases the creation of a custom callback by subclassing `RayDaskCallback` and overriding specific callback methods.  This approach offers more structured and reusable callback implementations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/dask-on-ray.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example showing how to subclass RayDaskCallback.\"\"\"\n\nimport time\n\nfrom ray.util.dask import RayDaskCallback\n\n\nclass MyCallback(RayDaskCallback):\n    def _ray_presubmit(self, dsk, state):\n        print(\"Submitting a dask graph of size: %s\" % len(dsk))\n\n    def _ray_postsubmit(self, dsk, state, id):\n        print(\"Submitted dask task with id: %s\" % id)\n\n    def _ray_pretask(self, dsk, state, task):\n        print(\"About to run dask task: %s\" % task)\n        state[\"t\"] = time.time()\n\n    def _ray_posttask(self, dsk, state, task, result):\n        duration = time.time() - state[\"t\"]\n        print(\"Task %s took %s s\" % (task, duration))\n\n\ncallback = MyCallback()\n\n```\n\n----------------------------------------\n\nTITLE: Cluster State Inspection with Ray - Python\nDESCRIPTION: Shows how to inspect Ray cluster state including node information and resource availability using the global state API. Demonstrates usage of ray.nodes() for cluster inspection.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/miscellaneous.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\nprint(ray.nodes())\n```\n\n----------------------------------------\n\nTITLE: Launching Ray Cluster on Slurm with Python\nDESCRIPTION: This script automates the process of launching a Ray cluster on a Slurm-managed system. It parses command-line arguments, generates Slurm job scripts for head and worker nodes, and submits these jobs to create the cluster. The script handles various configurations such as number of nodes, GPUs per node, and custom job names.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/slurm-launch.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport subprocess\nimport sys\nimport tempfile\nimport uuid\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Launch a Ray cluster on Slurm.\")\n    parser.add_argument(\n        \"--nodes\",\n        \"-n\",\n        type=int,\n        default=1,\n        help=\"Number of nodes to request for the Ray cluster. Default: 1\",\n    )\n    parser.add_argument(\n        \"--partition\",\n        \"-p\",\n        type=str,\n        default=None,\n        help=\"Partition to submit jobs to.\",\n    )\n    parser.add_argument(\n        \"--gpus-per-node\",\n        type=int,\n        default=0,\n        help=\"Number of GPUs to request per node. Default: 0\",\n    )\n    parser.add_argument(\n        \"--ray-port\",\n        type=int,\n        default=6379,\n        help=\"The port to use for the Ray head node. Default: 6379\",\n    )\n    parser.add_argument(\n        \"--redis-password\",\n        type=str,\n        default=None,\n        help=\"The password to use for the Ray Redis instance.\",\n    )\n    parser.add_argument(\n        \"--job-name\",\n        type=str,\n        default=None,\n        help=\"The job name to use for the Ray cluster.\",\n    )\n    parser.add_argument(\n        \"--no-start-ray\",\n        action=\"store_true\",\n        help=\"Don't start Ray after setting up the nodes.\",\n    )\n    parser.add_argument(\n        \"--large-cluster\",\n        action=\"store_true\",\n        help=\"Avoid timeout by breaking launching worker nodes into multiple\"\n        \" Slurm jobs.\",\n    )\n    parser.add_argument(\n        \"--head-cpus\",\n        type=int,\n        default=None,\n        help=\"Number of CPUs to allocate for the head node.\",\n    )\n    parser.add_argument(\n        \"--head-extra-sbatch\",\n        type=str,\n        default=None,\n        help=\"Extra SBATCH flags for head node (multi-line string)\",\n    )\n    parser.add_argument(\n        \"--worker-cpus\",\n        type=int,\n        default=None,\n        help=\"Number of CPUs to allocate for each worker node.\",\n    )\n    parser.add_argument(\n        \"--worker-extra-sbatch\",\n        type=str,\n        default=None,\n        help=\"Extra SBATCH flags for worker nodes (multi-line string)\",\n    )\n    return parser.parse_args()\n\n\ndef generate_head_node_script(args):\n    # ... (script content)\n\n\ndef generate_worker_node_script(args, head_ip):\n    # ... (script content)\n\n\ndef wait_for_job_completion(job_id):\n    # ... (function implementation)\n\n\ndef submit_slurm_job(script, job_name):\n    # ... (function implementation)\n\n\ndef main():\n    args = parse_args()\n\n    head_node_script = generate_head_node_script(args)\n    head_job_id = submit_slurm_job(head_node_script, f\"{args.job_name or 'ray'}_head\")\n    print(f\"Head node job ID: {head_job_id}\")\n\n    wait_for_job_completion(head_job_id)\n\n    head_ip = subprocess.check_output(\n        [\"srun\", \"--jobid\", head_job_id, \"hostname\", \"-i\"]\n    ).decode().strip()\n    print(f\"Head node IP: {head_ip}\")\n\n    if args.nodes > 1:\n        if args.large_cluster:\n            # ... (large cluster handling)\n        else:\n            worker_node_script = generate_worker_node_script(args, head_ip)\n            worker_job_id = submit_slurm_job(\n                worker_node_script, f\"{args.job_name or 'ray'}_workers\"\n            )\n            print(f\"Worker nodes job ID: {worker_job_id}\")\n\n    print(\"\\nTo connect to this Ray cluster:\")\n    print(f\"ray start --address={head_ip}:{args.ray_port}\")\n    if args.redis_password:\n        print(f\"--redis-password='{args.redis_password}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Inference with Trained XGBoost Model\nDESCRIPTION: Defines a prediction class and function for running inference with the trained XGBoost model. The implementation handles loading the model and preprocessor from checkpoint, applying preprocessing to new data, and generating predictions in a distributed manner.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Predict:\n    def __init__(self, checkpoint: Checkpoint):\n        self.model = XGBoostTrainer.get_model(checkpoint)\n        # extract the preprocessor from the checkpoint metadata\n        self.preprocessor = Preprocessor.deserialize(\n            checkpoint.get_metadata()[\"preprocessor_pkl\"]\n        )\n\n    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n        preprocessed_batch = self.preprocessor.transform_batch(batch)\n        dmatrix = xgboost.DMatrix(preprocessed_batch)\n        return {\"predictions\": self.model.predict(dmatrix)}\n\n\ndef predict_xgboost(result: Result):\n    _, _, test_dataset = prepare_data()\n\n    scores = test_dataset.map_batches(\n        Predict,\n        fn_constructor_args=[result.checkpoint],\n        concurrency=1,\n        batch_format=\"pandas\",\n    )\n\n    predicted_labels = scores.map_batches(\n        lambda df: (df > 0.5).astype(int), batch_format=\"pandas\"\n    )\n    print(\"PREDICTED LABELS\")\n    predicted_labels.show()\n```\n\n----------------------------------------\n\nTITLE: Callback Events Overview\nDESCRIPTION: This snippet provides an overview of the callback events available in RLlib, organized by the Algorithm and EnvRunner classes. It lists the events and when they are triggered during the training and evaluation process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-callback.rst#2025-04-12_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nAlgorithm\n    .__init__()\n        `on_algorithm_init` - After algorithm construction and setup.\n    .train()\n        `on_train_result` - After a training iteration.\n    .evaluate()\n        `on_evaluate_start` - Before evaluation starts using the eval ``EnvRunnerGroup``.\n        `on_evaluate_end` - After evaluation is finished.\n    .restore_from_path()\n        `on_checkpoint_loaded` - After a checkpoint's new state has been loaded.\n\nEnvRunner\n    .__init__()\n        `on_environment_created` - After the RL environment has been created.\n    .sample()\n        `on_episode_created` - After a new episode object has been created.\n        `on_episode_start` - After an episode object has started (after ``env.reset()``).\n        `on_episode_step` - After an episode object has stepped (after ``env.step()``).\n        `on_episode_end` - After an episode object has terminated (or truncated).\n        `on_sample_end` - At the end of the ``EnvRunner.sample()`` call.\n```\n\n----------------------------------------\n\nTITLE: Preparing Training Arguments for Gaudi Training\nDESCRIPTION: Configures training arguments for fine-tuning Llama-2 models on Intel Gaudi HPUs, supporting different execution modes and training strategies\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_training_args(config: Dict):\n    execution_mode = config[\"execution_mode\"]\n    use_lazy_mode = True if execution_mode == \"lazy\" else False\n    torch_compile_backend = \"hpu_backend\" if execution_mode == \"eager.compile\" else None\n\n    deepspeed = config[\"deepspeed\"] if \"deepspeed\" in config else None\n\n    return GaudiTrainingArguments(deepspeed=deepspeed,\n                                  output_dir=config[\"output\"],\n                                  do_train=True,\n                                  do_eval=False,\n                                  per_device_train_batch_size=config[\"batch_size_per_worker\"],\n                                  bf16=True,\n                                  learning_rate=config[\"lr\"],\n                                  save_strategy=\"no\",\n                                  torch_compile_backend=torch_compile_backend,\n                                  evaluation_strategy=\"no\",\n                                  lr_scheduler_type=\"cosine\",\n                                  num_train_epochs=config[\"epochs\"],\n                                  use_lazy_mode=use_lazy_mode,\n                                  use_habana=True,\n                                  pipelining_fwd_bwd=True,\n                                  save_only_model=True,\n                                  gradient_checkpointing=True,\n                                  warmup_ratio=0.03,\n                                  throughput_warmup_steps=3,\n                                  logging_steps=5)\n```\n\n----------------------------------------\n\nTITLE: Accessing Hugging Face Transformers Checkpoints in Ray Train\nDESCRIPTION: This snippet shows how to retrieve and load Hugging Face Transformers checkpoint files from a Ray Train checkpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom transformers import AutoModelForSequenceClassification\n\ncheckpoint = result.checkpoint\nwith checkpoint.as_directory() as ckpt_dir:\n    model = AutoModelForSequenceClassification.from_pretrained(ckpt_dir)\n```\n\n----------------------------------------\n\nTITLE: Accessing Gated Hugging Face Models with Ray Serve LLM\nDESCRIPTION: Python code for configuring Ray Serve to access gated Hugging Face models by passing authentication tokens through runtime environment variables while setting deployment options.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.llm import LLMConfig, LLMServer, LLMRouter\nimport os\n\nllm_config = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"llama-3-8b-instruct\",\n        model_source=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1, max_replicas=2,\n        )\n    ),\n    # Pass the desired accelerator type (e.g. A10G, L4, etc.)\n    accelerator_type=\"A10G\",\n    runtime_env=dict(\n        env_vars=dict(\n            HF_TOKEN=os.environ[\"HF_TOKEN\"]\n        )\n    ),\n)\n\n# Deploy the application\ndeployment = LLMServer.as_deployment(llm_config.get_serve_options(name_prefix=\"vLLM:\")).bind(llm_config)\nllm_app = LLMRouter.as_deployment().bind([deployment])\nserve.run(llm_app, blocking=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Transformed Model Python\nDESCRIPTION: Generates text from given prompts using the Hugging Face pipeline. It iteratively processes input sentences for text generation with specified sampling settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Generate from prompts!\nfor sentence in pipe(\n    [\"Romeo and Juliet\", \"Romeo\", \"Juliet\"], do_sample=True, min_length=20\n):\n    print(sentence)\n```\n\n----------------------------------------\n\nTITLE: Setting up GPT-J Text Generation Pipeline\nDESCRIPTION: Initializes a Hugging Face pipeline for text generation using the fine-tuned GPT-J model with automatic device mapping and float16 precision.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline, AutoTokenizer, GPTJForCausalLM\n\nmodel = GPTJForCausalLM.from_pretrained(\"/mnt/local_storage/checkpoint\")\ntokenizer = AutoTokenizer.from_pretrained(\"/mnt/local_storage/checkpoint\")\n\npipe = pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n```\n\n----------------------------------------\n\nTITLE: DQN Rainbow Configuration Hint\nDESCRIPTION: Configuration settings for implementing the full Rainbow DQN algorithm with advanced features like n-step learning, noisy networks, and distributional learning\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-algorithms.rst#2025-04-12_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n\"n_step\": [between 1 and 10],\n\"noisy\": True,\n\"num_atoms\": [more than 1]\n```\n\n----------------------------------------\n\nTITLE: Creating Fake Email Serve Application in Python\nDESCRIPTION: A simple Ray Serve application that generates fake email addresses using the Faker library. Demonstrates basic Serve application structure and dependency management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/docker.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Code not fully provided in snippets\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Data with StandardScaler in Ray\nDESCRIPTION: Demonstrates how to preprocess data using Ray's built-in StandardScaler for feature normalization. The preprocessor is fit on the training data and then applied to both training and validation datasets.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Load and split the dataset\ntrain_dataset, valid_dataset, _test_dataset = prepare_data()\n\n# pick some dataset columns to scale\ncolumns_to_scale = [\"mean radius\", \"mean texture\"]\n\n# Initialize the preprocessor\npreprocessor = StandardScaler(columns=columns_to_scale)\n# train the preprocessor on the training set\npreprocessor.fit(train_dataset)\n# apply the preprocessor to the training and validation sets\ntrain_dataset = preprocessor.transform(train_dataset)\nvalid_dataset = preprocessor.transform(valid_dataset)\n```\n\n----------------------------------------\n\nTITLE: Creating Object References in Ray (Python, Java, C++)\nDESCRIPTION: Demonstrates how to create object references using ray.put() in different programming languages. This stores an object in Ray's distributed object store and returns a reference to it.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\n\n# Put an object in Ray's object store.\ny = 1\nobject_ref = ray.put(y)\n```\n\nLANGUAGE: Java\nCODE:\n```\n// Put an object in Ray's object store.\nint y = 1;\nObjectRef<Integer> objectRef = Ray.put(y);\n```\n\nLANGUAGE: C++\nCODE:\n```\n// Put an object in Ray's object store.\nint y = 1;\nray::ObjectRef<int> object_ref = ray::Put(y);\n```\n\n----------------------------------------\n\nTITLE: Using setup_wandb in a Ray Tune Training Function\nDESCRIPTION: This function demonstrates how to use the setup_wandb utility within a Tune training function to initialize Wandb logging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-wandb.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_function_wandb(config):\n    wandb = setup_wandb(config, project=\"Wandb_example\")\n\n    for i in range(30):\n        loss = config[\"mean\"] + config[\"sd\"] * np.random.randn()\n        tune.report({\"loss\": loss})\n        wandb.log(dict(loss=loss))\n```\n\n----------------------------------------\n\nTITLE: Checking Ray Initialization - Python\nDESCRIPTION: The Python snippet checks if the Ray runtime is initialized by utilizing the `ray.is_initialized()` API, providing a mechanism to ensure that Ray is ready for executing distributed tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\nray.init()\nassert ray.is_initialized()\n\nray.shutdown()\nassert not ray.is_initialized()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Object Detection Results with PyTorch\nDESCRIPTION: Draws bounding boxes on the original image based on the model's predictions and displays the result.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision.utils import draw_bounding_boxes\nfrom torchvision.transforms.functional import to_pil_image\n\nlabels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\nbox = draw_bounding_boxes(img,\n                          boxes=prediction[\"boxes\"],\n                          labels=labels,\n                          colors=\"red\",\n                          width=4)\nim = to_pil_image(box.detach())\ndisplay(im)\n```\n\n----------------------------------------\n\nTITLE: Building and Initializing a PPO Algorithm Instance\nDESCRIPTION: Creates an actual Algorithm instance (PPO in this case) from the previously configured settings, which can then be used for training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Build the Algorithm (PPO).\nppo = config.build_algo()\n```\n\n----------------------------------------\n\nTITLE: Defining the Hartmann 6D Benchmark Function\nDESCRIPTION: Implements the Hartmann 6D function, a classic benchmark for global optimization with 6 local minima. This function will be used to evaluate the parameter configurations during optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef landscape(x):\n    \"\"\"\n    Hartmann 6D function containing 6 local minima.\n    It is a classic benchmark for developing global optimization algorithms.\n    \"\"\"\n    alpha = np.array([1.0, 1.2, 3.0, 3.2])\n    A = np.array(\n        [\n            [10, 3, 17, 3.5, 1.7, 8],\n            [0.05, 10, 17, 0.1, 8, 14],\n            [3, 3.5, 1.7, 10, 17, 8],\n            [17, 8, 0.05, 10, 0.1, 14],\n        ]\n    )\n    P = 10 ** (-4) * np.array(\n        [\n            [1312, 1696, 5569, 124, 8283, 5886],\n            [2329, 4135, 8307, 3736, 1004, 9991],\n            [2348, 1451, 3522, 2883, 3047, 6650],\n            [4047, 8828, 8732, 5743, 1091, 381],\n        ]\n    )\n    y = 0.0\n    for j, alpha_j in enumerate(alpha):\n        t = 0\n        for k in range(6):\n            t += A[j, k] * ((x[k] - P[j, k]) ** 2)\n        y -= alpha_j * np.exp(-t)\n    return y\n```\n\n----------------------------------------\n\nTITLE: Running PPO Algorithm using Ray Tune\nDESCRIPTION: This code snippet illustrates how to run a PPO (Proximal Policy Optimization) algorithm using Ray Tune, a tool for hyperparameter tuning. It sets up the configuration similarly to the direct method but utilizes Ray Tune for fitting the model over specified training iterations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/key-concepts.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\n# Configure.\nconfig = (\n    PPOConfig()\n    .environment(\"CartPole-v1\")\n    .training(\n        train_batch_size_per_learner=2000,\n        lr=0.0004,\n    )\n)\n\n# Train through Ray Tune.\nresults = tune.Tuner(\n    \"PPO\",\n    param_space=config,\n    # Train for 4000 timesteps (2 iterations).\n    run_config=tune.RunConfig(stop={\"num_env_steps_sampled_lifetime\": 4000}),\n).fit()\n```\n\n----------------------------------------\n\nTITLE: Implementing Conditional Search Spaces in Ray Tune\nDESCRIPTION: This code snippet shows how to create conditional search spaces in Ray Tune using the tune.sample_from() function. It demonstrates how to make the value of one hyperparameter depend on another.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"num_layers\": tune.choice([1, 2, 3]),\n    \"layers\": tune.sample_from(lambda spec: {\n        sampled_layers = []\n        for i in range(spec.config.num_layers):\n            sampled_layers.append(tune.sample_from(lambda _: np.random.randint(5, 50)))\n        return sampled_layers\n    })\n}\n```\n\n----------------------------------------\n\nTITLE: Tuning Keras Models with Hyperopt in Ray Tune\nDESCRIPTION: Example demonstrating how to tune a Keras model using Hyperopt search algorithm within Ray Tune, focusing on optimizing the activation parameter of the model's first layer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/index.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef objective(config):\n    model = Sequential()\n    model.add(Dense(\n        32, activation=config[\"activation\"], input_shape=(input_size,)))\n    model.add(Dense(1))\n    model.compile(loss=\"mse\", optimizer=SGD(lr=config[\"lr\"]))\n\n    for i in range(10):\n        model.fit(x_train, y_train, verbose=0)\n        score = model.evaluate(x_test, y_test, verbose=0)\n        tune.report(mean_accuracy=score)\n\nsearch_space = {\n    \"activation\": hp.choice(\"activation\", [\"relu\", \"tanh\"]),\n    \"lr\": hp.loguniform(\"lr\", 1e-5, 1e-1),\n}\n\nhyperopt_search = HyperOptSearch(\n    search_space, metric=\"mean_accuracy\", mode=\"max\")\n\nresults = tune.run(\n    objective,\n    search_alg=hyperopt_search,\n    num_samples=20,\n    metric=\"mean_accuracy\",\n    mode=\"max\"\n)\n```\n\n----------------------------------------\n\nTITLE: Capture Child Tasks in Placement Group\nDESCRIPTION: This Python snippet demonstrates how to automatically schedule child actors or tasks to the same placement group as their parent by setting `placement_group_capture_child_tasks` to True. This ensures that related tasks and actors are co-located for performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"        @ray.remote\n        class Child:\n            def ping(self):\n                return os.getpid()\n\n        @ray.remote(placement_group_capture_child_tasks=True)\n        class Parent:\n            def __init__(self):\n                self.child = Child.remote()\n\n            def ping(self):\n                return ray.get(self.child.ping.remote())\n\n        @ray.remote\n        def f():\n            return os.getpid()\n\n        pg = ray.util.placement_group([{\"CPU\": 1}])\n        ray.get(pg.ready())\n        p = Parent.remote()\n        assert ray.get(p.ping.remote()) == os.getpid()\n\n        # If a task is submitted in a placement group, it will be executed in the\n        # placement group.\n        assert ray.get(f.options(placement_group=pg).remote()) == os.getpid()\"\n```\n\n----------------------------------------\n\nTITLE: Loading RLModule from Checkpoint\nDESCRIPTION: This code shows how to create a new RLModule by loading it directly from a checkpoint file. It uses the static from_checkpoint method to reconstruct the module with its saved parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.core.rl_module.rl_module import RLModule\n\n# Create a new RLModule from the checkpoint.\nnew_module = RLModule.from_checkpoint(module_ckpt_path)\n```\n\n----------------------------------------\n\nTITLE: Customizing CLI Reporter in Ray Tune\nDESCRIPTION: Example showing how to configure the CLI output by setting the maximum number of progress rows and adding a custom metric column to the default reporting table.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/reporters.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray.tune\nfrom ray.tune import CLIReporter\n\n# Limit the number of rows.\nreporter = CLIReporter(max_progress_rows=10)\n# Add a custom metric column, in addition to the default metrics.\n# Note that this must be a metric that is returned in your training results.\nreporter.add_metric_column(\"custom_metric\")\ntuner = tune.Tuner(my_trainable, run_config=ray.tune.RunConfig(progress_reporter=reporter))\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Stopping Ray Tune Experiment with Custom Stopper Class\nDESCRIPTION: Implements a custom Stopper class that stops all trials once any trial reaches a mean accuracy of 0.8 or higher.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-stopping.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass CustomStopper(tune.Stopper):\n    def __init__(self):\n        self.should_stop = False\n\n    def __call__(self, trial_id, result):\n        if not self.should_stop and result[\"mean_accuracy\"] >= 0.8:\n            self.should_stop = True\n        return self.should_stop\n\n    def stop_all(self):\n        return self.should_stop\n\ntuner = tune.Tuner(\n    train_function,\n    tune_config=tune.TuneConfig(\n        num_samples=5,\n        stop=CustomStopper()\n    ),\n    param_space={\"base_accuracy\": 0.3}\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Configuring PPO Algorithm for CartPole-v1 Training in Python\nDESCRIPTION: This code snippet configures and trains a PPO agent on the 'CartPole-v1' environment, setting hyperparameters such as learning rate and the number of training epochs. It also includes metric definitions for stopping criteria during training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\nfrom ray.rllib.utils.metrics import (\n    ENV_RUNNER_RESULTS,\n    EVALUATION_RESULTS,\n    EPISODE_RETURN_MEAN,\n)\nfrom ray import tune\n\n# Configure the PPO algorithm.\nconfig = (\n    PPOConfig()\n    .environment(\"CartPole-v1\")\n    .training(\n        lr=0.0003,\n        # Run 6 SGD minibatch iterations on a batch.\n        num_epochs=6,\n        # Weigh the value function loss smaller than\n        # the policy loss.\n        vf_loss_coeff=0.01,\n    )\n    .rl_module(\n        model_config=DefaultModelConfig(\n            fcnet_hiddens=[32],\n            fcnet_activation=\"linear\",\n            # Share encoder layers between value network\n            # and policy.\n            vf_share_layers=True,\n        ),\n    )\n)\n\n# Define the metric to use for stopping.\nmetric = f\"{EVALUATION_RESULTS}/{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\"\n\n# Define the Tuner.\ntuner = tune.Tuner(\n    \"PPO\",\n    param_space=config,\n    run_config=tune.RunConfig(\n        stop={\n            metric: 450.0,\n        },\n        name=\"docs_rllib_offline_pretrain_ppo\",\n        verbose=2,\n        checkpoint_config=tune.CheckpointConfig(\n            checkpoint_frequency=1,\n            checkpoint_at_end=True,\n        ),\n    ),\n)\nresults = tuner.fit()\n\n# Store the best checkpoint to use it later for recording\n# an expert policy.\nbest_checkpoint = (\n    results\n    .get_best_result(\n        metric=metric,\n        mode=\"max\"\n    )\n    .checkpoint.path\n)\n```\n\n----------------------------------------\n\nTITLE: Minimal GCP Configuration (Autoscaler)\nDESCRIPTION: This YAML configuration showcases a minimal Ray autoscaler setup for GCP, defining the essential parameters needed to launch and manage a Ray cluster on Google Cloud Platform. It includes node types and resource allocation, demonstrating a basic but functional configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/gcp/example-minimal.yaml\n            :language: yaml\n\n```\n\n----------------------------------------\n\nTITLE: Restoring Native PyTorch Training State from Checkpoint\nDESCRIPTION: This example shows how to restore training state for a native PyTorch model from a Ray Train checkpoint, including model weights, optimizer state, and epoch number.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport ray.train as train\n\ndef train_func():\n    model = torch.nn.Linear(1, 1)\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n    checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            model_state_dict = torch.load(os.path.join(checkpoint_dir, \"model.pt\"))\n            model.load_state_dict(model_state_dict)\n            optimizer_state_dict = torch.load(os.path.join(checkpoint_dir, \"optimizer.pt\"))\n            optimizer.load_state_dict(optimizer_state_dict)\n            epoch = torch.load(os.path.join(checkpoint_dir, \"epoch.pt\"))\n    else:\n        epoch = 0\n\n    # Training loop starts here\n    for epoch in range(epoch, num_epochs):\n        # ... training code ...\n        if epoch % 5 == 0:\n            with train.checkpoint() as checkpoint_dir:\n                torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"model.pt\"))\n                torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, \"optimizer.pt\"))\n                torch.save(epoch, os.path.join(checkpoint_dir, \"epoch.pt\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Summarization Model in Python\nDESCRIPTION: Demonstrates using HuggingFace's SummarizationPipeline to create a text summarizer that runs locally. The code processes a snippet from 'A Tale of Two Cities' and outputs a condensed summary.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/getting_started.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\ntext = \"\"\"\nIt was the best of times, it was the worst of times, it was the age of\nwisdom, it was the age of foolishness, it was the epoch of belief, it\nwas the epoch of incredulity, it was the season of Light, it was the\nseason of Darkness, it was the spring of hope, it was the winter of\ndespair.\n\"\"\"\n\nsummary = summarizer(text, max_length=20, min_length=5, do_sample=False)[0][\"summary_text\"]\nprint(summary)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Configuration Update in Ray Serve Deployments\nDESCRIPTION: This Python code snippet shows how to implement a reconfigure method in a Ray Serve deployment. This allows for dynamic updates to configuration parameters without restarting deployment replicas, facilitating continuous adjustments in model operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/config.md#2025-04-12_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n@serve.deployment\nclass Model:\n    def reconfigure(self, config: Dict[str, Any]):\n        self.threshold = config[\"threshold\"]\n```\n\n----------------------------------------\n\nTITLE: Creating a TuneReportCheckpointCallback for Population Based Training\nDESCRIPTION: Configures a callback that handles both reporting metrics to Ray Tune and creating checkpoints in PyTorch Lightning. This callback is essential for Population Based Training (PBT) as it enables the scheduler to access model performance and save/load states.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nTuneReportCheckpointCallback(\n    metrics={\"loss\": \"ptl/val_loss\", \"mean_accuracy\": \"ptl/val_accuracy\"},\n    filename=\"checkpoint\",\n    on=\"validation_end\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Dependencies and Utility Functions for Hyperparameter Tuning in Python\nDESCRIPTION: This snippet imports necessary libraries, initializes Ray, and defines utility functions for generating random hyperparameters and loading data. It sets up the foundation for the hyperparameter tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_hyperparameter.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nfrom filelock import FileLock\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\nimport ray\n\nray.init()\n\n# The number of sets of random hyperparameters to try.\nnum_evaluations = 10\n\n\n# A function for generating random hyperparameters.\ndef generate_hyperparameters():\n    return {\n        \"learning_rate\": 10 ** np.random.uniform(-5, 1),\n        \"batch_size\": np.random.randint(1, 100),\n        \"momentum\": np.random.uniform(0, 1),\n    }\n\n\ndef get_data_loaders(batch_size):\n    mnist_transforms = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n\n    # We add FileLock here because multiple workers will want to\n    # download data, and this may cause overwrites since\n    # DataLoader is not threadsafe.\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_loader = torch.utils.data.DataLoader(\n            datasets.MNIST(\n                \"~/data\", train=True, download=True, transform=mnist_transforms\n            ),\n            batch_size=batch_size,\n            shuffle=True,\n        )\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n    return train_loader, test_loader\n```\n\n----------------------------------------\n\nTITLE: Installing Ray with pip\nDESCRIPTION: Installs Ray using pip, the Python package installer. This is the standard method for installing Ray and its dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install ray\"\n```\n\n----------------------------------------\n\nTITLE: Basic Ray on Spark Application Example\nDESCRIPTION: Example showing how to create a temporary Ray cluster within a Spark application. The code demonstrates cluster setup, Ray initialization, and proper shutdown procedures.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/spark.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\nfrom ray.util.spark import setup_ray_cluster, shutdown_ray_cluster, MAX_NUM_WORKER_NODES\nif __name__ == \"__main__\":\n    spark = SparkSession \\\n        .builder \\\n        .appName(\"Ray on spark example 1\") \\\n        .config(\"spark.task.cpus\", \"4\") \\\n        .getOrCreate()\n\n    setup_ray_cluster(max_worker_nodes=MAX_NUM_WORKER_NODES)\n\n    ray.init()\n    ...\n\n    shutdown_ray_cluster()\n```\n\n----------------------------------------\n\nTITLE: Defining a Multi-Objective Function in Python\nDESCRIPTION: This snippet defines a multi-objective function that enables simultaneous optimization of multiple metrics (loss and gain) during the tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef multi_objective(config):\n    # Hyperparameters\n    width, height = config[\"width\"], config[\"height\"]\n\n    for step in range(config[\"steps\"]):\n        # Iterative training function - can be any arbitrary training procedure\n        intermediate_score = evaluate(step, config[\"width\"], config[\"height\"], config[\"activation\"])\n        # Feed the score back back to Tune.\n        tune.report({\n           \"iterations\": step, \"loss\": intermediate_score, \"gain\": intermediate_score * width\n        })\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Agent Training with PPO in Python\nDESCRIPTION: This snippet sets up a multi-agent training configuration using PPO from the RLlib library. It defines the environment and policy mapping function for different agent types, including configuration overrides for specific agents. The configuration is then built, and the training process is initiated.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithm.ppo import PPOConfig\nfrom ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\nfrom ray.rllib.core.rl_module.rl_module import RLModuleSpec\n\nconfig = (\n    PPOConfig()\n    .environment(env=\"my_multiagent_env\")\n    .multi_agent(\n        policy_mapping_fn=lambda agent_id, episode, **kwargs: (\n            \"traffic_light\" if agent_id.startswith(\"traffic_light_\")\n            else random.choice([\"car1\", \"car2\"])\n        ),\n        algorithm_config_overrides_per_module={\n            \"car1\": PPOConfig.overrides(gamma=0.85),\n            \"car2\": PPOConfig.overrides(lr=0.00001),\n        },\n    )\n    .rl_module(\n        rl_module_spec=MultiRLModuleSpec(rl_module_specs={\n            \"car1\": RLModuleSpec(),\n            \"car2\": RLModuleSpec(),\n            \"traffic_light\": RLModuleSpec(),\n        }),\n    )\n)\n\nalgo = config.build()\nprint(algo.train())\n```\n\n----------------------------------------\n\nTITLE: GPU Resource Management in Ray Tune\nDESCRIPTION: Demonstrates how to allocate GPU resources to trials and combine CPU/GPU resource specifications. Shows automatic CUDA_VISIBLE_DEVICES management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-resources.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# If you have 8 GPUs, this will run 8 trials at once.\ntrainable_with_gpu = tune.with_resources(trainable, {\"gpu\": 1})\ntuner = tune.Tuner(\n    trainable_with_gpu,\n    tune_config=tune.TuneConfig(num_samples=10)\n)\nresults = tuner.fit()\n\n# If you have 4 CPUs and 1 GPU on your machine, this will run 1 trial at a time.\ntrainable_with_cpu_gpu = tune.with_resources(trainable, {\"cpu\": 2, \"gpu\": 1})\ntuner = tune.Tuner(\n    trainable_with_cpu_gpu,\n    tune_config=tune.TuneConfig(num_samples=10)\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Usage to Avoid Out-of-Memory Errors in Ray\nDESCRIPTION: This snippet shows how to configure the memory parameter when using Ray's map_batches function to avoid out-of-memory errors. It specifies that the function uses 1 GiB of memory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef uses_lots_of_memory(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    ...\n\n# Tell Ray that the function uses 1 GiB of memory\nds.map_batches(uses_lots_of_memory, memory=1 * 1024 * 1024)\n```\n\n----------------------------------------\n\nTITLE: Distributed Training of PyTorch Model on Fashion MNIST using Ray Train\nDESCRIPTION: This code snippet demonstrates the complete process of setting up and running distributed training for a PyTorch model on the Fashion MNIST dataset using Ray Train. It includes data loading, model definition, training function, and Ray Train configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_fashion_mnist_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom filelock import FileLock\nfrom torch.utils.data import DataLoader, random_split\nimport torch.nn.functional as F\nfrom ray import train\nimport ray.train.torch\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef train_func(config):\n    data_dir = train.get_data_dir()\n    with FileLock(f\"{data_dir}.lock\"):\n        dataset = datasets.FashionMNIST(\n            data_dir,\n            train=True,\n            download=True,\n            transform=transforms.Compose(\n                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n            ),\n        )\n    train_length = len(dataset)\n    test_length = 10000\n    train_dataset, val_dataset = random_split(\n        dataset, [train_length - test_length, test_length]\n    )\n    test_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=64,\n        shuffle=True,\n    )\n\n    train_loader = train.torch.prepare_data_loader(train_loader)\n    test_loader = train.torch.prepare_data_loader(test_loader)\n\n    model = Net()\n    model = train.torch.prepare_model(model)\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n\n    for epoch in range(10):\n        train_loss = 0\n        train_steps = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            train_steps += 1\n        train_loss /= train_steps\n\n        test_loss = 0\n        test_steps = 0\n        total = 0\n        correct = 0\n        for batch_idx, (data, target) in enumerate(test_loader):\n            output = model(data)\n            test_loss += F.nll_loss(output, target).item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target).sum().item()\n            test_steps += 1\n        test_loss /= test_steps\n        accuracy = correct / total\n\n        result = {\n            \"mean_accuracy\": accuracy,\n            \"mean_loss\": test_loss,\n            \"train_loss\": train_loss,\n        }\n        train.report(result, checkpoint=train.torch.to_cpu(model))\n    print(\"Finished Training\")\n\n\ndef train_fashion_mnist(num_workers=2, use_gpu=False):\n    trainer = train.Trainer(\n        backend=\"torch\", num_workers=num_workers, use_gpu=use_gpu\n    )\n    trainer.start()\n    result = trainer.run(\n        train_func,\n        config={\"lr\": 0.001, \"momentum\": 0.9},\n    )\n    trainer.shutdown()\n    print(f\"Best result: {result.metrics}\")\n\n\nif __name__ == \"__main__\n```\n\n----------------------------------------\n\nTITLE: Subclassing RLlibCallback in Python\nDESCRIPTION: This snippet demonstrates how to subclass RLlibCallback to implement a custom callback. The custom callback method on_algorithm_init is overridden to print a message when the algorithm is initialized. The snippet shows how to configure a DQNConfig to use the custom callback.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/callback.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.dqn import DQNConfig\nfrom ray.rllib.callbacks.callbacks import RLlibCallback\n\nclass MyCallback(RLlibCallback):\n    def on_algorithm_init(self, *, algorithm, metrics_logger, **kwargs):\n        print(f\"Algorithm {algorithm} has been initialized!\")\n\nconfig = (\n    DQNConfig()\n    .callbacks(MyCallback)\n)\n```\n\n----------------------------------------\n\nTITLE: vLLM Engine Configuration with Model Parallelism\nDESCRIPTION: Configuration example for handling larger models with tensor and pipeline parallelism in vLLM\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-llms.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = vLLMEngineProcessorConfig(\n    model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n    engine_kwargs={\n        \"max_model_len\": 16384,\n        \"tensor_parallel_size\": 2,\n        \"pipeline_parallel_size\": 2,\n        \"enable_chunked_prefill\": True,\n        \"max_num_batched_tokens\": 2048,\n    },\n    concurrency=1,\n    batch_size=64,\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Docker Image\nDESCRIPTION: This snippet shows how to specify a Docker image that the worker process will run in when using Ray. The provided example includes a specific image URI.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\"image_uri\": \"anyscale/ray:2.31.0-py39-cpu\"}\n```\n\n----------------------------------------\n\nTITLE: Creating DAG with Ray Remote Classes and Methods\nDESCRIPTION: Shows how to use Ray DAG API with remote classes and their methods. The example demonstrates actor instantiation and method chaining specific to the parent actor instance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-dag.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\n    def double(self):\n        self.value *= 2\n        return self.value\n\n# Build DAG\ndag = Counter.bind().double.bind().increment.bind()\n\n# Execute DAG\nresult = ray.get(dag.execute())\nprint(result)  # Output: 3\n```\n\n----------------------------------------\n\nTITLE: Exception Handling in Dynamic Generators\nDESCRIPTION: This code snippet explains how exceptions raised in generator functions are handled, especially in context with static and dynamic 'num_returns'. It outlines the accessibility of previously stored values and how exceptions are represented in the returned ObjectRefs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tasks/generators.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/generator.py\n    :language: python\n    :start-after: __generator_errors_start__\n    :end-before: __generator_errors_end__\n```\n\n----------------------------------------\n\nTITLE: Define Training Function with setup_mlflow\nDESCRIPTION: Defines a training function that uses the `setup_mlflow` utility to initialize MLflow.  It logs metrics to MLflow using `mlflow.log_metrics(...)` in addition to reporting to Tune. The tracking URI is passed via the config.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-mlflow.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef train_function_mlflow(config):\n    tracking_uri = config.pop(\"tracking_uri\", None)\n    setup_mlflow(\n        config,\n        experiment_name=\"setup_mlflow_example\",\n        tracking_uri=tracking_uri,\n    )\n\n    # Hyperparameters\n    width, height = config[\"width\"], config[\"height\"]\n\n    for step in range(config.get(\"steps\", 100)):\n        # Iterative training function - can be any arbitrary training procedure\n        intermediate_score = evaluation_fn(step, width, height)\n        # Log the metrics to mlflow\n        mlflow.log_metrics(dict(mean_loss=intermediate_score), step=step)\n        # Feed the score back to Tune.\n        tune.report({\"iterations\": step, \"mean_loss\": intermediate_score})\n        time.sleep(0.1)\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Serve and Scikit-learn (Bash)\nDESCRIPTION: This command installs Ray Serve and scikit-learn using pip. `-U` upgrades to the latest version, and `ray[serve]` includes all dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install -U \\\"ray[serve]\\\" scikit-learn\"\n```\n\n----------------------------------------\n\nTITLE: Updating a LearnerGroup - Python\nDESCRIPTION: This snippet shows how to perform updates on the LearnerGroup, including both blocking and non-blocking update approaches. It emphasizes the use of a sample batch and the handling of asynchronous results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-learner.rst#2025-04-12_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nTIMESTEPS = {\"num_env_steps_sampled_lifetime\": 250}\n\n# This is a blocking update.\nresults = learner_group.update(batch=DUMMY_BATCH, timesteps=TIMESTEPS)\n\n# This is a non-blocking update. The results are returned in a future\n# call to `update(..., async_update=True)`\n_ = learner_group.update(batch=DUMMY_BATCH, async_update=True, timesteps=TIMESTEPS)\n\n# Artificially wait for async request to be done to get the results\n# in the next call to\n# `LearnerGroup.update(..., async_update=True)`.\ntime.sleep(5)\nresults = learner_group.update(\n    batch=DUMMY_BATCH, async_update=True, timesteps=TIMESTEPS\n)\n# `results` is a list of n result dicts from various Learner actors.\nassert isinstance(results, list), results\nassert isinstance(results[0], dict), results\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Files with Ray Data in Python\nDESCRIPTION: Demonstrates reading CSV files using Ray Data's read_csv function. The example reads from an S3 bucket and prints the schema of the resulting dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Ray Sleeper Actors\nDESCRIPTION: This code snippet modifies the ex4 function to create five Sleeper actors for parallel execution. Each call to actor_func goes to a different Sleeper, allowing tasks to run concurrently and reducing the overall execution time. Key parameters are the independent Sleeper actors and the use of ray.get() to retrieve results. This implementation focuses on enhancing parallelism.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/optimize-performance.rst#2025-04-12_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef ex4():\n    # Modified to create five separate Sleepers\n    five_actors = [Sleeper.remote() for i in range(5)]\n\n    # Each call to actor_func now goes to a different Sleeper\n    five_results = []\n    for actor_example in five_actors:\n        five_results.append(actor_example.actor_func.remote())\n\n    ray.get(five_results)\n```\n\n----------------------------------------\n\nTITLE: Preparing GaudiTrainer for Llama Pre-training\nDESCRIPTION: Defines a function to set up the GaudiTrainer for pre-training the Llama model on Intel Gaudi HPUs. Configures the trainer with the model, Gaudi-specific settings, training arguments, and datasets.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_trainer(training_args, datasets, tokenizer, model):\n    gaudi_config = GaudiConfig.from_pretrained(\n        training_args.gaudi_config_name, revision=\"main\",\n    )\n\n    trainer = GaudiTrainer(\n        model=model,\n        gaudi_config=gaudi_config,\n        args=training_args,\n        train_dataset=datasets[\"train\"],\n        eval_dataset=None,\n        tokenizer=tokenizer,\n        data_collator=default_data_collator,\n    )\n    return trainer\n```\n\n----------------------------------------\n\nTITLE: Enabling Verbose Stats Logging in Ray Data (Python)\nDESCRIPTION: This snippet demonstrates how to enable verbose stats logging in Ray Data. By setting the 'verbose_stats_logs' property to True, Ray Data will provide more detailed performance statistics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/monitoring-your-workload.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.data import DataContext\n\ncontext = DataContext.get_current()\ncontext.verbose_stats_logs = True\n```\n\n----------------------------------------\n\nTITLE: Using AWS Neuron Cores in Ray Tasks and Actors\nDESCRIPTION: This snippet shows how to use AWS Neuron Cores in Ray tasks and actors. It initializes Ray with 2 Neuron Cores, defines a Neuron Core actor and task, and demonstrates how Ray assigns Neuron Cores and sets the NEURON_RT_VISIBLE_CORES environment variable.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\n\nray.init(resources={\"neuron_cores\": 2})\n\n@ray.remote(resources={\"neuron_cores\": 1})\nclass NeuronCoreActor:\n    def ping(self):\n        print(\"Neuron Core IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"neuron_cores\"]))\n        print(\"NEURON_RT_VISIBLE_CORES: {}\".format(os.environ[\"NEURON_RT_VISIBLE_CORES\"]))\n\n@ray.remote(resources={\"neuron_cores\": 1})\ndef neuron_core_task():\n    print(\"Neuron Core IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"neuron_cores\"]))\n```\n\n----------------------------------------\n\nTITLE: Setting Up Ray Training Storage Configuration\nDESCRIPTION: Demonstrates different ways to configure storage paths for training results using RunConfig, including local paths, cloud storage, and NFS paths\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/common/torch-configure-run.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import RunConfig\n\n# Local path (/some/local/path/unique_run_name)\nrun_config = RunConfig(storage_path=\"/some/local/path\", name=\"unique_run_name\")\n\n# Shared cloud storage URI (s3://bucket/unique_run_name)\nrun_config = RunConfig(storage_path=\"s3://bucket\", name=\"unique_run_name\")\n\n# Shared NFS path (/mnt/nfs/unique_run_name)\nrun_config = RunConfig(storage_path=\"/mnt/nfs\", name=\"unique_run_name\")\n```\n\n----------------------------------------\n\nTITLE: Reading Image Dataset with Ray Data\nDESCRIPTION: Demonstrates how to read image data into a Ray Dataset and display its schema, showing tensor representation as NumPy arrays.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-tensors.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@air-example-data/digits\")\nprint(ds)\n```\n\n----------------------------------------\n\nTITLE: Querying Deployments During Controller Recovery\nDESCRIPTION: This snippet demonstrates how to query deployments while the Serve controller is recovering, showing that the system remains functional during this process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> import requests\n>>> requests.get(\"http://localhost:8000\").json()\n347\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP Server in Ray Actor\nDESCRIPTION: Demonstrates how to set up an HTTP server inside a Ray actor to enable external communication with the actor from outside the Ray cluster. This pattern allows for REST API endpoints to be exposed through the actor.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/out-of-band-communication.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n../doc_code/actor-http-server.py\n```\n\n----------------------------------------\n\nTITLE: Custom Serialization for DBConnection Class\nDESCRIPTION: Demonstrates how to implement custom serialization for a class by defining the __reduce__ method. This example shows how to make a DBConnection object serializable in Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/serialization.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport sqlite3\n\nclass DBConnection:\n    def __init__(self, path):\n        self.path = path\n        self.conn = sqlite3.connect(path)\n\n    # without '__reduce__', the instance is unserializable.\n    def __reduce__(self):\n        deserializer = DBConnection\n        serialized_data = (self.path,)\n        return deserializer, serialized_data\n\noriginal = DBConnection(\"/tmp/db\")\nprint(original.conn)\n\ncopied = ray.get(ray.put(original))\nprint(copied.conn)\n```\n\n----------------------------------------\n\nTITLE: Parallel Rollout Worker Implementation\nDESCRIPTION: Implementation of a Ray actor class that performs rollouts and computes gradients in parallel\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_pong_example.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nray.init()\n\n\n@ray.remote\nclass RolloutWorker(object):\n    def __init__(self):\n        self.env = gym.make(\"ale_py:ALE/Pong-v5\")\n\n    def compute_gradient(self, model):\n        # Compute a simulation episode.\n        xs, hs, dlogps, drs = rollout(model, self.env)\n        reward_sum = sum(drs)\n        # Vectorize the arrays.\n        epx = np.vstack(xs)\n        eph = np.vstack(hs)\n        epdlogp = np.vstack(dlogps)\n        epr = np.vstack(drs)\n\n        # Compute the discounted reward backward through time.\n        discounted_epr = process_rewards(epr)\n        # Standardize the rewards to be unit normal (helps control the gradient\n        # estimator variance).\n        discounted_epr -= np.mean(discounted_epr)\n        discounted_epr /= np.std(discounted_epr)\n        # Modulate the gradient with advantage (the policy gradient magic\n        # happens right here).\n        epdlogp *= discounted_epr\n        return model.policy_backward(eph, epx, epdlogp), reward_sum\n```\n\n----------------------------------------\n\nTITLE: Custom CNN-Based RLModule\nDESCRIPTION: Demonstrates a custom CNN architecture as an RLModule for convolutional feature extraction tailored to the environment's visual observations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Custom CNN-based RLModule\n# This script demonstrates a custom CNN architecture for RL.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Working with Huawei Ascend NPUs in Ray\nDESCRIPTION: Demonstrates how to initialize Ray with Huawei Ascend NPU resources, create NPU-aware actors and tasks, and access NPU IDs and environment variables. Shows how Ray handles NPU allocation across tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\n\nray.init(resources={\"NPU\": 2})\n\n@ray.remote(resources={\"NPU\": 1})\nclass NPUActor:\n    def ping(self):\n        print(\"NPU IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"NPU\"]))\n        print(\"ASCEND_RT_VISIBLE_DEVICES: {}\".format(os.environ[\"ASCEND_RT_VISIBLE_DEVICES\"]))\n\n@ray.remote(resources={\"NPU\": 1})\ndef npu_task():\n    print(\"NPU IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"NPU\"]))\n    print(\"ASCEND_RT_VISIBLE_DEVICES: {}\".format(os.environ[\"ASCEND_RT_VISIBLE_DEVICES\"]))\n\nnpu_actor = NPUActor.remote()\nray.get(npu_actor.ping.remote())\n# The actor uses the first NPU so the task uses the second one.\nray.get(npu_task.remote())\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Configuration for Ray Serve Applications\nDESCRIPTION: This YAML snippet defines the configuration structure for deploying applications using Ray Serve, including global options such as proxy location, HTTP options, and application-specific settings like route prefixes and deployments. The structure accommodates multiple applications and their deployment configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/config.md#2025-04-12_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nproxy_location: ...\n\nhttp_options: \n  host: ...\n  port: ...\n  request_timeout_s: ...\n  keep_alive_timeout_s: ...\n\ngrpc_options:\n  port: ...\n  grpc_servicer_functions: ...\n\nlogging_config:\n  log_level: ...\n  logs_dir: ...\n  encoding: ...\n  enable_access_log: ...\n\napplications:\n- name: ...\n  route_prefix: ...\n  import_path: ...\n  runtime_env: ... \n  deployments:\n  - name: ...\n    num_replicas: ...\n    ...\n  - name:\n    ...\n```\n\n----------------------------------------\n\nTITLE: DreamBooth Fine-Tuning with LoRA in Python\nDESCRIPTION: This Python command demonstrates how to run DreamBooth fine-tuning with LoRA (Low-Rank Adaptation). It includes various parameters such as model directories, image directories, prompts, and training settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/README.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython train.py \\\n  --model_dir=$ORIG_MODEL_PATH \\\n  --output_dir=$TUNED_MODEL_DIR \\\n  --instance_images_dir=$IMAGES_OWN_DIR \\\n  --instance_prompt=\"photo of $UNIQUE_TOKEN $CLASS_NAME\" \\\n  --class_images_dir=$IMAGES_REG_DIR \\\n  --class_prompt=\"photo of a $CLASS_NAME\" \\\n  --train_batch_size=2 \\\n  --lr=1e-4 \\\n  --num_epochs=10 \\\n  --max_train_steps=400 \\\n  --num_workers $NUM_WORKERS\n  --use_lora\n```\n\n----------------------------------------\n\nTITLE: Allocating GPUs for Learner Actors\nDESCRIPTION: Shows how to set GPU resources per Learner actor in RLlib's configuration. The setting `learners(num_gpus_per_learner=1)` specifies one GPU per Learner actor, optimizing training throughput. Supports fractional GPU allocation for scenarios with limited GPU availability.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/scaling-guide.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfig.learners(num_gpus_per_learner=1)\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing to MongoDB with Ray Data\nDESCRIPTION: This snippet shows how to read and write data to MongoDB using Ray Data. It utilizes the `ray.data.read_mongo` and `ds.write_mongo` functions to interact with MongoDB collections, including specifying a pipeline for filtering and sorting.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Read a local MongoDB.\nds = ray.data.read_mongo(\n    uri=\"mongodb://localhost:27017\",\n    database=\"my_db\",\n    collection=\"my_collection\",\n    pipeline=[{\"$match\": {\"col\": {\"$gte\": 0, \"$lt\": 10}}}, {\"$sort\": \"sort_col\"}],\n)\n\n# Reading a remote MongoDB is the same.\nds = ray.data.read_mongo(\n    uri=\"mongodb://username:password@mongodb0.example.com:27017/?authSource=admin\",\n    database=\"my_db\",\n    collection=\"my_collection\",\n    pipeline=[{\"$match\": {\"col\": {\"$gte\": 0, \"$lt\": 10}}}, {\"$sort\": \"sort_col\"}],\n)\n\n# Write back to MongoDB.\nds.write_mongo(\n    MongoDatasource(),\n    uri=\"mongodb://username:password@mongodb0.example.com:27017/?authSource=admin\",\n    database=\"my_db\",\n    collection=\"my_collection\",\n)\n```\n\n----------------------------------------\n\nTITLE: GAN Model Definition for PBT Training\nDESCRIPTION: Defines Generator and Discriminator classes for a Deep Convolutional GAN (DCGAN) using PyTorch. These models are used in the PBT example to generate MNIST digits.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n   :language: python\n   :start-after: __GANmodel_begin__\n   :end-before: __GANmodel_end__\n```\n\n----------------------------------------\n\nTITLE: Setting up Dataset and Model with TensorFlow and Ray Train (Python)\nDESCRIPTION: This code initializes a dataset and a model using TensorFlow/Keras, ready for distributed training with Ray Train. Placeholders `__tf_setup_begin__` and `__tf_setup_end__` mark the boundaries of this code snippet.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/tf/tensorflow_quick_start.py\n:language: python\n:start-after: __tf_setup_begin__\n:end-before: __tf_setup_end__\n```\"\n```\n\n----------------------------------------\n\nTITLE: Configuring and Launching Ray TorchTrainer with FSDP\nDESCRIPTION: Sets up and launches a distributed training job using Ray Train's TorchTrainer with FSDP strategy. Configures checkpoint saving, scaling across multiple GPUs, hyperparameters, and executes the training with the specified dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import RunConfig, ScalingConfig, CheckpointConfig\n\n# Save Ray Train checkpoints according to the performance on validation set\nrun_config = RunConfig(\n    name=\"finetune_dolly-v2-7b\",\n    storage_path=storage_path,\n    checkpoint_config=CheckpointConfig(num_to_keep=1),\n)\n\n# Scale the FSDP training workload across 16 GPUs\n# You can change this config based on your compute resources.\nscaling_config = ScalingConfig(\n    num_workers=num_workers, use_gpu=True, trainer_resources={\"memory\": 100 * 1024 ** 3}\n)\n\n# Configuration to pass into train_func\ntrain_config = {\n    \"lr\": 2e-5,\n    \"eps\": 1e-8,\n    \"strategy\": fsdp_strategy,\n    \"batch_size_per_worker\": 10\n}\n\n# Define a TorchTrainer and launch you training workload\nray_trainer = TorchTrainer(\n    train_func,\n    train_loop_config=train_config,\n    run_config=run_config,\n    scaling_config=scaling_config,\n    datasets={\"train\": train_ds},\n)\nresult = ray_trainer.fit()\n\nresult\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Runtime - C++\nDESCRIPTION: In this C++ snippet, the Ray runtime is initialized by calling `ray::Init()`. Like other languages, this function must be called before utilizing any Ray functionalities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n#include <ray/api.h>\n// Other Ray APIs will not work until `ray::Init()` is called.\nray::Init()\n```\n\n----------------------------------------\n\nTITLE: LightLoad Deployment Configuration - Autoscaling\nDESCRIPTION: This YAML configuration defines a Ray Serve deployment named 'LightLoad' with autoscaling enabled.  It sets the target number of ongoing requests to 1, minimum replicas to 0, initial replicas to 0, and a maximum replicas to 200. It also defines the upscale and downscale delays, as well as the upscaling and downscaling factors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/advanced-autoscaling.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n\"- name: LightLoad\n  max_ongoing_requests: 3\n  autoscaling_config:\n    target_ongoing_requests: 1\n    min_replicas: 0\n    initial_replicas: 0\n    max_replicas: 200\n    upscale_delay_s: 3\n    downscale_delay_s: 60\n    upscaling_factor: 0.3\n    downscaling_factor: 0.3\n    metrics_interval_s: 2\n    look_back_period_s: 10\"\n```\n\n----------------------------------------\n\nTITLE: Limiting Trial Concurrency in Ray Tune\nDESCRIPTION: Shows how to limit the maximum number of concurrent trials using max_concurrent_trials parameter in TuneConfig.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-resources.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune import TuneConfig\n\nconfig = TuneConfig(\n    # ...\n    num_samples=100,\n    max_concurrent_trials=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Sending Requests to Specific Models with Ray Serve\nDESCRIPTION: In this code snippet, a request is structured to target a specific model by including the `serve_multiplexed_model_id` in the request header. This facilitates the routing of specific model requests to their corresponding replicas efficiently.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/model-multiplexing.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# Structure your request to include the multiplexer model ID in the header.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Image Training Benchmark\nDESCRIPTION: Commands to download and run the PyTorch training job submission script, which submits a training job to the Ray cluster and provides instructions for tracking job status.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/gpu-training-example.md#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# Step 3: Run the PyTorch image training benchmark.\n# Install Ray if needed\npip3 install -U \"ray[default]\"\n\n# Download the Python script\ncurl https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/doc_code/pytorch_training_e2e_submit.py -o pytorch_training_e2e_submit.py\n\n# Submit the training job to your ray cluster\npython3 pytorch_training_e2e_submit.py\n# Example STDOUT:\n# Use the following command to follow this Job's logs:\n# ray job logs 'raysubmit_jNQxy92MJ4zinaDX' --follow\n\n# Track job status\n# Substitute the Ray Job's submission id.\nray job logs 'raysubmit_xxxxxxxxxxxxxxxx' --address http://127.0.0.1:8265 --follow\n```\n\n----------------------------------------\n\nTITLE: Optimizing Ray Tasks with Batching (Python)\nDESCRIPTION: This code snippet demonstrates a better approach using batching to amortize the overhead of task parallelization in Ray, improving overall performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/too-fine-grained-tasks.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# __batching_start__\n@ray.remote\ndef batch_increment(batch):\n    return [x + 1 for x in batch]\n\ndata = list(range(1000000))\nbatch_size = 1000\nbatches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\nresults = ray.get([batch_increment.remote(batch) for batch in batches])\nresults = [item for sublist in results for item in sublist]\n# __batching_end__\n```\n\n----------------------------------------\n\nTITLE: Initializing a Spark Session with RayDP\nDESCRIPTION: Creates a Spark session using RayDP's init_spark function, specifying application name and resource allocation for executors including cores and memory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/raydp.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport raydp\n\nray.init()\nspark = raydp.init_spark(\n  app_name = \"example\",\n  num_executors = 10,\n  executor_cores = 64,\n  executor_memory = \"256GB\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Agent Algorithm with MultiRLModuleSpec\nDESCRIPTION: Demonstrates setting up a multi-agent algorithm using MultiRLModuleSpec where multiple agents share the same RLModule, including configuring the environment and agent mapping.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.core.rl_module.rl_module import RLModuleSpec\nfrom ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\nfrom ray.rllib.examples.envs.classes.multi_agent import MultiAgentCartPole\n\nconfig = (\n    PPOConfig()\n    .environment(MultiAgentCartPole, env_config={\"num_agents\": 2})\n    .rl_module(\n        rl_module_spec=MultiRLModuleSpec(\n            # All agents (0 and 1) use the same (single) RLModule.\n            rl_module_specs=RLModuleSpec(\n                module_class=MyRLModuleClass,\n                model_config={\"some_key\": \"some_setting\"},\n            )\n        ),\n    )\n)\nppo = config.build()\nprint(ppo.get_module())\n```\n\n----------------------------------------\n\nTITLE: Customizing Exploration Behavior\nDESCRIPTION: This code shows how to customize exploration behavior in RLlib using the `exploration_config` parameter. It demonstrates how to specify the exploration class (e.g., EpsilonGreedy) and its constructor arguments, such as initial epsilon, final epsilon, and epsilon timesteps. This allows fine-grained control over the agent's exploration strategy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-advanced-api.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example on how to setup Exploration for RLlib.\n\n.. code-block:: python\n\n    config = {\n        # ... other Algorithm config keys.\n\n        # The Exploration sub-class to use.\n        \"exploration_config\": {\n            # The Exploration sub-class by name (str) or full path to module+class.\n            \"type\": \"ray.rllib.utils.exploration.epsilon_greedy.EpsilonGreedy\",\n            # Add constructor kwargs here (if any).\n            \"initial_epsilon\": 1.0,\n            \"final_epsilon\": 0.02,\n            \"epsilon_timesteps\": 10000,\n        }\n    }\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Inspecting Batches with NumPy in Ray Datasets\nDESCRIPTION: This code demonstrates how to inspect batches of data in a Ray Dataset using take_batch() method with NumPy format, showing batch structure and image shape for an image dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/inspecting-data.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\n\nbatch = ds.take_batch(batch_size=2, batch_format=\"numpy\")\nprint(\"Batch:\", batch)\nprint(\"Image shape\", batch[\"image\"].shape)\n```\n\n----------------------------------------\n\nTITLE: RayService Zero Downtime Upgrade Operations\nDESCRIPTION: Commands for performing and verifying zero downtime upgrades on Ray clusters, including checking cluster status and testing service availability.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f ray-service.sample.yaml\n\nkubectl describe rayservices rayservice-sample\n\nkubectl describe rayclusters $YOUR_RAY_CLUSTER\n\nkubectl apply -f ray-service.sample.yaml\n\nkubectl get raycluster\n\ncurl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/fruit/ -d '[\"MANGO\", 2]'\n```\n\n----------------------------------------\n\nTITLE: Memory Profiling Ray Tasks\nDESCRIPTION: Code example demonstrating memory profiling implementation for Ray tasks, noting that tasks have shorter lifetimes which may result in multiple profiling files.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-memory.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n__memray_profiling_task_start__\n__memray_profiling_task_end__\n```\n\n----------------------------------------\n\nTITLE: Exporting Metrics in Ray Actor with Python\nDESCRIPTION: This snippet demonstrates how to use Ray's metrics API to define and export custom metrics in a Python Actor. The metrics, which include types like Counter, Gauge, and Histogram, are made available for Prometheus at a local endpoint. Key parameters include the metric names and their associated labels. Dependencies include Ray and Prometheus configured to scrape the specified endpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/add-app-metrics.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.metrics import Counter, Gauge, Histogram\n\n# Example Actor implementation exporting metrics\nclass MyActor:\n    def __init__(self):\n        self.request_latency_histogram = Histogram(\n            'ray_request_latency',\n            description='Latencies of requests in ms.',\n            tag_keys=('actor_name',)\n        )\n        self.curr_count_gauge = Gauge(\n            'ray_curr_count',\n            description='Current count held by the actor. Goes up and down.',\n            tag_keys=('actor_name',)\n        )\n        self.num_requests_counter = Counter(\n            'ray_num_requests_total',\n            description='Number of requests processed by the actor.',\n            tag_keys=('actor_name',)\n        )\n```\n\n----------------------------------------\n\nTITLE: Loading Stable Diffusion Pipeline\nDESCRIPTION: Implements functions to load either a fully fine-tuned model or a LoRA-tuned model pipeline. The pipeline is moved to CUDA device for GPU acceleration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/playground.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom os import environ\n\nimport torch\nfrom diffusers import DiffusionPipeline\n\nfrom dreambooth.generate_utils import load_lora_weights, get_pipeline\n\npipeline = None\n\ndef on_full_ft():\n    global pipeline\n    pipeline = get_pipeline(TUNED_MODEL_PATH)\n    pipeline.to(\"cuda\")\n    \ndef on_lora_ft():\n    assert ORIG_MODEL_PATH\n    assert LORA_WEIGHTS_DIR\n    global pipeline\n    pipeline = get_pipeline(ORIG_MODEL_PATH, LORA_WEIGHTS_DIR)\n    pipeline.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Processing DeepSpeed ZeRO-3 Checkpoints for Inference\nDESCRIPTION: This snippet demonstrates how to process DeepSpeed ZeRO-3 checkpoints after training. It removes optimizer states, consolidates the checkpoint into a single file, and adjusts the state dict keys for compatibility with HuggingFace models.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nfrom deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\ndef extract_fp32_ckpt_from_zero(zero_ckpt_dir):\n    state_dict = get_fp32_state_dict_from_zero_checkpoint(zero_ckpt_dir)\n    vicuna_state_dict = {\n        k.replace(\"_forward_module.model.\", \"\"): v for k, v in state_dict.items()\n    }\n    torch.save(vicuna_state_dict, os.path.join(zero_ckpt_dir, \"full_model.pt\"))\n\n\nfull_model_ckpt_path = \"/mnt/local_storage/checkpoint.ckpt/full_model.pt\"\nextract_fp32_ckpt_from_zero(\"/mnt/local_storage/checkpoint.ckpt\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Passing with Ray Queue in Python\nDESCRIPTION: This example illustrates how to use Ray's Queue utility for message passing between tasks or actors. It demonstrates creating a queue, putting items into it, and getting items from it.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/actor-utils.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/actor-queue.py\n```\n\n----------------------------------------\n\nTITLE: Synchronizing S3 Checkpoint Locally Python\nDESCRIPTION: Uses the os.system call to download the Ray Train checkpoint from the specified S3 bucket to local storage. This is necessary for loading model weights and tokenizer later.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.system(f\"aws s3 sync s3://{checkpoint.path} /mnt/local_storage/\")\n```\n\n----------------------------------------\n\nTITLE: Applying RayJob Configuration for Batch Inference\nDESCRIPTION: Kubernetes command to apply the RayJob configuration, which creates both the Ray cluster and submits the batch inference job to it. This starts the deployment of resources and execution of the job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-batch-inference-example.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f ray-job.batch-inference.yaml\n```\n\n----------------------------------------\n\nTITLE: Checkpointing a Trained RLlib Algorithm\nDESCRIPTION: Saves the current state of the PPO algorithm to disk, creating a checkpoint that can be loaded later for continued training or deployment, with options to specify a custom path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_path = ppo.save_to_path()\n\n# OR:\n# ppo.save_to_path([a checkpoint location of your choice])\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Image Classification with Ray and Huggingface\nDESCRIPTION: Installs the necessary Python packages for running the image classification example, including Ray with data support, PyTorch, Transformers, and Pillow.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q -U \"ray[data]\" torch transformers Pillow\n```\n\n----------------------------------------\n\nTITLE: Setting Up the Main BERT Training Function\nDESCRIPTION: This Python snippet sets up the main function for distributed BERT training using the Ray framework. It includes resource configuration and initializes the training process with the specified parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/bert.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef train_bert(num_workers=2):\n    global_batch_size = 8\n\n    train_config = {\n        \"lr\": 1e-3,\n        \"epochs\": 10,\n        \"batch_size_per_worker\": global_batch_size // num_workers,\n    }\n\n    scaling_config = ScalingConfig(num_workers=num_workers, resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n    torch_config = TorchConfig(backend = \"hccl\")\n    \n    # start your ray cluster\n    ray.init()\n    \n    # Initialize a Ray TorchTrainer\n    trainer = TorchTrainer(\n        train_loop_per_worker=train_func_per_worker,\n        train_loop_config=train_config,\n        torch_config=torch_config,\n        scaling_config=scaling_config,\n    )\n\n    result = trainer.fit()\n    print(f\"Training result: {result}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray for Distributed Tune Experiment (Python)\nDESCRIPTION: Python code snippet showing how to initialize Ray for a distributed Tune experiment using command line arguments to specify the Ray address.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--address\")\nargs = parser.parse_args()\nray.init(address=args.address)\n\ntuner = tune.Tuner(...)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Dataset Preprocessing Function for Fine-tuning\nDESCRIPTION: Transforms raw dataset into a format suitable for Llama-2 fine-tuning, creating prompts with instructions and inputs\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_dataset(raw_datasets):\n    PROMPT_DICT = {\n        \"prompt_with_input\": (\n            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n            \"Write a response that appropriately completes the request.\\n\\n\"\n            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n        ),\n        \"prompt_without_input\": (\n            \"Below is an instruction that describes a task. \"\n            \"Write a response that appropriately completes the request.\\n\\n\"\n            \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n        ),\n    }\n\n    def create_prompts(examples):\n        prompts = {}\n        prompts[\"source\"] = []\n        prompts[\"target\"] = []\n        for example in examples:\n            prompt_template = (\n                PROMPT_DICT[\"prompt_with_input\"] if example[\"input\"] != \"\" else PROMPT_DICT[\"prompt_without_input\"]\n            )\n            source = prompt_template.format_map(example)\n            prompts[\"source\"].append(source)\n            prompts[\"target\"].append(example[\"output\"])\n        return prompts\n\n    for key in raw_datasets:\n        prompts = create_prompts(raw_datasets[key])\n        columns_to_be_removed = list(raw_datasets[key].features.keys())\n        raw_datasets[key] = raw_datasets[key].add_column(\"prompt_sources\", prompts[\"source\"])\n        raw_datasets[key] = raw_datasets[key].add_column(\"prompt_targets\", prompts[\"target\"])\n        raw_datasets[key] = raw_datasets[key].remove_columns(columns_to_be_removed)\n```\n\n----------------------------------------\n\nTITLE: Using Ray Generators with Different Actor Models in Python\nDESCRIPTION: This snippet shows how Ray generators can be used with different actor execution models, including regular actors, async actors, and threaded actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-generator.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass RegularActor:\n    def generator(self):\n        for i in range(3):\n            yield i\n\n@ray.remote\nclass AsyncActor:\n    async def generator(self):\n        for i in range(3):\n            yield i\n\n@ray.remote(num_cpus=2)\nclass ThreadedActor:\n    def generator(self):\n        for i in range(3):\n            yield i\n\n# Usage\nregular_actor = RegularActor.remote()\ngen1 = regular_actor.generator.remote()\n\nasync_actor = AsyncActor.remote()\ngen2 = async_actor.generator.remote()\n\nthreaded_actor = ThreadedActor.remote()\ngen3 = threaded_actor.generator.remote()\n```\n\n----------------------------------------\n\nTITLE: Sending Inference Requests to Deployed Llama2-7b Model on HPU\nDESCRIPTION: This Python code snippet demonstrates how to send requests to the deployed Llama2-7b model to perform generation tasks. It includes the main client code for interacting with the model deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n__main_code_start__\n__main_code_end__\n```\n\n----------------------------------------\n\nTITLE: Client-side WebSocket Example - Python\nDESCRIPTION: This snippet demonstrates how to use the websockets package to query a deployment via WebSockets, showcasing the client-side implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"{literalinclude} doc_code/http_guide/websockets_example.py\\n:start-after: __websocket_serve_client_start__\\n:end-before: __websocket_serve_client_end__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Scaling Learner Actors for Distributed Training\nDESCRIPTION: Configures the number of Learner actors in RLlib for distributed training. Uses `learners(num_learners=m)` to set remote Learner actors, enabling data parallelism. Recommended to match the number of Learner actors with available GPUs for optimal performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/scaling-guide.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\nconfig = (\n    PPOConfig()\n    # Use 2 remote Learner actors (default is 0) for distributed data parallelism.\n    # Choosing 0 creates a local Learner instance on the main Algorithm process.\n    .learners(num_learners=2)\n)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing M5 Competition Data for Time Series Forecasting\nDESCRIPTION: This function retrieves and preprocesses data from the M5 forecasting competition, filtering for a specific item and formatting it for use with StatsForecast models.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/automl_for_time_series.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_m5_partition(unique_id: str) -> pd.DataFrame:\n    ds1 = pq.read_table(\n        \"s3://anonymous@m5-benchmarks/data/train/target.parquet\",\n        filters=[(\"item_id\", \"=\", unique_id)],\n    )\n    Y_df = ds1.to_pandas()\n    # StatsForecasts expects specific column names!\n    Y_df = Y_df.rename(\n        columns={\"item_id\": \"unique_id\", \"timestamp\": \"ds\", \"demand\": \"y\"}\n    )\n    Y_df[\"unique_id\"] = Y_df[\"unique_id\"].astype(str)\n    Y_df[\"ds\"] = pd.to_datetime(Y_df[\"ds\"])\n    Y_df = Y_df.dropna()\n    constant = 10\n    Y_df[\"y\"] += constant\n    return Y_df[Y_df.unique_id == unique_id]\n```\n\n----------------------------------------\n\nTITLE: Enabling Fast Downloads for Hugging Face Models with hf_transfer\nDESCRIPTION: Python code showing how to accelerate Hugging Face model downloads by enabling the HF_HUB_ENABLE_HF_TRANSFER environment variable in the Ray Serve LLM deployment configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.llm import LLMConfig, LLMServer, LLMRouter\nimport os\n\nllm_config = LLMConfig(\n    model_loading_config=dict(\n        model_id=\"llama-3-8b-instruct\",\n        model_source=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    ),\n    deployment_config=dict(\n        autoscaling_config=dict(\n            min_replicas=1, max_replicas=2,\n        )\n    ),\n    # Pass the desired accelerator type (e.g. A10G, L4, etc.)\n    accelerator_type=\"A10G\",\n    runtime_env=dict(\n        env_vars=dict(\n            HF_TOKEN=os.environ[\"HF_TOKEN\"],\n            HF_HUB_ENABLE_HF_TRANSFER=\"1\"\n        )\n    ),\n)\n\n# Deploy the application\ndeployment = LLMServer.as_deployment(llm_config.get_serve_options(name_prefix=\"vLLM:\")).bind(llm_config)\nllm_app = LLMRouter.as_deployment().bind([deployment])\nserve.run(llm_app, blocking=True)\n```\n\n----------------------------------------\n\nTITLE: Accessing Metrics DataFrame in Ray Train\nDESCRIPTION: This snippet demonstrates how to access all reported metrics as a pandas DataFrame from a Ray Train `Result` object. The `result.metrics_dataframe` attribute provides a tabular view of the metrics reported during training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/results.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Access metrics dataframe of all reported checkpoints.\"\"\"\n# Get the metrics dataframe of all checkpoints.\ndf = result.metrics_dataframe\nprint(f\"Metrics dataframe: {df}\")\n\n```\n\n----------------------------------------\n\nTITLE: Creating Ray Remote Task\nDESCRIPTION: Converting sequential function into a Ray remote task for parallel execution\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray \n\n\n@ray.remote\ndef retrieve_task(item):\n    return retrieve(item)\n```\n\n----------------------------------------\n\nTITLE: Retrieve Named Placement Group in Python\nDESCRIPTION: This Python snippet shows how to retrieve a named placement group within the same namespace.  This allows access to placement groups created by other drivers if directly passing the placement group handle is not feasible.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"        # Connecting to the cluster with a namespace.\n        ray.init(namespace=\"my_namespace\")\n\n        # Create a placement group with a name.\n        pg = ray.util.placement_group([{\"CPU\": 1}], name=\"name\")\n        ray.get(pg.ready())\n        assert pg.id().hex() in ray.get(ray.internal.worker._global_worker.core_worker.dump_placement_group_debug_info())\n\n        # Retrieve the placement group by the name.\n        pg_by_name = ray.util.get_placement_group(\"name\", namespace=\"my_namespace\")\n        assert pg_by_name.id() == pg.id()\n\n        # Clean up.\n        ray.util.remove_placement_group(pg)\n        assert pg.id().hex() not in ray.get(ray.internal.worker._global_worker.core_worker.dump_placement_group_debug_info())\n        ray.shutdown()\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Tune Tuner in Python\nDESCRIPTION: This snippet shows how to initialize a Ray Tune Tuner object with a training function and configuration for hyperparameter optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune_get_data_in_and_out.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune import Tuner\nfrom ray import tune\n\ntuner = Tuner(training_function, tune_config=tune.TuneConfig(num_samples=4))\n```\n\n----------------------------------------\n\nTITLE: Setting Up Ray Distributed Training with Habana Gaudi HPUs\nDESCRIPTION: This main function configures and initializes Ray for distributed training on Habana Gaudi processors. It sets up the training configuration including model parameters, DeepSpeed integration, and HPU-specific settings, then creates a TorchTrainer with the appropriate scaling config to utilize HPUs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef main(num_workers, execution_mode):\n    import ray\n    from ray.train import ScalingConfig\n    from ray.train.torch import TorchTrainer, TorchConfig\n\n    pretrain_config = {\n        \"datasets\": {\n            \"name\": \"wikitext\",\n            \"config_name\": \"wikitext-2-raw-v1\",\n        },\n        \"tokenizer\": {\n            \"name\": \"huggyllama/llama-7b\",\n            \"config\": {}\n        },\n        \"model\": {\n            \"name\": \"huggyllama/llama-7b\",\n            \"config\": {\n                \"torch_dtype\": \"bfloat16\",\n            },\n        },\n        \"training_args\": {\n            \"per_device_train_batch_size\": 1,\n            \"do_train\": True,\n            \"save_strategy\": \"no\",\n            \"output_dir\": \"/tmp/ray/pretrain-llama-2\",\n            \"logging_steps\": 1,\n            \"gaudi_config_name\": \"Habana/llama\",\n            \"use_habana\": True,\n            \"throughput_warmup_steps\": 3,\n            \"use_lazy_mode\": True,\n            \"overwrite_output_dir\": True,\n            \"seed\": 42,\n            \"bf16\": True,\n            \"report_to\":'tensorboard',\n            \"deepspeed\": {\n                \"steps_per_print\": 64,\n                \"train_batch_size\": \"auto\",\n                \"train_micro_batch_size_per_gpu\": \"auto\",\n                \"gradient_accumulation_steps\": \"auto\",\n                \"bf16\": {\n                    \"enabled\": True\n                },\n                \"gradient_clipping\": 1.0,\n                \"zero_optimization\": {\n                    \"stage\": 3,\n                    \"overlap_comm\": False,\n                    \"reduce_scatter\": False,\n                    \"contiguous_gradients\": False,\n                    \"stage3_gather_16bit_weights_on_model_save\": True\n                }\n            },\n        },\n    }\n\n    # if execution mode is eager with compile, must spcified with a compile backend\n    if execution_mode == \"eager.compile\":\n        pretrain_config[\"training_args\"].update({\"torch_compile_backend\": \"hpu_backend\"})\n\n    scaling_config = ScalingConfig(num_workers=num_workers,\n                                   use_gpu=False,\n                                   resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n\n    # Set backend to hccl in TorchConfig\n    torch_config = TorchConfig(backend=\"hccl\")\n\n    ray.init()\n\n    # Initialize a Ray TorchTrainer\n    trainer = TorchTrainer(\n        train_loop_per_worker=pretrain_llama,\n        train_loop_config=pretrain_config,\n        torch_config=torch_config,\n        scaling_config=scaling_config\n    )\n\n    result = trainer.fit()\n    print(result)\n```\n\n----------------------------------------\n\nTITLE: Custom Gradio Server Implementation\nDESCRIPTION: Implementation of a custom GradioServer that enables parallel model execution through Ray Serve\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/gradio-integration.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment\nclass MyGradioServer(GradioIngress):\n    def __init__(self, d1, d2):\n        self._d1 = d1\n        self._d2 = d2\n\n        def create_app():\n            async def fanout(text):\n                import asyncio\n                result1, result2 = await asyncio.gather(\n                    self._d1.generate.remote(text),\n                    self._d2.generate.remote(text),\n                )\n                return result1, result2\n\n            io = gr.Interface(\n                fn=fanout,\n                inputs=gr.Textbox(lines=3, placeholder=\"Text to complete\"),\n                outputs=[\"text\", \"text\"],\n                title=\"Text Generation with Multiple Models\",\n            )\n            return io\n\n        super().__init__(create_app)\n```\n\n----------------------------------------\n\nTITLE: Example of Deployment Configuration Priority YAML\nDESCRIPTION: This YAML configuration complements the Python example above. It specifies `num_replicas` in the config file, which will override the value set in the application code. Other settings not specified in the config will use either the application code's value or the default Serve value.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/configure-serve-deployment.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napplications:\n  - name: default\n    import_path: models:example_app \n    deployments:\n      - name: ExampleDeployment\n        num_replicas: 5\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet Files with Ray Data in Python\nDESCRIPTION: Demonstrates how to read Parquet files using Ray Data's read_parquet function. The example reads from an S3 bucket and prints the schema of the resulting dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Training Function with Parameters\nDESCRIPTION: Example of a training function that accepts configuration parameters for learning rate and epochs, demonstrating how to pass training parameters through train_loop_config.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/common/torch-configure-train_func.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_func(config):\n    lr = config[\"lr\"]\n    num_epochs = config[\"num_epochs\"]\n\nconfig = {\"lr\": 1e-4, \"num_epochs\": 10}\ntrainer = ray.train.torch.TorchTrainer(train_func, train_loop_config=config, ...)\n```\n\n----------------------------------------\n\nTITLE: Executing Distributed Tasks with Ray\nDESCRIPTION: This snippet demonstrates how to execute remote tasks using Ray, printing messages from each task, which may be deduplicated by Rays log deduplication feature. Key dependency: ray package. The task function prints a string concatenated with a random number. It employs ray.get() to retrieve results from multiple remote tasks. Output is deduplicated with repeats indicated in output summary messages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport random\n\n@ray.remote\ndef task():\n    print(\"Hello there, I am a task\", random.random())\n\nray.get([task.remote() for _ in range(100)])\n```\n\n----------------------------------------\n\nTITLE: KubeRay RayService Configuration for Custom Docker Image\nDESCRIPTION: YAML configuration for deploying a custom Ray Serve application using a pre-built Docker image in a Kubernetes environment with KubeRay.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/docker.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# Configuration details not fully provided in snippets\n```\n\n----------------------------------------\n\nTITLE: Full vSphere Configuration (Autoscaler)\nDESCRIPTION: This YAML configuration represents a comprehensive Ray autoscaler setup for vSphere, showcasing a detailed configuration with various options. It defines node types, resource allocation, and advanced settings for managing a Ray cluster on vSphere.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/vsphere/example-full.yaml\n            :language: yaml\n\n```\n\n----------------------------------------\n\nTITLE: Using Custom InputNode in Ray DAG\nDESCRIPTION: Demonstrates the usage of InputNode in a Ray DAG to represent user input values at runtime. InputNode should be used within a context manager and called as arguments of dag_node.execute().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-dag.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray.dag import InputNode\n\n@ray.remote\ndef add(x, y):\n    return x + y\n\nwith InputNode() as input_1, InputNode() as input_2:\n    dag = add.bind(input_1, input_2)\n\nresult = ray.get(dag.execute(1, 2))\nprint(result)  # Output: 3\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Resources for RayCluster Worker Group in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure a RayCluster worker group with GPU resources. It specifies GPU limits and requests, sets up autoscaling parameters, and uses the Ray ML GPU-enabled Docker image.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gpu.rst#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ngroupName: gpu-group\nreplicas: 0\nminReplicas: 0\nmaxReplicas: 5\n...\ntemplate:\n    spec:\n     ...\n     containers:\n      - name: ray-node\n        image: rayproject/ray-ml:2.6.3-gpu\n        ...\n        resources:\n         nvidia.com/gpu: 1 # Optional, included just for documentation.\n         cpu: 3\n         memory: 50Gi\n        limits:\n         nvidia.com/gpu: 1 # Required to use GPU.\n         cpu: 3\n         memory: 50Gi\n         ...\n```\n\n----------------------------------------\n\nTITLE: Utilizing Ray Actors in Java\nDESCRIPTION: This Java snippet demonstrates how to create a Ray actor and execute tasks using that actor. It shows the instantiation of an actor and the execution of tasks in a loop, with dependencies on the Ray Java API. Inputs are actor and task definitions, and output is the counter value printed out. The snippet assumes a working Ray cluster setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors.rst#2025-04-12_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nActorHandle<Counter> counter = Ray.actor(Counter::new).remote();\n\n// Start some tasks that use the actor.\nfor (int i = 0; i < 3; i++) {\n  Ray.task(MyRayApp::foo, counter).remote();\n}\n\n// Print the counter value.\nfor (int i = 0; i < 10; i++) {\n  TimeUnit.SECONDS.sleep(1);\n  System.out.println(counter.task(Counter::getCounter).remote().get());\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Ray for general Python applications\nDESCRIPTION: Installs Ray with default components for general Python applications. Also shows a minimal installation option without Dashboard or Cluster Launcher.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install -U \"ray[default]\"\n\n# If you don't want Ray Dashboard or Cluster Launcher, install Ray with minimal dependencies instead.\n# pip install -U \"ray\"\n```\n\n----------------------------------------\n\nTITLE: Objective Function for Ray Tune in Python\nDESCRIPTION: Implements an objective function which evaluates the performance of experiments over a defined number of steps and reports results via Ray Tune. It takes a configuration dictionary as input, containing hyperparameter information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef objective(config):\n    for step in range(config[\"steps\"]):\n        score = evaluate(step, config[\"width\"], config[\"height\"])\n        tune.report({\"iterations\": step, \"mean_loss\": score})\n```\n\n----------------------------------------\n\nTITLE: Executing Single-Worker TensorFlow Training Function (Python)\nDESCRIPTION: Executes the single-worker TensorFlow training function. Placeholders `__tf_single_run_begin__` and `__tf_single_run_end__` and the `:dedent: 0` parameter mark the code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/tf/tensorflow_quick_start.py\n:language: python\n:start-after: __tf_single_run_begin__\n:end-before: __tf_single_run_end__\n:dedent: 0\n```\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Asynchronous Vectorization for EnvRunners\nDESCRIPTION: Demonstrates the activation of asynchronous vectorization for sub-environments in RLlib using gymnasium's API. Configures `gym_env_vectorize_mode` to `ASYNC` for asynchronous environment operations, allowing each sub-environment to step or reset independently. Increases sampling speed with multiple sub-environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/scaling-guide.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nconfig.env_runners(\n    gym_env_vectorize_mode=gym.envs.registration.VectorizeMode.ASYNC,  # default is `SYNC`\n)\n```\n\n----------------------------------------\n\nTITLE: Using the Dask-on-Ray Scheduler in Python\nDESCRIPTION: This example demonstrates how to use the Dask-on-Ray scheduler to execute Dask computations on a Ray cluster. It imports the ray_dask_get scheduler and uses it with a Dask array computation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/dask-on-ray.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport dask.array as da\nfrom ray.util.dask import ray_dask_get\n\n# Start Ray\nray.init()\n\n# Create a Dask array\nx = da.random.random((1000, 1000), chunks=(100, 100))\n\n# Perform a computation using Dask-on-Ray\nresult = x.mean().compute(scheduler=ray_dask_get)\n```\n\n----------------------------------------\n\nTITLE: Testing Load Shedding with Parallel HTTP Requests\nDESCRIPTION: This Python code sends parallel HTTP requests to a Ray Serve deployment to test the load shedding behavior configured with `max_ongoing_requests` and `max_queued_requests`. It simulates multiple clients sending requests concurrently to observe how Serve handles the load. Requests exceeding the queue limit are expected to be rejected with a 503 error.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/best-practices.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport time\n\nimport requests\n\n\nasync def send_request(i: int):\n    try:\n        resp = requests.get(\"http://localhost:8000\")\n        print(f\"Request {i} finished with status code {resp.status_code}.\")\n    except requests.exceptions.ConnectionError:\n        print(f\"Request {i} failed.\")\n\n\nasync def main():\n    # give serve some time to start up\n    await asyncio.sleep(1)\n    tasks = [send_request(i) for i in range(10)]\n    await asyncio.gather(*tasks)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\n----------------------------------------\n\nTITLE: Reading TFRecords Files with Ray Data in Python\nDESCRIPTION: Illustrates reading TFRecords files using Ray Data's read_tfrecords function. The example reads from an S3 bucket and prints the schema of the resulting dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_tfrecords(\"s3://anonymous@ray-example-data/iris.tfrecords\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Converting Ray Dataset to Spark DataFrame in Python\nDESCRIPTION: This snippet demonstrates how to initialize a Spark session using raydp and convert a Ray Dataset to a Spark DataFrame. It reads CSV data from an S3 bucket and uses the to_spark() method for conversion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/saving-data.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport raydp\n\nspark = raydp.init_spark(\n    app_name = \"example\",\n    num_executors = 1,\n    executor_cores = 4,\n    executor_memory = \"512M\"\n)\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\ndf = ds.to_spark(spark)\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoint with PyTorch Lightning\nDESCRIPTION: Demonstrates how to use Ray Train's callback for PyTorch Lightning to save checkpoints. The callback reports metrics and checkpoints at the end of each training epoch.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef train_func():\n    model = LightningModule()\n    trainer = pl.Trainer(\n        # ... other trainer args ...\n        callbacks=[train.lightning.RayTrainReportCallback()],\n    )\n    trainer.fit(model)\n\ntrainer = train.torch.TorchTrainer(\n    train_func,\n    # ... other TorchTrainer args ...\n)\nresult = trainer.fit()\nprint(f\"Checkpoint saved at {result.checkpoint.path}\")\n```\n\n----------------------------------------\n\nTITLE: Connecting Ray Multiprocessing Pool to a Cluster in Python\nDESCRIPTION: Examples of different ways to connect a Ray multiprocessing Pool to a Ray cluster, including local cluster creation, connecting to the current node as head, or connecting to a remote head node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/multiprocessing.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.multiprocessing import Pool\n\n# Starts a new local Ray cluster.\npool = Pool()\n\n# Connects to a running Ray cluster, with the current node as the head node.\n# Alternatively, set the environment variable RAY_ADDRESS=\"auto\".\npool = Pool(ray_address=\"auto\")\n\n# Connects to a running Ray cluster, with a remote node as the head node.\n# Alternatively, set the environment variable RAY_ADDRESS=\"<ip_address>:<port>\".\npool = Pool(ray_address=\"<ip_address>:<port>\")\n```\n\n----------------------------------------\n\nTITLE: Modifying RayService Configuration for Redis - YAML\nDESCRIPTION: This YAML snippet illustrates how to update a RayService configuration to enable fault tolerance features that use an external Redis server. It includes necessary annotations and an environment variable for Redis connection details.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n\"\"\"apiVersion: ray.io/v1alpha1\nkind: RayService\nmetadata:\n  name: rayservice-sample\n  annotations:\n    ray.io/ft-enabled: \"true\"\n    ray.io/external-storage-namespace: \"my-raycluster-storage-namespace\"\nspec:\n  rayClusterConfig:\n    headGroupSpec:\n      template:\n        spec:\n          env:\n            - name: RAY_REDIS_ADDRESS\n              value: redis:6379\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray and Profiling Example Execution\nDESCRIPTION: The script initializes Ray and runs a profiling session using cProfile with the function ex4(). The purpose is to measure the performance of Ray actor tasks. Dependencies include cProfile and Ray. The main function initializes Ray and executes ex4() while recording profiling data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/optimize-performance.rst#2025-04-12_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef main():\n    ray.init()\n    cProfile.run('ex4()')\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Accessing Result Metrics in Ray Train\nDESCRIPTION: This snippet demonstrates how to access the metrics associated with the last reported checkpoint from a Ray Train `Result` object. The `result.metrics` attribute provides a dictionary-like access to these metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/results.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Access metrics of the last checkpoint.\"\"\"\n# Get the metrics of the last checkpoint.\nmetrics = result.metrics\nprint(f\"Last metrics: {metrics}\")\n\n```\n\n----------------------------------------\n\nTITLE: Reading JSON Lines with Ray Data\nDESCRIPTION: Shows how to read JSON Lines format files using ray.data.read_json(). Each JSON object becomes a row in the dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-text.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_json(\"s3://anonymous@ray-example-data/logs.json\")\n\nds.show(3)\n```\n\n----------------------------------------\n\nTITLE: Running the Tuning Experiment - Python\nDESCRIPTION: This code runs the optimization process using the defined objective function, tuning configuration, search algorithm, and scheduler. It aims to minimize the mean loss by exploring hyperparameter combinations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\\n    objective,\\n    tune_config=tune.TuneConfig(\\n        metric=\"mean_loss\",\\n        mode=\"min\",\\n        search_alg=algo,\\n        scheduler=scheduler,\\n        num_samples=num_samples,\\n    ),\\n    run_config=tune.RunConfig(\\n        name=\"bohb_exp\",\\n        stop={\"training_iteration\": 100},\\n    ),\\n    param_space=search_space,\\n)\\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Using Ray Generators with Asyncio in Python\nDESCRIPTION: This code demonstrates how to use Ray generators with asyncio. It shows how to use __anext__ and async for loops with ObjectRefGenerator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-generator.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\n@ray.remote\ndef async_generator():\n    for i in range(3):\n        yield i\n\nasync def main():\n    gen = async_generator.remote()\n    async for obj_ref in gen:\n        print(await obj_ref)\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Assigning Multiple GPUs to a Worker in Ray Train\nDESCRIPTION: Shows how to assign multiple GPUs to each worker by setting resources_per_worker, and how to access all available devices using get_devices() helper function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom ray.train import ScalingConfig\nfrom ray.train.torch import TorchTrainer, get_device, get_devices\n\n\ndef train_func():\n    assert torch.cuda.is_available()\n\n    device = get_device()\n    devices = get_devices()\n    assert device == torch.device(\"cuda:0\")\n    assert devices == [torch.device(\"cuda:0\"), torch.device(\"cuda:1\")]\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(\n        num_workers=1,\n        use_gpu=True,\n        resources_per_worker={\"GPU\": 2}\n    )\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Task Execution Order in Synchronous, Single-Threaded Actor in Python\nDESCRIPTION: This code snippet shows how tasks submitted to a synchronous, single-threaded actor are executed in the order they were submitted when coming from the same submitter. It uses a Counter class to demonstrate the sequential execution of tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/task-orders.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def add(self, addition):\n        self.value += addition\n        return self.value\n\ncounter = Counter.remote()\n\n# For tasks from the same submitter,\n# they are executed according to submission order.\nvalue0 = counter.add.remote(1)\nvalue1 = counter.add.remote(2)\n\n# Output: 1. The first submitted task is executed first.\nprint(ray.get(value0))\n# Output: 3. The later submitted task is executed later.\nprint(ray.get(value1))\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom RLModules\nDESCRIPTION: Provides guidance on implementing custom RL modules by subclassing the TorchRLModule or MultiRLModule base classes, detailing the setup for neural network subcomponents.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nTo implement your own neural network architecture and computation logic, subclass\n:py:class:`~ray.rllib.core.rl_module.torch_rl_module.TorchRLModule` for any single-agent learning experiment\nor for independent multi-agent learning.\n```\n\n----------------------------------------\n\nTITLE: Configuring Ax Search Algorithm with Constraints\nDESCRIPTION: Creates an AxSearch optimizer with parameter and outcome constraints. The parameter constraint limits x1+x2 to be at most 2.0, while the outcome constraint requires the L2 norm to be less than or equal to 1.25.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nalgo = AxSearch(\n    parameter_constraints=[\"x1 + x2 <= 2.0\"],\n    outcome_constraints=[\"l2norm <= 1.25\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Memory Requirement Specification in Ray\nDESCRIPTION: Example showing how to dynamically override memory requirements for tasks and actors at runtime using the options() method, allowing for flexible memory allocation based on runtime conditions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/memory-management.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# override the memory quota to 100MiB when submitting the task\nsome_function.options(memory=100 * 1024 * 1024).remote(x=1)\n\n# override the memory quota to 1GiB when creating the actor\nSomeActor.options(memory=1000 * 1024 * 1024).remote(a=1, b=2)\n```\n\n----------------------------------------\n\nTITLE: Creating GPU Node Pool with Queued Provisioning\nDESCRIPTION: Command to create a GPU-enabled node pool with queued provisioning and autoscaling capabilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-l4,count=1,gpu-driver-version=latest \\\n  --enable-queued-provisioning \\\n  --reservation-affinity=none  \\\n  --zone us-east4-c \\\n  --cluster kuberay-gpu-cluster \\\n  --num-nodes 0 \\\n  --min-nodes 0 \\\n  --max-nodes 10 \\\n  --enable-autoscaling \\\n  --machine-type g2-standard-4\n```\n\n----------------------------------------\n\nTITLE: Viewing Raylet Spilling Logs\nDESCRIPTION: This snippet shows the format of INFO level messages printed to the Raylet logs when object spilling occurs, including spill and restore statistics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/object-spilling.rst#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nlocal_object_manager.cc:166: Spilled 50 MiB, 1 objects, write throughput 230 MiB/s\nlocal_object_manager.cc:334: Restored 50 MiB, 1 objects, read throughput 505 MiB/s\n```\n\n----------------------------------------\n\nTITLE: Configuring HorovodTrainer with Custom Backend in Python\nDESCRIPTION: This code snippet shows how to create a HorovodTrainer with a custom backend configuration using HorovodConfig. It allows for more advanced setup of the Horovod environment within Ray Train.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/horovod.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\nfrom ray.train.horovod import HorovodTrainer, HorovodConfig\n\ntrainer = HorovodTrainer(\n    train_func,\n    tensorflow_backend=HorovodConfig(...),\n    scaling_config=ScalingConfig(num_workers=2),\n)\n```\n\n----------------------------------------\n\nTITLE: Programmatic Autoscaling Cluster Test in Python\nDESCRIPTION: Python code snippet demonstrating how to create a fake multi-node autoscaling cluster programmatically and run tasks that trigger autoscaling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/fake-autoscaler.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n   :language: python\n   :dedent: 4\n   :start-after: __example_begin__\n   :end-before: __example_end__\n```\n\n----------------------------------------\n\nTITLE: Conditional Search Space Definition in Python\nDESCRIPTION: Specifies a conditional hyperparameter search space using HyperOpt, enabling parameters such as 'mult' to depend on the value of 'activation'. This requires the use of nested dictionaries.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nconditional_space = {\n    \"activation\": hp.choice(\n        \"activation\",\n        [\n            {\"activation\": \"relu\", \"mult\": hp.uniform(\"mult\", 1, 2)},\n            {\"activation\": \"tanh\"},\n        ],\n    ),\n    \"width\": hp.uniform(\"width\", 0, 20),\n    \"height\": hp.uniform(\"height\", -100, 100),\n    \"steps\": 100,\n}\n```\n\n----------------------------------------\n\nTITLE: Managing Bulk Workflows in Ray\nDESCRIPTION: Shows bulk workflow management operations including listing workflows by status, resuming multiple workflows, and handling failed workflows. Demonstrates how to filter workflows by status and resume them in batch.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/management.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# List all running workflows.\nprint(workflow.list_all(\"RUNNING\"))\n\n# List RUNNING and CANCELED workflows.\nprint(workflow.list_all({\"RUNNING\", \"CANCELED\"}))\n\n# List all workflows.\nprint(workflow.list_all())\n\n# Resume all resumable workflows. This won't include failed workflow\nprint(workflow.resume_all())\n\n# To resume workflows including failed ones, use `include_failed=True`\nprint(workflow.resume_all(include_failed=True))\n```\n\n----------------------------------------\n\nTITLE: Using tune.with_resources for Resource Specification in Ray Tune\nDESCRIPTION: Example of using tune.with_resources to specify computational resources needed for a Ray Tune training job. This pattern allows users to allocate specific resources (like CPUs) to each trial based on workload requirements. For statsforecast, the n_jobs parameter should match the assigned CPUs per trial.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/start.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntune.with_resources\n```\n\n----------------------------------------\n\nTITLE: Setting Validation and Recovery Configurations in RLlib with Python\nDESCRIPTION: Adjusts configurations to disable initial validation of workers (`validate_env_runners_after_construction`) and configure how many iterations RLlib should wait (`num_consecutive_env_runner_failures_tolerance`) before concluding a total job failure. This is particularly useful for improving fault tolerance in flaky environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-fault-tolerance.rst#2025-04-12_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n\"validate_env_runners_after_construction = False\"\n```\n\nLANGUAGE: Python\nCODE:\n```\n\"num_consecutive_env_runner_failures_tolerance\"\n```\n\n----------------------------------------\n\nTITLE: Ray Tune Output Example for PyTorch Model Tuning\nDESCRIPTION: Shows sample output from a Ray Tune hyperparameter optimization run. The output displays trial results with various hyperparameter combinations, their performance metrics, and highlights the best performing configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-cifar.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n  Number of trials: 10 (10 TERMINATED)\n  +-------------------------+------------+-------+------+------+-------------+--------------+---------+------------+----------------------+\n  | Trial name              | status     | loc   |   l1 |   l2 |          lr |   batch_size |    loss |   accuracy |   training_iteration |\n  |-------------------------+------------+-------+------+------+-------------+--------------+---------+------------+----------------------|\n  | train_cifar_87d1f_00000 | TERMINATED |       |   64 |    4 | 0.00011629  |            2 | 1.87273 |     0.244  |                    2 |\n  | train_cifar_87d1f_00001 | TERMINATED |       |   32 |   64 | 0.000339763 |            8 | 1.23603 |     0.567  |                    8 |\n  | train_cifar_87d1f_00002 | TERMINATED |       |    8 |   16 | 0.00276249  |           16 | 1.1815  |     0.5836 |                   10 |\n  | train_cifar_87d1f_00003 | TERMINATED |       |    4 |   64 | 0.000648721 |            4 | 1.31131 |     0.5224 |                    8 |\n  | train_cifar_87d1f_00004 | TERMINATED |       |   32 |   16 | 0.000340753 |            8 | 1.26454 |     0.5444 |                    8 |\n  | train_cifar_87d1f_00005 | TERMINATED |       |    8 |    4 | 0.000699775 |            8 | 1.99594 |     0.1983 |                    2 |\n  | train_cifar_87d1f_00006 | TERMINATED |       |  256 |    8 | 0.0839654   |           16 | 2.3119  |     0.0993 |                    1 |\n  | train_cifar_87d1f_00007 | TERMINATED |       |   16 |  128 | 0.0758154   |           16 | 2.33575 |     0.1327 |                    1 |\n  | train_cifar_87d1f_00008 | TERMINATED |       |   16 |    8 | 0.0763312   |           16 | 2.31129 |     0.1042 |                    4 |\n  | train_cifar_87d1f_00009 | TERMINATED |       |  128 |   16 | 0.000124903 |            4 | 2.26917 |     0.1945 |                    1 |\n  +-------------------------+------------+-------+------+------+-------------+--------------+---------+------------+----------------------+\n\n\n  Best trial config: {'l1': 8, 'l2': 16, 'lr': 0.0027624906698231976, 'batch_size': 16, 'data_dir': '...'}\n  Best trial final validation loss: 1.1815014744281769\n  Best trial final validation accuracy: 0.5836\n  Best trial test set accuracy: 0.5806\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Tune Hyperparameter Search\nDESCRIPTION: Example of setting up and executing a basic Ray Tune hyperparameter search with uniform sampling. Creates a tuner object with a specified search space and configuration to evaluate a trainable function multiple times in parallel.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-lifecycle.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nspace = {\"x\": tune.uniform(0, 1)}\ntuner = tune.Tuner(\n    my_trainable,\n    param_space=space,\n    tune_config=tune.TuneConfig(num_samples=10),\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Loading GLUE Dataset with Hugging Face Datasets\nDESCRIPTION: This code uses the Hugging Face Datasets library to load the specified GLUE task dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nactual_task = \"mnli\" if task == \"mnli-mm\" else task\ndatasets = load_dataset(\"glue\", actual_task)\n```\n\n----------------------------------------\n\nTITLE: Defining Search Space for Hyperparameter Optimization\nDESCRIPTION: Configures the search space for the 6-dimensional optimization problem. Each dimension (x1 to x6) is defined as a uniform distribution between 0 and 1.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsearch_space = {\n    \"iterations\":100,\n    \"x1\": tune.uniform(0.0, 1.0),\n    \"x2\": tune.uniform(0.0, 1.0),\n    \"x3\": tune.uniform(0.0, 1.0),\n    \"x4\": tune.uniform(0.0, 1.0),\n    \"x5\": tune.uniform(0.0, 1.0),\n    \"x6\": tune.uniform(0.0, 1.0)\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Grid Search Comparison in Ray Tune\nDESCRIPTION: Sets up a traditional grid search experiment using the same initial hyperparameter configurations as PBT but without the PBT scheduler. This allows for direct comparison between the adaptive PBT approach and fixed grid search.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nif ray.is_initialized():\n    ray.shutdown()\nray.init()\n\ntuner = Tuner(\n    train_func,\n    param_space={\n        \"lr\": tune.qloguniform(1e-2, 1e-1, 5e-3),\n        \"h0\": tune.grid_search([0.0, 1.0]),\n        \"h1\": tune.sample_from(lambda spec: 1.0 - spec.config[\"h0\"]),\n    },\n    tune_config=tune.TuneConfig(\n        num_samples=1,\n        metric=\"Q\",\n        mode=\"max\",\n    ),\n    run_config=tune.RunConfig(\n        stop={\"training_iteration\": 100},\n        failure_config=tune.FailureConfig(max_failures=3),\n    ),\n)\n\ngrid_results = tuner.fit()\nif grid_results.errors:\n    raise RuntimeError\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Ray Serve Object Detection\nDESCRIPTION: Commands to install the required Python packages including Ray Serve, PyTorch, and other dependencies needed for the object detection service.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/object-detection.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[serve]\" requests torch pillow numpy opencv-python-headless pandas \"gitpython>=3.1.30\"\n```\n\n----------------------------------------\n\nTITLE: Building and Updating the Learner Actor in Ray - Python\nDESCRIPTION: This snippet builds the Learner actor and performs an update using a list of episodes fetched from the EnvRunners. It captures results from the update and prints the policy loss of the default policy. This is part of the training loop for reinforcement learning, ensuring that the actor learns from gathered experiences.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/key-concepts.rst#2025-04-12_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Build the Learner.\nray.get(learner_actor.build.remote())\n\n# Perform an update from the list of episodes we got from the `EnvRunners` above.\nlearner_results = ray.get(learner_actor.update.remote(\n    episodes=tree.flatten(episodes)\n))\nprint(learner_results[\"default_policy\"][\"policy_loss\"])\n```\n\n----------------------------------------\n\nTITLE: Loading Images for Annotations\nDESCRIPTION: Function to load images corresponding to annotations, adding them to the dataset as a new column.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_detection.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nfrom PIL import Image\n\ndef read_images(row: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    url = os.path.join(\"https://s3-us-west-2.amazonaws.com/air-example-data/AnimalDetection/JPEGImages\", row[\"filename\"])\n    response = requests.get(url)\n    image = Image.open(io.BytesIO(response.content))\n    row[\"image\"] = np.array(image)\n    return row\n\ndataset = annotations.map(read_images)\ndataset\n```\n\n----------------------------------------\n\nTITLE: Reading Image Dataset from S3 using Ray Data\nDESCRIPTION: Loads the Imagenette dataset from a public S3 bucket using Ray Data's read_images function. It disables progress bars and verbose logs for cleaner output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Disable progress bars and verbose logs\ncontext = ray.data.DataContext.get_current()\ncontext.enable_progress_bars = False\ncontext.verbose = False\n\ns3_uri = \"s3://anonymous@air-example-data-2/imagenette2/val/\"\n\nds = ray.data.read_images(\n    s3_uri, mode=\"RGB\"\n)\nds\n```\n\n----------------------------------------\n\nTITLE: Batch Size Configuration Example with Ray\nDESCRIPTION: Example demonstrating how to configure batch size for data processing with Ray. Shows assertion of batch size constraints in data processing pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/batch_inference.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nimport ray\n\nds = ray.data.from_numpy(np.ones((10, 100)))\n\ndef assert_batch(batch: Dict[str, np.ndarray]):\n    assert len(batch) == 2\n    return batch\n\n# Specify that each input batch should be of size 2.\nds.map_batches(assert_batch, batch_size=2)\n```\n\n----------------------------------------\n\nTITLE: Fetching Object Data in Ray (C++)\nDESCRIPTION: Shows how to retrieve object data from object references using ray::Get() in C++. It includes examples of getting single and multiple object references.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects.rst#2025-04-12_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// Get the value of one object ref.\nray::ObjectRef<int> obj_ref = ray::Put(1);\nassert(*obj_ref.Get() == 1);\n\n// Get the values of multiple object refs in parallel.\nstd::vector<ray::ObjectRef<int>> obj_refs;\nfor (int i = 0; i < 3; i++) {\n  obj_refs.emplace_back(ray::Put(i));\n}\nauto results = ray::Get(obj_refs);\nassert(results.size() == 3);\nassert(*results[0] == 0);\nassert(*results[1] == 1);\nassert(*results[2] == 2);\n```\n\n----------------------------------------\n\nTITLE: Implementing Auto-resume for Tune Experiments\nDESCRIPTION: Code example showing how to create a script that can either launch a new experiment or automatically restore an existing one using the Tuner.can_restore method, useful for production settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-fault-tolerance.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nexperiment_path = os.path.expanduser(\"~/ray_results/tune_fault_tolerance_guide\")\n\n# Define the trainable and param_space here...\nparam_space = {\"value\": tune.grid_search([1, 2, 3])}\n\nif tune.Tuner.can_restore(experiment_path):\n    # An experiment already exists\n    print(\"Restoring experiment...\")\n    tuner = tune.Tuner.restore(path=experiment_path, trainable=trainable)\nelse:\n    # Start from scratch\n    print(\"Starting new experiment...\")\n    tuner = tune.Tuner(\n        trainable,\n        param_space=param_space,\n        run_config=RunConfig(\n            name=\"tune_fault_tolerance_guide\",\n            storage_path=\"~/ray_results\",\n        ),\n    )\n\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Core Exceptions in Python\nDESCRIPTION: This code snippet demonstrates how to import various exception classes from the Ray Core library. These exceptions cover a wide range of error scenarios in distributed computing tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/exceptions.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ray.exceptions import RayError, RayTaskError, RayActorError, TaskCancelledError, TaskUnschedulableError, ActorDiedError, ActorUnschedulableError, ActorUnavailableError, AsyncioActorExit, LocalRayletDiedError, WorkerCrashedError, TaskPlacementGroupRemoved, ActorPlacementGroupRemoved, ObjectStoreFullError, OutOfDiskError, OutOfMemoryError, ObjectLostError, ObjectFetchTimedOutError, GetTimeoutError, OwnerDiedError, PendingCallsLimitExceeded, PlasmaObjectNotAvailable, ObjectReconstructionFailedError, ObjectReconstructionFailedMaxAttemptsExceededError, ObjectReconstructionFailedLineageEvictedError, RayChannelError, RayChannelTimeoutError, RayCgraphCapacityExceeded, RuntimeEnvSetupError, CrossLanguageError, RaySystemError, NodeDiedError\n```\n\n----------------------------------------\n\nTITLE: Configuring Scaling for Ray Train\nDESCRIPTION: This snippet demonstrates how to create ScalingConfig objects for different training scenarios. The configuration specifies the number of workers and whether each worker should use a GPU or CPU for computation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/overview.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\n\n# Single worker with a CPU\nscaling_config = ScalingConfig(num_workers=1, use_gpu=False)\n\n# Single worker with a GPU\nscaling_config = ScalingConfig(num_workers=1, use_gpu=True)\n\n# Multiple workers, each with a GPU\nscaling_config = ScalingConfig(num_workers=4, use_gpu=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring a PPO Algorithm with Environment Settings\nDESCRIPTION: Creates a configuration instance for PPO (Proximal Policy Optimization) algorithm and specifies the Pendulum-v1 environment to be used for training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\n# Create a config instance for the PPO algorithm.\nconfig = (\n    PPOConfig()\n    .environment(\"Pendulum-v1\")\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Resources in Ray Serve Deployment\nDESCRIPTION: Example demonstrating how to specify custom resource requirements in a Ray Serve deployment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/resource-allocation.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(ray_actor_options={\"resources\": {\"custom_resource\": 2}})\ndef func(*args):\n    return do_something_with_my_custom_resource()\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset from Dask DataFrame - Ray\nDESCRIPTION: This snippet illustrates creating a dataset from a Dask DataFrame using the `ray.data.from_dask` function. It showcases the conversion of a distributed Pandas DataFrame into a Ray dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nimport dask.dataframe as dd\nimport pandas as pd\nimport ray\n\n\ndf = pd.DataFrame({\"col1\": list(range(10000)), \"col2\": list(map(str, range(10000)))})\nddf = dd.from_pandas(df, npartitions=4)\n# Create a Dataset from a Dask DataFrame.\nds = ray.data.from_dask(ddf)\n\nds.show(3)\n```\n\n----------------------------------------\n\nTITLE: Building a Learner with PPOConfig - Python\nDESCRIPTION: This snippet demonstrates how to build a learner using the PPOConfig object. It sets hyperparameters for training and prepares the learner for operation within the specified environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-learner.rst#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nenv = gym.make(\"CartPole-v1\")\n\n# Create an AlgorithmConfig object from which we can build the\n# Learner.\nconfig = (\n    PPOConfig()\n    # Specify the Learner's hyperparameters.\n    .training(\n        use_kl_loss=True,\n        kl_coeff=0.01,\n        kl_target=0.05,\n        clip_param=0.2,\n        vf_clip_param=0.2,\n        entropy_coeff=0.05,\n        vf_loss_coeff=0.5\n    )\n)\n# Construct a new Learner using our config object.\nlearner = config.build_learner(env=env)\n\n# Needs to be called on the learner before calling any functions.\nlearner.build()\n```\n\n----------------------------------------\n\nTITLE: Submitting a Job using Ray Jobs REST API in Python\nDESCRIPTION: This snippet demonstrates how to submit a job to the Ray Jobs REST API using Python's requests library. It sends a POST request with job details and retrieves the job ID from the response.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/rest.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\nimport time\n\nresp = requests.post(\n    \"http://127.0.0.1:8265/api/jobs/\", # Don't forget the trailing slash!\n    json={\n        \"entrypoint\": \"echo hello\",\n        \"runtime_env\": {},\n        \"job_id\": None,\n        \"metadata\": {\"job_submission_id\": \"123\"}\n    }\n)\nrst = json.loads(resp.text)\njob_id = rst[\"job_id\"]\nprint(job_id)\n```\n\n----------------------------------------\n\nTITLE: Defining Possible and Active Agents in MultiAgentEnv\nDESCRIPTION: Demonstrates how to define possible and active agent IDs in a multi-agent environment, including techniques for managing dynamic agent populations\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, config=None):\n    super().__init__()\n    ...\n    # Define all agent IDs that might even show up in your episodes.\n    self.possible_agents = [\"agent_1\", \"agent_2\"]\n    \n    # If your agents never change throughout the episode, set\n    # `self.agents` to the same list as `self.possible_agents`.\n    self.agents = self.possible_agents = [\"agent_1\", \"agent_2\"]\n```\n\n----------------------------------------\n\nTITLE: Create and Retrieve Global Named Placement Group in C++\nDESCRIPTION: This C++ snippet demonstrates how to create a globally named placement group and retrieve it later. It leverages the `ray::CreatePlacementGroup` and `ray::GetGlobalPlacementGroup` functions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_16\n\nLANGUAGE: c++\nCODE:\n```\n\"          // Create a placement group with a globally unique name.\n          std::vector<std::unordered_map<std::string, double>> bundles{{{\"CPU\", 1.0}}};\n\n          ray::PlacementGroupCreationOptions options{\n              true/*global*/, \"global_name\", bundles, ray::PlacementStrategy::STRICT_SPREAD};\n\n          ray::PlacementGroup pg = ray::CreatePlacementGroup(options);\n          pg.Wait(60);\n\n          ...\n\n          // Retrieve the placement group later somewhere.\n          ray::PlacementGroup group = ray::GetGlobalPlacementGroup(\"global_name\");\n          assert(!group.Empty());\"\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving RLModule Checkpoints\nDESCRIPTION: This code demonstrates how to create an RLModule instance and save it to a checkpoint file. It shows the process of initializing a default PPO RLModule with a custom model configuration and saving it to a temporary directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\n\nimport gymnasium as gym\n\nfrom ray.rllib.algorithms.ppo.torch.default_ppo_torch_rl_module import DefaultPPOTorchRLModule\nfrom ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n\nenv = gym.make(\"CartPole-v1\")\n\n# Create an RLModule to later checkpoint.\nrl_module = DefaultPPOTorchRLModule(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    model_config=DefaultModelConfig(fcnet_hiddens=[32]),\n)\n\n# Finally, write the RLModule checkpoint.\nmodule_ckpt_path = tempfile.mkdtemp()\nrl_module.save_to_path(module_ckpt_path)\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Triton Server Integration Implementation\nDESCRIPTION: FastAPI-based Ray Serve deployment that initializes Triton Server and handles image generation requests. Includes GPU resource allocation and model loading logic.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/triton-server-integration.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nimport requests\nimport tritonserver\nfrom fastapi import FastAPI\nfrom PIL import Image\nfrom ray import serve\n\n\napp = FastAPI()\n\n@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n@serve.ingress(app)\nclass TritonDeployment:\n    def __init__(self):\n        self._triton_server = tritonserver\n\n        model_repository = [\"/workspace/models\"]\n\n        self._triton_server = tritonserver.Server(\n            model_repository=model_repository,\n            model_control_mode=tritonserver.ModelControlMode.EXPLICIT,\n            log_info=False,\n        )\n        self._triton_server.start(wait_until_ready=True)\n\n    @app.get(\"/generate\")\n    def generate(self, prompt: str, filename: str = \"generated_image.jpg\") -> None:\n        if not self._triton_server.model(\"stable_diffusion\").ready():\n            try:\n                self._triton_server.load(\"text_encoder\")\n                self._triton_server.load(\"vae\")\n                self._stable_diffusion = self._triton_server.load(\"stable_diffusion\")\n                if not self._stable_diffusion.ready():\n                    raise Exception(\"Model not ready\")\n            except Exception as error:\n                print(f\"Error can't load stable diffusion model, {error}\")\n                return\n\n        for response in self._stable_diffusion.infer(inputs={\"prompt\": [[prompt]]}):\n            generated_image = (\n                numpy.from_dlpack(response.outputs[\"generated_image\"])\n                .squeeze()\n                .astype(numpy.uint8)\n            )\n\n            image_ = Image.fromarray(generated_image)\n            image_.save(filename)\n\n\nif __name__ == \"__main__\":\n    # Deploy the deployment.\n    serve.run(TritonDeployment.bind())\n\n    # Query the deployment.\n    requests.get(\n        \"http://localhost:8000/generate\",\n        params={\"prompt\": \"dogs in new york, realistic, 4k, photograph\"},\n    )\n```\n\n----------------------------------------\n\nTITLE: Ray Core Time Series AutoML Implementation\nDESCRIPTION: Explanation of the recommended approach for AutoML implementation, suggesting the use of Ray Tune for hyperparameter tuning and AutoML tasks. The code demonstrates time series cross-validation strategy for model evaluation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/automl_for_time_series.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nWe strongly recommend using [Ray Tune] for hyperparameter tuning/AutoML, which will enable you to build it faster and more easily, and get the built-in benefits like logging, fault tolerance and many more. If you think your use case cannot be supported by Ray Tune, we'd love to get your feedback e.g. through a [Ray GitHub issue]\n```\n\n----------------------------------------\n\nTITLE: Performing Batch Inference with Trained LightGBM Model\nDESCRIPTION: Defines utility classes and functions to perform batch inference using the trained LightGBM model, including preprocessing steps.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightgbm/lightgbm_example.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom ray.train import Checkpoint\n\n\nclass Predict:\n\n    def __init__(self, checkpoint: Checkpoint):\n        self.model = LightGBMTrainer.get_model(checkpoint)\n        self.scaler = Preprocessor.deserialize(checkpoint.get_metadata()[\"scaler_pkl\"])\n        self.categorizer = Preprocessor.deserialize(checkpoint.get_metadata()[\"categorizer_pkl\"])\n\n    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n        preprocessed_batch = self.categorizer.transform_batch(self.scaler.transform_batch(batch))\n        return {\"predictions\": self.model.predict(preprocessed_batch)}\n\n\ndef predict_lightgbm(result: Result):\n    _, _, test_dataset = prepare_data()\n\n    scores = test_dataset.map_batches(\n        Predict, \n        fn_constructor_args=[result.checkpoint], \n        concurrency=1, \n        batch_format=\"pandas\"\n    )\n    \n    predicted_labels = scores.map_batches(lambda df: (df > 0.5).astype(int), batch_format=\"pandas\")\n    print(f\"PREDICTED LABELS\")\n    predicted_labels.show()\n```\n\n----------------------------------------\n\nTITLE: Using PBT Replay to Recreate Hyperparameter Schedules\nDESCRIPTION: This snippet shows how to use the Population Based Training Replay utility to recreate hyperparameter schedules from previous PBT runs. It configures the replay scheduler with an experiment directory and specific trial ID to replicate the hyperparameter schedule.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.tune.schedulers import PopulationBasedTrainingReplay\n\nreplay = PopulationBasedTrainingReplay(\n    experiment_dir=\"~/ray_results/pbt_experiment/\",\n    trial_id=\"XXXXX_00001\"\n)\ntuner = tune.Tuner(\n    train_fn,\n    tune_config=tune.TuneConfig(scheduler=replay)\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: RayJob Head Node Configuration with GPU Resources in YAML\nDESCRIPTION: YAML snippet defining the head node configuration for the Ray cluster used in the batch inference job. It specifies the Ray ML GPU image, requests 4 NVIDIA T4 GPUs, 54 CPUs, and 54GiB of memory, and includes volume mounting for code samples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-batch-inference-example.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n        spec:\n          containers:\n            - name: ray-head\n              image: rayproject/ray-ml:2.6.3-gpu\n              resources:\n                limits:\n                  nvidia.com/gpu: \"4\"\n                  cpu: \"54\"\n                  memory: \"54Gi\"\n                requests:\n                  nvidia.com/gpu: \"4\"\n                  cpu: \"54\"\n                  memory: \"54Gi\"\n              volumeMounts:\n                - mountPath: /home/ray/samples\n                  name: code-sample\n          nodeSelector:\n            cloud.google.com/gke-accelerator: nvidia-tesla-t4 # This is the GPU type we used in the GPU node pool.\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch MNIST Dataset to Ray Dataset\nDESCRIPTION: This example shows how to convert a built-in PyTorch dataset (MNIST) to a Ray Dataset. It imports torchvision and ray, downloads the MNIST dataset, and converts it to a Ray Dataset using the from_torch() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-pytorch.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision\nimport ray\n\nmnist = torchvision.datasets.MNIST(root=\"/tmp/\", download=True)\nds = ray.data.from_torch(mnist)\n\n# The data for each item of the Torch dataset is under the \"item\" key.\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Handling Application Errors with Ray API in Python\nDESCRIPTION: This snippet demonstrates how to define a remote function that raises a ValueError and illustrates how the ray.get() API raises this error when the function is executed. It highlights basic error handling for remote tasks in Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-failures.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\n@ray.remote\ndef f():\n    raise ValueError(\"it's an application error\")\n\n# Raises a ValueError.\ntry:\n    ray.get(f.remote())\nexcept ValueError as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Implementing the TrialScheduler Interface\nDESCRIPTION: The TrialScheduler interface provides methods that must be implemented by custom schedulers, including choosing trials to run and handling trial results and completion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.schedulers import TrialScheduler\n\n# Required methods:\n# TrialScheduler.choose_trial_to_run\n# TrialScheduler.on_trial_result\n# TrialScheduler.on_trial_complete\n```\n\n----------------------------------------\n\nTITLE: Sending Request to ImageClassifier\nDESCRIPTION: This Python command sends a request to the deployed ImageClassifier service using a client script.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/mobilenet-rayservice.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython mobilenet_req.py\n```\n\n----------------------------------------\n\nTITLE: Applying Preprocessing to Dataset with Ray Data\nDESCRIPTION: Uses Ray Data's map operation to apply the preprocessing function to the entire dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nds = ds.map(preprocess_image)\n```\n\n----------------------------------------\n\nTITLE: Reading Compressed CSV Files with Ray Data in Python\nDESCRIPTION: Demonstrates reading compressed CSV files using Ray Data's read_csv function. The example specifies the compression type in the arrow_open_stream_args parameter.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\n    \"s3://anonymous@ray-example-data/iris.csv.gz\",\n    arrow_open_stream_args={\"compression\": \"gzip\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoint from Single Worker in PyTorch\nDESCRIPTION: Demonstrates how to save a checkpoint from a single worker in a PyTorch training loop using Ray Train. This is the standard approach for DDP training where each worker has a full model copy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nif train.get_context().get_world_rank() == 0:\n    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n            },\n            f\"{temp_checkpoint_dir}/checkpoint.pt\",\n        )\n        checkpoint = train.Checkpoint.from_directory(temp_checkpoint_dir)\n        train.report({\"loss\": loss}, checkpoint=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Health Check in Serve Application - Python\nDESCRIPTION: This snippet describes how to implement a custom health-check method within a Serve deployment class to monitor the health of replicas, raising exceptions for unhealthy replicas. The health-check method should take no arguments and be designed to signal failure when needed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# Example of the check_health method\nclass MyDeployment:\n    def check_health(self):\n        # Perform health check logic\n        if condition_not_met:\n            raise Exception(\"Replica is unhealthy\")\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using GPUs in PyTorch Training Function with Ray Train\nDESCRIPTION: Demonstrates how to verify GPU availability and get the device in a PyTorch training function when using Ray Train with GPUs enabled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom ray.train import ScalingConfig\nfrom ray.train.torch import TorchTrainer, get_device\n\n\ndef train_func():\n    assert torch.cuda.is_available()\n\n    device = get_device()\n    assert device == torch.device(\"cuda:0\")\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(\n        num_workers=1,\n        use_gpu=True\n    )\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Dynamic Resource Allocation with Ray Remote Options - Python\nDESCRIPTION: Demonstrates how to dynamically adjust CPU and custom resource requirements for Ray actors and tasks using the .options() modifier. Shows creation of actors with varying resource needs and task execution with GPU requirements.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/miscellaneous.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(num_cpus=4)\nclass Counter(object):\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\na1 = Counter.options(num_cpus=1, resources={\"Custom1\": 1}).remote()\na2 = Counter.options(num_cpus=2, resources={\"Custom2\": 1}).remote()\na3 = Counter.options(num_cpus=3, resources={\"Custom3\": 1}).remote()\n```\n\n----------------------------------------\n\nTITLE: Ray Job Submission with Runtime Environment\nDESCRIPTION: Command line example for submitting a Ray job with runtime environment configuration using the Ray Jobs CLI.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nray job submit --address=\"http://<head-node-ip>:8265\" --runtime-env-json='{\"working_dir\": \"/data/my_files\", \"pip\": [\"emoji\"]}' -- python my_ray_script.py\n```\n\n----------------------------------------\n\nTITLE: Streaming Actor Logs with CLI\nDESCRIPTION: Shows how to stream logs from a specific actor using the Ray CLI. It uses the 'ray logs' command with the actor ID to continuously follow the log output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nray logs actor --id=<ACTOR_ID> --follow\n```\n\n----------------------------------------\n\nTITLE: Extending CLIReporter for Experiment Termination Reporting\nDESCRIPTION: Example of extending the CLIReporter class to customize reporting frequency, specifically to only report experiment progress when the experiment is done.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/reporters.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.experiment.trial import Trial\n\nclass ExperimentTerminationReporter(CLIReporter):\n    def should_report(self, trials, done=False):\n        \"\"\"Reports only on experiment termination.\"\"\"\n        return done\n\ntuner = tune.Tuner(my_trainable, run_config=ray.tune.RunConfig(progress_reporter=ExperimentTerminationReporter()))\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Reading NumPy Image Data\nDESCRIPTION: Shows how to load image data stored in NumPy format using ray.data.read_numpy().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-images.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_numpy(\"s3://anonymous@air-example-data/cifar-10/images.npy\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Replay Buffer Configuration Example\nDESCRIPTION: This code snippet demonstrates different ways to specify the type of replay buffer used in an RLlib configuration. It includes examples of directly setting the buffer type as a string, using a Python class, or accessing it through the `ReplayBuffer` catalog.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-replay-buffers.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python3\n\nimport gymnasium as gym\n\nimport ray\nfrom ray import air\nfrom ray import tune\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.examples.env.random_env import RandomEnv\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.utils.replay_buffers import ReplayBuffer\n\n\ndef main():\n    ray.init(ignore_reinit_error=True, local_mode=True)\n\n    # [1] Specify replay buffer type as string.\n    config = (\n        PPOConfig()\n        .environment(RandomEnv, config={\"observation_space\": gym.spaces.Box(-1.0, 1.0, shape=(10,)), \"action_space\": gym.spaces.Discrete(2)})\n        .rollouts(num_rollout_workers=0)\n        .training(train_batch_size=64)\n        .replay_buffer_config(type=\"MultiAgentReplayBuffer\")\n        .framework(\"torch\")\n    )\n    stop = {\n        \"training_iteration\": 1,\n    }\n    tune.Tuner(\n        \"PPO\",\n        param_space=config.to_dict(),\n        run_config=air.RunConfig(\n            stop=stop,\n            verbose=1,\n        ),\n    ).fit()\n\n    # [2] Specify replay buffer type as python class.\n    config = (\n        PPOConfig()\n        .environment(RandomEnv, config={\"observation_space\": gym.spaces.Box(-1.0, 1.0, shape=(10,)), \"action_space\": gym.spaces.Discrete(2)})\n        .rollouts(num_rollout_workers=0)\n        .training(train_batch_size=64)\n        .replay_buffer_config(type=ReplayBuffer)\n        .framework(\"torch\")\n    )\n    stop = {\n        \"training_iteration\": 1,\n    }\n    tune.Tuner(\n        \"PPO\",\n        param_space=config.to_dict(),\n        run_config=air.RunConfig(\n            stop=stop,\n            verbose=1,\n        ),\n    ).fit()\n\n    # [3] Specify replay buffer type through catalogue.\n    config = (\n        PPOConfig()\n        .environment(RandomEnv, config={\"observation_space\": gym.spaces.Box(-1.0, 1.0, shape=(10,)), \"action_space\": gym.spaces.Discrete(2)})\n        .rollouts(num_rollout_workers=0)\n        .training(train_batch_size=64)\n        .replay_buffer_config(type=ModelCatalog.get_preprocessor)\n        .framework(\"torch\")\n    )\n    stop = {\n        \"training_iteration\": 1,\n    }\n    tune.Tuner(\n        \"PPO\",\n        param_space=config.to_dict(),\n        run_config=air.RunConfig(\n            stop=stop,\n            verbose=1,\n        ),\n    ).fit()\n\n    ray.shutdown()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Redirecting Ray Tune Trainable Logs to Files\nDESCRIPTION: Shows how to redirect stdout and stderr from Tune Trainables to files by setting log_to_file=True in RunConfig. This captures logs from print statements, warnings.warn and logger.info.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    trainable,\n    run_config=RunConfig(log_to_file=True)\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Cluster with Runtime Environment\nDESCRIPTION: Sets up the Ray cluster and installs required Python packages on each node using a runtime environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(\n    runtime_env={\n        \"pip\": [\n            \"datasets\",\n            \"evaluate\",\n            \"transformers>=4.26.0\",\n            \"torch>=1.12.0\",\n            \"lightning>=2.0\",\n        ]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray ScalingConfig for Multi-GPU Training\nDESCRIPTION: Sets up a ScalingConfig for 16 workers using GPUs across multiple nodes. This configuration is suitable for a cluster with 4 nodes, each having 8 CPUs and 4 GPUs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nscaling_config = ScalingConfig(\n    num_workers=16,\n    use_gpu=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Storage (AWS S3, Google Cloud Storage) in Ray Train\nDESCRIPTION: Demonstrates how to configure cloud storage for Ray Train using a bucket URI with TorchTrainer and RunConfig\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/persistent-storage.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ray import train\nfrom ray.train.torch import TorchTrainer\n\ntrainer = TorchTrainer(\n    ...,\n    run_config=train.RunConfig(\n        storage_path=\"s3://bucket-name/sub-path/\",\n        name=\"experiment_name\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Tuning with Ray Tune and Horovod in Python\nDESCRIPTION: This snippet implements the 'tune_horovod' function for configuring and running hyperparameter tuning using HorovodTrainer, alongside Ray Tune's Tuner for searching optimal training parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/horovod_simple.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef tune_horovod(num_workers, num_samples, use_gpu, mode=\"square\", x_max=1.0):\n    horovod_trainer = HorovodTrainer(\n        train_loop_per_worker=train_loop_per_worker,\n        scaling_config=ScalingConfig(\n            trainer_resources={\"CPU\": 0}, num_workers=num_workers, use_gpu=use_gpu\n        ),\n        train_loop_config={\"mode\": mode, \"x_max\": x_max},\n    )\n\n    tuner = Tuner(\n        horovod_trainer,\n        param_space={\"train_loop_config\": {\"lr\": tune.uniform(0.1, 1)}},\n        tune_config=TuneConfig(mode=\"min\", metric=\"loss\", num_samples=num_samples),\n    )\n\n    result_grid = tuner.fit()\n\n    print(\"Best hyperparameters found were: \", result_grid.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Limiting Pending Tasks Using ray.wait() in Ray\nDESCRIPTION: This snippet shows how to use ray.wait() to limit the number of pending tasks. It maintains a list of in-flight tasks and uses ray.wait() to apply backpressure, preventing the accumulation of too many pending tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/limit-pending-tasks.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\n\n@ray.remote\ndef long_running_function():\n    # This is a long-running function\n    pass\n\nMAX_RUNNING_TASKS = 1000\nin_flight_tasks = []\nwhile True:\n    while len(in_flight_tasks) >= MAX_RUNNING_TASKS:\n        finished_tasks, in_flight_tasks = ray.wait(in_flight_tasks, num_returns=1)\n    in_flight_tasks.append(long_running_function.remote())\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Distributed Training with Ray\nDESCRIPTION: Configures and initializes a TorchTrainer for distributed training of the object detection model using Ray, then starts the training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_detection.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\nfrom ray.train.torch import TorchTrainer\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_loop_per_worker,\n    train_loop_config={\n        \"batch_size\": 2,\n        \"lr\": 0.02,\n        \"epochs\": 1,  # You'd normally train for 26 epochs.\n        \"momentum\": 0.9,\n        \"weight_decay\": 1e-4,\n        \"lr_steps\": [16, 22],\n        \"lr_gamma\": 0.1,\n    },\n    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),\n    datasets={\"train\": train_dataset},\n)\nresults = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Launching HPU Training with Environment Configuration in Python\nDESCRIPTION: This code snippet configures the environment variables for HPU training and initiates the training process. It sets HPU-specific environment variables based on the chosen execution mode (lazy/eager) and training method (DDP/DeepSpeed), then calls the train_llama function with the specified number of workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# set some environment variables\nos.environ[\"RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES\"] = \"0\"\n# if using RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES env var\n# you must set HABANA_VISIBLE_DEVICES, such as\n# os.environ[\"HABANA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n\n# execution_mode are [\"lazy\", \"eager\", \"eager.compile\"]\nexecution_mode = \"lazy\"\nos.environ[\"PT_HPU_LAZY_MODE\"] = \"1\" if execution_mode == \"lazy\" else \"0\"\n\n# training_method are [\"ddp\", \"deepspeed\"]\ntraining_method = \"deepspeed\"\nif training_method == \"deepspeed\":\n    os.environ[\"PT_HPU_MAX_COMPOUND_OP_SIZE\"] = \"10\"\n    os.environ[\"DEEPSPEED_HPU_ZERO3_SYNC_MARK_STEP_REQUIRED\"] = \"1\"\n\n# here use 4 HPUs\ntrain_llama(num_workers=4, execution_mode=execution_mode, training_method=training_method)\n```\n\n----------------------------------------\n\nTITLE: Configuring Nevergrad Search with Space\nDESCRIPTION: Configures the Nevergrad search algorithm using the Nevergrad-defined search space. Also sets the metric and mode for optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"algo = NevergradSearch(\\n    optimizer=ng.optimizers.OnePlusOne,\\n    space=space,\\n    metric=\\\"mean_loss\\\",\\n    mode=\\\"min\\\"\\n)\\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\"\n```\n\n----------------------------------------\n\nTITLE: Metadata Updates for Resumed Workflows in Python\nDESCRIPTION: Example demonstrating how the metadata stats are updated when a workflow is resumed. This shows how to intentionally fail a workflow, resume it after fixing the issue, and observe the metadata changes including updated start and end times.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/metadata.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nworkflow_id = \"simple\"\n\nerror_flag = Path(\"error\")\nerror_flag.touch()\n\n@ray.remote\ndef simple():\n    if error_flag.exists():\n        raise ValueError()\n    return 0\n\ntry:\n    workflow.run(simple.bind(), workflow_id=workflow_id)\nexcept ray.exceptions.RayTaskError:\n    pass\n\nworkflow_metadata_failed = workflow.get_metadata(workflow_id)\nassert workflow_metadata_failed[\"status\"] == \"FAILED\"\n\n# remove flag to make task success\nerror_flag.unlink()\nref = workflow.resume_async(workflow_id)\nassert ray.get(ref) == 0\n\nworkflow_metadata_resumed = workflow.get_metadata(workflow_id)\nassert workflow_metadata_resumed[\"status\"] == \"SUCCESSFUL\"\n\n# make sure resume updated running metrics\nassert workflow_metadata_resumed[\"stats\"][\"start_time\"] > workflow_metadata_failed[\"stats\"][\"start_time\"]\nassert workflow_metadata_resumed[\"stats\"][\"end_time\"] > workflow_metadata_failed[\"stats\"][\"end_time\"]\n```\n\n----------------------------------------\n\nTITLE: Basic Batch Handler Implementation - Python\nDESCRIPTION: Example showing the basic structure of a batch handler method using the @serve.batch decorator\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/batch.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@serve.batch\nasync def my_batch_handler(self, requests: List):\n    # Process multiple requests together\n    results = []\n    for request in requests:\n        results.append(request)  # processing logic here\n    return results\n```\n\n----------------------------------------\n\nTITLE: Curriculum Learning Environment Options\nDESCRIPTION: This code snippet defines environment options for curriculum learning in RLlib.  It sets up three tasks of different difficulties using the FrozenLake-v1 environment with different map configurations. This allows an agent to learn incrementally by navigating increasingly complex grid worlds.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-advanced-api.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example env options used in curriculum_learning.py.\n\n.. code-block:: python\n\n    \"env_config\": {\n        \"envs\": [{\n            \"map_name\": \"4x4\",\n            \"is_slippery\": False,\n            \"reward_done\": 20,\n        }, {\n            \"map_name\": \"8x8\",\n            \"is_slippery\": False,\n            \"reward_done\": 20,\n        }, {\n            \"map_name\": \"15x15\",\n            \"is_slippery\": True,\n            \"reward_done\": 20,\n        }],\n        \"env_task_fn\": def env_task_fn(env, env_context):\n            return env_context.config.worker_index % env_context.config.num_workers,\n        \"num_workers\": 3,\n    }\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Action Masking in RLModule\nDESCRIPTION: Implements an RLModule with action masking, allowing the restriction of certain actions based on observation data, beneficial for environments with conditional action availability.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Action masking\n# This script implements action masking functionality in an RLModule.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Training Function for Ray Tune\nDESCRIPTION: A sample training function used for demonstration in the stopping criteria examples. It simulates a training process with random accuracy improvements.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-stopping.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef train_function(config):\n    for i in range(20):\n        mean_accuracy = min(1.0, config[\"base_accuracy\"] + (0.1 * i))\n        accuracy = max(0.0, random.gauss(mean_accuracy, 0.05))\n        session.report({\"mean_accuracy\": mean_accuracy, \"accuracy\": accuracy})\n        time.sleep(0.5)\n```\n\n----------------------------------------\n\nTITLE: Logging in Custom Algorithm Training Step with Python\nDESCRIPTION: Illustrates logging metrics during a custom algorithm's training step using the Algorithm class in RLlib. The snippet shows metric logging using self.metrics, including log value and timing specific code execution blocks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n@override(Algorithm)\\ndef training_step(self) -> None:\\n    ...\\n\\n    # Log some value.\\n    self.metrics.log_value(\\\"some_mean_result\\\", 1.5, window=5)\\n\\n    ...\\n\\n    with self.metrics.log_time((\\\"timers\\\", \\\"some_code\\\")):\\n        ... # time some code\n```\n\n----------------------------------------\n\nTITLE: Printing Optimal Hyperparameters for Conditional Search in Python\nDESCRIPTION: This snippet prints the best hyperparameters found after running the experiments with a conditional search space, evaluated based on the defined objectives.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Best hyperparameters for loss found were: \", results.get_best_result(\"mean_loss\", \"min\").config)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image for PyTorch Object Detection\nDESCRIPTION: Applies necessary preprocessing transforms to the input image for the object detection model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimg = transforms.Compose([transforms.PILToTensor()])(img)\npreprocess = weights.transforms()\nbatch = [preprocess(img)]\n```\n\n----------------------------------------\n\nTITLE: Converting Ray Dataset to Pandas DataFrame\nDESCRIPTION: Shows how to convert a Ray Dataset to a pandas DataFrame using to_pandas(). Note that the data must fit in memory on the head node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/saving-data.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\ndf = ds.to_pandas()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: LightLoad Deployment Configuration - Autoscaling (Attempt 2)\nDESCRIPTION: This YAML configuration defines a Ray Serve deployment named 'LightLoad' with autoscaling enabled for the second attempt. It sets the target number of ongoing requests to 1, minimum replicas to 0, initial replicas to 0, and a maximum replicas to 200. It also defines the upscale and downscale delays, as well as the upscaling and downscaling factors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/advanced-autoscaling.md#2025-04-12_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n\"- name: LightLoad\n  max_ongoing_requests: 3\n  autoscaling_config:\n    target_ongoing_requests: 1\n    min_replicas: 0\n    initial_replicas: 0\n    max_replicas: 200\n    upscale_delay_s: 3\n    downscale_delay_s: 60\n    upscaling_factor: 0.3\n    downscaling_factor: 0.3\n    metrics_interval_s: 2\n    look_back_period_s: 10\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Results on a Small Batch\nDESCRIPTION: Takes a small batch of predictions and displays the classified images along with their predicted labels for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprediction_batch = predictions.take_batch(5)\n\nfrom PIL import Image\nfrom IPython.display import display\n\n\nimg_count = 0\nfor image, prediction in zip(prediction_batch[\"image\"], prediction_batch[\"label\"]):\n    print(\"Label: \", prediction)\n    print(\"Image:\")\n    # Use Jupyter to display the image inline.\n    img = Image.fromarray(image)\n    display(img)\n    img_count += 1\nprint(f\"Successfully displayed {img_count} images.\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Installation of required Python packages for running the parameter server implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_parameter_server.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install torch torchvision filelock\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install required packages including Ray Serve, Gradio, PyTorch and transformers\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/gradio-integration.md#2025-04-12_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ pip install \"ray[serve]\" gradio==3.50.2 torch transformers\n```\n\n----------------------------------------\n\nTITLE: Installing Ray with Cluster Launcher Support\nDESCRIPTION: Command to install Ray with default components using pip package manager.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/vsphere.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U ray[default]\n```\n\n----------------------------------------\n\nTITLE: Running Inference Benchmarks for RLlib with Torch Compile\nDESCRIPTION: Bash command to run inference benchmarks for RLlib using Torch compile. Allows specifying batch size, dynamo backend, and mode.\nSOURCE: https://github.com/ray-project/ray/blob/master/rllib/benchmarks/torch_compile/README.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./run_all_inference_bms.sh -bs <batch_size> --backend <dynamo_backend> --mode <dynamo_mode>\n```\n\n----------------------------------------\n\nTITLE: Reading from a Custom Datasource and Writing to a Custom Datasink in Python\nDESCRIPTION: This snippet illustrates how to read from a custom datasource and write to a custom datasink in Ray Data. It includes the structure for instantiating and using custom classes for data handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\n# Read from a custom datasource.\nds = ray.data.read_datasource(YourCustomDatasource(), **read_args)\n\n# Write to a custom datasink.\nds.write_datasink(YourCustomDatasink())\n```\n\n----------------------------------------\n\nTITLE: Providing Initial Set of Hyperparameters for the Search in Python\nDESCRIPTION: This snippet defines an initial set of hyperparameters that are believed to be useful for starting the optimization process. These parameters can lend insights into the search.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninitial_params = [\n    {\"width\": 1, \"height\": 2, \"activation\": \"relu\"},\n    {\"width\": 4, \"height\": 2, \"activation\": \"relu\"},\n]\n```\n\n----------------------------------------\n\nTITLE: Accessing Dashboard URL in Python\nDESCRIPTION: Python code that initializes Ray and retrieves the dashboard URL from the context object. This allows programmatic access to the dashboard URL.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/getting-started.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\ncontext = ray.init()\nprint(context.dashboard_url)\n```\n\n----------------------------------------\n\nTITLE: Running the Tune Optimization Experiment\nDESCRIPTION: Configures and executes the Ray Tune experiment with the defined objective function, search algorithm, and parameters. The experiment aims to minimize the 'landscape' metric.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n        metric=\"landscape\",\n        mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    run_config=tune.RunConfig(\n        name=\"ax\",\n        stop={\"timesteps_total\": stop_timesteps}\n    ),\n    param_space=search_space,\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Lightning Model for dolly-v2-7b\nDESCRIPTION: Creates a LightningModule that encapsulates the dolly-v2-7b model for fine-tuning, including forward pass, training step, and optimizer configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport lightning.pytorch as pl\n\nclass DollyV2Model(pl.LightningModule):\n    def __init__(self, lr=2e-5, eps=1e-8):\n        super().__init__()\n        self.save_hyperparameters()\n        self.lr = lr\n        self.eps = eps\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n    def forward(self, batch):\n        outputs = self.model(\n            batch[\"input_ids\"], \n            attention_mask=batch[\"attention_mask\"], \n            labels=batch[\"labels\"]\n        )\n        return outputs.loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self.forward(batch)\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=True)\n        return loss\n\n    def configure_optimizers(self):\n        if self.global_rank == 0:\n            print(self.trainer.model)\n        return torch.optim.AdamW(self.trainer.model.parameters(), lr=self.lr, eps=self.eps)\n```\n\n----------------------------------------\n\nTITLE: Logging with W&B in PyTorch Lightning\nDESCRIPTION: This snippet provides an example of using Weights & Biases (W&B) logging in a PyTorch Lightning context with Ray Train's TorchTrainer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. dropdown:: W&B\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking/lightning_exp_tracking_model_dl.py\n        :language: python\n        :start-after: __model_dl_start__\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking/lightning_exp_tracking_wandb.py\n        :language: python\n        :start-after: __lightning_experiment_tracking_wandb_start__\n```\n\n----------------------------------------\n\nTITLE: Setting Concurrency in Async Actors with Ray\nDESCRIPTION: Demonstrates how to set the number of concurrent tasks running in an async actor using the max_concurrency flag.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/async_api.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\n@ray.remote\nclass AsyncActor:\n    async def run_task(self):\n        print(\"started\")\n        await asyncio.sleep(1) # Network, I/O task here\n        print(\"ended\")\n\nactor = AsyncActor.options(max_concurrency=2).remote()\n\n# Only 2 tasks will be running concurrently. Once 2 finish, the next 2 should run.\nray.get([actor.run_task.remote() for _ in range(8)])\n```\n\n----------------------------------------\n\nTITLE: Counting Algorithm Distribution in Best Models\nDESCRIPTION: Analyzes the distribution of algorithms among the best models by calculating the normalized value counts. This shows the proportion of XGBoost vs. Linear Regression models that performed best across different locations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfinal_df[[\"algorithm\"]].astype(\"str\").value_counts(normalize=True)\n\n# 0.67 XGB\n# 0.33 Linear Regression\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet Image Data\nDESCRIPTION: Shows how to load image data stored in Parquet format using ray.data.read_parquet().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-images.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_parquet(\"s3://anonymous@air-example-data/cifar-10/parquet\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Converting to Distributed Multi-Worker TensorFlow Training (Python)\nDESCRIPTION: This shows how to convert a single-worker TensorFlow training function to distributed using `MultiWorkerMirroredStrategy` within Ray Train.  Placeholders `__tf_distributed_begin__` and `__tf_distributed_end__` mark the code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/tf/tensorflow_quick_start.py\n:language: python\n:start-after: __tf_distributed_begin__\n:end-before: __tf_distributed_end__\n```\"\n```\n\n----------------------------------------\n\nTITLE: Setting Test Mode Flag\nDESCRIPTION: Simple boolean flag initialization for smoke testing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-xgboost.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSMOKE_TEST = False\n```\n\nLANGUAGE: python\nCODE:\n```\nSMOKE_TEST = True\n```\n\n----------------------------------------\n\nTITLE: Distributed Checkpointing with PyTorch\nDESCRIPTION: Demonstrates how to implement distributed checkpointing in PyTorch using Ray Train. Each worker saves and reports its own checkpoint shard, which is useful for model-parallel training strategies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef train_func():\n    model = MyModel()\n    model = train.torch.prepare_model(model)\n    optimizer = optim.SGD(model.parameters(), lr=0.001)\n\n    for epoch in range(num_epochs):\n        # ... training loop ...\n\n        if should_checkpoint:\n            with tempfile.TemporaryDirectory() as tmpdir:\n                rank = train.get_context().get_world_rank()\n                torch.save(\n                    {\n                        \"epoch\": epoch,\n                        \"model_state_dict\": model.state_dict(),\n                        \"optimizer_state_dict\": optimizer.state_dict(),\n                    },\n                    f\"{tmpdir}/checkpoint_shard_{rank}.pt\",\n                )\n                checkpoint = train.Checkpoint.from_directory(tmpdir)\n                train.report({\"loss\": loss}, checkpoint=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Executing a Ray Generator Task in Python\nDESCRIPTION: This snippet illustrates how to execute a Ray generator task and iterate over its results. It shows both synchronous and asynchronous approaches to consuming the generated values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-generator.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ngen = streaming_generator.remote()\n\n# Synchronous iteration\nfor obj_ref in gen:\n    print(ray.get(obj_ref))\n\n# Asynchronous iteration\nasync for obj_ref in gen:\n    print(await obj_ref)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Agent Training with Policy Transfer in RLlib\nDESCRIPTION: Sets up multi-agent training configuration with a callback to transfer weights from a single-agent policy ('default_policy') to a specific agent ('p1') in the multi-agent setup. Uses Ray Tune for experiment execution with a defined stopping condition.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/checkpoints.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmulti_rl_module_component_tree = \"learner_group/learner/rl_module\"\n\ndef _on_algo_init(algorithm, **kwargs):\n    algorithm.restore_from_path(\n        path=Path(checkpoint_dir) / multi_rl_module_component_tree / \"default_policy\",\n        component=multi_rl_module_component_tree + \"/p1\",\n    )\n\nmulti_agent_config.callbacks(on_algorithm_init=_on_algo_init)\n\nresults = tune.Tuner(\n    multi_agent_config.algo_class,\n    param_space=multi_agent_config,\n    run_config=tune.RunConfig(stop={\"num_env_steps_sampled_lifetime\": 8000})\n).fit()\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Custom Runtime Environment for LLM Fine-tuning\nDESCRIPTION: Sets up Ray with a custom runtime environment including specific versions of deep learning libraries needed for fine-tuning Vicuna-13B. It defines constants for the number of workers, batch size, and model name.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nNUM_WORKERS = 16\nBATCH_SIZE_PER_WORKER = 8\nMODEL_NAME = \"lmsys/vicuna-13b-v1.3\"\n\nray.init(\n    runtime_env={\n        \"pip\": [\n            \"datasets==2.13.1\",\n            \"torch>=1.13.0\",\n            \"deepspeed==0.12.3\",\n            \"accelerate==0.20.3\",\n            \"transformers==4.30.2\",\n            \"lightning==2.0.3\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Ray Train Scaling and Run Configuration\nDESCRIPTION: Configures Ray Train's scaling (workers, GPUs) and run settings (checkpointing) for distributed hyperparameter tuning with resource management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import RunConfig, ScalingConfig, CheckpointConfig\n\nscaling_config = ScalingConfig(\n    num_workers=3, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n)\n\nrun_config = RunConfig(\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=2,\n        checkpoint_score_attribute=\"ptl/val_accuracy\",\n        checkpoint_score_order=\"max\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Using Large Datasets with Tune in Python\nDESCRIPTION: Demonstrates how to use tune.with_parameters() to efficiently pass large objects to trainable functions in Tune experiments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom ray import tune\n\ndata = np.random.random(size=100000000)\n\ndef train(config, data=None):\n    pass\n\ntune.run(\n    tune.with_parameters(train, data=data),\n    config={\"param\": tune.grid_search([1, 2, 3])}\n)\n```\n\n----------------------------------------\n\nTITLE: Training MNIST Classifier without Tuning\nDESCRIPTION: This snippet demonstrates how to train the MNIST classifier using a fixed configuration without hyperparameter tuning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef train_mnist_no_tune():\n    config = {\"layer_1_size\": 128, \"layer_2_size\": 256, \"lr\": 1e-3, \"batch_size\": 64}\n    train_mnist(config)\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Deployment Logs\nDESCRIPTION: Example log output showing successful deployment of the Stable Diffusion model using Ray Serve, including initialization and server startup messages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/aws-neuron-core-inference-stable-diffusion.md#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n2024-02-07 17:53:28,299\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265 \n(ProxyActor pid=25282) INFO 2024-02-07 17:53:31,751 proxy 172.31.10.188 proxy.py:1128 - Proxy actor fd464602af1e456162edf6f901000000 starting on node 5a8e0c24b22976f1f7672cc54f13ace25af3664a51429d8e332c0679.\n[...truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Creating Local ObjectRef References in Python\nDESCRIPTION: This snippet demonstrates creating two types of local ObjectRef references in Ray: one using ray.put() and another from a remote function call.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/memory-management.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\ndef f(arg):\n    return arg\n\na = ray.put(None)\nb = f.remote(None)\n```\n\n----------------------------------------\n\nTITLE: Training a PyTorch Model with Spark DataFrame using RayDP\nDESCRIPTION: Demonstrates creating a Spark DataFrame, splitting it into training/testing sets, and training a PyTorch model using RayDP's TorchEstimator API for distributed model training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/raydp.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col\ndf = spark.range(1, 1000)\n# calculate z = x + 2y + 1000\ndf = df.withColumn(\"x\", col(\"id\")*2)\\\n  .withColumn(\"y\", col(\"id\") + 200)\\\n  .withColumn(\"z\", col(\"x\") + 2*col(\"y\") + 1000)\n\nfrom raydp.utils import random_split\ntrain_df, test_df = random_split(df, [0.7, 0.3])\n\n# PyTorch Code \nimport torch\nclass LinearModel(torch.nn.Module):\n    def __init__(self):\n        super(LinearModel, self).__init__()\n        self.linear = torch.nn.Linear(2, 1)\n\n    def forward(self, x, y):\n        x = torch.cat([x, y], dim=1)\n        return self.linear(x)\n\nmodel = LinearModel()\noptimizer = torch.optim.Adam(model.parameters())\nloss_fn = torch.nn.MSELoss()\n\ndef lr_scheduler_creator(optimizer, config):\n    return torch.optim.lr_scheduler.MultiStepLR(\n      optimizer, milestones=[150, 250, 350], gamma=0.1)\n\n# You can use the RayDP Estimator API or libraries like Ray Train for distributed training.\nfrom raydp.torch import TorchEstimator\nestimator = TorchEstimator(\n  num_workers = 2,\n  model = model,\n  optimizer = optimizer,\n  loss = loss_fn,\n  lr_scheduler_creator=lr_scheduler_creator,\n  feature_columns = [\"x\", \"y\"],\n  label_column = [\"z\"],\n  batch_size = 1000,\n  num_epochs = 2\n)\n\nestimator.fit_on_spark(train_df, test_df)\n\npytorch_model = estimator.get_model()\n\nestimator.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Initializing RLlib MultiAgentEnv Base Implementation\nDESCRIPTION: Basic template for creating a multi-agent environment in RLlib, demonstrating the core method overrides for reset and step operations\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.env.multi_agent_env import MultiAgentEnv\n\nclass MyMultiAgentEnv(MultiAgentEnv):\n\n    def __init__(self, config=None):\n        super().__init__()\n        ...\n\n    def reset(self, *, seed=None, options=None):\n        ...\n        # return observation dict and infos dict.\n        return {\"agent_1\": [obs of agent_1], \"agent_2\": [obs of agent_2]}, {}\n\n    def step(self, action_dict):\n        # return observation dict, rewards dict, termination/truncation dicts, and infos dict\n        return {\"agent_1\": [obs of agent_1]}, {...}, ...\n```\n\n----------------------------------------\n\nTITLE: Using GPUs with TensorFlow in Ray Tasks\nDESCRIPTION: Shows how to create a Ray task that uses a GPU with TensorFlow. Ray automatically sets CUDA_VISIBLE_DEVICES environment variable which TensorFlow respects for GPU assignment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(num_gpus=1)\ndef gpu_task():\n    import tensorflow as tf\n\n    # Create a TensorFlow session. TensorFlow restricts itself to use the\n    # GPUs specified by the CUDA_VISIBLE_DEVICES environment variable.\n    tf.Session()\n```\n\n----------------------------------------\n\nTITLE: Utilizing Ray Actors in C++\nDESCRIPTION: This C++ snippet demonstrates how to create a Ray actor and execute tasks with that actor. It showcases the creation of an actor using the Ray C++ API and running tasks in loops. It prints out the counter value as output. Requires linking against the Ray C++ libraries and assumes a configured Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors.rst#2025-04-12_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nauto counter = ray::Actor(CreateCounter).Remote();\n\n// Start some tasks that use the actor.\nfor (int i = 0; i < 3; i++) {\n  ray::Task(Foo).Remote(counter);\n}\n\n// Print the counter value.\nfor (int i = 0; i < 10; i++) {\n  std::this_thread::sleep_for(std::chrono::seconds(1));\n  std::cout << *counter.Task(&Counter::GetCounter).Remote().Get() << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Binding the Translator Deployment\nDESCRIPTION: This snippet illustrates binding the Translator deployment, preparing it to be served. It showcases how to define a Ray Serve application, which allows for potential future scalability and multiple deployments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/getting_started.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\ndep = Translator.bind()\n```\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Evaluation Settings for an RLlib Algorithm\nDESCRIPTION: Sets up a separate evaluation process to run alongside training, specifying evaluation frequency, number of runners, and duration to assess the algorithm's performance on the environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconfig.evaluation(\n    # Run one evaluation round every iteration.\n    evaluation_interval=1,\n\n    # Create 2 eval EnvRunners in the extra EnvRunnerGroup.\n    evaluation_num_env_runners=2,\n\n    # Run evaluation for exactly 10 episodes. Note that because you have\n    # 2 EnvRunners, each one runs through 5 episodes.\n    evaluation_duration_unit=\"episodes\",\n    evaluation_duration=10,\n)\n\n# Rebuild the PPO, but with the extra evaluation EnvRunnerGroup\nppo_with_evaluation = config.build_algo()\n\nfor _ in range(3):\n    pprint(ppo_with_evaluation.train())\n```\n\n----------------------------------------\n\nTITLE: Creating a Failed Placement Group in Python\nDESCRIPTION: This snippet demonstrates creating a placement group that fails due to insufficient resources, showing how placement groups are atomically created.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# This placement group will be in PENDING state.\npg2 = ray.util.placement_group(\n    bundles=[{\"CPU\": 1}, {\"GPU\": 2}],\n    strategy=\"PACK\"\n)\n\n# Wait for pg2 to be created\nray.get(pg2.ready())\n```\n\n----------------------------------------\n\nTITLE: Configuring Vectorized Sub-Environments\nDESCRIPTION: This snippet configures the number of vectorized sub-environments per EnvRunner in RLlib. Requires gymnasium library for vectorization API support. The configuration `env_runners(num_envs_per_env_runner=p)` sets the number of sub-environments. Useful for batch processing of actions across multiple environments in parallel.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/scaling-guide.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\nconfig = (\n    PPOConfig()\n    # Use 10 sub-environments (vector) per EnvRunner.\n    .env_runners(num_envs_per_env_runner=10)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset from Python List - Ray\nDESCRIPTION: This snippet demonstrates how to create a Ray dataset from a list of Python integers using the `ray.data.from_items` function. It shows the basic usage of Ray for dataset creation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\n\nds = ray.data.from_items([1, 2, 3, 4, 5])\n\nprint(ds)\n```\n\n----------------------------------------\n\nTITLE: Building Compute Metrics Function\nDESCRIPTION: This function `build_compute_metrics_fn` constructs a function that computes metrics for a given task. It utilizes the `glue_compute_metrics` function from the `transformers` library to evaluate the model's performance based on predictions and labels.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_transformers.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef build_compute_metrics_fn(task_name: str) -> Callable[[EvalPrediction], Dict]:\n    \"\"\"Function from transformers/examples/text-classification/run_glue.py\"\"\"\n    output_mode = glue_output_modes[task_name]\n\n    def compute_metrics_fn(p: EvalPrediction):\n        if output_mode == \"classification\":\n            preds = np.argmax(p.predictions, axis=1)\n        elif output_mode == \"regression\":\n            preds = np.squeeze(p.predictions)\n        metrics = glue_compute_metrics(task_name, preds, p.label_ids)\n        return metrics\n\n    return compute_metrics_fn\n```\n\n----------------------------------------\n\nTITLE: Running Deduplication with Ray\nDESCRIPTION: Commands to execute distributed deduplication using Ray mode with the deduplication configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/data_juicer_distributed_data_processing.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# Run the tool from source\npython tools/process_data.py --config demos/process_on_ray/configs/dedup.yaml\n\n# Use the command-line tool\ndj-process --config demos/process_on_ray/configs/dedup.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Evaluation Duration in RLlib - Python\nDESCRIPTION: This code snippet demonstrates the setting of the evaluation duration in RLlib when using parallel evaluation settings. It is crucial to configure properly to ensure evaluation steps are as long as concurrent training steps.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-advanced-api.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Example configuration\n'evaluation_duration': 'auto',\n'evaluation_parallel_to_training': True\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Ray-based GBDT Training\nDESCRIPTION: Imports the necessary Python libraries for distributed training and inference with XGBoost on Ray. This includes Ray's data and train modules, as well as XGBoost and pandas for data manipulation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple\n\nimport pandas as pd\nimport xgboost\n\nimport ray\nfrom ray.data import Dataset, Preprocessor\nfrom ray.data.preprocessors import StandardScaler\nfrom ray.train import Checkpoint, CheckpointConfig, Result, RunConfig, ScalingConfig\nfrom ray.train.xgboost import XGBoostTrainer\n```\n\n----------------------------------------\n\nTITLE: Custom Metric in Ray Serve\nDESCRIPTION: Example of defining and using a custom metric in a Ray Serve deployment. It demonstrates how to tag metrics with deployment metadata.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.util.metrics import Counter\n\n@serve.deployment\nclass MyDeployment:\n    def __init__(self):\n        self.my_counter = Counter(\n            \"my_counter\",\n            description=\"The number of odd-numbered requests to this deployment.\",\n            tag_keys=(\"deployment\", \"model\"),\n        )\n        self.my_counter.set_default_tags({\"deployment\": \"MyDeployment\", \"model\": \"123\"})\n\n    def __call__(self, request):\n        if int(request) % 2 == 1:\n            self.my_counter.inc()\n        return request\n```\n\n----------------------------------------\n\nTITLE: Setting up Ray driver with code search path in Python\nDESCRIPTION: Demonstrates how to set the code search path for the Ray driver in Python, which is necessary for cross-language programming.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(\n    _temp_dir=\"/tmp/ray\",\n    _system_config={\"worker_code_search_path\": \"/path/to/jars:/path/to/pys\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Connecting Ray Worker Node to Head Node\nDESCRIPTION: Command to start a Ray worker node and connect it to a specified head node address and port.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/faq.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nray start --address xx.xx.xx.xx:nnnn\n```\n\n----------------------------------------\n\nTITLE: Neural Network Model Definition\nDESCRIPTION: Definition of a small ConvNet for MNIST classification with weight and gradient management methods.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_parameter_server.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ConvNet(nn.Module):\n    \"\"\"Small ConvNet for MNIST.\"\"\"\n\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n        self.fc = nn.Linear(192, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n        x = x.view(-1, 192)\n        x = self.fc(x)\n        return F.log_softmax(x, dim=1)\n\n    def get_weights(self):\n        return {k: v.cpu() for k, v in self.state_dict().items()}\n\n    def set_weights(self, weights):\n        self.load_state_dict(weights)\n\n    def get_gradients(self):\n        grads = []\n        for p in self.parameters():\n            grad = None if p.grad is None else p.grad.data.cpu().numpy()\n            grads.append(grad)\n        return grads\n\n    def set_gradients(self, gradients):\n        for g, p in zip(gradients, self.parameters()):\n            if g is not None:\n                p.grad = torch.from_numpy(g)\n```\n\n----------------------------------------\n\nTITLE: Using tune.with_parameters in Ray Tune for Large Constant Parameters\nDESCRIPTION: This snippet demonstrates how to use tune.with_parameters to pass large constant objects (like a DataFrame) into the training function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune_get_data_in_and_out.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef training_function(config, data):\n    model = {\n        \"hyperparameter_a\": config[\"hyperparameter_a\"],\n        \"hyperparameter_b\": config[\"hyperparameter_b\"],\n    }\n    epochs = config[\"epochs\"]\n\n    # Simulate training & evaluation - we obtain back a \"metric\" and a \"trained_model\".\n    for epoch in range(epochs):\n        # Simulate doing something expensive.\n        time.sleep(1)\n        metric = (0.1 + model[\"hyperparameter_a\"] * epoch / 100) ** (\n            -1\n        ) + model[\"hyperparameter_b\"] * 0.1 * data[\"A\"].sum()\n        trained_model = {\"state\": model, \"epoch\": epoch}\n\n\ndata = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\ntuner = Tuner(\n    tune.with_parameters(training_function, data=data),\n    param_space={\n        \"hyperparameter_a\": tune.uniform(0, 20),\n        \"hyperparameter_b\": tune.uniform(-100, 100),\n        \"epochs\": 10,\n    },\n    tune_config=tune.TuneConfig(num_samples=4),\n)\n```\n\n----------------------------------------\n\nTITLE: Experience Input Configuration API for RLlib Offline Learning\nDESCRIPTION: Comprehensive API for configuring the experience input for an agent in RLlib's offline learning, including options for specifying data sources, read methods, schemas, batch sizes, and filesystem options.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef offline_data(\n    self,\n    *,\n    # Specify how to generate experiences:\n    # - A local directory or file glob expression (for example \"/tmp/*.json\").\n    # - A cloud storage path or file glob expression (for example \"gs://rl/\").\n    # - A list of individual file paths/URIs (for example [\"/tmp/1.json\",\n    #   \"s3://bucket/2.json\"]).\n    # - A file or directory path in a given `input_filesystem`.\n    input_: Optional[Union[str, Callable[[IOContext], InputReader]]],\n    # Read method for the `ray.data.Dataset` to read in the\n    # offline data from `input_`. The default is `read_parquet` for Parquet\n    # files. See https://docs.ray.io/en/latest/data/api/input_output.html for\n    # more info about available read methods in `ray.data`.\n    input_read_method: Optional[Union[str, Callable]],\n    # Keyword args for `input_read_method`. These\n    # are passed into the read method without checking. Use these\n    # keyword args together with `map_batches_kwargs` and\n    # `iter_batches_kwargs` to tune the performance of the data pipeline. It\n    # is strongly recommended to rely on Ray Data's automatic read performance\n    # tuning\n    input_read_method_kwargs: Optional[Dict],\n    # Table schema for converting offline data to episodes.\n    # This schema maps the offline data columns to\n    # `ray.rllib.core.columns.Columns`:\n    # `{Columns.OBS: 'o_t', Columns.ACTIONS: 'a_t', ...}`. Columns in\n    # the data set that aren't mapped through this schema are sorted into\n    # episodes' `extra_model_outputs`. If no schema is passed in the default\n    # schema used is `ray.rllib.offline.offline_data.SCHEMA`. If your data set\n    # contains already the names in this schema, no `input_read_schema` is\n    # needed. The same applies, if the offline data is in RLlib's\n    # `EpisodeType` or old `SampleBatch` format\n    input_read_schema: Optional[Dict[str, str]],\n    # Whether offline data is already stored in RLlib's\n    # `EpisodeType` format, i.e. `ray.rllib.env.SingleAgentEpisode` (multi\n    # -agent is planned but not supported, yet). Reading episodes directly\n    # avoids additional transform steps and is usually faster and\n    # therefore the recommended format when your application remains fully\n    # inside of RLlib's schema. The other format is a columnar format and is\n    # agnostic to the RL framework used. Use the latter format, if you are\n    # unsure when to use the data or in which RL framework. The default is\n    # to read column data, i.e. `False`. `input_read_episodes` and\n    # `input_read_sample_batches` can't be `True` at the same time. See\n    # also `output_write_episodes` to define the output data format when\n    # recording.\n    input_read_episodes: Optional[bool],\n    # Whether offline data is stored in RLlib's old\n    # stack `SampleBatch` type. This is usually the case for older data\n    # recorded with RLlib in JSON line format. Reading in `SampleBatch`\n    # data needs extra transforms and might not concatenate episode chunks\n    # contained in different `SampleBatch`es in the data. If possible avoid\n    # to read `SampleBatch`es and convert them in a controlled form into\n    # RLlib's `EpisodeType` (i.e. `SingleAgentEpisode`). The default is\n    # `False`. `input_read_episodes` and `input_read_sample_batches` can't\n    # be True at the same time.\n    input_read_sample_batches: Optional[bool],\n    # Batch size to pull from the data set. This could\n    # differ from the `train_batch_size_per_learner`, if a dataset holds\n    # `EpisodeType` (i.e. `SingleAgentEpisode`) or `SampleBatch`, or any\n    # other data type that contains multiple timesteps in a single row of the\n    # dataset. In such cases a single batch of size\n    # `train_batch_size_per_learner` potentially pulls a multiple of\n    # `train_batch_size_per_learner` timesteps from the offline dataset. The\n    # default is `None` in which the `train_batch_size_per_learner` is pulled.\n    input_read_batch_size: Optional[int],\n    # A cloud filesystem to handle access to cloud storage when\n    # reading experiences. Can be \"gcs\" for Google Cloud Storage, \"s3\" for AWS\n    # S3 buckets, \"abs\" for Azure Blob Storage, or any filesystem supported\n    # by PyArrow. In general the file path is sufficient for accessing data\n    # from public or local storage systems. See\n    # https://arrow.apache.org/docs/python/filesystems.html for details.\n    input_filesystem: Optional[str],\n    # A dictionary holding the kwargs for the filesystem\n    # given by `input_filesystem`. See `gcsfs.GCSFilesystem` for GCS,\n    # `pyarrow.fs.S3FileSystem`, for S3, and `ablfs.AzureBlobFilesystem` for\n    # ABS filesystem arguments.\n    input_filesystem_kwargs: Optional[Dict],\n    # What input columns are compressed with LZ4 in the\n    # input data. If data is stored in RLlib's `SingleAgentEpisode` (\n    # `MultiAgentEpisode` not supported, yet). Note the providing\n    # `rllib.core.columns.Columns.OBS` also tries to decompress\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Batches with Local Shuffling in Ray Data\nDESCRIPTION: Shows how to iterate over batches with local shuffling using the local_shuffle_buffer_size parameter in Ray Data's iter_batches() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/iterating-over-data.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\nfor batch in ds.iter_batches(\n    batch_size=2,\n    batch_format=\"pandas\",\n    local_shuffle_buffer_size=250,\n):\n    print(batch)\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Crawling\nDESCRIPTION: Launching multiple parallel crawlers using Ray Tasks to crawl different sections of the Ray documentation simultaneously.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/web-crawler.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlinks = [find_links_task.remote(f\"{base}{lib}/index.html\", base)\n         for lib in [\"\", \"\", \"\", \"rllib\", \"tune\", \"serve\"]]\n```\n\n----------------------------------------\n\nTITLE: Customized Evaluation during Training\nDESCRIPTION: This code demonstrates how to configure customized evaluation during training in RLlib by setting the `evaluation_interval` parameter. This allows evaluating policies with different settings, such as exploration turned off, during the training process. The evaluation runs for a specified duration, measured in either episodes or timesteps.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-advanced-api.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example on how to setup evaluation for RLlib.\n\n.. code-block:: python\n\n    config = {\n        # ... other Algorithm config keys.\n\n        # Evaluate once per training iteration.\n        \"evaluation_interval\": 1,\n\n        # Run evaluation over 10 episodes.\n        \"evaluation_duration\": 10,\n\n        # Run evaluation over 30 timesteps.\n        \"evaluation_duration\": 30,\n\n        # OR: Run evaluation until 30 timesteps have been sampled.\n        # Note that this setting is not supported in single-agent mode.\n        \"evaluation_duration_unit\": \"timesteps\",\n    }\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Performing Compiled Graph Operations in Ray\nDESCRIPTION: Methods for compiling, executing, and visualizing compiled DAGs in Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/compiled-graph-api.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nray.dag.DAGNode.experimental_compile\nray.dag.compiled_dag_node.CompiledDAG.execute\nray.dag.compiled_dag_node.CompiledDAG.visualize\n```\n\n----------------------------------------\n\nTITLE: Reading NFS Files with Ray Data in Python\nDESCRIPTION: Illustrates reading files from NFS filesystems using Ray Data's read_parquet function. The example specifies a path on the mounted filesystem.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_parquet(\"/mnt/cluster_storage/iris.parquet\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Generating Memray Flamegraph\nDESCRIPTION: Command to generate a flamegraph visualization from the memory profiling data file\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-memory.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmemray flamegraph <memory profiling bin file>\n```\n\n----------------------------------------\n\nTITLE: Adjusting Batch Size Based on Number of Workers in Ray Train\nDESCRIPTION: Dynamically calculates the batch size per worker by dividing the total batch size by the number of workers in the distributed training setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    batch_size_per_worker = batch_size // train.get_context().get_world_size()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Kubernetes Cluster with GPU on GCP\nDESCRIPTION: Commands to create a Kubernetes cluster on GCP with a CPU node for the Ray head and a GPU node for workers. Includes setting up the necessary NVIDIA GPU drivers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/gpu-training-example.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Step 1: Set up a Kubernetes cluster on GCP.\n# e2-standard-8 => 8 vCPU; 32 GB RAM\ngcloud container clusters create gpu-cluster-1 \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-central1-c --machine-type e2-standard-8\n\n# Create a node-pool for GPU\n# n1-standard-8 => 8 vCPU; 30 GB RAM\ngcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-tesla-t4,count=1 \\\n  --zone us-central1-c --cluster gpu-cluster-1 \\\n  --num-nodes 1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n  --machine-type n1-standard-8\n\n# Install NVIDIA GPU device driver\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml\n```\n\n----------------------------------------\n\nTITLE: Custom Storage with pyarrow FileSystem\nDESCRIPTION: Configures custom storage using a pyarrow FileSystem implementation with S3-like storage\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/persistent-storage.rst#2025-04-12_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport pyarrow.fs\n\nfrom ray import train\nfrom ray.train.torch import TorchTrainer\n\nfs = pyarrow.fs.S3FileSystem(\n    endpoint_override=\"http://localhost:9000\",\n    access_key=...,\n    secret_key=...\n)\n\ntrainer = TorchTrainer(\n    ...,\n    run_config=train.RunConfig(\n        storage_filesystem=fs,\n        storage_path=\"bucket-name/sub-path\",\n        name=\"unique-run-id\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Using MetricsLogger in RLlib Callback for Average Angle Calculation\nDESCRIPTION: This snippet showcases a custom RLlibCallback implementation that calculates the average 'first-joint angle' in an Acrobot-v1 environment, leveraging MetricsLogger for logging the results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport numpy as np\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.callbacks.callbacks import RLlibCallback\n\n# Define a custom RLlibCallback.\nclass LogAcrobotAngle(RLlibCallback):\n    def on_episode_step(self, *, episode, env, **kwargs):\n        # Compute the angle at every episode step and store it temporarily in episode:\n        state = env.envs[0].unwrapped.state\n        deg_theta1 = math.degrees(math.atan2(state[1], state[0]))\n        episode.add_temporary_timestep_data(\"theta1\", deg_theta1)\n\n    def on_episode_end(self, *, episode, metrics_logger, **kwargs):\n        theta1s = episode.get_temporary_timestep_data(\"theta1\")\n        avg_theta1 = np.mean(theta1s)\n\n        # Log the resulting average angle - per episode - to the MetricsLogger.\n```\n\n----------------------------------------\n\nTITLE: Basic Ray Application Logging Example\nDESCRIPTION: Demonstrates how Ray redirects logs from Tasks and Actors to the driver process, showing the default behavior of log forwarding in Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\n\n@ray.remote\ndef task():\n    print(\"task\")\n\n@ray.remote\nclass Actor:\n    def method(self):\n        print(\"actor\")\n\nray.get(task.remote())\nactor = Actor.remote()\nray.get(actor.method.remote())\n```\n\n----------------------------------------\n\nTITLE: Ray remote task accessing working directory\nDESCRIPTION: This snippet defines a Ray remote task that reads a file from the specified working directory. The task 'f' opens and reads the 'hello.txt' file, demonstrating how Ray tasks can access files within the runtime environment's working directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create a Ray task, which inherits the above runtime env.\n@ray.remote\ndef f():\n    # The function will have its working directory changed to its node's\n    # local copy of /tmp/runtime_env_working_dir.\n    return open(\"hello.txt\").read()\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Up YAML for Redis Integration\nDESCRIPTION: YAML configuration for ray up command that includes Redis configuration for GCS fault tolerance. Modifies head_start_ray_commands to include Redis address and authentication.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/gcs.rst#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nhead_start_ray_commands:\n  - ray stop\n  - ulimit -n 65536; RAY_REDIS_ADDRESS=redis_ip:port ray start --head --redis-password PASSWORD --redis-username default --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host=0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: FastAPI Endpoint Implementation\nDESCRIPTION: Creates a FastAPI application with an endpoint for image generation requests.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/start.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\napp = FastAPI()\n\n\n@serve.deployment(num_replicas=1)\n@serve.ingress(app)\nclass APIIngress:\n    def __init__(self, diffusion_model_handle) -> None:\n        self.handle = diffusion_model_handle\n\n    @app.get(\n        \"/imagine\",\n        responses={200: {\"content\": {\"image/png\": {}}}},\n        response_class=Response,\n    )\n    async def generate(self, prompt: str, img_size: int = 776):\n        assert len(prompt), \"prompt parameter cannot be empty\"\n\n        image = await self.handle.generate.remote(prompt, img_size=img_size)\n\n        file_stream = BytesIO()\n        image.save(file_stream, \"PNG\")\n        return Response(content=file_stream.getvalue(), media_type=\"image/png\")\n```\n\n----------------------------------------\n\nTITLE: Returning Multiple Static Values from Ray Task\nDESCRIPTION: This example shows how to return multiple values from a Ray task when the number of returns is known beforehand. It uses the num_returns option to specify the number of return values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/return-ray-put.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(num_returns=2)\ndef task():\n    value1 = compute_value1()\n    value2 = compute_value2()\n    return value1, value2\n\n# Good: Get multiple values directly\nresult1, result2 = ray.get(task.remote())\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Resources for Tasks in Ray Workflow\nDESCRIPTION: This example shows how to assign resources such as CPUs and GPUs to workflow tasks using the same arguments as Ray tasks (num_cpus, num_gpus, and resources).\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef train_model():\n    pass  # This task is assigned to a GPU by Ray.\n\nworkflow.run(train_model.options(num_gpus=1).bind())\n```\n\n----------------------------------------\n\nTITLE: Deleting a Completed RayJob\nDESCRIPTION: Command to delete a RayJob custom resource after it has completed successfully. This step is important for cleaning up resources after job completion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete rayjob dev-pytorch-text-classifier-r6d4p\n```\n\n----------------------------------------\n\nTITLE: Configurating CNN in RLModule for Image-based Environments\nDESCRIPTION: Illustrates configuring a CNN for image-based RL environments such as Atari games. Utilizes RLlib's wrap_atari_for_new_api_stack utility for resizing and preprocessing, while defining a CNN stack with silu activations to process 64x64 images in RL environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym  # `pip install gymnasium[atari,accept-rom-license]`\n\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.env.wrappers.atari_wrappers import wrap_atari_for_new_api_stack\nfrom ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\nfrom ray.tune import register_env\n\nregister_env(\n    \"image_env\",\n    lambda _: wrap_atari_for_new_api_stack(\n        gym.make(\"ale_py:ALE/Pong-v5\"),\n        dim=64,  # resize original observation to 64x64x3\n        framestack=4,\n    )\n)\n\nconfig = (\n    PPOConfig()\n    .environment(\"image_env\")\n    .rl_module(\n        model_config=DefaultModelConfig(\n            conv_filters=[\n                [16, 4, 2],  # 1st CNN layer: num_filters, kernel, stride(, padding)?\n                [32, 4, 2],  # 2nd CNN layer\n                [64, 4, 2],  # etc..\n                [128, 4, 2],\n            ],\n            conv_activation=\"silu\",\n            head_fcnet_hiddens=[256],\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Batches with PyTorch in Ray Data\nDESCRIPTION: Shows how to iterate over batches of data in PyTorch tensor format using Ray Data's iter_torch_batches() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/iterating-over-data.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\n\nfor batch in ds.iter_torch_batches(batch_size=2):\n    print(batch)\n```\n\n----------------------------------------\n\nTITLE: Checking Serve Application Status using CLI\nDESCRIPTION: This snippet showcases how to check the status of a Serve application using the `serve status` command.  It begins by starting a Ray head node, deploying a Serve application using a YAML configuration file (`serve_config.yaml`), and then retrieving the application's status using `serve status`. The output displays the status of the proxies and the applications, including the deployment states and replica states.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"$ ray start --head\n$ serve deploy serve_config.yaml\n...\n\n$ serve status\nproxies:\n  cef533a072b0f03bf92a6b98cb4eb9153b7b7c7b7f15954feb2f38ec: HEALTHY\napplications:\n  default:\n    status: RUNNING\n    message: ''\n    last_deployed_time_s: 1694041157.2211847\n    deployments:\n      Translator:\n        status: HEALTHY\n        replica_states:\n          RUNNING: 1\n        message: ''\n      Summarizer:\n        status: HEALTHY\n        replica_states:\n          RUNNING: 1\n        message: ''\"\n```\n\n----------------------------------------\n\nTITLE: Using tune list-trials with filter flag\nDESCRIPTION: Example of using the tune list-trials command with the --filter flag to filter trials based on specific criteria. This example filters trials to show only the one with trial_id equal to 7b99a28a.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/cli.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ tune list-trials [EXPERIMENT_DIR] --filter \"trial_id == 7b99a28a\"\n\n+------------------+-----------------------+------------+\n| trainable_name   | experiment_tag        | trial_id   |\n|------------------+-----------------------+------------|\n| MyTrainableClass | 3_height=54,width=21  | 7b99a28a   |\n+------------------+-----------------------+------------+\nDropped columns: ['status', 'last_update_time']\nPlease increase your terminal size to view remaining columns.\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Training with Ray Tune\nDESCRIPTION: Initializes and runs the Ray Tune experiment, distributing the training function across the cluster for all dataset partitions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/start.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    trainable,\n    param_space=param_space,\n    run_config=RunConfig(storage_path=\"/mnt/cluster_storage\"),\n)\nresult_grid = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: End-to-End RLModule Example in RLlib\nDESCRIPTION: This example demonstrates how to integrate a custom RLModule into a complete workflow using the provided VPG torch RLModule example. It showcases the structure and necessary components for a functional RLlib setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example of a custom RLModule implementing a VPG (using PyTorch).\"\"\"\n\nimport logging\nfrom typing import Dict, Tuple\n\nimport gymnasium as gym\nimport numpy as np\n\nimport ray\nfrom ray.rllib.core.rl_module.rl_module import RLModule\nfrom ray.rllib.core.rl_module.rl_module import ModuleID\nfrom ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\nfrom ray.rllib.examples.models.simple_net import SimpleNet\nfrom ray.rllib.utils.annotations import override\nfrom ray.rllib.utils.framework import try_import_torch\nfrom ray.rllib.utils.torch_utils import FLOAT_MAX\nfrom ray.rllib.utils.typing import TensorType, ModuleConfigDict\n\ntorch, nn = try_import_torch()\n\nlogger = logging.getLogger(__name__)\n\n\nclass VPGRlModule(RLModule):\n    def __init__(self, config: ModuleConfigDict, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n\n        # Module IDs to use.\n        self.encoder_id = \"encoder\"\n        self.policy_id = \"policy\"\n        self.value_id = \"value\"\n\n        # The encoder is shared amongst the policy and value functions.\n        self.register_module(self.encoder_id, SimpleNet(config[\"encoder_config\"]))\n        # Policy head (produces action logits).\n        self.register_module(self.policy_id, SimpleNet(config[\"policy_config\"]))\n        # Value head (predicts the return (single float value)).\n        self.register_module(self.value_id, SimpleNet(config[\"value_config\"]))\n\n    @override(RLModule)\n    def forward_train(self, batch: Dict[str, TensorType], **kwargs) -> Dict[str, TensorType]:\n        # Pass the observations through the shared encoder.\n        encoder_out = self.forward_encoder(batch)[self.encoder_id]\n\n        # Calculate action logits.\n        policy_out = self.forward_policy(encoder_out)[self.policy_id]\n\n        # Calculate value function output.\n        value_out = self.forward_value(encoder_out)[self.value_id]\n\n        # Return the policy outputs, value function outputs, and the new state.\n        return {\n            \"logits\": policy_out,\n            \"vf_preds\": value_out,\n            # This is important to calculate the entropy loss correctly, which requires the\n            # action log probabilities to be unnormalized.\n            \"action_dist_inputs\": policy_out,\n        }\n\n    @override(RLModule)\n    def forward_inference(self, batch: Dict[str, TensorType], **kwargs) -> Dict[str, TensorType]:\n        # Pass the observations through the shared encoder.\n        encoder_out = self.forward_encoder(batch)[self.encoder_id]\n        # Calculate action logits.\n        policy_out = self.forward_policy(encoder_out)[self.policy_id]\n\n        return {\n            \"logits\": policy_out,\n            # This is important to calculate the entropy loss correctly, which requires the\n            # action log probabilities to be unnormalized.\n            \"action_dist_inputs\": policy_out,\n        }\n\n    def forward_value(self, input_dict: Dict[str, TensorType]) -> Dict[ModuleID, TensorType]:\n        # Value head (predicts the return (single float value)).\n        return {self.value_id: self.get_sub_module(self.value_id)(input_dict)}\n\n    def forward_encoder(self, input_dict: Dict[str, TensorType]) -> Dict[ModuleID, TensorType]:\n        # Pass the observations through the shared encoder.\n        return {self.encoder_id: self.get_sub_module(self.encoder_id)(input_dict[\"obs\"])}\n\n    def forward_policy(self, input_dict: Dict[str, TensorType]) -> Dict[ModuleID, TensorType]:\n        # Policy head (produces action logits).\n        return {self.policy_id: self.get_sub_module(self.policy_id)(input_dict)}\n\n    @override(RLModule)\n    def value_function(self, input_dict: Dict[str, TensorType]) -> TensorType:\n        # Pass the observations through the shared encoder.\n        encoder_out = self.forward_encoder(input_dict)[self.encoder_id]\n        # Calculate value function output.\n        value_out = self.forward_value(encoder_out)[self.value_id]\n        return value_out\n\n\nif __name__ == \"__main__\":\n    env_name = \"CartPole-v1\"\n    num_envs = 4\n\n    ray.init(num_cpus=4)\n\n    # 1) Define the RLModule.\n    rl_module_spec = SingleAgentRLModuleSpec(\n        module_config={\n            \"encoder_config\": {\n                \"num_units\": 64,\n            },\n            \"policy_config\": {\n                \"num_units\": 64,\n            },\n            \"value_config\": {\n                \"num_units\": 64,\n                \"final_layer_activation\": None,\n            },\n        },\n        observation_space=gym.spaces.Box(low=-1.0, high=1.0, shape=(4,)),\n        action_space=gym.spaces.Discrete(2),\n    )\n\n    # 2) Instantiate the RLModule.\n    rl_module = VPGRlModule(rl_module_spec.module_config)\n\n    # 3) Create a dummy batch (single environment).\n    dummy_batch = {\n        \"obs\": torch.from_numpy(\n            np.stack([\n                rl_module_spec.observation_space.sample() for _ in range(num_envs)\n            ]))\n    }\n\n    # 4) Perform a forward pass through the module.\n    out = rl_module.forward_train(dummy_batch)\n\n    # Print some information.\n    print(\"Dummy batch:\", dummy_batch)\n    print(\"Action logits:\", out[\"logits\"])\n    print(\"Value function predictions:\", out[\"vf_preds\"])\n    print(\"Value function: \", rl_module.value_function(dummy_batch))\n\n    ray.shutdown()\n\n```\n\n----------------------------------------\n\nTITLE: Adding Last 5 Rewards to Tensor Batch - Python\nDESCRIPTION: This snippet includes code for adding the last 5 rewards to the tensor batch used in the model's forward pass computation. It extends the model's capability to utilize historical reward data to improve learning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/single-agent-episode.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: doc_code/sa_episode.py\n    :language: python\n    :start-after: rllib-sa-episode-05-begin\n    :end-before: rllib-sa-episode-05-end\n```\n\n----------------------------------------\n\nTITLE: Allocating GPUs to Learners in AlgorithmConfig for Python\nDESCRIPTION: This code example demonstrates how to allocate GPUs to learners by setting the number of learners to 4 and assigning 2 GPUs per learner using AlgorithmConfig.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .learners(num_learners=4, num_gpus_per_learner=2)\n)\n```\n\n----------------------------------------\n\nTITLE: Disabling Access Logging in Serve Deployment\nDESCRIPTION: Example showing how to disable access logging for a specific deployment using the logging_config parameter.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(logging_config={\"disable_access_log\": True})\nclass Model:\n    def __call__(self):\n        logger = logging.getLogger(\"ray.serve\")\n        logger.info(\"hello world\")\n        return \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Ray Memory Monitor Out-of-Memory Error Message\nDESCRIPTION: Detailed error message shown when Ray's memory monitor kills a worker due to memory pressure. Includes information about the node, task, memory usage threshold, and top memory users to help diagnose the issue.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-memory.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n\nTask was killed due to the node running low on memory.\nMemory on the node (IP: 10.0.62.231, ID: e5d953ef03e55e26f13973ea1b5a0fd0ecc729cd820bc89e4aa50451) where the task (task ID: 43534ce9375fa8e4cd0d0ec285d9974a6a95897401000000, name=allocate_memory, pid=11362, memory used=1.25GB) was running was 27.71GB / 28.80GB (0.962273), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6f2ec5c8b0d5f5a66572859faf192d36743536c2e9702ea58084b037) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.62.231`. To see the logs of the worker, use `ray logs worker-6f2ec5c8b0d5f5a66572859faf192d36743536c2e9702ea58084b037*out -ip 10.0.62.231.`\nTop 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n410728\t8.47\t510953\t7.19\tray::allocate_memory\n610952\t6.15\tray::allocate_memory\n711164\t3.63\tray::allocate_memory\n811156\t3.63\tray::allocate_memory\n911362\t1.25\tray::allocate_memory\n107230\t0.09\tpython test.py --num-tasks 2011327\t0.08\t/home/ray/anaconda3/bin/python /home/ray/anaconda3/lib/python3.9/site-packages/ray/dashboard/dashboa...\n\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html.\n```\n\n----------------------------------------\n\nTITLE: Full Example of Ray Train with Checkpointing\nDESCRIPTION: Complete example demonstrating Ray Train configuration with checkpointing, metrics reporting, and storage configuration\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/persistent-storage.rst#2025-04-12_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport tempfile\n\nimport ray.train\nfrom ray.train import Checkpoint\nfrom ray.train.torch import TorchTrainer\n\ndef train_fn(config):\n    for i in range(10):\n        # Training logic here\n        metrics = {\"loss\": ...}\n\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            torch.save(..., os.path.join(temp_checkpoint_dir, \"checkpoint.pt\"))\n            train.report(\n                metrics,\n                checkpoint=Checkpoint.from_directory(temp_checkpoint_dir)\n            )\n\ntrainer = TorchTrainer(\n    train_fn,\n    scaling_config=ray.train.ScalingConfig(num_workers=2),\n    run_config=ray.train.RunConfig(\n        storage_path=\"s3://bucket-name/sub-path/\",\n        name=\"unique-run-id\",\n    )\n)\nresult: train.Result = trainer.fit()\nlast_checkpoint: Checkpoint = result.checkpoint\n```\n\n----------------------------------------\n\nTITLE: Saving Tensor Data in JSON Format\nDESCRIPTION: Demonstrates saving tensor data in JSON format using Ray Data's write_json method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-tensors.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\nds.write_json(\"/tmp/simple\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray Cluster Status with CLI\nDESCRIPTION: This bash snippet demonstrates how to use the `ray status` command to view the resource usage and scheduling resource requirements of placement groups in a Ray cluster. It provides insight into CPU, GPU, memory, and object store memory usage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n\"Resources\n---------------------------------------------------------------\nUsage:\n1.0/2.0 CPU (1.0 used of 1.0 reserved in placement groups)\n0.0/2.0 GPU (0.0 used of 1.0 reserved in placement groups)\n0B/4.29GiB memory\n0B/2.00GiB object_store_memory\"\n```\n\n----------------------------------------\n\nTITLE: Managing Deployments with Different Python Dependencies\nDESCRIPTION: Python example demonstrating how to serve deployments with conflicting dependencies using Ray's runtime environments feature\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/handling-dependencies.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Typical implementation showing runtime environment configuration for deployments\n```\n\n----------------------------------------\n\nTITLE: Complete Structured Logging Example\nDESCRIPTION: Comprehensive example showing structured logging configuration with tasks and actors, including custom log attributes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport logging\n\nray.init(\n    logging_config=ray.LoggingConfig(encoding=\"JSON\", log_level=\"INFO\", additional_log_standard_attrs=['name'])\n)\n\ndef init_logger():\n    \"\"\"Get the root logger\"\"\"\n    return logging.getLogger()\n\nlogger = logging.getLogger()\nlogger.info(\"Driver process\")\n\n@ray.remote\ndef f():\n    logger = init_logger()\n    logger.info(\"A Ray task\")\n\n@ray.remote\nclass actor:\n    def print_message(self):\n        logger = init_logger()\n        logger.info(\"A Ray actor\")\n\ntask_obj_ref = f.remote()\nray.get(task_obj_ref)\n\nactor_instance = actor.remote()\nray.get(actor_instance.print_message.remote())\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Dashboard Dependencies\nDESCRIPTION: Command to install Ray with default components required for the dashboard functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"ray[default]\"\n```\n\n----------------------------------------\n\nTITLE: Ray Memory Monitor Periodic OOM Summary\nDESCRIPTION: Log message showing the aggregated summary of workers killed due to memory pressure. Includes a count of workers killed, the node information, and recommendations for addressing out-of-memory issues.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-memory.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n(raylet) [2023-04-09 07:23:59,445 E 395 395] (raylet) node_manager.cc:3049: 10 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e5d953ef03e55e26f13973ea1b5a0fd0ecc729cd820bc89e4aa50451, IP: 10.0.62.231) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.62.231`\n(raylet) \n(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n```\n\n----------------------------------------\n\nTITLE: Alternative Implementation Using Ray Core Remote Functions\nDESCRIPTION: This code demonstrates how to implement similar functionality using Ray Core remote tasks instead of Tune. It shows the more basic approach of using Ray's distributed computing primitives directly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-run.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Start Ray\nray.init()\n\n@ray.remote\ndef train_model_task(config):\n    model_id = config[\"model_id\"]\n    final_score = model_id\n    return {\n        \"score\": final_score,\n        \"other_data\": ...,\n    }\n\n# Execute tasks in parallel\nfutures = []\nfor i in range(100):\n    config = {\"model_id\": f\"model_{i}\"}\n    futures.append(train_model_task.remote(config))\n\n# Get results\nresults = ray.get(futures)\nprint(\"Results:\", results)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Data Block Size Limits\nDESCRIPTION: Sets the minimum and maximum target block sizes for Ray Data using DataContext. Controls how data is partitioned and processed across the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/data-internals.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nctx = ray.data.DataContext.get_current()\nctx.target_min_block_size = 1 * 1024 * 1024\nctx.target_max_block_size = 128 * 1024 * 1024\n```\n\n----------------------------------------\n\nTITLE: Reporting Final Metrics with Function API in Python\nDESCRIPTION: Example demonstrating how to report only the final metrics at the end of training using tune.report(). This is useful when intermediate results aren't needed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef train_func(config):\n    final_score = 0\n    for step in range(100):\n        # train the model\n        final_score = objective(step, config[\"a\"], config[\"b\"])\n    tune.report(score=final_score)\n```\n\n----------------------------------------\n\nTITLE: Saving Predictions to Parquet File in Ray\nDESCRIPTION: This code removes the 'original_image' column from the predictions dataset and saves the results to a parquet file in a temporary directory using Ray's distributed file system.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/pytorch_resnet_batch_prediction.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# to make sure all results get written on the head node.\npredictions.drop_columns([\"original_image\"]).write_parquet(f\"local://{temp_dir}\")\nprint(f\"Predictions saved to `{temp_dir}`!\")\n```\n\n----------------------------------------\n\nTITLE: Handling NumPy Arrays in Ray Compiled Graph\nDESCRIPTION: Demonstrates proper handling of NumPy arrays in Compiled Graph execution to avoid hangs or RayChannelTimeoutError. Shows how to manage array references and garbage collection.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/troubleshooting.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Create a numpy array output\\ndag = CompiledDAG()\\n# Run the compiled graph multiple times\\nfor i in range(10):\\n    execution = dag.execute()\\n    # Get result\\n    result = ray.get(execution)\\n    # Do something with result...\\n    # Explicitly delete result before next iteration\\n    del result\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Restored Model\nDESCRIPTION: Uses the restored model to make predictions on the test data and displays the predicted vs. actual values in a DataFrame. This demonstrates how the restored model can be used for inference on new data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Perform batch prediction using restored model from checkpoint\npred_y = sample_model.predict(test_X)\n\n# Zip together predictions and actuals to visualize\npd.DataFrame(zip(pred_y, test_y), columns=[\"pred_y\", TARGET])[0:10]\n```\n\n----------------------------------------\n\nTITLE: Converting Ray Dataset to Modin DataFrame in Python\nDESCRIPTION: This snippet shows how to convert a Ray Dataset to a Modin DataFrame. It reads CSV data from an S3 bucket and uses the to_modin() method for conversion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/saving-data.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\nmdf = ds.to_modin()\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchTrainer with Scaling and Checkpoint Options\nDESCRIPTION: This snippet sets up the TorchTrainer with scaling configuration for distributed training across 4 GPUs and checkpoint configuration to save model checkpoints. It defines resource allocation and storage paths for the training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig, RunConfig, CheckpointConfig\n\n# Scale out model training across 4 GPUs.\nscaling_config = ScalingConfig(\n    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n)\n\n# Save the latest checkpoint\ncheckpoint_config = CheckpointConfig(num_to_keep=1)\n\n# Set experiment name and checkpoint configs\nrun_config = RunConfig(\n    name=\"finetune-resnet\",\n    storage_path=\"/tmp/ray_results\",\n    checkpoint_config=checkpoint_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Memory Usage with Default Batch Size in Ray Data\nDESCRIPTION: Example showing how using default batch size of 1024 with a 1GB dataset can lead to high memory usage of 4GB when processing large tensors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Pretend there are two CPUs.\nray.init(num_cpus=2)\n\n# Force Ray Data to use one task to show the memory issue.\nds = ray.data.range_tensor(1000, shape=(125_000, ), override_num_blocks=1)\n# The default batch size is 1024 rows.\nds = ds.map_batches(lambda batch: batch)\nprint(ds.materialize().stats())\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Samples for Hyperparameter Search in Python\nDESCRIPTION: Defines the number of hyperparameter combinations to be tried during the Tune run. The default value is set to 1000, which can be adjusted for testing or performance reasons.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 1000\n```\n\n----------------------------------------\n\nTITLE: Defining Search Algorithm and Scheduler - Python\nDESCRIPTION: This snippet sets up the optimization algorithm using TuneBOHB alongside a concurrency limiter and scheduler for managing trial execution effectively.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nalgo = TuneBOHB()\\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\\nscheduler = HyperBandForBOHB(\\n    time_attr=\"training_iteration\",\\n    max_t=100,\\n    reduction_factor=4,\\n    stop_last_trials=False,\\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Training Loop with Ray Train and PyTorch\nDESCRIPTION: This function defines the worker-specific training loop for distributed PyTorch training. It handles dataset preparation, distributed data loading, model preparation, and training/validation phases. The function also reports metrics and saves checkpoints at regular intervals.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom tempfile import TemporaryDirectory\n\nimport ray.train as train\nfrom ray.train import Checkpoint\n\n\n\ndef evaluate(logits, labels):\n    _, preds = torch.max(logits, 1)\n    corrects = torch.sum(preds == labels).item()\n    return corrects\n\n\ndef train_loop_per_worker(configs):\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    # Calculate the batch size for a single worker\n    worker_batch_size = configs[\"batch_size\"] // train.get_context().get_world_size()\n\n    # Download dataset once on local rank 0 worker\n    if train.get_context().get_local_rank() == 0:\n        download_datasets()\n    torch.distributed.barrier()\n\n    # Build datasets on each worker\n    torch_datasets = build_datasets()\n\n    # Prepare dataloader for each worker\n    dataloaders = dict()\n    dataloaders[\"train\"] = DataLoader(\n        torch_datasets[\"train\"], batch_size=worker_batch_size, shuffle=True\n    )\n    dataloaders[\"val\"] = DataLoader(\n        torch_datasets[\"val\"], batch_size=worker_batch_size, shuffle=False\n    )\n\n    # Distribute\n    dataloaders[\"train\"] = train.torch.prepare_data_loader(dataloaders[\"train\"])\n    dataloaders[\"val\"] = train.torch.prepare_data_loader(dataloaders[\"val\"])\n\n    device = train.torch.get_device()\n\n    # Prepare DDP Model, optimizer, and loss function\n    model = initialize_model()\n    model = train.torch.prepare_model(model)\n\n    optimizer = optim.SGD(\n        model.parameters(), lr=configs[\"lr\"], momentum=configs[\"momentum\"]\n    )\n    criterion = nn.CrossEntropyLoss()\n\n    # Start training loops\n    for epoch in range(configs[\"num_epochs\"]):\n        # Each epoch has a training and validation phase\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                model.train()  # Set model to training mode\n            else:\n                model.eval()  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            if train.get_context().get_world_size() > 1:\n                dataloaders[phase].sampler.set_epoch(epoch)\n\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                with torch.set_grad_enabled(phase == \"train\"):\n                    # Get model outputs and calculate loss\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n\n                # calculate statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += evaluate(outputs, labels)\n\n            size = len(torch_datasets[phase]) // train.get_context().get_world_size()\n            epoch_loss = running_loss / size\n            epoch_acc = running_corrects / size\n\n            if train.get_context().get_world_rank() == 0:\n                print(\n                    \"Epoch {}-{} Loss: {:.4f} Acc: {:.4f}\".format(\n                        epoch, phase, epoch_loss, epoch_acc\n                    )\n                )\n\n            # Report metrics and checkpoint every epoch\n            if phase == \"val\":\n                with TemporaryDirectory() as tmpdir:\n                    state_dict = {\n                        \"epoch\": epoch,\n                        \"model\": model.module.state_dict(),\n                        \"optimizer_state_dict\": optimizer.state_dict(),\n                    }\n                    torch.save(state_dict, os.path.join(tmpdir, \"checkpoint.pt\"))\n                    train.report(\n                        metrics={\"loss\": epoch_loss, \"acc\": epoch_acc},\n                        checkpoint=Checkpoint.from_directory(tmpdir),\n                    )\n```\n\n----------------------------------------\n\nTITLE: Manual Exception Handling and Retry in Ray (Python)\nDESCRIPTION: Demonstrates how to manually catch exceptions caused by failures in Ray and implement a retry mechanism. This approach is useful when Ray's built-in fault tolerance mechanisms are insufficient for specific use cases.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault-tolerance.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwhile True:\n    try:\n        # Some Ray operation that might fail\n        result = ray.get(some_task.remote())\n        break\n    except ray.exceptions.RayTaskError:\n        print(\"Task failed, retrying...\")\n        time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Default Resources in Python\nDESCRIPTION: Basic Ray initialization that automatically detects available resources on a single machine.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/configure.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# This automatically detects available resources in the single machine.\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Optimized Ray Test Class with Shared Cluster\nDESCRIPTION: Demonstrates how to share a Ray cluster across multiple tests in a test suite using class-level setup and teardown, improving test execution performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/testing-tips.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass RayClassTest(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # Start it once for the entire test suite/module\n        ray.init(num_cpus=4, num_gpus=0)\n\n    @classmethod\n    def tearDownClass(cls):\n        ray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Getting detailed information about a specific actor using CLI\nDESCRIPTION: The 'ray get actors' CLI command retrieves detailed information about a specific actor identified by its ID, showing all properties and current state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# In this case, 31405554844820381c2f0f8501000000\nray get actors <ACTOR_ID>\n```\n\nLANGUAGE: text\nCODE:\n```\n---\nactor_id: 31405554844820381c2f0f8501000000\nclass_name: Actor\ndeath_cause: null\nis_detached: false\nname: ''\npid: 96956\nresource_mapping: []\nserialized_runtime_env: '{}'\nstate: ALIVE\n```\n\n----------------------------------------\n\nTITLE: Connecting Modin to an Existing Ray Cluster with Autoscaler\nDESCRIPTION: Demonstrates how to connect Modin to an existing Ray cluster using the autoscaler. The code initializes Ray with 'address=\"auto\"' to connect to a running cluster automatically before performing dataframe operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/modin/index.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport modin.pandas as pd\nimport ray\n\nray.init(address=\"auto\")\ndf = pd.read_parquet(\"s3://my-bucket/big.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Storage Path for Ray Train Checkpoints in Python\nDESCRIPTION: This snippet demonstrates how to set up a storage path for Ray Train checkpoints. It provides examples for both cloud storage (S3) and NFS, emphasizing the need for external storage in multi-node setups.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nstorage_path=\"s3://your-bucket-here\"  # TODO: Set up cloud storage\n# storage_path=\"/mnt/path/to/nfs\"     # TODO: Alternatively, set up NFS\n```\n\n----------------------------------------\n\nTITLE: Configuring Experience Output in RLlib (Python)\nDESCRIPTION: This function configures experience output settings for RLlib agents. It allows specifying output locations, compression, file size limits, and file system options for storing agent experiences.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef offline_data(\n    output: Optional[str],\n    output_compress_columns: Optional[List[str]],\n    output_max_file_size: Optional[float],\n    output_max_rows_per_file: Optional[int],\n    output_write_method: Optional[str],\n    output_write_method_kwargs: Optional[Dict],\n    output_filesystem: Optional[str],\n    output_filesystem_kwargs: Optional[Dict],\n    output_write_episodes: Optional[bool],\n)\n```\n\n----------------------------------------\n\nTITLE: Registering PettingZoo Multi-Agent Environment\nDESCRIPTION: Python code for registering and configuring a PettingZoo multi-agent environment using RLlib\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.registry import register_env\nfrom ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\nfrom pettingzoo.sisl import waterworld_v4\nregister_env(\"env\", lambda _: PettingZooEnv(waterworld_v4.env()))\nconfig.environment(\"env\")\n```\n\n----------------------------------------\n\nTITLE: PyTorch Training Job Submission Script in Python\nDESCRIPTION: A Python script that uses the Ray Job SDK to submit a PyTorch image training benchmark job to a Ray cluster. The script specifies the training parameters and requirements for the job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/gpu-training-example.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray.job_submission import JobSubmissionClient\n\ntriton_server_template = \"\"\"\nimport ray\nfrom ray import train\nimport tempfile\nfrom ray.air import session\nfrom ray.train.examples.pytorch.torch_fashion_mnist import train_fashion_mnist\nfrom ray.train.torch import TorchTrainer\nimport os\nfrom ray.air.config import ScalingConfig\n\nray.init()\n\n# Create a temporary dir to download the data.\ndata_dir = tempfile.mkdtemp()\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_fashion_mnist,\n    train_loop_config={\"data_dir\": data_dir,\n                       \"lr\": 1e-3,\n                       \"batch_size\": 64,\n                       \"epochs\": 2},\n    scaling_config=ScalingConfig(\n        num_workers=2,\n        use_gpu=True,\n    ),\n    dataset_config=train.DataConfig(\n            datasets_to_split=[\"train_dataset\"],\n    ),\n)\nresult = trainer.fit()\nprint(f\"Finished training with results: {result}.\")\n\"\"\"\n\nif __name__ == \"__main__\":\n    # Connect to the ray cluster\n    client = JobSubmissionClient(\"http://127.0.0.1:8265\")\n    ray_job_id = client.submit_job(\n        # Entrypoint shell command to execute\n        entrypoint=\"python -c \\\"{}\\\" \".format(triton_server_template),\n        # Job submission options\n        submission_id=None,  # Autogenerate an ID\n        # Runtime environment\n        runtime_env={\n            \"pip\": [\"torch==2.0.1\", \"torchvision\", \"torch==2.0.1\"]\n        },\n    )\n    print(f\"Use the following command to follow this Job's logs:\")\n    print(f\"ray job logs '{ray_job_id}' --follow\")\n    print(f\"ray job logs '{ray_job_id}' --address http://127.0.0.1:8265 --follow\")\n```\n\n----------------------------------------\n\nTITLE: Installing RayCluster and Configuring Metrics\nDESCRIPTION: Shell commands to install a RayCluster, check its status, and forward the Prometheus metrics endpoint port.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# path: ray-operator/config/samples/\nkubectl apply -f ray-cluster.embed-grafana.yaml\n\n# Check ${RAYCLUSTER_HEAD_POD}\nkubectl get pod -l ray.io/node-type=head\n\n# Example output:\n# NAME                                  READY   STATUS    RESTARTS   AGE\n# raycluster-embed-grafana-head-98fqt   1/1     Running   0          11m\n\n# Wait until all Ray Pods are running and forward the port of the Prometheus metrics endpoint in a new terminal.\nkubectl port-forward ${RAYCLUSTER_HEAD_POD} 8080:8080\ncurl localhost:8080\n\n# Example output (Prometheus metrics format):\n# # HELP ray_spill_manager_request_total Number of {spill, restore} requests.\n# # TYPE ray_spill_manager_request_total gauge\n# ray_spill_manager_request_total{Component=\"raylet\", NodeAddress=\"10.244.0.13\", SessionName=\"session_2025-01-02_07-58-21_419367_11\", Type=\"FailedDeletion\", Version=\"2.9.0\", container=\"ray-head\", endpoint=\"metrics\", instance=\"10.244.0.13:8080\", job=\"prometheus-system/ray-head-monitor\", namespace=\"default\", pod=\"raycluster-embed-grafana-head-98fqt\", ray_io_cluster=\"raycluster-embed-grafana\"} 0\n\n# Ensure that the port (8080) for the metrics endpoint is also defined in the head's Kubernetes service.\nkubectl get service\n\n# NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                    AGE\n# raycluster-embed-grafana-head-svc   ClusterIP   None            <none>        44217/TCP,10001/TCP,44227/TCP,8265/TCP,6379/TCP,8080/TCP   13m\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Resource Allocation in Python\nDESCRIPTION: Example showing how to configure distributed trainables with complex resource requirements using PlacementGroupFactory. This allows for reserving resources for the main trainable and any additional actors it creates.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    tune.with_resources(my_trainable, tune.PlacementGroupFactory([\n        {\"CPU\": 1, \"GPU\": 1},\n        {\"GPU\": 1},\n        {\"GPU\": 1},\n        {\"GPU\": 1},\n        {\"GPU\": 1}\n    ])),\n    run_config=RunConfig(name=\"my_trainable\")\n)\n```\n\n----------------------------------------\n\nTITLE: Recording Tabular Data with RLlib Algorithm\nDESCRIPTION: Configures and runs an RLlib algorithm to record offline data in tabular format. It sets up the algorithm, loads a pre-trained model, and runs evaluation iterations to record data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define the output path and format. In this example you\n# want to store data directly in RLlib's episode objects.\n.offline_data(\n    output=tabular_data_path,\n    # You want to store for this example tabular data.\n    output_write_episodes=False,\n)\n\n# Build the algorithm.\nalgo = config.build()\n# Load the PPO-trained `RLModule` to use in recording.\nalgo.restore_from_path(\n    best_checkpoint,\n    # Load only the `RLModule` component here.\n    component=COMPONENT_RL_MODULE,\n)\n\n# Run 10 evaluation iterations and record the data.\nfor i in range(10):\n    print(f\"Iteration {i + 1}\")\n    res_eval = algo.evaluate()\n    print(res_eval)\n\n# Stop the algorithm. Note, this is important for when\n# defining `output_max_rows_per_file`. Otherwise,\n# remaining episodes in the `EnvRunner`s buffer isn't written to disk.\nalgo.stop()\n```\n\n----------------------------------------\n\nTITLE: Incorrect Ray Parallelization: Eager ray.get()\nDESCRIPTION: Demonstrates another common mistake where ray.get() is called immediately after each remote task invocation, which blocks execution and prevents parallelism. This results in sequential execution despite using Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresults = [ray.get(do_some_work.remote(x)) for x in range(4)]\n```\n\n----------------------------------------\n\nTITLE: Interactive GUI Implementation\nDESCRIPTION: Creates an interactive IPython widgets interface for generating images, including toggle buttons for model type selection, text input for prompts, and a generate button with output display.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/playground.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\noutput = widgets.Output()\ntoggle_buttons = widgets.ToggleButtons(\n    options=[\"Full fine-tuning\",\"LoRA fine-tuning\"],\n    disabled=False,\n    button_style='',\n    value=None,\n)\n\ndef toggle_callback(change):\n    with output:\n        clear_output()\n        if change[\"new\"] == \"Full fine-tuning\":\n            on_full_ft()\n        else:\n            on_lora_ft()\n        \ntoggle_buttons.observe(toggle_callback, names=\"value\")\n    \ninput_text = widgets.Text(\n    value=\"photo of a unqtkn dog on the beach\",\n    placeholder=\"\",\n    description=\"Prompt:\",\n    disabled=False,\n    layout=widgets.Layout(width=\"500px\"),\n)\n\nbutton = widgets.Button(description=\"Generate!\")\n\ndef on_button_clicked(b):\n    with output:\n        clear_output()\n        print(\"Generating images...\")\n        print(\n            \"(The output image may be completely black if it's filtered by \"\n            \"HuggingFace diffusers safety checkers.)\"\n        )\n        start_time = time.time()\n        images = generate(pipeline=pipeline, prompt=input_text.value, num_samples=2)\n        display(*images)\n        finish_time = time.time()\n        print(f\"Completed in {finish_time - start_time} seconds.\")\n\nbutton.on_click(on_button_clicked)\n\ndisplay(toggle_buttons, widgets.HBox([input_text, button]), output)\n```\n\n----------------------------------------\n\nTITLE: Defining Search Space for Hyperparameters in Python\nDESCRIPTION: Specifies the search space for hyperparameters that include continuous values for 'width' and 'height', as well as categorical choices for 'activation'. This defines the domain for hyperparameter optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsearch_config = {\n    \"steps\": 100,\n    \"width\": tune.uniform(0, 20),\n    \"height\": tune.uniform(-100, 100),\n    \"activation\": tune.choice([\"relu\", \"tanh\"])\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for PyTorch Object Detection and Ray Data\nDESCRIPTION: Installs the required Python packages for running the object detection example with Ray Data and PyTorch.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"ray[data]\" torchvision\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Ray Tune and Wandb Integration\nDESCRIPTION: This snippet shows the necessary imports for using Ray Tune with Weights & Biases integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-wandb.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nimport ray\nfrom ray import tune\nfrom ray.air.integrations.wandb import WandbLoggerCallback, setup_wandb\n```\n\n----------------------------------------\n\nTITLE: Enabling Reproducibility in PyTorch Ray Train\nDESCRIPTION: The code snippet demonstrates how to enhance reproducibility by adding `ray.train.torch.enable_reproducibility` at the top of a PyTorch training function. While it helps in limiting sources of nondeterministic behavior, complete reproducibility across executions is not always guaranteed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/reproducibility.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef train_func():\n    train.torch.enable_reproducibility()\n\n    model = NeuralNetwork()\n    model = train.torch.prepare_model(model)\n\n    ...\n```\n\n----------------------------------------\n\nTITLE: Custom OfflinePreLearner for Image Data Processing in RLlib\nDESCRIPTION: Implementation of a custom OfflinePreLearner that processes image data and transforms it into a learner-ready MultiAgentBatch format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n# This code is referenced but not shown in the snippet\n```\n\n----------------------------------------\n\nTITLE: Enabling Deterministic Execution in Ray Data\nDESCRIPTION: Example showing how to enable deterministic execution by preserving block ordering.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# By default, this is set to False.\nctx.execution_options.preserve_order = True\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Tune with CometLoggerCallback\nDESCRIPTION: This code configures Ray Tune to use the `CometLoggerCallback` for logging metrics and parameters to Comet. It creates a `tune.Tuner` instance with the training function, a tune configuration specifying the metric and mode, and a run configuration that includes the `CometLoggerCallback`. The `api_key` and `project_name` parameters are passed to the callback to authenticate with Comet. The `param_space` defines the search space for the hyperparameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-comet.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.air.integrations.comet import CometLoggerCallback\n\ntuner = tune.Tuner(\n    train_function,\n    tune_config=tune.TuneConfig(\n        metric=\"loss\",\n        mode=\"min\",\n    ),\n    run_config=tune.RunConfig(\n        callbacks=[\n            CometLoggerCallback(\n                api_key=api_key, project_name=project_name, tags=[\"comet_example\"]\n            )\n        ],\n    ),\n    param_space={\"mean\": tune.grid_search([1, 2, 3]), \"sd\": tune.uniform(0.2, 0.8)},\n)\nresults = tuner.fit()\n\nprint(results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Overriding Ray GPU Capacity in YAML Configuration\nDESCRIPTION: This YAML configuration snippet shows how to override the GPU capacity advertised to Ray scheduler using rayStartParams. This advanced configuration tells Ray that each pod has 2 GPU units regardless of the container resource limits.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gpu.rst#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nrayStartParams:\n    # Note that all rayStartParam values must be supplied as strings.\n    num-gpus: \"2\"\n```\n\n----------------------------------------\n\nTITLE: Basic Ray Test Class Setup in Python\nDESCRIPTION: Shows how to properly setup and teardown a Ray testing environment for individual tests using unittest. Creates a new Ray cluster for each test with fixed CPU and GPU resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/testing-tips.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\n\nclass RayTest(unittest.TestCase):\n    def setUp(self):\n        ray.init(num_cpus=4, num_gpus=0)\n\n    def tearDown(self):\n        ray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Running the Tune experiment\nDESCRIPTION: Runs the Tune experiment to minimize the mean loss of the objective function by searching the search space using the Nevergrad algorithm. This is the core step of the hyperparameter optimization process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"tuner = tune.Tuner(\\n    objective,\\n    tune_config=tune.TuneConfig(\\n        metric=\\\"mean_loss\\\",\\n        mode=\\\"min\\\",\\n        search_alg=algo,\\n        num_samples=num_samples,\\n    ),\\n    param_space=search_config,\\n)\\nresults = tuner.fit()\"\n```\n\n----------------------------------------\n\nTITLE: Defining Model Deployments\nDESCRIPTION: Ray Serve deployments for wrapping text generation models to enable parallel execution\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/gradio-integration.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment\nclass TextGenerationModel:\n    def __init__(self, model_id):\n        self._model = pipeline(\"text-generation\", model=model_id)\n\n    async def generate(self, text, min_length=20, max_length=100):\n        result = self._model(text, min_length=min_length, max_length=max_length)\n        return result[0][\"generated_text\"]\n```\n\n----------------------------------------\n\nTITLE: Adding GPU Nodepool with Autoscaling\nDESCRIPTION: Adds a GPU nodepool to the cluster using Standard_NC6s_v3 VM size with autoscaling enabled and GPU-specific node taints\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/azure-aks-gpu-cluster.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naz aks nodepool add \\\n   -g kuberay-rg \\\n   --cluster-name kuberay-gpu-cluster \\\n   --nodepool-name gpupool \\\n   --node-vm-size Standard_NC6s_v3 \\\n   --node-taints nvidia.com/gpu=present:NoSchedule \\\n   --min-count 0 \\\n   --max-count 3 \\\n   --enable-cluster-autoscaler\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Dynamic User Config in Deployments\nDESCRIPTION: This YAML snippet illustrates how to employ the user_config field to supply dynamic configurations for a deployment, allowing for on-the-fly adjustments to parameters such as model thresholds and other operational features.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/config.md#2025-04-12_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n...\ndeployments:\n    - name: Model\n      user_config:\n        threshold: 1.5\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing CoLA Dataset with Ray Data\nDESCRIPTION: This code snippet loads the CoLA dataset using Hugging Face datasets, creates Ray Datasets for training and validation, and applies tokenization using a BERT tokenizer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_cola_advanced.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset = load_dataset(\"glue\", \"cola\")\n\ntrain_dataset = ray.data.from_huggingface(dataset[\"train\"])\nvalidation_dataset = ray.data.from_huggingface(dataset[\"validation\"])\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ndef tokenize_sentence(batch):\n    outputs = tokenizer(\n        batch[\"sentence\"].tolist(),\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"np\",\n    )\n    outputs[\"label\"] = batch[\"label\"]\n    return outputs\n\ntrain_dataset = train_dataset.map_batches(tokenize_sentence, batch_format=\"numpy\")\nvalidation_dataset = validation_dataset.map_batches(tokenize_sentence, batch_format=\"numpy\")\n```\n\n----------------------------------------\n\nTITLE: R2D2 with PER configuration in RLlib\nDESCRIPTION: This code snippet demonstrates how to configure the R2D2 algorithm in RLlib to use Prioritized Experience Replay (PER). It highlights the specific lines of code responsible for enabling and configuring PER within the RLlib experiment setup, modifying the `replay_buffer_config`.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-replay-buffers.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example showing how to configure replay buffers.\n\nThis example shows how to configure a replay buffer when running an experiment.\n\"\"\"\n\nimport argparse\nimport os\n\nimport ray\nfrom ray import tune\nfrom ray.rllib.algorithms.r2d2 import R2D2Config\nfrom ray.rllib.examples.env.stateless_cartpole import StatelessCartPoleEnv\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \"--framework\",\n    choices=[\"tf\", \"torch\"],\n    default=\"torch\",\n    help=\"The DL framework to use.\",\n)\n\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    ray.init()\n\n    # An algorithm config.\n    config = (\n        R2D2Config()\n        .environment(StatelessCartPoleEnv)\n        .framework(args.framework)\n        # We should be able to train with min_sample_timesteps_per_reporting=1\n        # given we are using a very small env.\n        .reporting(min_sample_timesteps_per_reporting=1)\n        # Switch on the LSTM model.\n        .training(\n            model={\n                \"use_lstm\": True,\n                \"lstm_cell_size\": 64,\n                \"lstm_use_prev_action_reward\": True,\n                \"max_seq_len\": 20,\n            },\n            # Train very aggressively.\n            lr=0.0005,\n            replay_buffer_config={\n                \"type\": \"Prioritized\",\n                \"prioritized_replay_alpha\": 0.6,\n                \"prioritized_replay_beta\": 0.4,\n                \"prioritized_replay_eps\": 1e-6,\n            },\n        )\n        .rollouts(num_rollout_workers=1, num_envs_per_worker=1)\n        .resources(num_gpus=0)\n        .exploration(explore=False)\n        .debugging(seed=42)\n    )\n\n    stop = {\n        \"timesteps_total\": 20000,\n    }\n\n    # Run the experiment.\n    tuner = tune.Tuner(\n        \"R2D2\",\n        run_config=tune.RunConfig(stop=stop, verbose=1),\n        param_space=config.to_dict(),\n    )\n    tuner.fit()\n    ray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Installing a RayService with GCS Fault Tolerance\nDESCRIPTION: Applies a predefined YAML configuration that sets up a RayService with GCS fault tolerance, including Redis for state management and a 3-node RayCluster with a Ray Serve application.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-high-availability.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-service.high-availability.yaml\n```\n\n----------------------------------------\n\nTITLE: Implementing Checkpointing with Class API in Ray Tune\nDESCRIPTION: Demonstrates how to implement checkpoint and restore functionality using the Trainable Class API. This example defines both checkpoint and restore methods that handle the model state saving and loading.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-trial-checkpoints.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyTrainable(tune.Trainable):\n    def setup(self, config):\n        # Set up model with the provided config\n        self.model = Model(config)\n        self.epoch = 0\n        \n    def step(self):\n        # Train for one epoch\n        train_loss = self.model.train()\n        self.epoch += 1\n        return {\"loss\": train_loss, \"epoch\": self.epoch}\n        \n    def save_checkpoint(self, checkpoint_dir):\n        checkpoint_path = os.path.join(checkpoint_dir, \"model.pth\")\n        torch.save({\n            \"model_state_dict\": self.model.state_dict(),\n            \"epoch\": self.epoch\n        }, checkpoint_path)\n        return checkpoint_dir\n        \n    def load_checkpoint(self, checkpoint_dir):\n        checkpoint_path = os.path.join(checkpoint_dir, \"model.pth\")\n        checkpoint = torch.load(checkpoint_path)\n        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n        self.epoch = checkpoint[\"epoch\"]\n```\n\n----------------------------------------\n\nTITLE: Reading and Decoding TFRecord Image Data\nDESCRIPTION: Demonstrates loading and decoding image data from TFRecords format, including conversion of raw bytes to numpy arrays.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-images.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport io\nfrom typing import Any, Dict\nimport numpy as np\nfrom PIL import Image\nimport ray\n\ndef decode_bytes(row: Dict[str, Any]) -> Dict[str, Any]:\n    data = row[\"image\"]\n    image = Image.open(io.BytesIO(data))\n    row[\"image\"] = np.array(image)\n    return row\n\nds = (\n    ray.data.read_tfrecords(\n        \"s3://anonymous@air-example-data/cifar-10/tfrecords\"\n    )\n    .map(decode_bytes)\n)\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Configuring Structured Logging with ray.init\nDESCRIPTION: Initialize Ray with structured logging configuration using JSON encoding and INFO log level.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nray.init(\n    log_to_driver=False,\n    logging_config=ray.LoggingConfig(encoding=\"JSON\", log_level=\"INFO\")\n)\n```\n\n----------------------------------------\n\nTITLE: fsspec Filesystem Integration with Ray Train\nDESCRIPTION: Demonstrates wrapping an fsspec filesystem with pyarrow for custom storage configuration\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/persistent-storage.rst#2025-04-12_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Make sure to install: `pip install -U s3fs`\nimport s3fs\nimport pyarrow.fs\n\ns3_fs = s3fs.S3FileSystem(\n    key='miniokey...',\n    secret='asecretkey...',\n    endpoint_url='https://...'\n)\ncustom_fs = pyarrow.fs.PyFileSystem(pyarrow.fs.FSSpecHandler(s3_fs))\n\nrun_config = RunConfig(storage_path=\"minio_bucket\", storage_filesystem=custom_fs)\n```\n\n----------------------------------------\n\nTITLE: Create and Retrieve Named Placement Group in Java\nDESCRIPTION: This Java snippet creates a placement group with a unique name and demonstrates how to retrieve it later. It leverages the PlacementGroups API to create and retrieve the placement group.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_15\n\nLANGUAGE: java\nCODE:\n```\n\"          // Create a placement group with a unique name.\n          Map<String, Double> bundle = ImmutableMap.of(\"CPU\", 1.0);\n          List<Map<String, Double>> bundles = ImmutableList.of(bundle);\n\n          PlacementGroupCreationOptions options =\n            new PlacementGroupCreationOptions.Builder()\n              .setBundles(bundles)\n              .setStrategy(PlacementStrategy.STRICT_SPREAD)\n              .setName(\"global_name\")\n              .build();\n\n          PlacementGroup pg = PlacementGroups.createPlacementGroup(options);\n          pg.wait(60);\n\n          ...\n\n          // Retrieve the placement group later somewhere.\n          PlacementGroup group = PlacementGroups.getPlacementGroup(\"global_name\");\n          Assert.assertNotNull(group);\"\n```\n\n----------------------------------------\n\nTITLE: Logging Metrics in Function and Class Training APIs\nDESCRIPTION: Demonstrates how to log arbitrary metrics in both Function and Class-based training APIs in Ray Tune. The Function API uses tune.report() while the Class API returns a dictionary from the step() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-metrics.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef trainable(config):\n    for i in range(num_epochs):\n        ...\n        tune.report({\"acc\": accuracy, \"metric_foo\": random_metric_1, \"bar\": metric_2})\n\nclass Trainable(tune.Trainable):\n    def step(self):\n        ...\n        # don't call report here!\n        return dict(acc=accuracy, metric_foo=random_metric_1, bar=metric_2)\n```\n\n----------------------------------------\n\nTITLE: Saving and Restoring Search Algorithms in Ray Tune\nDESCRIPTION: Shows how to save and restore the state of a search algorithm between different tuning runs. This is useful for continuing optimization from a previous state or for transferring knowledge between related optimization tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/suggestion.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsearch_alg = HyperOptSearch()\n\ntuner_1 = tune.Tuner(\n    train_fn,\n    tune_config=tune.TuneConfig(search_alg=search_alg)\n)\nresults_1 = tuner_1.fit()\n\nsearch_alg.save(\"./my-checkpoint.pkl\")\n\n# Restore the saved state onto another search algorithm,\n# in a new tuning script\n\nsearch_alg2 = HyperOptSearch()\nsearch_alg2.restore(\"./my-checkpoint.pkl\")\n\ntuner_2 = tune.Tuner(\n    train_fn,\n    tune_config=tune.TuneConfig(search_alg=search_alg2)\n)\nresults_2 = tuner_2.fit()\n```\n\n----------------------------------------\n\nTITLE: Uploading and Running Tune Experiments\nDESCRIPTION: Commands for uploading local tune experiments to a cluster and executing them remotely. Includes options for running in tmux sessions and automatic cluster management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nray submit CLUSTER.YAML tune_experiment.py -- --address=localhost:6379\n\nray submit CLUSTER.YAML --tmux --start --stop tune_experiment.py -- --address=localhost:6379\n```\n\n----------------------------------------\n\nTITLE: Create Detached Placement Group in Python\nDESCRIPTION: This Python snippet shows how to create a detached placement group, ensuring that its lifetime is independent of the driver or detached actor that created it. This is done by specifying `lifetime=\"detached\"` during placement group creation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n\"        # Connecting to the cluster with a namespace.\n        ray.init(namespace=\"my_namespace\")\n\n        # Create a detached placement group with a name.\n        pg = ray.util.placement_group([{\"CPU\": 1}], name=\"detached_name\", lifetime=\"detached\")\n        ray.get(pg.ready())\n        assert pg.id().hex() in ray.get(ray.internal.worker._global_worker.core_worker.dump_placement_group_debug_info())\n\n        # Retrieve the placement group by the name.\n        pg_by_name = ray.util.get_placement_group(\"detached_name\", namespace=\"my_namespace\")\n        assert pg_by_name.id() == pg.id()\n\n        # Clean up.\n        ray.util.remove_placement_group(pg)\n        assert pg.id().hex() not in ray.get(ray.internal.worker._global_worker.core_worker.dump_placement_group_debug_info())\n        ray.shutdown()\"\n```\n\n----------------------------------------\n\nTITLE: Optimized Approach: Using ray.put() for Large Arguments in Ray Tasks (Python)\nDESCRIPTION: This code snippet shows the recommended approach for handling large arguments in Ray tasks. It uses ray.put() to store the large argument once in the object store and passes the reference to multiple tasks, improving performance and memory usage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/pass-large-arg-by-value.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport numpy as np\n\n@ray.remote\ndef task(arr):\n    return arr.sum()\n\nray.init()\n\n# Create a large array\nlarge_array = np.random.rand(1000000)\n\n# Better approach: Use ray.put() to store the argument once and pass by reference\nlarge_array_ref = ray.put(large_array)\nresults = ray.get([\n    task.remote(large_array_ref)\n    for _ in range(10)\n])\n\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Performing a Blocking Update on a Learner - Python\nDESCRIPTION: This snippet illustrates how to execute a blocking update for a Learner instance, demonstrating how to apply a training batch for weight updates during reinforcement learning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-learner.rst#2025-04-12_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nresult = learner.update(batch=DUMMY_BATCH, timesteps=TIMESTEPS)\n```\n\n----------------------------------------\n\nTITLE: Exception-Based Task Retries in Ray\nDESCRIPTION: Shows how to configure task retries based on specific exception types.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/tasks.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Example configuring exception-based task retries\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Serve Deployment in RayService\nDESCRIPTION: YAML configuration for Ray Serve that specifies a single deployment with specific replica settings. This configuration creates just one replica, leading to the scenario where one worker Pod won't have any replicas.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nserveConfigV2: |\n  applications:\n    - name: simple_app\n      import_path: ray-operator.config.samples.ray-serve.single_deployment_dag:DagNode\n      route_prefix: /basic\n      runtime_env:\n        working_dir: \"https://github.com/ray-project/kuberay/archive/master.zip\"\n      deployments:\n        - name: BaseService\n          num_replicas: 1\n          max_replicas_per_node: 1\n          ray_actor_options:\n            num_cpus: 0.1\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchTrainer in Ray Train Python\nDESCRIPTION: Sets up a TorchTrainer for distributed training with Ray Train. It defines the training configuration, including the number of epochs and batch size, and specifies scaling configurations such as the number of workers and GPU resources. The trainer utilizes processed datasets and a storage path for run configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import RunConfig, ScalingConfig\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func,\n    train_loop_config={\n        \"epochs\": 1,\n        \"batch_size\": batch_size,  # per device\n        \"steps_per_epoch\": steps_per_epoch,\n    },\n    scaling_config=ScalingConfig(\n        num_workers=num_workers,\n        use_gpu=use_gpu,\n        resources_per_worker={\"GPU\": 1, \"CPU\": cpus_per_worker},\n    ),\n    datasets=processed_datasets,\n    run_config=RunConfig(storage_path=storage_path),\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Nsight Profiling for Ray Compiled Graph (Python)\nDESCRIPTION: Code snippet showing how to create and execute a Compiled Graph with Nsight profiling enabled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/profiling.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create a Compiled Graph as usual\ndag = ray.data.range(100).map(lambda x: x + 1).map(lambda x: x * 2)\ncompiled_dag = dag.experimental_compile()\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Ray Training Function\nDESCRIPTION: Basic structure for a Ray training function that will be executed by each distributed worker.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/common/torch-configure-train_func.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef train_func():\n    # Your model training code here.\n    ...\n```\n\n----------------------------------------\n\nTITLE: Check Job Status\nDESCRIPTION: Retrieves the job status of the RayJob using jsonpath extraction.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nkubectl get rayjobs.ray.io rayjob-sample-shutdown -o jsonpath='{.status.jobStatus}'\n```\n\n----------------------------------------\n\nTITLE: Implementing a Stateful RLlibCallback in Python\nDESCRIPTION: This code shows how to create a stateful callback by subclassing RLlibCallback and implementing the on_episode_end method. The callback keeps track of the overall sum of rewards and prints it along with the episode return after each episode.  The PPOConfig is then configured to use this callback.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-callback.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.callbacks.callbacks import RLlibCallback\n\nclass EpisodeReturn(RLlibCallback):\n    def __init__(self):\n        super().__init__()\n        # Keep some global state in between individual callback events.\n        self.overall_sum_of_rewards = 0.0\n\n    def on_episode_end(self, *, episode, **kwargs):\n        self.overall_sum_of_rewards += episode.get_return()\n        print(f\"Episode done. R={episode.get_return()} Global SUM={self.overall_sum_of_rewards}\")\n\nppo = (\n    PPOConfig()\n    .environment(\"CartPole-v1\")\n    .callbacks(EpisodeReturn)\n    .build()\n)\nppo.train()\n```\n\nLANGUAGE: python\nCODE:\n```\nppo.stop()\n```\n\n----------------------------------------\n\nTITLE: Serve Build Command\nDESCRIPTION: This command generates a Serve configuration file from a Python file containing a Ray Serve application. It takes the module name and application name as input and outputs a YAML file containing the application's configuration.  The `-o` flag specifies the output file name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/index.md#2025-04-12_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ serve build text_ml:app -o serve_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Parallel HTTP Queries Implementation - Python\nDESCRIPTION: Example showing how to send parallel HTTP requests using Ray remote tasks\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/batch.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport requests\n\n@ray.remote\ndef send_query(text):\n    resp = requests.post(\"http://localhost:8000/\", params={\"text\": text})\n    return resp.text\n\n# Example batch of queries\ntexts = [\n    'Once upon a time,',\n    'Hi my name is Lewis and I like to',\n    'In a galaxy far far away',\n]\n\n# Send all queries in parallel\nresults = ray.get([send_query.remote(text) for text in texts])\n```\n\n----------------------------------------\n\nTITLE: Defining Objective Function - Python\nDESCRIPTION: This snippet outlines the objective function that evaluates the model's performance over a specified number of steps, leveraging Tune's reporting capabilities to optimize performance metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef objective(config):\\n    start = 0\\n    if tune.get_checkpoint():\\n        with tune.get_checkpoint().as_directory() as checkpoint_dir:\\n            start = int((Path(checkpoint_dir) / \"data.ckpt\").read_text())\\n\\n    for step in range(start, config[\"steps\"]):\\n        score = evaluate(step, config[\"width\"], config[\"height\"], config[\"activation\"])\\n        with tempfile.TemporaryDirectory() as checkpoint_dir:\\n            (Path(checkpoint_dir) / \"data.ckpt\").write_text(str(step))\\n            tune.report(\\n                {\"iterations\": step, \"mean_loss\": score},\\n                checkpoint=tune.Checkpoint.from_directory(checkpoint_dir)\\n            )\n```\n\n----------------------------------------\n\nTITLE: Swapping Code for Dynamic Value Handling in Ray\nDESCRIPTION: This snippet replaces a traditional return value mechanism with a generator function to yield values dynamically, optimizing memory usage. The generator yields return values one at a time instead of holding all values in memory, addressing potential performance issues.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tasks/generators.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/pattern_generators.py\n    :language: python\n    :start-after: __large_values_generator_start__\n    :end-before: __large_values_generator_end__\n```\n\n----------------------------------------\n\nTITLE: Pinning Objects in Memory with Ray in Python\nDESCRIPTION: This example shows how to create a numpy array, store it in the Ray object store, fetch it, and keep it pinned in memory even after deleting the original ObjectRef.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/memory-management.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\na = ray.put(np.zeros(1))\nb = ray.get(a)\ndel a\n```\n\n----------------------------------------\n\nTITLE: Generating Model Configurations for Hyperparameter Search\nDESCRIPTION: This function generates model configurations based on a given search space, allowing for grid search over hyperparameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/automl_for_time_series.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef generate_configurations(search_space: Dict[Type[_TS], Dict[str, list]]) -> _TS:\n    # Convert dict search space into configurations - models instantiated with specific arguments.\n    for model, model_search_space in search_space.items():\n        kwargs, values = model_search_space.keys(), model_search_space.values()\n        # Get a product - all combinations in the per-model grid.\n        for configuration in itertools.product(*values):\n            yield model(**dict(zip(kwargs, configuration)))\n```\n\n----------------------------------------\n\nTITLE: Defining Data Transforms for ResNet Training\nDESCRIPTION: Sets up data augmentation and normalization transforms for training and validation datasets using torchvision.transforms.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata_transforms = {\n    \"train\": transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ]),\n    \"val\": transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ]),\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Ray Dataset to Dask DataFrame\nDESCRIPTION: Shows how to convert a Ray Dataset to a Dask DataFrame using the to_dask() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/saving-data.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\ndf = ds.to_dask()\n\ndf\n```\n\n----------------------------------------\n\nTITLE: Specifying GPU Resources for Ray Tune Trials\nDESCRIPTION: Defines the resource requirements for each hyperparameter tuning trial, specifying CPU and GPU allocation. This configuration allows Ray Tune to properly schedule trials based on available hardware resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresources_per_trial = {\"cpu\": 1, \"gpu\": gpus_per_trial}\n```\n\n----------------------------------------\n\nTITLE: Instantiate TorchTrainer\nDESCRIPTION: Instantiates the TorchTrainer with the training function, scaling configuration, datasets, and run configuration. The scaling configuration specifies the number of workers and whether to use a GPU.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import RunConfig, ScalingConfig, CheckpointConfig\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n    datasets={\n        \"train\": ray_datasets[\"train\"],\n        \"eval\": ray_datasets[\"validation\"],\n    },\n    run_config=RunConfig(\n        checkpoint_config=CheckpointConfig(\n            num_to_keep=1,\n            checkpoint_score_attribute=\"eval_loss\",\n            checkpoint_score_order=\"min\",\n        ),\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for PyTorch Lightning with Ray Tune\nDESCRIPTION: Command to install the necessary packages for running PyTorch Lightning with Ray Tune, including Ray with tune support, PyTorch, TorchVision, and PyTorch Lightning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -q \"ray[tune]\" torch torchvision pytorch_lightning\n```\n\n----------------------------------------\n\nTITLE: Delayed Import for Deployment Dependencies\nDESCRIPTION: Python example showing how to use delayed imports to handle dependencies that may not be available in the driver program\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/handling-dependencies.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Typical implementation showing delayed import technique\n```\n\n----------------------------------------\n\nTITLE: Single Batch Pi Calculation\nDESCRIPTION: Executes a single remote task to calculate pi and measures the execution time.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/highly_parallel.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nSAMPLE_COUNT = 1000 * 1000\nstart = time.time() \nfuture = pi4_sample.remote(sample_count = SAMPLE_COUNT)\npi4 = ray.get(future)\nend = time.time()\ndur = end - start\nprint(f'Running {SAMPLE_COUNT} tests took {dur} seconds')\n```\n\n----------------------------------------\n\nTITLE: Limiting Shuffle Execution for Debugging in Ray Dataset\nDESCRIPTION: This example demonstrates how to limit a random shuffle operation to a specific number of output blocks for debugging purposes. It sets a configuration option in the DataContext to limit the shuffle execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/shuffling-data.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nctx = ray.data.DataContext.get_current()\nctx.set_config(\n    \"debug_limit_shuffle_execution_to_num_blocks\", 2\n)\n\nds = (\n    ray.data.range(1000, override_num_blocks=10)\n    .random_shuffle()\n    .materialize()\n)\nprint(ds.stats())\n```\n\n----------------------------------------\n\nTITLE: Adding Concurrency Limitation to Tune Search\nDESCRIPTION: Wraps the AxSearch algorithm with a ConcurrencyLimiter to restrict the number of concurrent trials to 4, which helps manage computational resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\n```\n\n----------------------------------------\n\nTITLE: Accessing Ray Training Results\nDESCRIPTION: Demonstrates how to access various training results including metrics, checkpoints, logs, and error information from the Result object\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/common/torch-configure-run.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresult.metrics     # The metrics reported during training.\nresult.checkpoint  # The latest checkpoint reported during training.\nresult.path        # The path where logs are stored.\nresult.error       # The exception that was raised, if training failed.\n```\n\n----------------------------------------\n\nTITLE: Client-side Structured Output Interaction (JSON Object)\nDESCRIPTION: Python code for client-side interaction with a Ray Serve LLM deployment to request structured JSON output. Demonstrates how to specify JSON object response format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Initialize client\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n\n# Request structured JSON output\nresponse = client.chat.completions.create(\n    model=\"qwen-0.5b\",\n    response_format={\"type\": \"json_object\"},\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that outputs JSON.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"List three colors in JSON format\"\n        }\n    ],\n    stream=True,\n)\n\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n# Example response:\n# {\n#   \"colors\": [\n#     \"red\",\n#     \"blue\",\n#     \"green\"\n#   ]\n# }\n```\n\n----------------------------------------\n\nTITLE: Shuffling File Order in Ray Dataset\nDESCRIPTION: This snippet demonstrates how to randomly shuffle the ordering of input files before reading them into a Ray Dataset. It uses the 'shuffle' parameter with the value 'files' in the read_images function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/shuffling-data.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\n    \"s3://anonymous@ray-example-data/image-datasets/simple\",\n    shuffle=\"files\",\n)\n```\n\n----------------------------------------\n\nTITLE: Prototyping HTTP Requests with Python Requests\nDESCRIPTION: This snippet demonstrates a basic pattern for making HTTP requests using the Python `requests` library. It's intended for prototyping and doesn't include production-ready features like retries or exponential backoff. The code shows a simple GET request to a Ray Serve endpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/best-practices.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Basic HTTP requests.\n\"\"\"\nimport requests\n\n\ndef send_request(host: str, route: str = \"/\") -> None:\n    response = requests.get(f\"http://{host}{route}\")\n    print(f\"Request finished with status code {response.status_code}.\\n\")\n\n\nif __name__ == \"__main__\":\n    host = \"localhost:8000\"\n    send_request(host)\n    send_request(host, route=\"/salutation\")\n\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Data Aggregation Functions in Python\nDESCRIPTION: This snippet shows how to import various aggregation functions from the ray.data.aggregate module. These functions can be used for performing aggregations on Ray Datasets.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/aggregate.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.data.aggregate import AggregateFnV2, AggregateFn, Count, Sum, Min, Max, Mean, Std, AbsMax, Quantile, Unique\n```\n\n----------------------------------------\n\nTITLE: Specifying Resources for Dask-on-Ray Tasks in Python\nDESCRIPTION: This example demonstrates how to use Dask's annotation API to specify resources or other Ray task options for Dask operations, which are then applied to the underlying Ray tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/dask-on-ray.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport dask\nimport dask.array as da\nfrom ray.util.dask import enable_dask_on_ray\n\n# Start Ray.\nray.init()\n\n# Tell Dask to use Ray as the scheduler.\nenable_dask_on_ray()\n\n# Create a large array.\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\n\n# Specify resources per call.\nwith dask.annotate(ray_remote_args={\"num_cpus\": 1}):  # 1 cpu per task\n    y = x + x\n\n# Specify resources for all tasks globally via .compute().\nresult = y.mean().compute(\n    ray_remote_args={\"num_cpus\": 2}  # Override to 2 cpus per task\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Ray Serve with RLlib\nDESCRIPTION: Shows how to integrate RLlib with Ray Serve to deploy trained RLModule instances as RESTful services, ideal for API-based production environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Using Ray Serve with RLlib\n# This script demonstrates deploying RL models as RESTful services.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Cluster from Command Line\nDESCRIPTION: Commands for starting Ray head and worker nodes with custom resource configurations using command line interface.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/configure.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# To start a head node.\n$ ray start --head --num-cpus=<NUM_CPUS> --num-gpus=<NUM_GPUS>\n\n# To start a non-head node.\n$ ray start --address=<address> --num-cpus=<NUM_CPUS> --num-gpus=<NUM_GPUS>\n\n# Specifying custom resources\nray start [--head] --num-cpus=<NUM_CPUS> --resources='{\"Resource1\": 4, \"Resource2\": 16}'\n```\n\n----------------------------------------\n\nTITLE: Exploration get_exploration_action Method\nDESCRIPTION: This code snippet showcases the implementation of the `get_exploration_action` method within an Exploration class. This method defines the exploratory behavior by taking the model's output, action distribution, model, timestep, and an `explore` switch as input. It outputs a tuple of the chosen action and the log-likelihood of that action.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-advanced-api.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example of a get_exploration_action method.\n\n.. code-block:: python\n\n    def get_exploration_action(\n        self,\n        distribution,  # ActionDistribution.\n        model=None,\n        timestep=None,\n        explore=True,\n    ):\n        # Use `timestep` and `explore` to determine the action to return.\n        if explore:\n            return distribution.sample(), distribution.logp(action)\n        else:\n            return distribution.deterministic_sample(), distribution.logp(action)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Constructing DAGs in Ray\nDESCRIPTION: Methods and classes for constructing Directed Acyclic Graphs (DAGs) in Ray, including actor method binding, tensor transport, and compiled DAG references.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/compiled-graph-api.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nray.actor.ActorMethod.bind\nray.dag.DAGNode.with_tensor_transport\nray.experimental.compiled_dag_ref.CompiledDAGRef\n```\n\n----------------------------------------\n\nTITLE: Adjusting Scaling Configuration for Smoke Test\nDESCRIPTION: Modifies the scaling configuration for smoke test mode to use CPUs instead of GPUs, making testing possible in environments without GPU acceleration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nif SMOKE_TEST:\n    scaling_config = ScalingConfig(\n        num_workers=3, use_gpu=False, resources_per_worker={\"CPU\": 1}\n    )\n```\n\n----------------------------------------\n\nTITLE: Converting XGBoost Example to LightGBM\nDESCRIPTION: Shows the necessary code changes to convert the XGBoost training example to use LightGBM instead. The main differences are in the trainer class and the objective and metric specifications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n- from ray.train.xgboost import XGBoostTrainer\n+ from ray.train.lightgbm import LightGBMTrainer\n\n- trainer = XGBoostTrainer(\n+ trainer = LightGBMTrainer(\n\n- \"objective\": \"binary:logistic\",\n+ \"objective\": \"binary\",\n- \"eval_metric\": [\"logloss\", \"error\"],\n+ \"metric\": [\"binary_logloss\", \"binary_error\"],\n```\n\n----------------------------------------\n\nTITLE: Tune with setup_mlflow\nDESCRIPTION: This function demonstrates how to use the `setup_mlflow` utility to configure a Tune run with MLflow. It sets the MLflow tracking URI and experiment name before running the Tune experiment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-mlflow.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef tune_with_setup(mlflow_tracking_uri, finish_fast=False):\n    # Set the experiment, or create a new one if does not exist yet.\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(experiment_name=\"setup_mlflow_example\")\n\n    tuner = tune.Tuner(\n        train_function_mlflow,\n        tune_config=tune.TuneConfig(num_samples=5),\n        run_config=tune.RunConfig(\n            name=\"mlflow\",\n        ),\n        param_space={\n            \"width\": tune.randint(10, 100),\n            \"height\": tune.randint(0, 100),\n            \"steps\": 5 if finish_fast else 100,\n            \"tracking_uri\": mlflow.get_tracking_uri(),\n        },\n    )\n    results = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Early Stopping with Ray Tune Schedulers\nDESCRIPTION: Combines dictionary stopping criteria with an early stopping scheduler (AsyncHyperBandScheduler) to stop underperforming trials early.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-stopping.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasha_scheduler = tune.schedulers.AsyncHyperBandScheduler(\n    max_t=20,\n    grace_period=5,\n    reduction_factor=3,\n    brackets=1\n)\n\ntuner = tune.Tuner(\n    train_function,\n    tune_config=tune.TuneConfig(\n        num_samples=5,\n        scheduler=asha_scheduler,\n        stop={\"mean_accuracy\": 0.98}\n    ),\n    param_space={\"base_accuracy\": 0.3}\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Factorial with Ray Workflow\nDESCRIPTION: This example shows how to implement a recursive factorial function using dynamic workflows. It demonstrates nesting and recursion within workflows, and shows execution using both workflow.run and ray.get methods.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef factorial(n: int) -> int:\n    if n == 1:\n        return 1\n    else:\n        # Here a DAG is passed to the continuation.\n        # The DAG will continue to be executed after this task.\n        return workflow.continuation(multiply.bind(n, factorial.bind(n - 1)))\n\n@ray.remote\ndef multiply(a: int, b: int) -> int:\n    return a * b\n\nassert workflow.run(factorial.bind(10)) == 3628800\n# You can also execute the code with Ray DAG engine.\nassert ray.get(factorial.bind(10).execute()) == 3628800\n```\n\n----------------------------------------\n\nTITLE: Using ResourceChangingScheduler for Dynamic Resource Allocation\nDESCRIPTION: ResourceChangingScheduler is a utility scheduler that allows trial resource requirements to be changed during tuning. It wraps around another scheduler and requires implementing Trainable.update_resources when using the class API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.schedulers import ResourceChangingScheduler\nfrom ray.tune.schedulers.resource_changing_scheduler import DistributeResources, DistributeResourcesToTopJob\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Resources in Ray Serve Deployment\nDESCRIPTION: Example showing how to allocate one GPU per replica in a Ray Serve deployment using ray_actor_options\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/resource-allocation.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(ray_actor_options={\"num_gpus\": 1})\ndef func(*args):\n    return do_something_with_my_gpu()\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Ray Dashboard\nDESCRIPTION: Command to set up port forwarding for accessing the Ray dashboard on localhost.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/aws-trainium/llama3.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward service/kuberay-trn1-head-svc 8265:8265 &\n```\n\n----------------------------------------\n\nTITLE: DeploymentHandle Query Implementation - Python\nDESCRIPTION: Example showing how to query the model using the deployment handle directly\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/batch.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import serve\n\ninput_batch = [\n    'Once upon a time,',\n    'Hi my name is Lewis and I like to',\n    'In a galaxy far far away',\n]\n\n# initialize using the 'auto' option to connect to the already-running Ray cluster\nray.init(address=\"auto\")\n\nhandle = serve.get_deployment_handle(\"BatchTextGenerator\", app_name=\"Text-Completion-App\")\nresponses = [handle.handle_batch.remote(text) for text in input_batch]\nresults = [r.result() for r in responses]\n```\n\n----------------------------------------\n\nTITLE: Resource Validation Check\nDESCRIPTION: Validates if the cluster has sufficient GPU resources for the requested number of replicas.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/start.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif NUM_REPLICAS > ray.available_resources()[\"GPU\"]:\n    print(\n        \"Your cluster does not currently have enough resources to run with these settings. \"\n        \"Consider decreasing the number of workers, or decreasing the resources needed \"\n        \"per worker. Ignore this if your cluster auto-scales.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: ResNet50 Autoscaling Example Code in Ray Serve (Python)\nDESCRIPTION: This Python snippet provides the application code for a ResNet50 workload configured with autoscaling in Ray Serve. It demonstrates how to set up a deployment that adjusts replica count in response to changing request load, ensuring efficient resource usage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/autoscaling-guide.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{literalinclude} doc_code/resnet50_example.py\n:language: python\n:start-after: __serve_example_begin__\n:end-before: __serve_example_end__\n```\n\n----------------------------------------\n\nTITLE: Implementing a Class Trainable API Example in Python\nDESCRIPTION: Example showing how to subclass tune.Trainable to implement the Class API approach. This includes setup, step, and cleanup methods for the training lifecycle.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MyTrainable(tune.Trainable):\n    def setup(self, config):\n        # config (dict): A dict containing hyperparameter values and other\n        #     configuration arguments.\n        # This dict is created from the given search space.\n        self.config = config\n        self.counter = 0\n\n    def step(self):  # This is called iteratively.\n        self.counter += 1\n        return {\n            \"score\": objective(self.counter, self.config[\"a\"], self.config[\"b\"])\n        }\n\n    def cleanup(self):\n        # Clean up any state\n        pass\n```\n\n----------------------------------------\n\nTITLE: Streaming Node Logs with Python SDK\nDESCRIPTION: Demonstrates how to stream logs from a specific node using the Ray Python SDK. It retrieves the node IP and uses the get_log function to continuously print log lines.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import get_log\n\n# Retrieve the Node IP from list_nodes() or ray.nodes()\n# The loop blocks with `follow=True`\nfor line in get_log(filename=\"raylet.out\", node_ip=<NODE_IP>, follow=True):\n    print(line)\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Cluster\nDESCRIPTION: This code initializes a local Ray cluster and prints the available cluster resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\nimport ray\n\nray.init()\n\npprint(ray.cluster_resources())\n```\n\n----------------------------------------\n\nTITLE: Performing Trial-level Analysis with Individual Result\nDESCRIPTION: Illustrates how to access and analyze individual trial results, including retrieving the best-performing trial, accessing configurations, and examining metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune_analyze_results.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune import Result\n\n# Get the result with the maximum test set `mean_accuracy`\nbest_result: Result = result_grid.get_best_result()\n\n# Get the result with the minimum `mean_accuracy`\nworst_performing_result: Result = result_grid.get_best_result(\n    metric=\"mean_accuracy\", mode=\"min\"\n)\n\nbest_result.config\n\nbest_result.path\n\n# Get the last Checkpoint associated with the best-performing trial\nbest_result.checkpoint\n\n# Get the last reported set of metrics\nbest_result.metrics\n\nresult_df = best_result.metrics_dataframe\nresult_df[[\"training_iteration\", \"mean_accuracy\", \"time_total_s\"]]\n```\n\n----------------------------------------\n\nTITLE: Updating Serve Config for Translator Language in YAML\nDESCRIPTION: This YAML snippet shows how to update the language of the Translator in the Serve configuration to German. It demonstrates modifying a specific component's user configuration within a Ray Service manifest.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/kubernetes.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n  - name: Translator\n    num_replicas: 1\n    user_config:\n      language: german\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS CLI for Efficient S3 Download in Python\nDESCRIPTION: This snippet shows how to configure the AWS CLI for improved download performance when retrieving checkpoints from S3. It sets parameters for concurrent requests, transfer client, bandwidth, and chunk size.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!aws configure set s3.max_concurrent_requests 32\n!aws configure set default.s3.preferred_transfer_client crt\n!aws configure set default.s3.target_bandwidth 100Gb/s\n!aws configure set default.s3.multipart_chunksize 8MB\n```\n\n----------------------------------------\n\nTITLE: Creating Detached Actors in Java\nDESCRIPTION: Demonstrates how to create a detached actor in Java, which persists even after the driver process exits. This allows the actor to be retrieved in a different driver.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/named-actors.rst#2025-04-12_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nSystem.setProperty(\"ray.job.namespace\", \"lifetime\");\nRay.init();\nActorHandle<Counter> counter = Ray.actor(Counter::new).setName(\"some_name\").setLifetime(ActorLifetime.DETACHED).remote();\n\nSystem.setProperty(\"ray.job.namespace\", \"lifetime\");\nRay.init();\nOptional<ActorHandle<Counter>> counter = Ray.getActor(\"some_name\");\nAssert.assertTrue(counter.isPresent());\n```\n\n----------------------------------------\n\nTITLE: Creating a Placement Group in Java\nDESCRIPTION: This snippet shows how to create a placement group in Java using Ray. It initializes Ray, constructs a list of bundles, and creates a placement group with specified options.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Initialize Ray.\nRay.init();\n\n// Construct a list of bundles.\nMap<String, Double> bundle = ImmutableMap.of(\"CPU\", 1.0);\nList<Map<String, Double>> bundles = ImmutableList.of(bundle);\n\n// Make a creation option with bundles and strategy.\nPlacementGroupCreationOptions options =\n  new PlacementGroupCreationOptions.Builder()\n    .setBundles(bundles)\n    .setStrategy(PlacementStrategy.STRICT_SPREAD)\n    .build();\n\nPlacementGroup pg = PlacementGroups.createPlacementGroup(options);\n```\n\n----------------------------------------\n\nTITLE: Implementing a Class-based Ray Tune Trainable with Wandb Integration\nDESCRIPTION: This class demonstrates how to create a Tune Trainable class that uses setup_wandb in its setup method for Wandb integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-wandb.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass WandbTrainable(tune.Trainable):\n    def setup(self, config):\n        self.wandb = setup_wandb(\n            config,\n            trial_id=self.trial_id,\n            trial_name=self.trial_name,\n            group=\"Example\",\n            project=\"Wandb_example\",\n        )\n\n    def step(self):\n        for i in range(30):\n            loss = self.config[\"mean\"] + self.config[\"sd\"] * np.random.randn()\n            self.wandb.log({\"loss\": loss})\n        return {\"loss\": loss, \"done\": True}\n\n    def save_checkpoint(self, checkpoint_dir: str):\n        pass\n\n    def load_checkpoint(self, checkpoint_dir: str):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Files with Custom Block Count in Python\nDESCRIPTION: This snippet demonstrates how to read CSV files using Ray Data, specifying a custom number of output blocks. It shows the difference between using the default auto-detection and manually setting the number of blocks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Pretend there are two CPUs.\nray.init(num_cpus=2)\n\n# Repeat the iris.csv file 16 times.\nds = ray.data.read_csv([\"example://iris.csv\"] * 16)\nprint(ds.materialize())\n```\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Pretend there are two CPUs.\nray.init(num_cpus=2)\n\n# Repeat the iris.csv file 16 times.\nds = ray.data.read_csv([\"example://iris.csv\"] * 16, override_num_blocks=16)\nprint(ds.materialize())\n```\n\n----------------------------------------\n\nTITLE: Implementing Efficient Ray Object Reference Handling in Python\nDESCRIPTION: This code snippet demonstrates a better approach where ray.get() is avoided for intermediate steps. Object references are passed directly to the reduce task, allowing for more efficient data transfer between workers without unnecessary copies to the driver.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/unnecessary-ray-get.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef generate_rollout():\n    # Generate a large rollout\n    return large_rollout\n\n@ray.remote\ndef reduce(rollouts):\n    # Aggregate rollouts\n    return aggregated_result\n\nrollouts = [generate_rollout.remote() for _ in range(10)]\nfinal_result = reduce.remote(rollouts)  # Pass references directly\nprint(ray.get(final_result))  # Only call ray.get() at the end\n```\n\n----------------------------------------\n\nTITLE: Configuring RayCluster Worker Group for TPU Usage in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure a RayCluster worker group to use TPUs in GKE. It specifies the number of TPU chips, the number of hosts per worker group replica, and the necessary node selectors for TPU scheduling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/tpu.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   groupName: tpu-group\n   replicas: 1\n   minReplicas: 0\n   maxReplicas: 1\n   numOfHosts: 2\n   ...\n   template:\n       spec:\n        ...\n        containers:\n         - name: ray-worker\n           image: rayproject/ray:2.9.0-py310\n           ...\n           resources:\n             google.com/tpu: \"4\" # Required to use TPUs.\n             ...\n           limits:\n             google.com/tpu: \"4\" # The resources and limits value is expected to be equal.\n             ...\n        nodeSelector:\n            cloud.google.com/gke-tpu-accelerator: tpu-v4-podslice\n            cloud.google.com/gke-tpu-topology: 2x2x2\n            ...\n```\n\n----------------------------------------\n\nTITLE: Logging to MLflow in PyTorch\nDESCRIPTION: This snippet shows how to log experiments in PyTorch using MLflow with Ray Train, emphasizing key lines of code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. dropdown:: Log to file-based MLflow\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking/torch_exp_tracking_mlflow.py\n        :emphasize-lines: 22, 23, 24, 25, 54, 55, 57, 58, 64\n        :language: python\n        :start-after: __start__\n        :end-before: __end__\n```\n\n----------------------------------------\n\nTITLE: Logging Structured Dictionaries with MetricsLogger in Python\nDESCRIPTION: Logs multiple scalar values within a nested dictionary structure using MetricsLogger's log_dict method. It facilitates setting the same logging parameters for each value within the dictionary.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.utils.metrics.metrics_logger import MetricsLogger\n\nlogger = MetricsLogger()\n\n# Log a bunch of scalar values within a nested dict.\nstats = {\"player1\": 100.0, \"player2\": 105.0}\nlogger.log_dict(stats, key=\"mean_scores\", reduce=\"mean\", window=10)\n\n# Later, do the same again.\nstats = {\"player1\": 150.0, \"player2\": 110.0}\nlogger.log_dict(stats, key=\"mean_scores\")\n\nprint(logger.peek((\"mean_scores\", \"player1\")))  # <- expect 125.0\n```\n\n----------------------------------------\n\nTITLE: Defining a Sampling Task in Ray\nDESCRIPTION: Creates a Ray task for sampling points and determining if they fall within a unit circle. It reports progress to the progress actor.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/monte_carlo_pi.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef sampling_task(num_samples, progress_actor):\n    num_inside = 0\n    for i in range(num_samples):\n        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n        if x * x + y * y <= 1:\n            num_inside += 1\n        if (i + 1) % 1_000_000 == 0:\n            ray.get(progress_actor.report_progress.remote(1_000_000))\n    return num_inside\n```\n\n----------------------------------------\n\nTITLE: Restoring Tune Experiments with Ray Object References\nDESCRIPTION: Code showing how to restore a Tune experiment that used Ray object references. The example recreates the necessary objects and references for a proper experiment restoration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-fault-tolerance.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# We need to recreate the objects and references\nmodel_1 = create_large_pretrained_model(\"resnet50\")\nmodel_2 = create_large_pretrained_model(\"densenet121\")\n\n# Put the models in Ray's object store again\nmodel_1_ref = ray.put(model_1)\nmodel_2_ref = ray.put(model_2)\n\n# Pass the new object references in the param_space\nparam_space = {\n    \"model\": tune.grid_search([model_1_ref, model_2_ref]),\n}\n\n# Restore the Tuner with the new param_space\ntuner = tune.Tuner.restore(\n    path=\"~/ray_results/tune_fault_tolerance_guide\",\n    trainable=trainable,\n    param_space=param_space,\n)\n\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Downloading RayJob Configuration YAML for Batch Inference\nDESCRIPTION: Command to download the sample RayJob configuration file that defines the batch inference job setup. This YAML file contains the complete configuration for both the Ray cluster and the inference job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-batch-inference-example.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-job.batch-inference.yaml\n```\n\n----------------------------------------\n\nTITLE: Using Dynamic Generator for Unknown Return Values in Ray\nDESCRIPTION: This code snippet demonstrates how to define a remote generator function that can dynamically return an unknown number of values when the number of expected return values is not predetermined. By setting 'num_returns=\"dynamic\"', the task will return a DynamicObjectRefGenerator, which can be iterated over to access the returned values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tasks/generators.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/generator.py\n    :language: python\n    :start-after: __dynamic_generator_start__\n    :end-before: __dynamic_generator_end__\n```\n\n----------------------------------------\n\nTITLE: Loading RLModule Checkpoint into Running Algorithm\nDESCRIPTION: This code demonstrates how to load an RLModule checkpoint into a running algorithm. It shows how to create an algorithm with a matching module configuration, then restore the module state from a checkpoint path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\n# Create a new Algorithm (with the changed module config: 32 units instead of the\n# default 256; otherwise loading the state of ``module`` fails due to a shape\n# mismatch).\nconfig = (\n    PPOConfig()\n    .environment(\"CartPole-v1\")\n    .rl_module(model_config=DefaultModelConfig(fcnet_hiddens=[32]))\n)\nppo = config.build()\n```\n\n----------------------------------------\n\nTITLE: Reading Local Parquet Files with Ray Data in Python\nDESCRIPTION: Demonstrates reading Parquet files from local disk using Ray Data's read_parquet function. The example uses the 'local://' schema to specify a local file path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_parquet(\"local:///tmp/iris.parquet\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Rows in Ray Data\nDESCRIPTION: Demonstrates how to iterate over individual rows of a dataset using Ray Data's iter_rows() method. Each row is represented as a dictionary.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/iterating-over-data.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\nfor row in ds.iter_rows():\n    print(row)\n```\n\n----------------------------------------\n\nTITLE: Passing Nested Arguments in Ray Workflow\nDESCRIPTION: This example shows how to pass a list of task outputs to a workflow task. It demonstrates that all ancestors of a task are fully executed before the task starts, ensuring exactly-once execution semantics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef add(values: List[ray.ObjectRef]) -> int:\n    # although those values are not resolved, they have been\n    # *fully executed and checkpointed*. This guarantees exactly-once\n    # execution semantics.\n    return sum(ray.get(values))\n\n@ray.remote\ndef get_val() -> int:\n    return 10\n\nret = add.bind([get_val.bind() for _ in range(3)])\nassert workflow.run(ret) == 30\n```\n\n----------------------------------------\n\nTITLE: Scaling Ray Clusters with request_resources\nDESCRIPTION: The request_resources() function allows programmatic control of cluster scaling within a Ray program. It bypasses normal upscaling speed constraints to immediately scale the cluster to accommodate requested resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/autoscaling/reference.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nray.autoscaler.sdk.request_resources()\n```\n\n----------------------------------------\n\nTITLE: Listing all Ray actors using CLI command\nDESCRIPTION: The 'ray list actors' CLI command lists all actors in the Ray cluster, showing their IDs, class names, PIDs, and states, providing a comprehensive view of actor status.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nray list actors\n```\n\nLANGUAGE: text\nCODE:\n```\n======== List: 2022-07-23 21:29:39.323925 ========\nStats:\n------------------------------\nTotal: 2\n\nTable:\n------------------------------\n    ACTOR_ID                          CLASS_NAME    NAME      PID  STATE\n0  31405554844820381c2f0f8501000000  Actor                 96956  ALIVE\n1  f36758a9f8871a9ca993b1d201000000  Actor                 96955  ALIVE\n```\n\n----------------------------------------\n\nTITLE: Configuring Failure Handling in Ray Train (Python)\nDESCRIPTION: This snippet demonstrates how to configure failure handling in Ray Train using FailureConfig and RunConfig. It sets the maximum number of failures to 3 before the training job is terminated.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/fault-tolerance.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import RunConfig, FailureConfig\n\nrun_config = RunConfig(\n    failure_config=FailureConfig(max_failures=3)\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing actor logs using Python SDK\nDESCRIPTION: Using Ray's State API in Python to access logs for a specific actor. The get_log function returns an iterator of log lines that can be processed in Python.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import get_log\n\n# In this case, ACTOR_ID is 31405554844820381c2f0f8501000000\nfor line in get_log(actor_id=<ACTOR_ID>):\n    print(line)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Event Listener in Ray Workflows\nDESCRIPTION: Implementation of the EventListener interface, which defines the structure for custom event listeners in Ray Workflows. The interface includes methods for polling events and handling event checkpointing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/events.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.workflow.common import Event\n\nclass EventListener:\n    def __init__(self):\n        \"\"\"Optional constructor. Only the constructor with no arguments will be\n          called.\"\"\"\n        pass\n\n    async def poll_for_event(self, *args, **kwargs) -> Event:\n        \"\"\"Should return only when the event is received.\"\"\"\n        raise NotImplementedError\n\n    async def event_checkpointed(self, event: Event) -> None:\n        \"\"\"Optional. Called after an event has been checkpointed and a transaction can\n          be safely committed.\"\"\"\n        pass\n```\n\n----------------------------------------\n\nTITLE: Loading Environment for Ray Job on Slurm\nDESCRIPTION: Example of loading modules or activating a conda environment at the beginning of a Slurm script for a Ray job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/slurm.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Example: module load pytorch/v1.4.0-gpu\n# Example: conda activate my-env\n\nconda activate my-env\n```\n\n----------------------------------------\n\nTITLE: Caching Preprocessed Dataset in Ray Object Store\nDESCRIPTION: This snippet demonstrates how to cache a preprocessed dataset in Ray's object store for improved performance. It includes data loading, preprocessing, materialization, and adding per-epoch transformations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport ray\n\n# Load the data.\ntrain_ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\n\n# Define a preprocessing function.\ndef normalize_length(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    new_col = batch[\"sepal.length\"] / np.max(batch[\"sepal.length\"])\n    batch[\"normalized.sepal.length\"] = new_col\n    del batch[\"sepal.length\"]\n    return batch\n\n# Preprocess the data. Transformations that are made before the materialize call\n# below are only run once.\ntrain_ds = train_ds.map_batches(normalize_length)\n\n# Materialize the dataset in object store memory.\n# Only do this if train_ds is small enough to fit in object store memory.\ntrain_ds = train_ds.materialize()\n\n# Dummy augmentation transform.\ndef augment_data(batch):\n    return batch\n\n# Add per-epoch preprocessing. Transformations that you want to run per-epoch, such\n# as data augmentation or randomization, should go after the materialize call.\ntrain_ds = train_ds.map_batches(augment_data)\n\n# Pass train_ds to the Trainer\n```\n\n----------------------------------------\n\nTITLE: Comparing Validation and Test Errors\nDESCRIPTION: Compares the error on the test set with the validation error recorded during training. This verification ensures that the model's performance is consistent between validation and test phases, indicating good generalization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Compare test error with training validation error\nprint(f\"Validation error: {final_df.error[sample_location_id]}\")\n\n# Validation and test errors should be reasonably close together.\n```\n\n----------------------------------------\n\nTITLE: Adjusting Dataset Size Calculation for Distributed Training\nDESCRIPTION: Modifies the effective dataset size by dividing it by the number of workers to account for data sharding in distributed training scenarios.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n    # Divide by word size\n    size = len(dataloader.dataset) // train.get_context().get_world_size()\n```\n\n----------------------------------------\n\nTITLE: Executing Ray CLI Commands on VM Cluster\nDESCRIPTION: Demonstrates how to execute Ray CLI commands on a VM cluster from outside the cluster using the 'ray exec' command.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n$ ray exec <cluster config file> \"ray status\"\n```\n\n----------------------------------------\n\nTITLE: Defining Python Counter Class\nDESCRIPTION: This Python code defines a simple Counter class that can be deployed as a Ray Serve deployment. The class has an `increase` method that increments the counter and returns the new value as a string.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/managing-java-deployments.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\n\n@serve.deployment\nclass Counter(object):\n    def __init__(self, value):\n        self.value = int(value)\n\n    def increase(self, delta):\n        self.value += int(delta)\n        return str(self.value)\n\n```\n\n----------------------------------------\n\nTITLE: Querying Ray Serve Application via Port Forwarding\nDESCRIPTION: Commands to set up port forwarding and send a query to the deployed Ray Serve application using curl\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/kubernetes.md#2025-04-12_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl port-forward service/rayservice-sample-serve-svc 8000\n$ curl -X POST -H \"Content-Type: application/json\" localhost:8000/summarize_translate -d '\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief\"'\nc'tait le meilleur des temps, c'tait le pire des temps .\n```\n\n----------------------------------------\n\nTITLE: Configuring Population Based Bandits (PB2) Scheduler\nDESCRIPTION: This snippet demonstrates how to set up the Population Based Bandits (PB2) scheduler, which uses Gaussian Process models instead of random perturbations to select new hyperparameter configurations. It shows how to specify the optimization metric, perturbation interval, and hyperparameter bounds.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.schedulers.pb2 import PB2\n\npb2_scheduler = PB2(\n    time_attr='time_total_s',\n    metric='mean_accuracy',\n    mode='max',\n    perturbation_interval=600.0,\n    hyperparam_bounds={\n        \"lr\": [1e-3, 1e-5],\n        \"alpha\": [0.0, 1.0],\n    ...    \n    }\n)\ntuner = tune.Tuner( ... , tune_config=tune.TuneConfig(scheduler=pb2_scheduler))\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Using Sequential Execution in Python\nDESCRIPTION: Demonstrates a sequential execution of a function that takes 1 second to complete, running four times. This baseline example takes approximately 4 seconds to complete, showing the performance without parallelization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\ndef do_some_work(x):\n    time.sleep(1) # Replace this with work you need to do.\n    return x\n\nstart = time.time()\nresults = [do_some_work(x) for x in range(4)]\nprint(\"duration =\", time.time() - start)\nprint(\"results =\", results)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset from Daft DataFrame - Ray\nDESCRIPTION: This snippet demonstrates how to create a Ray dataset from a Daft DataFrame using the `ray.data.from_daft` function. It includes executing a Daft query and showcasing the dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\nimport ray\n\nray.init()\n\ndf = daft.from_pydict({\"int_col\": [i for i in range(10000)], \"str_col\": [str(i) for i in range(10000)]})\nds = ray.data.from_daft(df)\n\nds.show(3)\n```\n\n----------------------------------------\n\nTITLE: Basic Ray Runtime Environment Example with Emoji Package\nDESCRIPTION: Demonstrates how to initialize Ray with a runtime environment that includes the emoji pip package and use it in a remote function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nruntime_env = {\"pip\": [\"emoji\"]}\n\nray.init(runtime_env=runtime_env)\n\n@ray.remote\ndef f():\n  import emoji\n  return emoji.emojize('Python is :thumbs_up:')\n\nprint(ray.get(f.remote()))\n```\n\n----------------------------------------\n\nTITLE: Helper Functions Implementation\nDESCRIPTION: Utility functions for data loading and model evaluation, including thread-safe data downloading and accuracy calculation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_parameter_server.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom filelock import FileLock\nimport numpy as np\n\nimport ray\n\n\ndef get_data_loader():\n    \"\"\"Safely downloads data. Returns training/validation set dataloader.\"\"\"\n    mnist_transforms = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n\n    # We add FileLock here because multiple workers will want to\n    # download data, and this may cause overwrites since\n    # DataLoader is not threadsafe.\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_loader = torch.utils.data.DataLoader(\n            datasets.MNIST(\n                \"~/data\", train=True, download=True, transform=mnist_transforms\n            ),\n            batch_size=128,\n            shuffle=True,\n        )\n        test_loader = torch.utils.data.DataLoader(\n            datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n            batch_size=128,\n            shuffle=True,\n        )\n    return train_loader, test_loader\n\n\ndef evaluate(model, test_loader):\n    \"\"\"Evaluates the accuracy of the model on a validation dataset.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(test_loader):\n            # This is only set to finish evaluation faster.\n            if batch_idx * len(data) > 1024:\n                break\n            outputs = model(data)\n            _, predicted = torch.max(outputs.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n    return 100.0 * correct / total\n```\n\n----------------------------------------\n\nTITLE: Installing RLlib and PyTorch\nDESCRIPTION: This code snippet demonstrates how to install RLlib and PyTorch using pip. RLlib relies on Ray, a library for distributed programming, and PyTorch for machine learning tasks. It is tailored for environments supported by Apple Silicon as well as for those requiring additional tools like Gymnasium for specific RL environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[rllib]\" torch\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Deployment Logging Settings\nDESCRIPTION: Example demonstrating how to configure different logging settings for multiple deployments in a Ray Serve application.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(logging_config={\"level\": \"DEBUG\"})\nclass Router:\n    def __call__(self):\n        logger = logging.getLogger(\"ray.serve\")\n        logger.debug(\"This debug message is from the router.\")\n        return serve.get_deployment(\"Model\").remote()\n\n@serve.deployment()\nclass Model:\n    def __call__(self):\n        return \"hello\"\n\nserve.run(\n    Router.bind(),\n    Model.bind(),\n    logging_config={\"encoding\": \"JSON\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Convert DeploymentResponse to Ray ObjectRef\nDESCRIPTION: This Python code shows how to convert a DeploymentResponse to a Ray ObjectRef using the _to_object_ref developer API. This allows you to integrate DeploymentHandle calls with Ray Actors or Tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/model_composition.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import serve\n\n@serve.deployment\nclass MyDeployment:\n    def __call__(self):\n        return \"hello world\"\n\nif __name__ == \"__main__\":\n    ray.init(ignore_reinit_error=True)\n\n    MyDeployment.deploy()\n\n    handle = serve.get_deployment(\"MyDeployment\").get_handle()\n    response = handle.remote()\n    object_ref = response._to_object_ref()\n\n    print(ray.get(object_ref))\n\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Ray Serve Service on Kubernetes\nDESCRIPTION: This command sets up port forwarding to access the Ray Serve service on localhost:8000 for sending requests to the Stable Diffusion model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/tpu-serve-stable-diffusion.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl port-forward svc/stable-diffusion-tpu-serve-svc 8000\n```\n\n----------------------------------------\n\nTITLE: Running DreamerV3 on DeepMind Control Suite - Shell Command\nDESCRIPTION: Command line instruction to run DreamerV3 on DeepMind Control Suite environments with vision-based observations.\nSOURCE: https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamerv3/README.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd ray/rllib/tuned_examples/dreamerv3/\npython dm_control_suite_vision.py --env DMC/cartpole/swingup\n```\n\n----------------------------------------\n\nTITLE: Generating Large Tensor Dataset with Single Block in Python\nDESCRIPTION: This code snippet shows how to generate a large tensor dataset using Ray Data, demonstrating that even with a single specified block, the output may be split into multiple blocks due to size constraints.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Pretend there are two CPUs.\nray.init(num_cpus=2)\n\n# Generate ~400MB of data.\nds = ray.data.range_tensor(5_000, shape=(10_000, ), override_num_blocks=1)\nprint(ds.materialize())\n```\n\n----------------------------------------\n\nTITLE: Initializing PPOConfig\nDESCRIPTION: This snippet initializes a PPOConfig object, which is used to configure the PPO algorithm. This is typically the first step in setting up an RLlib experiment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"from ray.rllib.algorithms.ppo import PPOConfig\nconfig = PPOConfig()\"\n```\n\n----------------------------------------\n\nTITLE: Testing GCS Bucket Access with Ray\nDESCRIPTION: A Python script that tests both reading from and writing to a Google Cloud Storage bucket from a Ray task. The script initializes Ray, defines a remote function that interacts with GCS, and executes it.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport os\nfrom google.cloud import storage\n\nGCP_GCS_BUCKET = \"my-bucket\"\nGCP_GCS_FILE = \"test_file.txt\"\n\nray.init(address=\"auto\")\n\n@ray.remote\ndef check_gcs_read_write():\n    client = storage.Client()\n    bucket = client.get_bucket(GCP_GCS_BUCKET)\n    blob = bucket.blob(GCP_GCS_FILE)\n\n    # Write to the bucket\n    blob.upload_from_string(\"Hello, Ray on GKE!\")\n\n    # Read from the bucket\n    content = blob.download_as_text()\n\n    return content\n\nresult = ray.get(check_gcs_read_write.remote())\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Loading Tune Experiment Results from Directory\nDESCRIPTION: Demonstrates how to load experiment results from a directory using a restored Tuner object, useful for analysis after the initial training script has exited.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune_analyze_results.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nexperiment_path = os.path.join(storage_path, exp_name)\nprint(f\"Loading results from {experiment_path}...\")\n\nrestored_tuner = tune.Tuner.restore(experiment_path, trainable=train_mnist)\nresult_grid = restored_tuner.get_results()\n```\n\n----------------------------------------\n\nTITLE: Defining Node Types for Ray Cluster in YAML\nDESCRIPTION: This YAML structure outlines the configuration for different node types in a Ray cluster, including node-specific settings, resources, and Docker configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n<node_type_1_name>:\n    node_config:\n        Node config\n    resources:\n        Resources\n    min_workers: int\n    max_workers: int\n    worker_setup_commands:\n        - str\n    docker:\n        Node Docker\n<node_type_2_name>:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Ray Cluster Node Management\nDESCRIPTION: Demonstrates how to add and remove nodes from a Ray cluster, including resource verification, useful for testing cluster scaling and failure handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/testing-tips.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmock_node = cluster.add_node(num_cpus=10)\n\nassert ray.cluster_resources()[\"CPU\"] == 20\n\ncluster.remove_node(mock_node)\n\nassert ray.cluster_resources()[\"CPU\"] == 10\n```\n\n----------------------------------------\n\nTITLE: Configuring Resources for Tune Trials\nDESCRIPTION: This code configures the resources allocated per trial, which helps Tune control parallelism across the cluster. It specifies CPU, GPU, and memory requirements for each trial.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-run.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Configure per-trial resources\nresources_per_trial = {\n    \"cpu\": 1,          # Number of CPUs to reserve per trial\n    \"gpu\": 0,          # Number of GPUs to reserve per trial\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU Profiling with Nsight Default Configuration in Ray\nDESCRIPTION: Python code demonstrating how to enable GPU profiling using Nsight with default options through Ray's runtime environment on a GPU-enabled Ray Actor. This configuration will automatically apply the default Nsight profiling settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/profiling.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport ray\n\nray.init()\n\n@ray.remote(num_gpus=1, runtime_env={ \"nsight\": \"default\"})\nclass RayActor:\n    def run():\n    a = torch.tensor([1.0, 2.0, 3.0]).cuda()\n    b = torch.tensor([4.0, 5.0, 6.0]).cuda()\n    c = a * b\n\n    print(\"Result on GPU:\", c)\n\nray_actor = RayActor.remote()\n# The Actor or Task process runs with : \"nsys profile [default options] ...\"\nray.get(ray_actor.run.remote())\n```\n\n----------------------------------------\n\nTITLE: Creating Pending Task References in Ray Python\nDESCRIPTION: This snippet demonstrates creating a pending task reference by submitting a task that depends on an object created via ray.put().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/memory-management.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef f(arg):\n    while True:\n        pass\n\na = ray.put(None)\nb = f.remote(a)\n```\n\n----------------------------------------\n\nTITLE: Better Approach #2: Creating Large Objects Inside Remote Functions\nDESCRIPTION: This approach demonstrates creating the large object inside the remote function using a lambda method, which is useful for unserializable objects or when object store is not suitable.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/closure-capture-large-objects.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport numpy as np\n\nray.init()\n\n@ray.remote\ndef f(large_data_creator):\n    large_data = large_data_creator()\n    return large_data[0]\n\nfuture = f.remote(lambda: np.random.random((10000, 10000)))\nray.get(future)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Tune with Local Storage (Python)\nDESCRIPTION: This snippet illustrates how to configure Ray Tune to use a custom local storage path for a single-node experiment. It sets up the Tuner with a specified local storage path and experiment name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-storage.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\n\ntuner = tune.Tuner(\n    trainable,\n    run_config=tune.RunConfig(\n        storage_path=\"/tmp/custom/storage/path\",\n        name=\"experiment_name\",\n    )\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Ray State API Example\nDESCRIPTION: Demonstrates usage of Ray's state API with remote tasks and actors. Creates multiple tasks running for 300 seconds and instantiates actor objects to showcase state monitoring.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\nray.init(num_cpus=4)\n\n@ray.remote\ndef task_running_300_seconds():\n    print(\"Start!\")\n    time.sleep(300)\n\n@ray.remote\nclass Actor:\n    def __init__(self):\n        print(\"Actor created\")\n\n# Create 2 tasks\ntasks = [task_running_300_seconds.remote() for _ in range(2)]\n\n# Create 2 actors\nactors = [Actor.remote() for _ in range(2)]\n\nray.get(tasks)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Workflows with Recursion\nDESCRIPTION: Shows how to create dynamic workflows that generate new tasks at runtime, using the Fibonacci sequence as an example of recursive workflow implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/key-concepts.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef add(a: int, b: int) -> int:\n    return a + b\n\n@ray.remote\ndef fib(n: int) -> int:\n    if n <= 1:\n        return n\n    # return a continuation of a DAG\n    return workflow.continuation(add.bind(fib.bind(n - 1), fib.bind(n - 2)))\n\nassert workflow.run(fib.bind(10)) == 55\n```\n\n----------------------------------------\n\nTITLE: Configuring Observation and Action Spaces for MultiAgentEnv\nDESCRIPTION: Shows how to define observation and action spaces for different agents in a multi-agent environment, including static and dynamic space assignment strategies\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\n\ndef __init__(self, config=None):\n    super().__init__()\n    ...\n    self.observation_spaces = {\n        \"agent_1\": gym.spaces.Box(-1.0, 1.0, (4,), np.float32),\n        \"agent_2\": gym.spaces.Box(-1.0, 1.0, (3,), np.float32),\n    }\n    self.action_spaces = {\n        \"agent_1\": gym.spaces.Discrete(2),\n        \"agent_2\": gym.spaces.Box(0.0, 1.0, (1,), np.float32),\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Training with Ray Train in Python\nDESCRIPTION: This code snippet shows the core of the training function used with Ray Train, including data loading, iteration, and result reporting.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/dreambooth_finetuning.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_fn(config):\n    # ... (setup code omitted for brevity)\n\n    train_dataset = session.get_dataset_shard(\"train\")\n\n    for epoch in range(config[\"num_train_epochs\"]):\n        for step, batch in enumerate(train_dataset.iter_torch_batches()):\n            # ... (training loop code omitted for brevity)\n\n        session.report(\n            {\n                \"epoch\": epoch,\n                \"step\": step,\n                \"loss\": total_loss.item() / len(train_dataset),\n            }\n        )\n\n# ... (additional code omitted)\n```\n\nLANGUAGE: python\nCODE:\n```\nargs = train_arguments()\ntrainer = TorchTrainer(\n    train_fn,\n    train_loop_config=vars(args),\n    scaling_config=ScalingConfig(\n        use_gpu=True,\n        num_workers=args.num_workers,\n    ),\n    datasets={\"train\": train_dataset},\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Adjusting number of samples (smoke test)\nDESCRIPTION: Reduces the number of samples for smoke tests to shorten the execution time.  This is useful for quickly verifying the code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"# If 1000 samples take too long, you can reduce this number.\\n# We override this number here for our smoke tests.\\nnum_samples = 10\"\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network Architecture for CIFAR10 Classification\nDESCRIPTION: This code defines a simple convolutional neural network (Net) for classifying CIFAR10 images. The network consists of two convolutional layers followed by three fully connected layers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/tune_cifar_torch_pbt_example.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Parallel Implementation Imports\nDESCRIPTION: Import statements for implementing parallel model execution with Ray Serve\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/gradio-integration.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.gradio_integrations import GradioIngress\nimport gradio as gr\nfrom transformers import pipeline\n```\n\n----------------------------------------\n\nTITLE: Adding Ray Train Callback to Hugging Face Transformers\nDESCRIPTION: Code snippet showing how to add the RayTrainReportCallback to a Hugging Face Transformers Trainer to enable reporting metrics and checkpoints to Ray Train.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-transformers.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\nfrom ray.train.huggingface.transformers import RayTrainReportCallback\n\ndef train_func():\n    ...\n    trainer = transformers.Trainer(...)\n    trainer.add_callback(RayTrainReportCallback())\n    ...\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding and Service Status Check\nDESCRIPTION: Commands to check RayService status and set up port forwarding for the Serve endpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/stable-diffusion-rayservice.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl describe rayservices.ray.io stable-diffusion\n\nkubectl port-forward svc/stable-diffusion-serve-svc 8000\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Call Trace with Complete TCP/IP Receive Path\nDESCRIPTION: Call stack trace showing the Netty NIO event loop with the complete TCP/IP receive path in the kernel. This trace illustrates the full journey of a network packet from Java through the kernel networking stack, including TCP protocol handling for received packets.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_14\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Manual Actor Termination via Handle in C++\nDESCRIPTION: Demonstrates killing a Ray actor in C++ using the Kill() method. This bypasses normal C++ exit handlers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/terminating-actors.rst#2025-04-12_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\nactor_handle.Kill();\n```\n\n----------------------------------------\n\nTITLE: Testing cross-language serialization from Java to Python\nDESCRIPTION: Demonstrates how to pass various data types from Java to Python and back, testing the cross-language serialization capabilities of Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_9\n\nLANGUAGE: java\nCODE:\n```\npackage io.ray.demo;\n\nimport io.ray.api.ObjectRef;\nimport io.ray.api.Ray;\nimport io.ray.api.function.PyFunction;\nimport java.math.BigInteger;\nimport org.testng.Assert;\n\npublic class SerializationDemo {\n\n  public static void main(String[] args) {\n    Ray.init();\n\n    Object[] inputs = new Object[]{\n        true,  // Boolean\n        Byte.MAX_VALUE,  // Byte\n        Short.MAX_VALUE,  // Short\n        Integer.MAX_VALUE,  // Integer\n        Long.MAX_VALUE,  // Long\n        BigInteger.valueOf(Long.MAX_VALUE),  // BigInteger\n        \"Hello World!\",  // String\n        1.234f,  // Float\n        1.234,  // Double\n        \"example binary\".getBytes()};  // byte[]\n    for (Object o : inputs) {\n      ObjectRef res = Ray.task(\n          PyFunction.of(\"ray_serialization\", \"py_return_input\", o.getClass()),\n          o).remote();\n      Assert.assertEquals(res.get(), o);\n    }\n\n    Ray.shutdown();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining an Objective Function for Tuning in Python\nDESCRIPTION: This snippet defines the objective function that will be optimized. It evaluates the score based on multiple steps and reports the score back to Tune for optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef objective(config):\n    for step in range(config[\"steps\"]):\n        score = evaluate(step, config[\"width\"], config[\"height\"], config[\"activation\"])\n        tune.report({\"iterations\": step, \"mean_loss\": score})\n```\n\n----------------------------------------\n\nTITLE: Loading Tokenizer for Llama Pre-training\nDESCRIPTION: Defines a function to load a tokenizer from Hugging Face for the Llama model. Uses the AutoTokenizer class from the transformers library with specific configuration options.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef load_tokenizer(config):\n    name = config[\"name\"]\n    tokenizer_kwargs = {\n        \"cache_dir\": None,\n        \"use_fast\": True,\n        \"revision\": \"main\",\n        \"token\": None,\n        \"trust_remote_code\": False,\n    }\n    return transformers.AutoTokenizer.from_pretrained(name, **tokenizer_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Implementing Setup Method for TorchRLModule in Python\nDESCRIPTION: Example of implementing the setup method for a custom TorchRLModule. It shows how to access attributes like observation_space and action_space, and how to build layers for the RLModule's forward passes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom ray.rllib.core.rl_module.torch.torch_rl_module import TorchRLModule\n\nclass MyTorchPolicy(TorchRLModule):\n    def setup(self):\n        # You have access here to the following already set attributes:\n        # self.observation_space\n        # self.action_space\n        # self.inference_only\n        # self.model_config  # <- a dict with custom settings\n\n        # Use the observation space (if a Box) to infer the input dimension.\n        input_dim = self.observation_space.shape[0]\n\n        # Use the model_config dict to extract the hidden dimension.\n        hidden_dim = self.model_config[\"fcnet_hiddens\"][0]\n\n        # Use the action space to infer the number of output nodes.\n        output_dim = self.action_space.n\n\n        # Build all the layers and subcomponents here you need for the\n        # RLModule's forward passes.\n        self._pi_head = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, output_dim),\n        )\n```\n\n----------------------------------------\n\nTITLE: Modifying Storage Unit and Interacting with Custom Buffer\nDESCRIPTION: This code example demonstrates how to modify the storage unit of a replay buffer and interact with a custom buffer in RLlib. It shows how to define a custom storage unit, configure the replay buffer to use it, and then add and sample experiences based on the modified storage unit.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-replay-buffers.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\n\nfrom ray.rllib.utils.replay_buffers import ReplayBuffer, StorageUnit\n\n\nclass MyStorageUnit(StorageUnit):\n    def add(self, **kwargs):\n        print(\"called add in MyStorageUnit!\")\n        super().add(**kwargs)\n\n\nif __name__ == \"__main__\":\n    # Setup the buffer.\n    replay_buffer = ReplayBuffer(capacity=100, storage_unit=\"episodes\")\n\n    # Add some data to the buffer.\n    for i in range(10):\n        transition = {\n            \"obs\": np.array([i]),\n            \"actions\": np.array([i]),\n            \"rewards\": np.array([i]),\n            \"next_obs\": np.array([i + 1]),\n            \"dones\": np.array([i % 2]),\n            \"truncateds\": np.array([i % 2]),\n        }\n        replay_buffer.add(**transition)\n\n    # Sample the data.\n    sample = replay_buffer.sample(num_items=5)\n\n    # Print the sample.\n    print(sample)\n```\n\n----------------------------------------\n\nTITLE: Creating and Retrieving Job-Scoped Named Actors in C++\nDESCRIPTION: Shows how to create an actor with a job-scope-unique name and retrieve it later using Ray in C++. The actor name is only valid within the job and cannot be accessed from another job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/named-actors.rst#2025-04-12_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n// Create an actor with a job-scope-unique name\nActorHandle<Counter> counter = ray::Actor(CreateCounter).SetName(\"some_name\").Remote();\n\n...\n\n// Retrieve the actor later somewhere in the same job\nboost::optional<ray::ActorHandle<Counter>> counter = ray::GetActor(\"some_name\");\n```\n\n----------------------------------------\n\nTITLE: Configuring PodMonitor for Ray Head Node Metrics\nDESCRIPTION: YAML configuration for a PodMonitor to collect metrics from Ray head nodes, including relabeling rules for better metric identification.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  labels:\n    # `release: $HELM_RELEASE`: Prometheus can only detect PodMonitor with this label.\n    release: prometheus\n  name: ray-head-monitor\n  namespace: prometheus-system\nspec:\n  jobLabel: ray-head\n  # Only select Kubernetes Pods in the \"default\" namespace.\n  namespaceSelector:\n    matchNames:\n      - default\n  # Only select Kubernetes Pods with \"matchLabels\".\n  selector:\n    matchLabels:\n      ray.io/node-type: head\n  # A list of endpoints allowed as part of this PodMonitor.\n  podMetricsEndpoints:\n    - port: metrics\n      relabelings:\n        - action: replace\n          sourceLabels:\n            - __meta_kubernetes_pod_label_ray_io_cluster\n          targetLabel: ray_io_cluster\n    - port: as-metrics # autoscaler metrics\n      relabelings:\n        - action: replace\n          sourceLabels:\n            - __meta_kubernetes_pod_label_ray_io_cluster\n          targetLabel: ray_io_cluster\n    - port: dash-metrics # dashboard metrics\n      relabelings:\n        - action: replace\n          sourceLabels:\n            - __meta_kubernetes_pod_label_ray_io_cluster\n          targetLabel: ray_io_cluster\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Object Ownership in Ray (Python)\nDESCRIPTION: This code snippet illustrates how object ownership works in Ray. It shows that the owner of an object is the worker process that creates the original ObjectRef, which is distinct from the worker that creates the value of the object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/objects.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\n\n@ray.remote\ndef f():\n    return 1\n\n@ray.remote\ndef g(x):\n    return x\n\n# Worker 1 (driver)\nx_id = f.remote()        # x_id is owned by worker 1\n\n# Worker 2\ny_id = g.remote(x_id)    # y_id is owned by worker 1, not worker 2\n\n# Worker 1 (driver)\nz_id = ray.put(2)        # z_id is owned by worker 1\n\nray.get([x_id, y_id, z_id])\n```\n\n----------------------------------------\n\nTITLE: Actor Method Calling Examples\nDESCRIPTION: Shows how to call actor methods using the remote operator and retrieve results using get() across languages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nobj_ref = counter.increment.remote()\nprint(ray.get(obj_ref))\n```\n\nLANGUAGE: java\nCODE:\n```\nObjectRef<Integer> objectRef = counter.task(&Counter::increment).remote();\nAssert.assertTrue(objectRef.get() == 1);\n```\n\nLANGUAGE: c++\nCODE:\n```\nauto object_ref = counter.Task(&Counter::increment).Remote();\nassert(*object_ref.Get() == 1);\n```\n\n----------------------------------------\n\nTITLE: Training Setup Configuration\nDESCRIPTION: Setup for parallel training including model initialization and training parameters\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_pong_example.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\niterations = 20\nbatch_size = 4\nmodel = Model()\nactors = [RolloutWorker.remote() for _ in range(batch_size)]\n\nrunning_reward = None\n# \"Xavier\" initialization.\n# Update buffers that add up gradients over a batch.\ngrad_buffer = {k: np.zeros_like(v) for k, v in model.weights.items()}\n```\n\n----------------------------------------\n\nTITLE: Logging Custom Loss in TorchLearner with Python\nDESCRIPTION: This snippet shows how to log a custom loss value within a loss computation method of a custom learner class in RLlib's TorchLearner. It demonstrates the use of self.metrics to log specific loss values for immediate feedback on loss calculations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n@override(TorchLearner)\\ndef compute_loss_for_module(self, *, module_id, config, batch, fwd_out):\\n    ...\\n\\n    loss_xyz = ...\\n\\n    # Log a specific loss term.\\n    self.metrics.log_value(\\\"special_loss_term\\\", loss_xyz, window=1)\\n\\n    total_loss = loss_abc + loss_xyz\\n\\n    return total_loss\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorflowTrainer Setup\nDESCRIPTION: Demonstrates basic setup of TensorflowTrainer with scaling configuration for distributed training. Shows how to initialize trainer with number of workers and GPU settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/distributed-tensorflow-keras.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\nfrom ray.train.tensorflow import TensorflowTrainer\n# For GPU Training, set `use_gpu` to True.\nuse_gpu = False\ntrainer = TensorflowTrainer(\n    train_func,\n    scaling_config=ScalingConfig(use_gpu=use_gpu, num_workers=2)\n)\n```\n\n----------------------------------------\n\nTITLE: Managing Fault Tolerance in Ray Tune Trials in Python\nDESCRIPTION: Shows configuration for enabling fault tolerance on the Ray Tune side to handle potential crashes of Ray Train driver processes. This feature ensures robustness in distributed training environments by configuring failure scenarios.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/hyperparameter-optimization.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/train_tune_interop.py\n    :language: python\n    :start-after: __fault_tolerance_start__\n    :end-before: __fault_tolerance_end__\n```\n\n----------------------------------------\n\nTITLE: Deploying a RayCluster with Queue Configuration\nDESCRIPTION: Commands to download and apply a RayCluster configuration that uses the Volcano scheduler with a specific queue.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n# Path: kuberay/ray-operator/config/samples\n# Includes  the `ray.io/scheduler-name: volcano` and `volcano.sh/queue-name: kuberay-test-queue` labels in the metadata.labels\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-cluster.volcano-scheduler-queue.yaml\nkubectl apply -f ray-cluster.volcano-scheduler-queue.yaml\n```\n\n----------------------------------------\n\nTITLE: League-Based Self-Play with OpenSpiel\nDESCRIPTION: Utilizes OpenSpiel to demonstrate league-based self-play, where agents compete against various versions of themselves to enhance learning through competitive interaction.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Self-play, league-based, with OpenSpiel\n# This script demonstrates how agents can improve through competition.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Returning Large Values Using Generators in Ray\nDESCRIPTION: This snippet shows how to rewrite the previous function as a generator, yielding one large numpy array at a time. This approach reduces heap memory usage by allowing the worker to free memory after each yield.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/generators.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef large_values_generator(num_returns):\n    for i in range(num_returns):\n        yield np.ones((25000, 1000), dtype=np.float32)\n        print(f\"({large_values_generator.name} pid={os.getpid()}) yielded return value {i}\")\n```\n\n----------------------------------------\n\nTITLE: Running Java Application in Multi-Node Ray Cluster\nDESCRIPTION: These commands start a Java application with a classpath and specified Ray address in a multi-node Ray cluster environment. The code search path for JAR files is set to enable worker nodes to load necessary classes. This setup is essential for operations requiring distributed computing over multiple nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/configure.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath <classpath> \\\n    -Dray.address=<address> \\\n    -Dray.job.code-search-path=/path/to/jars/ \\\n    <classname> <args>\n```\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath <classpath> \\\n    -Dray.address=<address> \\\n    -Dray.job.code-search-path=/path/to/jars1:/path/to/jars2:/path/to/pys1:/path/to/pys2 \\\n    <classname> <args>\n```\n\n----------------------------------------\n\nTITLE: Configuring Locality Settings in Ray Data\nDESCRIPTION: Example showing how to enable locality with output for ML ingest use cases.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nctx.execution_options.locality_with_output = True\n```\n\n----------------------------------------\n\nTITLE: Deploying Stable Diffusion with Ray Serve on Habana Gaudi\nDESCRIPTION: Creates a Ray Serve application for serving a fine-tuned Stable Diffusion model on Habana Gaudi hardware. The code defines two deployments: an API ingress that handles HTTP requests and a GaudiStableDiffusion deployment that loads the model and generates images. The application exposes an endpoint for image generation based on text prompts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/sd.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom optimum.habana.diffusers import GaudiStableDiffusionPipeline\nfrom io import BytesIO\nfrom fastapi import FastAPI\nfrom fastapi.responses import Response\n\nfrom ray import serve\nfrom ray.serve.handle import DeploymentHandle\n\n\napp = FastAPI()\n\n\n@serve.deployment(num_replicas=1)\n@serve.ingress(app)\nclass APIIngress:\n    def __init__(self, diffusion_model_handle: DeploymentHandle) -> None:\n        self.handle = diffusion_model_handle\n\n    @app.get(\n        \"/imagine\",\n        responses={200: {\"content\": {\"image/png\": {}}}},\n        response_class=Response,\n    )\n    async def generate(self, prompt: str, img_size: int = 512):\n        assert len(prompt), \"prompt parameter cannot be empty\"\n\n        image = await self.handle.generate.remote(prompt, img_size=img_size)\n        file_stream = BytesIO()\n        image.save(file_stream, \"PNG\")\n        return Response(content=file_stream.getvalue(), media_type=\"image/png\")\n\n\n@serve.deployment(\n    ray_actor_options={\"resources\": {\"HPU\": 1}}\n)\nclass GaudiStableDiffusion:\n    def __init__(self, model_id):\n        self.pipe = GaudiStableDiffusionPipeline.from_pretrained(\n            model_id,\n            torch_dtype=torch.bfloat16,\n            use_habana=True,\n            use_hpu_graphs=True,\n            gaudi_config=\"Habana/stable-diffusion\",\n        )\n\n    def generate(self, prompt: str, img_size: int = 512):\n        assert len(prompt), \"prompt parameter cannot be empty\"\n\n        image = self.pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n        return image\n\n\nentrypoint = APIIngress.bind(GaudiStableDiffusion.bind(\"/tmp/textual_inversion_cat/\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Constructor for Custom FileBasedDatasource in Python\nDESCRIPTION: This snippet shows how to implement the constructor for a custom FileBasedDatasource class. It calls the superclass constructor and specifies the files to read and valid file extensions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/custom-datasource-example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, files: List[str]):\n    super().__init__(\n        files,\n        file_extensions=[\".jpg\", \".jpeg\", \".png\", \".bmp\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Running Ray Tune with Class-based Trainable and Wandb Integration\nDESCRIPTION: This function demonstrates how to run a Tune experiment using the WandbTrainable class.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-wandb.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef tune_trainable():\n    \"\"\"Example for using a WandTrainableMixin with the class API\"\"\"\n    tuner = tune.Tuner(\n        WandbTrainable,\n        tune_config=tune.TuneConfig(\n            metric=\"loss\",\n            mode=\"min\",\n        ),\n        param_space={\n            \"mean\": tune.grid_search([1, 2, 3, 4, 5]),\n            \"sd\": tune.uniform(0.2, 0.8),\n        },\n    )\n    results = tuner.fit()\n\n    return results.get_best_result().config\n```\n\n----------------------------------------\n\nTITLE: Converting Ray Dataset to Daft DataFrame\nDESCRIPTION: Demonstrates converting a Ray Dataset to a Daft DataFrame using the to_daft() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/saving-data.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\ndf = ds.to_daft()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Default Scheduling Strategy in Ray\nDESCRIPTION: This snippet shows how to use the default scheduling strategy in Ray. It creates a remote function and calls it, allowing Ray to use its default scheduling mechanism.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef f():\n    return 1\n\nray.get(f.remote())\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with working directory\nDESCRIPTION: This snippet initializes Ray with a runtime environment that specifies a working directory. The 'working_dir' parameter sets the working directory for the Ray job to '/tmp/runtime_env_working_dir', ensuring tasks and actors will have access to files within that directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Specify a runtime environment for the entire Ray job\nray.init(runtime_env={\"working_dir\": \"/tmp/runtime_env_working_dir\"})\n```\n\n----------------------------------------\n\nTITLE: Testing Weight Transfer Verification in RLlib\nDESCRIPTION: Test code that verifies the successful transfer of weights between policies by comparing the state of the transferred policy with the original weights.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/checkpoints.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.utils.test_utils import check\n\n_weights_check_2 = multi_agent_config.build().get_module(\"p1\").get_state()\ncheck(_weights_check, _weights_check_2)\n```\n\n----------------------------------------\n\nTITLE: Local Files Setup for Ray Runtime Environment\nDESCRIPTION: Example showing how to set up local directories for use with Ray runtime environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\n\nos.makedirs(\"/tmp/runtime_env_working_dir\", exist_ok=True)\n```\n\n----------------------------------------\n\nTITLE: Metadata for Running Workflows in Python\nDESCRIPTION: Example showing how to retrieve and work with metadata for running workflows. This demonstrates that metadata returns an immediate result reflecting the current state of the workflow, with some fields only available after workflow completion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/metadata.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\n@ray.remote\ndef simple():\n    time.sleep(1000)\n    return 0\n\nworkflow.run_async(simple.bind(), workflow_id=\"workflow_id\")\n\n# make sure workflow task starts running\ntime.sleep(2)\n\nworkflow_metadata = workflow.get_metadata(\"workflow_id\")\nassert workflow_metadata[\"status\"] == \"RUNNING\"\nassert \"start_time\" in workflow_metadata[\"stats\"]\nassert \"end_time\" not in workflow_metadata[\"stats\"]\n\nworkflow.cancel(\"workflow_id\")\n\nworkflow_metadata = workflow.get_metadata(\"workflow_id\")\nassert workflow_metadata[\"status\"] == \"CANCELED\"\nassert \"start_time\" in workflow_metadata[\"stats\"]\nassert \"end_time\" not in workflow_metadata[\"stats\"]\n```\n\n----------------------------------------\n\nTITLE: Internal Actor Termination in C++\nDESCRIPTION: Shows how to terminate a Ray actor from within using ray::ExitActor() in C++. This is currently the only way to gracefully terminate C++ actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/terminating-actors.rst#2025-04-12_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\nray::ExitActor();\n```\n\n----------------------------------------\n\nTITLE: Client Script for Querying Batchbot and Streaming Results in Python\nDESCRIPTION: This code provides a client script to query the Batchbot and stream the results, demonstrating how to interact with the deployed model and handle the streamed output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nprompt = \"Tell me a joke\"\nwith requests.post(\n    \"http://localhost:8000/batchbot\",\n    params={\"prompt\": prompt},\n    stream=True,\n) as r:\n    for chunk in r.iter_content(chunk_size=None, decode_unicode=True):\n        print(chunk, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Starting Anyscale Session and Running Workload\nDESCRIPTION: Commands to start an Anyscale session and run a specific workload. The workload name and Ray wheel link are required parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/long_running_tests/README.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ anyscale start\n$ anyscale run test_workload --workload=<WORKLOAD_NAME> --wheel=<RAY_WHEEL_LINK>\n```\n\n----------------------------------------\n\nTITLE: Managing Named Actors in Namespaces\nDESCRIPTION: Demonstration of how named actors are scoped to their namespaces, showing actor creation and access across different namespace contexts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/namespaces.rst#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Job 1 creates two actors, \"orange\" and \"purple\" in the \"colors\" namespace.\nSystem.setProperty(\"ray.address\", \"localhost:10001\");\nSystem.setProperty(\"ray.job.namespace\", \"colors\");\ntry {\n    Ray.init();\n    Ray.actor(Actor::new).setName(\"orange\").remote();\n    Ray.actor(Actor::new).setName(\"purple\").remote();\n} finally {\n    Ray.shutdown();\n}\n```\n\nLANGUAGE: c++\nCODE:\n```\n// Job 1 creates two actors, \"orange\" and \"purple\" in the \"colors\" namespace.\nray::RayConfig config;\nconfig.ray_namespace = \"colors\";\nray::Init(config);\nray::Actor(RAY_FUNC(Counter::FactoryCreate)).SetName(\"orange\").Remote();\nray::Actor(RAY_FUNC(Counter::FactoryCreate)).SetName(\"purple\").Remote();\nray::Shutdown();\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Objective Optimization in Python\nDESCRIPTION: This snippet initializes the Optuna search algorithm for multi-objective optimization by specifying multiple metrics and modes for the optimization process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nsearcher = OptunaSearch(metric=[\"loss\", \"gain\"], mode=[\"min\", \"max\"])\nalgo = ConcurrencyLimiter(searcher, max_concurrent=4)\n```\n\n----------------------------------------\n\nTITLE: Setting Checkpoint at End with Class API in Ray Tune\nDESCRIPTION: Shows how to enable final checkpointing at the end of a trial by setting checkpoint_at_end=True in the checkpoint configuration. This ensures a checkpoint is created when the trial terminates successfully.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-trial-checkpoints.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    MyTrainable,\n    run_config=air.RunConfig(\n        checkpoint_config=CheckpointConfig(\n            checkpoint_frequency=5,\n            checkpoint_at_end=True,  # take a checkpoint when the trial ends\n            num_to_keep=2,\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Kueue Scheduler in Kubernetes\nDESCRIPTION: Command to install a specific version of Kueue, a Kubernetes-native job queueing system, using kubectl and applying the manifests from GitHub.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nVERSION=v0.6.0\nkubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/$VERSION/manifests.yaml\n```\n\n----------------------------------------\n\nTITLE: Loading Image Dataset from S3\nDESCRIPTION: Using Ray Data to load the Imagenette dataset from an S3 bucket into a Ray Dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/01_batch_inference/start.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\ns3_uri = \"s3://anonymous@air-example-data-2/imagenette2/train/\"\n\nds = ray.data.read_images(s3_uri, mode=\"RGB\")\nds\n```\n\n----------------------------------------\n\nTITLE: Installing RayService CR for TPU-enabled Stable Diffusion on Kubernetes\nDESCRIPTION: This command applies a YAML configuration to create a RayCluster with a single-host v4 TPU worker group of 2x2x1 topology for running Stable Diffusion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/tpu-serve-stable-diffusion.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-service.tpu-single-host.yaml\n```\n\n----------------------------------------\n\nTITLE: Initializing TorchTrainer with Checkpoint Recovery\nDESCRIPTION: Code snippet demonstrating how to configure TorchTrainer to automatically restore from the latest checkpoint if available, otherwise create a new trainer instance. Uses experiment_path to specify checkpoint storage location.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nexperiment_path = os.path.expanduser(\"/mnt/cluster_storage/finetune-resnet\")\nif TorchTrainer.can_restore(experiment_path):\n    trainer = TorchTrainer.restore(experiment_path,\n        train_loop_per_worker=train_loop_per_worker,\n        train_loop_config=train_loop_config,\n        scaling_config=scaling_config,\n        run_config=run_config,\n    )\nelse:\n    trainer = TorchTrainer(\n        train_loop_per_worker=train_loop_per_worker,\n        train_loop_config=train_loop_config,\n        scaling_config=scaling_config,\n        run_config=run_config,\n    )\n```\n\n----------------------------------------\n\nTITLE: Custom RL Environment Implementation\nDESCRIPTION: Example of implementing a custom RL environment by subclassing gymnasium.Env. Implements a 'ParrotEnv' where the agent learns to repeat observed values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\nclass ParrotEnv(gym.Env):\n    \"\"\"Environment in which the agent learns to repeat the seen observations.\n\n    Observations are float numbers indicating the to-be-repeated values,\n    e.g. -1.0, 5.1, or 3.2.\n    The action space is the same as the observation space.\n    Rewards are `r=-abs([observation] - [action])`, for all steps.\n    \"\"\"\n    def __init__(self, config=None):\n        # Since actions should repeat observations, their spaces must be the same.\n        self.observation_space = config.get(\n            \"obs_act_space\",\n            gym.spaces.Box(-1.0, 1.0, (1,), np.float32),\n        )\n        self.action_space = self.observation_space\n        self._cur_obs = None\n        self._episode_len = 0\n\n    def reset(self, *, seed=None, options=None):\n        \"\"\"Resets the environment, starting a new episode.\"\"\"\n        # Reset the episode len.\n        self._episode_len = 0\n        # Sample a random number from our observation space.\n        self._cur_obs = self.observation_space.sample()\n        # Return initial observation.\n        return self._cur_obs, {}\n\n    def step(self, action):\n        \"\"\"Takes a single step in the episode given `action`.\"\"\"\n        # Set `terminated` and `truncated` flags to True after 10 steps.\n        self._episode_len += 1\n        terminated = truncated = self._episode_len >= 10\n        # Compute the reward: `r = -abs([obs] - [action])`\n        reward = -sum(abs(self._cur_obs - action))\n        # Set a new observation (random sample).\n        self._cur_obs = self.observation_space.sample()\n        return self._cur_obs, reward, terminated, truncated, {}\n\n# Point your config to your custom env class:\nconfig = (\n    PPOConfig()\n    .environment(\n        ParrotEnv,\n        # Add `env_config={\"obs_act_space\": [some Box space]}` to customize.\n    )\n)\n\n# Build a PPO algorithm and train it.\nppo_w_custom_env = config.build_algo()\nppo_w_custom_env.train()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Rock-Paper-Scissors Game with Training\nDESCRIPTION: This script sets up a rock-paper-scissors game where two agents learn to play against each other, useful for performance evaluation in adversarial settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Rock-paper-scissors learned vs learned\n# This script trains agents to learn strategies to play against each other.\n# Useful for evaluating performance in simple adversarial settings.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Grid Search in Tune with Python\nDESCRIPTION: Shows how to specify a grid search for hyperparameters in a Tune experiment configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\n\ntune.run(\n    ...,\n    config={\n        \"param1\": tune.grid_search([1, 2, 3]),\n        \"param2\": tune.grid_search([4, 5, 6])\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Ray Mini-Cluster Setup and Testing\nDESCRIPTION: Shows how to create and use a mock multi-node Ray cluster for testing using cluster_utils.Cluster, including initialization and basic task execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/testing-tips.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.cluster_utils import Cluster\n\n# Starts a head-node for the cluster.\ncluster = Cluster(\n    initialize_head=True,\n    head_node_args={\n        \"num_cpus\": 10,\n    })\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Ray Pi Calculation\nDESCRIPTION: Imports necessary Python modules including Ray for parallel computing, math functions, and time for performance measurement.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/highly_parallel.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport random\nimport time\nimport math\nfrom fractions import Fraction\n```\n\n----------------------------------------\n\nTITLE: Visualizing Trial Progress with Early Stopping\nDESCRIPTION: Code to visualize the progress of multiple trials using the ASHAScheduler, showing how early stopping affects trial performance and duration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndfs = results.get_dataframe()\n\nfig, ax = plt.subplots(figsize=(12, 8))\ndfs.groupby([\"trial_id\"])[[\"training_iteration\", \"accuracy\"]].plot(\n    x=\"training_iteration\", y=\"accuracy\", ax=ax)\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Training Iteration\")\n```\n\n----------------------------------------\n\nTITLE: Ray Task Summary Command\nDESCRIPTION: CLI command to display summarized statistics of Ray tasks using the state API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\nray summary tasks\n```\n\n----------------------------------------\n\nTITLE: Training PPO Model with RLlib in Python\nDESCRIPTION: Demonstrates how to train a PPO (Proximal Policy Optimization) model using Ray's RLlib library. The code creates a PPO trainer, trains it for one iteration on the CartPole environment, and saves the checkpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport ray.rllib.agents.ppo as ppo\nfrom ray import serve\n\ndef train_ppo_model():\n    trainer = ppo.PPOTrainer(\n        config={\"framework\": \"torch\", \"num_workers\": 0},\n        env=\"CartPole-v0\",\n    )\n    # Train for one iteration\n    trainer.train()\n    trainer.save(\"/tmp/rllib_checkpoint\")\n    return \"/tmp/rllib_checkpoint/checkpoint_000001/checkpoint-1\"\n\n\ncheckpoint_path = train_ppo_model()\n```\n\n----------------------------------------\n\nTITLE: Post-Mortem Debugging in Ray\nDESCRIPTION: Sets up Ray with post-mortem debugging to automatically drop into the debugger when an exception occurs. This enables inspecting the state of the program at the moment the exception was raised without having to set breakpoints in advance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/ray-debugging.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(runtime_env={\"env_vars\": {\"RAY_DEBUG\": \"legacy\", \"RAY_DEBUG_POST_MORTEM\": \"1\"}})\n\n@ray.remote\ndef post_mortem(x):\n    x += 1\n    raise Exception(\"An exception is raised.\")\n    return x\n\nray.get(post_mortem.remote(10))\n```\n\n----------------------------------------\n\nTITLE: Checking Service Account Permissions\nDESCRIPTION: Command to verify if the ray-user service account has permission to access the RayCluster resource.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl auth can-i get rayclusters.ray.io/ray-cluster-with-auth --as=system:serviceaccount:default:ray-user\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Devices in PyTorch Lightning with Ray TorchTrainer\nDESCRIPTION: This diff shows how to properly configure GPU devices in a PyTorch Lightning Trainer when using Ray TorchTrainer. Instead of manually specifying devices, it demonstrates setting devices to 'auto' and using the 'auto' accelerator to let Ray handle GPU allocation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-pytorch-lightning.rst#2025-04-12_snippet_6\n\nLANGUAGE: diff\nCODE:\n```\nimport lightning.pytorch as pl\n\ndef train_func():\n    ...\n    trainer = pl.Trainer(\n        ...\n-        devices=[0,1,2,3],\n+        devices=\"auto\",\n+        accelerator=\"auto\",\n        ...\n    )\n    ...\n```\n\n----------------------------------------\n\nTITLE: Adding Ray Java Dependencies in Maven\nDESCRIPTION: Adds Ray Java API and Runtime dependencies to the pom.xml file. These dependencies are required for using Ray Java applications with Maven.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_19\n\nLANGUAGE: xml\nCODE:\n```\n\"<dependency>\\n  <groupId>io.ray</groupId>\\n  <artifactId>ray-api</artifactId>\\n  <version>${ray.version}</version>\\n</dependency>\\n<dependency>\\n  <groupId>io.ray</groupId>\\n  <artifactId>ray-runtime</artifactId>\\n  <version>${ray.version}</version>\\n</dependency>\"\n```\n\n----------------------------------------\n\nTITLE: Continue Training with Ray Tune and Restore Checkpoint in Python\nDESCRIPTION: Integrate custom callback code within Ray Tune to restore a state into an Algorithm during training. This allows seamless continuation of training with previous configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/checkpoints.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\n\nconfig.callbacks(\n    on_algorithm_init=(\n        lambda algorithm, _dir=checkpoint_dir, **kw: algorithm.restore_from_path(_dir)\n    ),\n)\n\nresults = tune.Tuner(\n    config.algo_class,\n    param_space=config,\n    run_config=tune.RunConfig(stop={\"num_env_steps_sampled_lifetime\": 8000})\n).fit()\n```\n\n----------------------------------------\n\nTITLE: Modifying a Ray Serve deployment with options()\nDESCRIPTION: This code snippet demonstrates how to modify a Ray Serve deployment using the `.options()` method.  It shows how to dynamically update parameters like `num_replicas` and `max_ongoing_requests` on an already-defined deployment. This allows for runtime adjustments to deployment settings without needing to redeploy the application.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/configure-serve-deployment.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# __options_start__\n\nmodel = model.options(num_replicas=5, max_ongoing_requests=200)\n\n# __options_end__\n```\n\n----------------------------------------\n\nTITLE: Grouping Agents in a Multi-Agent RLlib Environment using Python\nDESCRIPTION: This snippet demonstrates using RLlib's `with_agent_groups` method to group agents and assign policies. It supports creating Tuple spaces for collective action and observation, essential for centralized policy execution. Dependencies include the RLlib framework.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef with_agent_groups():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Setting End-to-End HTTP Request Timeout in Ray Serve\nDESCRIPTION: Shows how to configure an end-to-end request timeout using the `request_timeout_s` parameter in the `http_options` within the Serve config. This reduces the risk of slow requests blocking others and allows for client-side retries on timeout.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/performance.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nhttp_options: { request_timeout_s: 30 }\n```\n\n----------------------------------------\n\nTITLE: Creating Detached Actor Class in Python\nDESCRIPTION: Python script that defines and creates a detached Ray actor requiring 1 CPU. Used to demonstrate autoscaling behavior.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport sys\n\n@ray.remote(num_cpus=1)\nclass Actor:\n    pass\n\nray.init(namespace=\"default_namespace\")\nActor.options(name=sys.argv[1], lifetime=\"detached\").remote()\n```\n\n----------------------------------------\n\nTITLE: Implementing _read_stream for Custom FileBasedDatasource in Python\nDESCRIPTION: This snippet demonstrates how to implement the _read_stream method for a custom FileBasedDatasource. It reads image data from a file and yields blocks using a DelegatingBlockBuilder.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/custom-datasource-example.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _read_stream(\n    self,\n    f: \"pa.NativeFile\",\n    path: str,\n    **reader_args,\n):\n    import numpy as np\n    from PIL import Image\n\n    image = Image.open(f)\n    image_array = np.array(image)\n\n    builder = DelegatingBlockBuilder()\n    builder.add(\n        {\n            \"image\": image_array,\n            \"image_shape\": image_array.shape,\n            \"path\": path,\n        }\n    )\n    yield builder.build()\n```\n\n----------------------------------------\n\nTITLE: Scaling Out a Java Deployment\nDESCRIPTION: This snippet shows how to scale out a Ray Serve deployment by increasing the number of replicas using the Java API. It sets the `numReplicas` parameter to the desired number of replicas.  The deployment must already exist.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/managing-java-deployments.md#2025-04-12_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n    Serve.deployment(\"counter\").setNumReplicas(3).deploy(true);\n    System.out.println(\"\\\"counter\\\" deployment has been scaled out to 3 replicas.\");\n\n```\n\n----------------------------------------\n\nTITLE: Randomizing Block Order in Ray Dataset\nDESCRIPTION: This example shows how to randomize the order of blocks in a Ray Dataset using the randomize_block_order method. This operation requires materializing all blocks in memory before applying the randomization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/shuffling-data.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_text(\n    \"s3://anonymous@ray-example-data/sms_spam_collection_subset.txt\"\n)\n\n# Randomize the block order of this dataset.\nds = ds.randomize_block_order()\n```\n\n----------------------------------------\n\nTITLE: Compiling BERT Model for AWS NeuronCore\nDESCRIPTION: Loads and compiles a pre-trained emotion classification model from HuggingFace for AWS NeuronCore execution\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/aws-neuron-core-inference.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n__compile_neuron_code_start__\n__compile_neuron_code_end__\n```\n\n----------------------------------------\n\nTITLE: Creating Text Generation Pipeline\nDESCRIPTION: Initializes a HuggingFace text generation pipeline with the loaded model and configured tokenizer for text generation tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\n\ngenerator = pipeline(\n    \"text-generation\",\n    model=model,\n    device_map=device_map,\n    tokenizer=AutoTokenizer.from_pretrained(\n        MODEL_NAME, padding_side=\"left\", use_fast=False\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Custom LSTM-Based RLModule\nDESCRIPTION: Uses a custom LSTM architecture within an RLModule, enabling the processing of sequential data applicable to partially observable environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Custom LSTM-based RLModule\n# This script implements an LSTM for temporal sequence processing.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with RLlib Checkpoint\nDESCRIPTION: Demonstrates how to load a trained model from a checkpoint and run inference through a single episode. Uses PyTorch and Gymnasium for environment interaction.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nimport gymnasium as gym\nimport numpy as np\nimport torch\nfrom ray.rllib.core.rl_module import RLModule\n\n# Create only the neural network (RLModule) from our algorithm checkpoint.\n# See here (https://docs.ray.io/en/master/rllib/checkpoints.html)\n# to learn more about checkpointing and the specific \"path\" used.\nrl_module = RLModule.from_checkpoint(\n    Path(best_checkpoint.path)\n    / \"learner_group\"\n    / \"learner\"\n    / \"rl_module\"\n    / \"default_policy\"\n)\n\n# Create the RL environment to test against (same as was used for training earlier).\nenv = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n\nepisode_return = 0.0\ndone = False\n\n# Reset the env to get the initial observation.\nobs, info = env.reset()\n\nwhile not done:\n    # Uncomment this line to render the env.\n    # env.render()\n\n    # Compute the next action from a batch (B=1) of observations.\n    obs_batch = torch.from_numpy(obs).unsqueeze(0)  # add batch B=1 dimension\n    model_outputs = rl_module.forward_inference({\"obs\": obs_batch})\n\n    # Extract the action distribution parameters from the output and dissolve batch dim.\n    action_dist_params = model_outputs[\"action_dist_inputs\"][0].numpy()\n\n    # We have continuous actions -> take the mean (max likelihood).\n    greedy_action = np.clip(\n        action_dist_params[0:1],  # 0=mean, 1=log(stddev), [0:1]=use mean, but keep shape=(1,)\n        a_min=env.action_space.low[0],\n        a_max=env.action_space.high[0],\n    )\n    # For discrete actions, you should take the argmax over the logits:\n    # greedy_action = np.argmax(action_dist_params)\n\n    # Send the action to the environment for the next step.\n    obs, reward, terminated, truncated, info = env.step(greedy_action)\n\n    # Perform env-loop bookkeeping.\n    episode_return += reward\n    done = terminated or truncated\n\nprint(f\"Reached episode return of {episode_return}.\")\n```\n\n----------------------------------------\n\nTITLE: Allocating Resources for Modin Dataframes in Ray Tune\nDESCRIPTION: This example shows how to allocate additional CPU resources for trials using Modin dataframes to prevent resource starvation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.execution.placement_groups import PlacementGroupFactory\n\npgf = PlacementGroupFactory([\n    {\"CPU\": 1},  # for trainable\n    {\"CPU\": 1},  # for Modin tasks\n])\n\ntrainable_with_pgf = tune.with_resources(trainable, pgf)\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Ray Serve gRPC\nDESCRIPTION: Demonstrates how to handle gRPC errors using try-except blocks for RpcError exceptions. Shows proper error handling patterns for Ray Serve gRPC implementations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/grpc-guide.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:start-after: __begin_error_handle__\n:end-before: __end_error_handle__\n:language: python\n```\n\n----------------------------------------\n\nTITLE: Creating a Range Tensor with Ray Data in Python\nDESCRIPTION: This snippet demonstrates how to create a range tensor using Ray Data, specifying the shape as a 64x64 ndarray. It shows how to import the library, create the tensor, and print its schema.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\n\nds = ray.data.range_tensor(10, shape=(64, 64))\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Defining the Train Function - Python\nDESCRIPTION: This snippet defines a training function that simulates a training run by reporting a random loss value based on the provided configuration. This function will be used for tuning the hyperparameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-aim.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef train_function(config):\n    for _ in range(50):\n        loss = config[\"mean\"] + config[\"sd\"] * np.random.randn()\n        tune.report({\"loss\": loss})\n```\n\n----------------------------------------\n\nTITLE: Ray Compiled Graph Example for Optimized Remote Execution\nDESCRIPTION: Shows the Compiled Graph implementation for the same remote execution task with less than 50us overhead, demonstrating the static execution model and improved performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/ray-compiled-graph.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Compiled Graph for remote execution.\n# less than 50us overhead to invoke `recv` (during `graph.execute(data)`).\nwith InputNode() as inp:\n    graph = receiver.recv.bind(inp)\n\ngraph = graph.experimental_compile()\nref = graph.execute(data)\nray.get(ref)\n```\n\n----------------------------------------\n\nTITLE: Setting up Ray driver with code search path in Java\nDESCRIPTION: Shows how to set the code search path for the Ray driver in Java using command-line arguments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath <classpath> \\\n    -Dray.address=<address> \\\n    -Dray.job.code-search-path=/path/to/code/ \\\n    <classname> <args>\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray TorchTrainer for Hyperparameter Tuning\nDESCRIPTION: Creates a TorchTrainer instance to manage the distributed training process for hyperparameter tuning, using the previously defined training function and configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\n\n# Define a TorchTrainer without hyper-parameters for Tuner\nray_trainer = TorchTrainer(\n    train_func,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Helper Functions Implementation\nDESCRIPTION: Implementation of preprocessing, reward processing and rollout functions for the Pong environment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_pong_example.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess(img):\n    # Crop the image.\n    img = img[35:195]\n    # Downsample by factor of 2.\n    img = img[::2, ::2, 0]\n    # Erase background (background type 1).\n    img[img == 144] = 0\n    # Erase background (background type 2).\n    img[img == 109] = 0\n    # Set everything else (paddles, ball) to 1.\n    img[img != 0] = 1\n    return img.astype(float).ravel()\n\n\ndef process_rewards(r):\n    \"\"\"Compute discounted reward from a vector of rewards.\"\"\"\n    discounted_r = np.zeros_like(r)\n    running_add = 0\n    for t in reversed(range(0, r.size)):\n        # Reset the sum, since this was a game boundary (pong specific!).\n        if r[t] != 0:\n            running_add = 0\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r\n\n\ndef rollout(model, env):\n    \"\"\"Evaluates  env and model until the env returns \"Terminated\" or \"Truncated\".\n\n    Returns:\n        xs: A list of observations\n        hs: A list of model hidden states per observation\n        dlogps: A list of gradients\n        drs: A list of rewards.\n\n    \"\"\"\n    # Reset the game.\n    observation, info = env.reset()\n    # Note that prev_x is used in computing the difference frame.\n    prev_x = None\n    xs, hs, dlogps, drs = [], [], [], []\n    terminated = truncated = False\n    while not terminated and not truncated:\n        cur_x = preprocess(observation)\n        x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n        prev_x = cur_x\n\n        aprob, h = model.policy_forward(x)\n        # Sample an action.\n        action = 2 if np.random.uniform() < aprob else 3\n\n        # The observation.\n        xs.append(x)\n        # The hidden state.\n        hs.append(h)\n        y = 1 if action == 2 else 0  # A \"fake label\".\n        # The gradient that encourages the action that was taken to be\n        # taken (see http://cs231n.github.io/neural-networks-2/#losses if\n        # confused).\n        dlogps.append(y - aprob)\n\n        observation, reward, terminated, truncated, info = env.step(action)\n\n        # Record reward (has to be done after we call step() to get reward\n        # for previous action).\n        drs.append(reward)\n    return xs, hs, dlogps, drs\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Aim and Ray Tune - Python\nDESCRIPTION: This snippet imports the required libraries into the script. It prepares the environment for logging results using Aim within Ray's tuning framework.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-aim.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nimport ray\nfrom ray import tune\nfrom ray.tune.logger.aim import AimLoggerCallback\n```\n\n----------------------------------------\n\nTITLE: Deactivating New API Stack\nDESCRIPTION: This snippet shows how to deactivate the new API stack and switch back to the old one using the `api_stack()` method in the `AlgorithmConfig` object. This is useful for compatibility or debugging purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"config.api_stack(\n    enable_rl_module_and_learner=False,\n    enable_env_runner_and_connector_v2=False,\n)\"\n```\n\n----------------------------------------\n\nTITLE: Specifying GPU Type in Ray Train\nDESCRIPTION: Configures a specific accelerator type (A100 GPU) for training workers, ensuring workers run only on the specified GPU type in a heterogeneous cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nScalingConfig(\n    num_workers=1,\n    use_gpu=True,\n    accelerator_type=\"A100\"\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating DeepMind OpenSpiel with RLlib in Python\nDESCRIPTION: Provides a pattern for using the OpenSpiel library by DeepMind with RLlib through the `OpenSpielEnv` wrapper. The walkthrough includes setting up a Connect-Four environment using Ray's environment registration. Requires installation of OpenSpiel and RLlib.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pyspiel  # pip install open_spiel\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.env.wrappers.open_spiel import OpenSpielEnv\nfrom ray.tune.registry import register_env\n\nregister_env(\n    \"open_spiel_env\",\n    lambda cfg: OpenSpielEnv(pyspiel.load_game(\"connect_four\")),\n)\n\nconfig = PPOConfig().environment(\"open_spiel_env\")\n```\n\n----------------------------------------\n\nTITLE: Saving Tensor Data in NumPy Format\nDESCRIPTION: Shows how to save tensor data specifically as NumPy files, targeting a specific column.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-tensors.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\nds.write_numpy(\"/tmp/simple\", column=\"image\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Second RayCluster with Same Configuration\nDESCRIPTION: Command to create a second RayCluster with the same configuration but a different name, to demonstrate gang scheduling behavior.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n# Path: kuberay/ray-operator/config/samples\n# Includes the `ray.io/scheduler-name: volcano` and `volcano.sh/queue-name: kuberay-test-queue` labels in the metadata.labels\n# Replaces the name to test-cluster-1\nsed 's/test-cluster-0/test-cluster-1/' ray-cluster.volcano-scheduler-queue.yaml | kubectl apply -f-\n```\n\n----------------------------------------\n\nTITLE: Complete Configuration for RayCluster with Fluent Bit Logging Sidecar\nDESCRIPTION: This YAML configuration defines a complete setup for a single-pod RayCluster with a Fluent Bit sidecar container. It includes the ConfigMap for Fluent Bit configuration, volume mounts for sharing Ray logs, and container specifications with appropriate resource allocations for both the Ray head node and the logging sidecar.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.md#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Fluent Bit ConfigMap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentbit-config\ndata:\n  fluent-bit.conf: |\n    [INPUT]\n        Name tail\n        Path /tmp/ray/session_latest/logs/*\n        Tag ray\n        Path_Key true\n        Refresh_Interval 5\n    [FILTER]\n        Name modify\n        Match ray\n        Add POD_LABELS ${POD_LABELS}\n    [OUTPUT]\n        Name loki\n        Match *\n        Host loki-gateway\n        Port 80\n        Labels RayCluster=${POD_LABELS}\n        tenant_id test\n---\n# RayCluster CR with a FluentBit sidecar\napiVersion: ray.io/v1\nkind: RayCluster\nmetadata:\n  name: raycluster-fluentbit-sidecar-logs\nspec:\n  rayVersion: '2.9.0'\n  headGroupSpec:\n    rayStartParams: {}\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:2.9.0\n          # This config is meant for demonstration purposes only.\n          # Use larger Ray containers in production!\n          resources:\n            limits:\n              cpu: 1\n              memory: 2Gi\n            requests:\n              cpu: 500m\n              memory: 1Gi\n          # Share logs with Fluent Bit\n          volumeMounts:\n          - mountPath: /tmp/ray\n            name: ray-logs\n        # Fluent Bit sidecar\n        - name: fluentbit\n          image: fluent/fluent-bit:3.2.2\n          # Get Kubernetes metadata via downward API\n          env:\n          - name: POD_LABELS\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['ray.io/cluster']\n          # These resource requests for Fluent Bit should be sufficient in production.\n          resources:\n            requests:\n              cpu: 100m\n              memory: 128Mi\n            limits:\n              cpu: 100m\n              memory: 128Mi\n          volumeMounts:\n          - mountPath: /tmp/ray\n            name: ray-logs\n          - mountPath: /fluent-bit/etc/fluent-bit.conf\n            subPath: fluent-bit.conf\n            name: fluentbit-config\n        # Log and config volumes\n        volumes:\n        - name: ray-logs\n          emptyDir: {}\n        - name: fluentbit-config\n          configMap:\n            name: fluentbit-config\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Serve deployments via YAML\nDESCRIPTION: This YAML configuration file demonstrates how to specify deployment parameters, such as `num_replicas`, `max_ongoing_requests`, `graceful_shutdown_wait_loop_s`, `graceful_shutdown_timeout_s`, `health_check_period_s`, `health_check_timeout_s`, and `ray_actor_options`, within a Serve config file. This allows for modifying deployment settings without altering the application code, which is recommended for production environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/configure-serve-deployment.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napplications:\n- name: app1\n  import_path: configure_serve:translator_app\n  deployments:\n  - name: Translator\n    num_replicas: 2\n    max_ongoing_requests: 100\n    graceful_shutdown_wait_loop_s: 2.0\n    graceful_shutdown_timeout_s: 20.0\n    health_check_period_s: 10.0\n    health_check_timeout_s: 30.0\n    ray_actor_options:\n      num_cpus: 0.2\n      num_gpus: 0.0\n```\n\n----------------------------------------\n\nTITLE: Querying Progress Actor in Ray\nDESCRIPTION: Periodically checks the overall progress of the Monte Carlo simulation by calling the progress actor's method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/monte_carlo_pi.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwhile True:\n    progress = ray.get(progress_actor.get_progress.remote())\n    print(f\"Progress: {int(progress * 100)}%\")\n    if progress == 1:\n        break\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Training Scale and GPU Settings\nDESCRIPTION: Creates a ScalingConfig object to specify the number of distributed training workers and GPU usage configuration\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/common/torch-configure-run.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\nscaling_config = ScalingConfig(num_workers=2, use_gpu=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Deployment with Single and Multiple Replicas in Ray Serve (YAML)\nDESCRIPTION: This YAML snippet defines a Ray Serve deployment for a model with initially one replica, scalable up to ten replicas. The snippet illustrates how to configure deployment scaling manually by adjusting the `num_replicas` field.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/autoscaling-guide.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Deploy with a single replica\ndeployments:\n- name: Model\n  num_replicas: 1\n\n# Scale up to 10 replicas\ndeployments:\n- name: Model\n  num_replicas: 10\n```\n\n----------------------------------------\n\nTITLE: Configuring Replica Spreading in Ray Serve Deployments\nDESCRIPTION: Demonstrates how to set max_replicas_per_node to control replica distribution across nodes in Ray Serve deployments\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(max_replicas_per_node=1)\nclass Deployment1:\n  def __call__(self, request):\n    return \"hello\"\n\n@serve.deployment(max_replicas_per_node=2)\nclass Deployment2:\n  def __call__(self, request):\n    return \"world\"\n```\n\n----------------------------------------\n\nTITLE: Neural Network Model Implementation\nDESCRIPTION: Implementation of a simple neural network model class with forward pass, backward pass and parameter updates using RMSProp\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_pong_example.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Model(object):\n    \"\"\"This class holds the neural network weights.\"\"\"\n\n    def __init__(self):\n        self.weights = {}\n        self.weights[\"W1\"] = np.random.randn(H, D) / np.sqrt(D)\n        self.weights[\"W2\"] = np.random.randn(H) / np.sqrt(H)\n\n    def policy_forward(self, x):\n        h = np.dot(self.weights[\"W1\"], x)\n        h[h < 0] = 0  # ReLU nonlinearity.\n        logp = np.dot(self.weights[\"W2\"], h)\n        # Softmax\n        p = 1.0 / (1.0 + np.exp(-logp))\n        # Return probability of taking action 2, and hidden state.\n        return p, h\n\n    def policy_backward(self, eph, epx, epdlogp):\n        \"\"\"Backward pass to calculate gradients.\n\n        Arguments:\n            eph: Array of intermediate hidden states.\n            epx: Array of experiences (observations).\n            epdlogp: Array of logps (output of last layer before softmax).\n\n        \"\"\"\n        dW2 = np.dot(eph.T, epdlogp).ravel()\n        dh = np.outer(epdlogp, self.weights[\"W2\"])\n        # Backprop relu.\n        dh[eph <= 0] = 0\n        dW1 = np.dot(dh.T, epx)\n        return {\"W1\": dW1, \"W2\": dW2}\n\n    def update(self, grad_buffer, rmsprop_cache, lr, decay):\n        \"\"\"Applies the gradients to the model parameters with RMSProp.\"\"\"\n        for k, v in self.weights.items():\n            g = grad_buffer[k]\n            rmsprop_cache[k] = decay * rmsprop_cache[k] + (1 - decay) * g ** 2\n            self.weights[k] += lr * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n\n\ndef zero_grads(grad_buffer):\n    \"\"\"Reset the batch gradient buffer.\"\"\"\n    for k, v in grad_buffer.items():\n        grad_buffer[k] = np.zeros_like(v)\n```\n\n----------------------------------------\n\nTITLE: Creating Counter Actor in C++\nDESCRIPTION: Demonstrates creating a Counter actor in C++ using ray::Actor. Includes class definition, factory function, and actor creation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors.rst#2025-04-12_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\nclass Counter {\n\nprivate:\n    int value = 0;\n\npublic:\n  int Increment() {\n    value += 1;\n    return value;\n  }\n};\n\n// Factory function of Counter class.\nstatic Counter *CreateCounter() {\n    return new Counter();\n};\n\nRAY_REMOTE(&Counter::Increment, CreateCounter);\n\n// Create an actor from this class.\nauto counter = ray::Actor(CreateCounter).Remote();\n```\n\n----------------------------------------\n\nTITLE: Applying Updated RayCluster Config and Checking Status\nDESCRIPTION: This console command applies the modified Ray Service manifest with updated RayCluster configuration. It also shows how to check the status of the update using kubectl describe, revealing the pending service status.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/kubernetes.md#2025-04-12_snippet_8\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl apply -f ray-service.text-ml.yaml\n\n$ kubectl describe rayservice rayservice-sample\n...\n  pendingServiceStatus:\n    appStatus: {}\n    dashboardStatus:\n      healthLastUpdateTime: \"2022-07-18T21:54:53Z\"\n      lastUpdateTime: \"2022-07-18T21:54:54Z\"\n    rayClusterName: rayservice-sample-raycluster-bshfr\n    rayClusterStatus: {}\n...\n```\n\n----------------------------------------\n\nTITLE: Inspecting Batches with Pandas in Ray Datasets\nDESCRIPTION: This snippet shows how to inspect batches of data in a Ray Dataset using take_batch() method with Pandas format, displaying the result as a Pandas DataFrame.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/inspecting-data.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\nbatch = ds.take_batch(batch_size=2, batch_format=\"pandas\")\nprint(batch)\n```\n\n----------------------------------------\n\nTITLE: Stopping Ray Tune Trials by Time\nDESCRIPTION: Uses a dictionary stopping criterion to stop individual trials after a specified timeout using the auto-filled 'time_total_s' metric.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-stopping.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    train_function,\n    tune_config=tune.TuneConfig(\n        num_samples=5,\n        stop={\"time_total_s\": 10}\n    ),\n    param_space={\"base_accuracy\": 0.3}\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Setting Backend Log Level in Bash\nDESCRIPTION: This code sets the RAY_BACKEND_LOG_LEVEL environment variable to 'debug' and starts Ray. This enables detailed logging for the raylet process, which can be found in the raylet.err file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/debugging.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport RAY_BACKEND_LOG_LEVEL=debug\nray start\n```\n\n----------------------------------------\n\nTITLE: Configuring PrometheusRule for Alerting in YAML\nDESCRIPTION: This YAML configuration defines a PrometheusRule to create an alerting rule. It checks for the absence of Ray GCS metrics and triggers an alert if no metrics are emitted for 5 minutes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: ray-cluster-gcs-rules\n  namespace: prometheus-system\n  labels:\n    release: prometheus\nspec:\n  groups:\n  - name: ray-cluster-main-staging-gcs.rules\n    interval: 30s\n    rules:\n    - alert: MissingMetricRayGlobalControlStore\n      annotations:\n        description: Ray GCS is not emitting any metrics for Resource Update requests\n        summary: Ray GCS is not emitting metrics anymore\n      expr: |\n                      (\n                       absent(ray_gcs_update_resource_usage_time_bucket) == 1\n                      )\n      for: 5m\n      labels:\n        severity: critical\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained LightGBM Model\nDESCRIPTION: Executes the prediction function using the trained LightGBM model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightgbm/lightgbm_example.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredict_lightgbm(result)\n```\n\n----------------------------------------\n\nTITLE: Batch Inference with PyTorch Models\nDESCRIPTION: Demonstrates how to perform batch inference using a PyTorch model with Ray Datasets, including model initialization and prediction pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-pytorch.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport ray\n\nds = ray.data.from_numpy(np.ones((1, 100)))\n\nclass TorchPredictor:\n    def __init__(self):\n        self.model = nn.Sequential(\n            nn.Linear(in_features=100, out_features=1),\n            nn.Sigmoid(),\n        )\n        self.model.eval()\n\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        tensor = torch.as_tensor(batch[\"data\"], dtype=torch.float32)\n        with torch.inference_mode():\n            return {\"output\": self.model(tensor).numpy()}\n\npredictions = ds.map_batches(TorchPredictor, concurrency=2)\npredictions.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Define Training Function\nDESCRIPTION: Defines a training function that iteratively computes a score using the evaluation function and reports it to Tune. It retrieves width, height, and steps from the config dictionary, simulating a training loop.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-mlflow.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef train_function(config):\n    width, height = config[\"width\"], config[\"height\"]\n\n    for step in range(config.get(\"steps\", 100)):\n        # Iterative training function - can be any arbitrary training procedure\n        intermediate_score = evaluation_fn(step, width, height)\n        # Feed the score back to Tune.\n        tune.report({\"iterations\": step, \"mean_loss\": intermediate_score})\n        time.sleep(0.1)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using BOHB Scheduler in Ray Tune\nDESCRIPTION: BOHB is a variant of HyperBand that enables the BOHB Algorithm for hyperparameter optimization. It should be used with the Tune BOHB search algorithm and works with the BipedalWalker environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.schedulers import HyperBandForBOHB\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to Structured Logs\nDESCRIPTION: Example of adding extra metadata fields to log entries using the logger's extra parameter.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport logging\n\nray.init(\n    log_to_driver=False,\n    logging_config=ray.LoggingConfig(encoding=\"JSON\", log_level=\"INFO\")\n)\n\nlogger = logging.getLogger()\nlogger.info(\"Driver process with extra fields\", extra={\"username\": \"anyscale\"})\n```\n\n----------------------------------------\n\nTITLE: Extending CLIReporter for Trial Termination Reporting\nDESCRIPTION: Example of extending the CLIReporter class to report only when a trial has terminated, by tracking the number of terminated trials and checking for changes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/reporters.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass TrialTerminationReporter(CLIReporter):\n    def __init__(self):\n        super(TrialTerminationReporter, self).__init__()\n        self.num_terminated = 0\n\n    def should_report(self, trials, done=False):\n        \"\"\"Reports only on trial termination events.\"\"\"\n        old_num_terminated = self.num_terminated\n        self.num_terminated = len([t for t in trials if t.status == Trial.TERMINATED])\n        return self.num_terminated > old_num_terminated\n\ntuner = tune.Tuner(my_trainable, run_config=ray.tune.RunConfig(progress_reporter=TrialTerminationReporter()))\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Batching Deployment Class Example - Python\nDESCRIPTION: Example class showing how to implement a batching deployment with Ray Serve\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/batch.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass BatchingDeployment:\n    @serve.batch\n    async def my_batch_handler(self, requests: List):\n        results = []\n        for request in requests:\n            results.append(request.json())  # processing logic here\n        return results\n\n    async def __call__(self, request):\n        return await self.my_batch_handler(request)\n```\n\n----------------------------------------\n\nTITLE: Managing CPU Resource Yielding in Blocked Remote Ray Tasks\nDESCRIPTION: This snippet demonstrates how Ray handles CPU resource management when a task is blocked during execution. It highlights the release and reacquisition of CPU resources during a `ray.get` call while emphasizing the retention of GPU resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tasks/nested-tasks.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/nested-tasks.py\n    :language: python\n    :start-after: __yield_start__\n    :end-before: __yield_end__\n```\n\n----------------------------------------\n\nTITLE: Accessing Best Checkpoints in Ray Train\nDESCRIPTION: This snippet demonstrates how to access a list of all checkpoints and their associated metrics from a Ray Train `Result` object. The `result.best_checkpoints` attribute returns a list of tuples, where each tuple contains a `Checkpoint` object and its corresponding metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/results.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Access all checkpoints, ordered by their metrics.\"\"\"\n# Get the checkpoints, ordered by their metrics.\ncheckpoints = result.best_checkpoints\nprint(f\"Checkpoints: {checkpoints}\")\n\n```\n\n----------------------------------------\n\nTITLE: Autoscaling Configuration in Ray Serve (YAML)\nDESCRIPTION: This YAML snippet enables autoscaling for a Ray Serve deployment. By setting `num_replicas` to `auto`, it allows the deployment to dynamically adjust the number of replicas based on traffic, using parameters like `max_ongoing_requests`, `target_ongoing_requests`, `min_replicas`, and `max_replicas` to fine-tune the autoscaler behavior.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/autoscaling-guide.md#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Model\n  num_replicas: auto\n```\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Model\n  max_ongoing_requests: 5\n  autoscaling_config:\n    target_ongoing_requests: 2\n    min_replicas: 1\n    max_replicas: 100\n```\n\n----------------------------------------\n\nTITLE: Setting Placement Group Prefix in Ray Tune\nDESCRIPTION: TUNE_PLACEMENT_GROUP_PREFIX sets the prefix for placement groups created by Ray Tune, used to identify placement groups for cleanup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_PLACEMENT_GROUP_PREFIX=tune_pg_\n```\n\n----------------------------------------\n\nTITLE: Enabling RayCluster Status Conditions Feature\nDESCRIPTION: Helm command to upgrade KubeRay operator with the RayClusterStatusConditions feature gate enabled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/observability.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade kuberay-operator kuberay/kuberay-operator --version 1.2.2 \\\n  --set featureGates\\[0\\].name=RayClusterStatusConditions \\\n  --set featureGates\\[0\\].enabled=true\n```\n\n----------------------------------------\n\nTITLE: Using Timer Events in Ray Workflows\nDESCRIPTION: Example showing how to create and use timer events in Ray Workflows. The code creates two timer events with different durations and gathers their results using a workflow task.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/events.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport ray\nfrom ray import workflow\n\n# Create an event which finishes after 2 seconds.\nevent1_task = workflow.wait_for_event(workflow.event_listener.TimerListener, time.time() + 2)\n\n# Create another event which finishes after 1 seconds.\nevent2_task = workflow.wait_for_event(workflow.event_listener.TimerListener, time.time() + 1)\n\n@ray.remote\ndef gather(*args):\n    return args\n\n# Gather will run after 2 seconds when both event1 and event2 are done.\nworkflow.run(gather.bind(event1_task, event2_task))\n```\n\n----------------------------------------\n\nTITLE: Deploying Ray Cluster with KubeRay on Kubernetes\nDESCRIPTION: Commands to deploy a Ray cluster on Kubernetes using the KubeRay operator. Includes setting up port forwarding to access the Ray dashboard and testing the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/gpu-training-example.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Step 2: Deploy a Ray cluster on Kubernetes with the KubeRay operator.\n# Create the KubeRay operator\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.3.0\n\n# Create a Ray cluster\nkubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/ray-cluster.gpu.yaml\n\n# port forwarding\nkubectl port-forward services/raycluster-head-svc 8265:8265\n\n# Test cluster (optional)\nray job submit --address http://localhost:8265 -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n```\n\n----------------------------------------\n\nTITLE: Calculating Per Task Overhead in Python with Ray\nDESCRIPTION: This snippet calculates the overhead per task execution in milliseconds by measuring the time taken for a series of empty task invocations. It provides insight into the necessary task runtime to amortize invocation overhead and highlights variability in performance across different environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"per task overhead (ms) =\", (time.time() - start)*1000/num_calls)\n```\n\n----------------------------------------\n\nTITLE: Accessing the YuniKorn Dashboard for Monitoring\nDESCRIPTION: Command to set up port forwarding for accessing the YuniKorn web UI dashboard, which allows viewing the scheduling status of applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward svc/yunikorn-service 9889:9889 -n yunikorn\n```\n\n----------------------------------------\n\nTITLE: Adapting Stable Diffusion Fine-tuning Script for Ray\nDESCRIPTION: Python code modifications to adapt the Stable Diffusion fine-tuning script to use Ray's TorchTrainer. This includes setting up the Ray environment, configuring resources, and initializing the TorchTrainer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/sd.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    import ray\n    from ray import train\n    from ray.train import ScalingConfig, Checkpoint, CheckpointConfig, RunConfig\n    from ray.train.torch import TorchTrainer, TorchConfig\n\n    ray.init()\n\n    # Configure computation resources\n    # In ScalingConfig, require an HPU for each worker\n    scaling_config = ScalingConfig(num_workers=1, resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n    # Set backend to hccl in TorchConfig\n    torch_config = TorchConfig(backend = \"hccl\")\n    # Initialize a Ray TorchTrainer\n    trainer = TorchTrainer(\n        train_loop_per_worker=main,\n        torch_config=torch_config,\n        scaling_config=scaling_config,\n    )\n\n    result = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Updating User Configuration for Translation Language\nDESCRIPTION: YAML configuration for changing the translation language from French to German without restarting the application\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/inplace-updates.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napplications:\n- name: default\n  route_prefix: /\n  import_path: text_ml:app\n  runtime_env:\n    pip:\n      - torch\n      - transformers\n  deployments:\n  - name: Translator\n    num_replicas: 1\n    user_config:\n      language: german\n```\n\n----------------------------------------\n\nTITLE: Implementing Test Set Evaluation\nDESCRIPTION: Function to evaluate the best trained model on a hold-out test set, including model loading from checkpoints and accuracy calculation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-cifar.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef test_best_model(best_result, smoke_test=False):\n    best_trained_model = Net(best_result.config[\"l1\"], best_result.config[\"l2\"])\n    device = best_result.config[\"device\"]\n    if device == \"cuda\":\n        best_trained_model = nn.DataParallel(best_trained_model)\n    best_trained_model.to(device)\n\n    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n\n    model_state, _optimizer_state = torch.load(checkpoint_path)\n    best_trained_model.load_state_dict(model_state)\n\n    if smoke_test:\n        _trainset, testset = load_test_data()\n    else:\n        _trainset, testset = load_data()\n\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=4, shuffle=False, num_workers=2\n    )\n\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = best_trained_model(images)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    print(f\"Best trial test set accuracy: {correct / total}\")\n```\n\n----------------------------------------\n\nTITLE: Passing Additional Parameters to Ray Tune Trainables\nDESCRIPTION: This snippet demonstrates how to pass constant arguments like datasets to Ray Tune trainables using tune.with_parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom ray import tune\n\ndata = np.random.random(size=100000000)\n\ntuner = tune.Tuner(\n    tune.with_parameters(train_fn, data=data),\n    # ...\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Detached Actors in Python\nDESCRIPTION: Shows how to create a detached actor in Python, which persists even after the driver process exits. This allows the actor to be retrieved in a different driver.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/named-actors.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncounter = Counter.options(name=\"CounterActor\", lifetime=\"detached\").remote()\n\ncounter = ray.get_actor(\"CounterActor\")\n```\n\n----------------------------------------\n\nTITLE: Defining Concurrency Groups in Java Ray Actor\nDESCRIPTION: Shows implementation of concurrent actor with two concurrency groups in Java using ConcurrencyGroupBuilder. Demonstrates method assignment and remote execution patterns.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/concurrency_group_api.rst#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass ConcurrentActor {\n    public long f1() {\n        return Thread.currentThread().getId();\n    }\n\n    public long f2() {\n        return Thread.currentThread().getId();\n    }\n\n    public long f3(int a, int b) {\n        return Thread.currentThread().getId();\n    }\n\n    public long f4() {\n        return Thread.currentThread().getId();\n    }\n\n    public long f5() {\n        return Thread.currentThread().getId();\n    }\n}\n\nConcurrencyGroup group1 =\n    new ConcurrencyGroupBuilder<ConcurrentActor>()\n        .setName(\"io\")\n        .setMaxConcurrency(1)\n        .addMethod(ConcurrentActor::f1)\n        .addMethod(ConcurrentActor::f2)\n        .build();\nConcurrencyGroup group2 =\n    new ConcurrencyGroupBuilder<ConcurrentActor>()\n        .setName(\"compute\")\n        .setMaxConcurrency(1)\n        .addMethod(ConcurrentActor::f3)\n        .addMethod(ConcurrentActor::f4)\n        .build();\n\nActorHandle<ConcurrentActor> myActor = Ray.actor(ConcurrentActor::new)\n    .setConcurrencyGroups(group1, group2)\n    .remote();\n```\n\n----------------------------------------\n\nTITLE: Serve Deploy and Status Commands\nDESCRIPTION: These commands deploy the Ray Serve application defined in `serve_config.yaml` to a local Ray cluster and then check the status of the deployed application. `serve deploy` deploys the application. `serve status` retrieves and displays the status of the deployed application, including the health of deployments and replicas.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/index.md#2025-04-12_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n# Start a local Ray cluster.\nray start --head\n\n# Deploy the Text ML application to the local Ray cluster.\nserve deploy serve_config.yaml\n2022-08-16 12:51:22,043 SUCC scripts.py:180 --\nSent deploy request successfully!\n * Use `serve status` to check deployments' statuses.\n * Use `serve config` to see the running app's config.\n\n$ serve status\nproxies:\n  cef533a072b0f03bf92a6b98cb4eb9153b7b7c7b7f15954feb2f38ec: HEALTHY\napplications:\n  default:\n    status: RUNNING\n    message: ''\n    last_deployed_time_s: 1694041157.2211847\n    deployments:\n      Translator:\n        status: HEALTHY\n        replica_states:\n          RUNNING: 1\n        message: ''\n      Summarizer:\n        status: HEALTHY\n        replica_states:\n          RUNNING: 1\n        message: ''\n```\n\n----------------------------------------\n\nTITLE: Using getter APIs of SingleAgentEpisode\nDESCRIPTION: This code snippet illustrates how to use the getter methods of a SingleAgentEpisode to extract information, such as observations, actions, and rewards, using different indexing techniques.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/single-agent-episode.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example of how to access data from a SingleAgentEpisode via its getter methods.\n\"\"\"\n\nimport numpy as np\n\nfrom ray.rllib.env.single_agent_episode import SingleAgentEpisode\n\n# Create a dummy episode (see above).\nepisode = SingleAgentEpisode()\nepisode.reset(obs=1.0)\nepisode.add_action(action=0)\nepisode.add_reward(reward=1.0)\nepisode.add_observation(obs=2.0)\nepisode.add_action(action=1)\nepisode.add_reward(reward=-0.5)\nepisode.add_observation(obs=3.0)\nepisode.set_terminated(truncated=False)\nepisode.numpyize()\n\n# Get the entire observations column.\nprint(episode.get_observations())  # -> np.ndarray([1.0, 2.0, 3.0], dtype=np.float32)\n# Get the entire actions column.\nprint(episode.get_actions())  # -> np.ndarray([0, 1], dtype=np.int64)\n# Get the entire rewards column.\nprint(episode.get_rewards())  # -> np.ndarray([ 1. , -0.5], dtype=np.float32)\n\n# Get the last reward value.\nprint(episode.get_rewards(-1))  # -> -0.5\n\n# Get the first reward value.\nprint(episode.get_rewards(0))  # -> 1.0\n\n# Get the first two observation values.\nprint(episode.get_observations([0, 1]))  # -> np.ndarray([1., 2.], dtype=np.float32)\n\n# Use a slice to get all but the very last observation.\nprint(episode.get_observations(slice(0, -1)))  # -> np.ndarray([1., 2.], dtype=np.float32)\n\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes RBAC for Ray Cluster Access\nDESCRIPTION: YAML configuration that defines a Role and RoleBinding to grant the ray-user service account access to RayCluster resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# ray-cluster-rbac.yaml\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: ray-user\n  namespace: default\nrules:\n- apiGroups: [\"ray.io\"]\n  resources:\n  - 'rayclusters'\n  verbs: [\"*\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: ray-user\n  namespace: default\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: ray-user\nsubjects:\n- kind: ServiceAccount\n  name: ray-user\n  namespace: default\n```\n\n----------------------------------------\n\nTITLE: Using Ray Wait with Generators for Concurrency in Python\nDESCRIPTION: This code shows how to use ray.wait with generators to achieve concurrency without blocking threads. It demonstrates waiting for multiple generators simultaneously.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-generator.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\n@ray.remote\ndef async_generator(i):\n    for j in range(3):\n        yield i, j\n\nasync def process_generator(gen):\n    async for obj_ref in gen:\n        print(await obj_ref)\n\nasync def main():\n    generators = [async_generator.remote(i) for i in range(3)]\n    await asyncio.gather(*[process_generator(gen) for gen in generators])\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Logging to W&B in PyTorch\nDESCRIPTION: This snippet demonstrates how to log experiments to Weights & Biases (W&B) in PyTorch using Ray Train.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. dropdown:: Log to W&B\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking//torch_exp_tracking_wandb.py\n            :emphasize-lines: 15, 16, 17, 21, 22, 51, 52, 54, 55\n            :language: python\n            :start-after: __start__\n```\n\n----------------------------------------\n\nTITLE: Creating a GKE Cluster with Workload Identity\nDESCRIPTION: Creates a Google Kubernetes Engine cluster with Workload Identity enabled. The cluster has one node with autoscaling enabled, and uses e2-standard-8 machine type.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngcloud container clusters create cloud-bucket-cluster \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-west1-b --machine-type e2-standard-8 \\\n    --workload-pool=my-project-id.svc.id.goog # Replace my-project-id with your GCP project ID\n```\n\n----------------------------------------\n\nTITLE: Defining Data Reading and Transformation Functions\nDESCRIPTION: Implements functions for reading and filtering data from Parquet files using PyArrow and transforming the loaded data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n############\n# STEP 1.  Define Python functions to\n#          a) read and prepare a segment of data.\n############\n\n# Function to read a pyarrow.Table object using pyarrow parquet\ndef read_data(file: str, sample_id: np.int32) -> pd.DataFrame:\n\n    df = pq.read_table(\n        file,\n        filters=[\n            (\"passenger_count\", \">\", 0),\n            (\"trip_distance\", \">\", 0),\n            (\"fare_amount\", \">\", 0),\n            (\"pickup_location_id\", \"not in\", [264, 265]),\n            (\"dropoff_location_id\", \"not in\", [264, 265]),\n            (\"dropoff_location_id\", \"=\", sample_id),\n        ],\n        columns=[\n            \"pickup_at\",\n            \"dropoff_at\",\n            \"pickup_location_id\",\n            \"dropoff_location_id\",\n            \"passenger_count\",\n            \"trip_distance\",\n            \"fare_amount\",\n        ],\n    ).to_pandas()\n\n    return df\n\n\n# Function to transform a pandas dataframe\ndef transform_df(input_df: pd.DataFrame) -> pd.DataFrame:\n    df = input_df.copy()\n\n    # calculate trip_duration\n    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n    # filter trip_durations > 1 minute and less than 24 hours\n    df = df[df[\"trip_duration\"] > 60]\n    df = df[df[\"trip_duration\"] < 24 * 60 * 60]\n    # keep only necessary columns\n    df = df[\n        [\"dropoff_location_id\", \"passenger_count\", \"trip_distance\", \"trip_duration\"]\n    ].copy()\n    df[\"dropoff_location_id\"] = df[\"dropoff_location_id\"].fillna(-1)\n    return df\n```\n\n----------------------------------------\n\nTITLE: Configuring Gymnasium Environments in RLlib\nDESCRIPTION: Python configuration for setting up different types of Gymnasium environments including classic control, Atari, and MuJoCo\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconfig.environment(\"CartPole-v1\")  # Classic Control\nconfig.environment(\"ale_py:ALE/Pong-v5\")  # Atari\nconfig.environment(\"Hopper-v5\")  # MuJoCo\n```\n\n----------------------------------------\n\nTITLE: Installing Aim and Ray Tune Packages - Python\nDESCRIPTION: This snippet installs the necessary packages for integrating Aim with Ray Tune. It ensures that both Aim and Tune functionalities are available for logging experimental results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-aim.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install aim\n%pip install ray[tune]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Ray Actor Subprocess Cleanup on Worker Exit in Python\nDESCRIPTION: This example demonstrates how Ray can automatically clean up orphaned child processes. It creates a Ray actor that spawns a sleep process and then forcibly kills itself with SIGKILL. The subreaper mechanism ensures the orphaned process is eventually cleaned up.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/user-spawn-processes.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport psutil\nimport subprocess\nimport time\nimport os\n\nray.init(_system_config={\"kill_child_processes_on_worker_exit_with_raylet_subreaper\":True})\n\n@ray.remote\nclass MyActor:\n  def __init__(self):\n    pass\n\n  def start(self):\n    # Start a user process\n    process = subprocess.Popen([\"/bin/bash\", \"-c\", \"sleep 10000\"])\n    return process.pid\n\n  def signal_my_pid(self):\n    import signal\n    os.kill(os.getpid(), signal.SIGKILL)\n\n\nactor = MyActor.remote()\n\npid = ray.get(actor.start.remote())\nassert psutil.pid_exists(pid)  # the subprocess running\n\nactor.signal_my_pid.remote()  # sigkill'ed, the worker's subprocess killing no longer works\ntime.sleep(11)  # raylet kills orphans every 10s\nassert not psutil.pid_exists(pid)\n```\n\n----------------------------------------\n\nTITLE: Setting Local Storage Path for Checkpoints\nDESCRIPTION: Configures a local cluster storage path for saving model checkpoints during training. This path is used as the base directory for storing Ray Train outputs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstorage_path = \"/mnt/cluster_storage\"\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Metrics Example\nDESCRIPTION: Python script demonstrating the usage of Ray Serve with built-in metrics. It sets up a deployment that simulates processing time and handles requests.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import serve\nimport time\n\nserve.start()\n\n@serve.deployment\nclass Sleeper:\n    def __call__(self, _):\n        time.sleep(1)\n        return \"1\"\n\nSleeper.deploy()\n\nwhile True:\n    handle = serve.get_deployment(\"Sleeper\").get_handle()\n    ray.get(handle.remote(None))\n```\n\n----------------------------------------\n\nTITLE: Configuring PPO Algorithm with RLlib in Python\nDESCRIPTION: This snippet configures the PPO (Proximal Policy Optimization) algorithm using RLlib. Dependencies include ray and RLlib's connectors. The configuration specifies parameters like the environment type (Taxi-v3), number of environment runners, and evaluation settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.connectors.env_to_module import FlattenObservations\n\n# Configure the algorithm.\nconfig = (\n    PPOConfig()\n    .environment(\"Taxi-v3\")\n    .env_runners(\n        num_env_runners=2,\n        # Observations are discrete (ints) -> We need to flatten (one-hot) them.\n        env_to_module_connector=lambda env: FlattenObservations(),\n    )\n    .evaluation(evaluation_num_env_runners=1)\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Algorithm-Specific Parameters in Python\nDESCRIPTION: Shows how to set algorithm-specific parameters (for IMPALA) using the training method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Change an IMPALA-specific setting (the entropy coefficient).\nconfig.training(entropy_coeff=0.01)\n```\n\n----------------------------------------\n\nTITLE: Getting the Learner Class and Creating the Actor in Ray - Python\nDESCRIPTION: This snippet retrieves the default Learner class for PPO from the configuration and initializes a Learner actor as a remote Ray actor with specified configurations. It uses the `gym` library to create the environment 'Acrobot-v1'. Key functionality includes configuration retrieval and remote instantiation of the Learner actor.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/key-concepts.rst#2025-04-12_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Get the Learner class.\nppo_learner_class = config.get_default_learner_class()\n\n# Create the Learner actor.\nlearner_actor = ray.remote(ppo_learner_class).remote(\n    config=config,\n    module_spec=config.get_multi_rl_module_spec(env=gym.make(\"Acrobot-v1\")),\n)\n```\n\n----------------------------------------\n\nTITLE: Running RLlib Example Scripts\nDESCRIPTION: Demonstrates how to execute RLlib example scripts with command line arguments\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd ray/rllib/examples/multi_agent\n$ python multi_agent_pendulum.py --enable-new-api-stack --num-agents=2\n```\n\n----------------------------------------\n\nTITLE: Skipping Checkpoints in Ray Workflows with Python\nDESCRIPTION: Demonstrates how to skip checkpointing in Ray Workflows to improve performance when exactly-once execution semantics is not required. The example shows a workflow task that reads data with checkpointing disabled using the workflow.options() configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/advanced.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import workflow\n\n@ray.remote\ndef read_data(num: int):\n    return [i for i in range(num)]\n\ndata = read_data.options(**workflow.options(checkpoint=False)).bind(10)\n```\n\n----------------------------------------\n\nTITLE: Reading CSV and Mapping with Block Splitting in Python\nDESCRIPTION: This example demonstrates how Ray Data handles reading a CSV file and applying a map operation, showing the automatic splitting of blocks and the stats of the operation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/performance-tips.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Pretend there are two CPUs.\nray.init(num_cpus=2)\n\nds = ray.data.read_csv(\"example://iris.csv\").map(lambda row: row)\nprint(ds.materialize().stats())\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Scaling for Large CPU Memory Requirements (Python)\nDESCRIPTION: This Python code snippet demonstrates how to configure Ray's scaling to allocate specific resources for the trainer actor, which requires large CPU memory for checkpointing in large language model training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nscaling_config=air.ScalingConfig(\n    # \"large_cpu_mem\" is the tag used to identify this machine type in the\n    # cluster config.\n    trainer_resources={\"large_cpu_mem\": 0.01},\n    num_workers=args.num_devices,\n    use_gpu=True,\n    resources_per_worker={\"GPU\": 1},\n)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Non-Guaranteed Task Execution Order in Asynchronous Actor in Python\nDESCRIPTION: This code snippet illustrates that asynchronous or threaded actors in Ray do not guarantee task execution order. It shows how a task with a delayed dependency can be executed after a later submitted task, even when submitted from the same driver.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/task-orders.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport ray\n\n@ray.remote\nclass AsyncCounter:\n    def __init__(self):\n        self.value = 0\n\n    async def add(self, addition):\n        self.value += addition\n        return self.value\n\ncounter = AsyncCounter.remote()\n\n# Simulate delayed result resolution.\n@ray.remote\ndef delayed_resolution(value):\n    time.sleep(5)\n    return value\n\n# Submit tasks from the driver, with\n# the first submitted task waiting for\n# dependency resolution.\nvalue0 = counter.add.remote(delayed_resolution.remote(1))\nvalue1 = counter.add.remote(2)\n\n# Output: 3. The first submitted task is executed later.\nprint(ray.get(value0))\n# Output: 2. The later submitted task is executed first.\nprint(ray.get(value1))\n```\n\n----------------------------------------\n\nTITLE: Submitting Tasks Without Backpressure in Ray\nDESCRIPTION: This snippet demonstrates how to continuously submit tasks without limiting the number of pending tasks. This approach can potentially lead to out-of-memory issues if tasks are submitted faster than they can be processed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/limit-pending-tasks.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\n\n@ray.remote\ndef long_running_function():\n    # This is a long-running function\n    pass\n\nwhile True:\n    long_running_function.remote()\n```\n\n----------------------------------------\n\nTITLE: Implementing Checkpointing in Tune Trainable (Python)\nDESCRIPTION: Example implementation of checkpointing in a Tune Trainable to enable fault tolerance and resuming trials after preemption.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef setup(self, config):\n    self.model = ConvNet()\n    self.optimizer = optim.SGD(self.model.parameters(), lr=config[\"lr\"])\n    if self.loaded_checkpoint:\n        with open(self.loaded_checkpoint, \"rb\") as f:\n            checkpoint_data = pickle.load(f)\n        self.model.load_state_dict(checkpoint_data[\"model_state_dict\"])\n        self.optimizer.load_state_dict(checkpoint_data[\"optimizer_state_dict\"])\n\ndef save_checkpoint(self, checkpoint_dir):\n    path = os.path.join(checkpoint_dir, \"checkpoint.pkl\")\n    with open(path, \"wb\") as f:\n        pickle.dump(\n            {\n                \"model_state_dict\": self.model.state_dict(),\n                \"optimizer_state_dict\": self.optimizer.state_dict(),\n            },\n            f,\n        )\n    return path\n\ndef load_checkpoint(self, checkpoint_path):\n    self.loaded_checkpoint = checkpoint_path\n```\n\n----------------------------------------\n\nTITLE: Configuring Node Type Specific Min Workers\nDESCRIPTION: Sets minimum workers for a specific node type. Sum across all types must be less than cluster max_workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/configuring-autoscaling.rst#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\navailable_node_types:\n  <node_type_name>:\n    min_workers: 0\n```\n\n----------------------------------------\n\nTITLE: Executing LightGBM Model Training\nDESCRIPTION: Runs the LightGBM model training function with specified parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightgbm/lightgbm_example.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresult = train_lightgbm(num_workers=2, use_gpu=False)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hyperparameter Search Space with Ray Tune for PyTorch\nDESCRIPTION: Defines a configuration dictionary for Ray Tune to sample hyperparameters for a PyTorch model. It includes layer sizes as powers of 2, logarithmically uniform learning rates, batch size options, and testing parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-cifar.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"batch_size\": tune.choice([2, 4, 8, 16]),\n    \"smoke_test\": SMOKE_TEST,\n    \"num_trials\": 10 if not SMOKE_TEST else 2,\n    \"max_num_epochs\": 10 if not SMOKE_TEST else 2,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring External Node Provider in Ray Cluster\nDESCRIPTION: YAML configuration snippet showing how to register a custom node provider in the Ray cluster launcher config. This enables integration with custom cloud providers or cluster managers by specifying the provider type and module path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprovider:\n    type: \"external\"\n    module: \"my.module.MyCustomNodeProvider\"\n```\n\n----------------------------------------\n\nTITLE: Checking Redis Data and Storage Namespace\nDESCRIPTION: Shell commands to inspect Redis storage and verify the external storage namespace configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get rayclusters.ray.io raycluster-external-redis -o=jsonpath='{.metadata.uid}'\nkubectl get pods $HEAD_POD -o=jsonpath='{.spec.containers[0].env}' | jq\nexport REDIS_POD=$(kubectl get pods --selector=app=redis -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $REDIS_POD -- redis-cli -a \"5241590000000000\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for PPO Algorithm\nDESCRIPTION: Markdown content explaining PPO algorithm concepts, distributed implementations, and references to actual code implementations.\nSOURCE: https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/README.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Proximal Policy Optimization (PPO)\n\n## Overview\n\n[PPO](https://arxiv.org/abs/1707.06347) is a model-free on-policy RL algorithm that works\nwell for both discrete and continuous action space environments. PPO utilizes an\nactor-critic framework, where there are two networks, an actor (policy network) and\ncritic network (value function).\n\nThere are two formulations of PPO, which are both implemented in RLlib. The first\nformulation of PPO imitates the prior paper [TRPO](https://arxiv.org/abs/1502.05477)\nwithout the complexity of second-order optimization. In this formulation, for every\niteration, an old version of an actor-network is saved and the agent seeks to optimize\nthe RL objective while staying close to the old policy. This makes sure that the agent\ndoes not destabilize during training. In the second formulation, To mitigate destructive\nlarge policy updates, an issue discovered for vanilla policy gradient methods, PPO\nintroduces the surrogate objective, which clips large action probability ratios between\nthe current and old policy. Clipping has been shown in the paper to significantly\nimprove training stability and speed.\n\n## Distributed PPO Algorithms\n\nPPO is a core algorithm in RLlib due to its ability to scale well with the number of nodes.\n\nIn RLlib, we provide various implementations of distributed PPO, with different underlying\nexecution plans, as shown below:\n\n### Distributed baseline PPO ..\n.. is a synchronous distributed RL algorithm (this algo here).\nData collection nodes, which represent the old policy, gather data synchronously to\ncreate a large pool of on-policy data from which the agent performs minibatch\ngradient descent on.\n\n### Asychronous PPO (APPO)\n\n[See implementation here](https://github.com/ray-project/ray/blob/master/rllib/algorithms/appo/appo.py)\n\n### Decentralized Distributed PPO (DDPPO)\n\n[See implementation here](https://github.com/ray-project/ray/blob/master/rllib/algorithms/ddppo/ddppo.py)\n\n\n## Documentation & Implementation:\n\n### Proximal Policy Optimization (PPO).\n\n**[Detailed Documentation](https://docs.ray.io/en/master/rllib-algorithms.html#ppo)**\n\n**[Implementation](https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py)**\n```\n\n----------------------------------------\n\nTITLE: Basic Cluster Management\nDESCRIPTION: Core commands for starting, updating, and shutting down Ray clusters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nray up CLUSTER.YAML [-y]\n\nray down CLUSTER.YAML [-y]\n```\n\n----------------------------------------\n\nTITLE: Waiting for Placement Group Readiness in C++\nDESCRIPTION: This snippet shows how to wait for a placement group to be ready in C++ and how to retrieve all placement groups.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\n// Wait for the placement group to be ready within the specified time(unit is seconds).\nbool ready = pg.Wait(60);\nassert(ready);\n\n// You can look at placement group states using this API.\nstd::vector<ray::PlacementGroup> all_placement_group = ray::GetAllPlacementGroups();\nfor (const ray::PlacementGroup &group : all_placement_group) {\n  std::cout << group.GetName() << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset from Spark DataFrame - Ray\nDESCRIPTION: This snippet shows how to create a Ray dataset from a Spark DataFrame using the `ray.data.from_spark` function. It highlights the initialization of a Spark session and the conversion process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\nimport raydp\n\nspark = raydp.init_spark(app_name=\"Spark -> Datasets Example\",\n                                    num_executors=2,\n                                    executor_cores=2,\n                                    executor_memory=\"500MB\")\ndf = spark.createDataFrame([(i, str(i)) for i in range(10000)], [\"col1\", \"col2\"])\nds = ray.data.from_spark(df)\n\nds.show(3)\n```\n\n----------------------------------------\n\nTITLE: Using NodeAffinitySchedulingStrategy for Fault Tolerance in Ray (Python)\nDESCRIPTION: Demonstrates the use of NodeAffinitySchedulingStrategy to specify node affinity as a soft constraint. This allows tasks to be retried on other nodes if the target node fails, improving fault tolerance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault-tolerance.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n\nnode_id = ray.get_runtime_context().node_id\nf.options(scheduling_strategy=NodeAffinitySchedulingStrategy(\n    node_id, soft=True)).remote()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for PyTorch Lightning and Ray Tune\nDESCRIPTION: This snippet imports the necessary libraries for implementing a PyTorch Lightning model and using Ray Tune for hyperparameter optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\n\nimport torch\nimport pytorch_lightning as pl\nfrom filelock import FileLock\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn import functional as F\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\nimport os\n```\n\n----------------------------------------\n\nTITLE: Running the Multi-Objective Tuning Experiment in Python\nDESCRIPTION: This snippet runs the tuning experiment with the multi-objective function and captures results based on multiple success criteria defined in the tuning configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    multi_objective,\n    tune_config=tune.TuneConfig(\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space=search_space\n)\nresults = tuner.fit();\n```\n\n----------------------------------------\n\nTITLE: Checking Container Command and Args in KubeRay Pod\nDESCRIPTION: This shell script example shows how to apply a RayCluster configuration and inspect the resulting container command and args in the generated head Pod, displaying how user commands and the Ray start command are combined.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/pod-command.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# Prerequisite: There is a KubeRay operator in the Kubernetes cluster.\n\n# Create a RayCluster\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.head-command.yaml\n\n# Check ${RAYCLUSTER_HEAD_POD}\nkubectl get pod -l ray.io/node-type=head\n\n# Check `spec.containers.0.command` and `spec.containers.0.args`.\nkubectl describe pod ${RAYCLUSTER_HEAD_POD}\n\n# Command:\n#   /bin/bash\n#   -lc\n#   --\n# Args:\n#    echo 123  456  && ulimit -n 65536; ray start --head  --dashboard-host=0.0.0.0  --num-cpus=1  --block  --metrics-export-port=8080  --memory=2147483648\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Training with HorovodTrainer in Python\nDESCRIPTION: This snippet demonstrates how to start the distributed training process using the fit() method of the HorovodTrainer. It assumes that the trainer has been properly configured with a training function and scaling settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/horovod.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Reading GCS Files with Ray Data and GCSFS in Python\nDESCRIPTION: Demonstrates reading files from Google Cloud Storage using Ray Data with the GCSFS library. It creates a GCSFileSystem object and uses it with the read_parquet function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nfilesystem = gcsfs.GCSFileSystem(project=\"my-google-project\")\nds = ray.data.read_parquet(\n    \"gs://...\",\n    filesystem=filesystem\n)\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Initialize Ray\nDESCRIPTION: This code initializes Ray, first shutting down any existing Ray instance to ensure a clean start.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"if ray.is_initialized():\n    ray.shutdown()\nray.init()\"\n```\n\n----------------------------------------\n\nTITLE: Making Module in SingleAgentEnvRunner - Python\nDESCRIPTION: This function constructs a module that can be used for various purposes, such as defining additional behaviors or policies for the agent. It expands the functionality of the base environment runner.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_env_runner.rst#2025-04-12_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef make_module(self):\n    # Create a module for enhanced agent functionality\n    pass\n```\n\n----------------------------------------\n\nTITLE: Retrieving Best Results and Checkpoints from Ray Tune Experiments\nDESCRIPTION: Shows how to extract the best performing trial results and corresponding checkpoints from a completed Ray Tune experiment for further analysis or deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Get the best result of the final iteration, based on a particular metric.\nbest_result = results.get_best_result(\n    metric=\"env_runners/episode_return_mean\",\n    mode=\"max\",\n    scope=\"last\",\n)\n\n# Get the best checkpoint corresponding to the best result\n# from the preceding experiment.\nbest_checkpoint = best_result.checkpoint\n```\n\n----------------------------------------\n\nTITLE: Accessing Result Paths in Ray Train\nDESCRIPTION: Shows how to access filesystem paths from Result and Checkpoint objects returned by trainer.fit(). This demonstrates how to retrieve storage locations including filesystem type and path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/persistent-storage.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresult.filesystem, result.path\n# S3FileSystem, \"bucket-name/sub-path/unique-run-id\"\n\nresult.checkpoint.filesystem, result.checkpoint.path\n# S3FileSystem, \"bucket-name/sub-path/unique-run-id/checkpoint_epoch=0\"\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Head Node\nDESCRIPTION: Command to start a local Ray cluster by initializing the head node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/quickstart.rst#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nray start --head\n```\n\n----------------------------------------\n\nTITLE: Basic Metrics Reporting with Ray Tune\nDESCRIPTION: Demonstrates how to report metrics during training using tune.report and configure optimization objectives through tune.TuneConfig. The example shows a training function that reports a metric value each epoch while simulating a training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune_get_data_in_and_out.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import train\n\n\ndef training_function(config, data):\n    model = {\n        \"hyperparameter_a\": config[\"hyperparameter_a\"],\n        \"hyperparameter_b\": config[\"hyperparameter_b\"],\n    }\n    epochs = config[\"epochs\"]\n\n    # Simulate training & evaluation - we obtain back a \"metric\" and a \"trained_model\".\n    for epoch in range(epochs):\n        # Simulate doing something expensive.\n        time.sleep(1)\n        metric = (0.1 + model[\"hyperparameter_a\"] * epoch / 100) ** (\n            -1\n        ) + model[\"hyperparameter_b\"] * 0.1 * data[\"A\"].sum()\n        trained_model = {\"state\": model, \"epoch\": epoch}\n        tune.report(metrics={\"metric\": metric})\n\n\ntuner = Tuner(\n    tune.with_parameters(training_function, data=data),\n    param_space={\n        \"hyperparameter_a\": tune.uniform(0, 20),\n        \"hyperparameter_b\": tune.uniform(-100, 100),\n        \"epochs\": 10,\n    },\n    tune_config=tune.TuneConfig(num_samples=4, metric=\"metric\", mode=\"max\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Manual Checkpointing with Class API in Ray Tune\nDESCRIPTION: Shows how to manually trigger checkpoints in a Class API Trainable by returning should_checkpoint: True in the result dictionary. This can be particularly useful on spot instances to handle preemption.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-trial-checkpoints.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyTrainable(tune.Trainable):\n    def step(self):\n        # ... training code ...\n        \n        # Detect if we are at risk of preemption\n        if detect_instance_preemption():\n            # Signal that we want to checkpoint when returning\n            return {\"loss\": loss, \"should_checkpoint\": True}\n        return {\"loss\": loss}\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Format in Ray Datasets\nDESCRIPTION: Demonstrates how to configure the batch format when using map_batches in Ray Datasets. Examples are provided for both NumPy and pandas formats.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport ray\n\ndef increase_brightness(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    batch[\"image\"] = np.clip(batch[\"image\"] + 4, 0, 255)\n    return batch\n\nds = (\n    ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\n    .map_batches(increase_brightness, batch_format=\"numpy\")\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport ray\n\ndef drop_nas(batch: pd.DataFrame) -> pd.DataFrame:\n    return batch.dropna()\n\nds = (\n    ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n    .map_batches(drop_nas, batch_format=\"pandas\")\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing PBT with Increased Population Size\nDESCRIPTION: Sets up a PBT experiment with a larger population size (4 trials instead of 2). This demonstrates how PBT behaves with more diversity in the exploration space and how multiple trials can discover different good solutions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nif ray.is_initialized():\n    ray.shutdown()\nray.init()\nperturbation_interval = 4\npbt_scheduler = PopulationBasedTraining(\n    time_attr=\"training_iteration\",\n    perturbation_interval=perturbation_interval,\n    quantile_fraction=0.5,\n    resample_probability=0.5,\n    hyperparam_mutations={\n        \"lr\": tune.qloguniform(5e-3, 1e-1, 5e-4),\n        \"h0\": tune.uniform(0.0, 1.0),\n        \"h1\": tune.uniform(0.0, 1.0),\n    },\n    synch=True,\n)\ntuner = Tuner(\n    train_func,\n    param_space={\n        \"lr\": tune.qloguniform(5e-3, 1e-1, 5e-4),\n        \"h0\": tune.grid_search([0.0, 1.0, 0.01, 0.99]),  # 4 trials\n        \"h1\": tune.sample_from(lambda spec: 1.0 - spec.config[\"h0\"]),\n        \"num_training_iterations\": 100,\n        \"checkpoint_interval\": perturbation_interval,\n    },\n    tune_config=TuneConfig(\n        num_samples=1,\n        metric=\"Q\",\n        mode=\"max\",\n        # Set the PBT scheduler in this config\n        scheduler=pbt_scheduler,\n    ),\n    run_config=tune.RunConfig(\n        stop={\"training_iteration\": 100},\n        failure_config=tune.FailureConfig(max_failures=3),\n    ),\n)\npbt_4_results = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Logging Custom Artifacts with Class API in Ray Tune\nDESCRIPTION: Example of logging custom artifacts, models, or using third-party logging libraries within a Tune trainable using the Class API with setup and step methods.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport logging_library  # ex: mlflow, wandb\nfrom ray import tune\n\nclass CustomLogging(tune.Trainable)\n    def setup(self, config):\n        trial_id = self.trial_id\n        logging_library.init(\n            name=trial_id,\n            id=trial_id,\n            resume=trial_id,\n            reinit=True,\n            allow_val_change=True\n        )\n        logging_library.set_log_path(os.getcwd())\n\n    def step(self):\n        logging_library.log_model(...)\n\n        # You can also write to a file directly.\n```\n\n----------------------------------------\n\nTITLE: Global Row Shuffling in Ray Dataset\nDESCRIPTION: This snippet demonstrates how to perform a global shuffle of all rows in a Ray Dataset using the random_shuffle method. This is the most comprehensive but also the slowest shuffling option.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/shuffling-data.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = (\n    ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\n    .random_shuffle()\n)\n```\n\n----------------------------------------\n\nTITLE: Capturing Task and Actor Call Sites with Ray in Python\nDESCRIPTION: Instructions on how to enable call site capture in Ray for debugging purposes. Setting the appropriate environment variable allows capturing stack traces, though this feature is disabled by default to prevent performance degradation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/general-debugging.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Enable stack trace capture\nray.init(runtime_env={\"env_vars\": {\"RAY_record_task_actor_creation_sites\": \"true\"}})\n\n@ray.remote\ndef my_task():\n    return 42\n\n# Capture the stack trace upon task invocation.\nfuture = my_task.remote()\nresult = ray.get(future)\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\n# Capture the stack trace upon actor creation.\ncounter = Counter.remote()\n\n# Capture the stack trace upon method invocation.\ncounter.increment.remote()\n```\n\n----------------------------------------\n\nTITLE: Global Counter Actor in RLlib\nDESCRIPTION: This code demonstrates how to maintain a shared global counter using Ray actors in RLlib.  The environment increments the counter, and the driver program reads it periodically. This showcases a basic form of global coordination between different processes managed by RLlib.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-advanced-api.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example for using a global counter with RLlib.\n\n.. code-block:: python\n\n    from ray.util.collective.common import get_backend\n\n    # A simple shared counter actor that can be incremented by multiple workers.\n    @ray.remote\n    class SharedCounter:\n        def __init__(self):\n            self.counter = 0\n            self.backend = get_backend()\n            # self.group = collectives.new_group(\n            #     world_size=world_size,\n            #     backend=self.backend,\n            #     timeout=timeout_s,\n            # ) # TODO: how to deal with CollectiveGroup\n\n        def increment(self):\n            self.counter += 1\n\n        def value(self):\n            return self.counter\n\n    counter = SharedCounter.options(name=\"counter\").remote()\n\n    # Get a handle to the actor using its global name. Any process can do this.\n    counter = ray.get_actor(\"counter\")\n\n    # Increment the counter from an environment.\n    class MyEnv(gym.Env):\n        def __init__(self, config):\n            self.counter = ray.get_actor(\"counter\")\n\n        def step(self, action):\n            self.counter.increment.remote()\n            return ob, reward, done, {}\n\n        def reset(self):\n            return ob\n\n    # Periodically read the counter from the driver.\n    print(ray.get(counter.value.remote()))\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoint with Hugging Face Transformers\nDESCRIPTION: Demonstrates how to use Ray Train's callback for Hugging Face Transformers to save checkpoints. It configures the trainer to save and evaluate at specific intervals.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef train_func():\n    # ... other setup code ...\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=16,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_steps=500,\n        save_total_limit=3,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n        callbacks=[train.huggingface.transformers.RayTrainReportCallback()],\n    )\n\n    trainer.train()\n\ntrainer = train.torch.TorchTrainer(\n    train_func,\n    # ... other TorchTrainer args ...\n)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining Python function that raises an exception\nDESCRIPTION: Defines a Python function that raises a ZeroDivisionError, used to demonstrate cross-language exception handling in Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\ndef raise_exception():\n    1 / 0  # This will raise a ZeroDivisionError\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Ray Generator Tasks in Python\nDESCRIPTION: This code demonstrates how errors are handled in Ray generator tasks. It shows that exceptions are propagated through object references and can be caught when calling ray.get().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-generator.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef error_generator():\n    yield 0\n    yield 1\n    raise ValueError(\"Error after second yield\")\n    yield 2  # This line will never be executed\n\ngen = error_generator.remote()\nprint(ray.get(next(gen)))  # Prints 0\nprint(ray.get(next(gen)))  # Prints 1\ntry:\n    print(ray.get(next(gen)))  # Raises ValueError\nexcept ValueError as e:\n    print(f\"Caught exception: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Custom and Conditional Search Spaces in Ray Tune\nDESCRIPTION: Demonstrates how to use tune.sample_from to create custom and conditional search spaces, where one parameter depends on another.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-search-spaces.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    ...,\n    param_space={\n        # A random function\n        \"alpha\": tune.sample_from(lambda _: np.random.uniform(100)),\n        # Use the `spec.config` namespace to access other hyperparameters\n        \"beta\": tune.sample_from(lambda spec: spec.config.alpha * np.random.normal())\n    }\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Increasing Training Batch Size in AlgorithmConfig for Python\nDESCRIPTION: This example demonstrates how to increase the training batch size to 4000 timesteps per learner iteration while keeping the prelearner buffer capacity at 500 timesteps.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .training(\n        # Train on a batch of 4000 timesteps each iteration.\n        train_batch_size_per_learner=4000,\n    )\n    .offline_data(\n        # Read in RLlib's new stack `SingleAgentEpisode` data.\n        input_read_episodes=True\n        # Define an input read batch size of 10 episodes.\n        input_read_batch_size=10,\n        # Set the replay buffer in the `OfflinePrelearner`\n        # to 1,000 timesteps.\n        prelearner_buffer_kwargs={\n            \"capacity\": 500,\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Cert-Manager for TPU Webhook\nDESCRIPTION: Installs cert-manager which is required for TLS certificate injection when manually deploying the Ray TPU initialization webhook.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-tpu-cluster.md#2025-04-12_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install --create-namespace --namespace cert-manager --set installCRDs=true --set global.leaderElection.namespace=cert-manager cert-manager jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Creating Conditional Grid Search in Ray Tune\nDESCRIPTION: This code snippet demonstrates how to perform a conditional grid search in Ray Tune when parameters depend on each other. It uses a helper function to create a list of valid parameter tuples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _iter():\n    for a in range(5, 11):\n        for b in range(a):\n            yield a, b\n\nconfig = {\n    \"ab\": tune.grid_search(list(_iter()))\n}\n```\n\n----------------------------------------\n\nTITLE: Building manylinux2014 x86_64 Ray Wheel using Docker\nDESCRIPTION: This command builds a Ray wheel for Python 3.9 on manylinux2014 x86_64 platform. It uses a Docker container and mounts the current directory to /ray. The resulting wheel files are placed in the .whl directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/README-building-wheels.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -ti --rm \\\n    -e BUILDKITE_COMMIT=\"$(git rev-parse HEAD)\" \\\n    -e BUILD_ONE_PYTHON_ONLY=py39 \\\n    -w /ray -v \"$(pwd)\":/ray \\\n    quay.io/pypa/manylinux2014_x86_64:2024-07-02-9ac04ee \\\n    /ray/python/build-wheel-manylinux2014.sh\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Reporter by Implementing ProgressReporter\nDESCRIPTION: Example showing how to create a completely custom reporter by extending the ProgressReporter interface directly. This custom implementation prints system information and trial information to the standard output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/reporters.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune import ProgressReporter\n\nclass CustomReporter(ProgressReporter):\n\n    def should_report(self, trials, done=False):\n        return True\n\n    def report(self, trials, *sys_info):\n        print(*sys_info)\n        print(\"\\n\".join([str(trial) for trial in trials]))\n\ntuner = tune.Tuner(my_trainable, run_config=ray.tune.RunConfig(progress_reporter=CustomReporter()))\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for PyTorch Lightning and Ray Train\nDESCRIPTION: Installs the necessary Python packages for running the PyTorch Lightning example with Ray Train, including torchmetrics and pytorch_lightning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_mnist_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"torchmetrics>=0.9\" \"pytorch_lightning>=1.6\"\n```\n\n----------------------------------------\n\nTITLE: Textbot Setup and Imports\nDESCRIPTION: Initial imports and setup for the streaming text bot implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom typing import AsyncGenerator\n\nfrom fastapi import FastAPI\nfrom ray import serve\nfrom starlette.responses import StreamingResponse\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n```\n\n----------------------------------------\n\nTITLE: Generate Traffic in RayCluster\nDESCRIPTION: Commands to execute a Python script that generates internal traffic in the RayCluster for visualization purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/istio.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- python -c \"import ray; ray.get([ray.remote(lambda x: print(x)).remote(i) for i in range(5000)])\"\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Custom Offline RL Pipeline with Image Data in RLlib\nDESCRIPTION: Demonstrates running a complete example using custom OfflineData and OfflinePreLearner classes for processing image data in an offline reinforcement learning scenario.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n# This code is referenced but not shown in the snippet\n```\n\n----------------------------------------\n\nTITLE: Viewing Ray's Logging Directory Structure\nDESCRIPTION: Example of the default Ray logging directory structure, showing how Ray organizes session folders with the latest session symlinked to session_latest.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n tmp/ray\n    session_latest\n       logs\n       ...\n    session_2023-05-14_21-19-58_128000_45083\n       logs\n       ...\n    session_2023-05-15_21-54-19_361265_24281\n    ...\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Ray Project\nDESCRIPTION: This code snippet lists the required Python packages and their versions for various environment adapters and testing frameworks used in the Ray project. It includes dependencies for Atari, MuJoCo, PettingZoo, Minigrid, OpenSpiel, Unity3D, and other related libraries.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/rllib-test-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Environment adapters.\n# ---------------------\n# Atari\nale_py==0.10.1\nimageio==2.34.2\nopencv-python-headless==4.8.1.78\n\n# For testing MuJoCo envs with gymnasium.\nmujoco==3.2.4\ndm_control==1.0.12; python_version < \"3.12\"\n\n# For tests on PettingZoo's multi-agent envs.\npettingzoo==1.24.3\npymunk==6.2.1\ntinyscaler==1.2.8\nshimmy==2.0.0\nsupersuit==3.9.3\n\n# For tests on minigrid.\nminigrid==2.3.1\ntensorflow_estimator\n\n# DeepMind's OpenSpiel\nopen-spiel==1.4\n\n# Unity3D testing\nmlagents_envs==0.28.0\n\n# Requires libtorrent which is unavailable for arm64\nh5py==3.10.0\n\n# Requirements for rendering.\nmoviepy\n```\n\n----------------------------------------\n\nTITLE: Ray init and pip dependencies\nDESCRIPTION: This snippet initializes Ray with a runtime environment that specifies a pip dependency. It uses the 'pip' parameter to install the 'requests' package, ensuring that the package is available for Ray tasks and actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport requests\n\n# This example runs on a local machine, but you can also do\n# ray.init(address=..., runtime_env=...) to connect to a cluster.\nray.init(runtime_env={\"pip\": [\"requests\"]})\n```\n\n----------------------------------------\n\nTITLE: Binding Batchbot to a Language Model in Python\nDESCRIPTION: This snippet demonstrates how to bind the Batchbot to a specific language model, in this case, the 'microsoft/DialoGPT-small' model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\napp = Batchbot.bind()\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Logging Format with serve.run\nDESCRIPTION: Shows how to configure JSON logging format using the logging_config parameter in serve.run.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nserve.run(\n    Model.bind(),\n    logging_config={\"encoding\": \"JSON\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Commands for installing Ray, Optimum-Habana, and DeepSpeed packages within the Gaudi container environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install ray[tune,serve]\npip install git+https://github.com/huggingface/optimum-habana.git\n# Replace 1.20.0 with the driver version of the container.\npip install git+https://github.com/HabanaAI/DeepSpeed.git@1.20.0\n# Only needed by the DeepSpeed example.\nexport RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES=1\n```\n\n----------------------------------------\n\nTITLE: Constructing MultiRLModuleSpec for Multiple Models\nDESCRIPTION: Illustrates how to create a MultiRLModuleSpec that manages multiple RLModules, defining their respective observation and action spaces and model configurations. The MultiRLModule instance is constructed from this spec.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom ray.rllib.algorithms.bc.torch.default_bc_torch_rl_module import DefaultBCTorchRLModule\nfrom ray.rllib.core.rl_module.rl_module import RLModuleSpec\nfrom ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n\n# First construct the MultiRLModuleSpec.\nspec = MultiRLModuleSpec(\n    rl_module_specs={\n        \"module_1\": RLModuleSpec(\n            module_class=DefaultBCTorchRLModule,\n\n            # Define the spaces for only this sub-module.\n            observation_space=gym.spaces.Box(low=-1, high=1, shape=(10,)),\n            action_space=gym.spaces.Discrete(2),\n\n            # A custom dict that's accessible inside your class as\n            # `self.model_config`.\n            model_config={\"fcnet_hiddens\": [32]},\n        ),\n        \"module_2\": RLModuleSpec(\n            module_class=DefaultBCTorchRLModule,\n\n            # Define the spaces for only this sub-module.\n            observation_space=gym.spaces.Box(low=-1, high=1, shape=(5,)),\n            action_space=gym.spaces.Discrete(2),\n\n            # A custom dict that's accessible inside your class as\n            # `self.model_config`.\n            model_config={\"fcnet_hiddens\": [16]},\n        ),\n    },\n)\n\n# Construct the actual MultiRLModule instance with .build():\nmulti_rl_module = spec.build()\n```\n\n----------------------------------------\n\nTITLE: Using Ray Wait with Mixed References and Generators in Python\nDESCRIPTION: This snippet demonstrates how to use ray.wait with a mix of regular Ray object references and generators. It shows how to handle different types of references in a single wait operation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-generator.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef regular_task():\n    time.sleep(1)\n    return \"regular task result\"\n\n@ray.remote\ndef generator_task():\n    for i in range(3):\n        time.sleep(0.5)\n        yield f\"generator result {i}\"\n\nreg_ref = regular_task.remote()\ngen = generator_task.remote()\n\nwhile True:\n    ready, not_ready = ray.wait([reg_ref, gen], num_returns=1, timeout=0.1)\n    if ready:\n        if ready[0] == reg_ref:\n            print(ray.get(reg_ref))\n            reg_ref = None\n        else:\n            print(ray.get(next(gen)))\n    if not reg_ref and not gen:\n        break\n```\n\n----------------------------------------\n\nTITLE: KubeRay Runtime Environment Configuration\nDESCRIPTION: YAML configuration for specifying runtime environment in a KubeRay RayJob manifest, including pip dependencies and environment variables.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  runtimeEnvYAML: |\n    pip:\n      - requests==2.26.0\n      - pendulum==2.1.2\n    env_vars:\n      KEY: \"VALUE\"\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Object References in Ray (Python)\nDESCRIPTION: Demonstrates how to create nested object references in Ray using Python. This allows building composite objects that hold references to other objects.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects.rst#2025-04-12_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Objects can be nested within each other. Ray will keep the inner object\n# alive via reference counting until all outer object references are deleted.\nobject_ref_2 = ray.put([object_ref])\n```\n\n----------------------------------------\n\nTITLE: Deploying Ray Serve Object Detection Service\nDESCRIPTION: Command to deploy the object detection service using Ray Serve.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/object-detection.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nserve run object_detection:entrypoint\n```\n\n----------------------------------------\n\nTITLE: Enabling Ray Train V2 for Migration\nDESCRIPTION: Setting the environment variable to enable Ray Train V2 functionality, which is required when migrating from the deprecated <Framework>Trainer.restore API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/fault-tolerance.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nRAY_TRAIN_V2_ENABLED=1\n```\n\n----------------------------------------\n\nTITLE: Returning Dynamic Multiple Values from Ray Task\nDESCRIPTION: This snippet demonstrates the use of a dynamic generator pattern for returning an unknown number of values from a Ray task. It allows for efficient handling of multiple return values when the count is not known in advance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/return-ray-put.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef task():\n    for value in compute_values():\n        yield value\n\n# Good: Get values lazily\nfor value in ray.get(task.remote()):\n    process(value)\n```\n\n----------------------------------------\n\nTITLE: Ray Log Level Enum in C++\nDESCRIPTION: This snippet shows the C++ enum definition for Ray log levels. It includes DEBUG, INFO, WARNING, ERROR, and FATAL levels, which correspond to different levels of logging verbosity in the Ray backend.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/debugging.rst#2025-04-12_snippet_2\n\nLANGUAGE: C\nCODE:\n```\nenum class RayLogLevel {\n  DEBUG = -1,\n  INFO = 0,\n  WARNING = 1,\n  ERROR = 2,\n  FATAL = 3,\n};\n```\n\n----------------------------------------\n\nTITLE: Testing Ray Installation\nDESCRIPTION: Runs a mini test suite to verify the Ray installation. This confirms that Ray is functioning correctly within the Docker container or the installed environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n\"python -m pytest -v python/ray/tests/test_mini.py\"\n```\n\n----------------------------------------\n\nTITLE: Setting up PyTorch and Ray Tune Dependencies\nDESCRIPTION: Basic package imports required for PyTorch model training and Ray Tune integration including torch, torchvision, and ray.tune modules.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-cifar.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom filelock import FileLock\nfrom torch.utils.data import random_split\n\nfrom ray import train, tune\nfrom ray.tune.schedulers import ASHAScheduler\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Batches with NumPy in Ray Data\nDESCRIPTION: Shows how to iterate over batches of data in NumPy format using Ray Data's iter_batches() method with the 'numpy' batch format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/iterating-over-data.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\n\nfor batch in ds.iter_batches(batch_size=2, batch_format=\"numpy\"):\n    print(batch)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Resources in KubeRay YAML\nDESCRIPTION: Example showing how to specify custom resource capacities for Ray pods. The resources field defines Custom1 and Custom2 capacities that will be used by the Ray scheduler and autoscaler.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/config.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nrayStartParams:\n    resources: '\"{\\\"Custom1\\\": 1, \\\"Custom2\\\": 5}\"'\n```\n\n----------------------------------------\n\nTITLE: Accessing Task Summary using Ray CLI\nDESCRIPTION: Command to display a summary of tasks in a Ray cluster. Requires Ray installation with 'default' extras and dashboard component.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/reference/cli.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nray summary tasks\n```\n\n----------------------------------------\n\nTITLE: Setting and Retrieving Custom Request ID in Ray Serve\nDESCRIPTION: This code demonstrates how to set a custom request ID for each HTTP request and retrieve it from the response in Ray Serve.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nheaders = {\"X-Request-ID\": \"123-234\"}\nresponse = requests.get(\"http://127.0.0.1:8000\", headers=headers)\nprint(response.headers[\"X-Request-ID\"])  # 123-234\n```\n\n----------------------------------------\n\nTITLE: Ray-Enabled Task Distribution\nDESCRIPTION: Modified version of the task distribution script using Ray for distributed execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/getting-started.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import Counter\nimport socket\nimport time\n\nimport ray\n\nray.init()\n\n@ray.remote\ndef f():\n    time.sleep(0.001)\n    # Return IP address.\n    return socket.gethostbyname(\"localhost\")\n\nobject_ids = [f.remote() for _ in range(10000)]\nip_addresses = ray.get(object_ids)\nprint(Counter(ip_addresses))\n```\n\n----------------------------------------\n\nTITLE: Logging Nested Values with MetricsLogger in Python\nDESCRIPTION: This snippet logs a value under a nested key using the Ray RLlib MetricsLogger. It demonstrates the usage of a tuple for nested keys to organize metrics hierarchically.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Log a value under a deeper nested key.\nlogger.log_value((\"some\", \"nested\", \"key\"), -1.0)\nprint(logger.peek((\"some\", \"nested\", \"key\")))  # expect: -1.0\n```\n\n----------------------------------------\n\nTITLE: Creating Ray Actors in Java\nDESCRIPTION: Shows how to create Ray actors in Java to maintain state and execute methods in distributed worker processes\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_25\n\nLANGUAGE: java\nCODE:\n```\nimport io.ray.api.ActorHandle;\nimport io.ray.api.ObjectRef;\nimport io.ray.api.Ray;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class RayDemo {\n    public static class Counter {\n        private int value = 0;\n        public void increment() {\n            this.value += 1;\n        }\n        public int read() {\n            return this.value;\n        }\n    }\n\n    public static void main(String[] args) {\n        Ray.init();\n        List<ActorHandle<Counter>> counters = new ArrayList<>();\n        for (int i = 0; i < 4; i++) {\n            counters.add(Ray.actor(Counter::new).remote());\n        }\n\n        for (ActorHandle<Counter> counter : counters) {\n            counter.task(Counter::increment).remote();\n        }\n        List<ObjectRef<Integer>> objectRefList = counters.stream()\n            .map(counter -> counter.task(Counter::read).remote())\n            .collect(Collectors.toList());\n        System.out.println(Ray.get(objectRefList));  // [1, 1, 1, 1]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Returning Final Metrics from Function API in Python\nDESCRIPTION: Alternative approach to report final metrics by returning them from the training function instead of using tune.report(). This provides a cleaner way to return final results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_func(config):\n    final_score = 0\n    for step in range(100):\n        # train the model\n        final_score = objective(step, config[\"a\"], config[\"b\"])\n    return {\"score\": final_score}\n```\n\n----------------------------------------\n\nTITLE: Selecting Best Models by Location\nDESCRIPTION: Processes the results DataFrame to keep only the model with minimum error for each location_id. The data is filtered, sorted by error, and indexed by location_id to create a final DataFrame of best models for deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Keep only 1 model per location_id with minimum error\nfinal_df = results_df.copy()\nfinal_df = final_df.loc[(final_df.error > 0), :]\nfinal_df = final_df.loc[final_df.groupby(\"location_id\")[\"error\"].idxmin()]\nfinal_df.sort_values(by=[\"error\"], inplace=True)\nfinal_df.set_index(\"location_id\", inplace=True, drop=True)\nfinal_df\n```\n\n----------------------------------------\n\nTITLE: Memory Profiling Ray Actors\nDESCRIPTION: Example code showing how to implement memory profiling for Ray actors using memray context manager. The profiling files are written to the Ray session logs directory for easy access through the dashboard.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-memory.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n__memray_profiling_start__\n__memray_profiling_end__\n```\n\n----------------------------------------\n\nTITLE: Local Shuffle Buffer in Ray Dataset Iteration\nDESCRIPTION: This code shows how to use a local shuffle buffer when iterating over batches in a Ray Dataset. It specifies a 'local_shuffle_buffer_size' to shuffle rows up to the provided buffer size during iteration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/shuffling-data.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\")\n\nfor batch in ds.iter_batches(\n    batch_size=2,\n    batch_format=\"numpy\",\n    local_shuffle_buffer_size=250,\n):\n    print(batch)\n```\n\n----------------------------------------\n\nTITLE: Using Runtime Environments for Serve run\nDESCRIPTION: A bash command incorporating `serve run` with runtime environments, specifying environment variables, dependencies, and working directories for remote clusters using JSON or YAML configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dev-workflow.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n\"serve run  --address=ray://<head-node-ip-address>:10001 --runtime-env-json='{\"env_vars\": {\"MY_ENV_VAR\": \"my-value\"}, \"working_dir\": \"./project/src\", \"pip\": [\\\"requests\\\", \\\"chess\\\"]}' local_dev:app\"\n```\n\n----------------------------------------\n\nTITLE: Standalone YAML Configuration for LLM Deployment\nDESCRIPTION: Define LLM model configurations across multiple YAML files for modular and scalable production deployment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n# config.yaml\napplications:\n- args:\n    llm_configs:\n        - models/qwen-0.5b.yaml\n        - models/qwen-1.5b.yaml\n  import_path: ray.serve.llm:build_openai_app\n  name: llm_app\n  route_prefix: \"/\"\n\n# models/qwen-0.5b.yaml\nmodel_loading_config:\n  model_id: qwen-0.5b\n  model_source: Qwen/Qwen2.5-0.5B-Instruct\naccelerator_type: A10G\ndeployment_config:\n  autoscaling_config:\n    min_replicas: 1\n    max_replicas: 2\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 Transfer Settings\nDESCRIPTION: Configures AWS S3 transfer parameters for optimal data transfer performance with specific bandwidth and chunk size settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n!aws configure set s3.max_concurrent_requests 32\n!aws configure set default.s3.preferred_transfer_client crt\n!aws configure set default.s3.target_bandwidth 100Gb/s\n!aws configure set default.s3.multipart_chunksize 8MB\n```\n\n----------------------------------------\n\nTITLE: Reading Text Files with Ray Data in Python\nDESCRIPTION: Illustrates reading text files using Ray Data's read_text function. The example reads from an S3 bucket and prints the schema of the resulting dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Runtime Concurrency Group Configuration in Python\nDESCRIPTION: Demonstrates how to dynamically set concurrency group for method execution at runtime in Python.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/concurrency_group_api.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Executed in the \"io\" group (as defined in the actor class).\na.f2.options().remote()\n\n# Executed in the \"compute\" group.\na.f2.options(concurrency_group=\"compute\").remote()\n```\n\n----------------------------------------\n\nTITLE: Training PPO Model with Ray RLlib in Python\nDESCRIPTION: This snippet demonstrates how to import Ray and RLlib, define a function to train a PPO model on the CartPole environment, and save the model checkpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/template.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport ray.rllib.agents.ppo as ppo\nfrom ray import serve\n\ndef train_ppo_model():\n    trainer = ppo.PPOTrainer(\n        config={\"framework\": \"torch\", \"num_workers\": 0},\n        env=\"CartPole-v0\",\n    )\n    # Train for one iteration\n    trainer.train()\n    trainer.save(\"/tmp/rllib_checkpoint\")\n    return \"/tmp/rllib_checkpoint/checkpoint_000001/checkpoint-1\"\n\n\ncheckpoint_path = train_ppo_model()\n```\n\n----------------------------------------\n\nTITLE: Setting Ray Dashboard Address Environment Variable\nDESCRIPTION: Manages the Ray Dashboard address environment variable, allowing customization of the address for Serve CLI commands. This enables users to ensure they are using the correct dashboard address for deployments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/deploy-vm.md#2025-04-12_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ echo $RAY_DASHBOARD_ADDRESS\n\n$ export RAY_DASHBOARD_ADDRESS=[YOUR VALUE]\n\n$ unset RAY_DASHBOARD_ADDRESS\n```\n\n----------------------------------------\n\nTITLE: Executing and retrieving result from Ray task\nDESCRIPTION: This snippet executes a Ray remote task and prints the result. It calls the 'f' task remotely using 'f.remote()' and then retrieves the result using 'ray.get()'.  The output will be the content of the 'hello.txt' file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(ray.get(f.remote()))\n```\n\n----------------------------------------\n\nTITLE: Displaying Reinforcement Learning Algorithm Performance Table in ASCII Format\nDESCRIPTION: This code snippet shows a formatted ASCII table containing performance metrics for various reinforcement learning algorithms. It includes columns for trial name, status, iterations, total time, timesteps, reward, and episode statistics.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.4.0/rllib_tests/learning_tests.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nNumber of trials: 14/14 (14 TERMINATED)\n+-------------------------------------------------+------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n| Trial name                                      | status     |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n|-------------------------------------------------+------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n| A2C_TF_BreakoutNoFrameskip-v4_a7184_00000       | TERMINATED |    352 |          3606.77 | 2852500 |    3.63  |               11     |                0     |             716.08 |\n| A2C_TORCH_BreakoutNoFrameskip-v4_a7184_00001    | TERMINATED |    351 |          3608.87 | 2523000 |   18.52  |               48     |                6     |            2628.03 |\n| APEX_TF_BreakoutNoFrameskip-v4_a7184_00002      | TERMINATED |    104 |          3606.78 | 5265120 |   12.09  |               29     |                3     |            2022.17 |\n| APEX_TORCH_BreakoutNoFrameskip-v4_a7184_00003   | TERMINATED |     83 |          3628.33 | 4170400 |   11.29  |               30     |                2     |            1945.28 |\n| DQN_TF_BreakoutNoFrameskip-v4_a7184_00004       | TERMINATED |     22 |          3682.02 |  230000 |   15.77  |               30     |                1     |            2092.43 |\n| DQN_TORCH_BreakoutNoFrameskip-v4_a7184_00005    | TERMINATED |     21 |          3692.11 |  220000 |   13.91  |               30     |                5     |            2002.19 |\n| IMPALA_TF_BreakoutNoFrameskip-v4_a7184_00006    | TERMINATED |    357 |          3609.24 | 7618000 |  345.44  |              438     |               41     |            9281.76 |\n| IMPALA_TORCH_BreakoutNoFrameskip-v4_a7184_00007 | TERMINATED |    359 |          3603.18 | 6755500 |  355.43  |              441     |               69     |            9519.54 |\n| PPO_TF_BreakoutNoFrameskip-v4_a7184_00008       | TERMINATED |   1336 |          3600.41 | 6680000 |   45.31  |              186     |                4     |            4325.41 |\n| PPO_TORCH_BreakoutNoFrameskip-v4_a7184_00009    | TERMINATED |    575 |          3605.2  | 2875000 |   21.97  |              222     |                8     |            2426.03 |\n| SAC_TF_HalfCheetahBulletEnv-v0_a7184_00010      | TERMINATED |     43 |          3601.63 |   52000 |  549.44  |              669.02  |              438.538 |            1000    |\n| SAC_TORCH_HalfCheetahBulletEnv-v0_a7184_00011   | TERMINATED |     17 |          3719.08 |   26000 | -500.77  |             -388.206 |             -677.982 |            1000    |\n| DDPG_TF_HalfCheetahBulletEnv-v0_a7184_00012     | TERMINATED |    213 |          3600.1  |  213000 | -845.139 |              344.203 |            -2012.73  |            1000    |\n| DDPG_TORCH_HalfCheetahBulletEnv-v0_a7184_00013  | TERMINATED |    157 |          3616.63 |  157000 | -897.804 |              419.357 |            -1718.27  |            1000    |\n+-------------------------------------------------+------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n```\n\n----------------------------------------\n\nTITLE: Logging with MLflow in PyTorch Lightning\nDESCRIPTION: This example illustrates how to log to MLflow using PyTorch Lightning in conjunction with Ray Train.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. dropdown:: MLflow\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking/lightning_exp_tracking_model_dl.py\n        :language: python\n        :start-after: __model_dl_start__\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking/lightning_exp_tracking_mlflow.py\n        :language: python\n        :start-after: __lightning_experiment_tracking_mlflow_start__\n        :end-before: __lightning_experiment_tracking_mlflow_end__\n```\n\n----------------------------------------\n\nTITLE: Sending Text-to-Image Prompt to Stable Diffusion Model\nDESCRIPTION: This command runs the Python script to submit a text-to-image prompt to the Stable Diffusion model server and save the results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/tpu-serve-stable-diffusion.md#2025-04-12_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npython stable_diffusion_tpu_req.py  --save_pictures\n```\n\n----------------------------------------\n\nTITLE: Overriding Container Commands in KubeRay RayCluster YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to use the ray.io/overwrite-container-cmd annotation to completely replace the default container command with a custom one, while still having access to the generated Ray start command via the KUBERAY_GEN_RAY_START_CMD environment variable.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/pod-command.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: ray.io/v1\nkind: RayCluster\nmetadata:\n  annotations:\n    # If this annotation is set to \"true\", KubeRay will respect the container `command` and `args`.\n    ray.io/overwrite-container-cmd: \"true\"\n  ...\nspec:\n  headGroupSpec:\n    rayStartParams: {}\n    # Pod template\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:2.8.0\n          # Because the annotation \"ray.io/overwrite-container-cmd\" is set to \"true\",\n          # KubeRay will overwrite the generated container command with `command` and\n          # `args` in the following. Hence, you need to specify the `ulimit` command\n          # by yourself to avoid Ray scalability issues.\n          command: [\"/bin/bash\", \"-lc\", \"--\"]\n          # Starting from v1.1.0, KubeRay injects the environment variable `KUBERAY_GEN_RAY_START_CMD`\n          # into the Ray container. This variable can be used to retrieve the generated Ray start command.\n          # Note that this environment variable does not include the `ulimit` command.\n          args: [\"ulimit -n 65536; echo head; $KUBERAY_GEN_RAY_START_CMD\"]\n          ...\n```\n\n----------------------------------------\n\nTITLE: Using Ray State CLI for Actor Status Inspection\nDESCRIPTION: Commands to access the head pod and use Ray State CLI for checking the status of Ray Serve application actors. This helps verify if all expected actors are running correctly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Log into the head Pod\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- ray summary actors\n\n# [Example output]:\n# ======== Actors Summary: 2023-07-11 17:58:24.625032 ========\n# Stats:\n# ------------------------------------\n# total_actors: 14\n\n\n# Table (group by class):\n# ------------------------------------\n#     CLASS_NAME                          STATE_COUNTS\n# 0   ServeController                     ALIVE: 1\n# 1   ServeReplica:fruit_app_OrangeStand  ALIVE: 1\n# 2   ProxyActor                          ALIVE: 3\n# 4   ServeReplica:math_app_Multiplier    ALIVE: 1\n# 5   ServeReplica:math_app_create_order  ALIVE: 1\n# 7   ServeReplica:fruit_app_FruitMarket  ALIVE: 1\n# 8   ServeReplica:math_app_Adder         ALIVE: 1\n# 9   ServeReplica:math_app_Router        ALIVE: 1\n# 10  ServeReplica:fruit_app_MangoStand   ALIVE: 1\n# 11  ServeReplica:fruit_app_PearStand    ALIVE: 1\n```\n\n----------------------------------------\n\nTITLE: Verifying RayService Deployment Status\nDESCRIPTION: A series of kubectl commands to check the status of the RayService, RayCluster, Ray Pods, and related Kubernetes services. These commands help verify that the deployment is running correctly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# Step 4.1: List all RayService custom resources in the `default` namespace.\nkubectl get rayservice\n\n# [Example output]\n# NAME                SERVICE STATUS   NUM SERVE ENDPOINTS\n# rayservice-sample   Running          1\n\n# Step 4.2: List all RayCluster custom resources in the `default` namespace.\nkubectl get raycluster\n\n# [Example output]\n# NAME                                 DESIRED WORKERS   AVAILABLE WORKERS   CPUS    MEMORY   GPUS   STATUS   AGE\n# rayservice-sample-raycluster-bwrp8   1                 1                   2500m   4Gi      0      ready    83s\n\n# Step 4.3: List all Ray Pods in the `default` namespace.\nkubectl get pods -l=ray.io/is-ray-node=yes\n\n# [Example output]\n# NAME                                                          READY   STATUS    RESTARTS   AGE\n# rayservice-sample-raycluster-bwrp8-head-p8dnc                 1/1     Running   0          105s\n# rayservice-sample-raycluster-bwrp8-small-group-worker-hbwr2   1/1     Running   0          105s\n\n# Step 4.4: Check the `Ready` condition of the RayService.\n# The RayService is ready to serve requests when the condition is `True`.\nkubectl describe rayservices.ray.io rayservice-sample\n\n# [Example output]\n# Conditions:\n#   Last Transition Time:  2025-02-13T04:55:37Z\n#   Message:               Number of serve endpoints is greater than 0\n#   Observed Generation:   1\n#   Reason:                NonZeroServeEndpoints\n#   Status:                True\n#   Type:                  Ready\n\n# Step 4.5: List services in the `default` namespace.\nkubectl get services\n\n# NAME                                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                   AGE\n# ...\n# rayservice-sample-head-svc                    ClusterIP   None           <none>        10001/TCP,8265/TCP,6379/TCP,8080/TCP,8000/TCP   77s\n# rayservice-sample-raycluster-bwrp8-head-svc   ClusterIP   None           <none>        10001/TCP,8265/TCP,6379/TCP,8080/TCP,8000/TCP   2m16s\n# rayservice-sample-serve-svc                   ClusterIP   10.96.212.79   <none>        8000/TCP                                        77s\n```\n\n----------------------------------------\n\nTITLE: Submitting Python Script to Ray Cluster on Slurm\nDESCRIPTION: Invokes the Python script to run on the Ray cluster. Uses -u argument for unbuffered output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/slurm.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -u /path/to/your/script.py\n```\n\n----------------------------------------\n\nTITLE: Executing Ray Program on Head Pod\nDESCRIPTION: Connects to the Ray cluster by executing a Python script on the head pod using kubectl exec.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl exec deployment-ray-head-xxxxx -it -c ray-head -- python -c \"import ray; ray.init('auto')\"\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Ray Preprocessor\nDESCRIPTION: ReStructuredText documentation defining the structure and API references for Ray's preprocessor module. Includes sections for preprocessor interface, generic preprocessors, categorical encoders, feature scalers, and k-bins discretizers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/preprocessor.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _preprocessor-ref:\n\nPreprocessor\n============\n\nPreprocessor Interface\n------------------------\n\n.. currentmodule:: ray.data\n\nConstructor\n~~~~~~~~~~~\n\n.. autosummary::\n    :nosignatures:\n    :toctree: doc/\n\n    ~preprocessor.Preprocessor\n\nFit/Transform APIs\n~~~~~~~~~~~~~~~~~~\n\n.. autosummary::\n    :nosignatures:\n    :toctree: doc/\n\n    ~preprocessor.Preprocessor.fit\n    ~preprocessor.Preprocessor.fit_transform\n    ~preprocessor.Preprocessor.transform\n    ~preprocessor.Preprocessor.transform_batch\n    ~preprocessor.PreprocessorNotFittedException\n```\n\n----------------------------------------\n\nTITLE: Garbage Collection of Ray Generator Object References in Python\nDESCRIPTION: This snippet illustrates how garbage collection works with Ray generator object references. It shows that unconsumed references are garbage collected when the generator is deleted.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/ray-generator.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ngen = streaming_generator.remote()\nref1 = next(gen)  # This ref is counted\nray.get(ref1)  # This works\ndel gen  # Garbage collects unconsumed refs\n# Subsequent calls to ray.get() for unconsumed refs will fail\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Resources Per Worker in Ray Train\nDESCRIPTION: Specifies custom resource requirements for each training worker, allocating multiple CPUs and GPUs per worker as needed for computation-intensive training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\n\nscaling_config = ScalingConfig(\n    num_workers=8,\n    resources_per_worker={\n        \"CPU\": 4,\n        \"GPU\": 2,\n    },\n    use_gpu=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Libraries with Conda\nDESCRIPTION: Installs specific Ray libraries (Data, Train, Tune, Serve, RLlib) from conda-forge, along with their dependencies. This allows users to selectively install the Ray components they need.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n\"conda install -c conda-forge \\\"ray-data\\\"   # installs Ray + dependencies for Ray Data\\nconda install -c conda-forge \\\"ray-train\\\"  # installs Ray + dependencies for Ray Train\\nconda install -c conda-forge \\\"ray-tune\\\"   # installs Ray + dependencies for Ray Tune\\nconda install -c conda-forge \\\"ray-serve\\\"  # installs Ray + dependencies for Ray Serve\\nconda install -c conda-forge \\\"ray-rllib\\\"  # installs Ray + dependencies for Ray RLlib\"\n```\n\n----------------------------------------\n\nTITLE: Mocking Wandb API for Testing Ray Tune Integration\nDESCRIPTION: This snippet shows how to mock the Wandb API for testing purposes when running Tune experiments with Wandb integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-wandb.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nmock_api = True\n\nif mock_api:\n    os.environ.setdefault(\"WANDB_MODE\", \"disabled\")\n    os.environ.setdefault(\"WANDB_API_KEY\", \"abcd\")\n    ray.init(\n        runtime_env={\"env_vars\": {\"WANDB_MODE\": \"disabled\", \"WANDB_API_KEY\": \"abcd\"}}\n    )\n\ntune_with_callback()\ntune_with_setup()\ntune_trainable()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies with pip\nDESCRIPTION: Installation command for required Python packages including Ray Serve, requests, PyTorch and transformers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/text-classification.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[serve]\" requests torch transformers\n```\n\n----------------------------------------\n\nTITLE: Defining Single-Worker TensorFlow Training Function (Python)\nDESCRIPTION: Defines a single-worker TensorFlow training function for use with Ray Train. Placeholders `__tf_single_begin__` and `__tf_single_end__` delimit the code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/tf/tensorflow_quick_start.py\n:language: python\n:start-after: __tf_single_begin__\n:end-before: __tf_single_end__\n```\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray in Python\nDESCRIPTION: Initializes Ray with logging configuration turned off. This is required to execute any Ray-based distributed computing tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nray.init(configure_logging=False)\n```\n\n----------------------------------------\n\nTITLE: Scheduling Actors in Placement Group - Java\nDESCRIPTION: This snippet demonstrates scheduling a Java actor with a placement group in Ray. The actor class requires a constructor and methods for functional operations. It is deployed on a GPU bundle within the placement group. Dependencies include Ray Java APIs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\npublic static class Counter {\n  private int value;\n\n  public Counter(int initValue) {\n    this.value = initValue;\n  }\n\n  public int getValue() {\n    return value;\n  }\n\n  public static String ping() {\n    return \"pong\";\n  }\n}\n\n// Create GPU actors on a gpu bundle.\nfor (int index = 0; index < 1; index++) {\n  Ray.actor(Counter::new, 1)\n    .setPlacementGroup(pg, 0)\n    .remote();\n}\n```\n\n----------------------------------------\n\nTITLE: Dynamic Space Assignment for MultiAgentEnv\nDESCRIPTION: Alternative method for dynamically assigning observation and action spaces based on agent ID patterns using method overriding\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/multi-agent-envs.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_observation_space(self, agent_id):\n    if agent_id.startswith(\"robot_\"):\n        return gym.spaces.Box(0, 255, (84, 84, 3), np.uint8)\n    elif agent_id.startswith(\"decision_maker\"):\n        return gym.spaces.Discrete(2)\n    else:\n        raise ValueError(f\"bad agent id: {agent_id}!\")\n```\n\n----------------------------------------\n\nTITLE: Running Ray Docker Container with GPU Support\nDESCRIPTION: Runs the Ray Docker container with GPU support, specifying the shared memory size and the ray version. The `--gpus all` flag enables access to all GPUs in the system.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n\"docker run --shm-size=<shm-size> -t -i --gpus all rayproject/ray:<ray-version>-gpu\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Settings for Ray Cluster in YAML\nDESCRIPTION: This YAML structure defines Docker-specific configuration options for a Ray cluster, including image settings, container options, and runtime detection flags.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nimage: str\nhead_image: str\nworker_image: str\ncontainer_name: str\npull_before_run: bool\nrun_options:\n    - str\nhead_run_options:\n    - str\nworker_run_options:\n    - str\ndisable_automatic_runtime_detection: bool\ndisable_shm_size_detection: bool\n```\n\n----------------------------------------\n\nTITLE: Inspecting Tabular Data Schema with Ray\nDESCRIPTION: Reads the recorded tabular data into a Ray dataset and prints its schema to confirm the columnar format of the data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import data\n\n# Read the tabular data into a Ray dataset.\nds = ray.data.read_parquet(tabular_data_path)\n# Now, print its schema.\nprint(\"Tabular data schema of expert experiences:\\n\")\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Using Google Cloud IAM for Authentication\nDESCRIPTION: Commands to use Google Cloud IAM for authentication by getting an access token and setting it as the authorization header for Ray job submission.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport RAY_JOB_HEADERS=\"{\\\"Authorization\\\": \\\"Bearer $(gcloud auth print-access-token)\\\"}\"\n\nray job submit --address http://localhost:8265  -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n```\n\n----------------------------------------\n\nTITLE: Processing Results in Submission Order Anti-Pattern in Python with Ray\nDESCRIPTION: This code snippet demonstrates the anti-pattern of processing results in submission order using ray.get(). It shows how this approach can lead to longer runtimes by waiting for slower tasks to complete before processing faster ones that finished earlier.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/ray-get-submission-order.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\n@ray.remote\ndef task(i):\n    # Simulate different execution times\n    time.sleep(i)\n    return i\n\nnum_tasks = 5\n\n# Submit tasks\nresult_refs = [task.remote(i) for i in range(num_tasks)]\n\n# Anti-pattern: Process results in submission order\nfor i, result_ref in enumerate(result_refs):\n    result = ray.get(result_ref)\n    print(f\"Task {i} finished with result {result}\")\n\n# This approach may waste time waiting for slower tasks submitted earlier\n# while faster tasks submitted later have already finished.\n```\n\n----------------------------------------\n\nTITLE: Using Counters with MetricsLogger in Python\nDESCRIPTION: Illustrates how to count operations using the 'reduce=\"sum\"' argument with MetricsLogger. The snippet demonstrates accumulation of counts over time, with or without clearing on reduction.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.utils.metrics.metrics_logger import MetricsLogger\n\nlogger = MetricsLogger()\n\nlogger.log_value(\"my_counter\", 50, reduce=\"sum\", window=None)\nlogger.log_value(\"my_counter\", 25)\nlogger.peek(\"my_counter\")  # expect: 75\n\n# Even if your logger gets \"reduced\" from time to time, the counter keeps increasing\n# because we set clear_on_reduce=False (default behavior):\nlogger.reduce()\nlogger.peek(\"my_counter\")  # still expect: 75\n\n# To clear the sum after each \"reduce\" event, set `clear_on_reduce=True`:\nlogger.log_value(\"my_temp_counter\", 50, reduce=\"sum\", window=None, clear_on_reduce=True)\nlogger.log_value(\"my_temp_counter\", 25)\nlogger.peek(\"my_counter\")  # expect: 75\nlogger.reduce()\nlogger.peek(\"my_counter\")  # expect: 0 (upon reduction, all values are cleared)\n```\n\n----------------------------------------\n\nTITLE: Defining Default Configuration for MNIST Classifier\nDESCRIPTION: A default configuration dictionary that specifies the initial hyperparameter values for the MNIST classifier, including layer sizes and learning rate.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndefault_config = {\n    \"layer_1_size\": 128,\n    \"layer_2_size\": 256,\n    \"lr\": 1e-3,\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray and Dependencies\nDESCRIPTION: Sets up the required imports and initializes Ray for distributed computing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/start.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\nfrom fastapi.responses import Response\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport requests\nimport time\nimport uuid\n\nimport ray\nfrom ray import serve\n\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Raising an Exception in Python\nDESCRIPTION: This code snippet demonstrates raising an exception in Python. It illustrates how to trigger an error condition within a Python program, which can then be handled by exception handling mechanisms. The purpose is to ensure that unexpected situations can be reported and managed properly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/ray-debugging.rst#2025-04-12_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nraise Exception(\"An exception is raised.\")\n```\n\n----------------------------------------\n\nTITLE: Setting Learning Rate in Python\nDESCRIPTION: Demonstrates how to configure the learning rate for model updates.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconfig.training(lr=0.0001)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment for Stable Diffusion Client\nDESCRIPTION: These commands create a Python virtual environment and install the required dependencies for running the Stable Diffusion client script.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/tpu-serve-stable-diffusion.md#2025-04-12_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m venv myenv\nsource myenv/bin/activate\n\npip install numpy pillow requests tqdm\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Ray Dashboard Service\nDESCRIPTION: Shell commands to setup port forwarding for accessing the Ray Dashboard with embedded Grafana panels through a local browser connection on port 8265.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward svc/raycluster-embed-grafana-head-svc 8265:8265\n# Visit http://127.0.0.1:8265/#/metrics in your browser.\n```\n\n----------------------------------------\n\nTITLE: DQN Configuration\nDESCRIPTION: This section refers to the DQNConfig class in RLlib, highlighting its training-related members. It provides a way to configure the DQN algorithm's training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-algorithms.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ray.rllib.algorithms.dqn.dqn.DQNConfig\n   :members: training\n```\n\n----------------------------------------\n\nTITLE: Fractional NPU Allocation with Huawei Ascend\nDESCRIPTION: Shows how to allocate fractional NPU resources when using Huawei Ascend hardware. This example initializes Ray with one NPU and creates four tasks that each use 25% of the NPU.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nray.init(num_cpus=4, resources={\"NPU\": 1})\n\n@ray.remote(resources={\"NPU\": 0.25})\ndef f():\n    import time\n\n    time.sleep(1)\n\n# The four tasks created here can execute concurrently\n# and share the same NPU.\nray.get([f.remote() for _ in range(4)])\n```\n\n----------------------------------------\n\nTITLE: Implementing write_row_to_file for Custom RowBasedFileDatasink in Python\nDESCRIPTION: This snippet shows how to implement the write_row_to_file method for a custom RowBasedFileDatasink. It writes image data from a row to a file using the PIL library.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/custom-datasource-example.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef write_row_to_file(\n    self,\n    row: Dict[str, Any],\n    f: \"pa.NativeFile\",\n    path: str,\n):\n    from PIL import Image\n\n    image_array = row[\"image\"]\n    image = Image.fromarray(image_array)\n    image.save(f, format=\"PNG\")\n```\n\n----------------------------------------\n\nTITLE: Compiling Ray Project Requirements with uv pip\nDESCRIPTION: Command to compile dependencies for Ray project with specific constraints, using uv pip to generate hashes and handle special packages. The command specifies multiple index sources and requirements files to create a compiled dependencies file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cu121 --find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c python/requirements_compiled_rayllm_test_py311_cu121.txt python/requirements.txt python/requirements/llm/llm-requirements.txt -o python/requirements_compiled_rayllm_py311_cu121.txt\n```\n\n----------------------------------------\n\nTITLE: Custom Text Processing PreLearner Implementation\nDESCRIPTION: Implements a custom OfflinePreLearner for processing text data into training batches, including tokenization and episode construction.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\nimport uuid\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom ray import data\nfrom ray.rllib.env.single_agent_episode import SingleAgentEpisode\nfrom ray.rllib.offline.offline_prelearner import OfflinePreLearner, SCHEMA\nfrom ray.rllib.utils.annotations import override\nfrom ray.rllib.utils.typing import EpisodeType\n\nclass TextOfflinePreLearner(OfflinePreLearner):\n\n    @staticmethod\n    @override(OfflinePreLearner)\n    def _map_to_episodes(\n        is_multi_agent: bool,\n        batch: Dict[str, Union[list, np.ndarray]],\n        schema: Dict[str, str] = SCHEMA,\n        to_numpy: bool = False,\n        input_compress_columns: Optional[List[str]] = None,\n        observation_space: gym.Space = None,\n        action_space: gym.Space = None,\n        vocabulary: Dict[str, Any] = None,\n        **kwargs: Dict[str, Any],\n    ) -> Dict[str, List[EpisodeType]]:\n\n        # If we have no vocabulary raise an error.\n        if not vocabulary:\n            raise ValueError(\n                \"No `vocabulary`. It needs a vocabulary in form of dictionary \",\n                \"mapping tokens to their IDs.\"\n            )\n        # Define container for episodes.\n        episodes = []\n\n        # Data comes in batches of string arrays under the `\"text\"` key.\n        for text in batch[\"text\"]:\n            # Split the text and tokenize.\n            tokens = text.split(\" \")\n            # Encode tokens.\n            encoded = [vocabulary[token] for token in tokens]\n            one_hot_vectors = np.zeros((len(tokens), len(vocabulary), 1, 1))\n            for i, token in enumerate(tokens):\n                if token in vocabulary:\n                    one_hot_vectors[i][vocabulary[token] - 1] = 1.0\n\n            # Build the `SingleAgentEpisode`.\n            episode = SingleAgentEpisode(\n                # Generate a unique ID.\n                id_=uuid.uuid4().hex,\n                # agent_id=\"default_policy\",\n                # module_id=\"default_policy\",\n                # We use the starting token with all added tokens as observations.\n                observations=[ohv for ohv in one_hot_vectors],\n                observation_space=observation_space,\n                # Actions are defined to be the \"chosen\" follow-up token after\n                # given the observation.\n                actions=encoded[1:],\n                action_space=action_space,\n                # Rewards are zero until the end of a sequence.\n                rewards=[0.0 for i in range(len(encoded) - 2)] + [1.0],\n                # The episode is always terminated (as sentences in the dataset are).\n                terminated=True,\n                truncated=False,\n                # No lookback. You want the episode to start at timestep zero.\n                len_lookback_buffer=0,\n                t_started=0,\n            )\n\n            # If episodes should be numpy'ized. Some connectors need this.\n            if to_numpy:\n                episode.to_numpy()\n\n            # Append the episode to the list of episodes.\n            episodes.append(episode)\n\n        # Return a batch with key `\"episodes\"`.\n        return {\"episodes\": episodes}\n\n# Define the dataset.\nds = data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\n\n# Create a vocabulary.\ntokens = []\nfor b in ds.iter_rows():\n    tokens.extend(b[\"text\"].split(\" \"))\nvocabulary = {token: idx for idx, token in enumerate(set(tokens), start=1)}\n\n# Take a small batch of 10 from the dataset.\nbatch = ds.take_batch(10)\n\n# Now use your `OfflinePreLearner`.\nepisodes = TextOfflinePreLearner._map_to_episodes(\n    is_multi_agent=False,\n    batch=batch,\n    to_numpy=True,\n    schema=None,\n    input_compress_columns=False,\n    action_space=None,\n    observation_space=None,\n    vocabulary=vocabulary,\n)\n\n# Show the constructed episodes.\nprint(f\"Episodes: {episodes}\")\n```\n\n----------------------------------------\n\nTITLE: Streaming Worker Logs with CLI\nDESCRIPTION: Demonstrates how to stream logs from a specific worker process using the Ray CLI. It uses the 'ray logs' command with the worker's PID to continuously follow the log output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nray logs worker --pid=<PID> --follow\n```\n\n----------------------------------------\n\nTITLE: Preparing Hugging Face Transformers Trainer for Ray Train\nDESCRIPTION: Code snippet demonstrating how to prepare a Hugging Face Transformers Trainer for Ray Train integration using the prepare_trainer utility function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/getting-started-transformers.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\nimport ray.train.huggingface.transformers\n\ndef train_func():\n    ...\n    trainer = transformers.Trainer(...)\n    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)\n    trainer.train()\n    ...\n```\n\n----------------------------------------\n\nTITLE: Initializing RMSProp Cache\nDESCRIPTION: This code initializes the RMSProp cache. It creates a dictionary where keys are the model's weight names and values are NumPy arrays of the same shape as the weights, initialized to zero. This cache is used to store the moving average of the squared gradients during the RMSProp update step.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_pong_example.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"rmsprop_cache = {k: np.zeros_like(v) for k, v in model.weights.items()}\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Ray Pod Logs for Serve Application Debugging\nDESCRIPTION: Commands to access a Ray pod's shell and examine Ray Serve log files. These logs contain system-level and user-level information from the Serve controller and HTTP proxy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec -it $RAY_POD -n $YOUR_NAMESPACE -- bash\n# Check the logs under /tmp/ray/session_latest/logs/serve/\n```\n\n----------------------------------------\n\nTITLE: Setting Verbose Debug Environment Variable for Ray Serialization\nDESCRIPTION: Shows how to set the RAY_PICKLE_VERBOSE_DEBUG environment variable to enable a more detailed, Python-based serialization backend for debugging purposes, though it comes with a performance penalty.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/serialization.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nRAY_PICKLE_VERBOSE_DEBUG='2'\n```\n\n----------------------------------------\n\nTITLE: Deploying KubeRay Operator and Ray Cluster\nDESCRIPTION: Shell commands to deploy the KubeRay operator and create a Ray cluster using a configuration file. Includes options for standard and autoscaling cluster deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/ml-example.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/ray/releases/2.0.0/doc/source/cluster/kubernetes/configs/xgboost-benchmark.yaml\n```\n\n----------------------------------------\n\nTITLE: Async Execution in Ray Compiled Graph\nDESCRIPTION: Example of using async/await patterns with Ray Compiled Graph for non-blocking execution in asyncio event loops.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/quickstart.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith ray.dag.InputNode() as input_node:\n    output = a.echo.bind(input_node)\n    dag = output.experimental_compile(enable_async=True)\n\n# Submit input and get future\nfuture = await dag.execute_async(\"hello\")\n# Get result\nresult = await future\n```\n\n----------------------------------------\n\nTITLE: Splitting Dataset into Train and Test Sets\nDESCRIPTION: Splits the dataset into training and testing subsets using Ray Data's train_test_split method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_detection.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_dataset, test_dataset = dataset.train_test_split(0.2)\n```\n\n----------------------------------------\n\nTITLE: Logging Results with Third-Party Library in Python\nDESCRIPTION: This code shows how to log results using a third-party logging library in a Ray Tune trainable. It filters the result dictionary and logs the filtered data with the training iteration as the step.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef log_result(self, result):\n    res_dict = {\n        str(k): v\n        for k, v in result.items()\n        if (v and \"config\" not in k and not isinstance(v, str))\n    }\n    step = result[\"training_iteration\"]\n    logging_library.log(res_dict, step=step)\n```\n\n----------------------------------------\n\nTITLE: Listing Ray Worker Pods with kubectl\nDESCRIPTION: This command lists the Ray worker pods in the default namespace that match the label selector used in the PodMonitor configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get pod -n default -l ray.io/node-type=worker\n```\n\n----------------------------------------\n\nTITLE: Modifying Training Function to Use Config Parameters in Ray Tune\nDESCRIPTION: This code updates the training function to use hyperparameters and epochs from the config dictionary, demonstrating how to pass data through search spaces.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune_get_data_in_and_out.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef training_function(config):\n    # For now, we have nothing here.\n    data = None\n\n    model = {\n        \"hyperparameter_a\": config[\"hyperparameter_a\"],\n        \"hyperparameter_b\": config[\"hyperparameter_b\"],\n    }\n    epochs = config[\"epochs\"]\n\n    # Simulate training & evaluation - we obtain back a \"metric\" and a \"trained_model\".\n    for epoch in range(epochs):\n        # Simulate doing something expensive.\n        time.sleep(1)\n        metric = (0.1 + model[\"hyperparameter_a\"] * epoch / 100) ** (\n            -1\n        ) + model[\"hyperparameter_b\"] * 0.1 * data[\"A\"].sum()\n        trained_model = {\"state\": model, \"epoch\": epoch}\n\n\ntuner = Tuner(\n    training_function,\n    param_space={\n        \"hyperparameter_a\": tune.uniform(0, 20),\n        \"hyperparameter_b\": tune.uniform(-100, 100),\n        \"epochs\": 10,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Pong Environment\nDESCRIPTION: Install the required Gymnasium library with Atari environment support\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_pong_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install gymnasium[atari]==0.28.1\n```\n\n----------------------------------------\n\nTITLE: Exporting Ray Timeline as JSON\nDESCRIPTION: Example of how to export a Ray timeline trace to a JSON file for performance analysis. This allows users to visualize task execution and identify bottlenecks outside of the Dashboard.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/optimize-performance.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\n\nray.timeline(filename=\"timeline.json\")\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Train and PyTorch (Bash)\nDESCRIPTION: This bash command installs the necessary Ray Train and PyTorch packages using pip. It includes the `-U` flag to upgrade the packages to the latest version and specifies the `ray[train]` extra to install Ray Train with all its dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install -U \\\"ray[train]\\\" torch torchvision\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Runners for Parallel Sample Collection\nDESCRIPTION: Specifies the number of EnvRunner actors to use for collecting samples during training, enabling parallel execution across multiple CPU cores.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/getting-started.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig.env_runners(num_env_runners=2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Learners for CPU\nDESCRIPTION: This snippet shows how to configure the number of learners and CPUs per learner when GPUs are not available, enabling multi-CPU training. It sets the number of learners and specifies the CPU and GPU resources for each learner.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"config.learners(\n    num_learners=2,  # or >2\n    num_cpus_per_learner=1,  # <- default\n    num_gpus_per_learner=0,  # <- default\n)\"\n```\n\n----------------------------------------\n\nTITLE: Setting Slurm sbatch Directives for Ray Job\nDESCRIPTION: Slurm sbatch directives to allocate nodes and resources for a Ray job. Specifies job name, number of nodes, CPUs, memory, and GPUs per task.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/slurm.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n#SBATCH --job-name=my-workload\n#SBATCH --nodes=4\n#SBATCH --exclusive\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=5\n#SBATCH --mem-per-cpu=1GB\n#SBATCH --gpus-per-task=1\n```\n\n----------------------------------------\n\nTITLE: System Log Format Examples\nDESCRIPTION: Examples of Ray's system logging formats for both Python and C++ components.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n%(asctime)s\\t%(levelname)s %(filename)s:%(lineno)s -- %(message)s\n```\n\nLANGUAGE: bash\nCODE:\n```\n[year-month-day, time, pid, thread_id] (component) [file]:[line] [message]\n```\n\n----------------------------------------\n\nTITLE: Modifying Ray Task CPU Requirements for Load Distribution\nDESCRIPTION: Demonstrates changing the CPU requirements in a Ray task annotation to control resource allocation. This example shows how to decrease the load on Ray nodes by increasing resource requirements per task.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/k8s-autoscaler.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(num_cpus=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(num_cpus=4)\n```\n\n----------------------------------------\n\nTITLE: Catching Subclassed Exceptions in Ray Tasks\nDESCRIPTION: Demonstrates how to catch user-defined exceptions that can be subclassed when executing Ray tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/tasks.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example catching subclassed user exceptions\n```\n\n----------------------------------------\n\nTITLE: Importing Lightning Progress Bar Callback\nDESCRIPTION: Imports the TQDM progress bar callback from PyTorch Lightning. This callback is used to display a progress bar during training to monitor progress.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import TQDMProgressBar\n```\n\n----------------------------------------\n\nTITLE: Specifying Local Working Directory in Runtime Environment\nDESCRIPTION: Example of specifying a local directory path as a working directory in a runtime environment dictionary using Python\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nruntime_env = {..., \"working_dir\": \"/some_path/example_dir\", ...}\n```\n\n----------------------------------------\n\nTITLE: HeavyLoad Deployment Configuration - Autoscaling\nDESCRIPTION: This YAML configuration defines a Ray Serve deployment named 'HeavyLoad' with autoscaling enabled. It sets the target number of ongoing requests to 1, minimum replicas to 0, initial replicas to 0, and a maximum replicas to 200. It also defines the upscale and downscale delays, as well as the upscaling and downscaling factors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/advanced-autoscaling.md#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n\"- name: HeavyLoad\n  max_ongoing_requests: 3\n  autoscaling_config:\n    target_ongoing_requests: 1\n    min_replicas: 0\n    initial_replicas: 0\n    max_replicas: 200\n    upscale_delay_s: 3\n    downscale_delay_s: 60\n    upscaling_factor: 0.3\n    downscaling_factor: 0.3\n    metrics_interval_s: 2\n    look_back_period_s: 10\"\n```\n\n----------------------------------------\n\nTITLE: Applying Batch Inference to Dataset with Ray Data\nDESCRIPTION: Uses Ray Data's map_batches operation to apply the object detection model to the entire dataset in batches.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nds = ds.map_batches(\n    ObjectDetectionModel,\n    # Use 4 model replicas. Change this number based on the number of GPUs in your cluster.\n    concurrency=4,\n    batch_size=4,  # Use the largest batch size that can fit in GPU memory.\n    # Specify 1 GPU per model replica. Set to 0 if you are doing CPU inference.\n    num_gpus=1,\n)\n```\n\n----------------------------------------\n\nTITLE: HTTP Client Script for Ray Serve in Python\nDESCRIPTION: This code snippet shows an HTTP client script that sends a POST request to a running Ray Serve FastAPI application to translate text from English to French. It requires the server to be running and accessible at the specified URL. The expected input is a JSON payload with an English sentence, and the output is a translated French sentence, returned as a JSON response.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/develop-and-deploy.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"../serve/doc_code/develop_and_deploy.py\\n:start-after: __client_function_start__\\n:end-before: __client_function_end__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Continuation in Ray Workflow\nDESCRIPTION: This snippet demonstrates how to use the workflow.continuation function to create a DAG that will be executed after the current function finishes. It shows both the workflow.run and ray.get methods for execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef bar():\n    return 10\n\n@ray.remote\ndef foo():\n    # This will return a DAG to be executed\n    # after this function is finished.\n    return workflow.continuation(bar.bind())\n\nassert ray.get(foo.bind().execute()) == 10\nassert workflow.run(foo.bind()) == 10\n```\n\n----------------------------------------\n\nTITLE: Managing Named Actors in Different Namespaces using Python\nDESCRIPTION: Demonstrates how to create and retrieve named actors in different namespaces using Ray in Python. This shows how named actors are scoped by namespace and how to access them across different jobs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/named-actors.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\nclass Actor:\n  pass\n\n# driver_1.py\n# Job 1 creates an actor, \"orange\" in the \"colors\" namespace.\nray.init(address=\"auto\", namespace=\"colors\")\nActor.options(name=\"orange\", lifetime=\"detached\").remote()\n\n# driver_2.py\n# Job 2 is now connecting to a different namespace.\nray.init(address=\"auto\", namespace=\"fruit\")\n# This fails because \"orange\" was defined in the \"colors\" namespace.\nray.get_actor(\"orange\")\n# You can also specify the namespace explicitly.\nray.get_actor(\"orange\", namespace=\"colors\")\n\n# driver_3.py\n# Job 3 connects to the original \"colors\" namespace\nray.init(address=\"auto\", namespace=\"colors\")\n# This returns the \"orange\" actor we created in the first job.\nray.get_actor(\"orange\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Filtering NYC Taxi Data from S3\nDESCRIPTION: Loads NYC Taxi dataset from S3, identifies unique dropoff location IDs, and configures the subset of data to use based on smoke test settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define some global variables.\nTARGET = \"trip_duration\"\ns3_partitions = pds.dataset(\n    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\",\n    partitioning=[\"year\", \"month\"],\n)\ns3_files = [f\"s3://anonymous@{file}\" for file in s3_partitions.files]\n\n# Obtain all location IDs\nall_location_ids = (\n    pq.read_table(s3_files[0], columns=[\"dropoff_location_id\"])[\"dropoff_location_id\"]\n    .unique()\n    .to_pylist()\n)\n# drop [264, 265]\nall_location_ids.remove(264)\nall_location_ids.remove(265)\n\n# Use smoke testing or not.\nstarting_idx = -1 if SMOKE_TEST else 0\n# TODO: drop location 199 to test error-handling before final git checkin\nsample_locations = [141, 229, 173] if SMOKE_TEST else all_location_ids\n\n# Display what data will be used.\ns3_files = s3_files[starting_idx:]\nprint(f\"NYC Taxi using {len(s3_files)} file(s)!\")\nprint(f\"s3_files: {s3_files}\")\nprint(f\"Locations: {sample_locations}\")\n```\n\n----------------------------------------\n\nTITLE: Ray remote task using requests\nDESCRIPTION: This snippet defines a Ray remote task that uses the 'requests' package. The task 'reqs' makes an HTTP GET request to 'https://www.ray.io/' and returns the status code.  This demonstrates how tasks can leverage installed pip packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef reqs():\n    return requests.get(\"https://www.ray.io/\").status_code\n```\n\n----------------------------------------\n\nTITLE: Installing Gymnasium for Atari and MuJoCo Examples\nDESCRIPTION: This snippet shows how to install Gymnasium with additional dependencies for running Atari and MuJoCo reinforcement learning environments. This setup is necessary for executing specific RL examples which require these additional components.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"gymnasium[atari,accept-rom-license,mujoco]\"\n```\n\n----------------------------------------\n\nTITLE: Stopping Ray Tune Experiment with Dictionary Criteria\nDESCRIPTION: Uses a dictionary to define stopping criteria based on the number of iterations and mean accuracy threshold. The experiment stops when either condition is met.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-stopping.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    train_function,\n    tune_config=tune.TuneConfig(\n        num_samples=5,\n        stop={\n            \"training_iteration\": 10,\n            \"mean_accuracy\": 0.8\n        }\n    ),\n    param_space={\"base_accuracy\": 0.3}\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Ray - C++\nDESCRIPTION: This C++ snippet shows how to gracefully shut down a Ray instance using `ray::Shutdown()`, ensuring that the program cleans up resources appropriately after execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n#include <ray/api.h>\nray::Init()\n... // ray program\nray::Shutdown()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for PyTorch and Ray Train\nDESCRIPTION: This snippet shows the necessary imports for PyTorch and additional libraries used in the tutorial.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nimport os\nfrom tempfile import TemporaryDirectory\n```\n\n----------------------------------------\n\nTITLE: Creating the RayCluster in Kubernetes\nDESCRIPTION: Applies the RayCluster YAML manifest to create the Ray cluster in the Kubernetes environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f ray-cluster.gke-bucket.yaml\n```\n\n----------------------------------------\n\nTITLE: Define the Q function\nDESCRIPTION: Defines the true reward function Q(theta), which represents an elliptic paraboloid. This function is what we want to optimize, but it's not directly used for training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"def Q(theta):\n    # equation for an elliptic paraboloid with a center at (0, 0, 1.2)\n    return 1.2 - (3 / 4 * theta[0] ** 2 + theta[1] ** 2)\"\n```\n\n----------------------------------------\n\nTITLE: Inefficient Parallelization of Tiny Tasks with Ray\nDESCRIPTION: Shows how parallelizing very small tasks with Ray can actually be slower than sequential execution due to the overhead of remote task invocation, scheduling, and inter-process communication.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport ray\n\n@ray.remote\ndef tiny_work(x):\n    time.sleep(0.0001) # Replace this with work you need to do.\n    return x\n\nstart = time.time()\nresult_ids = [tiny_work.remote(x) for x in range(100000)]\nresults = ray.get(result_ids)\nprint(\"duration =\", time.time() - start)\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Communication Backend in Ray Train\nDESCRIPTION: Overrides the default communication backend for PyTorch Distributed by configuring TorchConfig with a specific backend. By default, Ray Train uses NCCL for GPU training and Gloo otherwise.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchConfig, TorchTrainer\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(\n        num_workers=num_training_workers,\n        use_gpu=True, # Defaults to NCCL\n    ),\n    torch_config=TorchConfig(backend=\"gloo\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Building Ray Dashboard (Bash/JavaScript)\nDESCRIPTION: Commands to build the Ray dashboard by installing Node.js dependencies and running the build script.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd python/ray/dashboard/client\nnpm ci\nnpm run build\ncd -\n```\n\n----------------------------------------\n\nTITLE: Setting User-defined Metadata in Python\nDESCRIPTION: Example demonstrating how to add custom metadata to both a workflow and a workflow task. Workflow-level metadata is added through the run() method, while task-level metadata is added through workflow options.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/metadata.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nworkflow.run(add.options(**workflow.options(task_id=\"add_task\", metadata={\"task_k\": \"task_v\"})).bind(10, 20),\n    workflow_id=\"add_example_3\", metadata={\"workflow_k\": \"workflow_v\"})\n\nassert workflow.get_metadata(\"add_example_3\")[\"user_metadata\"] == {\"workflow_k\": \"workflow_v\"}\nassert workflow.get_metadata(\"add_example_3\", task_id=\"add_task\")[\"user_metadata\"] == {\"task_k\": \"task_v\"}\n```\n\n----------------------------------------\n\nTITLE: Memory Profiling with Jemalloc\nDESCRIPTION: Complete setup for memory profiling using jemalloc, including installation and configuration for Ray components.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/profiling.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Install jemalloc\nwget https://github.com/jemalloc/jemalloc/releases/download/5.2.1/jemalloc-5.2.1.tar.bz2 \ntar -xf jemalloc-5.2.1.tar.bz2 \ncd jemalloc-5.2.1\nexport JEMALLOC_DIR=$PWD\n./configure --enable-prof --enable-prof-libunwind \nmake\nsudo make install\n\n# Verify jeprof is installed.\nwhich jeprof\n\n# Start a Ray head node with jemalloc enabled.\n# (1) `prof_prefix` defines the path to the output profile files and the prefix of their file names.\n# (2) This example only profiles the GCS server component.\nRAY_JEMALLOC_CONF=prof:true,lg_prof_interval:33,lg_prof_sample:17,prof_final:true,prof_leak:true,prof_prefix:$PATH_TO_OUTPUT_DIR/jeprof.out \\\nRAY_JEMALLOC_LIB_PATH=$JEMALLOC_DIR/lib/libjemalloc.so \\\nRAY_JEMALLOC_PROFILE=gcs_server \\\nray start --head\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow Dependencies with Platform-Specific Requirements\nDESCRIPTION: Configuration for installing TensorFlow and related packages with specific version constraints and platform-specific conditions for Mac M1/M2 compatibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/dl-gpu-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\ntensorflow==2.15.1; python_version < '3.12' and (sys_platform != 'darwin' or platform_machine != 'arm64')\ntensorflow-macos==2.15.1; python_version < '3.12' and sys_platform == 'darwin' and platform_machine == 'arm64'\ntensorflow-probability==0.23.0; python_version < '3.12'\ntensorflow-datasets; python_version < '3.12'\n```\n\n----------------------------------------\n\nTITLE: Visualizing PBT Results with Matplotlib\nDESCRIPTION: Creates visualization plots to show parameter history and Q value history for PBT trials. This helps understand how parameters evolve during training and how the objective function improves over time.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw=dict(width_ratios=[1.5, 1]))\n\ncolors = [\"red\", \"black\"]\nlabels = [\"h = [1, 0]\", \"h = [0, 1]\"]\n\nplot_parameter_history(\n    pbt_results,\n    colors,\n    labels,\n    perturbation_interval=perturbation_interval,\n    fig=fig,\n    ax=axs[0],\n)\nplot_Q_history(pbt_results, colors, labels, ax=axs[1])\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Spilling Directory in Python\nDESCRIPTION: This snippet shows how to set a custom directory for object spilling using the ray.init() function in Python.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/object-spilling.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nray.init(object_spilling_directory=\"/path/to/spill/dir\")\n```\n\n----------------------------------------\n\nTITLE: Controlling Output File Count in Ray Data\nDESCRIPTION: Shows how to control the number of output files when writing Ray Dataset by configuring min_rows_per_write parameter.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/saving-data.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\nds.write_csv(\"/tmp/few_files/\", min_rows_per_write=75)\n\nprint(os.listdir(\"/tmp/few_files/\"))\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Ray and LightGBM\nDESCRIPTION: Installs the required Ray packages for data processing and training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightgbm/lightgbm_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU \"ray[data,train]\"\n```\n\n----------------------------------------\n\nTITLE: Untyped Application Builder Function in Python\nDESCRIPTION: Demonstrates a basic application builder function that accepts a dictionary of arguments and creates a Ray Serve deployment with configurable parameters\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/app-builder-guide.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef app_builder(args):\n    message = args.get(\"message\", \"Hello World\")\n    return Deployment.bind(HelloWorld, message)\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Size in RLlib's Offline RL API\nDESCRIPTION: This code snippet shows how to configure the training batch size using the 'train_batch_size_per_learner' attribute in RLlib's Offline RL API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .training(\n        train_batch_size_per_learner=1024,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Configuration for Ray Serve Deployment\nDESCRIPTION: YAML configuration for deploying a Ray Serve application on Kubernetes with fault tolerance considerations\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# Kubernetes configuration for Ray Serve deployment\n# Full config not shown in snippet\n```\n\n----------------------------------------\n\nTITLE: Periodic Checkpointing with Function API in Ray Tune\nDESCRIPTION: Shows how to implement periodic checkpointing with the Function API by manually checking if the current epoch is divisible by a checkpoint frequency value. This replaces the automatic checkpoint_frequency parameter available in the Class API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-trial-checkpoints.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_function(config):\n    for epoch in range(num_epochs):\n        # ... train for one epoch ...\n        \n        # Checkpoint every 3 epochs\n        checkpoint_freq = 3\n        if epoch % checkpoint_freq == 0:\n            torch.save({\"epoch\": epoch, **model.state_dict()}, \"model.pt\")\n            checkpoint = ray.train.Checkpoint.from_directory(\".\")\n            tune.report(loss=loss, checkpoint=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Basic ReplayBuffer Interaction in RLlib\nDESCRIPTION: This code shows the most basic interaction scheme with a :py:class:`~ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer`.  It demonstrates how to add data to the buffer's storage using the `add()` method and replay it using the `sample()` method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-replay-buffers.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\n\nfrom ray.rllib.utils.replay_buffers import ReplayBuffer\n\n\ndef main():\n    # Setup the buffer.\n    replay_buffer = ReplayBuffer(capacity=100)\n\n    # Add some data to the buffer.\n    for i in range(10):\n        transition = {\n            \"obs\": np.array([i]),\n            \"actions\": np.array([i]),\n            \"rewards\": np.array([i]),\n            \"next_obs\": np.array([i + 1]),\n            \"dones\": np.array([i % 2]),\n            \"truncateds\": np.array([i % 2]),\n        }\n        replay_buffer.add(**transition)\n\n    # Sample the data.\n    sample = replay_buffer.sample(num_items=5)\n\n    # Print the sample.\n    print(sample)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Profiling for Ray Compiled Graph (Bash)\nDESCRIPTION: Command to enable PyTorch profiling for a Ray Compiled Graph script. It sets the RAY_CGRAPH_ENABLE_TORCH_PROFILING environment variable to 1 before running the Python script.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/profiling.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nRAY_CGRAPH_ENABLE_TORCH_PROFILING=1 python3 example.py\n```\n\n----------------------------------------\n\nTITLE: Loading Test Data for Smoke Testing\nDESCRIPTION: Utility function to load synthetic data for quick testing and validation of the training pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-cifar.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef load_test_data():\n    trainset = torchvision.datasets.FakeData(\n        128, (3, 32, 32), num_classes=10, transform=transforms.ToTensor()\n    )\n    testset = torchvision.datasets.FakeData(\n        16, (3, 32, 32), num_classes=10, transform=transforms.ToTensor()\n    )\n    return trainset, testset\n```\n\n----------------------------------------\n\nTITLE: Initializing Training Configuration for Pytorch ResNet Finetuning\nDESCRIPTION: Defines a dictionary containing training loop configuration parameters such as input size, batch size, number of epochs, learning rate, and momentum.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_loop_config = {\n    \"input_size\": 224,  # Input image size (224 x 224)\n    \"batch_size\": 32,  # Batch size for training\n    \"num_epochs\": 10,  # Number of epochs to train for\n    \"lr\": 0.001,  # Learning Rate\n    \"momentum\": 0.9,  # SGD optimizer momentum\n}\n```\n\n----------------------------------------\n\nTITLE: Prometheus Configuration for Ray Service Discovery\nDESCRIPTION: YAML configuration for Prometheus to use file-based service discovery for scraping Ray metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/metrics.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# Prometheus config file\n\n# my global config\nglobal:\n  scrape_interval:     2s\n  evaluation_interval: 2s\n\n# Scrape from Ray.\nscrape_configs:\n- job_name: 'ray'\n  file_sd_configs:\n  - files:\n    - '/tmp/ray/prom_metrics_service_discovery.json'\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Train and TensorFlow (Bash)\nDESCRIPTION: This bash command installs the necessary Ray Train and TensorFlow packages using pip. The `-U` flag ensures the packages are upgraded, and `ray[train]` includes Ray Train dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install -U \\\"ray[train]\\\" tensorflow\"\n```\n\n----------------------------------------\n\nTITLE: Converting Tune Results to Pandas DataFrame\nDESCRIPTION: Shows how to convert training results into pandas DataFrames for additional analysis and data manipulation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Get a DataFrame with the last results for each trial\ndf = results.get_dataframe()\n\n# Get a DataFrame for all results\ndf_all = results.get_dataframe(filter_metric=\"epoch\")\n```\n\n----------------------------------------\n\nTITLE: Configuring ExecutionOptions in Ray Data (Python)\nDESCRIPTION: Demonstrates how to modify ExecutionOptions attributes in the current DataContext object. This example sets the verbose_progress option to True.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/execution-configurations.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nctx = ray.data.DataContext.get_current()\nctx.execution_options.verbose_progress = True\n```\n\n----------------------------------------\n\nTITLE: Setting NCCL Network Interface for Distributed Training\nDESCRIPTION: Configures the network interface cards used for NCCL communication between GPUs by setting the NCCL_SOCKET_IFNAME environment variable through Ray's runtime environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nruntime_env = {\"env_vars\": {\"NCCL_SOCKET_IFNAME\": \"ens5\"}}\nray.init(runtime_env=runtime_env)\n\ntrainer = TorchTrainer(...)\n```\n\n----------------------------------------\n\nTITLE: Limiting Concurrent Trials in Python\nDESCRIPTION: This snippet wraps the Optuna search algorithm with a concurrency limiter to restrict the number of trials that can run concurrently.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nalgo = ConcurrencyLimiter(algo, max_concurrent=4)\n```\n\n----------------------------------------\n\nTITLE: Configuring Learners in Python\nDESCRIPTION: Shows how to set the number of Learner actors used for model updates.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nconfig.learners(num_learners=2)\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Ray - Python\nDESCRIPTION: The Python snippet illustrates how to cleanly shut down an active Ray instance using `ray.shutdown()`, ensuring that resources are released properly after execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\nray.init()\n... # ray program\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Running Llama Fine-tuning Script\nDESCRIPTION: Basic command to launch fine-tuning for Llama-2 7B model with test mode enabled\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./run_llama_ft.sh --size=7b --as-test\n```\n\n----------------------------------------\n\nTITLE: Defining Accuracy Metrics for Model Evaluation\nDESCRIPTION: This Python snippet sets up the accuracy metric for evaluating the trained BERT model. It defines a function to compute accuracy based on the model's predictions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/bert.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Metrics\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n```\n\n----------------------------------------\n\nTITLE: Git Branch Syncing Configuration for Ray Clusters\nDESCRIPTION: YAML configuration for automatically syncing git branches to cluster workers using file mounts and setup commands.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-cli.rst#2025-04-12_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nfile_mounts: {\n    \"/tmp/current_branch_sha\": \"/path/to/local/repo/.git/refs/heads/<YOUR_BRANCH_NAME>\",\n}\n\nsetup_commands:\n    - test -e <REPO_NAME> || git clone https://github.com/<REPO_ORG>/<REPO_NAME>.git\n    - cd <REPO_NAME> && git fetch && git checkout `cat /tmp/current_branch_sha`\n```\n\n----------------------------------------\n\nTITLE: Making Inference Requests\nDESCRIPTION: Example code showing how to send inference requests to the deployed model endpoint\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/aws-neuron-core-inference.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresponse = requests.get(f\"http://127.0.0.1:8000/infer?sentence=Ray is super cool\")\nprint(response.status_code, response.json())\n```\n\n----------------------------------------\n\nTITLE: Define Custom Action Distribution Classes (Placeholder)\nDESCRIPTION: This snippet demonstrates how to define custom action distribution classes for inference, exploration, and training within RLlib's new API stack.  The placeholders `YOUR_INFERENCE_DIST_CLASS`, `YOUR_EXPLORATION_DIST_CLASS`, and `YOUR_TRAIN_DIST_CLASS` should be replaced with actual custom distribution classes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n            \"from ray.rllib.models.torch.torch_distributions import (\\n                YOUR_INFERENCE_DIST_CLASS,\\n                YOUR_EXPLORATION_DIST_CLASS,\\n                YOUR_TRAIN_DIST_CLASS,\\n            )\\n\\n                def get_inference_action_dist_cls(self):\\n                    return YOUR_INFERENCE_DIST_CLASS\\n\\n                def get_exploration_action_dist_cls(self):\\n                    return YOUR_EXPLORATION_DIST_CLASS\\n\\n                def get_train_action_dist_cls(self):\\n                    return YOUR_TRAIN_DIST_CLASS\"\n```\n\n----------------------------------------\n\nTITLE: Building Whisper Podman Image\nDESCRIPTION: This Bash snippet demonstrates building a Podman image for the Whisper application from its Dockerfile. It includes setting an environment variable for the image tag and automatically pushing it to a container registry.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/multi-app-container.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Build the image from the Dockerfile using Podman\nexport IMG1=alice/whisper_image:latest\npodman build -t $IMG1 -f whisper.Dockerfile .\n# Push to a registry. This step is unnecessary if you are deploying Serve locally.\npodman push $IMG1\n```\n\n----------------------------------------\n\nTITLE: Specifying Hugging Face Library Dependencies for Ray Project\nDESCRIPTION: This snippet defines the required versions of Hugging Face libraries used in the Ray project. It includes the Transformers library for state-of-the-art Natural Language Processing and the Accelerate library for easy use of hardware acceleration.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/core-requirements.txt#2025-04-12_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Huggingface\ntransformers==4.36.2\naccelerate==0.28.0\n```\n\n----------------------------------------\n\nTITLE: Configuring AlgorithmConfig for Offline RL in Python\nDESCRIPTION: This snippet demonstrates how to configure AlgorithmConfig for offline reinforcement learning. It sets the training batch size, enables episode-based input, configures the input read batch size, and sets the prelearner buffer capacity.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .training(\n        # Train on a batch of 1000 timesteps each iteration.\n        train_batch_size_per_learner=1000,\n    )\n    .offline_data(\n        # Read in RLlib's new stack `SingleAgentEpisode` data.\n        input_read_episodes=True\n        # Define an input read batch size of 10 episodes.\n        input_read_batch_size=10,\n        # Set the replay buffer in the `OfflinePrelearner`\n        # to 1,000 timesteps.\n        prelearner_buffer_kwargs={\n            \"capacity\": 1000,\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Rollout Fragment Length in Python\nDESCRIPTION: Shows how to configure the number of timesteps for each EnvRunner to step through environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nconfig.env_runners(rollout_fragment_length=50)\n```\n\n----------------------------------------\n\nTITLE: Job Status Monitoring\nDESCRIPTION: Complete example showing job submission, status monitoring, and log retrieval\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/sdk.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.job_submission import JobSubmissionClient, JobStatus\nimport time\n\n# If using a remote cluster, replace 127.0.0.1 with the head node's IP address.\nclient = JobSubmissionClient(\"http://127.0.0.1:8265\")\njob_id = client.submit_job(\n    # Entrypoint shell command to execute\n    entrypoint=\"python script.py\",\n    # Path to the local directory that contains the script.py file\n    runtime_env={\"working_dir\": \"./\"}\n)\nprint(job_id)\n\ndef wait_until_status(job_id, status_to_wait_for, timeout_seconds=5):\n    start = time.time()\n    while time.time() - start <= timeout_seconds:\n        status = client.get_job_status(job_id)\n        print(f\"status: {status}\")\n        if status in status_to_wait_for:\n            break\n        time.sleep(1)\n\n\nwait_until_status(job_id, {JobStatus.SUCCEEDED, JobStatus.STOPPED, JobStatus.FAILED})\nlogs = client.get_job_logs(job_id)\nprint(logs)\n```\n\n----------------------------------------\n\nTITLE: Getting Serve Applications Info with Ray Serve REST API\nDESCRIPTION: This snippet demonstrates how to retrieve cluster-level information and details about Serve applications using a GET request to the `/api/serve/applications/` endpoint. It includes the HTTP request headers required for the API call.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/api/index.md#2025-04-12_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nGET /api/serve/applications/ HTTP/1.1\nHost: http://localhost:8265/\nAccept: application/json\n```\n\n----------------------------------------\n\nTITLE: Installing Ray for Python Development (Bash)\nDESCRIPTION: Commands to install Ray and its dependencies for Python development using pip.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd python/\npip install -r requirements.txt\npip install -e . --verbose\n```\n\n----------------------------------------\n\nTITLE: Setting Storage Admin Permissions on GCS Bucket\nDESCRIPTION: Grants the IAM service account Storage Admin role permissions on the specified Google Cloud Storage bucket.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngsutil iam ch serviceAccount:my-iam-sa@my-project-id.iam.gserviceaccount.com:roles/storage.admin gs://my-bucket\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Head Node on Slurm\nDESCRIPTION: Starts the Ray head node runtime using srun as a background task. Specifies the number of CPUs and GPUs for Ray to use.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/slurm.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsrun --nodes=1 --ntasks=1 -w \"$head_node\" \\\n    ray start --head --node-ip-address=\"$head_node_ip\" \\\n        --port=6379 \\\n        --num-cpus \"${SLURM_CPUS_PER_TASK}\" --num-gpus \"${SLURM_GPUS_PER_TASK}\" --block &\n```\n\n----------------------------------------\n\nTITLE: Configuring PPO RLModule for Discrete Observations in Python\nDESCRIPTION: Demonstrates configuring a PPO RLModule using FlattenObservations for environments with discrete spaces, like FrozenLake-v1. This transformation is critical for converting integer observations to one-hot encoded vectors, suitable for processing in RL models.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.connectors.env_to_module import FlattenObservations\n\nconfig = (\n    PPOConfig()\n    .environment(\"FrozenLake-v1\")\n    .env_runners(env_to_module_connector=lambda env: FlattenObservations())\n)\n```\n\n----------------------------------------\n\nTITLE: Task Exception Handling in Python with Ray\nDESCRIPTION: Example showing basic task exception handling patterns in Ray, demonstrating how exceptions are wrapped in RayTaskError and propagated to the caller.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/tasks.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example showing how exceptions are handled and propagated\n```\n\n----------------------------------------\n\nTITLE: Objective Function for Nested Configuration in Python\nDESCRIPTION: Defines an objective function that handles nested dictionaries for conditional search spaces, adjusting the function evaluation based on given conditions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef objective_two(config):\n    width, height = config[\"width\"], config[\"height\"]\n    sub_dict = config[\"activation\"]\n    mult = sub_dict.get(\"mult\", 1)\n    \n    for step in range(config[\"steps\"]):\n        intermediate_score = evaluation_fn(step, width, height, mult)\n        tune.report({\"iterations\": step, \"mean_loss\": intermediate_score})\n        time.sleep(0.1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Offline Data Input in RLlib (Python)\nDESCRIPTION: This function configures offline data input settings for RLlib, including data materialization, preprocessing, and batching options. It allows fine-tuning of the data pipeline performance and memory usage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndef offline_data(\n    input_compress_columns: Optional[List[str]],\n    materialize_data: Optional[bool],\n    materialize_mapped_data: Optional[bool],\n    map_batches_kwargs: Optional[Dict],\n    iter_batches_kwargs: Optional[Dict],\n    prelearner_class: Optional[Type],\n    prelearner_buffer_class: Optional[Type],\n    prelearner_buffer_kwargs: Optional[Dict],\n    dataset_num_iters_per_learner: Optional[int],\n)\n```\n\n----------------------------------------\n\nTITLE: Getting detailed information about a specific actor using Python SDK\nDESCRIPTION: Using Ray's State API in Python to get detailed information about a specific actor identified by its ID. Requires providing the actor ID to retrieve the ActorState object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import get_actor\n# In this case, 31405554844820381c2f0f8501000000\nprint(get_actor(id=<ACTOR_ID>))\n```\n\n----------------------------------------\n\nTITLE: Processing Data Without Concurrency Limit in Ray\nDESCRIPTION: This snippet demonstrates a Ray task that processes data without any concurrency limit. It defines a remote function for data processing and applies it to a list of input files.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/limit-running-tasks.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef process_data(file):\n    # Load data from file into memory and process it\n    return result\n\nresults = ray.get([process_data.remote(file) for file in input_files])\n```\n\n----------------------------------------\n\nTITLE: Accessing Checkpoints and Performing Inference with MNIST Model\nDESCRIPTION: Shows how to load a checkpointed model from the best-performing trial and use it for inference on a sample MNIST image.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune_analyze_results.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom ray.tune.examples.mnist_pytorch import ConvNet, get_data_loaders\n\nmodel = ConvNet()\n\nwith best_result.checkpoint.as_directory() as checkpoint_dir:\n    # The model state dict was saved under `model.pt` by the training function\n    # imported from `ray.tune.examples.mnist_pytorch`\n    model.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"model.pt\")))\n\nimport matplotlib.pyplot as plt\n\n_, test_loader = get_data_loaders()\ntest_img = next(iter(test_loader))[0][0]\n\npredicted_class = torch.argmax(model(test_img)).item()\nprint(\"Predicted Class =\", predicted_class)\n\n# Need to reshape to (batch_size, channels, width, height)\ntest_img = test_img.numpy().reshape((1, 1, 28, 28))\nplt.figure(figsize=(2, 2))\nplt.imshow(test_img.reshape((28, 28)))\n```\n\n----------------------------------------\n\nTITLE: Accessing Actor Summary using Ray CLI\nDESCRIPTION: Command to display a summary of actors in a Ray cluster. Requires Ray installation with 'default' extras and dashboard component.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/reference/cli.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nray summary actors\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for DeepSpeed Training\nDESCRIPTION: Command to install necessary Python packages for running DeepSpeed with Ray Train, including deep learning frameworks and utilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/deepspeed.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed torch datasets transformers torchmetrics \"ray[train]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment using RLlib and Gymnasium - Tune Lambda\nDESCRIPTION: An example of providing environment information to RLlib configuration using a lambda registered with Ray Tune. This example illustrates how to use a custom environment using a creator function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-env.rst#2025-04-12_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom ray.tune.registry import register_env\n\ndef env_creator(config):\n    return MyDummyEnv(config)  # Return a gymnasium.Env instance.\n\nregister_env(\"my_env\", env_creator)\nconfig = (\n    PPOConfig()\n    .environment(\"my_env\")  # <- Tune registered string pointing to your custom env creator.\n)\nalgo = config.build()\nprint(algo.train())\n```\n\nLANGUAGE: Python\nCODE:\n```\nalgo.stop()\n```\n\n----------------------------------------\n\nTITLE: Reading Custom Data Using read_datasource in Python\nDESCRIPTION: This code snippet shows how to use the read_datasource function to read images into a Dataset using a custom ImageDatasource.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/custom-datasource-example.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_datasource(ImageDatasource([\"/path/to/image1.jpg\", \"/path/to/image2.png\"]))\n```\n\n----------------------------------------\n\nTITLE: Configuring High Priority RayJob YAML\nDESCRIPTION: YAML configuration for a production RayJob with high priority settings. Includes queue name and priority class labels for resource management in Kubernetes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n  generateName: prod-pytorch-text-classifier-\n  labels:\n    kueue.x-k8s.io/queue-name: user-queue\n    kueue.x-k8s.io/priority-class: prod-priority\n```\n\n----------------------------------------\n\nTITLE: Remote Function with Long-Running Loop in Ray\nDESCRIPTION: Definition of a remote function that runs indefinitely in a while loop, used to demonstrate how arguments are referenced in memory during task execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/memory-management.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef f(arg):\n    while True:\n        pass\n\na = ray.put(None)\nb = f.remote(a)\n```\n\n----------------------------------------\n\nTITLE: Executing Sampling Tasks in Ray\nDESCRIPTION: Launches multiple sampling tasks in parallel using Ray, distributing the workload across available resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/monte_carlo_pi.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnum_workers = 10\nsamples_per_worker = num_samples // num_workers\nresults = []\nfor _ in range(num_workers):\n    results.append(sampling_task.remote(samples_per_worker, progress_actor))\n```\n\n----------------------------------------\n\nTITLE: Logging Example for Model Multiplexing in Ray Serve\nDESCRIPTION: This example highlights lines expected in the deployment logs when loading and evicting models during a multiplexed model deployment. It illustrates how the system behaves when the number of models exceeds the configured limits.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/model-multiplexing.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nINFO 2023-05-24 01:19:03,853 default_Model default_Model#EjYmnQ CUpzhwUUNw / default replica.py:442 - Started executing request CUpzhwUUNw\nINFO 2023-05-24 01:19:03,854 default_Model default_Model#EjYmnQ CUpzhwUUNw / default multiplex.py:131 - Loading model '1'.\nINFO 2023-05-24 01:19:04,859 default_Model default_Model#EjYmnQ CUpzhwUUNw / default replica.py:542 - __CALL__ OK 1005.8ms\n\"\"\"\n```\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nINFO 2023-05-24 01:19:15,988 default_Model default_Model#rimNjA WzjTbJvbPN / default replica.py:442 - Started executing request WzjTbJvbPN\nINFO 2023-05-24 01:19:15,988 default_Model default_Model#rimNjA WzjTbJvbPN / default multiplex.py:145 - Unloading model '3'.\nINFO 2023-05-24 01:19:15,988 default_Model default_Model#rimNjA WzjTbJvbPN / default multiplex.py:131 - Loading model '4'.\nINFO 2023-05-24 01:19:16,993 default_Model default_Model#rimNjA WzjTbJvbPN / default replica.py:542 - __CALL__ OK 1005.7ms\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Docker Build and Push Commands\nDESCRIPTION: Console commands for building a custom Docker image and pushing it to DockerHub for distribution and deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/docker.md#2025-04-12_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ndocker build . -t your_dockerhub_username/custom_image_name:latest\n\ndocker image push your_dockerhub_username/custom_image_name:latest\n```\n\n----------------------------------------\n\nTITLE: Retrieving Workflow Results in Python\nDESCRIPTION: This example demonstrates how to retrieve workflow results using workflow_id and how to access individual task results using task_id.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/basics.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import workflow\n\ntry:\n    # Cleanup previous workflows\n    # An exception will be raised if it doesn't exist.\n    workflow.delete(\"add_example\")\nexcept workflow.exceptions.WorkflowNotFoundError:\n    pass\n\n@ray.remote\ndef add(left: int, right: int) -> int:\n    return left + right\n\n@ray.remote\ndef get_val() -> int:\n    return 10\n\nret = add.bind(get_val.bind(), 20)\n\nprint(workflow.run(ret, workflow_id=\"add_example\"))\n\nprint(workflow.get_output(\"add_example\"))\n# \"workflow.get_output_async\" is an asynchronous version\n```\n\n----------------------------------------\n\nTITLE: Defining Comet API Key and Project Name\nDESCRIPTION: This code snippet sets the Comet API key and project name, which are required for the Comet integration. Replace `YOUR_COMET_API_KEY` with your actual Comet API key and `YOUR_COMET_PROJECT_NAME` with the name of your Comet project. These values are used by the CometLoggerCallback to authenticate with the Comet platform and log the experiment data to the correct project.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-comet.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\napi_key = \"YOUR_COMET_API_KEY\"\nproject_name = \"YOUR_COMET_PROJECT_NAME\"\n```\n\n----------------------------------------\n\nTITLE: Analyzing Ray Tune Results with Pandas\nDESCRIPTION: Extracts various metrics (errors, checkpoints, locations, and algorithms) from the Tune results and assembles them into a pandas DataFrame for analysis. This provides a structured way to view the outcomes of the tuning experiment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# get a list of training loss errors\nerrors = [i.metrics.get(\"error\", 10000.0) for i in results]\n\n# get a list of checkpoints\ncheckpoints = [i.checkpoint for i in results]\n\n# get a list of locations\nlocations = [i.config[\"location\"] for i in results]\n\n# get a list of model params\nalgorithms = [i.config[\"algorithm\"] for i in results]\n\n# Assemble a pandas dataframe from Tune results\nresults_df = pd.DataFrame(\n    zip(locations, errors, algorithms, checkpoints),\n    columns=[\"location_id\", \"error\", \"algorithm\", \"checkpoint\"],\n)\nresults_df.head(8)\n```\n\n----------------------------------------\n\nTITLE: Custom Optimization for Dask DataFrame Shuffling on Ray in Python\nDESCRIPTION: This example shows how to leverage Dask-on-Ray's custom optimizer for DataFrame operations to speed up shuffling operations by utilizing Ray's multiple-return task capabilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/dask-on-ray.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport dask.dataframe as dd\nimport pandas as pd\nfrom ray.util.dask import enable_dask_on_ray, dataframe_optimize\n\n# Start Ray.\nray.init()\n\n# Tell Dask to use Ray as the scheduler.\nenable_dask_on_ray()\n\n# Tell Dask to use the Ray-optimized shuffle algorithm.\ndask.config.set({\"dataframe.optimize\": dataframe_optimize})\n\n# Create a Dask DataFrame\ndf = pd.DataFrame({\"x\": range(10), \"y\": range(10)})\ndask_df = dd.from_pandas(df, npartitions=2)\n\n# Perform a shuffle operation. Under the hood, this will leverage a\n# Ray-optimized implementation.\ndf_shuffled = dask_df.shuffle(on=\"x\")\nresult = df_shuffled.compute()\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Application Client\nDESCRIPTION: This Python code defines a client to interact with the deployed Ray Serve application. It sends a POST request with text data to the application's endpoint and prints the response.  It utilizes the `requests` library.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/index.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\ntext = \"This is a string of text to summarize and translate.\"\n\nresponse = requests.post(\"http://localhost:8000/\", data=text.encode(\"utf-8\"))\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Distributed Debugger Dependencies\nDESCRIPTION: Commands for creating a Python virtual environment and installing Ray with debugging support. It sets up a conda environment with Python 3.9 and installs Ray with default components along with debugpy for debugging capabilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/ray-distributed-debugger.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n myenv python=3.9\nconda activate myenv\npip install \"ray[default]\" debugpy\n```\n\n----------------------------------------\n\nTITLE: Autoregressive Action RLModule Example in RLlib\nDESCRIPTION: This example demonstrates how to implement an autoregressive action model within an RLModule, where the sampling of one action component is conditioned on the value of another. It showcases the custom forward logic required for such models.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n'''Example of a custom RLModule with auto-regressive actions.'''\nimport logging\nfrom typing import Dict, Tuple\n\nimport gymnasium as gym\nimport numpy as np\n\nimport ray\nfrom ray.rllib.core.rl_module.rl_module import RLModule\nfrom ray.rllib.core.rl_module.rl_module import ModuleID\nfrom ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\nfrom ray.rllib.examples.models.simple_net import SimpleNet\nfrom ray.rllib.models.torch.torch_distributions import TorchDistributionWrapper\nfrom ray.rllib.utils.annotations import override\nfrom ray.rllib.utils.framework import try_import_torch\nfrom ray.rllib.utils.torch_utils import FLOAT_MAX\nfrom ray.rllib.utils.typing import TensorType, ModuleConfigDict\n\ntorch, nn = try_import_torch()\n\nlogger = logging.getLogger(__name__)\n\n\nclass AutoRegressiveActionsRlModule(RLModule):\n    def __init__(self, config: ModuleConfigDict, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n\n        # Module IDs to use.\n        self.encoder_id = \"encoder\"\n        self.policy_head_a_id = \"policy_head_a\"\n        self.policy_head_b_id = \"policy_head_b\"\n\n        # The encoder is shared amongst the policy and value functions.\n        self.register_module(self.encoder_id, SimpleNet(config[\"encoder_config\"]))\n        # Policy head (produces action logits).\n        self.register_module(\n            self.policy_head_a_id,\n            SimpleNet(config[\"policy_head_a_config\"]),\n        )\n        self.register_module(\n            self.policy_head_b_id,\n            SimpleNet(config[\"policy_head_b_config\"]),\n        )\n\n    @override(RLModule)\n    def forward_train(self, batch: Dict[str, TensorType], **kwargs) -> Dict[str, TensorType]:\n        return self._forward(batch=batch, exploration=True)\n\n    @override(RLModule)\n    def forward_inference(self, batch: Dict[str, TensorType], **kwargs) -> Dict[str, TensorType]:\n        return self._forward(batch=batch, exploration=False)\n\n    def _forward(self, batch: Dict[str, TensorType], exploration: bool) -> Dict[str, TensorType]:\n        # Pass the observations through the shared encoder.\n        encoder_out = self.forward_encoder(batch)[self.encoder_id]\n\n        # Calculate action logits.\n        policy_out_a = self.forward_policy_head_a(encoder_out)[self.policy_head_a_id]\n        # Get action distribution class for action component `a`.\n        if exploration:\n            dist_class_a = self.get_exploration_action_dist_cls()\n        else:\n            dist_class_a = self.get_inference_action_dist_cls()\n        # Construct distribution object.\n        dist_a = dist_class_a(policy_out_a, self)\n        # Sample action component `a` (stochastic during training, deterministic during\n        # inference).\n        action_component_a = dist_a.sample()\n\n        # Calculate action logits for `b` (given the sampled value for `a`).\n        policy_out_b = self.forward_policy_head_b(torch.cat([encoder_out, action_component_a], dim=-1))[\n            self.policy_head_b_id\n        ]\n        # Get action distribution class for action component `b`.\n        if exploration:\n            dist_class_b = self.get_exploration_action_dist_cls()\n        else:\n            dist_class_b = self.get_inference_action_dist_cls()\n        # Construct distribution object.\n        dist_b = dist_class_b(policy_out_b, self)\n\n        return {\n            \"action_dist_inputs\": torch.cat([policy_out_a, policy_out_b], dim=-1),\n            \"actions\": torch.cat([action_component_a, dist_b.sample()], dim=-1),\n        }\n\n    def forward_encoder(self, input_dict: Dict[str, TensorType]) -> Dict[ModuleID, TensorType]:\n        # Pass the observations through the shared encoder.\n        return {self.encoder_id: self.get_sub_module(self.encoder_id)(input_dict[\"obs\"])}\n\n    def forward_policy_head_a(self, input_dict: Dict[str, TensorType]) -> Dict[ModuleID, TensorType]:\n        # Policy head (produces action logits).\n        return {self.policy_head_a_id: self.get_sub_module(self.policy_head_a_id)(input_dict)}\n\n    def forward_policy_head_b(self, input_dict: Dict[str, TensorType]) -> Dict[ModuleID, TensorType]:\n        # Policy head (produces action logits).\n        return {self.policy_head_b_id: self.get_sub_module(self.policy_head_b_id)(input_dict)}\n\n    @override(RLModule)\n    def get_train_action_dist_cls(self):\n        return TorchDistributionWrapper\n\n    @override(RLModule)\n    def get_inference_action_dist_cls(self):\n        return TorchDistributionWrapper\n\n    @override(RLModule)\n    def value_function(self, input_dict: Dict[str, TensorType]) -> TensorType:\n        raise NotImplementedError\n\n\nif __name__ == \"__main__\":\n    env_name = \"CartPole-v1\"\n    num_envs = 4\n\n    ray.init(num_cpus=4)\n\n    # 1) Define the RLModule.\n    rl_module_spec = SingleAgentRLModuleSpec(\n        module_config={\n            \"encoder_config\": {\n                \"num_units\": 64,\n            },\n            \"policy_head_a_config\": {\n                \"num_units\": 64,\n            },\n            \"policy_head_b_config\": {\n                \"num_units\": 64,\n            },\n        },\n        observation_space=gym.spaces.Box(low=-1.0, high=1.0, shape=(4,)),\n        action_space=gym.spaces.Tuple((gym.spaces.Discrete(2), gym.spaces.Discrete(2))),\n    )\n\n    # 2) Instantiate the RLModule.\n    rl_module = AutoRegressiveActionsRlModule(rl_module_spec.module_config)\n\n    # 3) Create a dummy batch (single environment).\n    dummy_batch = {\n        \"obs\": torch.from_numpy(\n            np.stack([\n                rl_module_spec.observation_space.sample() for _ in range(num_envs)\n            ]))\n    }\n\n    # 4) Perform a forward pass through the module.\n    out = rl_module.forward_train(dummy_batch)\n\n    # Print some information.\n    print(\"Dummy batch:\", dummy_batch)\n    print(\"Action outputs:\", out[\"actions\"])\n\n    ray.shutdown()\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Namespace\nDESCRIPTION: Examples of how to initialize Ray with a specific namespace across different programming languages. The namespace must be set before connecting to the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/namespaces.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nray.init(namespace=\"hello\")\n```\n\nLANGUAGE: java\nCODE:\n```\nSystem.setProperty(\"ray.job.namespace\", \"hello\"); // set it before Ray.init()\nRay.init();\n```\n\nLANGUAGE: c++\nCODE:\n```\nray::RayConfig config;\nconfig.ray_namespace = \"hello\";\nray::Init(config);\n```\n\n----------------------------------------\n\nTITLE: Requesting Image Generation from Stable Diffusion API\nDESCRIPTION: Client code for interacting with the Ray Serve Stable Diffusion API. The code constructs a request to the '/imagine' endpoint with a text prompt, receives the image data in the response, and saves it to a file. It also shows how to display the generated image in a Jupyter notebook.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/sd.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nprompt = \"a <cat-toy> is dancing on the grass.\"\ninput = \"%20\".join(prompt.split(\" \"))\nresp = requests.get(f\"http://127.0.0.1:8000/imagine?prompt={input}\")\nwith open(\"output.png\", 'wb') as f:\n    f.write(resp.content)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image\nImage(filename='ouput.png')\n```\n\n----------------------------------------\n\nTITLE: Creating and Executing a Remote Ray Task with Logging\nDESCRIPTION: Demonstrates how to initialize Ray, define a remote task function that prints a message, and execute it. The example shows how Ray's distributed logs are captured and redirected to the driver output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/key-concepts.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Initiate a driver.\nray.init()\n\n@ray.remote\ndef task_foo():\n    print(\"task!\")\n\nray.get(task_foo.remote())\n```\n\n----------------------------------------\n\nTITLE: Defining an Objective Function Example in Python\nDESCRIPTION: A sample objective function that will be maximized during the training process. This serves as the example goal throughout the documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef objective(x, a, b):\n    return a * (x ** 0.5) + b * x\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray TorchTrainer Configuration\nDESCRIPTION: Sets up a TorchTrainer with specific training loop configuration, scaling settings, and dataset parameters for distributed training using Ray Train.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import RunConfig, ScalingConfig\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func,\n    train_loop_config={\n        \"epochs\": 1,\n        \"batch_size\": batch_size,  # per device\n        \"steps_per_epoch\": steps_per_epoch,\n    },\n    scaling_config=ScalingConfig(\n        num_workers=num_workers,\n        use_gpu=use_gpu,\n        resources_per_worker={\"GPU\": 1, \"CPU\": cpus_per_worker},\n    ),\n    datasets=processed_datasets,\n    run_config=RunConfig(storage_path=storage_path),\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Batch Inference Results with Ray Data\nDESCRIPTION: Takes a small batch from the processed dataset and visualizes the object detection results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision.transforms.functional import convert_image_dtype, to_tensor\n\nbatch = ds.take_batch(batch_size=2)\nfor image, labels, boxes in zip(batch[\"image\"], batch[\"labels\"], batch[\"boxes\"]):\n    image = convert_image_dtype(to_tensor(image), torch.uint8)\n    labels = [weights.meta[\"categories\"][i] for i in labels]\n    boxes = torch.from_numpy(boxes)\n    img = to_pil_image(draw_bounding_boxes(\n        image,\n        boxes,\n        labels=labels,\n        colors=\"red\",\n        width=4,\n    ))\n    display(img)\n```\n\n----------------------------------------\n\nTITLE: Running Ray Cluster and Deploying Serve Config\nDESCRIPTION: Deploys a Serve configuration file to a local Ray cluster. The command initializes the Ray cluster and then deploys the specified configuration file using the 'serve deploy' command.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/deploy-vm.md#2025-04-12_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ ray start --head\n...\n\n$ serve deploy serve_config.yaml\n2022-06-20 17:26:31,106\tSUCC scripts.py:139 --\nSent deploy request successfully!\n * Use `serve status` to check deployments' statuses.\n * Use `serve config` to see the running app's config.\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Accelerator Type Resource (Python)\nDESCRIPTION: This code initializes Ray with a custom resource representing a specific accelerator type (NVIDIA Tesla V100). It uses `ray.util.accelerators` and `ray._private.ray_constants` to define the resource name. This initialization is hidden from the user and is used to simulate having a node with a V100 GPU.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    :hide:\n\n    ray.shutdown()\n    import ray.util.accelerators\n    import ray._private.ray_constants as ray_constants\n\n    v100_resource_name = f\"{ray_constants.RESOURCE_CONSTRAINT_PREFIX}{ray.util.accelerators.NVIDIA_TESLA_V100}\"\n    ray.init(num_gpus=4, resources={v100_resource_name: 1})\n\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Default Package with Conda\nDESCRIPTION: Installs the Ray default package from conda-forge, which includes support for the dashboard and cluster launcher. This provides a comprehensive Ray installation for most use cases.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n\"conda install -c conda-forge \\\"ray-default\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Head Node Environment Setup Commands\nDESCRIPTION: Bash commands for activating the environment on the Ray head node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/yarn.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsource environment/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Passing Callables to config.callbacks in Python\nDESCRIPTION: This snippet shows how to pass individual callables directly to the config.callbacks method of a DQNConfig. A lambda function is used to print a message when the algorithm is initialized, demonstrating a more concise approach compared to subclassing RLlibCallback.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/callback.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.dqn import DQNConfig\n\nconfig = (\n    DQNConfig()\n    .callbacks(\n        on_algorithm_init=(\n            lambda algorithm, **kwargs: print(f\"Algorithm {algorithm} has been initialized!\")\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying a RayCluster with py-spy Support\nDESCRIPTION: Command to apply a predefined RayCluster configuration with SYS_PTRACE capability enabled, allowing py-spy profiling on Ray workers and the head node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/pyspy.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.py-spy.yaml\n```\n\n----------------------------------------\n\nTITLE: Define the Qhat function\nDESCRIPTION: Defines the estimated reward function Qhat(theta, h), which depends on both the model parameters (theta) and the hyperparameters (h). This is the function we actually optimize against during training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"def Qhat(theta, h):\n    return 1.2 - (h[0] * theta[0] ** 2 + h[1] * theta[1] ** 2)\"\n```\n\n----------------------------------------\n\nTITLE: Summarizing Ray tasks using CLI command\nDESCRIPTION: The 'ray summary tasks' CLI command provides a summary of all tasks in the Ray cluster, grouping them by function name and showing their current state counts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nray summary tasks\n```\n\nLANGUAGE: text\nCODE:\n```\n======== Tasks Summary: 2022-07-22 08:54:38.332537 ========\nStats:\n------------------------------------\ntotal_actor_scheduled: 2\ntotal_actor_tasks: 0\ntotal_tasks: 2\n\n\nTable (group by func_name):\n------------------------------------\n    FUNC_OR_CLASS_NAME        STATE_COUNTS    TYPE\n0   task_running_300_seconds  RUNNING: 2      NORMAL_TASK\n1   Actor.__init__            FINISHED: 2     ACTOR_CREATION_TASK\n```\n\n----------------------------------------\n\nTITLE: Summarizing Ray tasks using Python SDK\nDESCRIPTION: Using Ray's State API in Python to get a summary of all tasks in the cluster, showing the same information as the CLI command but in Python dictionary format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import summarize_tasks\nprint(summarize_tasks())\n```\n\n----------------------------------------\n\nTITLE: Configuring PPO with torch compile during sampling\nDESCRIPTION: This Python code snippet demonstrates how to configure the PPO algorithm to use a compiled module during sampling. It sets the framework to \"torch\", enables `torch_compile_worker`, and specifies the dynamo backend and mode.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-torch2x.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nconfig = PPOConfig().framework(\n        \"torch\",\n        torch_compile_worker=True,\n        torch_compile_worker_dynamo_backend=\"ipex\",\n        torch_compile_worker_dynamo_mode=\"default\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Starting Ray with Tracing via Command Line\nDESCRIPTION: Shell command to start Ray with tracing enabled using the default tracing startup hook. This approach allows you to enable tracing when starting a Ray cluster from the command line.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/ray-tracing.rst#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ ray start --head --tracing-startup-hook=ray.util.tracing.setup_local_tmp_tracing:setup_tracing\n$ python\n    ray.init()\n    @ray.remote\n    def my_function():\n        return 1\n\n    obj_ref = my_function.remote()\n```\n\n----------------------------------------\n\nTITLE: Activating Conda Environment\nDESCRIPTION: Activates the conda environment, ensuring that subsequent Ray installation commands are executed within the correct environment. This is crucial for managing dependencies and avoiding conflicts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n\"source ~/.bash_profile\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n\"conda activate\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Ray Compiled Graph Structure (Python)\nDESCRIPTION: Code snippet demonstrating how to visualize the structure of a Ray Compiled Graph using the visualize method after compilation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/profiling.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\ndag = ray.data.range(100).map(lambda x: x + 1).map(lambda x: x * 2)\ncompiled_dag = dag.experimental_compile()\ncompiled_dag.visualize()\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables\nDESCRIPTION: This snippet illustrates how to define environment variables that are set for the Ray runtime environment. It shows how to use existing environment variables and the mechanism to reference them within the configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\"OMP_NUM_THREADS\": \"32\", \"TF_WARNINGS\": \"none\"}\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Prometheus for Ray\nDESCRIPTION: Commands to stop the Prometheus instance launched for Ray metrics collection.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/metrics.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# case 1: Ray > 2.40\nray metrics shutdown-prometheus\n\n# case 2: Otherwise\n# Run `ps aux | grep prometheus` to find the PID of the Prometheus process. Then, kill the process.\nkill <PID>\n```\n\n----------------------------------------\n\nTITLE: Checking Ray Job Output\nDESCRIPTION: kubectl command to view the logs of the Ray job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs -l=job-name=rayjob-sample\n```\n\n----------------------------------------\n\nTITLE: Properly Setting Config Values Through Methods vs. Direct Assignment in Python\nDESCRIPTION: Shows the correct way to set configuration values using methods instead of directly setting attributes, which is important for value checking and type-safety.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# WRONG!\nconfig.env = \"CartPole-v1\"  # <- don't set attributes directly\n\n# CORRECT!\nconfig.environment(env=\"CartPole-v1\")  # call the proper method\n```\n\n----------------------------------------\n\nTITLE: Animating Training Progress with Increased Population\nDESCRIPTION: Creates an animation to visualize the training progress with four PBT trials, showing more complex behaviors and interactions between multiple trials as they explore the parameter space.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmake_animation(\n    pbt_4_results,\n    colors,\n    labels,\n    perturbation_interval=perturbation_interval,\n    filename=\"pbt4.gif\",\n)\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Cluster Setup Commands\nDESCRIPTION: Series of bash commands for setting up and managing a KubeRay cluster with autoscaling, including cluster creation and pod management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-cluster.autoscaler.yaml\n\nkubectl get pods -l=ray.io/is-ray-node=yes\n\nkubectl get configmaps\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Tune with Network Filesystem (Python)\nDESCRIPTION: This example shows how to set up Ray Tune to use a network filesystem (e.g., AWS EFS) as the storage path for experiment outputs. It configures the Tuner with a shared network filesystem path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-storage.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\n\ntuner = tune.Tuner(\n    trainable,\n    run_config=tune.RunConfig(\n        name=\"experiment_name\",\n        storage_path=\"/mnt/path/to/shared/storage/\",\n    )\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Disabling Log Redirection to Driver\nDESCRIPTION: Shows how to disable the default behavior of routing all worker logs to the driver, which can be useful in large-scale deployments to reduce log volume.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# Task and Actor logs are not copied to the driver stdout.\nray.init(log_to_driver=False)\n```\n\n----------------------------------------\n\nTITLE: Running Inference Benchmark\nDESCRIPTION: This command executes the inference benchmark script, allowing users to specify the dynamo backend, mode, and batch size. This benchmark measures the performance of RLModule.forward_exploration with and without torch.compile().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-torch2x.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython ./run_inference_bm.py --backend <dynamo_backend> --mode <dynamo_mode> -bs <batch_size>\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Optuna and Ray Tune in Python\nDESCRIPTION: This snippet imports the necessary libraries required for setting up the Optuna optimization and Ray Tune functionalities. It includes Ray, time for simulation, and types for type annotations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom typing import Dict, Optional, Any\n\nimport ray\nfrom ray import tune\nfrom ray.tune.search import ConcurrencyLimiter\nfrom ray.tune.search.optuna import OptunaSearch\n```\n\n----------------------------------------\n\nTITLE: IMPALA Configuration\nDESCRIPTION: This section refers to the IMPALAConfig class in RLlib, highlighting its training-related members. It provides a way to configure the IMPALA algorithm's training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-algorithms.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ray.rllib.algorithms.impala.impala.IMPALAConfig\n   :members: training\n```\n\n----------------------------------------\n\nTITLE: Checking Ray Initialization - Java\nDESCRIPTION: In this snippet, the initial state of the Ray runtime is verified using `Ray.isInitialized()` in Java, ensuring that the environment is correctly set up before proceeding.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nimport io.ray.api.Ray;\n\npublic class MyRayApp {\n\n  public static void main(String[] args) {\n    Ray.init();\n    Assert.assertTrue(Ray.isInitialized());\n    Ray.shutdown();\n    Assert.assertFalse(Ray.isInitialized());\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Training Function with Ray Tune\nDESCRIPTION: This code defines a simple training function that simulates a `loss` metric. It uses `numpy` for random number generation and `ray.tune` for reporting the loss during each iteration. The function takes a `config` dictionary as input, which specifies the mean and standard deviation for generating the loss values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-comet.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom ray import tune\n\n\ndef train_function(config):\n    for i in range(30):\n        loss = config[\"mean\"] + config[\"sd\"] * np.random.randn()\n        tune.report({\"loss\": loss})\n```\n\n----------------------------------------\n\nTITLE: Documenting Ray Memory Command in reStructuredText\nDESCRIPTION: This snippet uses the click directive to generate documentation for the 'ray memory' command from the ray.scripts.scripts module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/cli.rst#2025-04-12_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. click:: ray.scripts.scripts:memory\n   :prog: ray memory\n   :show-nested:\n```\n\n----------------------------------------\n\nTITLE: Incrementing Counter Actor in Python\nDESCRIPTION: Python script that connects to the existing Ray cluster, retrieves the detached counter actor by name, and increments its value. Demonstrates actor persistence and accessibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(namespace=\"default_namespace\")\ncounter = ray.get_actor(\"counter_actor\")\nprint(ray.get(counter.increment.remote()))\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Agent Environment in RLlib\nDESCRIPTION: Python configuration for setting up a multi-agent environment with custom policy mapping\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.examples.envs.classes.multi_agent import MultiAgentCartPole\nfrom ray import tune\ntune.register_env(\"env\", lambda cfg: MultiAgentCartPole(cfg))\nconfig.environment(\"env\", env_config={\"num_agents\": 2})\nconfig.multi_agent(\n    policies={\"p0\", \"p1\"},\n    policy_mapping_fn=lambda aid, *a, **kw: f\"p{aid}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies: fsspec Configuration\nDESCRIPTION: Definition of the fsspec package dependency with version 2023.5.0 and corresponding SHA256 hashes. The comment indicates this package is required via python/requirements.txt and python/requirements_compiled_ray_test_py311_cu121.txt files.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nfsspec==2023.5.0 \\\n    --hash=sha256:51a4ad01a5bb66fcc58036e288c0d53d3975a0df2a5dc59a93b59bade0391f2a \\\n    --hash=sha256:b3b56e00fb93ea321bc9e5d9cf6f8522a0198b20eb24e02774d329e9c6fb84ce\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Learning Rate Schedule\nDESCRIPTION: This snippet shows how to configure a learning rate schedule using the `training()` method in the `AlgorithmConfig` object. It uses a list of 2-tuples to define the learning rate at different timesteps.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"config.training(\n    lr=[\n        [0, 1e-5],  # <- initial value at timestep 0\n        [1000000, 1e-4],  # <- final value at 1M timesteps\n    ],\n)\"\n```\n\n----------------------------------------\n\nTITLE: Printing Required APIs for SAC Algorithm in Python\nDESCRIPTION: Example of how to print the required APIs for a specific algorithm (SAC in this case) that custom RLModules need to implement.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Import the config of the algorithm of your choice.\nfrom ray.rllib.algorithms.sac import SACConfig\n\n# Print out the abstract APIs, you need to subclass from and whose\n# abstract methods you need to implement, besides the ``setup()`` and ``_forward_..()``\n# methods.\nprint(\n    SACConfig()\n    .get_default_learner_class()\n    .rl_module_required_apis()\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Prometheus for Ray Metrics Collection\nDESCRIPTION: Command to download and start Prometheus locally with a configuration that scrapes metrics from a local Ray Cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/metrics.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nray metrics launch-prometheus\n```\n\n----------------------------------------\n\nTITLE: Accessing Result Path in Ray Train\nDESCRIPTION: This snippet demonstrates how to access the storage path of a Ray Train `Result` object. The `result.path` attribute returns the path where the training results are stored.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/results.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Access the path where results are stored.\"\"\"\n# Get the path where the results are stored.\npath = result.path\nprint(f\"Path: {path}\")\n\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Verification for Ray Job Submission Client in Python\nDESCRIPTION: This snippet demonstrates how to create a JobSubmissionClient instance with custom TLS verification. It shows how to specify a custom certificate file for HTTPS connections to the job server.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/sdk.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclient = JobSubmissionClient(\"https://<job-server-url>\", verify=\"/path/to/cert.pem\")\n```\n\n----------------------------------------\n\nTITLE: Stopping a Job using Ray Jobs REST API in Python\nDESCRIPTION: This snippet demonstrates how to stop a job using the Ray Jobs REST API. It sends a POST request to the job's stop endpoint and prints the 'stopped' status from the response.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/rest.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport time\nimport requests\n\nresp = requests.post(\n    \"http://127.0.0.1:8265/api/jobs/{job_or_submission_id}/stop\",\n)\nrst = json.loads(resp.text)\njson = rst.json()\nstopped = json[\"stopped\"]\nprint(stopped)\n```\n\n----------------------------------------\n\nTITLE: Executing Final Reduce and Results Collection\nDESCRIPTION: Executes the reduce phase across partitions and collects final word counts. Includes sorting and displaying results in descending order of frequency.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/map_reduce.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noutputs = []\nfor i in range(num_partitions):\n    outputs.append(\n        apply_reduce.remote(*[partition[i] for partition in map_results])\n    )\n\ncounts = {k: v for output in ray.get(outputs) for k, v in output.items()}\n\nsorted_counts = sorted(counts.items(), key=lambda item: item[1], reverse=True)\nfor count in sorted_counts:\n    print(f\"{count[0].decode('utf-8')}: {count[1]}\")\n```\n\n----------------------------------------\n\nTITLE: Serve Run CLI Configuration\nDESCRIPTION: Example of passing arguments to a Ray Serve application using the serve run command line interface\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/app-builder-guide.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nserve run hello:app_builder message=\"Hello from CLI\"\n```\n\n----------------------------------------\n\nTITLE: Create and Retrieve Job-Scoped Named Placement Group in C++\nDESCRIPTION: This C++ snippet demonstrates how to create a non-global (job-scoped) named placement group and retrieve it within the same job. It leverages the `ray::CreatePlacementGroup` and `ray::GetPlacementGroup` functions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_17\n\nLANGUAGE: c++\nCODE:\n```\n\"          // Create a placement group with a job-scope-unique name.\n          std::vector<std::unordered_map<std::string, double>> bundles{{{\"CPU\", 1.0}}};\n\n          ray::PlacementGroupCreationOptions options{\n              false/*non-global*/, \"non_global_name\", bundles, ray::PlacementStrategy::STRICT_SPREAD};\n\n          ray::PlacementGroup pg = ray::CreatePlacementGroup(options);\n          pg.Wait(60);\n\n          ...\n\n          // Retrieve the placement group later somewhere in the same job.\n          ray::PlacementGroup group = ray::GetPlacementGroup(\"non_global_name\");\n          assert(!group.Empty());\"\n```\n\n----------------------------------------\n\nTITLE: Installing RayCluster with GCS FT\nDESCRIPTION: Shell commands to download and apply the Ray cluster configuration with external Redis for GCS fault tolerance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.external-redis.yaml\nkubectl apply -f ray-cluster.external-redis.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Training Dataset\nDESCRIPTION: Python command to generate training and test datasets along with special tokens\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython create_dataset.py\n```\n\n----------------------------------------\n\nTITLE: Custom ReplayBuffer Implementation in RLlib\nDESCRIPTION: This code snippet demonstrates how to implement a custom ReplayBuffer class in RLlib and use it with the SimpleQ algorithm. It outlines the structure of a custom buffer, showcasing how to override methods and integrate it into an RLlib configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-replay-buffers.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\n\nfrom ray.rllib.algorithms.simple_q import SimpleQConfig\nfrom ray.rllib.env import EnvContext, PettingZooEnv\nfrom ray.rllib.examples.env.rock_paper_scissors import RockPaperScissorsEnv\nfrom ray.rllib.policy.sample_batch import SampleBatch\nfrom ray.rllib.utils.replay_buffers import ReplayBuffer\n\n\nclass MyReplayBuffer(ReplayBuffer):\n    def __init__(self, capacity: int = 100):\n        super().__init__(capacity=capacity)\n\n    def add(self, **kwargs) -> None:\n        print(\"called add in MyReplayBuffer!\")\n        super().add(**kwargs)\n\n    def sample(self, num_items: int, **kwargs) -> SampleBatch:\n        print(\"called sample in MyReplayBuffer!\")\n        return super().sample(num_items=num_items, **kwargs)\n\n\nif __name__ == \"__main__\":\n    # This example shows how to configure a replay buffer when running an experiment.\n    # This is a toy example, but it shows how to use a custom replay buffer with SimpleQ.\n    env_config = {\"max_cycles\": 20}\n    # We set `disable_env_checking` to True because `gymnasium.make` will check\n    # the env (that we are wrapping with PettingZooEnv) and throw an error if the\n    # observation/action spaces do not match, but we do our own checking within\n    # PettingZooEnv already.\n    # Also note that we need to set render_mode=None as rendering is not supported.\n    rock_paper_scissors_env = gym.make(\"RockPaperScissors-v0\", **env_config, render_mode=None)\n    wrapped_env = PettingZooEnv(rock_paper_scissors_env, disable_env_checking=True)\n\n    config = (\n        SimpleQConfig()\n        .environment(wrapped_env, env_config=env_config)\n        .rollouts(num_rollout_workers=0)\n        .training(train_batch_size=64)\n        .replay_buffer_config(type=MyReplayBuffer)\n        .framework(\"torch\")\n    )\n\n    # Now execute the configuration using Tune.\n    from ray import tune\n\n    tune.Tuner(\"SimpleQ\", param_space=config.to_dict(), run_config=tune.RunConfig(stop={\"training_iteration\": 1})).fit()\n```\n\n----------------------------------------\n\nTITLE: Downloading CloudWatch Config Files (Bash)\nDESCRIPTION: These Bash commands create a directory for CloudWatch configurations and download example agent and dashboard config files from the Ray GitHub repository.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ mkdir cloudwatch\n$ cd cloudwatch\n$ wget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/aws/cloudwatch/example-cloudwatch-agent-config.json\n$ wget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/aws/cloudwatch/example-cloudwatch-dashboard-config.json\n```\n\n----------------------------------------\n\nTITLE: Installing Ray from a specific commit example\nDESCRIPTION: Concrete example for installing Ray 3.0.0.dev0 for Python 3.9 on MacOS from commit 4f2ec46c3adb6ba9f412f09a9732f436c4a5d0c9.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install https://s3-us-west-2.amazonaws.com/ray-wheels/master/4f2ec46c3adb6ba9f412f09a9732f436c4a5d0c9/ray-3.0.0.dev0-cp39-cp39-macosx_10_15_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Ray Tune and Forecasting\nDESCRIPTION: Imports the necessary Python libraries for data manipulation, visualization, forecasting, and Ray Tune integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/start.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, AutoETS, MSTL\n\nfrom ray import train, tune\nfrom ray.train import RunConfig\n```\n\n----------------------------------------\n\nTITLE: RayJob Metadata Configuration for Higher Priority\nDESCRIPTION: YAML snippet showing how to modify RayJob metadata to use the higher priority class for demonstrating preemption.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n  generateName: rayjob-sample-\n  labels:\n    kueue.x-k8s.io/queue-name: user-queue\n    kueue.x-k8s.io/priority-class: prod-priority\n```\n\n----------------------------------------\n\nTITLE: Implementing Constructor for Custom RowBasedFileDatasink in Python\nDESCRIPTION: This snippet demonstrates how to implement the constructor for a custom RowBasedFileDatasink class. It calls the superclass constructor and specifies the output directory and file format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/custom-datasource-example.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, output_dir: str):\n    super().__init__(output_dir, file_format=\"png\")\n```\n\n----------------------------------------\n\nTITLE: Working with Anonymous Namespaces\nDESCRIPTION: Examples of using anonymous namespaces when no explicit namespace is specified, demonstrating how each job gets its own unique namespace.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/namespaces.rst#2025-04-12_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// Job 1 connects to an anonymous namespace by default.\nSystem.setProperty(\"ray.address\", \"localhost:10001\");\ntry {\n    Ray.init();\n    Ray.actor(Actor::new).setName(\"my_actor\").remote();\n} finally {\n    Ray.shutdown();\n}\n```\n\nLANGUAGE: c++\nCODE:\n```\n// Job 1 connects to an anonymous namespace by default.\nray::RayConfig config;\nray::Init(config);\nray::Actor(RAY_FUNC(Counter::FactoryCreate)).SetName(\"my_actor\").Remote();\nray::Shutdown();\n```\n\n----------------------------------------\n\nTITLE: Ray Cluster Management Commands\nDESCRIPTION: Series of commands to download configuration template, start, connect to, and tear down a Ray cluster using the cluster launcher.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Download the example-full.yaml\nwget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/local/example-full.yaml\n\n# Update the example-full.yaml to update head_ip, worker_ips, and ssh_user.\n# vi example-full.yaml\n\n# Create or update the cluster. When the command finishes, it will print\n# out the command that can be used to SSH into the cluster head node.\nray up example-full.yaml\n\n# Get a remote screen on the head node.\nray attach example-full.yaml\n# Try running a Ray program.\n\n# Tear down the cluster.\nray down example-full.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying Serve Locally and Testing\nDESCRIPTION: This Python snippet shows how to add code for deploying and testing a Ray Serve setup locally. It builds on previous examples to ensure local functionality. Code must be executed in a Python environment supporting Ray Serve.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dev-workflow.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n```{literalinclude} ../doc_code/local_dev.py\n:start-after: __local_dev_handle_start__\n:end-before: __local_dev_handle_end__\n:language: python\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generating CA Private Key and Self-Signed Certificate\nDESCRIPTION: Commands to generate a self-signed certificate and private key for the Certificate Authority (CA), and instructions to create a Kubernetes Secret.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/tls.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Step 1-1: Generate a self-signed certificate and a new private key file for CA.\nopenssl req -x509 \\\n            -sha256 -days 3650 \\\n            -nodes \\\n            -newkey rsa:2048 \\\n            -subj \"/CN=*.kuberay.com/C=US/L=San Francisco\" \\\n            -keyout ca.key -out ca.crt\n\n# Step 1-2: Check the CA's public key from the self-signed certificate.\nopenssl x509 -in ca.crt -noout -text\n\n# Step 1-3\n# Method 1: Use `cat $FILENAME | base64` to encode `ca.key` and `ca.crt`.\n#           Then, paste the encoding strings to the Kubernetes Secret in `ray-cluster.tls.yaml`.\n\n# Method 2: Use kubectl to encode the certificate as Kubernetes Secret automatically.\n#           (Note: You should comment out the Kubernetes Secret in `ray-cluster.tls.yaml`.)\nkubectl create secret generic ca-tls --from-file=ca.key --from-file=ca.crt\n```\n\n----------------------------------------\n\nTITLE: Reading S3 Parquet Files with Ray Data in Python\nDESCRIPTION: Shows how to read Parquet files from Amazon S3 using Ray Data's read_parquet function. The example specifies an S3 URI with the 's3://' scheme.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Running Ray Application with Exception for Post-Mortem Debugging\nDESCRIPTION: Command to execute a Ray application with an argument that triggers an exception, enabling post-mortem debugging. This illustrates how to intentionally cause an exception for debugging purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/ray-distributed-debugger.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython job.py raise-exception\n```\n\n----------------------------------------\n\nTITLE: Listing all Ray actors using Python SDK\nDESCRIPTION: Using Ray's State API in Python to list all actors in the cluster, returning ActorState objects that contain detailed information about each actor.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import list_actors\nprint(list_actors())\n```\n\n----------------------------------------\n\nTITLE: Defining Llama2-7b Model Deployment for HPU Inference in Python\nDESCRIPTION: This code snippet defines a deployment that serves a Llama2-7b model using an HPU. It enables HPU graph optimizations for better performance and sets up the model for inference tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n__model_def_start__\n__model_def_end__\n```\n\n----------------------------------------\n\nTITLE: Comparing PyArrow Table and Ray Data APIs\nDESCRIPTION: This code snippet presents a table comparing common PyArrow Table operations with their Ray Data equivalents. It covers operations like getting schema, counting rows, filtering, and various data transformations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/from_other_data_libs.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npa.Table.schema        # PyArrow\nds.schema()            # Ray Data\n\npa.Table.num_rows      # PyArrow\nds.count()             # Ray Data\n\npa.Table.filter()      # PyArrow\nds.filter()            # Ray Data\n\npa.Table.drop()        # PyArrow\nds.drop_columns()      # Ray Data\n\npa.Table.add_column()  # PyArrow\nds.add_column()        # Ray Data\n\npa.Table.groupby()     # PyArrow\nds.groupby()           # Ray Data\n\npa.Table.sort_by()     # PyArrow\nds.sort()              # Ray Data\n```\n\n----------------------------------------\n\nTITLE: Example YAML Configuration for a Text ML Model in Ray Serve\nDESCRIPTION: This YAML snippet illustrates a specific configuration for a Text ML Model, showcasing settings for proxy location, HTTP options, and application deployments. It includes details on runtime environment dependencies and deployment parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/config.md#2025-04-12_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nproxy_location: EveryNode\n\nhttp_options:\n  host: 0.0.0.0\n  port: 8000\n\napplications:\n- name: default\n  route_prefix: /\n  import_path: text_ml:app\n  runtime_env:\n    pip:\n      - torch\n      - transformers\n  deployments:\n  - name: Translator\n    num_replicas: 1\n    user_config:\n      language: french\n  - name: Summarizer\n    num_replicas: 1\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray in Python\nDESCRIPTION: Python code to initialize Ray in a Jupyter notebook when using Azure portal deployment method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/azure.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray; ray.init()\n```\n\n----------------------------------------\n\nTITLE: Logging Maximum Values with MetricsLogger in Python\nDESCRIPTION: Logs a maximum value with the MetricsLogger, using the 'reduce=\"max\"' option. This allows tracking the highest value encountered in a series of metrics, supporting infinite window reduction.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Log a maximum value.\nlogger.log_value(key=\"max_value\", value=0.0, reduce=\"max\")\n```\n\n----------------------------------------\n\nTITLE: Default Concurrency Group Configuration in Python\nDESCRIPTION: Example showing how to configure default concurrency group settings alongside custom groups in Python Ray actor.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/concurrency_group_api.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(concurrency_groups={\"io\": 2})\nclass AsyncIOActor:\n    async def f1(self):\n        pass\n\nactor = AsyncIOActor.options(max_concurrency=10).remote()\n```\n\n----------------------------------------\n\nTITLE: Setting Warning Threshold for Long Operations\nDESCRIPTION: TUNE_WARN_THRESHOLD_S sets the threshold (in seconds) for logging warnings when Tune event loop operations take too long. Defaults to 0.5 seconds.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_WARN_THRESHOLD_S=0.5\n```\n\n----------------------------------------\n\nTITLE: Setting Permissions for netrc File in Bash\nDESCRIPTION: Command to set the correct read/write permissions for the .netrc file to ensure security.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/runtime_env_auth.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nchmod 600 \"$HOME/.netrc\"\n```\n\n----------------------------------------\n\nTITLE: Example PriorityClass Configuration in YAML\nDESCRIPTION: YAML snippet showing how to configure a RayCluster with Volcano scheduler and priority class name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nlabels:\n  ray.io/scheduler-name: volcano\n  ray.io/priority-class-name: <replace with correct PriorityClass resource name>\n```\n\n----------------------------------------\n\nTITLE: Restoring Algorithm's RLModule from Checkpoint Path\nDESCRIPTION: This code shows how to restore the saved RLModule state into an existing algorithm. It demonstrates specifying the component path within the algorithm to apply the checkpoint correctly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nppo.restore_from_path(\n    module_ckpt_path,  # <- NOT an Algorithm checkpoint, but single-agent RLModule one.\n\n    # Therefore, we have to provide the exact path (of RLlib components) down\n    # to the individual RLModule within the algorithm, which is:\n    component=\"learner_group/learner/rl_module/default_policy\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Storage Path for Checkpoints\nDESCRIPTION: Example configuration of storage path for persisting model checkpoints to external storage. The code shows options for either S3 cloud storage or NFS paths to ensure checkpoint accessibility after multi-node training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstorage_path=\"s3://your-bucket-here\"  # TODO: Set up cloud storage\n# storage_path=\"/mnt/path/to/nfs\"     # TODO: Alternatively, set up NFS\n```\n\n----------------------------------------\n\nTITLE: Running PBT Experiment with Ray Tune\nDESCRIPTION: Executes the Population Based Training experiment by calling the fit method on the tuner object, which launches the trials and returns the results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npbt_results = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Kubeconfig Credentials\nDESCRIPTION: Gets and updates the kubeconfig file with credentials for the newly created AKS cluster\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/azure-aks-gpu-cluster.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naz aks get-credentials --resource-group kuberay-rg \\\n    --name kuberay-gpu-cluster \\\n    --overwrite-existing\n```\n\n----------------------------------------\n\nTITLE: Enabling Push-Based Shuffle in Ray Dataset\nDESCRIPTION: This code shows how to enable push-based shuffle for large-scale performance improvement in Ray Datasets. It sets the use_push_based_shuffle flag in the DataContext to True.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/shuffling-data.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nctx = ray.data.DataContext.get_current()\nctx.use_push_based_shuffle = True\n\nds = (\n    ray.data.range(1000)\n    .random_shuffle()\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Dataset\nDESCRIPTION: Loads the 'tiny_shakespeare' dataset using the datasets library and converts it into a Ray dataset for distributed processing. This prepares the text data for fine-tuning the GPT-J model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nprint(\"Loading tiny_shakespeare dataset\")\ncurrent_dataset = load_dataset(\"tiny_shakespeare\")\ncurrent_dataset\n```\n\nLANGUAGE: python\nCODE:\n```\nimport ray.data\n\nray_datasets = {\n    \"train\": ray.data.from_huggingface(current_dataset[\"train\"]),\n    \"validation\": ray.data.from_huggingface(current_dataset[\"validation\"]),\n}\n\nray_datasets\n```\n\n----------------------------------------\n\nTITLE: Running Ray Serve with Runtime Environment to Fix Dependencies\nDESCRIPTION: Commands to restart the Ray Serve application with the required python-multipart dependency using a runtime environment configuration. This resolves the form parsing error.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md#2025-04-12_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n# In the head Pod, stop the Ray Serve application\nserve shutdown\n\n# Check the Ray Serve application status\nserve status\n# [Example output]\n# There are no applications running on this cluster.\n\n# Launch the Ray Serve application with runtime environment.\nserve run mobilenet.mobilenet:app --runtime-env-json='{\"pip\": [\"python-multipart==0.0.6\"]}'\n\n# (On your local machine) Submit a request to the Ray Serve application again, and you should get the correct prediction.\npython3 mobilenet_req.py\n# [Example output]\n# {\"prediction\": [\"n02123159\", \"tiger_cat\", 0.2994779646396637]}\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for Ray CI Reproduction Tool\nDESCRIPTION: This snippet lists Python package dependencies required for the repro-ci.py script, including boto3 for AWS interaction, click for CLI development, paramiko for SSH connectivity, pyyaml for YAML parsing, and pybuildkite for BuildKite API integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/ci/repro-ci-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Dependencies for running repro-ci.py\nboto3\nclick\nparamiko\npyyaml\npybuildkite\n```\n\n----------------------------------------\n\nTITLE: Waiting for Placement Group Readiness in Python\nDESCRIPTION: This snippet shows how to wait for a placement group to be ready in Python using either the ready() or wait() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Wait for pg to be created\nray.get(pg.ready())\n\n# You can also use the `.wait()` method\npg.wait(timeout_seconds=60)\n```\n\n----------------------------------------\n\nTITLE: Installing Kubernetes Prometheus Stack\nDESCRIPTION: Shell commands to install the Kubernetes Prometheus Stack using a provided install script and verify the installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# Path: kuberay/\n./install/prometheus/install.sh\n\n# Check the installation\nkubectl get all -n prometheus-system\n\n# (part of the output)\n# NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\n# deployment.apps/prometheus-grafana                    1/1     1            1           46s\n# deployment.apps/prometheus-kube-prometheus-operator   1/1     1            1           46s\n# deployment.apps/prometheus-kube-state-metrics         1/1     1            1           46s\n```\n\n----------------------------------------\n\nTITLE: Printing Best Hyperparameters (explicit metric)\nDESCRIPTION: Prints the best hyperparameters found by the Tune experiment, explicitly specifying the metric and mode. This is necessary if the metric and mode were not set in the TuneConfig.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"print(\\\"Best hyperparameters found were: \\\", results.get_best_result(\\\"mean_loss\\\", \\\"min\\\").config)\"\n```\n\n----------------------------------------\n\nTITLE: Creating GPU Node Pool in GKE Cluster\nDESCRIPTION: This command adds a GPU node pool to the existing cluster. It specifies an NVIDIA L4 GPU, uses the g2-standard-4 machine type, and enables autoscaling with a minimum of 0 and maximum of 1 node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-l4-vws,count=1 \\\n  --zone us-west1-b \\\n  --cluster kuberay-gpu-cluster \\\n  --num-nodes 1 \\\n  --min-nodes 0 \\\n  --max-nodes 1 \\\n  --enable-autoscaling \\\n  --machine-type g2-standard-4\n```\n\n----------------------------------------\n\nTITLE: Increasing File Descriptors Limit for Ray in Bash\nDESCRIPTION: This snippet provides commands to increase the limit of open file descriptors for Ray components to avoid running out of file descriptors in a clustered environment. It is crucial for ensuring the stability of connections in a large Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-failures.rst#2025-04-12_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n# Start head node components with higher ulimit.\nulimit -n 65536 ray start --head\n\n# Start worker node components with higher ulimit.\nulimit -n 65536 ray start --address <head_node>\n\n# Start a Ray driver with higher ulimit.\nulimit -n 65536 <python script>\n```\n\n----------------------------------------\n\nTITLE: Setting Event Stats Print Interval (Bash)\nDESCRIPTION: This bash command sets the `RAY_event_stats_print_interval_ms` environment variable to 1000, causing the worker kill summary to be printed every second.  The default interval is one minute. This is useful for observing memory monitor activity more frequently.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/ray-oom-prevention.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRAY_event_stats_print_interval_ms=1000 python oom.py\n```\n\n----------------------------------------\n\nTITLE: Deploying a RayJob with Kueue Integration\nDESCRIPTION: Command to create a RayJob custom resource in Kubernetes with Kueue integration. This deploys the job to be managed by Kueue for priority scheduling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f ray-job.pytorch-distributed-training.yaml\n```\n\n----------------------------------------\n\nTITLE: Querying Scikit-learn Model via HTTP\nDESCRIPTION: Python code to send a test request to the deployed Scikit-learn classifier\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/serve-ml-models.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nsample_request_input = {\n    \"sepal length\": 1.2,\n    \"sepal width\": 1.0,\n    \"petal length\": 1.1,\n    \"petal width\": 0.9,\n}\nresponse = requests.get(\"http://localhost:8000/\", json=sample_request_input)\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Reading Binary Files with Ray Data in Python\nDESCRIPTION: Shows how to read binary files using Ray Data's read_binary_files function. The example reads from an S3 bucket and prints the schema of the resulting dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_binary_files(\"s3://anonymous@ray-example-data/documents\")\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Checking RayJob Status with kubectl\nDESCRIPTION: Shows how to check the status of a RayJob using kubectl, demonstrating the suspended state when waiting for GPU resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get rayjob pytorch-text-classifier-rj4sg -o yaml\napiVersion: ray.io/v1\nkind: RayJob\nmetadata:\n  name: pytorch-text-classifier-rj4sg\n  labels:\n    kueue.x-k8s.io/queue-name: user-queue\n...\n...\n...\nstatus:\n  jobDeploymentStatus: Suspended  # RayJob suspended\n  jobId: pytorch-text-classifier-rj4sg-pj9hx\n  jobStatus: PENDING\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Runners in Python\nDESCRIPTION: Demonstrates how to set the number of EnvRunner actors and configure environment vectorization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconfig.env_runners(num_env_runners=4)\n\n# Also use `num_envs_per_env_runner` to vectorize your environment on each EnvRunner actor.\n# Note that this option is only available in single-agent setups.\n#  The Ray Team is working on a solution for this restriction.\nconfig.env_runners(num_envs_per_env_runner=10)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Tune Search Space\nDESCRIPTION: Sets up the parameter space for Ray Tune, using the dataset partition IDs as a grid search parameter. This allows Ray Tune to distribute training across all partitions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/start.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nparam_space = {\n    \"data_partition_id\": tune.grid_search(data_partition_ids),\n}\n```\n\n----------------------------------------\n\nTITLE: Metrics Logging with MLFlow Callback in Ray Tune\nDESCRIPTION: Shows how to integrate MLFlow logging with Ray Tune using the MLflowLoggerCallback. The example includes a training function that reports metrics and configures the MLFlow callback for logging experiment results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune_get_data_in_and_out.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray.tune\nfrom ray.tune.logger.mlflow import MLflowLoggerCallback\n\n\ndef training_function(config, data):\n    model = {\n        \"hyperparameter_a\": config[\"hyperparameter_a\"],\n        \"hyperparameter_b\": config[\"hyperparameter_b\"],\n    }\n    epochs = config[\"epochs\"]\n\n    # Simulate training & evaluation - we obtain back a \"metric\" and a \"trained_model\".\n    for epoch in range(epochs):\n        # Simulate doing something expensive.\n        time.sleep(1)\n        metric = (0.1 + model[\"hyperparameter_a\"] * epoch / 100) ** (\n            -1\n        ) + model[\"hyperparameter_b\"] * 0.1 * data[\"A\"].sum()\n        trained_model = {\"state\": model, \"epoch\": epoch}\n        tune.report(metrics={\"metric\": metric})\n\n\ntuner = tune.Tuner(\n    tune.with_parameters(training_function, data=data),\n    param_space={\n        \"hyperparameter_a\": tune.uniform(0, 20),\n        \"hyperparameter_b\": tune.uniform(-100, 100),\n        \"epochs\": 10,\n    },\n    tune_config=tune.TuneConfig(num_samples=4, metric=\"metric\", mode=\"max\"),\n    run_config=tune.RunConfig(\n        callbacks=[MLflowLoggerCallback(experiment_name=\"example\")]\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies - Python\nDESCRIPTION: Command to install Ray Serve and transformers packages required for the tutorial\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/batch.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install \"ray[serve] transformers\"\n```\n\n----------------------------------------\n\nTITLE: Preserving Original Working Directory in Ray Train\nDESCRIPTION: Demonstrates how to configure Ray Train to preserve the original working directory of training workers rather than using the default Ray session directory. This uses environment variables and Ray runtime configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/persistent-storage.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport ray\nimport ray.train\nfrom ray.train.torch import TorchTrainer\n\nos.environ[\"RAY_CHDIR_TO_TRIAL_DIR\"] = \"0\"\n\n# Write some file in the current working directory\nwith open(\"./data.txt\", \"w\") as f:\n    f.write(\"some data\")\n\n# Set the working directory in the Ray runtime environment\nray.init(runtime_env={\"working_dir\": \".\"})\n\ndef train_fn_per_worker(config):\n    # Check that each worker can access the working directory\n    # NOTE: The working directory is copied to each worker and is read only.\n    assert os.path.exists(\"./data.txt\"), os.getcwd()\n\ntrainer = TorchTrainer(\n    train_fn_per_worker,\n    scaling_config=ray.train.ScalingConfig(num_workers=2),\n    run_config=ray.train.RunConfig(\n        # storage_path=...,\n    ),\n)\ntrainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Output of a Distributed Ray Task with Log Deduplication\nDESCRIPTION: Displays the deduplicated output from a Ray task. The first message is printed in full, with subsequent duplicate messages summarized to indicate their frequency using a repeat count across the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n2023-03-27 15:08:34,195\tINFO worker.py:1603 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265\n(task pid=534172) Hello there, I am a task 0.20583517821231412\n(task pid=534174) Hello there, I am a task 0.17536720316370757 [repeated 99x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication)\n```\n\n----------------------------------------\n\nTITLE: Job Submission with Dependencies\nDESCRIPTION: Example showing how to specify dependencies using runtime environment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/sdk.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\njob_id = client.submit_job(\n    # Entrypoint shell command to execute\n    entrypoint=\"python script.py\",\n    # Runtime environment for the job, specifying a working directory and pip package\n    runtime_env={\n        \"working_dir\": \"./\",\n        \"pip\": [\"requests==2.26.0\"]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Non-Subclassable Exceptions in Ray\nDESCRIPTION: Shows how to handle exceptions that cannot be subclassed by accessing the exception cause field.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/tasks.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example handling non-subclassable exceptions\n```\n\n----------------------------------------\n\nTITLE: SAC Configuration\nDESCRIPTION: This section refers to the SACConfig class in RLlib, highlighting its training-related members. It provides a way to configure the SAC algorithm's training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-algorithms.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ray.rllib.algorithms.sac.sac.SACConfig\n   :members: training\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Scheduling Strategies in Python\nDESCRIPTION: This snippet shows the import paths for Ray's scheduling strategy classes. It includes PlacementGroupSchedulingStrategy and NodeAffinitySchedulingStrategy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/scheduling.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nray.util.scheduling_strategies.PlacementGroupSchedulingStrategy\nray.util.scheduling_strategies.NodeAffinitySchedulingStrategy\n```\n\n----------------------------------------\n\nTITLE: Continuing to Log Values in MetricsLogger - Python\nDESCRIPTION: This snippet illustrates how to continue logging new values under an existing key without needing to repeat the reduction type or window arguments. The example shows how logged values are stored and how the window parameter affects the averaging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlogger.log_value(\"loss\", 0.02)  # don't have to repeat `reduce` or `window` args,\n                                    # because the key already exists.\nlogger.log_value(\"loss\", 0.03)\nlogger.log_value(\"loss\", 0.04)\nlogger.log_value(\"loss\", 0.05)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Displaying a Sample Image for Object Detection\nDESCRIPTION: Downloads a sample image from a URL and displays it using PIL (Python Imaging Library).\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom PIL import Image\n\nurl = \"https://s3-us-west-2.amazonaws.com/air-example-data/AnimalDetection/JPEGImages/2007_000063.jpg\"\nimg = Image.open(requests.get(url, stream=True).raw)\ndisplay(img)\n```\n\n----------------------------------------\n\nTITLE: Disabling Ray Dashboard in Python\nDESCRIPTION: This Python code snippet demonstrates how to disable the Ray Dashboard when initializing Ray programmatically.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/configure-manage-dashboard.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init(include_dashboard=False)\n```\n\n----------------------------------------\n\nTITLE: Inspecting RayService Custom Resource Status\nDESCRIPTION: Command to retrieve detailed information about a RayService resource, including its status and events. This provides insight into the current state and any errors that occurred during deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe rayservice $RAYSERVICE_NAME -n $YOUR_NAMESPACE\n```\n\n----------------------------------------\n\nTITLE: Limiting Dataset Size for Testing Vicuna Fine-tuning\nDESCRIPTION: This snippet limits the size of the processed dataset to accelerate release tests. It ensures each worker has 16 batches of data to work with during the fine-tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# To accelerate release tests\nprocessed_ds = processed_ds.limit(16 * 8 * 16)  # each worker has 16 batches\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Samples in Tune Grid Search with Python\nDESCRIPTION: Demonstrates how to take multiple random samples in a grid search by adding num_samples to the experiment configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\n\ntune.run(\n    ...,\n    config={\n        \"param1\": tune.grid_search([1, 2, 3]),\n        \"param2\": tune.grid_search([4, 5, 6])\n    },\n    num_samples=10  # Will run 3x3x10 = 90 trials\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Outlines Package with Hash Verification in Bash\nDESCRIPTION: Defines the Outlines package version 0.1.11 with SHA256 hash verification for secure package installation. Required for the vllm package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\noutlines==0.1.11 \\\n    --hash=sha256:0997bd9da1cc050e430bd08995dc7d4bd855918bafa4531e49d3f37110a23aba \\\n    --hash=sha256:f5a5f2242ed9802d3aab7a92789bf4008d734c576be9258cc0a297f690124727\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Generation Pipeline with Hugging Face Transformers in Python\nDESCRIPTION: Sets up a text generation pipeline using the loaded language model. The pipeline handles tokenization and generation tasks, configuring the tokenizer with left-side padding for optimal performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\n\ngenerator = pipeline(\n    \"text-generation\",\n    model=model,\n    device_map=device_map,\n    tokenizer=AutoTokenizer.from_pretrained(\n        MODEL_NAME, padding_side=\"left\", use_fast=False\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding to Access Ray Dashboard\nDESCRIPTION: Command to establish port forwarding for accessing the Ray Dashboard UI. The dashboard provides visual monitoring of Ray Serve applications and their status.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward $RAY_POD -n $YOUR_NAMESPACE 8265:8265\n# Check $YOUR_IP:8265 in your browser\n```\n\n----------------------------------------\n\nTITLE: Computing Fibonacci Numbers Using Prefect Task Looping\nDESCRIPTION: This code demonstrates Prefect's task looping functionality to calculate Fibonacci numbers up to a maximum value M. It uses an external API to calculate each Fibonacci number, maintains state between iterations via the task_loop_result context, and uses the LOOP signal to continue iteration until the threshold is reached.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/workflow/examples/comparisons/prefect/compute_fib_prefect.py.txt#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom datetime import timedelta\n\nimport prefect\nfrom prefect import task\nfrom prefect import Flow, Parameter\nfrom prefect.engine.signals import LOOP\n\n\n@task(max_retries=5, retry_delay=timedelta(seconds=2))\ndef compute_large_fibonacci(M):\n    # we extract the accumulated task loop result from context\n    loop_payload = prefect.context.get(\"task_loop_result\", {})\n\n    n = loop_payload.get(\"n\", 1)\n    fib = loop_payload.get(\"fib\", 1)\n\n    next_fib = requests.post(\n        \"https://nemo.api.stdlib.com/fibonacci@0.0.1/\", data={\"nth\": n}\n    ).json()\n\n    if next_fib > M:\n        return fib  # return statements end the loop\n\n    raise LOOP(message=f\"Fib {n}={next_fib}\", result=dict(n=n + 1, fib=next_fib))\n\n\nif __name__ == \"__main__\":\n    with Flow(\"fibonacci\") as flow:\n        M = Parameter(\"M\")\n        fib_num = compute_large_fibonacci(M)\n\n    flow_state = flow.run(M=100)\n    print(flow_state.result[fib_num].result) # 89\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP-enabled Strategy for Ray Serve\nDESCRIPTION: Modified version of Strategy that supports HTTP requests through Ray Serve, accepting JSON input and returning String output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/java.md#2025-04-12_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic class HttpStrategyOnRayServe {\n    public String call(String requestJson) {\n        JsonObject jsonObject = JsonParser.parseString(requestJson).getAsJsonObject();\n        long time = jsonObject.get(\"time\").getAsLong();\n        String bank = jsonObject.get(\"bank\").getAsString();\n        String indicator = jsonObject.get(\"indicator\").getAsString();\n        double result = Math.random();\n        return String.valueOf(result);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Saving Text Data with Ray\nDESCRIPTION: Example of saving text data to Parquet format using Ray Data's write operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-text.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\n\nds.write_parquet(\"local:///tmp/results\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Multi-Host v4 TPU Node Pool\nDESCRIPTION: Adds a multi-host TPU node pool with a v4 TPU topology (2x2x2) to the GKE cluster. Configures 2 nodes with autoscaling enabled and ct4p-hightpu-4t machine type.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-tpu-cluster.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container node-pools create v4-8 \\\n  --zone $ZONE \\\n  --cluster $CLUSTER_NAME \\\n  --num-nodes 2 \\\n  --min-nodes 0 \\\n  --max-nodes 10 \\\n  --enable-autoscaling \\\n  --machine-type ct4p-hightpu-4t \\\n  --tpu-topology 2x2x2\n```\n\n----------------------------------------\n\nTITLE: Creating an RLModule from an Algorithm Checkpoint in Python\nDESCRIPTION: To recreate a new RLModule from an existing Algorithm checkpoint, users can specify the path to the RLModule's checkpoint, allowing independent evaluation or deployment of the RLModule.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/checkpoints.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nimport torch\n\nfrom ray.rllib.core.rl_module.rl_module import RLModule\n\nrl_module_checkpoint_dir = Path(checkpoint_dir) / \"learner_group\" / \"learner\" / \"rl_module\" / \"default_policy\"\nrl_module = RLModule.from_checkpoint(rl_module_checkpoint_dir)\nresults = rl_module.forward_inference({\"obs\": torch.tensor([0.5, 0.25, -0.3]).unsqueeze(0).float()})\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Accessing a Java Deployment\nDESCRIPTION: This snippet shows how to access a deployed instance by its name using the Java API. It retrieves the deployment instance using `Serve.getDeployment()`.  The deployment must already be deployed and running for this to work.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/managing-java-deployments.md#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n    ActorHandle<Counter> counter = Serve.getDeployment(\"counter\").getHandle();\n    System.out.println(\"Current counter value: \" + Ray.get(counter.task(Counter::getValue).remote()));\n\n```\n\n----------------------------------------\n\nTITLE: Starting Ray with Metrics Export\nDESCRIPTION: Command to start a Ray head node with metrics export enabled on port 8080.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/metrics.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nray start --head --metrics-export-port=8080\n```\n\n----------------------------------------\n\nTITLE: Implementing Job Driver Fault Tolerance in Ray Train (Python)\nDESCRIPTION: This snippet demonstrates best practices for implementing job driver fault tolerance in Ray Train. It includes handling command-line arguments for storage path and run name, which are crucial for resuming training after interruptions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/fault-tolerance.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nfrom ray.train import RunConfig, FailureConfig, ScalingConfig\nfrom ray.train.torch import TorchTrainer\n\ndef train_func():\n    # Your training logic here\n    pass\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--storage_path\", type=str, required=True)\n    parser.add_argument(\"--run_name\", type=str, required=True)\n    args = parser.parse_args()\n\n    trainer = TorchTrainer(\n        train_func,\n        scaling_config=ScalingConfig(num_workers=4),\n        run_config=RunConfig(\n            failure_config=FailureConfig(max_failures=3),\n            name=args.run_name,\n            storage_path=args.storage_path\n        )\n    )\n\n    results = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Client-side Multi-LoRA Model Interaction\nDESCRIPTION: Python code for client-side interaction with a multi-LoRA model deployed using Ray Serve LLM. Demonstrates how to make a request to a specific LoRA checkpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Initialize client\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n\n# Make a request to the desired lora checkpoint\nresponse = client.chat.completions.create(\n    model=\"qwen-0.5b:lora_model_1_ckpt\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    stream=True,\n)\n\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Viewing Ray Cluster Logs\nDESCRIPTION: Kubernetes commands to view logs from the head pod, including specific logs for the autoscaler component.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/kuberay/README.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs <head pod name>\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs <head pod name> -c autoscaler\n```\n\n----------------------------------------\n\nTITLE: Dataset Tokenization Function for Llama-2 Fine-tuning\nDESCRIPTION: Converts preprocessed dataset to tokenized format, handling sequence length, adding tokens, and preparing data for model training\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_dataset_to_tokenizer(raw_datasets, tokenizer):\n    max_seq_length = 512\n    tokenizer.pad_token_id = 0\n    tokenizer.eos_token_id = 1\n    tokenizer.bos_token_id = 2\n\n    def tokenize(prompt, add_eos_token=True):\n        results = tokenizer(\n            prompt,\n            truncation=True,\n            max_length=max_seq_length,\n            padding=False,\n            return_tensors=None,\n        )\n        for i in range(len(results[\"input_ids\"])):\n            if (\n                results[\"input_ids\"][i][-1] != tokenizer.eos_token_id\n                and len(results[\"input_ids\"][i]) < max_seq_length\n                and add_eos_token\n            ):\n                results[\"input_ids\"][i].append(tokenizer.eos_token_id)\n                results[\"attention_mask\"][i].append(1)\n\n        results[\"labels\"] = copy.deepcopy(results[\"input_ids\"])\n        results[\"input_id_len\"] = [len(result) for result in results[\"input_ids\"]]\n        return results\n\n    def preprocess_function(examples):\n        keys = list(examples.data.keys())\n        if len(keys) != 2:\n            raise ValueError(\"Unsupported dataset format\")\n\n        st = [s + t for s, t in zip(examples[keys[0]], examples[keys[1]])]\n\n        examples_tokenized = tokenize(st)\n        input_ids = examples_tokenized[\"input_ids\"]\n        labels = examples_tokenized[\"labels\"]\n        return {\n            \"input_ids\": input_ids,\n            \"labels\": labels,\n            \"attention_mask\": examples_tokenized[\"attention_mask\"],\n        }\n\n    tokenized_datasets = raw_datasets.map(\n        preprocess_function,\n        batched=True,\n        load_from_cache_file=True,\n    )\n\n    def concatenate_data(dataset, max_seq_length):\n        concatenated_dataset = {}\n        for column in dataset.features:\n            concatenated_data = [item for sample in dataset[column] for item in sample]\n            reshaped_data = [\n                concatenated_data[i * max_seq_length : (i + 1) * max_seq_length]\n                for i in range(len(concatenated_data) // max_seq_length)\n            ]\n            concatenated_dataset[column] = reshaped_data\n        return datasets.Dataset.from_dict(concatenated_dataset)\n\n    tokenized_datasets_ = tokenized_datasets[\"train\"].remove_columns([\"prompt_sources\", \"prompt_targets\"])\n    tokenized_datasets[\"train\"] = concatenate_data(tokenized_datasets_, max_seq_length)\n\n    return tokenized_datasets\n```\n\n----------------------------------------\n\nTITLE: Forward Compatibility in Callback APIs\nDESCRIPTION: This Python function demonstrates the use of `**kwargs` to allow for future expansion of a callback API without breaking existing implementations. It provides flexibility for additional arguments in the future.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef tune_user_callback(model, score, **future_kwargs):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Internal Actor Termination in Java\nDESCRIPTION: Demonstrates how to terminate a Ray actor from within using Ray.exitActor() in Java. This is currently the only way to gracefully terminate Java actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/terminating-actors.rst#2025-04-12_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nRay.exitActor();\n```\n\n----------------------------------------\n\nTITLE: Anti-pattern: Fetching Too Many Objects with ray.get in Python\nDESCRIPTION: This code snippet demonstrates the anti-pattern of fetching too many objects at once using ray.get(), which can lead to memory issues in Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/ray-get-too-many-objects.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nresults = ray.get([long_running_task.remote() for _ in range(10000)])\nfor result in results:\n    process_result(result)\n```\n\n----------------------------------------\n\nTITLE: Updating a Java Deployment\nDESCRIPTION: This snippet demonstrates how to update a deployment's configuration and redeploy it using the Java API. It modifies the initial value of the counter deployment and redeploys the deployment with the updated configuration.  The specified deployment name must already exist.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/managing-java-deployments.md#2025-04-12_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n    Serve.deployment(\"counter\").setInitArgs(new Object[] {2}).deploy(true);\n    System.out.println(\"\\\"counter\\\" deployment has been updated.\");\n    counter = Serve.getDeployment(\"counter\").getHandle();\n    System.out.println(\"Current counter value: \" + Ray.get(counter.task(Counter::getValue).remote()));\n\n```\n\n----------------------------------------\n\nTITLE: GPU Node Tolerations Configuration in YAML\nDESCRIPTION: YAML configuration for worker pod tolerations to ensure proper GPU node scheduling in the RayService deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/stable-diffusion-rayservice.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntolerations:\n    - key: \"ray.io/node-type\"\n      operator: \"Equal\"\n      value: \"worker\"\n      effect: \"NoSchedule\"\n```\n\n----------------------------------------\n\nTITLE: Saving Prediction Results\nDESCRIPTION: Saving the model predictions to local storage in Parquet format after removing the original images.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/01_batch_inference/start.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\n\ntemp_dir = tempfile.mkdtemp()\n\npredictions.drop_columns([\"original_image\"]).write_parquet(f\"local://{temp_dir}\")\nprint(f\"Predictions saved to `{temp_dir}`!\")\n```\n\n----------------------------------------\n\nTITLE: Creating Ray Actors in Python\nDESCRIPTION: Demonstrates how to create Ray actors to maintain state and execute methods in parallel across distributed workers\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init()\n\n@ray.remote\nclass Counter(object):\n    def __init__(self):\n        self.n = 0\n\n    def increment(self):\n        self.n += 1\n\n    def read(self):\n        return self.n\n\ncounters = [Counter.remote() for i in range(4)]\n[c.increment.remote() for c in counters]\nfutures = [c.read.remote() for c in counters]\nprint(ray.get(futures)) # [1, 1, 1, 1]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating an Anti-pattern and Solutions in Ray\nDESCRIPTION: This Python code snippet demonstrates an anti-pattern where new processes are forked in Ray application code, leading to potential issues with shared sockets and corrupted messages. It suggests using the 'spawn' method or Ray tasks and actors instead. The example requires an understanding of Ray's process lifecycle management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/fork-new-processes.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/anti_pattern_fork_new_processes.py\n    :language: python\n```\n\n----------------------------------------\n\nTITLE: Deploying a Ray Cluster with Authentication Enabled\nDESCRIPTION: Command to deploy a RayCluster with kube-rbac-proxy for authentication and authorization from a pre-configured YAML file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/refs/heads/master/ray-operator/config/samples/ray-cluster.auth.yaml\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Anti-Pattern: Redefining Remote Function in Loop (Python)\nDESCRIPTION: This code snippet shows the anti-pattern of redefining a remote function inside a loop, which leads to repeated pickling, uploading, downloading, and unpickling, causing performance issues.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/redefine-task-actor-loop.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# __anti_pattern_start__\nfor i in range(10):\n    @ray.remote\n    def f():\n        return i\n    \n    ray.get([f.remote() for _ in range(1000)])\n# __anti_pattern_end__\n```\n\n----------------------------------------\n\nTITLE: Installing Snowflake Connector for Python\nDESCRIPTION: This command shows how to install the Snowflake Connector for Python using pip, which is necessary for connecting to Snowflake data warehouses from Python.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_27\n\nLANGUAGE: console\nCODE:\n```\npip install snowflake-connector-python\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Releasing RLlib Resources in Python\nDESCRIPTION: This code snippet evaluates a trained algorithm and releases resources used during the training (e.g., remote actors such as EnvRunners and Learners) in RLlib. This is essential for finalizing the process and managing system resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# ... and evaluate it.\npprint(algo.evaluate())\n\n# Release the algo's resources (remote actors, like EnvRunners and Learners).\nalgo.stop()\n```\n\n----------------------------------------\n\nTITLE: Using asyncio.shield for Request Handling - Python\nDESCRIPTION: This snippet demonstrates the use of asyncio.shield to prevent asynchronous calls from being interrupted by cancellation. It explains how to manage the cancellation error within the SnoringSleeper deployment's __call__() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"{literalinclude} doc_code/http_guide/disconnects.py\\n:start-after: __start_shielded_disconnect__\\n:end-before: __end_shielded_disconnect__\\n:language: python\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Runtime Environment with Public Remote URI in Python\nDESCRIPTION: Example of setting up a runtime environment using a public GitHub repository URL as the working directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/runtime_env_auth.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nruntime_env = {\"working_dir\": (\n        \"https://github.com/\"\n        \"username/repo/archive/refs/heads/master.zip\"\n    )\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PodMonitor for Ray Worker Nodes in YAML\nDESCRIPTION: This YAML configuration sets up a PodMonitor to collect metrics from Ray worker pods. It selects pods with the label 'ray.io/node-type: worker' in the default namespace and defines the metrics endpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: ray-workers-monitor\n  namespace: prometheus-system\n  labels:\n    release: prometheus\nspec:\n  jobLabel: ray-workers\n  namespaceSelector:\n    matchNames:\n      - default\n  selector:\n    matchLabels:\n      ray.io/node-type: worker\n  podMetricsEndpoints:\n  - port: metrics\n    relabelings:\n    - sourceLabels: [__meta_kubernetes_pod_label_ray_io_cluster]\n      targetLabel: ray_io_cluster\n```\n\n----------------------------------------\n\nTITLE: Filter Objects by PID and Reference Type\nDESCRIPTION: Command to list objects filtered by process ID and reference type.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nray list objects -f pid=<PID> -f reference_type=LOCAL_REFERENCE\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import list_objects\nlist_objects(filters=[(\"pid\", \"=\", 1234), (\"reference_type\", \"=\", \"LOCAL_REFERENCE\")])\n```\n\n----------------------------------------\n\nTITLE: Converting Hugging Face Datasets to Ray Data\nDESCRIPTION: This code converts the Hugging Face datasets to Ray Data format for efficient processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport ray.data\n\nray_datasets = {\n    \"train\": ray.data.from_huggingface(datasets[\"train\"]),\n    \"validation\": ray.data.from_huggingface(datasets[\"validation\"]),\n    \"test\": ray.data.from_huggingface(datasets[\"test\"]),\n}\nray_datasets\n```\n\n----------------------------------------\n\nTITLE: Using ray status CLI command to monitor cluster\nDESCRIPTION: The 'ray status' command displays node status and resource usage information, showing healthy nodes, pending nodes, resource usage, and demands. This helps monitor the overall state of a Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ ray status\n======== Autoscaler status: 2021-10-12 13:10:21.035674 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n 1 ray.head.default\n 2 ray.worker.cpu\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/10.0 CPU\n 0.00/70.437 GiB memory\n 0.00/10.306 GiB object_store_memory\n\nDemands:\n (no resource demands)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Ray Tune with Ax Integration\nDESCRIPTION: Imports the necessary Python libraries including numpy for mathematical operations, Ray and Tune for distributed optimization, and AxSearch for integration with Ax platform.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport time\n\nimport ray\nfrom ray import tune\nfrom ray.tune.search.ax import AxSearch\n```\n\n----------------------------------------\n\nTITLE: Defining Supported Accelerator Types in Ray\nDESCRIPTION: Code that defines the available accelerator types supported by the Ray framework for distributed computing. The code specifies the supported accelerator types as an enumeration or constant.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/accelerator-types.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# This would be the content of accelerators.py, but it's not directly shown in the input. The literalinclude directive suggests it exists but the actual code is not provided in the snippet.\n```\n\n----------------------------------------\n\nTITLE: RayJob Metadata Configuration for Kueue Integration\nDESCRIPTION: YAML snippet showing how to modify RayJob metadata to utilize Kueue for scheduling, specifying the queue name and priority class.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n  generateName: rayjob-sample-\n  labels:\n    kueue.x-k8s.io/queue-name: user-queue\n    kueue.x-k8s.io/priority-class: dev-priority\n```\n\n----------------------------------------\n\nTITLE: Deploying Ray Serve Application with Configuration\nDESCRIPTION: Initial deployment of a text summarization and translation application using Ray Serve CLI commands\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/inplace-updates.md#2025-04-12_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ ray start --head\n$ serve deploy serve_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Check Job Deployment Status\nDESCRIPTION: Retrieves the deployment status of the RayJob using jsonpath extraction.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nkubectl get rayjobs.ray.io rayjob-sample-shutdown -o jsonpath='{.status.jobDeploymentStatus}'\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Batchbot Implementation in Python\nDESCRIPTION: This snippet shows the necessary imports for implementing the Batchbot, including Ray Serve, FastAPI, and Hugging Face libraries.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom queue import Queue\nfrom threading import Thread\n\nfrom fastapi import FastAPI\nfrom starlette.responses import StreamingResponse\nimport serve\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n```\n\n----------------------------------------\n\nTITLE: Automatic Throughput Measurements with MetricsLogger in Python\nDESCRIPTION: Demonstrates automatic throughput computation by logging with 'reduce=\"sum\"' and setting 'with_throughput=True'. This allows tracking the rate of events per second.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom ray.rllib.utils.metrics.metrics_logger import MetricsLogger\n\nlogger = MetricsLogger()\n\nfor _ in range(3):\n    logger.log_value(\"lifetime_count\", 5, reduce=\"sum\", with_throughput=True)\n\n    # RLlib triggers a new throughput computation at each `reduce()` call\n    logger.reduce()\n    time.sleep(1.0)\n\n    # Expect the first call to return NaN because we don't have a proper start time for the time delta.\n    # From the second call on, expect a value of roughly 5/sec.\n    print(logger.peek(\"lifetime_count\", throughput=True))\n```\n\n----------------------------------------\n\nTITLE: Killing Proxy Actor using Python\nDESCRIPTION: This snippet demonstrates how to simulate a proxy failure by getting a handle to a specific ProxyActor and killing it using Ray's Python API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> import ray\n>>> proxy_handle = ray.get_actor(\"SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-91f9a685e662313a0075efcb7fd894249a5bdae7ee88837bea7985a0\", namespace=\"serve\")\n>>> ray.kill(proxy_handle, no_restart=False)\n>>> exit()\n```\n\n----------------------------------------\n\nTITLE: Specifying LLVM Package Dependency\nDESCRIPTION: Defines the required version and multiple hash values for the llvmlite package, which is a dependency for numba.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nllvmlite==0.43.0 \\\n    --hash=sha256:14f0e4bf2fd2d9a75a3534111e8ebeb08eda2f33e9bdd6dfa13282afacdde0ed \\\n    --hash=sha256:18e9953c748b105668487b7c81a3e97b046d8abf95c4ddc0cd3c94f4e4651ae8 \\\n    --hash=sha256:35d80d61d0cda2d767f72de99450766250560399edc309da16937b93d3b676e7 \\\n    --hash=sha256:3e8d0618cb9bfe40ac38a9633f2493d4d4e9fcc2f438d39a4e854f39cc0f5f98 \\\n    --hash=sha256:47e147cdda9037f94b399bf03bfd8a6b6b1f2f90be94a454e3386f006455a9b4 \\\n    --hash=sha256:6912a87782acdff6eb8bf01675ed01d60ca1f2551f8176a300a886f09e836a6a \\\n    --hash=sha256:6d4fd101f571a31acb1559ae1af30f30b1dc4b3186669f92ad780e17c81e91bc \\\n    --hash=sha256:74937acd22dc11b33946b67dca7680e6d103d6e90eeaaaf932603bec6fe7b03a \\\n    --hash=sha256:7a2872ee80dcf6b5dbdc838763d26554c2a18aa833d31a2635bff16aafefb9c9 \\\n    --hash=sha256:7d434ec7e2ce3cc8f452d1cd9a28591745de022f931d67be688a737320dfcead \\\n    --hash=sha256:977525a1e5f4059316b183fb4fd34fa858c9eade31f165427a3977c95e3ee749 \\\n    --hash=sha256:9cd2a7376f7b3367019b664c21f0c61766219faa3b03731113ead75107f3b66c \\\n    --hash=sha256:a289af9a1687c6cf463478f0fa8e8aa3b6fb813317b0d70bf1ed0759eab6f761 \\\n    --hash=sha256:ae2b5b5c3ef67354824fb75517c8db5fbe93bc02cd9671f3c62271626bc041d5 \\\n    --hash=sha256:bc9efc739cc6ed760f795806f67889923f7274276f0eb45092a1473e40d9b867 \\\n    --hash=sha256:c1da416ab53e4f7f3bc8d4eeba36d801cc1894b9fbfbf2022b29b6bad34a7df2 \\\n    --hash=sha256:d5bd550001d26450bd90777736c69d68c487d17bf371438f975229b2b8241a91 \\\n    --hash=sha256:df6509e1507ca0760787a199d19439cc887bfd82226f5af746d6977bd9f66844 \\\n    --hash=sha256:e0a9a1a39d4bf3517f2af9d23d479b4175ead205c592ceeb8b89af48a327ea57 \\\n    --hash=sha256:eccce86bba940bae0d8d48ed925f21dbb813519169246e2ab292b5092aba121f \\\n    --hash=sha256:f99b600aa7f65235a5a05d0b9a9f31150c390f31261f2a0ba678e26823ec38f7\n    # via numba\n```\n\n----------------------------------------\n\nTITLE: Configuring FAQ Document Structure with reStructuredText\nDESCRIPTION: This snippet defines a reStructuredText document structure for Ray's FAQ pages. It marks the document as orphaned (not in the main TOC), creates a toctree for sub-pages, and includes an external help reference.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-references/faq.rst#2025-04-12_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n:orphan:\n\nFAQ\n==============\n\n.. toctree::\n    :maxdepth: 1\n    :caption: Frequently Asked Questions\n\n    ./../tune/faq.rst\n\n\nFurther Questions or Issues?\n----------------------------\n\n.. include:: /_includes/_help.rst\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX to TensorRT Engine\nDESCRIPTION: Command to convert ONNX model to TensorRT engine format with FP16 precision and configurable shape parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/triton-server-integration.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec --onnx=vae.onnx --saveEngine=vae.plan --minShapes=latent_sample:1x4x64x64 --optShapes=latent_sample:4x4x64x64 --maxShapes=latent_sample:8x4x64x64 --fp16\n```\n\n----------------------------------------\n\nTITLE: Starting a Ray Cluster Head Node in Bash\nDESCRIPTION: This bash command starts a Ray cluster locally by initializing the head node. It's a prerequisite for using Ray Client.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/ray-client.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nray start --head\n```\n\n----------------------------------------\n\nTITLE: List Running Tasks\nDESCRIPTION: Command to list all running tasks in the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nray list tasks -f state=RUNNING\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import list_tasks\nlist_tasks(filters=[(\"state\", \"=\", \"RUNNING\")])\n```\n\n----------------------------------------\n\nTITLE: Generating TLS Certificates for Ray Authentication\nDESCRIPTION: Commands for generating self-signed TLS certificates for secure Ray cluster authentication.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/configure.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nopenssl req -x509 \\\n              -sha256 -days 3650 \\\n              -nodes \\\n              -newkey rsa:2048 \\\n              -subj \"/CN=*.ray.io/C=US/L=San Francisco\" \\\n              -keyout ca.key -out ca.crt\n\ncat ca.key | base64\ncat ca.crt | base64\n```\n\n----------------------------------------\n\nTITLE: Creating a Placement Group in C++\nDESCRIPTION: This snippet demonstrates how to create a placement group in C++ using Ray. It initializes Ray, constructs a list of bundles, and creates a placement group with specified options.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n// Initialize Ray.\nray::Init();\n\n// Construct a list of bundles.\nstd::vector<std::unordered_map<std::string, double>> bundles{{\"CPU\", 1.0}};\n\n// Make a creation option with bundles and strategy.\nray::internal::PlacementGroupCreationOptions options{\n    false, \"my_pg\", bundles, ray::internal::PlacementStrategy::PACK};\n\nray::PlacementGroup pg = ray::CreatePlacementGroup(options);\n```\n\n----------------------------------------\n\nTITLE: Customizing Actor Log Prefixes with __repr__\nDESCRIPTION: Example of how to customize Actor log prefixes by implementing the __repr__ method, making it easier to distinguish between different Actor instances in logs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\n\n@ray.remote\nclass MyActor:\n    def __init__(self, index):\n        self.index = index\n\n    def __repr__(self):\n        return f\"MyActor(index={self.index})\"\n\n    def say_hello(self):\n        print(\"hello there\")\n\nactors = [MyActor.remote(i) for i in range(1, 3)]\nray.get([actor.say_hello.remote() for actor in actors])\n```\n\n----------------------------------------\n\nTITLE: Git Remote Setup and Pull for Ray Development\nDESCRIPTION: Commands to set up remote repository and pull latest changes from Ray master branch\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add upstream https://github.com/ray-project/ray.git\ngit pull . upstream/master\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry for Ray Tracing\nDESCRIPTION: Commands to install the required OpenTelemetry packages including the API, SDK, and OTLP exporter. These packages are necessary prerequisites for enabling tracing functionality in Ray applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/ray-tracing.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install opentelemetry-api==1.1.0\npip install opentelemetry-sdk==1.1.0\npip install opentelemetry-exporter-otlp==1.1.0\n```\n\n----------------------------------------\n\nTITLE: Executing HPU-based Distributed Training\nDESCRIPTION: A simple call to the train_resnet function that initiates the distributed training with 2 worker nodes. Each worker will use one HPU accelerator as specified in the scaling configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrain_resnet(num_workers=2) \n\n```\n\n----------------------------------------\n\nTITLE: Configuring Non-Default Ports in KubeRay YAML\nDESCRIPTION: Example configuration for specifying custom ports for Ray head service components including GCS, dashboard, and client ports.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/config.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nports:\n- containerPort: 6380\n  name: gcs\n- containerPort: 8266\n  name: dashboard\n- containerPort: 10002\n  name: client\n```\n\nLANGUAGE: yaml\nCODE:\n```\nrayStartParams:\n  port: \"6380\"\n  dashboard-port: \"8266\"\n  ray-client-server-port: \"10002\"\n  ...\n```\n\n----------------------------------------\n\nTITLE: Example of Remote Function Outputs with Ray in Python\nDESCRIPTION: These code snippets represent the expected outputs when calling remote functions `g` and `h`. The first output features `ObjectRef` objects indicating async execution, while the second output illustrates concrete results from executed tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tasks/nested-tasks.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> ray.get(g.remote())\n    [ObjectRef(b1457ba0911ae84989aae86f89409e953dd9a80e),\n     ObjectRef(7c14a1d13a56d8dc01e800761a66f09201104275),\n     ObjectRef(99763728ffc1a2c0766a2000ebabded52514e9a6),\n     ObjectRef(9c2f372e1933b04b2936bb6f58161285829b9914)]\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> ray.get(h.remote())\n    [1, 1, 1, 1]\n```\n\n----------------------------------------\n\nTITLE: Logging Custom Artifacts with Function API in Ray Tune\nDESCRIPTION: Example of logging custom artifacts, models, or using third-party logging libraries within a Tune trainable using the Function API. Works with the trial's working directory to store artifacts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport logging_library  # ex: mlflow, wandb\nfrom ray import train\n\ndef trainable(config):\n    logging_library.init(\n        name=trial_id,\n        id=trial_id,\n        resume=trial_id,\n        reinit=True,\n        allow_val_change=True)\n    logging_library.set_log_path(os.getcwd())\n\n    for step in range(100):\n        logging_library.log_model(...)\n        logging_library.log(results, step=step)\n\n        # You can also just write to a file directly.\n        # The working directory is set to the trial directory, so\n        # you don't need to worry about multiple workers saving\n        # to the same location.\n        with open(f\"./artifact_{step}.txt\", \"w\") as f:\n            f.write(\"Artifact Data\")\n\n        tune.report(results)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Environment Data from SingleAgentEpisode in Python\nDESCRIPTION: This snippet describes how to obtain various pieces of data from the environment during an episode, including observations, information, actions, rewards, and model outputs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_episode.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n    Methods:\n    - get_observations(): retrieves the observations for each step in the episode.\n    - get_infos(): fetches additional info metrics of the episode.\n    - get_actions(): returns the actions taken by the agent.\n    - get_rewards(): gets the rewards received during the episode.\n    - get_extra_model_outputs(): retrieves additional outputs from the model.\n    - get_temporary_timestep_data(): obtains temporary data for the current timestep.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Import necessary Python libraries including numpy, os, ray, time and gymnasium\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_pong_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport os\nimport ray\nimport time\n\nimport gymnasium as gym\n```\n\n----------------------------------------\n\nTITLE: Installing Ray on Arch Linux using yay\nDESCRIPTION: Installs Ray on Arch Linux using the yay AUR helper. This simplifies the installation process by automatically resolving dependencies from the Arch User Repository.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n\"yay -S python-ray\"\n```\n\n----------------------------------------\n\nTITLE: Contributing Community Examples\nDESCRIPTION: This YAML snippet provides a structure for adding community examples to a documentation site, specifying details like title, skill level, and link. It is part of the contributions process for community libraries.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\n- title: Serve a Java App\n  skill_level: advanced\n  link: tutorials/java\n  contributor: community\n```\n\n----------------------------------------\n\nTITLE: Deleting Serve Applications with Ray Serve REST API\nDESCRIPTION: This snippet shows how to shut down Serve and all applications running on the Ray cluster using a DELETE request to the `/api/serve/applications/` endpoint. It includes the necessary HTTP request headers for the API call.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/api/index.md#2025-04-12_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nDELETE /api/serve/applications/ HTTP/1.1\nHost: http://localhost:8265/\nAccept: application/json\n```\n\n----------------------------------------\n\nTITLE: Custom Learner Connector Configuration\nDESCRIPTION: Shows how to define a custom learner connector using the training configuration approach, suitable for manipulating raw episodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef _make_learner_connector(input_observation_space, input_action_space):\n    # Create the learner connector.\n    return CustomLearnerConnector(\n        parameter_1=0.3,\n        parameter_2=100,\n    )\n\nconfig = (\n    AlgorithmConfig()\n    .training(\n        # Add the connector pipeline as the starting point for\n        # the learner connector pipeline.\n        learner_connector=_make_learner_connector,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Image Streaming on GKE Clusters\nDESCRIPTION: Command to create a GKE cluster with Image streaming enabled. This feature allows containers to start while images are still being downloaded, significantly reducing startup time for Ray workloads using Artifact Registry images.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/reduce-image-pull-latency.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngcloud container clusters create CLUSTER_NAME \\\n    --zone=COMPUTE_ZONE \\\n    --image-type=\"COS_CONTAINERD\" \\\n    --enable-image-streaming\n```\n\n----------------------------------------\n\nTITLE: Minimal Azure Configuration (Autoscaler)\nDESCRIPTION: This YAML configuration showcases a minimal Ray autoscaler setup for Azure, defining the essential parameters needed to launch and manage a Ray cluster on Azure. It includes node types and resource allocation, demonstrating a basic but functional configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/azure/example-minimal.yaml\n            :language: yaml\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating cross-language exception handling in Ray\nDESCRIPTION: Shows how to handle exceptions that occur across language boundaries in Ray, demonstrating the propagation of error information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\n\n# Get Java function reference\nraise_exception = ray.java_function(\"io.ray.demo.MyRayClass\", \"raiseExceptionFromPython\")\n\n# Call Java function and get the result\nobj_ref = raise_exception.remote()\n\ntry:\n    ray.get(obj_ref)  # <-- raise exception from here.\nexcept Exception as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Running Tuning and Accessing Results in Ray Tune\nDESCRIPTION: This code snippet shows how to run a tuning experiment using the Tuner object and access the results as a DataFrame.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune_get_data_in_and_out.md#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresults = tuner.fit()\nresults.get_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Running Ray Application for Debugging\nDESCRIPTION: Command to execute a Ray application for debugging purposes. This runs the job.py script which contains Ray tasks with embedded breakpoints.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/ray-distributed-debugger.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython job.py\n```\n\n----------------------------------------\n\nTITLE: Testing High Availability by Deleting the Ray Head Pod\nDESCRIPTION: Tests the high availability feature by deleting the Ray head Pod and observing that the service continues to handle requests, demonstrating the fault tolerance of the RayService setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-high-availability.md#2025-04-12_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# Step 7.1: Delete the Ray head Pod.\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl delete pod $HEAD_POD\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Progress Bar for Ray Data Iterable Dataset in Python\nDESCRIPTION: This snippet defines a custom progress bar class 'DollyV2ProgressBar' that extends TQDMProgressBar. It's designed to work with Ray Data Iterable Dataset, resetting the progress bar for each epoch based on the number of iterations.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass DollyV2ProgressBar(TQDMProgressBar):\n    def __init__(self, num_iters_per_epoch, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_iters_per_epoch = num_iters_per_epoch\n    \n    def on_train_epoch_start(self, trainer, *_):\n        super().on_train_epoch_start(trainer, *_)\n        self.train_progress_bar.reset(self.num_iters_per_epoch)\n\nnum_iters_per_epoch = train_ds.count() // (num_workers * batch_size_per_worker)\nprog_bar = DollyV2ProgressBar(num_iters_per_epoch)\n```\n\n----------------------------------------\n\nTITLE: Compressing Directory for Remote URI Deployment\nDESCRIPTION: Bash command to compress a directory into a zip file, ensuring a single top-level directory is created for use as a working directory\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\ncd /some_path\nzip -r zip_file_name.zip example_dir\n```\n\n----------------------------------------\n\nTITLE: Zipp Package Version Pin\nDESCRIPTION: Version specification for zipp package (3.21.0) with corresponding SHA256 hashes for verification. Includes metadata about requirement source from compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_41\n\nLANGUAGE: txt\nCODE:\n```\nzipp==3.21.0 \\\n    --hash=sha256:2c9958f6430a2040341a52eb608ed6dd93ef4392e02ffe219417c1b28b5dd1f4 \\\n    --hash=sha256:ac1bbe05fd2991f160ebce24ffbac5f6d11d83dc90891255885223d42b3cd931\n```\n\n----------------------------------------\n\nTITLE: Accessing Ray Cluster Head Pod\nDESCRIPTION: Commands to identify and access the head pod of the Ray cluster. This allows developers to execute commands directly within the Ray cluster environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- bash\n```\n\n----------------------------------------\n\nTITLE: Installing Ray nightly builds\nDESCRIPTION: Commands for installing Ray daily (nightly) releases. Shows how to cleanly remove previous installations and install the nightly wheel with either default components or minimal dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Clean removal of previous install\npip uninstall -y ray\n# Install Ray with support for the dashboard + cluster launcher\npip install -U \"ray[default] @ LINK_TO_WHEEL.whl\"\n\n# Install Ray with minimal dependencies\n# pip install -U LINK_TO_WHEEL.whl\n```\n\n----------------------------------------\n\nTITLE: Monitoring Ray Cluster Status\nDESCRIPTION: Commands for accessing the Ray dashboard and monitoring cluster status.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-cli.rst#2025-04-12_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n$ ray dashboard cluster.yaml\n\n$ ray status\n\n$ watch -n 1 ray status\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray - Python\nDESCRIPTION: This snippet initializes Ray without logging configuration, which is necessary to run distributed tasks under Rays architecture.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nray.init(configure_logging=False)\n```\n\n----------------------------------------\n\nTITLE: Sending Requests to Test RayService\nDESCRIPTION: Demonstrates how to send test requests to the RayService by executing a Python script from a separate Ray Pod. The script sends consecutive requests with at most one in-flight request at a time.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-high-availability.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# Log into the separate Ray Pod.\nkubectl exec -it ray-pod -- bash\n\n# Send requests to the RayService.\npython3 samples/query.py\n\n# This script sends the same request to the RayService consecutively, ensuring at most one in-flight request at a time.\n# The request is equivalent to `curl -X POST -H 'Content-Type: application/json' localhost:8000/fruit/ -d '[\"PEAR\", 12]'`.\n\n# [Example output]\n# req_index : 2197, num_fail: 0\n# response: 12\n# req_index : 2198, num_fail: 0\n# response: 12\n# req_index : 2199, num_fail: 0\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Helm Chart\nDESCRIPTION: Basic steps for cloning the KubeRay repository and installing the Helm chart with modified RBAC configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/helm-chart-rbac.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Step 1: Clone the KubeRay repository\n# Step 2: Modify the helm-chart/kuberay-operator/values.yaml\n# Step 3: Install the KubeRay Helm chart (path: helm-chart/kuberay-operator)\nhelm install kuberay-operator .\n```\n\n----------------------------------------\n\nTITLE: Defining a Progress Actor in Ray\nDESCRIPTION: Creates a Ray actor class to track progress of sampling tasks. The actor has methods to report and retrieve progress.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/monte_carlo_pi.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass ProgressActor:\n    def __init__(self, total_num_samples):\n        self.total_num_samples = total_num_samples\n        self.num_samples_completed = 0\n\n    def report_progress(self, num_samples):\n        self.num_samples_completed += num_samples\n\n    def get_progress(self):\n        return self.num_samples_completed / self.total_num_samples\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Resources for Ray Actors in Joblib\nDESCRIPTION: This snippet shows how to configure Ray actors with GPU resources when using the Ray joblib backend. This is useful for GPU-accelerated estimators like those in cuML.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/joblib.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Allows to use GPU-enabled estimators, such as cuML\nwith joblib.parallel_backend('ray', ray_remote_args=dict(num_gpus=1)):\n    search.fit(digits.data, digits.target)\n```\n\n----------------------------------------\n\nTITLE: Verifying AWS Credentials\nDESCRIPTION: Command to verify AWS credentials configuration on worker nodes\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naws configure list\n```\n\n----------------------------------------\n\nTITLE: Executing Nested Remote Functions in Python with Ray\nDESCRIPTION: This snippet illustrates the behavior of nested remote functions in Ray. It shows how functions `g` and `h` can be called to produce specific output, demonstrating the nested task execution. It is crucial that the function `f` is declared before `g` and `h` to avoid incomplete definitions due to tasks being pickled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tasks/nested-tasks.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/nested-tasks.py\n    :language: python\n    :start-after: __nested_start__\n    :end-before: __nested_end__\n```\n\n----------------------------------------\n\nTITLE: Testing Ray Serve Object Detection API\nDESCRIPTION: Python script to test the deployed object detection service by sending an image URL and saving the annotated output with detected objects.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/object-detection.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\n# Sample image URL for testing\nimage_url = \"https://ultralytics.com/images/zidane.jpg\"\n\n# Send request to the object detection service\nresp = requests.get(f\"http://127.0.0.1:8000/detect?image_url={image_url}\")\n\n# Save the annotated image with detected objects\nwith open(\"output.jpeg\", 'wb') as f:\n    f.write(resp.content)\n```\n\n----------------------------------------\n\nTITLE: Navigating to Documentation Directory\nDESCRIPTION: Command to change into the Ray documentation directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd ray/doc\n```\n\n----------------------------------------\n\nTITLE: Implementing Task Dependencies in Ray (Python)\nDESCRIPTION: Demonstrates how to create dependent tasks in Ray using remote functions. It shows how Ray automatically handles task dependencies and object references.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef follow_up_task(retrieve_result):\n    original_item, _ = retrieve_result\n    follow_up_result = retrieve(original_item + 1)\n    return retrieve_result, follow_up_result\n\n\nretrieve_refs = [retrieve_task.remote(item, db_object_ref) for item in [0, 2, 4, 6]]\nfollow_up_refs = [follow_up_task.remote(ref) for ref in retrieve_refs]\n\nresult = [print(data) for data in ray.get(follow_up_refs)]\n```\n\n----------------------------------------\n\nTITLE: Inspecting Dataset Schema\nDESCRIPTION: Displays the schema of the loaded dataset to show the structure of the data, particularly the 'image' column containing Numpy arrays.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nds.schema()\n```\n\n----------------------------------------\n\nTITLE: Batching Requests in Ray Serve with Python\nDESCRIPTION: This code snippet demonstrates how to enable batching for a deployment using the ray.serve.batch decorator in Python. The decorator batches requests allowing ML models to handle multiple inputs simultaneously, improving throughput. The method signature must be asynchronous, accepting lists of inputs and returning a list of outputs, ensuring their lengths match. The snippet also explains configuring batch wait timeout and maximum batch size for optimal performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dyn-req-batch.md#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"\n  Example of enabling batching using the ray.serve.batch decorator\n  \n  async def example_batch_method(self, arg1: List[int], arg2: List[str]) -> List[int]:\n      # Method implementation\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Anti-pattern: Passing Large Argument by Value in Ray Tasks (Python)\nDESCRIPTION: This code snippet demonstrates the inefficient way of passing a large argument by value to multiple Ray tasks. This approach causes Ray to store multiple copies of the argument in the object store, leading to performance issues.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/pass-large-arg-by-value.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport numpy as np\n\n@ray.remote\ndef task(arr):\n    return arr.sum()\n\nray.init()\n\n# Create a large array\nlarge_array = np.random.rand(1000000)\n\n# Anti-pattern: Passing the same large argument by value to multiple tasks\nresults = ray.get([\n    task.remote(large_array)\n    for _ in range(10)\n])\n\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Serve REST API PUT Request Example\nDESCRIPTION: This example demonstrates how to deploy a list of Serve applications declaratively using the Serve REST API.  It sends a PUT request to the `/api/serve/applications/` endpoint with a JSON payload that defines the desired application configuration, including name, route prefix, import path, runtime environment, and deployments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/api/index.md#2025-04-12_snippet_0\n\nLANGUAGE: http\nCODE:\n```\n\"PUT /api/serve/applications/ HTTP/1.1\nHost: http://localhost:8265/\nAccept: application/json\nContent-Type: application/json\n\n{\n    \"applications\": [\n        {\n            \"name\": \"text_app\",\n            \"route_prefix\": \"/\",\n            \"import_path\": \"text_ml:app\",\n            \"runtime_env\": {\n                \"working_dir\": \"https://github.com/ray-project/serve_config_examples/archive/HEAD.zip\"\n            },\n            \"deployments\": [\n                {\"name\": \"Translator\", \"user_config\": {\"language\": \"french\"}},\n                {\"name\": \"Summarizer\"},\n            ]\n        },\n    ]\n}\"\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Tune and ASHA Scheduler\nDESCRIPTION: Imports for Ray Tune and the Asynchronous Hyperband Algorithm (ASHA) scheduler, which will be used to efficiently search the hyperparameter space.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\n```\n\n----------------------------------------\n\nTITLE: Visualizing Generator and Discriminator Losses in GAN Training\nDESCRIPTION: Creates a plot showing the Generator and Discriminator losses over time for all PBT trials, helping to diagnose training stability and convergence.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\nfor i, df in enumerate(result_dfs):\n    axs[0].plot(df[\"lossg\"], label=i)\naxs[0].legend()\naxs[0].set_title(\"Generator Loss During Training\")\naxs[0].set_xlabel(\"Training Iterations\")\naxs[0].set_ylabel(\"Generator Loss\")\n\nfor i, df in enumerate(result_dfs):\n    axs[1].plot(df[\"lossd\"], label=i)\naxs[1].legend()\naxs[1].set_title(\"Discriminator Loss During Training\")\naxs[1].set_xlabel(\"Training Iterations\")\naxs[1].set_ylabel(\"Discriminator Loss\")\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Creating GCS Bucket for Ray Filesystem\nDESCRIPTION: This command creates a Google Cloud Storage bucket that Ray will use as the remote filesystem for distributed checkpointing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nBUCKET=<your GCS bucket>\ngcloud storage buckets create gs://$BUCKET --uniform-bucket-level-access\n```\n\n----------------------------------------\n\nTITLE: Running Hyperparameter Optimization with Ray Tune in Python\nDESCRIPTION: Executes the experiment by attempting to minimize the 'mean_loss' using the specified search space and algorithm over a predetermined number of samples. This is done via Ray Tune's Tuner class.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n        metric=\"mean_loss\",\n        mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space=search_config,\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Ray Resource Data using CLI\nDESCRIPTION: Command to get specific resource data from a Ray cluster. Part of the State CLI for accessing detailed information about Ray resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/reference/cli.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nray get\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Runtime Environment\nDESCRIPTION: Initializes Ray with a runtime environment that includes necessary Python packages for GPT-J fine-tuning, ensuring all workers have access to required dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(\n    runtime_env={\n        \"pip\": [\n            \"datasets\",\n            \"evaluate\",\n            \"accelerate==0.18.0\",\n            \"transformers==4.26.0\",\n            \"torch>=1.12.0\",\n            \"deepspeed==0.12.3\",\n        ],\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Minimal AWS Ray Cluster Configuration\nDESCRIPTION: YAML configuration for setting up a minimal Ray cluster on AWS.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/getting-started.rst#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# A unique identifier for the head node and workers of this cluster.\ncluster_name: minimal\n\n# Cloud-provider specific configuration.\nprovider:\n    type: aws\n    region: us-west-2\n```\n\n----------------------------------------\n\nTITLE: Defining the search space\nDESCRIPTION: Defines the hyperparameter search space using Tune's search space API. This specifies the range of values for each hyperparameter that will be explored during the optimization process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"search_config = {\\n    \\\"steps\\\": 100,\\n    \\\"width\\\": tune.uniform(0, 20),\\n    \\\"height\\\": tune.uniform(-100, 100),\\n    \\\"activation\\\": tune.choice([\\\"relu, tanh\\\"])\\n}\"\n```\n\n----------------------------------------\n\nTITLE: Summarizing Ray actors using Python SDK\nDESCRIPTION: Using Ray's State API in Python to get a summary of all actors in the cluster, returning a dictionary with information about actor counts by class and state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import summarize_actors\nprint(summarize_actors())\n```\n\n----------------------------------------\n\nTITLE: Configuring Torch Compile for RLlib Exploration\nDESCRIPTION: Python code snippet showing how to configure Torch compile settings for the exploration phase in RLlib, including specifying the backend and mode.\nSOURCE: https://github.com/ray-project/ray/blob/master/rllib/benchmarks/torch_compile/README.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig.framework(\n    \"torch\",\n    torch_compile_worker=True,\n    torch_compile_worker_dynamo_backend=\"ipex\"\n    torch_compile_worker_dynamo_mode=\"default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring SYS_PTRACE Capability for Kubernetes Pods\nDESCRIPTION: Security context configuration required in the RayCluster YAML to enable py-spy profiling by adding the SYS_PTRACE capability to containers in both head and worker pods.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/pyspy.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsecurityContext:\n  capabilities:\n    add:\n    - SYS_PTRACE\n```\n\n----------------------------------------\n\nTITLE: Defining Python function for cross-language serialization\nDESCRIPTION: Defines a Python function that returns its input, used to demonstrate cross-language data serialization between Java and Python.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\ndef py_return_input(data):\n    return data\n```\n\n----------------------------------------\n\nTITLE: Push the model to Hugging Face Hub\nDESCRIPTION: Pushes the trained model to the Hugging Face Hub, making it available for others to use. This requires the user to be authenticated and Git LFS to be installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nmodel.push_to_hub()\n```\n\n----------------------------------------\n\nTITLE: Checking ProvisioningRequest Status - Initial\nDESCRIPTION: Shows how to check the status of a ProvisioningRequest for GPU resources using kubectl, displaying the initial state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get provisioningrequest\nNAME                                                      ACCEPTED   PROVISIONED   FAILED   AGE\nrayjob-pytorch-text-classifier-nv77q-e95ec-rayjob-gpu-1   True       False         False    22s\n```\n\n----------------------------------------\n\nTITLE: Running Python Test Files\nDESCRIPTION: Commands to run individual Python test files or specific tests in Ray\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython -m pytest -v -s python/ray/tests/test_basic.py\npython -m pytest -v -s test_file.py::name_of_the_test\n```\n\n----------------------------------------\n\nTITLE: Specifying Pydantic Package Version with Hash Verification for Ray Project\nDESCRIPTION: Defines Pydantic version 2.9.2 with SHA256 hashes for verification. Pydantic is required by the FastAPI dependency and is used for data validation and settings management.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_36\n\nLANGUAGE: pip requirements\nCODE:\n```\npydantic==2.9.2 \\\n    --hash=sha256:d155cef71265d1e9807ed1c32b4c8deec042a44a50a4188b25ac67ecd81a9c0f \\\n    --hash=sha256:f048cec7b26778210e28a0459867920654d48e5e62db0958433636cde4254f12\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\n    #   fastapi\n```\n\n----------------------------------------\n\nTITLE: Disabling Usage Stats via Environment Variable\nDESCRIPTION: Setting environment variable to disable Ray usage statistics collection\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/usage-stats.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRAY_USAGE_STATS_ENABLED=0 ray start --head\n```\n\n----------------------------------------\n\nTITLE: Injecting Callback Latency in Bash\nDESCRIPTION: This command starts Ray with an injected latency for the PrepareBundleResources RPC method. It sets the RAY_testing_asio_delay_us environment variable to introduce a delay between 2 to 2 seconds for testing purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/debugging.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nRAY_testing_asio_delay_us=\"NodeManagerService.grpc_client.PrepareBundleResources=2000000:2000000\" ray start --head\n```\n\n----------------------------------------\n\nTITLE: Connecting to Ray Cluster Shell\nDESCRIPTION: Demonstrates how to attach to a Ray cluster remotely using the ray attach command and initialize Ray within the cluster environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/getting-started.rst#2025-04-12_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# From a remote client:\n$ ray attach config.yaml\n\n# Now on the head node...\n$ python -c \"import ray; ray.init()\"\n```\n\n----------------------------------------\n\nTITLE: Starting Gaudi Docker Container\nDESCRIPTION: Commands to pull and run the Gaudi container with appropriate runtime settings and device configurations for AI acceleration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\ndocker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n```\n\n----------------------------------------\n\nTITLE: Status Report and Performance Metrics Table\nDESCRIPTION: Comprehensive status table showing memory usage, resource allocation, and detailed training metrics for 24 trials across different RL algorithms. Each row represents a trial with metrics including status, iterations, total time, timesteps, and achieved reward.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.3/rllib_regression.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n== Status ==\nMemory usage on this node: 133.8/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/64 CPUs, 0.0/8 GPUs, 0.0/323.54 GiB heap, 0.0/98.39 GiB objects\nResult logdir: /home/ubuntu/ray_results/apex\nResult logdir: /home/ubuntu/ray_results/atari-a2c\nResult logdir: /home/ubuntu/ray_results/atari-basic-dqn\nResult logdir: /home/ubuntu/ray_results/atari-impala\nResult logdir: /home/ubuntu/ray_results/atari-ppo-tf\nResult logdir: /home/ubuntu/ray_results/atari-ppo-torch\nNumber of trials: 24 (24 TERMINATED)\n+-------------------------------------+------------+-------+--------+------------------+---------+----------+\n| Trial name                          | status     | loc   |   iter |   total time (s) |      ts |   reward |\n|-------------------------------------+------------+-------+--------+------------------+---------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_00000 | TERMINATED |       |    293 |          3610.57 | 7244000 |    95.15 |\n| IMPALA_BreakoutNoFrameskip-v4_00001 | TERMINATED |       |    293 |          3610.01 | 7237500 |   233.92 |\n| IMPALA_BreakoutNoFrameskip-v4_00002 | TERMINATED |       |    293 |          3609.84 | 7285500 |   374.97 |\n| IMPALA_BreakoutNoFrameskip-v4_00003 | TERMINATED |       |    293 |          3609.78 | 7309500 |   371.48 |\n| PPO_BreakoutNoFrameskip-v4_00004    | TERMINATED |       |    673 |          3603.29 | 3365000 |    50.99 |\n| PPO_BreakoutNoFrameskip-v4_00005    | TERMINATED |       |    898 |          3602.34 | 4490000 |    56.53 |\n| PPO_BreakoutNoFrameskip-v4_00006    | TERMINATED |       |    903 |          3602.76 | 4515000 |    54.87 |\n| PPO_BreakoutNoFrameskip-v4_00007    | TERMINATED |       |    886 |          3603.85 | 4430000 |    36.06 |\n| PPO_BreakoutNoFrameskip-v4_00008    | TERMINATED |       |    305 |          3603.15 | 1525000 |    15.52 |\n| PPO_BreakoutNoFrameskip-v4_00009    | TERMINATED |       |    307 |          3607.11 | 1535000 |    19.75 |\n| PPO_BreakoutNoFrameskip-v4_00010    | TERMINATED |       |    257 |          3605.7  | 1285000 |    41.43 |\n| PPO_BreakoutNoFrameskip-v4_00011    | TERMINATED |       |    253 |          3613.21 | 1265000 |    23.07 |\n| APEX_BreakoutNoFrameskip-v4_00012   | TERMINATED |       |    114 |          3620.53 | 5090400 |    34.63 |\n| APEX_BreakoutNoFrameskip-v4_00013   | TERMINATED |       |    116 |          3619.38 | 7491040 |    78.16 |\n| APEX_BreakoutNoFrameskip-v4_00014   | TERMINATED |       |    115 |          3628.51 | 5769120 |    48.27 |\n| APEX_BreakoutNoFrameskip-v4_00015   | TERMINATED |       |    115 |          3627.06 | 5807200 |    49.18 |\n| A2C_BreakoutNoFrameskip-v4_00016    | TERMINATED |       |    351 |          3607.65 | 2904000 |   108.71 |\n| A2C_BreakoutNoFrameskip-v4_00017    | TERMINATED |       |    350 |          3608.21 | 2933000 |    63.93 |\n| A2C_BreakoutNoFrameskip-v4_00018    | TERMINATED |       |    351 |          3605.63 | 2898500 |   169.93 |\n| A2C_BreakoutNoFrameskip-v4_00019    | TERMINATED |       |    353 |          3606.64 | 3836000 |   232.4  |\n| DQN_BreakoutNoFrameskip-v4_00020    | TERMINATED |       |     27 |          3686.19 |  270000 |    17.53 |\n| DQN_BreakoutNoFrameskip-v4_00021    | TERMINATED |       |     27 |          3651.85 |  270000 |    18.34 |\n| DQN_BreakoutNoFrameskip-v4_00022    | TERMINATED |       |     27 |          3669.77 |  270000 |    15.45 |\n| DQN_BreakoutNoFrameskip-v4_00023    | TERMINATED |       |     27 |          3688.08 |  270000 |    12.25 |\n+-------------------------------------+------------+-------+--------+------------------+---------+----------+\n```\n\n----------------------------------------\n\nTITLE: Implementing TextOfflinePreLearner Class for RLlib\nDESCRIPTION: A custom OfflinePreLearner implementation that processes text data into episodes and applies frame stacking. It handles text tokenization, converts data to episodes, and prepares batches for training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nclass TextOfflinePreLearner(OfflinePreLearner):\n    @override(OfflinePreLearner)\n    def __init__(\n        self,\n        config: \"AlgorithmConfig\",\n        learner: Union[Learner, List[ActorHandle]] = None,\n        locality_hints: Optional[List[str]] = None,\n        spaces: Optional[Tuple[gym.Space, gym.Space]] = None,\n        module_spec: Optional[MultiRLModuleSpec] = None,\n        module_state: Optional[Dict[ModuleID, Any]] = None,\n        vocabulary: Dict[str, Any] = None,\n        **kwargs: Dict[str, Any],\n    ):\n        self.config = config\n        self.spaces = spaces\n        self.vocabulary = vocabulary\n        self.vocabulary_size = len(self.vocabulary)\n\n        self._module = module_spec.build()\n        if module_state:\n            self._module.set_state(module_state)\n\n        self._learner_connector = LearnerConnectorPipeline(\n            connectors=[\n                FrameStackingLearner(\n                    num_frames=4,\n                )\n            ],\n            input_action_space=module_spec.action_space,\n            input_observation_space=module_spec.observation_space,\n        )\n        self._learner_connector.append(\n            AddObservationsFromEpisodesToBatch(as_learner_connector=True),\n        )\n        self._learner_connector.append(\n            AddColumnsFromEpisodesToTrainBatch(),\n        )\n        self._learner_connector.append(\n            BatchIndividualItems(multi_agent=False),\n        )\n        self._learner_connector.append(\n            NumpyToTensor(as_learner_connector=True),\n        )\n```\n\n----------------------------------------\n\nTITLE: Transforming Rows with Flat Map in Ray Datasets\nDESCRIPTION: Demonstrates using the flat_map method to transform rows into multiple output rows in a Ray Dataset. The function duplicates each input row.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/transforming-data.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List\nimport ray\n\ndef duplicate_row(row: Dict[str, Any]) -> List[Dict[str, Any]]:\n    return [row] * 2\n\nprint(\n    ray.data.range(3)\n    .flat_map(duplicate_row)\n    .take_all()\n)\n```\n\n----------------------------------------\n\nTITLE: Head Pod Command and Args Example with Overwritten Container Command\nDESCRIPTION: This YAML snippet shows what the command and args look like in the head Pod when using the ray.io/overwrite-container-cmd annotation set to true, illustrating how user commands and the ray start command are combined.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/pod-command.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nCommand:\n  /bin/bash\n  -lc\n  --\nArgs:\n  ulimit -n 65536; echo head; $KUBERAY_GEN_RAY_START_CMD\n```\n\n----------------------------------------\n\nTITLE: Restructured Text Grid Layout for Documentation Navigation\nDESCRIPTION: A grid layout using restructured text (RST) syntax to create a navigation section with four cards containing links to different documentation sections including Getting Started, Key Concepts, Examples, and API Reference.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/index.md#2025-04-12_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. grid:: 1 2 2 2\n    :gutter: 1\n    :class-container: container pb-3\n\n    .. grid-item-card::\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        **Getting Started**\n        ^^^\n\n        Start with our quick start tutorials for :ref:`deploying a single model locally <serve-getting-started>` and how to :ref:`convert an existing model into a Ray Serve deployment <converting-to-ray-serve-application>` .\n\n        +++\n        .. button-ref:: serve-getting-started\n            :color: primary\n            :outline:\n            :expand:\n\n            Get Started with Ray Serve\n\n    .. grid-item-card::\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        **Key Concepts**\n        ^^^\n\n        Understand the key concepts behind Ray Serve.\n        Learn about :ref:`Deployments <serve-key-concepts-deployment>`, :ref:`how to query them <serve-key-concepts-ingress-deployment>`, and using :ref:`DeploymentHandles <serve-key-concepts-deployment-handle>` to compose multiple models and business logic together.\n\n        +++\n        .. button-ref:: serve-key-concepts\n            :color: primary\n            :outline:\n            :expand:\n\n            Learn Key Concepts\n\n    .. grid-item-card::\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        **Examples**\n        ^^^\n\n        Follow the tutorials to learn how to integrate Ray Serve with :ref:`TensorFlow <serve-ml-models-tutorial>`, and :ref:`Scikit-Learn <serve-ml-models-tutorial>`.\n\n        +++\n        .. button-ref:: examples\n            :color: primary\n            :outline:\n            :expand:\n            :ref-type: doc\n\n            Serve Examples\n\n    .. grid-item-card::\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        **API Reference**\n        ^^^\n\n        Get more in-depth information about the Ray Serve API.\n\n        +++\n        .. button-ref:: serve-api\n            :color: primary\n            :outline:\n            :expand:\n\n            Read the API Reference\n```\n\n----------------------------------------\n\nTITLE: Restoring Ray Tune Experiment from Cloud Storage (Python)\nDESCRIPTION: This snippet shows how to resume a Ray Tune experiment from a previously saved state in cloud storage. It uses the Tuner.restore method to load the experiment state and continue the run.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-storage.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\ntuner = tune.Tuner.restore(\n    \"s3://my-checkpoints-bucket/path/my-tune-exp\",\n    trainable=my_trainable,\n    resume_errored=True,\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Ray Task Resource Annotation Example in Python\nDESCRIPTION: Shows how to annotate Ray tasks with CPU requirements that the Ray autoscaler uses to determine scaling. This example demonstrates setting CPU requirements for a task that would require scaling to meet resource demands.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/k8s-autoscaler.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(num_cpus=5)\n```\n\n----------------------------------------\n\nTITLE: Specifying Jupyter Server Package Dependency\nDESCRIPTION: Defines the required version and hash values for the jupyter-server package, along with comments indicating its dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\njupyter-server==1.24.0 \\\n    --hash=sha256:23368e8e214baf82b313d4c5a0d828ca73015e1a192ce3829bd74e62fab8d046 \\\n    --hash=sha256:c88ddbe862966ea1aea8c3ccb89a5903abd8fbcfe5cd14090ef549d403332c37\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   jupyter-server-fileid\n    #   jupyterlab\n    #   jupyterlab-server\n    #   nbclassic\n    #   notebook-shim\n```\n\n----------------------------------------\n\nTITLE: Running Text Generation with Fine-tuned Model\nDESCRIPTION: Generates text completions using the fine-tuned Dolly-v2-7b model. Takes several prompt examples, runs them through the text generation pipeline with sampling, and prints the results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor prompt in [\"This is\", \"I am\", \"Once more\"]:\n    print(nlp_pipeline(prompt, max_new_tokens=20, do_sample=True, pad_token_id=tokenizer.eos_token_id))\n```\n\n----------------------------------------\n\nTITLE: Deploying Ray Cluster on AWS\nDESCRIPTION: Shell command to deploy the Ray cluster on AWS using the configuration file. This command creates a Ray head node and 9 worker nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/examples/ml-example.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nray up -y cluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Specifying Supported Python Versions\nDESCRIPTION: Lists the supported Python versions for the Ray project, ranging from Python 3.8 to Python 3.11.\nSOURCE: https://github.com/ray-project/ray/blob/master/docker/retag-lambda/python_versions.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npy38\npy39\npy310\npy311\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Pending Trials with Placement Groups\nDESCRIPTION: TUNE_MAX_PENDING_TRIALS_PG sets the maximum number of pending trials when using placement groups. Defaults to 'auto'.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_MAX_PENDING_TRIALS_PG=auto\n```\n\n----------------------------------------\n\nTITLE: CPU to GPU Tensor Communication\nDESCRIPTION: Example showing how to optimize torch.Tensor transfers between CPU and GPU devices using type hints in Ray Compiled Graph.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/quickstart.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith ray.dag.InputNode() as input_node:\n    output = actor.process.bind(input_node)\n    dag = output.experimental_compile()\n\ndag.execute(torch.zeros(10))\nray.get(dag)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Timer Event Listener in Ray Workflows\nDESCRIPTION: Sample implementation of a TimerListener which sleeps until a specified timestamp and then returns. This demonstrates a simple custom event listener in Ray Workflows.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/events.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TimerListener(EventListener):\n    async def poll_for_event(self, timestamp):\n        await asyncio.sleep(timestamp - time.time())\n```\n\n----------------------------------------\n\nTITLE: Initializing LLM with Accelerate for Efficient Memory Usage in Python\nDESCRIPTION: Efficiently loads a large language model using Accelerate's device mapping capabilities. The code initializes an empty model on a meta device, creates a device map that distributes the model across GPU and CPU memory, and then loads the model weights according to this mapping.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport ray\nimport lightning.pytorch as pl\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\nfrom accelerate import (\n    init_empty_weights,\n    infer_auto_device_map,\n    load_checkpoint_and_dispatch,\n)\n\n# Initialize a model on meta device\nwith init_empty_weights():\n    config = AutoConfig.from_pretrained(MODEL_NAME)\n    meta_model = AutoModelForCausalLM.from_config(config)\nmeta_model.tie_weights()\n\n# Define the device mapping\ndevice_map = infer_auto_device_map(\n    meta_model,\n    max_memory={0: \"15GB\", \"cpu\": \"60GB\"},\n    no_split_module_classes=[\"LlamaDecoderLayer\"],\n)\n\n# Load the model parameters\nmodel = load_checkpoint_and_dispatch(\n    meta_model,\n    checkpoint=full_model_ckpt_path,\n    device_map=device_map,\n)\n```\n\n----------------------------------------\n\nTITLE: Captured ObjectRef References Example in Ray\nDESCRIPTION: Example showing how object references can be captured inside other objects, demonstrating how references are tracked when one object reference is stored inside another and the original reference is deleted.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/memory-management.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\na = ray.put(None)\nb = ray.put([a])\ndel a\n```\n\n----------------------------------------\n\nTITLE: Custom Serialization Helper for Specific Objects\nDESCRIPTION: Demonstrates how to create a helper class to serialize specific instances of an unserializable class. This approach allows serialization of individual objects without modifying the original class.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/serialization.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport threading\n\nclass A:\n    def __init__(self, x):\n        self.x = x\n        self.lock = threading.Lock()  # could not serialize!\n\ntry:\n   ray.get(ray.put(A(1)))  # fail!\nexcept TypeError:\n   pass\n\nclass SerializationHelperForA:\n    \"\"\"A helper class for serialization.\"\"\"\n    def __init__(self, a):\n        self.a = a\n\n    def __reduce__(self):\n        return A, (self.a.x,)\n\nray.get(ray.put(SerializationHelperForA(A(1))))  # success!\n# the serializer only works for a specific object, not all A\n# instances, so we still expect failure here.\ntry:\n   ray.get(ray.put(A(1)))  # still fail!\nexcept TypeError:\n   pass\n```\n\n----------------------------------------\n\nTITLE: Setting Base URLs for Crawling\nDESCRIPTION: Defining the base URL and documentation URL for crawling the Ray documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/web-crawler.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbase = \"https://docs.ray.io/en/latest/\"\ndocs = base + \"index.html\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Runtime Environment in RayJob YAML\nDESCRIPTION: Example of specifying a runtime environment for a Ray job, including pip dependencies and environment variables.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  runtimeEnvYAML: |\n    pip:\n      - requests==2.26.0\n      - pendulum==2.1.2\n    env_vars:\n      KEY: \"VALUE\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Provider Settings for Ray Cluster in YAML\nDESCRIPTION: This YAML structure defines provider-specific settings for different cloud platforms (AWS, Azure, GCP, vSphere) in a Ray cluster configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# AWS\ntype: str\nregion: str\navailability_zone: str\ncache_stopped_nodes: bool\nsecurity_group:\n    Security Group\nuse_internal_ips: bool\n\n# Azure\ntype: str\nlocation: str\nresource_group: str\nsubscription_id: str\nmsi_name: str\nmsi_resource_group: str\ncache_stopped_nodes: bool\nuse_internal_ips: bool\nuse_external_head_ip: bool\n\n# GCP\ntype: str\nregion: str\navailability_zone: str\nproject_id: str\ncache_stopped_nodes: bool\nuse_internal_ips: bool\n\n# vSphere\ntype: str\nvsphere_config:\n    vSphere Config\n```\n\n----------------------------------------\n\nTITLE: Running Ray Tune Optimization\nDESCRIPTION: This code snippet shows how to execute the Ray Tune optimization process using the configured Tuner object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune_get_data_in_and_out.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Environment for Ray Documentation\nDESCRIPTION: Commands to create and activate a Python environment specifically for building Ray documentation using conda.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda create -n docs python=3.12\nconda activate docs\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up the Text Summarizer RayService\nDESCRIPTION: Command to delete the RayService and clean up the deployed text summarizer resources from the Kubernetes cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/text-summarizer-rayservice.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl delete -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-service.text-summarizer.yaml\n```\n\n----------------------------------------\n\nTITLE: Generating Images with LoRA Fine-Tuned Model in Python\nDESCRIPTION: This Python command demonstrates how to generate images using a LoRA fine-tuned Stable Diffusion model. It specifies both the original model directory and the LoRA weights directory, along with output settings and prompts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/README.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython generate.py \\\n  --model_dir=$ORIG_MODEL_PATH \\\n  --lora_weights_dir=$TUNED_MODEL_DIR \\\n  --output_dir=$IMAGES_NEW_DIR \\\n  --prompts=\"photo of a $UNIQUE_TOKEN $CLASS_NAME\" \\\n  --num_samples_per_prompt=5\n```\n\n----------------------------------------\n\nTITLE: Creating a Range Dataset with Ray Data\nDESCRIPTION: This snippet demonstrates how to create a synthetic Ray Dataset from a range of integers using `ray.data.range`. The dataset will contain a single column named 'id' with integer values from 0 up to (but not including) the specified range.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.range(10000)\n\nprint(ds.schema())\n```\n\n----------------------------------------\n\nTITLE: Basic Tuner Execution in Ray Tune\nDESCRIPTION: Example of running a basic tuning experiment with Ray Tune using a simple search space for parameters a and b.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nparam_space = {\n    \"a\": 0.3,\n    \"b\": 0.05,\n}\ntuner = tune.Tuner(\n    trainable,\n    param_space=param_space,\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Setting up Ray driver with multiple directories in Python\nDESCRIPTION: Demonstrates how to set multiple directories for code search path in Python, useful when Python and Java code are in different locations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(\n    _temp_dir=\"/tmp/ray\",\n    _system_config={\"worker_code_search_path\": \"/path/to/jars:/path/to/pys\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Worker Node Ray Setup\nDESCRIPTION: Commands for setting up Ray worker nodes using the head node address from Skein.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/yarn.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nRAY_HEAD_ADDRESS=$(skein kv get current --key=RAY_HEAD_ADDRESS)\nray start --object-store-memory=200000000 --memory 200000000 --num-cpus=1 --address=$RAY_HEAD_ADDRESS:6379 --block; ray stop\n```\n\n----------------------------------------\n\nTITLE: Running Ray Serve Application with Required Image\nDESCRIPTION: Command to run the Ray Serve application after switching to the ray-ml image that contains TensorFlow. This demonstrates successful loading of the MobileNet model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nserve run mobilenet.mobilenet:app\n```\n\n----------------------------------------\n\nTITLE: Updating Ray Remote Function Definitions in Python\nDESCRIPTION: Illustrates the challenge of ensuring that modified remote function definitions are picked up by Ray. The solution involves using importlib's reload function to reload external modules when necessary.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/general-debugging.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote\ndef f():\n    return 1\n\n@ray.remote\ndef f():\n    return 2\n\nprint(ray.get(f.remote()))  # Expected to print 2.\n\n# Solution when `f` depends on an external module\n@ray.remote\ndef f():\n    from importlib import reload\n    import file  # Make sure to import the module after reload\n    reload(file)\n    return file.h()\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Cluster with Helm\nDESCRIPTION: Command to install a KubeRay cluster using Helm package manager. This sets up the base Ray cluster where the Serve application will be deployed and tested.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nhelm install raycluster kuberay/ray-cluster --version 1.3.0\n```\n\n----------------------------------------\n\nTITLE: Referencing a Hyperopt Conditional Search Space Example in Ray Tune\nDESCRIPTION: This snippet is a literal include directive that references an example Python file demonstrating how to use conditional search spaces with Hyperopt in Ray Tune. The file path indicates it's part of the Ray project's examples directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/hyperopt_conditional_search_space_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. literalinclude:: /../../python/ray/tune/examples/hyperopt_conditional_search_space_example.py\n```\n\n----------------------------------------\n\nTITLE: Applying Updated Ray Service Manifest in Kubernetes\nDESCRIPTION: This console command applies the modified Ray Service manifest to update the application configuration in Kubernetes. It also shows how to check the status of the update using kubectl describe.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/kubernetes.md#2025-04-12_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl apply -f ray-service.text-ml.yaml\n\n$ kubectl describe rayservice rayservice-sample\n...\n  Serve Deployment Statuses:\n    text_ml_app_Translator:\n      Health Last Update Time:  2023-09-07T18:21:36Z\n      Last Update Time:         2023-09-07T18:21:36Z\n      Status:                   UPDATING\n...\n```\n\n----------------------------------------\n\nTITLE: Creating RayJob with Higher Priority for Preemption Demo\nDESCRIPTION: Command to create a RayJob with higher priority class that will preempt the lower priority RayJob.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f ray-job.kueue-toy-sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Web Applications on Ray Clusters\nDESCRIPTION: Command to setup port forwarding for web-based applications like Jupyter notebooks running on the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-cli.rst#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ ray exec cluster.yaml --port-forward=8899 'source ~/anaconda3/bin/activate tensorflow_p36 && jupyter notebook --port=8899'\n```\n\n----------------------------------------\n\nTITLE: Allocating Resources to Ray Tune Trials in Python\nDESCRIPTION: This snippet demonstrates how to allocate specific CPU and GPU resources to Ray Tune trials using tune.with_resources. It also shows how to request custom resources and use fractional GPUs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrainable_with_resources = tune.with_resources(\n    trainable,\n    {\"cpu\": 2, \"gpu\": 0.5, \"custom_resources\": {\"hdd\": 80}}\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Ray Project Dependencies with Python Version Constraints\nDESCRIPTION: Requirements file specifying Python package dependencies with version constraints based on Python version compatibility. Includes packages for data processing (daft, dask, pandas), cloud services (aioboto3), and web services (flask_cors).\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/data-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngetdaft==0.4.3\ndask[complete]==2022.10.2; python_version < '3.12'\ndistributed==2022.10.2; python_version < '3.12'\ndask[complete]==2024.6.0; python_version >= '3.12'\ndistributed==2024.6.0; python_version >= '3.12'\naioboto3==11.2.0\ncrc32c==2.3\nflask_cors\nmodin==0.22.2; python_version < '3.12'\npandas==1.5.3; python_version < '3.12'\nmodin==0.31.0; python_version >= '3.12'\npandas==2.2.2; python_version >= '3.12'\nresponses==0.13.4\npymars>=0.8.3; python_version < \"3.12\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray Framework Components using Sphinx Grid\nDESCRIPTION: A Sphinx reStructuredText grid layout that presents the three main components of Ray: machine learning workload scaling, distributed application building, and large-scale workload deployment. Each component includes a brief description and a button linking to relevant documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/index.md#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. grid:: 1 2 3 3\n    :gutter: 1\n    :class-container: container pb-3\n\n    .. grid-item-card::\n\n        **Scale machine learning workloads**\n        ^^^\n        Build ML applications with a toolkit of libraries for distributed\n        :doc:`data processing <../data/data>`,\n        :doc:`model training <../train/train>`,\n        :doc:`tuning <../tune/index>`,\n        :doc:`reinforcement learning <../rllib/index>`,\n        :doc:`model serving <../serve/index>`,\n        and :doc:`more <../ray-more-libs/index>`.\n        +++\n        .. button-ref:: libraries-quickstart\n            :color: primary\n            :outline:\n            :expand:\n\n            Ray AI Libraries\n\n    .. grid-item-card::\n\n        **Build distributed applications**\n        ^^^\n        Build and run distributed applications with a\n        :doc:`simple and flexible API <../ray-core/walkthrough>`.\n        :doc:`Parallelize <../ray-core/walkthrough>` single machine code with\n        little to zero code changes.\n\n        +++\n        .. button-ref:: ../ray-core/walkthrough\n            :color: primary\n            :outline:\n            :expand:\n\n            Ray Core\n\n    .. grid-item-card::\n\n        **Deploy large-scale workloads**\n        ^^^\n        Deploy workloads on :doc:`AWS, GCP, Azure <../cluster/getting-started>` or\n        :doc:`on premise <../cluster/vms/user-guides/launching-clusters/on-premises>`.\n        Use Ray cluster managers to run Ray on existing\n        :doc:`Kubernetes <../cluster/kubernetes/index>`,\n        :doc:`YARN <../cluster/vms/user-guides/community/yarn>`,\n        or :doc:`Slurm <../cluster/vms/user-guides/community/slurm>` clusters.\n        +++\n        .. button-ref:: ../cluster/getting-started\n            :color: primary\n            :outline:\n            :expand:\n\n            Ray Clusters\n```\n\n----------------------------------------\n\nTITLE: Verifying TLS Authentication for Ray Cluster\nDESCRIPTION: Commands to verify the TLS authentication setup by logging into a worker Pod and testing connections to the head Pod.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/tls.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Log in to the worker Pod\nkubectl exec -it ${WORKER_POD} -- bash\n\n# Since the head Pod has the certificate of $FQ_RAY_IP, the connection to the worker Pods\n# will be established successfully, and the exit code of the ray health-check command\n# should be 0.\nray health-check --address $FQ_RAY_IP:6379\necho $? # 0\n\n# Since the head Pod has the certificate of $RAY_IP, the connection will fail and an error\n# message similar to the following will be displayed: \"Peer name raycluster-tls-head-svc is\n# not in peer certificate\".\nray health-check --address $RAY_IP:6379\n\n# If you add `DNS.3 = $RAY_IP` to the [alt_names] section in `gencert_head.sh`,\n# the head Pod will generate the certificate of $RAY_IP.\n#\n# For KubeRay versions prior to 0.5.0, this step is necessary because Ray workers in earlier\n# versions use $RAY_IP to connect with Ray head.\n```\n\n----------------------------------------\n\nTITLE: Collecting Actor Method Results for cProfile Analysis\nDESCRIPTION: Example showing a suboptimal pattern of sequential actor method calls with Ray. This code is used to demonstrate performance profiling with cProfile to identify bottlenecks in actor-based applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/optimize-performance.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef ex4():\n    # This is suboptimal in Ray, and should only be used for the sake of this example\n    actor_example = Sleeper.remote()\n\n    five_results = []\n    for i in range(5):\n        five_results.append(actor_example.actor_func.remote())\n\n    # Wait until the end to call ray.get()\n    ray.get(five_results)\n```\n\n----------------------------------------\n\nTITLE: Creating a RayService for Text Summarization in Kubernetes\nDESCRIPTION: Command to apply the RayService YAML configuration for text summarizer deployment on Kubernetes using kubectl.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/text-summarizer-rayservice.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Create a RayService\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-service.text-summarizer.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for Ray\nDESCRIPTION: Creates a new conda environment named 'ray' with Python 3.9 and activates it. This isolates Ray's dependencies from other projects.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n\"# also works with mamba\\nconda create -c conda-forge python=3.9 -n ray\\nconda activate ray\"\n```\n\n----------------------------------------\n\nTITLE: Returning Single Value from Ray Task\nDESCRIPTION: This snippet demonstrates the correct way to return a single value from a Ray task. It shows that values should be returned directly, regardless of their size, to allow for potential performance optimizations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/return-ray-put.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef task():\n    value = compute_value()\n    # Good: Return the value directly\n    return value\n\n# Good: Get the value directly\nresult = ray.get(task.remote())\n```\n\n----------------------------------------\n\nTITLE: Defining Data Partitions for Training\nDESCRIPTION: Retrieves the list of unique dataset partition IDs from the M4 hourly dataset. This will be used to configure the training function for each partition.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/start.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata_partition_ids = list(\n    pd.read_parquet(\n        \"https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet\",\n        columns=[\"unique_id\"],\n    )[\"unique_id\"].unique()\n)\nprint(f\"Training on a total of {len(data_partition_ids)} dataset partitions.\")\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with ByteToMessageDecoder Channel Read\nDESCRIPTION: This stack trace shows Netty's event processing flow with a focus on the ByteToMessageDecoder's channelRead method. The trace demonstrates how bytes are read from channels and passed to decoders in the Netty pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_42\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Accessing Ray Dashboard to Verify Serve Applications\nDESCRIPTION: Sets up port forwarding to access the Ray Dashboard and verify the correct configuration of HTTPProxyActors and Ray Serve replicas. The head node should have HTTPProxyActor but no Ray Serve replicas, while worker nodes should have both.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-high-availability.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# Port forward the Ray Dashboard.\nkubectl port-forward svc/rayservice-ha-head-svc 8265:8265\n# Visit ${YOUR_IP}:8265 in your browser for the Dashboard (e.g. 127.0.0.1:8265)\n# Check:\n# (1) Both head and worker nodes have HTTPProxyActors.\n# (2) Only worker nodes have Ray Serve replicas.\n# (3) Each worker node has one Ray Serve replica for each Ray Serve deployment.\n```\n\n----------------------------------------\n\nTITLE: Defining Trainable Function for Ray Tune in Python\nDESCRIPTION: Illustrates how to define an objective function and a trainable function for use with Ray Tune. The trainable function iterates 20 times, computes scores using the objective function, and reports the scores to Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import train\n\n\ndef objective(x, a, b):  # Define an objective function.\n    return a * (x ** 0.5) + b\n\n\ndef trainable(config):  # Pass a \"config\" dictionary into your trainable.\n\n    for x in range(20):  # \"Train\" for 20 iterations and compute intermediate scores.\n        score = objective(x, config[\"a\"], config[\"b\"])\n\n        train.report({\"score\": score})  # Send the score to Tune.\n```\n\n----------------------------------------\n\nTITLE: Limiting Dataset Size for Testing\nDESCRIPTION: Limits the dataset size to accelerate testing by ensuring each worker has only 10 batches to process. This is useful for development and testing the training pipeline without using the full dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# To accelerate release tests\ntrain_ds = train_ds.limit(num_workers * batch_size_per_worker * 10)  # each worker has 10 batches\n```\n\n----------------------------------------\n\nTITLE: Accessing Current Namespace\nDESCRIPTION: Examples of how to retrieve the current namespace information using runtime context APIs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/namespaces.rst#2025-04-12_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nSystem.setProperty(\"ray.job.namespace\", \"colors\");\ntry {\n    Ray.init();\n    // Will print namespace name \"colors\".\n    System.out.println(Ray.getRuntimeContext().getNamespace());\n} finally {\n    Ray.shutdown();\n}\n```\n\nLANGUAGE: c++\nCODE:\n```\nray::RayConfig config;\nconfig.ray_namespace = \"colors\";\nray::Init(config);\n// Will print namespace name \"colors\".\nstd::cout << ray::GetNamespace() << std::endl;\nray::Shutdown();\n```\n\n----------------------------------------\n\nTITLE: Calling Python from Java in Ray\nDESCRIPTION: Demonstrates how to call Python remote functions and create Python actors from Java using Ray's cross-language feature.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_7\n\nLANGUAGE: java\nCODE:\n```\npackage io.ray.demo;\n\nimport io.ray.api.ObjectRef;\nimport io.ray.api.PyActorHandle;\nimport io.ray.api.Ray;\nimport io.ray.api.function.PyActorClass;\nimport io.ray.api.function.PyActorMethod;\nimport io.ray.api.function.PyFunction;\nimport org.testng.Assert;\n\npublic class JavaCallPythonDemo {\n\n  public static void main(String[] args) {\n    // Set the code-search-path to the directory of your `ray_demo.py` file.\n    System.setProperty(\"ray.job.code-search-path\", \"/path/to/the_dir/\");\n    Ray.init();\n\n    // Define a Python class.\n    PyActorClass actorClass = PyActorClass.of(\n        \"ray_demo\", \"Counter\");\n\n    // Create a Python actor and call actor method.\n    PyActorHandle actor = Ray.actor(actorClass).remote();\n    ObjectRef objRef1 = actor.task(\n        PyActorMethod.of(\"increment\", int.class)).remote();\n    Assert.assertEquals(objRef1.get(), 1);\n    ObjectRef objRef2 = actor.task(\n        PyActorMethod.of(\"increment\", int.class)).remote();\n    Assert.assertEquals(objRef2.get(), 2);\n\n    // Call the Python remote function.\n    ObjectRef objRef3 = Ray.task(PyFunction.of(\n        \"ray_demo\", \"add\", int.class), 1, 2).remote();\n    Assert.assertEquals(objRef3.get(), 3);\n\n    Ray.shutdown();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Serve Applications Info Response (abridged JSON)\nDESCRIPTION: This JSON response is an example of the data returned when querying the `/api/serve/applications/` endpoint, providing details about the controller, proxies, HTTP options, deployed applications, and their configurations. It shows the structure and content of the JSON response with specific information about the Ray Serve cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/api/index.md#2025-04-12_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"controller_info\": {\n        \"node_id\": \"cef533a072b0f03bf92a6b98cb4eb9153b7b7c7b7f15954feb2f38ec\",\n        \"node_ip\": \"10.0.29.214\",\n        \"actor_id\": \"1d214b7bdf07446ea0ed9d7001000000\",\n        \"actor_name\": \"SERVE_CONTROLLER_ACTOR\",\n        \"worker_id\": \"adf416ae436a806ca302d4712e0df163245aba7ab835b0e0f4d85819\",\n        \"log_file_path\": \"/serve/controller_29778.log\"\n    },\n    \"proxy_location\": \"EveryNode\",\n    \"http_options\": {\n        \"host\": \"0.0.0.0\",\n        \"port\": 8000,\n        \"root_path\": \"\",\n        \"request_timeout_s\": null,\n        \"keep_alive_timeout_s\": 5\n    },\n    \"grpc_options\": {\n        \"port\": 9000,\n        \"grpc_servicer_functions\": []\n    },\n    \"proxies\": {\n        \"cef533a072b0f03bf92a6b98cb4eb9153b7b7c7b7f15954feb2f38ec\": {\n            \"node_id\": \"cef533a072b0f03bf92a6b98cb4eb9153b7b7c7b7f15954feb2f38ec\",\n            \"node_ip\": \"10.0.29.214\",\n            \"actor_id\": \"b7a16b8342e1ced620ae638901000000\",\n            \"actor_name\": \"SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-cef533a072b0f03bf92a6b98cb4eb9153b7b7c7b7f15954feb2f38ec\",\n            \"worker_id\": \"206b7fe05b65fac7fdceec3c9af1da5bee82b0e1dbb97f8bf732d530\",\n            \"log_file_path\": \"/serve/http_proxy_10.0.29.214.log\",\n            \"status\": \"HEALTHY\"\n        }\n    },\n    \"deploy_mode\": \"MULTI_APP\",\n    \"applications\": {\n        \"app1\": {\n            \"name\": \"app1\",\n            \"route_prefix\": \"/\",\n            \"docs_path\": null,\n            \"status\": \"RUNNING\",\n            \"message\": \"\",\n            \"last_deployed_time_s\": 1694042836.1912267,\n            \"deployed_app_config\": {\n                \"name\": \"app1\",\n                \"route_prefix\": \"/\",\n                \"import_path\": \"src.text-test:app\",\n                \"deployments\": [\n                    {\n                        \"name\": \"Translator\",\n                        \"num_replicas\": 1,\n                        \"user_config\": {\n                            \"language\": \"german\"\n                        }\n                    }\n                ]\n            },\n            \"deployments\": {\n                \"Translator\": {\n                    \"name\": \"Translator\",\n                    \"status\": \"HEALTHY\",\n                    \"message\": \"\",\n                    \"deployment_config\": {\n                        \"name\": \"Translator\",\n                        \"num_replicas\": 1,\n                        \"max_ongoing_requests\": 100,\n                        \"user_config\": {\n                            \"language\": \"german\"\n                        },\n                        \"graceful_shutdown_wait_loop_s\": 2.0,\n                        \"graceful_shutdown_timeout_s\": 20.0,\n                        \"health_check_period_s\": 10.0,\n                        \"health_check_timeout_s\": 30.0,\n                        \"ray_actor_options\": {\n                            \"runtime_env\": {\n                                \"env_vars\": {}\n                            },\n                            \"num_cpus\": 1.0\n                        },\n                        \"is_driver_deployment\": false\n                    },\n                    \"replicas\": [\n                        {\n                            \"node_id\": \"cef533a072b0f03bf92a6b98cb4eb9153b7b7c7b7f15954feb2f38ec\",\n                            \"node_ip\": \"10.0.29.214\",\n                            \"actor_id\": \"4bb8479ad0c9e9087fee651901000000\",\n                            \"actor_name\": \"SERVE_REPLICA::app1#Translator#oMhRlb\",\n                            \"worker_id\": \"1624afa1822b62108ead72443ce72ef3c0f280f3075b89dd5c5d5e5f\",\n                            \"log_file_path\": \"/serve/deployment_Translator_app1#Translator#oMhRlb.log\",\n                            \"replica_id\": \"app1#Translator#oMhRlb\",\n                            \"state\": \"RUNNING\",\n                            \"pid\": 29892,\n                            \"start_time_s\": 1694042840.577496\n                        }\n                    ]\n                },\n                \"Summarizer\": {\n                    \"name\": \"Summarizer\",\n                    \"status\": \"HEALTHY\",\n                    \"message\": \"\",\n                    \"deployment_config\": {\n                        \"name\": \"Summarizer\",\n                        \"num_replicas\": 1,\n                        \"max_ongoing_requests\": 100,\n                        \"user_config\": null,\n                        \"graceful_shutdown_wait_loop_s\": 2.0,\n                        \"graceful_shutdown_timeout_s\": 20.0,\n                        \"health_check_period_s\": 10.0,\n                        \"health_check_timeout_s\": 30.0,\n                        \"ray_actor_options\": {\n                            \"runtime_env\": {},\n                            \"num_cpus\": 1.0\n                        },\n                        \"is_driver_deployment\": false\n                    },\n                    \"replicas\": [\n                        {\n                            \"node_id\": \"cef533a072b0f03bf92a6b98cb4eb9153b7b7c7b7f15954feb2f38ec\",\n                            \"node_ip\": \"10.0.29.214\",\n                            \"actor_id\": \"7118ae807cffc1c99ad5ad2701000000\",\n                            \"actor_name\": \"SERVE_REPLICA::app1#Summarizer#cwiPXg\",\n                            \"worker_id\": \"12de2ac83c18ce4a61a443a1f3308294caf5a586f9aa320b29deed92\",\n                            \"log_file_path\": \"/serve/deployment_Summarizer_app1#Summarizer#cwiPXg.log\",\n                            \"replica_id\": \"app1#Summarizer#cwiPXg\",\n                            \"state\": \"RUNNING\",\n                            \"pid\": 29893,\n                            \"start_time_s\": 1694042840.5789504\n                        }\n                    ]\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting a Ray Job using CLI\nDESCRIPTION: Demonstrates the correct syntax for submitting a Ray job using the CLI. It emphasizes the proper placement of arguments and the entrypoint command.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/cli.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nray job submit --working-dir=\".\" -- python script.py\n```\n\n----------------------------------------\n\nTITLE: Deploying to a Remote Ray Cluster\nDESCRIPTION: Deploys a Serve application to a specified remote Ray cluster by providing the remote cluster's dashboard address. This allows users to specify the remote address for deployment instead of the default local address.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/deploy-vm.md#2025-04-12_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ serve deploy config_file.yaml -a http://127.0.0.1:8265\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs necessary Python packages for running the notebook. These packages include datasets, evaluate, accelerate, transformers, torch, and deepspeed. This step ensures that the environment is ready for data processing and model training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install -q \"datasets\" \"evaluate\" \"accelerate==0.18.0\" \"transformers==4.26.0\" \"torch>=1.12.0\" \"deepspeed==0.12.3\"\n```\n\n----------------------------------------\n\nTITLE: Prometheus Query for Ray Dashboard API Performance Metrics\nDESCRIPTION: A Prometheus query to calculate the 95th percentile of API request durations in Ray Dashboard. This query aggregates all metrics collected over a 5-minute window to monitor dashboard performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/configure-manage-dashboard.md#2025-04-12_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nhistogram_quantile(0.95, sum(rate(ray_dashboard_api_requests_duration_seconds_bucket[5m])) by (le))\n```\n\n----------------------------------------\n\nTITLE: Generating Code Using Fine-tuned Model\nDESCRIPTION: Demonstrates the usage of the fine-tuned model to generate code snippets based on the test cases using a prompt template.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfor case in testcases:\n    prompt = PROMPT_TEMPLATE.format(intent=case[\"intent\"], snippet=\"\")\n    output = generator(prompt, max_new_tokens=30, do_sample=True)\n    print(output[0][\"generated_text\"])\n```\n\n----------------------------------------\n\nTITLE: Specifying GCSFuse Mount Options\nDESCRIPTION: This YAML snippet demonstrates how to specify mount options when defining the GCSFuse container volume, including bucket name and mount options for implicit directories and UID/GID settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ncsi:\n  driver: gcsfuse.csi.storage.gke.io\n  volumeAttributes:\n    bucketName: GCS_BUCKET\n    mountOptions: \"implicit-dirs,uid=1000,gid=100\"\n```\n\n----------------------------------------\n\nTITLE: Client Script to Test Translator Model\nDESCRIPTION: This client script demonstrates how to send a POST request to the Translator service to obtain translations. It encapsulates the usage of HTTP requests within a testing context to interact with the deployed model, verifying that it correctly processes incoming data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/getting_started.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport requests\n\nresponse = requests.post('http://127.0.0.1:8000/', json={'text': 'Hello world!'})\nprint(response.json())\n```\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Queue-Based Event Listener with Exactly-Once Delivery\nDESCRIPTION: Example of a custom event listener that consumes events from a Kafka-like queue with exactly-once delivery semantics. It demonstrates proper event checkpointing and commitment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/events.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nKafkaEventType = ...\n\nclass QueueEventListener:\n    def __init__(self):\n        # Initialize the poll consumer.\n        self.consumer = Consumer({'enable.auto.commit': False})\n\n    async def poll_for_event(self, topic) -> KafkaEventType:\n        self.consumer.subscribe(topic)\n\n        message = await self.consumer.poll()\n        return message\n\n    async def event_checkpointed(self, event: KafkaEventType) -> None:\n         self.consumer.commit(event, asynchronous=False)\n```\n\n----------------------------------------\n\nTITLE: Printing Ray Cluster Resources\nDESCRIPTION: Displays available resources in the Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(ray.cluster_resources())\n```\n\n----------------------------------------\n\nTITLE: Updating RayCluster Worker Replicas in YAML\nDESCRIPTION: This YAML snippet shows how to update the number of worker nodes in a RayCluster configuration. It sets the number of replicas in the worker group to 2, demonstrating how to scale the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/kubernetes.md#2025-04-12_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nworkerGroupSpecs:\n  # the number of pods in the worker group.\n  - replicas: 2\n```\n\n----------------------------------------\n\nTITLE: Exporting Ray Prometheus Metrics to CloudWatch using YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates the head_setup_commands configuration to export Ray's Prometheus metrics to CloudWatch. It includes steps to make scripts executable, copy configuration files, and restart the CloudWatch agent.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nhead_setup_commands:\n  # Make `ray_prometheus_waiter.sh` executable.\n  - >\n    RAY_INSTALL_DIR=`pip show ray | grep -Po \"(?<=Location:).*\"`\n    && sudo chmod +x $RAY_INSTALL_DIR/ray/autoscaler/aws/cloudwatch/ray_prometheus_waiter.sh\n  # Copy `prometheus.yml` to Unified CloudWatch Agent folder\n  - >\n    RAY_INSTALL_DIR=`pip show ray | grep -Po \"(?<=Location:).*\"`\n    && sudo cp -f $RAY_INSTALL_DIR/ray/autoscaler/aws/cloudwatch/prometheus.yml /opt/aws/amazon-cloudwatch-agent/etc\n  # First get current cluster name, then let the Unified CloudWatch Agent restart and use `AmazonCloudWatch-ray_agent_config_{cluster_name}` parameter at SSM Parameter Store.\n  - >\n    nohup sudo sh -c \"`pip show ray | grep -Po \"(?<=Location:).*\"`/ray/autoscaler/aws/cloudwatch/ray_prometheus_waiter.sh\n    `cat ~/ray_bootstrap_config.yaml | jq '.cluster_name'`\n    >> '/opt/aws/amazon-cloudwatch-agent/logs/ray_prometheus_waiter.out' 2>> '/opt/aws/amazon-cloudwatch-agent/logs/ray_prometheus_waiter.err'\" &\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Collective Library in Python\nDESCRIPTION: Code snippet showing how to import the Ray collective communication package in your actor/task or driver code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/ray-collective.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray.util.collective as col\n```\n\n----------------------------------------\n\nTITLE: Listing Ray Resources using CLI\nDESCRIPTION: Command to list resources in a Ray cluster. Part of the State CLI for accessing live state of various Ray resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/reference/cli.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nray list\n```\n\n----------------------------------------\n\nTITLE: Running PPO Benchmarks with Torch Compile in RLlib\nDESCRIPTION: Python command to run PPO algorithm benchmarks using Torch compile in RLlib. Allows specifying the backend and mode for compilation.\nSOURCE: https://github.com/ray-project/ray/blob/master/rllib/benchmarks/torch_compile/README.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython ./run_ppo_with_inference_bm.py --backend <backend> --mode <mode>\n```\n\n----------------------------------------\n\nTITLE: Creating RayJob with Lower Priority for Preemption Demo\nDESCRIPTION: Command to create a RayJob with lower priority class for demonstrating priority-based preemption with Kueue.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f ray-job.kueue-toy-sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Running GPU Image Training Benchmark with PyTorch\nDESCRIPTION: This command runs a PyTorch training benchmark using a ResNet model on 1GB of image data with a single worker.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/benchmarks.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython pytorch_training_e2e.py --data-size-gb=1\n```\n\n----------------------------------------\n\nTITLE: Reducing Samples for Smoke Testing\nDESCRIPTION: Reduces the number of samples for quick testing or demonstration purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Reducing samples for smoke tests\nnum_samples = 10\n```\n\n----------------------------------------\n\nTITLE: Running Ray Example Script\nDESCRIPTION: Command to execute the user's Ray program.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/yarn.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython example.py\n```\n\n----------------------------------------\n\nTITLE: Creating GKE Cluster with Autoscaling\nDESCRIPTION: Command to create a Google Kubernetes Engine cluster with autoscaling enabled and basic compute resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngcloud container clusters create kuberay-gpu-cluster \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-east4-c --machine-type e2-standard-4\n```\n\n----------------------------------------\n\nTITLE: Recording Expert Data to Local Disk in RLlib with Python\nDESCRIPTION: This snippet outlines the steps required to load a trained PPO policy and record expert data during the evaluation phase. The configuration specifies the output path and ensures that recordings are stored in RLlib's episode format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.core import (\n    COMPONENT_LEARNER_GROUP,\n    COMPONENT_LEARNER,\n    COMPONENT_RL_MODULE,\n    DEFAULT_MODULE_ID,\n)\nfrom ray.rllib.core.rl_module import RLModuleSpec\n\n# Store recording data under the following path.\ndata_path = \"/tmp/docs_rllib_offline_recording\"\n\n# Configure the algorithm for recording.\nconfig = (\n    PPOConfig()\n    # The environment needs to be specified.\n    .environment(\n        env=\"CartPole-v1\",\n    )\n    # Make sure to sample complete episodes because\n    # you want to record RLlib's episode objects.\n    .env_runners(\n        batch_mode=\"complete_episodes\",\n    )\n    # Set up 5 evaluation `EnvRunners` for recording.\n    # Sample 50 episodes in each evaluation rollout.\n    .evaluation(\n        evaluation_num_env_runners=5,\n        evaluation_duration=50,\n        evaluation_duration_unit=\"episodes\",\n    )\n    # Use the checkpointed expert policy from the preceding PPO training.\n    # Note, we have to use the same `model_config` as\n    # the one with which the expert policy was trained, otherwise\n    # the module state can't be loaded.\n    .rl_module(\n        model_config=DefaultModelConfig(\n            fcnet_hiddens=[32],\n            fcnet_activation=\"linear\",\n            # Share encoder layers between value network\n            # and policy.\n            vf_share_layers=True,\n        ),\n    )\n    # Define the output path and format. In this example you\n    # want to store data directly in RLlib's episode objects.\n    # Each Parquet file should hold no more than 25 episodes.\n    .offline_data(\n        output=data_path,\n        output_write_episodes=True,\n        output_max_rows_per_file=25,\n    )\n)\n\n# Build the algorithm.\nalgo = config.build()\n# Load now the PPO-trained `RLModule` to use in recording.\nalgo.restore_from_path(\n    best_checkpoint,\n    # Load only the `RLModule` component here.\n    component=COMPONENT_RL_MODULE,\n)\n\n# Run 10 evaluation iterations and record the data.\nfor i in range(10):\n    print(f\"Iteration {i + 1}\")\n    eval_results = algo.evaluate()\n    print(eval_results)\n\n# Stop the algorithm. Note, this is important for when\n# defining `output_max_rows_per_file`. Otherwise,\n# remaining episodes in the `EnvRunner`s buffer isn't written to disk.\nalgo.stop()\n```\n\n----------------------------------------\n\nTITLE: Pinned Pillow Dependency with Hash Verification\nDESCRIPTION: Specifies Pillow version 10.3.0 with SHA256 hash verification for secure installation. Used by multiple dependencies including imageio, mistral-common, scikit-image, torchvision, and vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    --hash=sha256:0ea2a783a2bdf2a561808fe4a7a12e9aa3799b701ba305de596bc48b8bdfce9d \\\n    --hash=sha256:1530e8f3a4b965eb6a7785cf17a426c779333eb62c9a7d1bbcf3ffd5bf77a4aa \\\n    --hash=sha256:16563993329b79513f59142a6b02055e10514c1a8e86dca8b48a893e33cf91e3 \\\n    --hash=sha256:19aeb96d43902f0a783946a0a87dbdad5c84c936025b8419da0a0cd7724356b1 \\\n    --hash=sha256:1a1d1915db1a4fdb2754b9de292642a39a7fb28f1736699527bb649484fb966a \\\n    --hash=sha256:1b87bd9d81d179bd8ab871603bd80d8645729939f90b71e62914e816a76fc6bd \\\n    --hash=sha256:1dfc94946bc60ea375cc39cff0b8da6c7e5f8fcdc1d946beb8da5c216156ddd8 \\\n    --hash=sha256:2034f6759a722da3a3dbd91a81148cf884e91d1b747992ca288ab88c1de15999 \\\n    --hash=sha256:261ddb7ca91fcf71757979534fb4c128448b5b4c55cb6152d280312062f69599 \\\n    --hash=sha256:2ed854e716a89b1afcedea551cd85f2eb2a807613752ab997b9974aaa0d56936 \\\n    --hash=sha256:3102045a10945173d38336f6e71a8dc71bcaeed55c3123ad4af82c52807b9375 \\\n    --hash=sha256:339894035d0ede518b16073bdc2feef4c991ee991a29774b33e515f1d308e08d \\\n    --hash=sha256:412444afb8c4c7a6cc11a47dade32982439925537e483be7c0ae0cf96c4f6a0b \\\n    --hash=sha256:4203efca580f0dd6f882ca211f923168548f7ba334c189e9eab1178ab840bf60 \\\n    --hash=sha256:45ebc7b45406febf07fef35d856f0293a92e7417ae7933207e90bf9090b70572 \\\n    --hash=sha256:4b5ec25d8b17217d635f8935dbc1b9aa5907962fae29dff220f2659487891cd3 \\\n    --hash=sha256:4c8e73e99da7db1b4cad7f8d682cf6abad7844da39834c288fbfa394a47bbced \\\n    --hash=sha256:4e6f7d1c414191c1199f8996d3f2282b9ebea0945693fb67392c75a3a320941f \\\n    --hash=sha256:4eaa22f0d22b1a7e93ff0a596d57fdede2e550aecffb5a1ef1106aaece48e96b \\\n    --hash=sha256:50b8eae8f7334ec826d6eeffaeeb00e36b5e24aa0b9df322c247539714c6df19 \\\n    --hash=sha256:50fd3f6b26e3441ae07b7c979309638b72abc1a25da31a81a7fbd9495713ef4f \\\n    --hash=sha256:51243f1ed5161b9945011a7360e997729776f6e5d7005ba0c6879267d4c5139d \\\n    --hash=sha256:5d512aafa1d32efa014fa041d38868fda85028e3f930a96f85d49c7d8ddc0383 \\\n    --hash=sha256:5f77cf66e96ae734717d341c145c5949c63180842a545c47a0ce7ae52ca83795 \\\n    --hash=sha256:6b02471b72526ab8a18c39cb7967b72d194ec53c1fd0a70b050565a0f366d355 \\\n    --hash=sha256:6fb1b30043271ec92dc65f6d9f0b7a830c210b8a96423074b15c7bc999975f57 \\\n    --hash=sha256:7161ec49ef0800947dc5570f86568a7bb36fa97dd09e9827dc02b718c5643f09 \\\n    --hash=sha256:72d622d262e463dfb7595202d229f5f3ab4b852289a1cd09650362db23b9eb0b \\\n    --hash=sha256:74d28c17412d9caa1066f7a31df8403ec23d5268ba46cd0ad2c50fb82ae40462 \\\n    --hash=sha256:78618cdbccaa74d3f88d0ad6cb8ac3007f1a6fa5c6f19af64b55ca170bfa1edf \\\n    --hash=sha256:793b4e24db2e8742ca6423d3fde8396db336698c55cd34b660663ee9e45ed37f \\\n    --hash=sha256:798232c92e7665fe82ac085f9d8e8ca98826f8e27859d9a96b41d519ecd2e49a \\\n    --hash=sha256:81d09caa7b27ef4e61cb7d8fbf1714f5aec1c6b6c5270ee53504981e6e9121ad \\\n    --hash=sha256:8ab74c06ffdab957d7670c2a5a6e1a70181cd10b727cd788c4dd9005b6a8acd9 \\\n    --hash=sha256:8eb0908e954d093b02a543dc963984d6e99ad2b5e36503d8a0aaf040505f747d \\\n    --hash=sha256:90b9e29824800e90c84e4022dd5cc16eb2d9605ee13f05d47641eb183cd73d45 \\\n    --hash=sha256:9797a6c8fe16f25749b371c02e2ade0efb51155e767a971c61734b1bf6293994 \\\n    --hash=sha256:9d2455fbf44c914840c793e89aa82d0e1763a14253a000743719ae5946814b2d \\\n    --hash=sha256:9d3bea1c75f8c53ee4d505c3e67d8c158ad4df0d83170605b50b64025917f338 \\\n    --hash=sha256:9e2ec1e921fd07c7cda7962bad283acc2f2a9ccc1b971ee4b216b75fad6f0463 \\\n    --hash=sha256:9e91179a242bbc99be65e139e30690e081fe6cb91a8e77faf4c409653de39451 \\\n    --hash=sha256:a0eaa93d054751ee9964afa21c06247779b90440ca41d184aeb5d410f20ff591 \\\n    --hash=sha256:a2c405445c79c3f5a124573a051062300936b0281fee57637e706453e452746c \\\n    --hash=sha256:aa7e402ce11f0885305bfb6afb3434b3cd8f53b563ac065452d9d5654c7b86fd \\\n    --hash=sha256:aff76a55a8aa8364d25400a210a65ff59d0168e0b4285ba6bf2bd83cf675ba32 \\\n    --hash=sha256:b09b86b27a064c9624d0a6c54da01c1beaf5b6cadfa609cf63789b1d08a797b9 \\\n    --hash=sha256:b14f16f94cbc61215115b9b1236f9c18403c15dd3c52cf629072afa9d54c1cbf \\\n    --hash=sha256:b50811d664d392f02f7761621303eba9d1b056fb1868c8cdf4231279645c25f5 \\\n    --hash=sha256:b7bc2176354defba3edc2b9a777744462da2f8e921fbaf61e52acb95bafa9828 \\\n    --hash=sha256:c78e1b00a87ce43bb37642c0812315b411e856a905d58d597750eb79802aaaa3 \\\n    --hash=sha256:c83341b89884e2b2e55886e8fbbf37c3fa5efd6c8907124aeb72f285ae5696e5 \\\n    --hash=sha256:ca2870d5d10d8726a27396d3ca4cf7976cec0f3cb706debe88e3a5bd4610f7d2 \\\n    --hash=sha256:ccce24b7ad89adb5a1e34a6ba96ac2530046763912806ad4c247356a8f33a67b \\\n    --hash=sha256:cd5e14fbf22a87321b24c88669aad3a51ec052eb145315b3da3b7e3cc105b9a2 \\\n    --hash=sha256:ce49c67f4ea0609933d01c0731b34b8695a7a748d6c8d186f95e7d085d2fe475 \\\n    --hash=sha256:d33891be6df59d93df4d846640f0e46f1a807339f09e79a8040bc887bdcd7ed3 \\\n    --hash=sha256:d3b2348a78bc939b4fed6552abfd2e7988e0f81443ef3911a4b8498ca084f6eb \\\n    --hash=sha256:d886f5d353333b4771d21267c7ecc75b710f1a73d72d03ca06df49b09015a9ef \\\n    --hash=sha256:d93480005693d247f8346bc8ee28c72a2191bdf1f6b5db469c096c0c867ac015 \\\n    --hash=sha256:dc1a390a82755a8c26c9964d457d4c9cbec5405896cba94cf51f36ea0d855002 \\\n    --hash=sha256:dd78700f5788ae180b5ee8902c6aea5a5726bac7c364b202b4b3e3ba2d293170 \\\n    --hash=sha256:e46f38133e5a060d46bd630faa4d9fa0202377495df1f068a8299fd78c84de84 \\\n    --hash=sha256:e4b878386c4bf293578b48fc570b84ecfe477d3b77ba39a6e87150af77f40c57 \\\n    --hash=sha256:f0d0591a0aeaefdaf9a5e545e7485f89910c977087e7de2b6c388aec32011e9f \\\n    --hash=sha256:fdcbb4068117dfd9ce0138d068ac512843c52295ed996ae6dd1faf537b6dbc27 \\\n    --hash=sha256:ff61bfd9253c3915e6d41c651d5f962da23eda633cf02262990094a18a55371a\n```\n\n----------------------------------------\n\nTITLE: Kernel-level System Call for Socket Read Operation\nDESCRIPTION: This snippet shows the kernel-level system call chain for reading from a socket. It includes the transition from user space to kernel space and the various kernel functions involved in processing the read operation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_107\n\nLANGUAGE: kernel\nCODE:\n```\nread;system_call_fastpath_[k];sys_read_[k];vfs_read_[k];do_sync_read_[k];sock_aio_read_[k];sock_aio_read.part.13_[k];do_sock_read.isra.12_[k];inet_recvmsg_[k];tcp_recvmsg_[k];__kfree_skb_[k];skb_release_data_[k]\n```\n\n----------------------------------------\n\nTITLE: Downloading RayService Example Configuration\nDESCRIPTION: Command to download a sample RayService configuration yaml file for a text ML application from the KubeRay repository\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/kubernetes.md#2025-04-12_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ curl -o ray-service.text-ml.yaml https://raw.githubusercontent.com/ray-project/kuberay/5b1a5a11f5df76db2d66ed332ff0802dc3bbff76/ray-operator/config/samples/ray-service.text-ml.yaml\n```\n\n----------------------------------------\n\nTITLE: Including PBT Function in Ray\nDESCRIPTION: This snippet includes a literal example of a PBT function implemented in Python using Ray's tuning library. It showcases how the function can be utilized to optimize learning rates across multiple trials. Required dependencies include the Ray library and its tuning module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/pbt_function.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. literalinclude:: /../../python/ray/tune/examples/pbt_function.py\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Status Verification Commands\nDESCRIPTION: Series of kubectl commands to verify the status of RayService, RayCluster, and pods\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get rayservice\nkubectl get raycluster\nkubectl get pods -l=ray.io/is-ray-node=yes\nkubectl describe rayservices.ray.io rayservice-sample\nkubectl get services\n```\n\n----------------------------------------\n\nTITLE: Inspecting Ray Serve Actors using State API\nDESCRIPTION: This snippet demonstrates how to use the Ray State API to inspect actors in a Serve application, including listing all actors and filtering for specific actor types.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ ray summary actors\n\n======== Actors Summary: 2022-10-04 21:06:33.678706 ========\nStats:\n------------------------------------\ntotal_actors: 10\n\n\nTable (group by class):\n------------------------------------\n    CLASS_NAME              STATE_COUNTS\n0   ProxyActor          ALIVE: 3\n1   ServeReplica:SleepyPid  ALIVE: 6\n2   ServeController         ALIVE: 1\n\n$ ray list actors --filter \"class_name=ServeController\"\n\n======== List: 2022-10-04 21:09:14.915881 ========\nStats:\n------------------------------\nTotal: 1\n\nTable:\n------------------------------\n    ACTOR_ID                          CLASS_NAME       STATE    NAME                      PID\n 0  70a718c973c2ce9471d318f701000000  ServeController  ALIVE    SERVE_CONTROLLER_ACTOR  48570\n```\n\n----------------------------------------\n\nTITLE: Diagnosing RayService and Pod Status\nDESCRIPTION: Commands to inspect the RayService status and diagnose which worker Pods are ready or unready. This helps identify Pods that lack Ray Serve replicas.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# Step 4.1: Wait until the RayService is ready to serve requests.\nkubectl describe rayservices.ray.io rayservice-no-ray-serve-replica\n\n# Step 4.2: List all Ray Pods in the `default` namespace.\nkubectl get pods -l=ray.io/is-ray-node=yes\n\n# Step 4.3: Check unready worker pod events\nkubectl describe pods {YOUR_UNREADY_WORKER_POD_NAME}\n```\n\n----------------------------------------\n\nTITLE: Ray Security Policy Documentation in Markdown\nDESCRIPTION: Documents Ray's security policy, directing users to security documentation and providing contact information for vulnerability reporting\nSOURCE: https://github.com/ray-project/ray/blob/master/SECURITY.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Security Policy\n\nIf you are deploying Ray, read [Security](doc/source/ray-security/index.md).\n\n## Reporting a vulnerability\n\nPlease report security issues to `security@anyscale.com`.\n```\n\n----------------------------------------\n\nTITLE: Managing Long-running Jobs\nDESCRIPTION: Example demonstrating how to submit and stop a long-running job\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/sdk.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\njob_id = client.submit_job(\n    # Entrypoint shell command to execute\n    entrypoint=\"python -c 'import time; print(\\\"Sleeping...\\\"); time.sleep(60)'\"\n)\nwait_until_status(job_id, {JobStatus.RUNNING})\nprint(f'Stopping job {job_id}')\nclient.stop_job(job_id)\nwait_until_status(job_id, {JobStatus.SUCCEEDED, JobStatus.STOPPED, JobStatus.FAILED})\nlogs = client.get_job_logs(job_id)\nprint(logs)\n```\n\n----------------------------------------\n\nTITLE: Spark Submit Command for Long-Running Cluster\nDESCRIPTION: Bash command to submit the long-running Ray cluster application to a Spark standalone cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/spark.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nspark-submit \\\n  --master spark://{spark_master_IP}:{spark_master_port} \\\n  path/to/long-running-ray-cluster-on-spark.py\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries\nDESCRIPTION: Imports the required libraries for the Ray Tune experiment, including Ray, Nevergrad, and the Tune search algorithms. These libraries are essential for defining and running the hyperparameter optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"import time\\n\\nimport ray\\nimport nevergrad as ng\\nfrom ray import tune\\nfrom ray.tune.search import ConcurrencyLimiter\\nfrom ray.tune.search.nevergrad import NevergradSearch\"\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters\nDESCRIPTION: Set the neural network and training hyperparameters including hidden layer size, discount factor, and learning rate\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/plot_pong_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nH = 200  # The number of hidden layer neurons.\ngamma = 0.99  # The discount factor for reward.\ndecay_rate = 0.99  # The decay factor for RMSProp leaky sum of grad^2.\nD = 80 * 80  # The input dimensionality: 80x80 grid.\nlearning_rate = 1e-4  # Magnitude of the update.\n```\n\n----------------------------------------\n\nTITLE: Inspecting a Single Batch from Ray Data Dataset\nDESCRIPTION: Takes a single batch from the preprocessed dataset to inspect its structure.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/batch_inference_object_detection.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsingle_batch = ds.take_batch(batch_size=3)\ndisplay(single_batch)\n```\n\n----------------------------------------\n\nTITLE: Setting Up NGINX Ingress on Kind for Ray Cluster\nDESCRIPTION: Bash commands for creating a Kind cluster, installing NGINX ingress controller, deploying KubeRay operator, and configuring ingress for a Ray cluster on Kind.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Step 1: Create a Kind cluster with `extraPortMappings` and `node-labels`\n# Reference for the setting up of Kind cluster: https://kind.sigs.k8s.io/docs/user/ingress/\ncat <<EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n\n# Step 2: Install NGINX ingress controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\nsleep 10 # Wait for the Kubernetes API Server to create the related resources\nkubectl wait --namespace ingress-nginx \\\n  --for=condition=ready pod \\\n  --selector=app.kubernetes.io/component=controller \\\n  --timeout=90s\n\n# Step 3: Install KubeRay operator and CRD\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.3.0\n\n# Step 4: Install RayCluster and create an ingress separately.\n# More information about change of setting was documented in https://github.com/ray-project/kuberay/pull/699\n# and `ray-operator/config/samples/ray-cluster.separate-ingress.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-cluster.separate-ingress.yaml\nkubectl apply -f ray-operator/config/samples/ray-cluster.separate-ingress.yaml\n\n# Step 5: Check the ingress created in Step 4.\nkubectl describe ingress raycluster-ingress-head-ingress\n\n# Step 6: Check `<ip>/raycluster-ingress/` on your browser. You will see the Ray Dashboard.\n#        [Note] The forward slash at the end of the address is necessary. `<ip>/raycluster-ingress`\n#               will report \"404 Not Found\".\n```\n\n----------------------------------------\n\nTITLE: Setting RAY_RUNTIME_ENV_HOOK environment variable\nDESCRIPTION: This snippet sets the RAY_RUNTIME_ENV_HOOK environment variable, enabling the uv runtime environment hook. This is necessary to enable uv for package management in Ray runtime environments.  It is a temporary workaround until uv support becomes the default.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nexport RAY_RUNTIME_ENV_HOOK=ray._private.runtime_env.uv_runtime_env_hook.hook\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Llama Pre-training on Intel Gaudi\nDESCRIPTION: Imports necessary Python libraries and modules for dataset handling, model training, and Intel Gaudi-specific optimizations using the optimum-habana package.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python\n\nimport os\nfrom typing import Any, Dict\nfrom torch.utils.data import DataLoader\n\nimport transformers\nfrom itertools import chain\nfrom datasets import load_dataset\nfrom transformers import default_data_collator\nfrom transformers.testing_utils import CaptureLogger\nfrom optimum.habana import GaudiConfig, GaudiTrainer, GaudiTrainingArguments\nfrom optimum.habana.utils import set_seed\n```\n\n----------------------------------------\n\nTITLE: Merging Environment Variables\nDESCRIPTION: This snippet provides examples of how environment variable settings between different runtime environment specifications can be merged, with an example that leads to a conflict.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\"pip\": [\"requests\", \"chess\"], \"env_vars\": {\"A\": \"a\", \"B\": \"b\"}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Dashboard Port in VM Cluster Launcher YAML\nDESCRIPTION: This YAML snippet shows how to configure the Ray Dashboard port in the head_start_ray_commands section of the Cluster Launcher's YAML file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/configure-manage-dashboard.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nhead_start_ray_commands:\n  - ray stop\n  # Replace ${YOUR_PORT} with the port number you need.\n  - ulimit -n 65536; ray start --head --dashboard-port=${YOUR_PORT} --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating and Retrieving Named Actors in Java\nDESCRIPTION: Shows how to create an actor with a name and retrieve it later using Ray in Java. This allows accessing the actor from any job in the Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/named-actors.rst#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Create an actor with a name.\nActorHandle<Counter> counter = Ray.actor(Counter::new).setName(\"some_name\").remote();\n\n...\n\n// Retrieve the actor later somewhere\nOptional<ActorHandle<Counter>> counter = Ray.getActor(\"some_name\");\nAssert.assertTrue(counter.isPresent());\n```\n\n----------------------------------------\n\nTITLE: Installing Ray from a specific commit\nDESCRIPTION: Template and example for installing Ray wheels from a specific commit on the master branch. Requires specifying the commit hash, Ray version, OS, and Python version.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install https://s3-us-west-2.amazonaws.com/ray-wheels/master/{COMMIT_HASH}/ray-{RAY_VERSION}-{PYTHON_VERSION}-{PYTHON_VERSION}-{OS_VERSION}.whl\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Hugging Face Authentication\nDESCRIPTION: Creates a Kubernetes Secret containing a Hugging Face access token. This token is required to download gated language models from Hugging Face Hub.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/vllm-rayservice.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport HF_TOKEN=<Hugging Face access token>\nkubectl create secret generic hf-secret   --from-literal=hf_api_token=${HF_TOKEN}   --dry-run=client -o yaml | kubectl apply -f -\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Services in Skein YAML\nDESCRIPTION: YAML configuration for defining Ray head and worker services in Skein, specifying resource allocations and instance counts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/yarn.rst#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n    ray-head:\n        # There should only be one instance of the head node per cluster.\n        instances: 1\n        resources:\n            # The resources for the worker node.\n            vcores: 1\n            memory: 2048\n        files:\n            ...\n        script:\n            ...\n    ray-worker:\n        # Number of ray worker nodes to start initially.\n        # This can be scaled using 'skein container scale'.\n        instances: 3\n        resources:\n            # The resources for the worker node.\n            vcores: 1\n            memory: 2048\n        files:\n            ...\n        script:\n            ...\n```\n\n----------------------------------------\n\nTITLE: Logging with TensorBoard in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to log experiments using TensorBoard within PyTorch Lightning, using Ray Train's TorchTrainer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. dropdown:: TensorBoard\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking/lightning_exp_tracking_model_dl.py\n        :language: python\n        :start-after: __model_dl_start__\n\n    .. literalinclude:: ../../../../python/ray/train/examples/experiment_tracking/lightning_exp_tracking_tensorboard.py\n        :language: python\n        :start-after: __lightning_experiment_tracking_tensorboard_start__\n        :end-before: __lightning_experiment_tracking_tensorboard_end__\n```\n\n----------------------------------------\n\nTITLE: Implementing Strategy Class for Bank Calculations\nDESCRIPTION: Example Strategy class that calculates financial indicators for different banks using nested loops. This represents the initial implementation before conversion to Ray Serve.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/java.md#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass Strategy {\n    public Map<String, Map<String, Double>> calc(long time, Map<String, List<String>> bankIndicators) {\n        Map<String, Map<String, Double>> result = new HashMap<>();\n        for (Map.Entry<String, List<String>> entry : bankIndicators.entrySet()) {\n            result.put(entry.getKey(), calcBankIndicators(time, entry.getKey(), entry.getValue()));\n        }\n        return result;\n    }\n\n    private Map<String, Double> calcBankIndicators(long time, String bank, List<String> indicators) {\n        Map<String, Double> result = new HashMap<>();\n        for (String indicator : indicators) {\n            result.put(indicator, calcIndicator(time, bank, indicator));\n        }\n        return result;\n    }\n\n    private double calcIndicator(long time, String bank, String indicator) {\n        return Math.random();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Deployed LLM using OpenAI Python Client\nDESCRIPTION: Example of making a streaming chat completion request to the deployed LLM model using OpenAI Python client\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Initialize client\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n\n# Basic chat completion with streaming\nresponse = client.chat.completions.create(\n    model=\"qwen-0.5b\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    stream=True\n)\n\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Creating a RayCluster with kubectl\nDESCRIPTION: Command to apply a RayCluster custom resource using kubectl to deploy a Ray cluster on Kubernetes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/config.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f raycluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Inspecting Serve Configuration using CLI\nDESCRIPTION: This snippet demonstrates how to retrieve the Serve application's configuration using the `serve config` command. It first starts a Ray head node, deploys a Serve application using a YAML configuration file (`serve_config.yaml`), and then retrieves the configuration using `serve config`. The output shows the application's name, route prefix, import path, runtime environment, and deployment details.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"$ ray start --head\n$ serve deploy serve_config.yaml\n...\n\n$ serve config\nname: default\nroute_prefix: /\nimport_path: text_ml:app\nruntime_env:\n  pip:\n    - torch\n    - transformers\ndeployments:\n- name: Translator\n  num_replicas: 1\n  user_config:\n    language: french\n- name: Summarizer\n  num_replicas: 1\"\n```\n\n----------------------------------------\n\nTITLE: Launching TensorBoard to Visualize Ray Tune Results\nDESCRIPTION: Command to start TensorBoard and point it to the Ray Tune experiment output directory for visualization of training metrics and results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ tensorboard --logdir=~/ray_results/my_experiment\n```\n\n----------------------------------------\n\nTITLE: Creating a Second RayCluster with YuniKorn\nDESCRIPTION: Command to create a second RayCluster with the same configuration but a different name, using sed to modify the existing YAML file and apply it to Kubernetes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n# Replace the name with `test-yunikorn-1`.\nsed 's/test-yunikorn-0/test-yunikorn-1/' ray-cluster.yunikorn-scheduler.yaml | kubectl apply -f-\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Cluster with Kind\nDESCRIPTION: Shell command to create a Kubernetes cluster using Kind with a specific node version.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n----------------------------------------\n\nTITLE: Starting Kiali Dashboard for Istio Visualization\nDESCRIPTION: This command launches the Kiali dashboard, which provides a visual interface for observing Istio service mesh traffic and configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/istio.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nistioctl dashboard kiali\n```\n\n----------------------------------------\n\nTITLE: Main Execution Block for Ray Tune Experiment\nDESCRIPTION: This is the main execution block that initializes Ray and runs the tune_cifar function to start the distributed training and hyperparameter tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/tune_cifar_torch_pbt_example.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    ray.init()\n    tune_cifar(num_workers=2, use_gpu=False)\n```\n\n----------------------------------------\n\nTITLE: Installing Ray with Minimal Dependencies using Conda\nDESCRIPTION: Installs Ray with minimal dependencies from conda-forge.  This provides a smaller installation footprint if only the core Ray functionality is needed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n\"# conda install -c conda-forge ray\"\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Identifier Length in Ray Tune\nDESCRIPTION: TUNE_MAX_LEN_IDENTIFIER sets the maximum length allowed for trial subdirectory names that contain parameter values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_MAX_LEN_IDENTIFIER=100\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Ray Serve in Kubernetes\nDESCRIPTION: Shell commands to check the RayService status, get service information, and set up port forwarding to access the Ray Serve endpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/text-summarizer-rayservice.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# Step 4.1: Wait until the RayService is ready to serve requests.\nkubectl describe rayservices text-summarizer\n\n# Step 4.2: Get the service name.\nkubectl get services\n\n# [Example output]\n# text-summarizer-head-svc                    ClusterIP   None             <none>        10001/TCP,8265/TCP,6379/TCP,8080/TCP,8000/TCP   31s\n# text-summarizer-raycluster-tb9zf-head-svc   ClusterIP   None             <none>        10001/TCP,8265/TCP,6379/TCP,8080/TCP,8000/TCP   108s\n# text-summarizer-serve-svc                   ClusterIP   34.118.226.139   <none>        8000/TCP                                        31s\n\n# Step 4.3: Forward the port of Serve.\nkubectl port-forward svc/text-summarizer-serve-svc 8000\n```\n\n----------------------------------------\n\nTITLE: Setting Ray Job Headers\nDESCRIPTION: Environment variable setup for adding custom headers to Ray job HTTP requests.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/quickstart.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport RAY_JOB_HEADERS='{\"KEY\": \"VALUE\"}'\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray on a Single Machine\nDESCRIPTION: Basic command to initialize Ray on a single machine. This is the simplest way to use Ray before scaling to a multi-node cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/getting-started.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nray.init\n```\n\n----------------------------------------\n\nTITLE: Importing Local Modules Across Ray Workers with py_modules\nDESCRIPTION: This code demonstrates how to use the py_modules field in runtime_env to make local modules available to all Ray workers. The example shows connecting to a Ray cluster and ensuring a local module 'my_module' is accessible within remote functions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport my_module\n\nray.init(\"ray://123.456.7.89:10001\", runtime_env={\"py_modules\": [my_module]})\n\n@ray.remote\ndef test_my_module():\n    # No need to import my_module inside this function.\n    my_module.test()\n\nray.get(test_my_module.remote())\n```\n\n----------------------------------------\n\nTITLE: Defining gitdb and gitpython Packages with Cryptographic Hashes in requirements.txt\nDESCRIPTION: This snippet defines the gitdb and gitpython package dependencies with specific versions and SHA256 hashes. It shows their dependency relationship where gitpython requires gitdb, and indicates which requirement files reference these packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\ngitdb==4.0.11 \\\n    --hash=sha256:81a3407ddd2ee8df444cbacea00e2d038e40150acfa3001696fe0dcf1d3adfa4 \\\n    --hash=sha256:bf5421126136d6d0af55bc1e7c1af1c397a34f5b7bd79e776cd3e89785c2b04b\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   gitpython\ngitpython==3.1.40 \\\n    --hash=sha256:22b126e9ffb671fdd0c129796343a02bf67bf2994b35449ffc9321aa755e18a4 \\\n    --hash=sha256:cf14627d5a8049ffbf49915732e5eddbe8134c3bdb9d476e6182b676fc573f8a\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Processing Netty NIO Events and TCP Operations in Ray\nDESCRIPTION: This stack trace shows the execution path of a Netty NIO event being processed, including channel reading, flushing, and writing operations. It extends into kernel-level TCP and IP operations, demonstrating the full stack of network communication in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_20\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];tcp_v4_do_rcv_[k];tcp_rcv_established_[k];tcp_ack_[k];tcp_clean_rtx_queue_[k];bictcp_acked_[k]\n```\n\n----------------------------------------\n\nTITLE: Submitting Ray Jobs with kubectl-ray\nDESCRIPTION: Commands to submit Ray jobs using the kubectl-ray plugin, both with and without a YAML file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kubectl-plugin.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl ray job submit --name rayjob-sample --working-dir . -- python sample_code.py\nwget https://raw.githubusercontent.com/ray-project/kuberay/refs/heads/master/ray-operator/config/samples/ray-job.interactive-mode.yaml\nkubectl ray job submit -f ray-job.interactive-mode.yaml --working-dir . -- python sample_code.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Frozen VM for Ray Cluster on vSphere\nDESCRIPTION: Example YAML configuration for setting up a frozen VM to be deployed from an OVF template in a Ray cluster on vSphere. This configuration specifies the name of the frozen VM and the library item containing the OVF template.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nfrozen_vm:\n    name: single-frozen-vm\n    library_item: frozen-vm-template\n```\n\n----------------------------------------\n\nTITLE: Starting Training Process\nDESCRIPTION: This Python snippet calls the `train_bert` function to initiate the training process. The number of workers can be adjusted based on the machine configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/bert.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_bert(num_workers=2)\n```\n\n----------------------------------------\n\nTITLE: Downloading Hymenoptera Dataset for ResNet Training\nDESCRIPTION: Function to download and extract the Hymenoptera dataset containing ant and bee images for binary classification.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef download_datasets():\n    os.system(\"wget https://download.pytorch.org/tutorial/hymenoptera_data.zip >/dev/null 2>&1\")\n    os.system(\"unzip hymenoptera_data.zip >/dev/null 2>&1\")\n```\n\n----------------------------------------\n\nTITLE: AWS SDK Installation and Credentials Configuration\nDESCRIPTION: Commands to install AWS Python SDK (boto3) and configure AWS credentials using either environment variables or credentials file\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# install AWS Python SDK (boto3)\npip install -U boto3\n\n# setup AWS credentials using environment variables\nexport AWS_ACCESS_KEY_ID=foo\nexport AWS_SECRET_ACCESS_KEY=bar\nexport AWS_SESSION_TOKEN=baz\n\n# alternatively, you can setup AWS credentials using ~/.aws/credentials file\necho \"[default]\naws_access_key_id=foo\naws_secret_access_key=bar\naws_session_token=baz\" >> ~/.aws/credentials\n```\n\n----------------------------------------\n\nTITLE: Starting Ray with Debugger in Bash\nDESCRIPTION: This snippet demonstrates how to start a Ray process (raylet) in gdb and tmux for debugging purposes. It sets environment variables to enable gdb and tmux for the raylet process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/debugging.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nRAY_RAYLET_GDB=1 RAY_RAYLET_TMUX=1 python\n```\n\n----------------------------------------\n\nTITLE: Example of Python API with Forced Kwargs\nDESCRIPTION: This Python function demonstrates a method of enforcing keyword arguments after the '*' operator, which helps maintain backwards compatibility for API calls. The function defines two optional parameters with default values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef foo_bar(file, *, opt1=x, opt2=y)\n    pass\n```\n\n----------------------------------------\n\nTITLE: Listing Ray Actors with CLI Command\nDESCRIPTION: Shows how to list all the Actors from a Ray Cluster using the Ray CLI command. This command displays Actor IDs, class names, names, PIDs, and states.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/key-concepts.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nray list actors\n```\n\n----------------------------------------\n\nTITLE: Defining Nested Hyperparameter Search Spaces in Python\nDESCRIPTION: This code snippet demonstrates how to define nested hyperparameter search spaces using dictionaries in Ray Tune. It shows the structure for specifying hyperparameters for different layers of a neural network.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"alpha\": tune.uniform(0.0, 1.0),\n    \"beta\": tune.sample_from(lambda _: np.random.randint(1, 5)),\n    \"nn_layers\": {\n        \"n\": tune.choice([1, 2, 3]),\n        \"n_units\": tune.choice([64, 128, 256]),\n        \"activation\": tune.choice([\"relu\", \"tanh\"])\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Ray Mode Configuration Example\nDESCRIPTION: YAML configuration for enabling Ray distributed execution mode in Data-Juicer with basic settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/data_juicer_distributed_data_processing.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\ndataset_path: './demos/process_on_ray/data/demo-dataset.jsonl'\nexport_path: './outputs/demo/demo-processed'\n\nexecutor_type: 'ray'  # Set the executor type to \"ray\"\nray_address: 'auto'  # Set an automatic Ray address\n...\n```\n\n----------------------------------------\n\nTITLE: Parametric Actions Custom Model (TFModelV2) in RLlib\nDESCRIPTION: This TFModelV2 example shows how to create a custom model that can handle environments with variable-length or parametric action spaces. The model uses action masks and embeddings to compute action logits, enabling it to work with algorithms in the DQN and policy-gradient families.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass MyParamActionEnv(gym.Env):\n    def __init__(self, max_avail_actions):\n        self.action_space = Discrete(max_avail_actions)\n        self.observation_space = Dict({\n            \"action_mask\": Box(0, 1, shape=(max_avail_actions, )),\n            \"avail_actions\": Box(-1, 1, shape=(max_avail_actions, action_embedding_sz)),\n            \"real_obs\": ...,\n        })\n\n```\n\nLANGUAGE: python\nCODE:\n```\nclass ParametricActionsModel(TFModelV2):\n    def __init__(self, obs_space,\n                 action_space,\n                 num_outputs,\n                 model_config,\n                 name,\n                 true_obs_shape=(4,),\n                 action_embed_size=2):\n        super(ParametricActionsModel, self).__init__(\n            obs_space, action_space, num_outputs, model_config, name)\n        self.action_embed_model = FullyConnectedNetwork(...)\n    def forward(self, input_dict, state, seq_lens):\n        # Extract the available actions tensor from the observation.\n        avail_actions = input_dict[\"obs\"][\"avail_actions\"]\n        action_mask = input_dict[\"obs\"][\"action_mask\"]\n        # Compute the predicted action embedding\n        action_embed, _ = self.action_embed_model({\n            \"obs\": input_dict[\"obs\"][\"cart\"]\n        })\n        # Expand the model output to [BATCH, 1, EMBED_SIZE]. Note that the\n        # avail actions tensor is of shape [BATCH, MAX_ACTIONS, EMBED_SIZE].\n        intent_vector = tf.expand_dims(action_embed, 1)\n        # Batch dot product => shape of logits is [BATCH, MAX_ACTIONS].\n        action_logits = tf.reduce_sum(avail_actions * intent_vector, axis=2)\n        # Mask out invalid actions (use tf.float32.min for stability)\n        inf_mask = tf.maximum(tf.log(action_mask), tf.float32.min)\n        return action_logits + inf_mask, state\n\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for GKE Cluster Access\nDESCRIPTION: This command downloads Google Cloud credentials and configures the Kubernetes CLI (kubectl) to use them for accessing the created GKE cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container clusters get-credentials kuberay-gpu-cluster --zone us-west1-b\n```\n\n----------------------------------------\n\nTITLE: Sending HTTP Requests via curl\nDESCRIPTION: Command to send HTTP PUT requests using curl to the Ray Serve application running locally. This is useful for testing deployed services using simple HTTP calls.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dev-workflow.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n\"curl -X PUT \\\"http://localhost:8000/?name=Ray\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for SSH Access to Ray Cluster\nDESCRIPTION: Kubectl command to set up port forwarding from local port 2222 to the SSH port (22) on the Ray cluster head service, enabling remote SSH connection for debugging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/ray-distributed-debugger.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward service/raycluster-sample-head-svc 2222:22\n```\n\n----------------------------------------\n\nTITLE: Pretraining a Single-Agent Policy for Multi-Agent Training\nDESCRIPTION: Demonstrates the approach of pretraining a single-agent model followed by training it in a multi-agent environment, useful for initializing complex scenarios.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Pretrain single-agent policy, then train in multi-agent Env\n# This script showcases the transfer of single-agent pretraining to multi-agent environments.\n```\n\n```\n\n----------------------------------------\n\nTITLE: List RayClusters\nDESCRIPTION: Lists all RayCluster custom resources in the default namespace to verify deletion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nkubectl get raycluster\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Cluster with Kind\nDESCRIPTION: Creates a Kubernetes cluster using Kind with version 1.26.0 as the foundation for the RayService high availability setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-high-availability.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n----------------------------------------\n\nTITLE: Deploying RayCluster with Helm\nDESCRIPTION: Deploys a RayCluster using Helm chart with specified version\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nhelm install raycluster kuberay/ray-cluster --version 1.3.0\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Service Account\nDESCRIPTION: Creates a Kubernetes service account that will be linked to the Google Cloud IAM service account.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create serviceaccount my-ksa\n```\n\n----------------------------------------\n\nTITLE: Specifying S3FS Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the S3FS package dependency with version 2023.5.0 and SHA-256 hashes for verification. The comments indicate this package is required by the release/ray_release/byod/requirements_compiled.txt and requirements_byod_3.9.in files.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_34\n\nLANGUAGE: pip\nCODE:\n```\ns3fs==2023.5.0 \\\n    --hash=sha256:0d82c4fa43d1214117f56b239c3e03c9a2886f41c31000c1c967ac6030d20362 \\\n    --hash=sha256:106b5d9a1000e6af413f918156ba4b96789ac832b7e08c99d186eb08164e6981\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n```\n\n----------------------------------------\n\nTITLE: JavaScript Object Property Access in Netty/Vert.x Request Processing\nDESCRIPTION: This stack trace extends the previous one, showing the process of accessing a property on a JavaScript object during the execution of server-side JavaScript code in response to an HTTP request.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_76\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.createFunctionActivation_[j];org/mozilla/javascript/IdScriptableObject:.get_[j]\n```\n\n----------------------------------------\n\nTITLE: Building for Production\nDESCRIPTION: Creates an optimized production build in the build folder with minified bundles and hashed filenames.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/README.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Operator\nDESCRIPTION: Command to install the KubeRay operator using a utility script.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n../scripts/doctest-utils.sh install_kuberay_operator\n```\n\n----------------------------------------\n\nTITLE: Query Task Attempts CLI Command\nDESCRIPTION: CLI command to view task attempt failures and retries for a specific task ID.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/tasks.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nray list tasks -f task_id=16310a0f0a45af5cffffffffffffffffffffffff01000000\n```\n\n----------------------------------------\n\nTITLE: Inspecting Custom Resource Status\nDESCRIPTION: Command to examine the status and events of Ray-related custom resources in Kubernetes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/observability.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe [raycluster|rayjob|rayservice] $CUSTOM_RESOURCE_NAME -n $YOUR_NAMESPACE\n```\n\n----------------------------------------\n\nTITLE: Example RLlink EPISODES_AND_GET_STATE Message\nDESCRIPTION: Demonstrates an EPISODES_AND_GET_STATE message that sends a batch of sampling data to the server and requests updated model weights. The message includes episode observations, actions, rewards, and termination status.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/external-envs.rst#2025-04-12_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"type\": \"EPISODES_AND_GET_STATE\",\n  \"episodes\": [\n    {\n      \"obs\": [[...]],  // List of observations\n      \"actions\": [...],  // List of actions\n      \"rewards\": [...],  // List of rewards\n      \"is_terminated\": false,\n      \"is_truncated\": false\n    }\n  ],\n  \"env_steps\": 128\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating  Estimate using Ray Results\nDESCRIPTION: Retrieves results from all sampling tasks, calculates the final estimate of , and prints the result.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/monte_carlo_pi.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntotal_num_inside = sum(ray.get(results))\npi_estimate = 4 * total_num_inside / num_samples\nprint(f\"Estimated value of  is: {pi_estimate}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Storage for Distributed Tune Experiment (Python)\nDESCRIPTION: Python code showing how to configure cloud storage (S3 in this example) for checkpointing in a distributed Tune experiment using RunConfig.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom my_module import my_trainable\n\ntuner = tune.Tuner(\n    my_trainable,\n    run_config=tune.RunConfig(\n        name=\"experiment_name\",\n        storage_path=\"s3://bucket-name/sub-path/\",\n    )\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Verifying EKS Node Groups Configuration\nDESCRIPTION: Command to verify the status and configuration of CPU and GPU node groups in the EKS cluster using eksctl. Shows detailed information about node groups including instance types and capacity.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\neksctl get nodegroup --cluster ${YOUR_EKS_NAME}\n\n# CLUSTER         NODEGROUP       STATUS  CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID                        ASG NAME                           TYPE\n# ${YOUR_EKS_NAME}     cpu-node-group  ACTIVE  2023-06-05T21:31:49Z    0               1               1                       m5.xlarge       AL2_x86_64                      eks-cpu-node-group-...     managed\n# ${YOUR_EKS_NAME}     gpu-node-group  ACTIVE  2023-06-05T22:01:44Z    0               1               1                       g5.12xlarge     BOTTLEROCKET_x86_64_NVIDIA      eks-gpu-node-group-...     managed\n```\n\n----------------------------------------\n\nTITLE: Creating a Table of Contents in Markdown for Ray Observability Documentation\nDESCRIPTION: A markdown toctree directive that organizes the observability documentation sections, including debugging applications, CLI SDK, logging configuration, profiling, application metrics, and Ray tracing. The toctree is hidden in the rendered output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/index.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n\nDebugging Applications <debug-apps/index>\ncli-sdk\nconfigure-logging\nprofiling\nadd-app-metrics\nray-tracing\n```\n```\n\n----------------------------------------\n\nTITLE: Launching Ray Cluster - Bash\nDESCRIPTION: This Bash command snippet shows how to initiate a Ray cluster by using the `ray up` command, effectively setting up the infrastructure for distributed Ray applications to run.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nRAY_ADDRESS=<address> ./<binary> <args>\n```\n\n----------------------------------------\n\nTITLE: Configuring Kueue Resources for Gang Scheduling\nDESCRIPTION: YAML configuration defining Kueue resources including ResourceFlavor, AdmissionCheck, ProvisioningRequestConfig, ClusterQueue, and LocalQueue for gang scheduling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"default-flavor\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: AdmissionCheck\nmetadata:\n  name: rayjob-gpu\nspec:\n  controllerName: kueue.x-k8s.io/provisioning-request\n  parameters:\n    apiGroup: kueue.x-k8s.io\n    kind: ProvisioningRequestConfig\n    name: rayjob-gpu-config\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ProvisioningRequestConfig\nmetadata:\n  name: rayjob-gpu-config\nspec:\n  provisioningClassName: queued-provisioning.gke.io\n  managedResources:\n  - nvidia.com/gpu\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue\"\nspec:\n  namespaceSelector: {} # match all\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"nvidia.com/gpu\"]\n    flavors:\n    - name: \"default-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 10000 # infinite quotas\n      - name: \"memory\"\n        nominalQuota: 10000Gi # infinite quotas\n      - name: \"nvidia.com/gpu\"\n        nominalQuota: 10000 # infinite quotas\n  admissionChecks:\n  - rayjob-gpu\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: \"default\"\n  name: \"user-queue\"\nspec:\n  clusterQueue: \"cluster-queue\"\n```\n\n----------------------------------------\n\nTITLE: Specifying protobuf Dependency with Hash Verification\nDESCRIPTION: Defines the protobuf package dependency at version 3.20.3 with cryptographic hash verification for package integrity. This pinned version ensures consistent behavior across development environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_30\n\nLANGUAGE: plaintext\nCODE:\n```\nprotobuf==3.20.3 \\\n    --hash=sha256:03038ac1cfbc41aa21f6afcbcd357281d7521b4157926f30ebecc8d4ea59dcb7 \\\n    --hash=sha256:28545383d61f55b57cf4df63eebd9827754fd2dc25f80c5253f9184235db242c \\\n    --hash=sha256:2e3427429c9cffebf259491be0af70189607f365c2f41c7c3764af6f337105f2 \\\n    --hash=sha256:398a9e0c3eaceb34ec1aee71894ca3299605fa8e761544934378bbc6c97de23b \\\n    --hash=sha256:44246bab5dd4b7fbd3c0c80b6f16686808fab0e4aca819ade6e8d294a29c7050 \\\n    --hash=sha256:447d43819997825d4e71bf5769d869b968ce96848b6479397e29fc24c4a5dfe9 \\\n    --hash=sha256:67a3598f0a2dcbc58d02dd1928544e7d88f764b47d4a286202913f0b2801c2e7 \\\n    --hash=sha256:74480f79a023f90dc6e18febbf7b8bac7508420f2006fabd512013c0c238f454 \\\n    --hash=sha256:819559cafa1a373b7096a482b504ae8a857c89593cf3a25af743ac9ecbd23480 \\\n    --hash=sha256:899dc660cd599d7352d6f10d83c95df430a38b410c1b66b407a6b29265d66469 \\\n    --hash=sha256:8c0c984a1b8fef4086329ff8dd19ac77576b384079247c770f29cc8ce3afa06c \\\n    --hash=sha256:9aae4406ea63d825636cc11ffb34ad3379335803216ee3a856787bcf5ccc751e \\\n    --hash=sha256:a7ca6d488aa8ff7f329d4c545b2dbad8ac31464f1d8b1c87ad1346717731e4db \\\n    --hash=sha256:b6cc7ba72a8850621bfec987cb72623e703b7fe2b9127a161ce61e61558ad905 \\\n    --hash=sha256:bf01b5720be110540be4286e791db73f84a2b721072a3711efff6c324cdf074b \\\n    --hash=sha256:c02ce36ec760252242a33967d51c289fd0e1c0e6e5cc9397e2279177716add86 \\\n    --hash=sha256:d9e4432ff660d67d775c66ac42a67cf2453c27cb4d738fc22cb53b5d84c135d4\n```\n\n----------------------------------------\n\nTITLE: Configuring Progress Bar Truncation in Python\nDESCRIPTION: Code example showing how to modify the progress bar name truncation threshold in Ray Data by updating the MAX_NAME_LENGTH constant.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/monitoring-your-workload.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nray.data._internal.progress_bar.ProgressBar.MAX_NAME_LENGTH = 42\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Cluster\nDESCRIPTION: Connects to an existing Ray cluster using auto-detection of the cluster address.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/highly_parallel.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nray.init(address='auto')\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with Mozilla JavaScript Context Access\nDESCRIPTION: This stack trace shows Netty's event processing flow with JavaScript engine integration. The trace illustrates how Vert.x handlers interact with Mozilla's Rhino JavaScript engine, accessing the WrapFactory during message processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_46\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/mozilla/javascript/Context:.getWrapFactory_[j] 2\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Ray in Python\nDESCRIPTION: Terminates the Ray environment, ensuring that all Ray processes are properly closed. It is necessary to perform cleanup after experiments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Converting Ray Dataset to Mars DataFrame in Python\nDESCRIPTION: This snippet demonstrates how to convert a Ray Dataset to a Mars DataFrame. It reads CSV data from an S3 bucket and uses the to_mars() method for conversion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/saving-data.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\nmdf = ds.to_mars()\n```\n\n----------------------------------------\n\nTITLE: Accessing Checkpoint from Training Results Python\nDESCRIPTION: Retrieves the last checkpoint from the training results obtained using Ray Train. This checkpoint can be utilized to resume training or for inference tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint = results.checkpoint\ncheckpoint\n```\n\n----------------------------------------\n\nTITLE: Inspecting GCS Bucket Contents\nDESCRIPTION: This command uses gsutil to list the contents of the GCS bucket after the RayJob completes, showing the checkpoint files and other training artifacts stored in the remote filesystem.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ngsutil ls gs://my-ray-bucket/**\n```\n\n----------------------------------------\n\nTITLE: Basic Ray Hello World Script\nDESCRIPTION: Sample Python script demonstrating basic Ray task execution with automatic cluster connection.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/quickstart.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# script.py\nimport ray\n\n@ray.remote\ndef hello_world():\n    return \"hello world\"\n\n# Automatically connect to the running Ray cluster.\nray.init()\nprint(ray.get(hello_world.remote()))\n```\n\n----------------------------------------\n\nTITLE: Setting Framework to Torch\nDESCRIPTION: This snippet demonstrates how to set the deep learning framework to PyTorch using the `framework()` method in the `AlgorithmConfig` object. It also illustrates removing TensorFlow-specific settings, as the new API stack primarily supports PyTorch.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"# Make sure you always set the framework to \\\"torch\\\"...\nconfig.framework(\\\"torch\\\")\n\n# ... and drop all tf-specific settings.\nconfig.framework(\n    eager_tracing=True,\n    eager_max_retraces=20,\n    tf_session_args={},\n    local_tf_session_args={},\n)\"\n```\n\n----------------------------------------\n\nTITLE: Recording Expert Policy Data in Tabular Format\nDESCRIPTION: Configuration setup for recording expert policy experiences in tabular format using PPO, including environment setup and evaluation parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.core import (\n    COMPONENT_LEARNER_GROUP,\n    COMPONENT_LEARNER,\n    COMPONENT_RL_MODULE,\n    DEFAULT_MODULE_ID,\n)\nfrom ray.rllib.core.rl_module import RLModuleSpec\n\ntabular_data_path = \"tmp/docs_rllib_offline_recording_tabular\"\n\nconfig = (\n    PPOConfig()\n    .environment(\n        env=\"CartPole-v1\",\n    )\n    .env_runners(\n        batch_mode=\"complete_episodes\",\n    )\n    .evaluation(\n        evaluation_num_env_runners=5,\n        evaluation_duration=50,\n    )\n    .rl_module(\n        model_config=DefaultModelConfig(\n            fcnet_hiddens=[32],\n        )\n```\n\n----------------------------------------\n\nTITLE: Demonstrating GAN Results with the Best Model\nDESCRIPTION: Loads the best GAN model from its checkpoint and demonstrates the generated images using a utility function from the example.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.examples.pbt_dcgan_mnist.common import demo_gan\n\nwith best_result.checkpoint.as_directory() as best_checkpoint:\n    demo_gan([best_checkpoint])\n```\n\n----------------------------------------\n\nTITLE: Configuring PyArrow GCS Filesystem in RLlib\nDESCRIPTION: Configuration for using PyArrow filesystem with Google Cloud Storage, including authentication settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.fs\n\n# Define the PyArrow filesystem\ngcs = pyarrow.fs.GcsFilesystem(\n    # This is needed to resolve the hostname for public buckets.\n    anonymous=True,\n    retry_time_limit=timedelta(seconds=15)\n)\n\n# Define the configuration.\nconfig= (\n    AlgorithmConfig()\n    .offline_data(\n        # NOTE: Use a relative file path now\n        input_=\"<public-bucket>/dir1\",\n        input_filesystem=gcs,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting a RayCluster to Free YuniKorn Queue Resources\nDESCRIPTION: Command to delete the first RayCluster to free up resources in the YuniKorn queue, allowing the second RayCluster to be scheduled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete raycluster test-yunikorn-0\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Compiled Graph Dependencies\nDESCRIPTION: Instructions for installing Ray with Compiled Graph support using pip.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/quickstart.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[cgraph]\"\n\n# For a ray version before 2.41, use the following instead:\n# pip install \"ray[adag]\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Latest CloudWatch Agent AMI (Bash)\nDESCRIPTION: This Bash command uses the AWS CLI to find the latest available Unified CloudWatch Agent Image for the us-west-2 region.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\naws ec2 describe-images --region us-west-2 --filters \"Name=owner-id,Values=160082703681\" \"Name=name,Values=*cloudwatch*\" --query 'Images[*].[ImageId,CreationDate]' --output text | sort -k2 -r | head -n1\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Head Node\nDESCRIPTION: Command to initialize the Ray head node with memory configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/yarn.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nray start --head --port=6379 --object-store-memory=200000000 --memory 200000000 --num-cpus=1\n```\n\n----------------------------------------\n\nTITLE: Setting Optimization Parameters for Tune Run\nDESCRIPTION: Defines the number of hyperparameter samples to try and the maximum number of timesteps for each trial.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 100\nstop_timesteps = 200\n```\n\n----------------------------------------\n\nTITLE: Launching Aim UI for Results Visualization - Python\nDESCRIPTION: This snippet demonstrates how to launch the Aim UI for visualizing the logged results. It provides a command to initiate the UI, allowing users to view their experiment's metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-aim.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment the following line to launch the Aim UI!\n#!aim up --repo=/tmp/ray_results/aim_example\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost Training Benchmark on Multiple Nodes\nDESCRIPTION: This command runs an XGBoost training benchmark using Ray Train's XGBoostTrainer on 10 nodes with 100GB of data, demonstrating distributed data parallelism.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/benchmarks.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython train_batch_inference_benchmark.py \"xgboost\" --size=100GB\n```\n\n----------------------------------------\n\nTITLE: Preparing Environment and Directories for DreamBooth Fine-tuning in Bash\nDESCRIPTION: This bash script sets up the necessary directories and environment variables for the DreamBooth fine-tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/dreambooth_finetuning.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport DATA_PREFIX=\"/tmp/dreambooth\"\n\nexport INSTANCE_DIR=\"$DATA_PREFIX/instance\"\nexport CLASS_DIR=\"$DATA_PREFIX/class\"\nexport OUTPUT_DIR=\"$DATA_PREFIX/model\"\nexport CACHE_DIR=\"$DATA_PREFIX/cache\"\n\nmkdir -p \"$INSTANCE_DIR\" \"$CLASS_DIR\" \"$OUTPUT_DIR\" \"$CACHE_DIR\"\n\nexport PRETRAINED_MODEL=\"CompVis/stable-diffusion-v1-4\"\nexport ORIG_MODEL_PATH=\"$CACHE_DIR/orig_model\"\n\nexport INSTANCE_PROMPT=\"a photo of unqtkn dog\"\nexport CLASS_PROMPT=\"a photo of dog\"\nexport CLASS_NAME=\"dog\"\n\nexport IMAGES_OWN_DIR=\"$DATA_PREFIX/my_images\"\nmkdir -p \"$IMAGES_OWN_DIR\"\n```\n\n----------------------------------------\n\nTITLE: Checking Pod Status in Kubernetes\nDESCRIPTION: Command and output to verify the status of pods in the Ray cluster\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods\n\n# NAME                                            READY   STATUS         RESTARTS   AGE\n# test-cluster-1-worker-worker-n5g8k              1/1     Running        0          9m4s\n# test-cluster-1-head-6668q                       1/1     Running        0          9m4s\n# test-cluster-1-worker-worker-6tzf7              1/1     Running        0          9m4s\n```\n\n----------------------------------------\n\nTITLE: Default Concurrency Group Configuration in Java\nDESCRIPTION: Shows how to set up default concurrency group alongside custom groups in Java Ray actor implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/concurrency_group_api.rst#2025-04-12_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nclass ConcurrentActor:\n    public long f1() {\n        return Thread.currentThread().getId();\n    }\n\nConcurrencyGroup group =\n    new ConcurrencyGroupBuilder<ConcurrentActor>()\n        .setName(\"io\")\n        .setMaxConcurrency(2)\n        .addMethod(ConcurrentActor::f1)\n        .build();\n\nActorHandle<ConcurrentActor> myActor = Ray.actor(ConcurrentActor::new)\n      .setConcurrencyGroups(group1)\n      .setMaxConcurrency(10)\n      .remote();\n```\n\n----------------------------------------\n\nTITLE: Ray Cluster Management Commands\nDESCRIPTION: Commands to download config, create, connect to, and tear down a Ray cluster on AWS\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Download the example-full.yaml\nwget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/aws/example-full.yaml\n\n# Create or update the cluster. When the command finishes, it will print\n# out the command that can be used to SSH into the cluster head node.\nray up example-full.yaml\n\n# Get a remote shell on the head node.\nray attach example-full.yaml\n\n# Try running a Ray program.\npython -c 'import ray; ray.init()'\nexit\n\n# Tear down the cluster.\nray down example-full.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Operator with Apache YuniKorn Support\nDESCRIPTION: Helm command to install the KubeRay operator with YuniKorn support by setting the batchScheduler.name parameter to yunikorn.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.3.0 --set batchScheduler.name=yunikorn\n```\n\n----------------------------------------\n\nTITLE: Setting Force Trial Cleanup Time in Ray Tune\nDESCRIPTION: TUNE_FORCE_TRIAL_CLEANUP_S sets the grace period in seconds before forcefully terminating trials. Defaults to 600 seconds (10 minutes).\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_FORCE_TRIAL_CLEANUP_S=600\n```\n\n----------------------------------------\n\nTITLE: Wait for Job Completion\nDESCRIPTION: Waits for the rayjob-sample-shutdown job to complete with a 500-second timeout.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nkubectl wait --for=condition=complete job/rayjob-sample-shutdown --timeout=500s\n```\n\n----------------------------------------\n\nTITLE: Calculating Training Steps per Epoch\nDESCRIPTION: Calculates the number of training steps per epoch based on dataset size, batch size and number of workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 16\ntrain_ds_size = processed_datasets[\"train\"].count()\nsteps_per_epoch = train_ds_size // (batch_size * num_workers)\n```\n\n----------------------------------------\n\nTITLE: Configuring CPUs for Main Process\nDESCRIPTION: This snippet shows how to configure the number of CPUs for the main process using the `resources()` method in the `AlgorithmConfig` object. It replaces the old stack's `num_cpus_for_local_worker` setting.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"config.resources(num_cpus_for_main_process=0)  # default is 1\"\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Tune and PyTorch Lightning Integration\nDESCRIPTION: This snippet imports the necessary modules from Ray Tune and its PyTorch Lightning integration for hyperparameter tuning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\nfrom ray.tune.integration.pytorch_lightning import (\n    TuneReportCallback,\n    TuneReportCheckpointCallback,\n)\n```\n\n----------------------------------------\n\nTITLE: Getting Basic Information from SingleAgentEpisode in Python\nDESCRIPTION: This section outlines methods to retrieve fundamental details of an episode, such as its length, return value, duration, completion status, and step information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_episode.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n    Methods:\n    - __len__(): returns the length of the episode.\n    - get_return(): retrieves the total return from the episode.\n    - get_duration_s(): gets the duration of the episode in seconds.\n    - is_done(): checks if the episode has completed.\n    - is_numpy(): verifies if the data is in NumPy format.\n    - env_steps(): returns the number of environment steps taken.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Task Timing Callbacks with Dask-on-Ray in Python\nDESCRIPTION: This example demonstrates how to use Ray-specific callbacks in Dask to measure and log the execution time of each task, using ray_pretask and ray_posttask hooks for task introspection.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/dask-on-ray.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\nimport dask.array as da\nfrom ray.util.dask import enable_dask_on_ray\n\n# Start Ray.\nray.init()\n\n# Tell Dask to use Ray as the scheduler.\nenable_dask_on_ray()\n\n# Create a large array.\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\n\ndef timer_callback():\n    \"\"\"Time tasks.\"\"\"\n    timings = {}\n    \n    def ray_pretask(key, object_refs):\n        \"\"\"Called before executing a task.\"\"\"\n        # Start the timer when the task starts.\n        start = time.time()\n        return start\n\n    def ray_posttask(key, result, pre_state):\n        \"\"\"Called after executing a task.\"\"\"\n        # Stop the timer when the task completes.\n        stop = time.time()\n        # Pre_state contains the start time we passed from the\n        # ray_pretask callback.\n        start = pre_state\n        # Print results.\n        duration = stop - start\n        print(f\"Task {key} took {duration} seconds.\")\n        timings[key] = duration\n        return result\n\n    return {\"ray_pretask\": ray_pretask, \"ray_posttask\": ray_posttask}\n\n# Calculate the mean using our custom task timer.\nwith dask.config.set(callbacks=timer_callback()):\n    result = x.mean().compute()\n```\n\n----------------------------------------\n\nTITLE: Define the gradient of Qhat function\nDESCRIPTION: This defines the grad_Qhat(theta, h) function, which calculates the gradient of the estimated reward function with respect to both the model parameters (theta) and the hyperparameters (h). This gradient is used to update the model parameters during training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"def grad_Qhat(theta, h):\n    theta_grad = -2 * h * theta\n    theta_grad[0] *= 3 / 4\n    h_grad = -np.square(theta)\n    h_grad[0] *= 3 / 4\n    return {\\\"theta\\\": theta_grad, \\\"h\\\": h_grad}\"\n```\n\n----------------------------------------\n\nTITLE: Default Tune CLI Output Example in Bash\nDESCRIPTION: Example of the default command-line output format that Ray Tune displays during experiment execution. Shows trial status, parameters, metrics, and resource usage in a tabular format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/reporters.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n== Status ==\nMemory usage on this node: 11.4/16.0 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 4/12 CPUs, 0/0 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects\nResult logdir: /Users/foo/ray_results/myexp\nNumber of trials: 4 (4 RUNNING)\n+----------------------+----------+---------------------+-----------+--------+--------+--------+--------+------------------+-------+\n| Trial name           | status   | loc                 |    param1 | param2 | param3 |    acc |   loss |   total time (s) |  iter |\n|----------------------+----------+---------------------+-----------+--------+--------+--------+--------+------------------+-------|\n| MyTrainable_a826033a | RUNNING  | 10.234.98.164:31115 | 0.303706  | 0.0761 | 0.4328 | 0.1289 | 1.8572 |          7.54952 |    15 |\n| MyTrainable_a8263fc6 | RUNNING  | 10.234.98.164:31117 | 0.929276  | 0.158  | 0.3417 | 0.4865 | 1.6307 |          7.0501  |    14 |\n| MyTrainable_a8267914 | RUNNING  | 10.234.98.164:31111 | 0.068426  | 0.0319 | 0.1147 | 0.9585 | 1.9603 |          7.0477  |    14 |\n| MyTrainable_a826b7bc | RUNNING  | 10.234.98.164:31112 | 0.729127  | 0.0748 | 0.1784 | 0.1797 | 1.7161 |          7.05715 |    14 |\n+----------------------+----------+---------------------+-----------+--------+--------+--------+--------+------------------+-------+\n```\n\n----------------------------------------\n\nTITLE: Pyproject.toml example for uv\nDESCRIPTION: This snippet shows an example `pyproject.toml` file used for managing dependencies with uv. It specifies the project name, version, and dependencies (emoji and ray). This file is used by uv to install and manage the project's dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[project]\nname = \"test\"\nversion = \"0.1\"\ndependencies = [\n  \"emoji\",\n  \"ray\",\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running PPO with Custom Callback\nDESCRIPTION: This code configures and runs the PPO algorithm with the custom callback `LogAcrobotAngle`. It sets up the environment to be \"Acrobot-v1\" and specifies the `LogAcrobotAngle` class as the callback. The code then trains the PPO agent for 10 iterations and prints the average theta1 angle and the episode return mean from the results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-callback.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconfig = (\n    PPOConfig()\n    .environment(\"Acrobot-v1\")\n    .callbacks(\n        callbacks_class=LogAcrobotAngle,\n    )\n)\nppo = config.build()\n\n# Train n times. Expect to find `theta1_mean` in the results under:\n# `env_runners/theta1_mean`\nfor i in range(10):\n    results = ppo.train()\n    print(\n        f\"iter={i} \"\n        f\"theta1_mean={results['env_runners']['theta1_mean']} \"\n        f\"R={results['env_runners']['episode_return_mean']}\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Existing Frozen VM in vSphere for Ray\nDESCRIPTION: YAML configuration for using an existing single frozen VM in vSphere with Ray. This setup specifies the name of the frozen VM to be used.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nfrozen_vm:\n    name: existing-single-frozen-vm\n```\n\n----------------------------------------\n\nTITLE: Loading Datasets for Llama Pre-training\nDESCRIPTION: Defines a function to load datasets from Hugging Face, including handling of validation split if not present in the original dataset. Uses the datasets library for efficient data loading.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef load_datasets(config):\n    dataset_name = config[\"name\"] \n    dataset_config_name = config[\"config_name\"]\n\n    # Downloading and loading a dataset from the hub.\n    raw_datasets = load_dataset(\n        dataset_name,\n        dataset_config_name,\n        cache_dir=None,\n        token=None,\n        streaming=False,\n    )\n    if \"validation\" not in raw_datasets.keys():\n        raw_datasets[\"validation\"] = load_dataset(\n            dataset_name,\n            dataset_config_name,\n            split=f\"train[:{data_args.validation_split_percentage}%]\",\n            cache_dir=None,\n            token=None,\n            streaming=False,\n        )\n        raw_datasets[\"train\"] = load_dataset(\n            dataset_name,\n            dataset_config_name,\n            split=f\"train[{data_args.validation_split_percentage}%:]\",\n            cache_dir=None,\n            token=None,\n            streaming=False,\n        )\n\n    return raw_datasets\n```\n\n----------------------------------------\n\nTITLE: Installing Ray using pip\nDESCRIPTION: This command installs the Ray framework using pip, the Python package installer. It's the simplest way to get started with Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/README.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ray\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Cluster for 7B Language Model Training (YAML)\nDESCRIPTION: This YAML configuration defines the cluster setup for training a 7B parameter language model. It specifies a head node and GPU worker nodes with their instance types and scaling limits.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nhead_node_type:\n  name: head_node_type\n  instance_type: m5.xlarge\n\nworker_node_types:\n- name: gpu_worker\n  instance_type: g5.4xlarge\n  min_workers: 0\n  max_workers: 16\n  use_spot: false\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray for Distributed Computing\nDESCRIPTION: This code initializes Ray, setting up the distributed computing environment for parallel execution of tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/automl_for_time_series.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nray.init(ignore_reinit_error=True)\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Application for Fault Tolerance Demonstration\nDESCRIPTION: Python code snippet showcasing a Serve application used to demonstrate system recovery and failure handling\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Python code for sleepy_pid.py fault tolerance example\n# Full code not shown in snippet\n```\n\n----------------------------------------\n\nTITLE: Managing Interactive Sessions on Ray Clusters\nDESCRIPTION: Commands for attaching to interactive screen or tmux sessions on Ray clusters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-cli.rst#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# Open a screen on the cluster\n$ ray attach cluster.yaml\n\n# Open a screen on a new cluster called 'session-1'\n$ ray attach cluster.yaml --start --cluster-name=session-1\n\n# Attach to tmux session on cluster (creates a new one if none available)\n$ ray attach cluster.yaml --tmux\n```\n\n----------------------------------------\n\nTITLE: Installing Memray for Memory Profiling\nDESCRIPTION: Command to install the memray package for memory profiling Ray tasks and actors\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-memory.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install memray\n```\n\n----------------------------------------\n\nTITLE: Evaluating ResNet Model Predictions\nDESCRIPTION: Function to compute the number of correct predictions by comparing predicted labels with true labels for model evaluation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(logits, labels):\n    _, preds = torch.max(logits, 1)\n    corrects = torch.sum(preds == labels).item()\n    return corrects\n```\n\n----------------------------------------\n\nTITLE: Binding IAM Role to Kubernetes ServiceAccount\nDESCRIPTION: This command binds the 'roles/storage.objectUser' role to the Kubernetes service account and bucket IAM policy, allowing access to the GCS bucket.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nPROJECT_ID=<your project ID>\nPROJECT_NUMBER=<your project number>\ngcloud storage buckets add-iam-policy-binding gs://${BUCKET} --member \"principal://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/${PROJECT_ID}.svc.id.goog/subject/ns/default/sa/pytorch-distributed-training\"  --role \"roles/storage.objectUser\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Miniforge for MacOS\nDESCRIPTION: Downloads the Miniforge installer for MacOS (arm64 architecture) using wget. This is a prerequisite for installing Ray on M1 Macs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n\"wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\"\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Llama-2 Fine-tuning\nDESCRIPTION: Imports essential Python libraries for model training, including PyTorch, Transformers, Ray, and Habana Gaudi utilities\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport copy\nfrom typing import Dict\n\nimport torch\n\nimport datasets\nimport transformers\nfrom transformers import DataCollatorForLanguageModeling\n\nfrom tqdm import tqdm\n\nimport peft\n\nfrom optimum.habana import GaudiTrainer, GaudiConfig, GaudiTrainingArguments\nfrom optimum.habana.transformers.modeling_utils import adapt_transformers_to_gaudi\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel App Deployment\nDESCRIPTION: Final configuration to deploy the parallel model execution setup using Ray Serve\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/gradio-integration.md#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nd1 = TextGenerationModel.bind(\"gpt2\")\nd2 = TextGenerationModel.bind(\"distilgpt2\")\napp = MyGradioServer.bind(d1, d2)\n```\n\n----------------------------------------\n\nTITLE: Typed Application Builder with Pydantic Model\nDESCRIPTION: Implements a type-safe application builder using a Pydantic model for argument validation and type conversion\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/app-builder-guide.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass HelloWorldArgs(BaseModel):\n    message: str = \"Hello World\"\n\ndef app_builder(args: HelloWorldArgs):\n    return Deployment.bind(HelloWorld, args.message)\n```\n\n----------------------------------------\n\nTITLE: Building Custom Learner Connector Pipeline in RLlib MARWIL\nDESCRIPTION: Demonstrates overriding the build_learner_connector method to customize the pipeline by adding episode timesteps, next observations, and GAE computations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n@override(AlgorithmConfig)\ndef build_learner_connector(\n    self,\n    input_observation_space,\n    input_action_space,\n    device=None,\n):\n    pipeline = super().build_learner_connector(\n        input_observation_space=input_observation_space,\n        input_action_space=input_action_space,\n        device=device,\n    )\n\n    # Before anything, add one ts to each episode (and record this in the loss\n    # mask, so that the computations at this extra ts aren't used to compute\n    # the loss).\n    pipeline.prepend(AddOneTsToEpisodesAndTruncate())\n\n    # Prepend the \"add-NEXT_OBS-from-episodes-to-train-batch\" connector piece (right\n    # after the corresponding \"add-OBS-...\" default piece).\n    pipeline.insert_after(\n        AddObservationsFromEpisodesToBatch,\n        AddNextObservationsFromEpisodesToTrainBatch(),\n    )\n\n    # At the end of the pipeline (when the batch is already completed), add the\n    # GAE connector, which performs a vf forward pass, then computes the GAE\n    # computations, and puts the results of this (advantages, value targets)\n    # directly back in the batch. This is then the batch used for\n    # `forward_train` and `compute_losses`.\n    pipeline.append(\n        GeneralAdvantageEstimation(gamma=self.gamma, lambda_=self.lambda_)\n    )\n\n    return pipeline\n```\n\n----------------------------------------\n\nTITLE: Running Local Development with serve.run in Python\nDESCRIPTION: This snippet demonstrates the use of `serve.run` to locally test a Ray Serve application in Python. It leverages Python for integration tests without deploying to a cloud provider. The code is run using `serve.run` to execute locally.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dev-workflow.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n```{literalinclude} ../doc_code/local_dev.py\n:start-after: __local_dev_start__\n:end-before: __local_dev_end__\n:language: python\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Data Processing with Ray Example\nDESCRIPTION: Example demonstrating how to transform data using Ray with batch processing and custom transformations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/writing-code-snippets.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport numpy as np\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\n# Compute a \"petal area\" attribute.\ndef transform_batch(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n    vec_a = batch[\"petal length (cm)\"]\n    vec_b = batch[\"petal width (cm)\"]\n    batch[\"petal area (cm^2)\"] = vec_a * vec_b\n    return batch\n\ntransformed_ds = ds.map_batches(transform_batch)\nprint(transformed_ds.materialize())\n```\n\n----------------------------------------\n\nTITLE: Viewing Network Policies for Ray Cluster\nDESCRIPTION: Lists the network policies applied to the static Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl get networkpolicies\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Specific Address in Python\nDESCRIPTION: This Python code snippet shows how to initialize Ray by connecting to a specific address. It demonstrates setting the address parameter in the ray.init() function.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/project_files/requirements_project/requirements.txt#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init(address='localhost:6379')\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Training Parity Benchmark on CPU\nDESCRIPTION: This command runs a benchmark comparing Ray Train's TorchTrainer to native PyTorch Distributed on CPU, using 4 workers across multiple nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/benchmarks.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython workloads/torch_benchmark.py run --num-runs 3 --num-epochs 20 --num-workers 4 --cpus-per-worker 8\n```\n\n----------------------------------------\n\nTITLE: Creating a Kind Cluster for KubeRay and Kueue Integration\nDESCRIPTION: Command to create a local Kind Kubernetes cluster for testing KubeRay and Kueue integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkind create cluster\n```\n\n----------------------------------------\n\nTITLE: Customizing Worker Process Loggers in Ray\nDESCRIPTION: Provides Python code examples for setting up logging in Ray worker processes, useful for defining custom log handling in Actors and Tasks. Dependencies include Ray and Pythons logging module. Includes Ray initialization and use of ray methods to control logger outputs per process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport logging\n# Initiate a driver.\nray.init()\n\n@ray.remote\nclass Actor:\n    def __init__(self):\n        # Basic config automatically configures logs to\n        # stream to stdout and stderr.\n        # Set the severity to INFO so that info logs are printed to stdout.\n        logging.basicConfig(level=logging.INFO)\n\n    def log(self, msg):\n        logger = logging.getLogger(__name__)\n        logger.info(msg)\n\nactor = Actor.remote()\nray.get(actor.log.remote(\"A log message for an actor.\"))\n\n@ray.remote\ndef f(msg):\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    logger.info(msg)\n\nray.get(f.remote(\"A log message for a task.\"))\n```\n\n----------------------------------------\n\nTITLE: Creating reStructuredText Reference for Ray Ecosystem Map\nDESCRIPTION: This snippet defines a reference anchor for the Ray ecosystem map section and creates explanatory text about the maturity levels of different Ray components and integrations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/index.rst#2025-04-12_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. _air-ecosystem-map:\n\nEcosystem Map\n-------------\n\nThe following map visualizes the landscape and maturity of Ray components and their integrations. Solid lines denote integrations between Ray components; dotted lines denote integrations with the broader ML ecosystem.\n\n* **Stable**: This component is stable.\n* **Beta**: This component is under development and APIs may be subject to change.\n* **Alpha**: This component is in early development.\n* **Community-Maintained**: These integrations are community-maintained and may vary in quality.\n\n.. image:: /images/air-ecosystem.svg\n```\n\n----------------------------------------\n\nTITLE: Downloading Stable Diffusion TPU Request Script\nDESCRIPTION: This command downloads a Python script used to send text-to-image prompts to the Stable Diffusion model server.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/tpu-serve-stable-diffusion.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncurl -LO https://raw.githubusercontent.com/ray-project/serve_config_examples/master/stable_diffusion/stable_diffusion_tpu_req.py\n```\n\n----------------------------------------\n\nTITLE: Installing Psycopg2 for PostgreSQL\nDESCRIPTION: This command installs the `psycopg2-binary` package using pip. `psycopg2` is a popular PostgreSQL adapter for Python, allowing you to connect and interact with PostgreSQL databases.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_25\n\nLANGUAGE: console\nCODE:\n```\npip install psycopg2-binary\n```\n\n----------------------------------------\n\nTITLE: Formatting Bazel Files\nDESCRIPTION: This shell script formats Bazel files to ensure proper style and structure according to project specifications. It should be run from the root project directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n./ci/lint/bazel-format.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Ray Tune Experiment\nDESCRIPTION: Defines a search space for Ray Tune that includes different algorithms (XGBoost and LinearRegression) and NYC taxi pickup locations. Creates a tuner object with the training function and executes the experiment with tuner.fit().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n############\n# STEP 2. Customize distributed compute scaling.\n############\n# Use Ray Tune default resources config which is 1 CPU core for each task.\n\n############\n# STEP 3. Define a search space dict of all config parameters.\n############\nsearch_space = {\n    \"algorithm\": tune.grid_search(\n        [LinearRegression(fit_intercept=True), xgb.XGBRegressor(max_depth=4)]\n    ),\n    \"location\": tune.grid_search(sample_locations),\n}\n\n# Optional STEP 4. Specify the hyperparameter tuning search strategy.\n\n############\n# STEP 5. Run the experiment with Ray Tune APIs.\n# https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html\n############\nstart = time.time()\n\n# Define a tuner object.\ntuner = tune.Tuner(\n    train_model,\n    param_space=search_space,\n    run_config=tune.RunConfig(\n        # redirect logs to relative path instead of default ~/ray_results/\n        name=\"batch_tuning\",\n    ),\n)\n\n# Fit the tuner object.\nresults = tuner.fit()\n\ntotal_time_taken = time.time() - start\nprint(f\"Total number of models: {len(results)}\")\nprint(f\"TOTAL TIME TAKEN: {total_time_taken/60:.2f} minutes\")\n\n# Total number of models: 6\n# TOTAL TIME TAKEN: 0.37 minutes\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Hash Verification\nDESCRIPTION: This code defines specific Python package dependencies for the Ray project with exact versions and hash values for security verification. Each package includes comments indicating which requirements files reference it, providing context for dependency management.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-proto==1.1.0 \\\n    --hash=sha256:237ef4fdd7f752b2fe740352643f8ef82733bd8e0db8b46ed808125ac7c7f112 \\\n    --hash=sha256:ff1ad9a3c572075883c2af0053cefdfaba005d71eade783c4524d34660d53b60\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\n```\n\n----------------------------------------\n\nTITLE: Defining Packaging and Pandas Dependencies\nDESCRIPTION: Specifies the Packaging and Pandas packages with their exact versions and SHA256 hash verification. The comments section lists which components in the Ray project depend on these packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   lazy-loader\n    #   scikit-image\n    #   tensorboardx\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    --hash=sha256:41179ce559943d83a9b4bbacb736b04c928b095b5f25dd2b7389eda08f46f373 \\\n    --hash=sha256:478ff646ca42b20376e4ed3fa2e8d7341e8a63105586efe54fa2508ee087f328 \\\n    --hash=sha256:50869a35cbb0f2e0cd5ec04b191e7b12ed688874bd05dd777c19b28cbea90996 \\\n    --hash=sha256:565fa34a5434d38e9d250af3c12ff931abaf88050551d9fbcdfafca50d62babf \\\n    --hash=sha256:5f2b952406a1588ad4cad5b3f55f520e82e902388a6d5a4a91baa8d38d23c7f6 \\\n    --hash=sha256:5fbcb19d6fceb9e946b3e23258757c7b225ba450990d9ed63ccceeb8cae609f7 \\\n    --hash=sha256:6973549c01ca91ec96199e940495219c887ea815b2083722821f1d7abfa2b4dc \\\n    --hash=sha256:74a3fd7e5a7ec052f183273dc7b0acd3a863edf7520f5d3a1765c04ffdb3b0b1 \\\n    --hash=sha256:7a0a56cef15fd1586726dace5616db75ebcfec9179a3a55e78f72c5639fa2a23 \\\n    --hash=sha256:7cec0bee9f294e5de5bbfc14d0573f65526071029d036b753ee6507d2a21480a \\\n    --hash=sha256:87bd9c03da1ac870a6d2c8902a0e1fd4267ca00f13bc494c9e5a9020920e1d51 \\\n    --hash=sha256:972d8a45395f2a2d26733eb8d0f629b2f90bebe8e8eddbb8829b180c09639572 \\\n    --hash=sha256:9842b6f4b8479e41968eced654487258ed81df7d1c9b7b870ceea24ed9459b31 \\\n    --hash=sha256:9f69c4029613de47816b1bb30ff5ac778686688751a5e9c99ad8c7031f6508e5 \\\n    --hash=sha256:a50d9a4336a9621cab7b8eb3fb11adb82de58f9b91d84c2cd526576b881a0c5a \\\n    --hash=sha256:bc4c368f42b551bf72fac35c5128963a171b40dce866fb066540eeaf46faa003 \\\n    --hash=sha256:c39a8da13cede5adcd3be1182883aea1c925476f4e84b2807a46e2775306305d \\\n    --hash=sha256:c3ac844a0fe00bfaeb2c9b51ab1424e5c8744f89860b138434a363b1f620f354 \\\n    --hash=sha256:c4c00e0b0597c8e4f59e8d461f797e5d70b4d025880516a8261b2817c47759ee \\\n    --hash=sha256:c74a62747864ed568f5a82a49a23a8d7fe171d0c69038b38cedf0976831296fa \\\n    --hash=sha256:dd05f7783b3274aa206a1af06f0ceed3f9b412cf665b7247eacd83be41cf7bf0 \\\n    --hash=sha256:dfd681c5dc216037e0b0a2c821f5ed99ba9f03ebcf119c7dac0e9a7b960b9ec9 \\\n    --hash=sha256:e474390e60ed609cec869b0da796ad94f420bb057d86784191eefc62b65819ae \\\n    --hash=sha256:f76d097d12c82a535fda9dfe5e8dd4127952b45fea9b0276cb30cca5ea313fbc\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Vale using Homebrew (macOS)\nDESCRIPTION: Command to install Vale on macOS using the Homebrew package manager. Vale is a syntax-aware linter for prose that's used to check documentation style.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nbrew install vale\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents for Ray Train Documentation in reStructuredText\nDESCRIPTION: This snippet defines a table of contents for Ray Train user guides using reStructuredText syntax. It creates a hierarchical documentation structure with a maximum depth of 2, linking to various guide pages covering topics from data loading to hyperparameter optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _train-user-guides:\n\nRay Train User Guides\n=====================\n\n.. toctree::\n    :maxdepth: 2\n\n    user-guides/data-loading-preprocessing\n    user-guides/using-gpus\n    user-guides/persistent-storage\n    user-guides/monitoring-logging\n    user-guides/checkpoints\n    user-guides/experiment-tracking\n    user-guides/results\n    user-guides/fault-tolerance\n    user-guides/reproducibility\n    Hyperparameter Optimization <user-guides/hyperparameter-optimization>\n```\n\n----------------------------------------\n\nTITLE: Upgrading KubeRay from v1.1.0 to v1.2.2\nDESCRIPTION: Commands to upgrade KubeRay installation from version 1.1.0 to 1.2.2, including CRD updates, operator upgrade, and verification steps using Helm and kubectl.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/upgrade-guide.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Upgrade the CRD to v1.2.2.\n# Note: This example uses kubectl because Helm doesn't support lifecycle management of CRDs.\n# See the Helm documentation for more details: https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#some-caveats-and-explanations\n$ kubectl replace -k \"github.com/ray-project/kuberay/ray-operator/config/crd?ref=v1.2.2\"\n\n# Upgrade kuberay-operator to v1.2.2. This step doesn't upgrade the CRDs.\n$ helm upgrade kuberay-operator kuberay/kuberay-operator --version v1.2.2\n\n# Install a RayCluster using the v1.2.2 helm chart to verify the success of the upgrade.\n$ helm install raycluster kuberay/ray-cluster --version 1.2.2\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Ray Dashboard\nDESCRIPTION: YAML configuration for setting required environment variables to integrate Grafana with Ray Dashboard.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  - name: RAY_GRAFANA_IFRAME_HOST\n    value: http://127.0.0.1:3000\n  - name: RAY_GRAFANA_HOST\n    value: http://prometheus-grafana.prometheus-system.svc:80\n  - name: RAY_PROMETHEUS_HOST\n    value: http://prometheus-kube-prometheus-prometheus.prometheus-system.svc:9090\n```\n\n----------------------------------------\n\nTITLE: Displaying Training Results and Checkpoint Information\nDESCRIPTION: This simple code snippet prints the metrics from the final training result and the checkpoint information after training has completed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Last result: {result.metrics}\")\nprint(f\"Checkpoint: {result.checkpoint}\")\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA GPU Device Driver on GKE\nDESCRIPTION: These commands manually install the NVIDIA GPU device driver on the GKE cluster and verify the GPU allocation on the nodes. This step is optional and only needed if there are issues with the default GPU drivers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# Install NVIDIA GPU device driver\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml\n\n# Verify that your nodes have allocatable GPUs\nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n\n# Verify that your nodes have allocatable GPUs\n# NAME     GPU\n# ......   <none>\n# ......   1\n```\n\n----------------------------------------\n\nTITLE: Packaging Lambda Function\nDESCRIPTION: Creates a zip file containing all files in the docker/retag-lambda directory. This zip file will be used to update the Lambda function.\nSOURCE: https://github.com/ray-project/ray/blob/master/docker/retag-lambda/README.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npushd docker/retag-lambda\nzip retag-lambda.zip *\n```\n\n----------------------------------------\n\nTITLE: Deduplication Configuration Example\nDESCRIPTION: YAML configuration for distributed deduplication using Ray with MinHash algorithm.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/data_juicer_distributed_data_processing.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nproject_name: 'demo-dedup'\ndataset_path: './demos/process_on_ray/data/'\nexport_path: './outputs/demo-dedup/demo-ray-bts-dedup-processed'\n\nexecutor_type: 'ray'  # Set the executor type to \"ray\"\nray_address: 'auto'  # Set an automatic Ray address\n\n# process schedule\n# a list of several process operators with their arguments\nprocess:\n  - ray_bts_minhash_deduplicator:  # a distributed version of minhash deduplicator\n      tokenization: 'character'\n```\n\n----------------------------------------\n\nTITLE: Deploying RayService and Checking Kubernetes Resources\nDESCRIPTION: Commands to apply the RayService configuration and verify the deployment of pods, services, and RayService resources\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/kubernetes.md#2025-04-12_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl apply -f ray-service.text-ml.yaml\n\n$ kubectl get rayservices\nNAME                AGE\nrayservice-sample   7s\n\n$ kubectl get pods\nNAME                                                      READY   STATUS    RESTARTS   AGE\nservice-sample-raycluster-454c4-worker-small-group-b6mmg  1/1     Running   0          XXs\nkuberay-operator-7fbdbf8c89-4lrnr                         1/1     Running   0          XXs\nrayservice-sample-raycluster-454c4-head-krk9d             1/1     Running   0          XXs\n\n$ kubectl get services\nrayservice-sample-head-svc                         ClusterIP   ...        8080/TCP,6379/TCP,8265/TCP,10001/TCP,8000/TCP,52365/TCP   XXs\nrayservice-sample-raycluster-454c4-dashboard-svc   ClusterIP   ...        52365/TCP                                                 XXs\nrayservice-sample-raycluster-454c4-head-svc        ClusterIP   ...        8000/TCP,52365/TCP,8080/TCP,6379/TCP,8265/TCP,10001/TCP   XXs\nrayservice-sample-serve-svc                        ClusterIP   ...        8000/TCP                                                  XXs\n```\n\n----------------------------------------\n\nTITLE: Configuring GKE Ingress for Ray Cluster\nDESCRIPTION: YAML configuration for setting up GKE Ingress to access a Ray cluster, followed by bash commands for deploying and managing the ingress setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ray-cluster-ingress\n  annotations:\n    kubernetes.io/ingress.class: \"gce-internal\"\nspec:\n  rules:\n    - http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: raycluster-kuberay-head-svc # Update this line with your head service in Step 3 below.\n                port:\n                  number: 8265\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Step 1: Install KubeRay operator and CRD\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.3.0\n\n# Step 2: Install a RayCluster\nhelm install raycluster kuberay/ray-cluster --version 1.3.0\n\n# Step 3: Edit ray-cluster-gclb-ingress.yaml to replace the service name with the name of the head service from the RayCluster. (Output of `kubectl get svc`)\n\n# Step 4: Apply the Ingress configuration\nkubectl apply -f ray-cluster-gclb-ingress.yaml\n\n# Step 5: Check ingress created by Step 4.\nkubectl describe ingress ray-cluster-ingress\n\n# Step 6: After a few minutes, GKE allocates an external IP for the ingress. Check it using:\nkubectl get ingress ray-cluster-ingress\n\n# Step 7: Check Ray Dashboard by visiting the allocated external IP in your browser. (In this example, it is 34.160.82.156)\n\n# Step 8: Delete the ingress.\nkubectl delete ingress ray-cluster-ingress\n```\n\n----------------------------------------\n\nTITLE: Configuring Runtime Environment with Remote Code\nDESCRIPTION: Example YAML configuration for deploying a text ML application with remote code and specific pip dependencies\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/handling-dependencies.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nimport_path: text_ml:app\\n\\nruntime_env:\\n    working_dir: \"https://github.com/ray-project/serve_config_examples/archive/HEAD.zip\"\\n    pip:\\n      - torch\\n      - transformers\n```\n\n----------------------------------------\n\nTITLE: Listing S3 Bucket Structure for LLM Model Files\nDESCRIPTION: AWS CLI command showing the structure of an LLM model stored in S3, displaying all required files for a Llama 3.2 model deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ aws s3 ls air-example-data/rayllm-ossci/meta-Llama-3.2-1B-Instruct/\n2025-03-25 11:37:48       1519 .gitattributes\n2025-03-25 11:37:48       7712 LICENSE.txt\n2025-03-25 11:37:48      41742 README.md\n2025-03-25 11:37:48       6021 USE_POLICY.md\n2025-03-25 11:37:48        877 config.json\n2025-03-25 11:37:48        189 generation_config.json\n2025-03-25 11:37:48 2471645608 model.safetensors\n2025-03-25 11:37:53        296 special_tokens_map.json\n2025-03-25 11:37:53    9085657 tokenizer.json\n2025-03-25 11:37:53      54528 tokenizer_config.json\n```\n\n----------------------------------------\n\nTITLE: Monitoring Ray Job Logs\nDESCRIPTION: Shell command to follow the logs of the submitted Ray job, providing real-time updates on the job's progress and output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/examples/ml-example.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# Substitute the Ray Job's submission id.\nray job logs 'raysubmit_xxxxxxxxxxxxxxxx' --address=\"http://localhost:8265\" --follow\n```\n\n----------------------------------------\n\nTITLE: RayDaskCallback Direct Initialization in Python\nDESCRIPTION: This snippet demonstrates how to directly instantiate `RayDaskCallback` with callback functions provided as constructor arguments. This allows for simple and direct customization of callback behavior without subclassing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/dask-on-ray.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example showing how to use the RayDaskCallback directly, \\\npassing the callback functions as constructor arguments.\"\"\"\n\nimport time\n\nfrom ray.util.dask import RayDaskCallback\n\n\ndef cb_presubmit(dsk, state):\n    print(\"Submitting a dask graph of size: %s\" % len(dsk))\n\n\ndef cb_postsubmit(dsk, state, id):\n    print(\"Submitted dask task with id: %s\" % id)\n\n\ndef cb_pretask(dsk, state, task):\n    print(\"About to run dask task: %s\" % task)\n    state[\"t\"] = time.time()\n\n\ndef cb_posttask(dsk, state, task, result):\n    duration = time.time() - state[\"t\"]\n    print(\"Task %s took %s s\" % (task, duration))\n\n\ncallback = RayDaskCallback(\n    _ray_presubmit=cb_presubmit,\n    _ray_postsubmit=cb_postsubmit,\n    _ray_pretask=cb_pretask,\n    _ray_posttask=cb_posttask,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring GCS Cloud Storage Input in RLlib\nDESCRIPTION: Configuration for reading offline data from Google Cloud Storage bucket.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nconfig=(\n    AlgorithmConfig()\n    .offline_data(\n        input_=\"gs://<your-bucket>/dir1\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Last Checkpoint in Ray Train\nDESCRIPTION: This snippet demonstrates how to access the last saved checkpoint from a Ray Train `Result` object. The `result.checkpoint` attribute returns a `Checkpoint` object representing the last saved state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/results.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Access the last checkpoint.\"\"\"\n# Get the last checkpoint.\ncheckpoint = result.checkpoint\nprint(f\"Last checkpoint: {checkpoint}\")\n\n```\n\n----------------------------------------\n\nTITLE: Last Task Example\nDESCRIPTION: This Python code demonstrates a scenario where a workload fails when the last task of a caller is killed by the memory monitor, even with infinite retries.  The code defines a task that intentionally consumes more memory than available, triggering the memory monitor. The workload will fail with an OOM error.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/ray-oom-prevention.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example of a workload that fails due to OOM.\n\nNote that the workload will fail even though the task is set to retry forever,\nbecause it is the last task of the caller.\n\"\"\"\n\nimport ray\nimport time\n\n@ray.remote(max_retries=-1)\ndef task_oom_retry(i):\n    # Sleep a while, and then allocate memory that causes the worker\n    # to exceed its memory limit.\n    time.sleep(1)\n    # Each float takes 8 bytes, so this requires 1GiB.\n    gb = 1024 * 1024 * 1024\n    # This will cause OOM error.\n    dummy = bytearray(int(1.1 * gb))\n    del dummy\n    return i\n\nif __name__ == \"__main__\":\n    ray.init()\n    # Cause ray to OOM as the first task.\n    print(\"task failed with OutOfMemoryError, which is expected\")\n    task_oom_retry.remote(1).result()\n    ray.shutdown()\n\n```\n\n----------------------------------------\n\nTITLE: Getting Head Pod Name\nDESCRIPTION: Retrieves and stores the head pod name in an environment variable\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\necho $HEAD_POD\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Cluster with Docker Compose\nDESCRIPTION: Command to start a Ray cluster using the fake multinode Docker setup, which creates containerized nodes for more realistic testing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/fake-autoscaler.rst#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ RAY_FAKE_CLUSTER=1 ray up -y ./python/ray/autoscaler/_private/fake_multi_node/example_docker.yaml\n```\n\n----------------------------------------\n\nTITLE: Kubernetes and Ray Cluster Management Commands\nDESCRIPTION: Command line instructions for applying configurations, verifying deployment, and managing the Ray cluster state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-persistent-ft.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/samples/ray-cluster.persistent-redis.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get persistentvolumes\nkubectl get pods\n# Should see redis-0 running.\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl ray session raycluster-external-redis\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl delete pods --all\n```\n\n----------------------------------------\n\nTITLE: Real-time Monitoring of Kubernetes Pod Status\nDESCRIPTION: Continuously monitors the status of Kubernetes pods in real-time using the watch command.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n! watch -n 1 kubectl get pod\n```\n\n----------------------------------------\n\nTITLE: Running Ray Docker Container\nDESCRIPTION: Runs the Ray Docker container, specifying the shared memory size. The `-t` and `-i` flags enable interactive use. This creates a running Ray environment within the container.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n\"docker run --shm-size=<shm-size> -t -i rayproject/ray\"\n```\n\n----------------------------------------\n\nTITLE: Adjusting Buffer Capacity in AlgorithmConfig for Python\nDESCRIPTION: This code snippet shows how to modify the AlgorithmConfig to reduce the prelearner buffer capacity to 500 timesteps while keeping other parameters the same.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .training(\n        # Train on a batch of 1000 timesteps each iteration.\n        train_batch_size_per_learner=1000,\n    )\n    .offline_data(\n        # Read in RLlib's new stack `SingleAgentEpisode` data.\n        input_read_episodes=True\n        # Define an input read batch size of 10 episodes.\n        input_read_batch_size=10,\n        # Set the replay buffer in the `OfflinePrelearner`\n        # to 500 timesteps.\n        prelearner_buffer_kwargs={\n            \"capacity\": 500,\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Using Placement Groups for Distributed Training in Ray Tune\nDESCRIPTION: This example shows how to use placement groups to request additional resources for distributed training scenarios in Ray Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.execution.placement_groups import PlacementGroupFactory\n\npgf = PlacementGroupFactory([\n    {\"CPU\": 1, \"GPU\": 1},  # for trainable\n    {\"CPU\": 1},  # for remote tasks\n    {\"CPU\": 1}   # for remote tasks\n], strategy=\"PACK\")\n\ntrainable_with_pgf = tune.with_resources(trainable, pgf)\n```\n\n----------------------------------------\n\nTITLE: Watching Single Namespace Configuration\nDESCRIPTION: Commands to set up and verify RBAC configuration for watching a single namespace using Role and RoleBinding.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/helm-chart-rbac.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Create a Kubernetes cluster using Kind.\nkind create cluster --image=kindest/node:v1.26.0\n\n# Create namespaces.\nkubectl create ns n1\nkubectl create ns n2\n\n# Install a KubeRay operator.\n# Set `singleNamespaceInstall` to true in the `values.yaml` file.\n# (path: helm-chart/kuberay-operator)\nhelm install kuberay-operator .\n\n# Check ClusterRole.\nkubectl get clusterrole | grep kuberay\n# (nothing found)\n\n# Check Role.\nkubectl get role --all-namespaces | grep kuberay\n#default       kuberay-operator                                 2023-10-15T05:18:03Z\n#default       kuberay-operator-leader-election                 2023-10-15T05:18:03Z\n\n# Install RayCluster in the `default`, `n1`, and `n2` namespaces.\nhelm install raycluster kuberay/ray-cluster --version 1.3.0\nhelm install raycluster kuberay/ray-cluster --version 1.3.0 -n n1\nhelm install raycluster kuberay/ray-cluster --version 1.3.0 -n n2\n\n# KubeRay only creates a RayCluster in the `default` namespace.\nkubectl get raycluster -A\n# NAMESPACE   NAME                 DESIRED WORKERS   AVAILABLE WORKERS   STATUS   AGE\n# default     raycluster-kuberay   1                 1                   ready    54s\n# n1          raycluster-kuberay                                                  50s\n# n2          raycluster-kuberay                                                  44s\n```\n\n----------------------------------------\n\nTITLE: Dataset JSON Format Example\nDESCRIPTION: Example of the required JSON format for input data with assistant and user tokens\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"input\": \"<ASSISTANT>How can I help you?</ASSISTANT><USER>how is the weather?</USER>\"}\n```\n\n----------------------------------------\n\nTITLE: Printing Best Hyperparameters from Multi-Objective Tuning in Python\nDESCRIPTION: This snippet prints out the best hyperparameters found for each of the optimized metrics (loss and gain) after running the multi-objective tuning experiment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Best hyperparameters for loss found were: \", results.get_best_result(\"loss\", \"min\").config)\nprint(\"Best hyperparameters for gain found were: \", results.get_best_result(\"gain\", \"max\").config)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Ray Cluster\nDESCRIPTION: Shell command to delete the Ray cluster after the workload is completed, ensuring proper resource cleanup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/examples/ml-example.md#2025-04-12_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nray down -y cluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Function Thread Timeout in Ray Tune\nDESCRIPTION: TUNE_FUNCTION_THREAD_TIMEOUT_S sets how long (in seconds) the function API waits for threads to finish after instructing them to complete. Defaults to 2 seconds.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_FUNCTION_THREAD_TIMEOUT_S=2\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Dependencies - Cloud Provider Setup\nDESCRIPTION: Shell commands for installing Ray and cloud provider-specific Python dependencies for different platforms.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/getting-started.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install -U \"ray[default]\" boto3\n$ pip install -U \"ray[default]\" google-api-python-client\n$ pip install -U \"ray[default]\" azure-cli azure-core\n$ pip install -U \"ray[default]\" aliyun-python-sdk-core aliyun-python-sdk-ecs\n$ pip install -U \"ray[default]\" \"git+https://github.com/vmware/vsphere-automation-sdk-python.git\"\n```\n\n----------------------------------------\n\nTITLE: Application Code - Ray Serve Example\nDESCRIPTION: This code snippet, extracted from `autoscale_model_comp_example.py`, shows an example of how to set up a Ray Serve application with autoscaling.  The exact functionality of the code is not provided, but it is intended to demonstrate the Ray Serve usage within the context of the document.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/advanced-autoscaling.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n{literalinclude} ../doc_code/autoscale_model_comp_example.py\n:language: python\n:start-after: __serve_example_begin__\n:end-before: __serve_example_end__\n```\n\n----------------------------------------\n\nTITLE: Examining Pod Scheduling Status\nDESCRIPTION: Command to check the pod details showing that Volcano cannot schedule the gang due to insufficient resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe pod test-cluster-1-head-6668q | tail -n 3\n\n# Type     Reason            Age   From     Message\n# ----     ------            ----  ----     -------\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries - Python\nDESCRIPTION: This snippet installs the necessary libraries required for running Ray Tune with BOHB. It ensures that the ray library along with ConfigSpace and hpbandster versions are available for use.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install ray[tune]\\n!pip install ConfigSpace==0.4.18\\n!pip install hpbandster==0.7.4\n```\n\n----------------------------------------\n\nTITLE: Basic Ray Script Example\nDESCRIPTION: Sample Python script demonstrating basic Ray usage with a remote function\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/sdk.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# script.py\nimport ray\n\n@ray.remote\ndef hello_world():\n    return \"hello world\"\n\nray.init()\nprint(ray.get(hello_world.remote()))\n```\n\n----------------------------------------\n\nTITLE: Define Evaluation Function\nDESCRIPTION: Defines a simple evaluation function that calculates an intermediate score based on the step, width, and height parameters. This function is used within the training function to simulate a training process and generate metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-mlflow.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef evaluation_fn(step, width, height):\n    return (0.1 + width * step / 100) ** (-1) + height * 0.1\n```\n\n----------------------------------------\n\nTITLE: Connecting to Ray Job Server\nDESCRIPTION: Shell command to forward the remote Ray dashboard port to localhost, allowing access to the Job server for workload submission and monitoring.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/examples/ml-example.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nray dashboard cluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Install necessary Python packages including optimum-neuron, diffusers, ray[serve], requests, and transformers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/aws-neuron-core-inference-stable-diffusion.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"optimum-neuron==0.0.13\" \"diffusers==0.21.4\"\npip install \"ray[serve]\" requests transformers\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Starting a TorchTrainer with Ray Train\nDESCRIPTION: Initializes a TorchTrainer with the training function, hyperparameters, and scaling configuration, then executes the distributed training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig\n\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func,\n    train_loop_config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n    scaling_config=ScalingConfig(num_workers=2, use_gpu=False),\n)\nresult = trainer.fit()\nprint(f\"Last result: {result.metrics}\")\n```\n\n----------------------------------------\n\nTITLE: IAM Instance Profile Configuration\nDESCRIPTION: YAML configuration for setting up custom IAM instance profile for Ray nodes\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\navailable_node_types:\n  ray.worker.default:\n    node_config:\n      ...\n      IamInstanceProfile:\n        Arn: arn:aws:iam::YOUR_AWS_ACCOUNT:YOUR_INSTANCE_PROFILE\n```\n\n----------------------------------------\n\nTITLE: Summarizing Ray actors using CLI command\nDESCRIPTION: The 'ray summary actors' CLI command provides a summary of all actors in the Ray cluster, grouping them by class and showing their current state counts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nray summary actors\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Cluster with Kind\nDESCRIPTION: Creates a Kubernetes cluster using Kind with Kubernetes v1.26.0 for local development and testing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for GKE Cluster Creation\nDESCRIPTION: Sets the required environment variables for creating a GKE cluster including the cluster name, compute zone, and cluster version.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-tpu-cluster.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport CLUSTER_NAME=CLUSTER_NAME\nexport COMPUTE_ZONE=ZONE\nexport CLUSTER_VERSION=CLUSTER_VERSION\n```\n\n----------------------------------------\n\nTITLE: NVIDIA ML Support Dependencies for Ray\nDESCRIPTION: Dependencies for NVIDIA machine learning support in Ray, including CUDA utilities, NVIDIA ML Python bindings, and NCCL for multi-GPU communication. These packages enable GPU acceleration and monitoring for distributed machine learning applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_19\n\nLANGUAGE: pip\nCODE:\n```\nnvidia-ml-py==12.570.86 \\\n    --hash=sha256:0508d4a0c7b6d015cf574530b95a62ed4fc89da3b8b47e1aefe6777db170ec8b \\\n    --hash=sha256:58907de35a845abd13dcb227f18298f3b5dd94a72d04c9e594e77711e95c0b51\n    # via pynvml\nnvidia-nccl-cu12==2.21.5 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:8579076d30a8c24988834445f8d633c697d42397e92ffc3f63fa26766d25e0a0\n    # via torch\n```\n\n----------------------------------------\n\nTITLE: Task State Query CLI Command\nDESCRIPTION: CLI command to query task exit details using Ray's State API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/tasks.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nray list tasks\n```\n\n----------------------------------------\n\nTITLE: Setting Discount Factor in Python\nDESCRIPTION: Shows how to configure the RL discount factor gamma.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nconfig.training(gamma=0.995)\n```\n\n----------------------------------------\n\nTITLE: Forwarding Port for Ray Serve Service\nDESCRIPTION: These commands check the RayService status and forward the port for the Ray Serve service to allow local access.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/mobilenet-rayservice.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nkubectl describe rayservice/rayservice-mobilenet\nkubectl port-forward svc/rayservice-mobilenet-serve-svc 8000\n```\n\n----------------------------------------\n\nTITLE: Auto-Generating Serve Configuration Using CLI\nDESCRIPTION: This console command demonstrates how to auto-generate a Serve configuration file using the 'serve build' command, which outputs the configuration for deployments and their parameters into a specified YAML file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/config.md#2025-04-12_snippet_2\n\nLANGUAGE: Console\nCODE:\n```\n$ ls\ntext_ml.py\n\n$ serve build text_ml:app -o serve_config.yaml\n\n$ ls\ntext_ml.py\nserve_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Viewing Pod Status for Gang Scheduling\nDESCRIPTION: Command to view the status of pods from both RayClusters, showing how Volcano's gang scheduling prevents partial deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods\n\n# NAME                                            READY   STATUS         RESTARTS   AGE\n# test-cluster-0-worker-worker-ddfbz              1/1     Running        0          7m\n# test-cluster-0-head-vst5j                       1/1     Running        0          7m\n# test-cluster-0-worker-worker-57pc7              1/1     Running        0          6m59s\n# test-cluster-1-worker-worker-6tzf7              0/1     Pending        0          2m12s\n# test-cluster-1-head-6668q                       0/1     Pending        0          2m12s\n# test-cluster-1-worker-worker-n5g8k              0/1     Pending        0          2m12s\n```\n\n----------------------------------------\n\nTITLE: Installing lz4 Package with Hash Verification\nDESCRIPTION: Package installation specification for lz4 version 4.3.3 with hash verification to ensure package integrity and prevent supply chain attacks. This includes multiple hash values for different platform distributions of the package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nlz4==4.3.3 \\\n    --hash=sha256:01fe674ef2889dbb9899d8a67361e0c4a2c833af5aeb37dd505727cf5d2a131e \\\n    --hash=sha256:054b4631a355606e99a42396f5db4d22046a3397ffc3269a348ec41eaebd69d2 \\\n    --hash=sha256:0a136e44a16fc98b1abc404fbabf7f1fada2bdab6a7e970974fb81cf55b636d0 \\\n    --hash=sha256:0e9c410b11a31dbdc94c05ac3c480cb4b222460faf9231f12538d0074e56c563 \\\n    --hash=sha256:222a7e35137d7539c9c33bb53fcbb26510c5748779364014235afc62b0ec797f \\\n    --hash=sha256:24b3206de56b7a537eda3a8123c644a2b7bf111f0af53bc14bed90ce5562d1aa \\\n    --hash=sha256:2b901c7784caac9a1ded4555258207d9e9697e746cc8532129f150ffe1f6ba0d \\\n    --hash=sha256:2f7b1839f795315e480fb87d9bc60b186a98e3e5d17203c6e757611ef7dcef61 \\\n    --hash=sha256:30e8c20b8857adef7be045c65f47ab1e2c4fabba86a9fa9a997d7674a31ea6b6 \\\n    --hash=sha256:31ea4be9d0059c00b2572d700bf2c1bc82f241f2c3282034a759c9a4d6ca4dc2 \\\n    --hash=sha256:337cb94488a1b060ef1685187d6ad4ba8bc61d26d631d7ba909ee984ea736be1 \\\n    --hash=sha256:33c9a6fd20767ccaf70649982f8f3eeb0884035c150c0b818ea660152cf3c809 \\\n    --hash=sha256:363ab65bf31338eb364062a15f302fc0fab0a49426051429866d71c793c23394 \\\n    --hash=sha256:43cf03059c0f941b772c8aeb42a0813d68d7081c009542301637e5782f8a33e2 \\\n    --hash=sha256:56f4fe9c6327adb97406f27a66420b22ce02d71a5c365c48d6b656b4aaeb7775 \\\n    --hash=sha256:5d35533bf2cee56f38ced91f766cd0038b6abf46f438a80d50c52750088be93f \\\n    --hash=sha256:6756212507405f270b66b3ff7f564618de0606395c0fe10a7ae2ffcbbe0b1fba \\\n    --hash=sha256:6cdc60e21ec70266947a48839b437d46025076eb4b12c76bd47f8e5eb8a75dcc \\\n    --hash=sha256:abc197e4aca8b63f5ae200af03eb95fb4b5055a8f990079b5bdf042f568469dd \\\n    --hash=sha256:b14d948e6dce389f9a7afc666d60dd1e35fa2138a8ec5306d30cd2e30d36b40c \\\n    --hash=sha256:b47839b53956e2737229d70714f1d75f33e8ac26e52c267f0197b3189ca6de24 \\\n    --hash=sha256:b6d9ec061b9eca86e4dcc003d93334b95d53909afd5a32c6e4f222157b50c071 \\\n    --hash=sha256:b891880c187e96339474af2a3b2bfb11a8e4732ff5034be919aa9029484cd201 \\\n    --hash=sha256:bca8fccc15e3add173da91be8f34121578dc777711ffd98d399be35487c934bf \\\n    --hash=sha256:c81703b12475da73a5d66618856d04b1307e43428a7e59d98cfe5a5d608a74c6 \\\n    --hash=sha256:d2507ee9c99dbddd191c86f0e0c8b724c76d26b0602db9ea23232304382e1f21 \\\n    --hash=sha256:e36cd7b9d4d920d3bfc2369840da506fa68258f7bb176b8743189793c055e43d \\\n    --hash=sha256:e7d84b479ddf39fe3ea05387f10b779155fc0990125f4fb35d636114e1c63a2e \\\n    --hash=sha256:eac9af361e0d98335a02ff12fb56caeb7ea1196cf1a49dbf6f17828a131da807 \\\n    --hash=sha256:edfd858985c23523f4e5a7526ca6ee65ff930207a7ec8a8f57a01eae506aaee7 \\\n    --hash=sha256:ee9ff50557a942d187ec85462bb0960207e7ec5b19b3b48949263993771c6205 \\\n    --hash=sha256:f0e822cd7644995d9ba248cb4b67859701748a93e2ab7fc9bc18c599a52e4604 \\\n    --hash=sha256:f180904f33bdd1e92967923a43c22899e303906d19b2cf8bb547db6653ea6e7d \\\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequential Data Retrieval\nDESCRIPTION: Basic implementation of a sequential data retrieval function with simulated processing time\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\ndatabase = [\n    \"Learning\", \"Ray\",\n    \"Flexible\", \"Distributed\", \"Python\", \"for\", \"Machine\", \"Learning\"\n]\n\n\ndef retrieve(item):\n    time.sleep(item / 10.)\n    return item, database[item]\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana for Embedding in Ray Dashboard\nDESCRIPTION: YAML configuration to allow embedding Grafana panels in Ray Dashboard and enable anonymous access.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ngrafana:\n  grafana.ini:\n    security:\n      allow_embedding: true\n    auth.anonymous:\n      enabled: true\n      org_role: Viewer\n```\n\n----------------------------------------\n\nTITLE: Python Requirements List for Ray Release Tests\nDESCRIPTION: A requirements.txt style list of Python packages needed for running Ray release tests. Includes packages for AWS (boto3), Google Cloud, GitHub integration, Kubernetes, and various utility libraries. Some packages have specific version constraints like pydantic and protobuf.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nclick\nanyscale\nslackclient\nboto3\ngoogle-cloud-storage\njinja2\npybuildkite\nPyGithub\npydantic < 1.10.0\npyyaml\ntyper[all]\ntoml\npython-dotenv\nPyGithub\nexpiringdict\nrequests\nprotobuf >= 3.15.3, != 3.19.5\npytz\nretry\nkubernetes\n```\n\n----------------------------------------\n\nTITLE: Multiple Cluster Management\nDESCRIPTION: Commands for launching multiple clusters using the same configuration with different names.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nray up CLUSTER.YAML -n=\"cluster1\"\nray up CLUSTER.YAML -n=\"cluster2\"\nray up CLUSTER.YAML -n=\"cluster3\"\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Cluster with Kind\nDESCRIPTION: Command to create a Kubernetes cluster using Kind for local development and testing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nkind create cluster\n```\n\n----------------------------------------\n\nTITLE: Printing Best Hyperparameters Found with Ray Tune in Python\nDESCRIPTION: Outputs the best hyperparameters discovered that minimize the mean loss as determined by the previously defined objective function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Animating PBT Training Progress\nDESCRIPTION: Creates an animation to visualize the training progress of PBT over time, showing how parameters move through space and when exploitation occurs. The animation is saved as a GIF file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmake_animation(\n    pbt_results,\n    colors,\n    labels,\n    perturbation_interval=perturbation_interval,\n    filename=\"pbt.gif\",\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Gaudi HPU Status Information\nDESCRIPTION: Command output showing the status of Intel Gaudi HPUs on the system, including memory usage, temperature, power consumption and compute processes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n+-----------------------------------------------------------------------------+\n| HL-SMI Version:                              hl-1.20.0-fw-58.1.1.1          |\n| Driver Version:                                     1.19.1-6f47ddd          |\n| Nic Driver Version:                                 1.19.1-f071c23          |\n|-------------------------------+----------------------+----------------------+\n| AIP  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncor-Events|\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | AIP-Util  Compute M. |\n|===============================+======================+======================|\n|   0  HL-225              N/A  | 0000:9a:00.0     N/A |                   0  |\n| N/A   22C   N/A  96W /  600W  |   768MiB /  98304MiB |     0%            0% |\n|-------------------------------+----------------------+----------------------|\n|   1  HL-225              N/A  | 0000:9b:00.0     N/A |                   0  |\n| N/A   24C   N/A  78W /  600W  |   768MiB /  98304MiB |     0%            0% |\n|-------------------------------+----------------------+----------------------|\n|   2  HL-225              N/A  | 0000:b3:00.0     N/A |                   0  |\n| N/A   25C   N/A  81W /  600W  |   768MiB /  98304MiB |     0%            0% |\n|-------------------------------+----------------------+----------------------|\n|   3  HL-225              N/A  | 0000:b4:00.0     N/A |                   0  |\n| N/A   22C   N/A  92W /  600W  | 96565MiB /  98304MiB |     0%           98% |\n|-------------------------------+----------------------+----------------------|\n|   4  HL-225              N/A  | 0000:33:00.0     N/A |                   0  |\n| N/A   22C   N/A  83W /  600W  |   768MiB /  98304MiB |     0%            0% |\n|-------------------------------+----------------------+----------------------|\n|   5  HL-225              N/A  | 0000:4e:00.0     N/A |                   0  |\n| N/A   21C   N/A  80W /  600W  | 96564MiB /  98304MiB |     0%           98% |\n|-------------------------------+----------------------+----------------------|\n|   6  HL-225              N/A  | 0000:34:00.0     N/A |                   0  |\n| N/A   25C   N/A  86W /  600W  |   768MiB /  98304MiB |     0%            0% |\n|-------------------------------+----------------------+----------------------|\n|   7  HL-225              N/A  | 0000:4d:00.0     N/A |                   0  |\n| N/A   30C   N/A 100W /  600W  | 17538MiB /  98304MiB |     0%           17% |\n|-------------------------------+----------------------+----------------------|\n| Compute Processes:                                               AIP Memory |\n|  AIP       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|   0        N/A   N/A    N/A                                      N/A        |\n|   1        N/A   N/A    N/A                                      N/A        |\n|   2        N/A   N/A    N/A                                      N/A        |\n|   3        N/A   N/A    N/A                                      N/A        |\n|   4        N/A   N/A    N/A                                      N/A        |\n|   5        N/A   N/A    N/A                                      N/A        |\n|   6        N/A   N/A    N/A                                      N/A        |\n|   7       107684     C   ray::_RayTrainW                         16770MiB    \n+=============================================================================+\n```\n\n----------------------------------------\n\nTITLE: Documenting Ray Train API Sections in RST\nDESCRIPTION: reStructuredText markup defining the structure and sections of the Ray Train API documentation. Includes autosummary directives for generating documentation from docstrings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/api/api.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _train-api:\\n\\nRay Train API\\n=============\\n\\n.. currentmodule:: ray\\n\\n.. important::\\n\\n    These API references are for the revamped Ray Train V2 implementation...\n```\n\n----------------------------------------\n\nTITLE: AWS IP Fetch Pattern\nDESCRIPTION: Regular expression pattern for validating fetched IP addresses from AWS environment\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_submit.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.+\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with SHA256 Hashes\nDESCRIPTION: A requirements.txt style file listing package dependencies with their versions and corresponding SHA256 hashes for security verification. Includes packages like rsa 4.7.2, s3transfer 0.6.2, scikit-image 0.24.0, and scipy 1.11.4.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_33\n\nLANGUAGE: text\nCODE:\n```\nrsa==4.7.2 \\\n    --hash=sha256:78f9a9bf4e7be0c5ded4583326e7461e3a3c5aae24073648b4bdfa797d78c9d2 \\\n    --hash=sha256:9d689e6ca1b3038bc82bf8d23e944b6b6037bc02301a574935b2dd946e0353b9\n\ns3transfer==0.6.2 \\\n    --hash=sha256:b014be3a8a2aab98cfe1abc7229cc5a9a0cf05eb9c1f2b86b230fd8df3f78084 \\\n    --hash=sha256:cab66d3380cca3e70939ef2255d01cd8aece6a4907a9528740f668c4b0611861\n\nscikit-image==0.24.0 \\\n    --hash=sha256:18836a18d3a7b6aca5376a2d805f0045826bc6c9fc85331659c33b4813e0b563\n```\n\n----------------------------------------\n\nTITLE: Listing PyYAML Package with Hash Values\nDESCRIPTION: Definition for the PyYAML package dependency with version 6.0.1 and corresponding SHA256 hash values. This entry shows multiple hash values for different builds and platforms of the package.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_32\n\nLANGUAGE: text\nCODE:\n```\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \\\n    --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \\\n    --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \\\n    --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \\\n    --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \\\n    --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \\\n\n```\n\n----------------------------------------\n\nTITLE: Downloading RayCluster YAML Manifest\nDESCRIPTION: Downloads a pre-configured RayCluster YAML manifest for GKE with Cloud Storage bucket access from the KubeRay GitHub repository.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-cluster.gke-bucket.yaml\n```\n\n----------------------------------------\n\nTITLE: Running C++ Tests with Bazel\nDESCRIPTION: Commands to run all C++ tests or specific C++ tests using Bazel\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbazel test $(bazel query 'kind(cc_test, ...)')\nbazel test $(bazel query 'kind(cc_test, ...)') --test_filter=ClientConnectionTest --test_output=streamed\n```\n\n----------------------------------------\n\nTITLE: Submitting Python Scripts to Ray Clusters\nDESCRIPTION: Commands for executing Python scripts on Ray clusters with support for command-line arguments and tmux sessions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-cli.rst#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Run a Python script in a detached tmux session\n$ ray submit cluster.yaml --tmux --start --stop tune_experiment.py\n\n# Run a Python script with arguments.\n# This executes script.py on the head node of the cluster, using\n# the command: python ~/script.py --arg1 --arg2 --arg3\n$ ray submit cluster.yaml script.py -- --arg1 --arg2 --arg3\n```\n\n----------------------------------------\n\nTITLE: Configuring Scheduling Strategy and Data Context in Python\nDESCRIPTION: This snippet demonstrates how to configure a 'SPREAD' scheduling strategy for learners and set up the data context for locality-aware execution in RLlib's Offline RL API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom ray import data\nfrom ray.rllib.algorithms.algorithm_config.AlgorithmConfig\n\n# Configure a \"SPREAD\" scheduling strategy for learners.\nos.environ[\"TRAIN_ENABLE_WORKER_SPREAD_ENV\"] = \"1\"\n\n# Get the current data context.\ndata_context = data.DataContext.get_current()\n# Set the execution options such that the Ray Data tries to match\n# the locality of an output stream with where learners are located.\ndata_context.execution_options = data.ExecutionOptions(\n    locality_with_output=True,\n)\n\n# Build the config.\nconfig = (\n    AlgorithmConfig()\n    .learners(\n        # Scale the learners.\n        num_learners=4,\n        num_gpus_per_learner=2,\n    )\n    .offline_data(\n        ...,\n        # Run in each RLlib training iteration 10\n        # iterations per learner (each of them with\n        # `train_batch_size_per_learner`).\n        dataset_num_iters_per_learner=20,\n    )\n)\n\n# Build the algorithm from the config.\nalgo = config.build()\n\n# Train for 10 iterations.\nfor _ in range(10):\n    res = algo.train()\n```\n\n----------------------------------------\n\nTITLE: Disable Child Task Capture in Placement Group\nDESCRIPTION: This Python snippet demonstrates how to prevent child tasks and actors from being automatically scheduled to the same placement group as their parent. This is achieved by specifying `PlacementGroupSchedulingStrategy(placement_group=None)`.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"        @ray.remote\n        class Child:\n            def ping(self):\n                return os.getpid()\n\n        @ray.remote(max_restarts=0)\n        class Parent:\n            def __init__(self):\n                self.child = Child.options(scheduling_strategy=PlacementGroupSchedulingStrategy(\n                    placement_group=None)).remote()\n\n            def ping(self):\n                return ray.get(self.child.ping.remote())\n\n        pg = ray.util.placement_group([{\"CPU\": 1}])\n        ray.get(pg.ready())\n        p = Parent.remote()\n        assert ray.get(p.ping.remote()) != os.getpid()\"\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Stream Cancellation Output Example\nDESCRIPTION: Terminal output showing the execution of the streaming example, including Ray initialization, server startup, stream processing, and cancellation handling when the client disconnects.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ python stream.py\n[2023-07-10 16:08:41]  INFO ray._private.worker::Started a local Ray instance. View the dashboard at http://127.0.0.1:8265\n(ServeController pid=50801) INFO 2023-07-10 16:08:42,296 controller 40401 deployment_state.py:1259 - Deploying new version of deployment default_StreamingResponder.\n(ProxyActor pid=50803) INFO:     Started server process [50803]\n(ServeController pid=50805) INFO 2023-07-10 16:08:42,963 controller 50805 deployment_state.py:1586 - Adding 1 replica to deployment default_StreamingResponder.\nGot result 0.0s after start: '0'\nGot result 0.1s after start: '1'\nGot result 0.2s after start: '2'\nGot result 0.3s after start: '3'\nGot result 0.4s after start: '4'\nGot result 0.5s after start: '5'\nGot result 0.6s after start: '6'\nGot result 0.7s after start: '7'\nGot result 0.8s after start: '8'\nGot result 0.9s after start: '9'\nGot result 1.0s after start: '10'\nClient disconnecting\n(ServeReplica:default_StreamingResponder pid=50842) Cancelled! Exiting.\n(ServeReplica:default_StreamingResponder pid=50842) INFO 2023-07-10 16:08:45,756 default_StreamingResponder default_StreamingResponder#cmpnmF ahteNDQSWx / default replica.py:691 - __CALL__ OK 1019.1ms\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running Ray on Slurm Cluster\nDESCRIPTION: This Bash script sets up and runs Ray on a Slurm-managed cluster. It defines environment variables, initializes the Ray head node, and launches worker nodes. The script uses Slurm job arrays to manage multiple worker nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/slurm-basic.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n\n#SBATCH --job-name=ray\n#SBATCH --output=ray.out\n#SBATCH --error=ray.err\n\n#SBATCH --nodes=3\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --time=01:00:00\n\n# Load modules or set environment variables if necessary\nmodule load cuda/11.7\n\n# Set environment variables\nexport REDIS_PASSWORD=$(uuidgen)\nexport HEAD_NODE=$(hostname)\n\n# Start the Ray head node\nif [ \"$SLURM_PROCID\" -eq 0 ]; then\n    port=6379\n    ray start --head --node-ip-address=$HEAD_NODE --port=$port --redis-password=$REDIS_PASSWORD\n    sleep infinity\nfi\n\n# Start Ray worker nodes\nif [ \"$SLURM_PROCID\" -gt 0 ]; then\n    ray start --address ${HEAD_NODE}:6379 --redis-password=$REDIS_PASSWORD\n    sleep infinity\nfi\n```\n\n----------------------------------------\n\nTITLE: Resource Configuration YAML Example\nDESCRIPTION: YAML configuration showing resource limits and requests for autoscaler container\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nresources:\n  limits:\n    cpu: \"500m\"\n    memory: \"512Mi\"\n  requests:\n    cpu: \"500m\"\n    memory: \"512Mi\"\n```\n\n----------------------------------------\n\nTITLE: Package Hash List\nDESCRIPTION: List of SHA256 hashes for package dependencies and their versions. Includes hashes for pyzmq version 26.0.3 and related dependencies used via bokeh, dask and distributed packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_30\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \\\n--hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d\n```\n\n----------------------------------------\n\nTITLE: Printing Results of Hyperparameter Optimization in Python\nDESCRIPTION: This snippet prints out the best hyperparameters found during the tuning experiment based on the report from the results object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Deploying a RayCluster with Volcano Scheduler\nDESCRIPTION: Commands to download and apply a RayCluster configuration that uses the Volcano scheduler, then verify the deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Path: kuberay/ray-operator/config/samples\n# Includes label `ray.io/scheduler-name: volcano` in the metadata.labels\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-cluster.volcano-scheduler.yaml\nkubectl apply -f ray-cluster.volcano-scheduler.yaml\n\n# Check the RayCluster\nkubectl get pod -l ray.io/cluster=test-cluster-0\n# NAME                                 READY   STATUS    RESTARTS   AGE\n# test-cluster-0-head-jj9bg            1/1     Running   0          36s\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry SDK Package with Hash Verification\nDESCRIPTION: Defines the opentelemetry-sdk package version 1.1.0 with SHA256 hash verification and notes that it's required by multiple requirement files including cloud-requirements.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_20\n\nLANGUAGE: text\nCODE:\n```\nopentelemetry-sdk==1.1.0 \\\n    --hash=sha256:ba29274aab656572e97e0339afaad6f2bded4102324b1475ab7412079498df6e \\\n    --hash=sha256:da7dfa6188e8a39f34b99495260e6a1d398c86a9de064c7f0805db6f16733d94\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\n```\n\n----------------------------------------\n\nTITLE: Writing Artifacts to Trial Directory in Python\nDESCRIPTION: This snippet demonstrates how to write artifacts to the trial directory in a Ray Tune trainable. It uses the current working directory, which is set to the trial directory by default.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwith open(f\"./artifact_{self.iteration}.txt\", \"w\") as f:\n    f.write(\"Artifact Data\")\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset from Arrow Table - Ray\nDESCRIPTION: This snippet shows how to create a Ray dataset from an Arrow table using the `ray.data.from_arrow` function. It demonstrates the conversion process for a dataset backed by Arrow data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nimport pyarrow as pa\n\ntable = pa.table({\n    \"food\": [\"spam\", \"ham\", \"eggs\"],\n    \"price\": [9.34, 5.37, 0.94]\n})\nds = ray.data.from_arrow(table)\n\nprint(ds)\n```\n\n----------------------------------------\n\nTITLE: Remote Function Invocation with Ray - First Example\nDESCRIPTION: In this example, a remote function `no_work` is invoked multiple times using a large input array, demonstrating performance issues associated with repeated object copying to the Ray object store. It underscores the cost of invoking remote functions with large objects.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport numpy as np\nimport ray\n\n@ray.remote\ndef no_work(a):\n    return\n\nstart = time.time()\na = np.zeros((5000, 5000))\nresult_ids = [no_work.remote(a) for x in range(10)]\nresults = ray.get(result_ids)\nprint(\"duration =\", time.time() - start)\n```\n\n----------------------------------------\n\nTITLE: Implementing Spread Scheduling Strategy in Ray\nDESCRIPTION: This code demonstrates how to use the SPREAD scheduling strategy in Ray. It defines a remote function with the SPREAD strategy and executes it, causing Ray to distribute tasks across available nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/index.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(scheduling_strategy=\"SPREAD\")\ndef f():\n    return 1\n\nray.get(f.remote())\n```\n\n----------------------------------------\n\nTITLE: Constructing RLModule Object with Gym Environment\nDESCRIPTION: Creates an instance of DefaultBCTorchRLModule, demonstrating how to initialize an RLModule with environment observation and action spaces, and a model configuration. The code uses the Gymnasium library to create the environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom ray.rllib.algorithms.bc.torch.default_bc_torch_rl_module import DefaultBCTorchRLModule\n\n# Create an env object to know the spaces.\nenv = gym.make(\"CartPole-v1\")\n\n# Construct the actual RLModule object.\nrl_module = DefaultBCTorchRLModule(\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    # A custom dict that's accessible inside your class as `self.model_config`.\n    model_config={\"fcnet_hiddens\": [64]},\n)\n```\n\n----------------------------------------\n\nTITLE: Making Environment in SingleAgentEnvRunner - Python\nDESCRIPTION: This method creates or resets the environment used by the agent. It is essential for preparing the agent to start a new episode or reset its state to the initial configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_env_runner.rst#2025-04-12_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef make_env(self):\n    # Create the environment for the agent\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Operator and Creating RayCluster with TLS\nDESCRIPTION: Commands to download the ray-cluster.tls.yaml file and create a RayCluster with TLS authentication enabled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/tls.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install KubeRay operator\n# `ray-cluster.tls.yaml` will cover from Step 1 to Step 3\n\n# Download `ray-cluster.tls.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-cluster.tls.yaml\n\n# Create a RayCluster\nkubectl apply -f ray-cluster.tls.yaml\n\n# Jump to Step 4 \"Verify TLS authentication\" to verify the connection.\n```\n\n----------------------------------------\n\nTITLE: Creating Detached Counter Actor in Python\nDESCRIPTION: Python script that creates a detached Ray actor named 'counter_actor' with a simple increment functionality. The actor is initialized in the 'default_namespace' and set as detached for persistence.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n@ray.remote(num_cpus=1)\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\nray.init(namespace=\"default_namespace\")\nCounter.options(name=\"counter_actor\", lifetime=\"detached\").remote()\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Train for Distributed Training in Python\nDESCRIPTION: Import statement for Ray Train, which is required to enable distributed training functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport ray.train as train\n```\n\n----------------------------------------\n\nTITLE: Preparing Ray Serve Application Code\nDESCRIPTION: Commands to clone the repository containing Ray Serve example applications and attempt to run the MobileNet classifier. The initial attempt fails due to missing TensorFlow dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# Execute the following command in the head Pod\ngit clone https://github.com/ray-project/serve_config_examples.git\ncd serve_config_examples\n\n# Try to launch the Ray Serve application\nserve run mobilenet.mobilenet:app\n# [Error message]\n#     from tensorflow.keras.preprocessing import image\n# ModuleNotFoundError: No module named 'tensorflow'\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Ray Dashboard\nDESCRIPTION: Command to forward the Ray Dashboard port to local machine, enabling access to profiling data through the dashboard interface.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/pyspy.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward svc/raycluster-py-spy-head-svc 8265:8265\n```\n\n----------------------------------------\n\nTITLE: Register Ray Head Address\nDESCRIPTION: Command to store the Ray head node address in Skein's key-value store.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/yarn.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nskein kv put --key=RAY_HEAD_ADDRESS --value=$(hostname -i) current\n```\n\n----------------------------------------\n\nTITLE: Deleting Ray Jobs with kubectl-ray\nDESCRIPTION: Commands to delete Ray jobs using the kubectl-ray plugin.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kubectl-plugin.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl ray delete rayjob/rayjob-sample\nkubectl ray delete rayjob/rayjob-interactive-mode\n```\n\n----------------------------------------\n\nTITLE: Waiting for RayCluster Provisioning\nDESCRIPTION: Waits for the RayCluster to reach the provisioned state with a timeout\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nkubectl wait --for=condition=RayClusterProvisioned raycluster/raycluster-kuberay --timeout=500s\n```\n\n----------------------------------------\n\nTITLE: Setting Databricks Host Environment Variable\nDESCRIPTION: This command sets the `DATABRICKS_HOST` environment variable, which is required when not running on the Databricks runtime.  Replace `<workspace-id>` and `<random-number>` with the actual values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_30\n\nLANGUAGE: console\nCODE:\n```\nexport DATABRICKS_HOST=adb-<workspace-id>.<random-number>.azuredatabricks.net\n```\n\n----------------------------------------\n\nTITLE: Terminating Ray Cluster\nDESCRIPTION: Shows how to shut down a Ray cluster using the ray down command with the -y flag for automatic confirmation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/getting-started.rst#2025-04-12_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n$ ray down -y config.yaml\n```\n\n----------------------------------------\n\nTITLE: Using ray stack CLI command to dump stack traces\nDESCRIPTION: The ray stack command can be used to dump stack traces of all Ray Worker processes on the current node once py-spy is installed. This tool is automatically installed if the Ray Dashboard component is included during Ray installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-hangs.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nray stack\n```\n\n----------------------------------------\n\nTITLE: Viewing Pending Pods Due to YuniKorn Gang Scheduling\nDESCRIPTION: Command to list Pods showing the second RayCluster's Pods in Pending state due to insufficient resources in the YuniKorn queue for gang scheduling all the Pods together.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl get pods\n\nNAME                                      READY   STATUS    RESTARTS   AGE\ntest-yunikorn-0-head-98fmp                1/1     Running   0          4m22s\ntest-yunikorn-0-worker-worker-42tgg       1/1     Running   0          4m22s\ntest-yunikorn-0-worker-worker-467mn       1/1     Running   0          4m22s\ntest-yunikorn-1-head-xl2r5                0/1     Pending   0          71s\ntest-yunikorn-1-worker-worker-l6ttz       0/1     Pending   0          71s\ntest-yunikorn-1-worker-worker-vjsts       0/1     Pending   0          71s\ntg-test-yunikorn-1-headgroup-vgzvoot0dh   0/1     Pending   0          69s\ntg-test-yunikorn-1-worker-eyti2bn2jv      1/1     Running   0          69s\ntg-test-yunikorn-1-worker-k8it0x6s73      0/1     Pending   0          69s\n```\n\n----------------------------------------\n\nTITLE: GCS Fault Tolerance YAML Configuration\nDESCRIPTION: YAML configuration examples for enabling GCS fault tolerance and connecting to external Redis in KubeRay.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nkind: RayCluster\nmetadata:\nspec:\n  gcsFaultToleranceOptions: # <- Add this field to enable GCS fault tolerance.\n```\n\nLANGUAGE: yaml\nCODE:\n```\nkind: RayCluster\nmetadata:\nspec:\n  gcsFaultToleranceOptions:\n    redisAddress: \"redis:6379\" # <- Add redis address here.\n```\n\nLANGUAGE: yaml\nCODE:\n```\nkind: RayCluster\nmetadata:\nspec:\n  gcsFaultToleranceOptions:\n    redisAddress: \"redis:6379\"\n    redisPassword: # <- Add redis password from a Kubernetes secret.\n      valueFrom:\n        secretKeyRef:\n          name: redis-password-secret\n          key: password\n```\n\nLANGUAGE: yaml\nCODE:\n```\nkind: RayCluster\nmetadata:\nspec:\n  gcsFaultToleranceOptions:\n    externalStorageNamespace: \"my-raycluster-storage\" # <- Add this option to specify a storage namespace\n```\n\n----------------------------------------\n\nTITLE: Checking Ray Cluster Status\nDESCRIPTION: Shell command to execute 'ray status' on the cluster, providing information about autoscaling status and Ray resource usage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/examples/ml-example.md#2025-04-12_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nray exec cluster.yaml 'ray status'\n```\n\n----------------------------------------\n\nTITLE: Referencing GroupedData API Documentation in reStructuredText\nDESCRIPTION: This snippet sets up the documentation page for the GroupedData API in Ray Data. It includes a reference label, defines the current module, and links to the full GroupedData class documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/grouped_data.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _grouped-dataset-api:\n\nGroupedData API\n===============\n\n.. currentmodule:: ray.data\n\nGroupedData objects are returned by groupby call:\n:meth:`Dataset.groupby() <ray.data.Dataset.groupby>`.\n\n.. include:: ray.data.grouped_data.GroupedData.rst\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM Dependency\nDESCRIPTION: Installation command for vLLM package required for LLM inference\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-llms.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U vllm==0.7.2\n```\n\n----------------------------------------\n\nTITLE: Configuring GCSFuse Pod Annotations\nDESCRIPTION: This YAML snippet shows how to configure Pod annotations for finer-grained control of the GCSFuse sidecar container, including CPU, memory, and storage limits.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nannotations:\n  gke-gcsfuse/volumes: \"true\"\n  gke-gcsfuse/cpu-limit: \"0\"\n  gke-gcsfuse/memory-limit: 5Gi\n  gke-gcsfuse/ephemeral-storage-limit: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus and Grafana for KubeRay\nDESCRIPTION: Executes a shell script to install Prometheus and Grafana for monitoring the KubeRay benchmark.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# Path: kuberay/\n./install/prometheus/install.sh\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray\nDESCRIPTION: Initializes Ray with logging disabled. This step is necessary before running any Ray-based computations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"ray.init(configure_logging=False)\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Cluster Launcher\nDESCRIPTION: Command to install Ray with cluster launcher support using pip.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[default]\"\n```\n\n----------------------------------------\n\nTITLE: Using Ray Objects in Workflows\nDESCRIPTION: Demonstrates how to use Ray objects within workflows, including creating and passing ObjectRefs between tasks, and working with lists of ObjectRefs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/key-concepts.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom typing import List\n\n@ray.remote\ndef hello():\n    return \"hello\"\n\n@ray.remote\ndef words() -> List[ray.ObjectRef]:\n    # NOTE: Here it is \".remote()\" instead of \".bind()\", so\n    # it creates an ObjectRef instead of a DAG.\n    return [hello.remote(), ray.put(\"world\")]\n\n@ray.remote\ndef concat(words: List[ray.ObjectRef]) -> str:\n    return \" \".join([ray.get(w) for w in words])\n\nassert workflow.run(concat.bind(words.bind())) == \"hello world\"\n```\n\n----------------------------------------\n\nTITLE: Ray Memory Output for Captured References\nDESCRIPTION: Output of 'ray memory' command showing how object references are captured inside other objects, demonstrating the CAPTURED_IN_OBJECT reference type for nested object references.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/memory-management.rst#2025-04-12_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n--- Summary for node address: 192.168.0.15 ---\nMem Used by Objects  Local References  Pinned Count  Pending Tasks  Captured in Objects  Actor Handles\n233 MiB              1                 0             0              1                    0\n\n--- Object references for node address: 192.168.0.15 ---\nIP Address    PID    Type    Object Ref                                                Size    Reference Type      Call Site\n192.168.0.15  7473   Driver  ffffffffffffffffffffffffffffffffffffffff0100000001000000  15 MiB  CAPTURED_IN_OBJECT  (put object)  |\n                                                                                                                  test.py:\n                                                                                                                  <module>:41\n\n192.168.0.15  7473   Driver  ffffffffffffffffffffffffffffffffffffffff0100000002000000  218 MiB  LOCAL_REFERENCE     (put object)  |\n                                                                                                                  test.py:\n                                                                                                                  <module>:42\n```\n\n----------------------------------------\n\nTITLE: LoRA Fine-tuning Launch\nDESCRIPTION: Command to start LoRA fine-tuning for 7B model\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./run_llama_ft.sh --size=7b --lora\n```\n\n----------------------------------------\n\nTITLE: Customizing Ray Tune Log File Names\nDESCRIPTION: Demonstrates ways to specify custom file names for Tune logs, either as a single combined file or separate files for stdout and stderr.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    trainable,\n    run_config=RunConfig(log_to_file=\"std_combined.log\")\n)\ntuner.fit()\n\ntuner = tune.Tuner(\n    trainable,\n    run_config=RunConfig(log_to_file=(\"my_stdout.log\", \"my_stderr.log\")))\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Execution Statistics in Ray Datasets\nDESCRIPTION: This code demonstrates how to view execution statistics for a Ray Dataset pipeline, including operator details, task execution times, memory usage, and throughput metrics using the stats() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/inspecting-data.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport datasets\n\ndef f(batch):\n    return batch\n\ndef g(row):\n    return True\n\nhf_ds = datasets.load_dataset(\"mnist\", \"mnist\")\nds = (\n    ray.data.from_huggingface(hf_ds[\"train\"])\n    .map_batches(f)\n    .filter(g)\n    .materialize()\n)\n\nprint(ds.stats())\n```\n\n----------------------------------------\n\nTITLE: Creating Nodes in Ray Cluster (Python)\nDESCRIPTION: Initiates the creation of nodes by calling the create_node function, which internally calls _create_node.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/vsphere/ARCHITECTURE.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncreate_node\n```\n\n----------------------------------------\n\nTITLE: Configuring Volcano Support in values.yaml\nDESCRIPTION: YAML configuration to enable Volcano batch scheduler support in the KubeRay Operator's values.yaml file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# values.yaml file\nbatchScheduler:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Cluster Status\nDESCRIPTION: Series of kubectl commands to check the status of RayCluster, Pods, and RayJob.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl wait --for=condition=RayClusterProvisioned raycluster/$(kubectl get rayjob rayjob-sample -o jsonpath='{.status.rayClusterName}') --timeout=500s\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl wait --for=condition=ready pod -l job-name=rayjob-sample --timeout=500s\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get rayjob\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get raycluster\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods --sort-by='.metadata.creationTimestamp'\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl wait --for=condition=complete job/rayjob-sample --timeout=500s\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get rayjobs.ray.io rayjob-sample -o jsonpath='{.status.jobStatus}'\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get rayjobs.ray.io rayjob-sample -o jsonpath='{.status.jobDeploymentStatus}'\n```\n\n----------------------------------------\n\nTITLE: Running serve run on Remote Cluster\nDESCRIPTION: This bash command executes `serve run` on a remote Ray cluster, specifying the cluster address and working directory. This setup is required for deploying applications on remote environments with similar configurations to local machines.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dev-workflow.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n\"serve run  --address=ray://<head-node-ip-address>:10001 --working-dir=\\\"./project/src\\\" local_dev:app\"\n```\n\n----------------------------------------\n\nTITLE: Converting Strategy to Ray Serve Deployment\nDESCRIPTION: Implementation of StrategyOnRayServe class that adapts the original Strategy logic for distributed computing using Ray Serve.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/java.md#2025-04-12_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic class StrategyOnRayServe {\n    public double calc(long time, String bank, String indicator) {\n        return Math.random();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: GPU-Enabled Batch Prediction Task\nDESCRIPTION: Demonstrates how to implement a GPU-enabled prediction task using PyTorch and Ray's GPU resource allocation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/batch_prediction.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@ray.remote(num_gpus=1)\ndef make_torch_prediction(model: torch.nn.Module, shard_path):\n    # Move model to GPU.\n    model.to(torch.device(\"cuda\"))\n    inputs = pq.read_table(shard_path).to_pandas().to_numpy()\n\n    results = []\n    # for each tensor in inputs:\n    #   results.append(model(tensor))\n    #\n    # Write out the results right in task instead of returning back\n    # to the driver node (unless you have to), to avoid congest/overload\n    # driver node.\n    # ...\n\n    # Here we just return simple/light meta information.\n    return len(results)\n```\n\n----------------------------------------\n\nTITLE: Running DreamBooth Fine-Tuning Script in Bash\nDESCRIPTION: This snippet shows how to execute the DreamBooth fine-tuning script using bash commands. It sets the script as executable and runs it with default settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/README.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x ./dreambooth_run.sh\n./dreambooth_run.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Anyscale and Initializing Project for Ray Distributed Tests\nDESCRIPTION: These commands install the Anyscale package and initialize the project for running long-running distributed tests.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/long_running_distributed_tests/README.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install anyscale\n$ anyscale init\n```\n\n----------------------------------------\n\nTITLE: CLI Deployment Command - Console\nDESCRIPTION: Command line instruction to deploy the text generator service using Serve CLI\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/batch.md#2025-04-12_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ serve run tutorial_batch:generator --name \"Text-Completion-App\"\n```\n\n----------------------------------------\n\nTITLE: Installing RayService for MobileNet Classifier\nDESCRIPTION: This command applies a Kubernetes YAML configuration to create a RayService for the MobileNet image classifier.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/mobilenet-rayservice.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-service.mobilenet.yaml\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Versions and Hashes\nDESCRIPTION: This code snippet shows how to specify Python package dependencies in a requirements file, including exact versions and hash values for security. It demonstrates the format for multiple packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nargon2-cffi==23.1.0 \\\n    --hash=sha256:879c3e79a2729ce768ebb7d36d4609e3a78a4ca2ec3a9f12286ca057e3d0db08 \\\n    --hash=sha256:c670642b78ba29641818ab2e68bd4e6a78ba53b7eff7b4c3815ae16abf91c7ea\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jupyter-server\n    #   nbclassic\n    #   notebook\nargon2-cffi-bindings==21.2.0 \\\n    --hash=sha256:20ef543a89dee4db46a1a6e206cd015360e5a75822f76df533845c3cbaf72670 \\\n    --hash=sha256:2c3e3cc67fdb7d82c4718f19b4e7a87123caf8a93fde7e23cf66ac0337d3cb3f \\\n    --hash=sha256:3b9ef65804859d335dc6b31582cad2c5166f0c3e7975f324d9ffaa34ee7e6583 \\\n    --hash=sha256:3e385d1c39c520c08b53d63300c3ecc28622f076f4c2b0e6d7e796e9f6502194 \\\n    --hash=sha256:58ed19212051f49a523abb1dbe954337dc82d947fb6e5a0da60f7c8471a8476c \\\n    --hash=sha256:5e00316dabdaea0b2dd82d141cc66889ced0cdcbfa599e8b471cf22c620c329a \\\n    --hash=sha256:603ca0aba86b1349b147cab91ae970c63118a0f30444d4bc80355937c950c082 \\\n    --hash=sha256:6a22ad9800121b71099d0fb0a65323810a15f2e292f2ba450810a7316e128ee5 \\\n    --hash=sha256:8cd69c07dd875537a824deec19f978e0f2078fdda07fd5c42ac29668dda5f40f \\\n    --hash=sha256:93f9bf70084f97245ba10ee36575f0c3f1e7d7724d67d8e5b08e61787c320ed7 \\\n    --hash=sha256:9524464572e12979364b7d600abf96181d3541da11e23ddf565a32e70bd4dc0d \\\n    --hash=sha256:b2ef1c30440dbbcba7a5dc3e319408b59676e2e039e2ae11a8775ecf482b192f \\\n    --hash=sha256:b746dba803a79238e925d9046a63aa26bf86ab2a2fe74ce6b009a1c3f5c8f2ae \\\n    --hash=sha256:bb89ceffa6c791807d1305ceb77dbfacc5aa499891d2c55661c6459651fc39e3 \\\n    --hash=sha256:bd46088725ef7f58b5a1ef7ca06647ebaf0eb4baff7d1d0d177c6cc8744abd86 \\\n    --hash=sha256:ccb949252cb2ab3a08c02024acb77cfb179492d5701c7cbdbfd776124d4d2367 \\\n    --hash=sha256:d4966ef5848d820776f5f562a7d45fdd70c2f330c961d0d745b784034bd9f48d \\\n    --hash=sha256:e415e3f62c8d124ee16018e491a009937f8cf7ebf5eb430ffc5de21b900dad93 \\\n    --hash=sha256:ed2937d286e2ad0cc79a7087d3c272832865f779430e0cc2b4f3718d3159b0cb \\\n    --hash=sha256:f1152ac548bd5b8bcecfb0b0371f082037e47128653df2e8ba6e914d384f3c3e \\\n    --hash=sha256:f9f8b450ed0547e3d473fdc8612083fd08dd2120d6ac8f73828df9b7d45bb351\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   argon2-cffi\n```\n\n----------------------------------------\n\nTITLE: Verifying RayJob Deployment Status\nDESCRIPTION: Commands to check the status of deployed RayJob and associated Kubernetes resources. These commands help in monitoring the job's progress and cluster state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pod\nkubectl get rayjobs.ray.io dev-pytorch-text-classifier-r6d4p -o jsonpath='{.status.jobStatus}'\nkubectl get rayjobs.ray.io dev-pytorch-text-classifier-r6d4p -o jsonpath='{.status.jobDeploymentStatus}'\n```\n\n----------------------------------------\n\nTITLE: Defining Authentication Settings for Ray Cluster in YAML\nDESCRIPTION: This YAML structure outlines authentication settings for different cloud providers (AWS, Azure, GCP, vSphere) in a Ray cluster configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# AWS\nssh_user: str\nssh_private_key: str\n\n# Azure\nssh_user: str\nssh_private_key: str\nssh_public_key: str\n\n# GCP\nssh_user: str\nssh_private_key: str\n\n# vSphere\nssh_user: str\n```\n\n----------------------------------------\n\nTITLE: RayService Status Output for LLM Deployment\nDESCRIPTION: Expected YAML output showing a healthy RayService deployment with the LLM application running and the VLLMDeployment in a healthy state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/vllm-rayservice.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nstatus:\n  activeServiceStatus:\n    applicationStatuses:\n      llm:\n        healthLastUpdateTime: \"2024-08-08T22:56:50Z\"\n        serveDeploymentStatuses:\n          VLLMDeployment:\n            healthLastUpdateTime: \"2024-08-08T22:56:50Z\"\n            status: HEALTHY\n        status: RUNNING\n```\n\n----------------------------------------\n\nTITLE: Killing Deployment Replica using Python\nDESCRIPTION: This code shows how to simulate a deployment replica failure by getting a handle to a specific replica actor and killing it using Ray's Python API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> import ray\n>>> replica_handle = ray.get_actor(\"SERVE_REPLICA::SleepyPid#RlRptP\", namespace=\"serve\")\n>>> ray.kill(replica_handle, no_restart=True)\n>>> exit()\n```\n\n----------------------------------------\n\nTITLE: Basic Trial Parallelism in Ray Tune\nDESCRIPTION: Demonstrates default concurrent trial execution based on available CPU cores. Will run N concurrent trials where N is the number of CPUs on the machine.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-resources.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    trainable,\n    tune_config=tune.TuneConfig(num_samples=10)\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Command-Line Interface for Training Configuration in Python\nDESCRIPTION: This snippet demonstrates how to parse command-line arguments for the training configuration such as mode, learning rate, and number of workers using argparse, including smoke test conditions for quick execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/horovod_simple.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--mode\", type=str, default=\"square\", choices=[\"square\", \"cubic\"]\n    )\n    parser.add_argument(\n        \"--learning_rate\", type=float, default=0.1, dest=\"learning_rate\"\n    )\n    parser.add_argument(\"--x_max\", type=float, default=1.0, dest=\"x_max\")\n    parser.add_argument(\"--gpu\", action=\"store_true\")\n    parser.add_argument(\n        \"--smoke-test\", action=\"store_true\", help=(\"Finish quickly for testing.\")\n    )\n    parser.add_argument(\"--num-workers\", type=int, default=2)\n    args, _ = parser.parse_known_args()\n\n    if args.smoke_test:\n        ray.init(num_cpus=4)\n\n    tune_horovod(\n        num_workers=args.num_workers,\n        num_samples=2 if args.smoke_test else 10,\n        use_gpu=args.gpu,\n        mode=args.mode,\n        x_max=args.x_max,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents in Sphinx Documentation\nDESCRIPTION: Sphinx documentation toctree directive that organizes the documentation structure for Ray Jobs, including quickstart, SDK, package references, CLI, REST, and Ray client documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/index.md#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{toctree}\n:maxdepth: '1'\n\nquickstart\nsdk\njobs-package-ref\ncli\nrest\nray-client\n```\n\n----------------------------------------\n\nTITLE: Installing psutil with Pinned Version and Hashes\nDESCRIPTION: Specifies psutil package with version 5.9.6 and SHA256 hashes for verification. Comments indicate this is required by distributed, locust, and petastorm packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_20\n\nLANGUAGE: pip\nCODE:\n```\npsutil==5.9.6 \\\n    --hash=sha256:10e8c17b4f898d64b121149afb136c53ea8b68c7531155147867b7b1ac9e7e28 \\\n    --hash=sha256:18cd22c5db486f33998f37e2bb054cc62fd06646995285e02a51b1e08da97017 \\\n    --hash=sha256:3ebf2158c16cc69db777e3c7decb3c0f43a7af94a60d72e87b2823aebac3d602 \\\n    --hash=sha256:51dc3d54607c73148f63732c727856f5febec1c7c336f8f41fcbd6315cce76ac \\\n    --hash=sha256:6e5fb8dc711a514da83098bc5234264e551ad980cec5f85dabf4d38ed6f15e9a \\\n    --hash=sha256:70cb3beb98bc3fd5ac9ac617a327af7e7f826373ee64c80efd4eb2856e5051e9 \\\n    --hash=sha256:748c9dd2583ed86347ed65d0035f45fa8c851e8d90354c122ab72319b5f366f4 \\\n    --hash=sha256:91ecd2d9c00db9817a4b4192107cf6954addb5d9d67a969a4f436dbc9200f88c \\\n    --hash=sha256:92e0cc43c524834af53e9d3369245e6cc3b130e78e26100d1f63cdb0abeb3d3c \\\n    --hash=sha256:a6f01f03bf1843280f4ad16f4bde26b817847b4c1a0db59bf6419807bc5ce05c \\\n    --hash=sha256:c69596f9fc2f8acd574a12d5f8b7b1ba3765a641ea5d60fb4736bf3c08a8214a \\\n    --hash=sha256:ca2780f5e038379e520281e4c032dddd086906ddff9ef0d1b9dcf00710e5071c \\\n    --hash=sha256:daecbcbd29b289aac14ece28eca6a3e60aa361754cf6da3dfb20d4d32b6c7f57 \\\n    --hash=sha256:e4b92ddcd7dd4cdd3f900180ea1e104932c7bce234fb88976e2a3b296441225a \\\n    --hash=sha256:fb8a697f11b0f5994550555fcfe3e69799e5b060c8ecf9e2f75c69302cc35c0d \\\n    --hash=sha256:ff18b8d1a784b810df0b0fff3bcb50ab941c3b8e2c8de5726f9c71c601c611aa\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   distributed\n    #   locust\n    #   petastorm\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Client Connection in Python\nDESCRIPTION: Examples of connecting to a Ray cluster from the same Kubernetes namespace and from a different namespace using ray.init().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/config.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nray.init(\"ray://raycluster-example-head-svc:10001\")\n```\n\nLANGUAGE: python\nCODE:\n```\nray.init(\"ray://raycluster-example-head-svc.default.svc.cluster.local:10001\")\n```\n\n----------------------------------------\n\nTITLE: Deploying Network Policy for Static Ray Cluster\nDESCRIPTION: Applies a Kubernetes network policy to allow bidirectional communication among Ray cluster nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster-networkpolicy.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret with CA Credentials\nDESCRIPTION: This command creates a Kubernetes secret named 'ca-tls' using CA certificate and key files. Prerequisites include having the CA certificate (ca.crt) and key (ca.key) files accessible at specified paths. This secret is used to encode and manage the CA keypair securely in a Kubernetes environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/configure.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic ca-tls --from-file=ca.crt=<path-to-ca.crt> --from-file=ca.key=<path-to-ca.key>\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Cluster on AWS\nDESCRIPTION: This snippet shows the initialization commands run on the head node of the Ray cluster. It sets environment variables and starts the Ray runtime.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_up_record.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport RAY_USAGE_STATS_ENABLED=1;export RAY_OVERRIDE_RESOURCES='{\"CPU\":1}';export RAY_OVERRIDE_LABELS='{\"key1\":\"value1\"}';ray stop\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport RAY_USAGE_STATS_ENABLED=1;export RAY_OVERRIDE_RESOURCES='{\"CPU\":1}';export RAY_OVERRIDE_LABELS='{\"key1\":\"value1\"}';ray start --head --autoscaling-config=~/ray_bootstrap_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Ray Cluster Management Commands\nDESCRIPTION: Lists useful commands for managing the Ray cluster, including termination, IP retrieval, dashboard access, job submission, debugging, and monitoring.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_up.txt#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n  To terminate the cluster:\n    ray down .+\n\n  To retrieve the IP address of the cluster head:\n    ray get-head-ip .+\n\n  To port-forward the cluster's Ray Dashboard to the local machine:\n    ray dashboard .+\n\n  To submit a job to the cluster, port-forward the Ray Dashboard in another terminal and run:\n    ray job submit --address .+\n\n  To connect to a terminal on the cluster head for debugging:\n    ray attach .+\n\n  To monitor autoscaling:\n    ray exec .+ 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'\n```\n\n----------------------------------------\n\nTITLE: Ray Worker Task Execution Error Trace\nDESCRIPTION: This code snippet shows the full stack trace of a SystemExit exception occurring in a Ray worker process during task execution. The error originates from the 'ping' method in a stress test for dead actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.6.0/stress_tests/dead_actors.txt#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nTraceback (most recent call last):\n  File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n  File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"stress_tests/test_dead_actors.py\", line 28, in ping\nSystemExit: -1\n```\n\n----------------------------------------\n\nTITLE: Specifying Ray Modules for Local Development in Docker Containers\nDESCRIPTION: Configure environment variables to mount specific Ray modules within the node containers for local development. This allows for customization of which top-level Ray directories should be accessible in the Docker setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/fake-autoscaler.rst#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nFAKE_CLUSTER_DEV_MODULES=\"autoscaler,tune\"\n```\n\n----------------------------------------\n\nTITLE: Reporting Intermediate Metrics with Function API in Python\nDESCRIPTION: Example of using the Function API to train a model and report intermediate metrics at each step using tune.report(). This allows for tracking progress during training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_func(config):\n    for step in range(100):\n        # train the model...\n        score = objective(step, config[\"a\"], config[\"b\"])\n        tune.report(score=score)\n```\n\n----------------------------------------\n\nTITLE: Implementing Transform Task in Airflow TaskFlow API\nDESCRIPTION: Creates a transform task that processes the order data dictionary and calculates the total order value.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/workflow/examples/comparisons/airflow/etl_airflow.py.txt#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@task(multiple_outputs=True)\ndef transform(order_data_dict: dict):\n    \"\"\"\n    #### Transform task\n    A simple Transform task which takes in the collection of order data and\n    computes the total order value.\n    \"\"\"\n    total_order_value = 0\n\n    for value in order_data_dict.values():\n        total_order_value += value\n\n    return {\"total_order_value\": total_order_value}\n```\n\n----------------------------------------\n\nTITLE: Specifying google-cloud-storage Package Dependency with Hash Values\nDESCRIPTION: Definition of the google-cloud-storage package dependency at version 2.14.0 with associated hash values for verification. The comment indicates this package is required by the cloud requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_13\n\nLANGUAGE: text\nCODE:\n```\ngoogle-cloud-storage==2.14.0 \\\n    --hash=sha256:2d23fcf59b55e7b45336729c148bb1c464468c69d5efbaee30f7201dd90eb97e \\\n    --hash=sha256:8641243bbf2a2042c16a6399551fbb13f062cbc9a2de38d6c0bb5426962e9dbd\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Pathspec Package with Hash Verification in Bash\nDESCRIPTION: Defines the pathspec package version 0.11.2 with SHA256 hash verification for secure package installation. Required by cloud-requirements.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\npathspec==0.11.2 \\\n    --hash=sha256:1d6ed233af05e679efb96b1851550ea95bbb64b7c490b0f5aa52996c11e92a20 \\\n    --hash=sha256:e0d8d0ac2f12da61956eb2306b69f9469b42f4deb0f3cb6ed47b9cce9996ced3\n```\n\n----------------------------------------\n\nTITLE: Overloaded Function Handling in Ray - Java\nDESCRIPTION: Shows how to work with overloaded Java methods in Ray, including both normal task calls and actor method calls. Demonstrates proper type casting for method references to handle overloading.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/miscellaneous.rst#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic static class MyRayApp {\n  public static int overloadFunction() {\n    return 1;\n  }\n\n  public static int overloadFunction(int x) {\n    return x;\n  }\n}\n\n// Invoke overloaded functions.\nAssert.assertEquals((int) Ray.task((RayFunc0<Integer>) MyRayApp::overloadFunction).remote().get(), 1);\nAssert.assertEquals((int) Ray.task((RayFunc1<Integer, Integer>) MyRayApp::overloadFunction, 2).remote().get(), 2);\n```\n\n----------------------------------------\n\nTITLE: Viewing Fine-tuning Options\nDESCRIPTION: Command to display all available options for the fine-tuning script\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython finetune_hf_llm.py --help\n```\n\n----------------------------------------\n\nTITLE: Running the Experiment with Initial Evaluations in Python\nDESCRIPTION: This snippet runs the tuning process while using the provided initial hyperparameters for evaluation, similar to previous experiments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n        metric=\"mean_loss\",\n        mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space=search_space,\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Task-level Metadata in Python\nDESCRIPTION: Example showing how to retrieve metadata for an individual workflow task by providing the task_id. This demonstrates how to set a specific task ID using workflow options and then access task-specific metadata.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/metadata.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nworkflow.run(\n    add.options(\n        **workflow.options(task_id=\"add_task\")\n    ).bind(10, 20), workflow_id=\"add_example_2\")\n\ntask_metadata = workflow.get_metadata(\"add_example_2\", task_id=\"add_task\")\n\nassert \"start_time\" in workflow_metadata[\"stats\"]\nassert \"end_time\" in workflow_metadata[\"stats\"]\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Deployment Startup Logs\nDESCRIPTION: These logs show the startup process of the Ray Serve deployment, including initialization of the DeepSpeed workers and loading of the model checkpoints.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n2025-03-03 06:21:57,692 INFO scripts.py:494 -- Running import path: 'infer-ds:entrypoint'.\n2025-03-03 06:22:03,064 INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 \nINFO 2025-03-03 06:22:07,343 serve 170212 -- Started Serve in namespace \"serve\".\nINFO 2025-03-03 06:22:07,343 serve 170212 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n(ServeController pid=170719) INFO 2025-03-03 06:22:07,377 controller 170719 -- Deploying new version of Deployment(name='DeepSpeedLlamaModel', app='default') (initial target replicas: 1).\n(ProxyActor pid=170723) INFO 2025-03-03 06:22:07,290 proxy 100.83.111.228 -- Proxy starting on node 47721c925467a877497e66104328bb72dc7bd7f900a63b2f1fdb48b2 (HTTP port: 8000).\n(ProxyActor pid=170723) INFO 2025-03-03 06:22:07,325 proxy 100.83.111.228 -- Got updated endpoints: {}.\n(ProxyActor pid=170723) INFO 2025-03-03 06:22:07,379 proxy 100.83.111.228 -- Got updated endpoints: {Deployment(name='DeepSpeedLlamaModel', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n(ServeController pid=170719) INFO 2025-03-03 06:22:07,478 controller 170719 -- Adding 1 replica to Deployment(name='DeepSpeedLlamaModel', app='default').\n(ProxyActor pid=170723) INFO 2025-03-03 06:22:07,422 proxy 100.83.111.228 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7fa557945210>.\n(DeepSpeedInferenceWorker pid=179962) [WARNING|utils.py:212] 2025-03-03 06:22:14,611 >> optimum-habana v1.15.0 has been validated for SynapseAI v1.19.0 but habana-frameworks v1.20.0.543 was found, this could lead to undefined behavior!\n(DeepSpeedInferenceWorker pid=179963) /usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n(DeepSpeedInferenceWorker pid=179963)   warnings.warn(\n(DeepSpeedInferenceWorker pid=179964) [WARNING|utils.py:212] 2025-03-03 06:22:14,613 >> optimum-habana v1.15.0 has been validated for SynapseAI v1.19.0 but habana-frameworks v1.20.0.543 was found, this could lead to undefined behavior! [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(DeepSpeedInferenceWorker pid=179962) [2025-03-03 06:22:23,502] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to hpu (auto detect)\nLoading 2 checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n(DeepSpeedInferenceWorker pid=179962) [2025-03-03 06:22:24,032] [INFO] [logging.py:105:log_dist] [Rank -1] DeepSpeed info: version=0.16.1+hpu.synapse.v1.20.0, git-hash=61543a96, git-branch=1.20.0\n(DeepSpeedInferenceWorker pid=179962) [2025-03-03 06:22:24,035] [INFO] [logging.py:105:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n(DeepSpeedInferenceWorker pid=179962) [2025-03-03 06:22:24,048] [INFO] [comm.py:652:init_distributed] cdb=None\n(DeepSpeedInferenceWorker pid=179963) ============================= HABANA PT BRIDGE CONFIGURATION =========================== \n(DeepSpeedInferenceWorker pid=179963)  PT_HPU_LAZY_MODE = 1\n(DeepSpeedInferenceWorker pid=179963)  PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024\n(DeepSpeedInferenceWorker pid=179963)  PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n(DeepSpeedInferenceWorker pid=179963)  PT_HPU_LAZY_ACC_PAR_MODE = 0\n(DeepSpeedInferenceWorker pid=179963)  PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n(DeepSpeedInferenceWorker pid=179963)  PT_HPU_EAGER_PIPELINE_ENABLE = 1\n(DeepSpeedInferenceWorker pid=179963)  PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n(DeepSpeedInferenceWorker pid=179963)  PT_HPU_ENABLE_LAZY_COLLECTIVES = 1\n(DeepSpeedInferenceWorker pid=179963) ---------------------------: System Configuration :---------------------------\n(DeepSpeedInferenceWorker pid=179963) Num CPU Cores : 160\n(DeepSpeedInferenceWorker pid=179963) CPU RAM       : 1056374420 KB\n(DeepSpeedInferenceWorker pid=179963) ------------------------------------------------------------------------------\n(DeepSpeedInferenceWorker pid=179964) /usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations [repeated 3x across cluster]\n(DeepSpeedInferenceWorker pid=179964)   warnings.warn( [repeated 3x across cluster]\nLoading 2 checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s] [repeated 3x across cluster]\n(ServeController pid=170719) WARNING 2025-03-03 06:22:37,562 controller 170719 -- Deployment 'DeepSpeedLlamaModel' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n(ServeController pid=170719) This may be caused by a slow __init__ or reconfigure method.\nLoading 2 checkpoint shards:  50%|     | 1/2 [00:17<00:17, 17.51s/it]\nLoading 2 checkpoint shards: 100%|| 2/2 [00:21<00:00,  9.57s/it]\nLoading 2 checkpoint shards: 100%|| 2/2 [00:21<00:00, 10.88s/it]\nLoading 2 checkpoint shards:  50%|     | 1/2 [00:18<00:18, 18.70s/it] [repeated 3x across cluster]\nINFO 2025-03-03 06:22:48,569 serve 170212 -- Application 'default' is ready at http://127.0.0.1:8000/.\nINFO 2025-03-03 06:22:48,569 serve 170212 -- Deployed app 'default' successfully.\n```\n\n----------------------------------------\n\nTITLE: Linking IAM Service Account to Kubernetes Service Account\nDESCRIPTION: Adds an IAM policy binding to the IAM service account, granting the Kubernetes service account permission to act as the IAM service account.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngcloud iam service-accounts add-iam-policy-binding my-iam-sa@my-project-id.iam.gserviceaccount.com \\\n    --role roles/iam.workloadIdentityUser \\\n    --member \"serviceAccount:my-project-id.svc.id.goog[default/my-ksa]\"\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding to Access LLM Service\nDESCRIPTION: Command to set up port forwarding from the local machine to the deployed Ray Serve service. This allows local access to the LLM's API endpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/vllm-rayservice.md#2025-04-12_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n$ kubectl port-forward svc/llama-3-8b-serve-svc 8000\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Ray Build on MacOS (Bash)\nDESCRIPTION: Commands to install necessary dependencies for building Ray from source on MacOS using Homebrew.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbrew update\nbrew install wget\n\n# Install Bazel.\nci/env/install-bazel.sh\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Cleanup Operations\nDESCRIPTION: Commands for cleaning up RayService deployments and related Kubernetes resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete -f ray-service.sample.yaml\n\nhelm uninstall kuberay-operator\n\nkubectl delete pod curl\n```\n\n----------------------------------------\n\nTITLE: Installing Ray C++ API with pip\nDESCRIPTION: This bash command installs the Ray C++ API using pip. It also generates a Ray C++ project template.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n  pip install -U ray[cpp]\n\n  # Create a Ray C++ project template to start with.\n  ray cpp --generate-bazel-project-template-to ray-template\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Storage Output in RLlib\nDESCRIPTION: Configuration for writing experiences to a cloud storage bucket.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nconfig= (\n    AlgorithmConfig()\n    .offline_data(\n        output=\"gs://<your-bucket>/dir1\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Getting and Setting State for Learner in Python\nDESCRIPTION: The snippet shows how to handle the state of individual learners, specifically retrieving and updating the full state or just the RLModule weights. Dependencies include Ray's RLlib core components. Key parameters allow specification of state components to manipulate, such as RLModule or optimizer states, with outputs typically being state dictionaries.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-learner.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.core import COMPONENT_RL_MODULE\n\n# Get the Learner's RLModule weights and optimizer states.\nstate = learner.get_state()\n# Note that `state` is now a dict:\n# {\n#    COMPONENT_RL_MODULE: [RLModule's state],\n#    COMPONENT_OPTIMIZER: [Optimizer states],\n# }\nlearner.set_state(state)\n\n# Only get the RLModule weights (as numpy, not torch/tf).\nrl_module_only_state = learner.get_state(components=COMPONENT_RL_MODULE)\n# Note that `rl_module_only_state` is now a dict:\n# {COMPONENT_RL_MODULE: [RLModule's state]}\nlearner.module.set_state(rl_module_only_state)\n```\n\n----------------------------------------\n\nTITLE: Accessing actor logs using CLI command\nDESCRIPTION: The 'ray logs actor' CLI command retrieves logs for a specific actor identified by its ID, showing the actor's log output which can be useful for debugging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nray list actors\n# In this case, ACTOR_ID is 31405554844820381c2f0f8501000000\nray logs actor --id <ACTOR_ID>\n```\n\nLANGUAGE: text\nCODE:\n```\n--- Log has been truncated to last 1000 lines. Use `--tail` flag to toggle. ---\n\n:actor_name:Actor\nActor created\n```\n\n----------------------------------------\n\nTITLE: Driver Deployment Configuration - Autoscaling\nDESCRIPTION: This YAML configuration defines a Ray Serve deployment named 'Driver' with autoscaling enabled. It sets the target number of ongoing requests to 20, minimum replicas to 1, initial replicas to 1, and a maximum replicas to 10. The upscale and downscale delays and factors are configured to manage the autoscaling behavior.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/advanced-autoscaling.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n\"- name: Driver\n  max_ongoing_requests: 200\n  autoscaling_config:\n    target_ongoing_requests: 20\n    min_replicas: 1\n    initial_replicas: 1\n    max_replicas: 10\n    upscale_delay_s: 3\n    downscale_delay_s: 60\n    upscaling_factor: 0.3\n    downscaling_factor: 0.3\n    metrics_interval_s: 2\n    look_back_period_s: 10\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Development Environment for Ray Multinode Docker\nDESCRIPTION: Set environment variables for local development of the fake multi-node docker module. This configuration mounts the Ray autoscaler directory to the started nodes for easier development and debugging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/fake-autoscaler.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nFAKE_CLUSTER_DEV=\"auto\"\n```\n\n----------------------------------------\n\nTITLE: Running Data Processing with Ray\nDESCRIPTION: Commands to execute data processing using Ray mode with the demo configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/data_juicer_distributed_data_processing.md#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Run the tool from source\npython tools/process_data.py --config demos/process_on_ray/configs/demo.yaml\n\n# Use the command-line tool\ndj-process --config demos/process_on_ray/configs/demo.yaml\n```\n\n----------------------------------------\n\nTITLE: Ray Resource Configuration YAML\nDESCRIPTION: Example YAML configuration for Ray head and worker groups showing resource specifications and start parameters\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md#2025-04-12_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nheadGroupSpec:\n  rayStartParams:\n    num-cpus: \"0\"\n  template:\n    spec:\n      containers:\n      - name: ray-head\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: \"2G\"\n          requests:\n            cpu: \"1\"\n            memory: \"2G\"\n...\nworkerGroupSpecs:\n- groupName: small-group\n  rayStartParams: {}\n  template:\n    spec:\n      containers:\n      - name: ray-worker\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: \"1G\"\n          requests:\n            cpu: \"1\"\n            memory: \"1G\"\n```\n\n----------------------------------------\n\nTITLE: Managing Ray Cluster Nodes (Python)\nDESCRIPTION: Functions for finding running nodes (non_terminated_nodes) and requesting new nodes (create_node) during autoscaling.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/vsphere/ARCHITECTURE.md#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnon_terminated_nodes\ncreate_node\n```\n\n----------------------------------------\n\nTITLE: Deleting Static Ray Cluster\nDESCRIPTION: Removes the static Ray cluster service and deployments from Kubernetes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl delete -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Kueue in Kubernetes\nDESCRIPTION: Command to install a specific version of Kueue in a Kubernetes cluster using kubectl. This step is essential for setting up the job queueing system.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nVERSION=v0.8.2\nkubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/$VERSION/manifests.yaml\n```\n\n----------------------------------------\n\nTITLE: Redis Backup Configuration with RDB and AOF\nDESCRIPTION: Redis configuration for enabling periodic backups and append-only file logging. Sets up 60-second backups after 1000 writes and enables AOF journaling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-persistent-ft.md#2025-04-12_snippet_1\n\nLANGUAGE: redis\nCODE:\n```\n# Dump a backup every 60s, if there are 1000 writes since the prev. backup.\nsave 60 1000\ndbfilename dump.rdb\n\n# Enable the append-only log file.\nappendonly yes\nappendfilename \"appendonly.aof\"\n```\n\n----------------------------------------\n\nTITLE: Referencing Ray Project Release Test Location in Markdown\nDESCRIPTION: This snippet provides a markdown link to the location of the release tests for the Ray project on GitHub. The tests are part of a Python script that builds the pipeline for the release process.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/README.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<https://github.com/ray-project/ray/blob/master/release/.buildkite/build_pipeline.py>\n```\n\n----------------------------------------\n\nTITLE: Running the Updated Tuning Experiment - Python\nDESCRIPTION: This code runs a new tuning experiment using the earlier defined objective function and configuration space tailored for BOHB integrations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\\n    objective,\\n    tune_config=tune.TuneConfig(\\n        metric=\"mean_loss\",\\n        mode=\"min\",\\n        search_alg=algo,\\n        scheduler=scheduler,\\n        num_samples=num_samples,\\n    ),\\n    run_config=tune.RunConfig(\\n        name=\"bohb_exp_2\",\\n        stop={\"training_iteration\": 100},\\n    ),\\n)\\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Creating Ray Cluster\nDESCRIPTION: Command to apply the Ray cluster configuration to Kubernetes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/aws-trainium/llama3.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f 1-llama3-finetune-trn1-create-raycluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Tune with PyTorch Dependencies\nDESCRIPTION: Command to install Ray Tune and the required PyTorch dependencies to run the tutorial examples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/getting-started.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install \"ray[tune]\" torch torchvision\n```\n\n----------------------------------------\n\nTITLE: Enabling Non-Linux Ray Clusters\nDESCRIPTION: Environment variable setting to enable Ray clusters on Windows or OSX, which are not officially supported for multi-node deployments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/getting-started.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRAY_ENABLE_WINDOWS_OR_OSX_CLUSTER=1\n```\n\n----------------------------------------\n\nTITLE: Defining OpenTelemetry Dependencies for Observability in Ray\nDESCRIPTION: Package specifications for OpenTelemetry libraries used for distributed tracing and monitoring in Ray. Includes API, SDK, exporters, and protocol implementations for observability infrastructure.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_19\n\nLANGUAGE: pip\nCODE:\n```\nopentelemetry-api==1.1.0 \\\n    --hash=sha256:38555cd773df903a2f7440778d6f8b48a86fd388604b171969bdbde4b746a558 \\\n    --hash=sha256:704a3b2a7511d2c9065013d362a8371bc452ae6c0521941de680af2a5ca94884\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Data and Ray Train\nDESCRIPTION: Bash command to install Ray Data and Ray Train packages using pip.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/data-loading-preprocessing.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"ray[data,train]\"\n```\n\n----------------------------------------\n\nTITLE: Using Existing Resource Pool of Frozen VMs in vSphere for Ray\nDESCRIPTION: YAML configuration for using an existing resource pool of frozen VMs in vSphere with Ray. This setup only specifies the resource pool name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nfrozen_vm:\n    resource_pool: frozen-vm-resource-pool\n```\n\n----------------------------------------\n\nTITLE: Copying Files from Kubernetes Pod\nDESCRIPTION: Kubernetes command to copy files from a pod to the local filesystem, useful for retrieving logs or other data for analysis.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/kuberay/README.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl cp <pod>:/path/to/file /target/path/in/local/filesystem\n```\n\n----------------------------------------\n\nTITLE: Reducing number of samples for smoke tests\nDESCRIPTION: This snippet reduces the number of samples for smoke tests, likely to speed up the execution of the experiment during testing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# We reduce the num samples in this hidden cell for our smoke tests.\nnum_samples = 10\n\n```\n\n----------------------------------------\n\nTITLE: Anyscale Production Deployment Command\nDESCRIPTION: Command for deploying the Stable Diffusion service to production using Anyscale Service.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/README.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nanyscale service rollout -f service.yaml --name {ENTER_NAME_FOR_SERVICE}\n```\n\n----------------------------------------\n\nTITLE: Enhanced Ray Cluster Task Distribution\nDESCRIPTION: Extended version of the Ray script with additional cluster information and detailed task execution reporting.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/getting-started.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import Counter\nimport socket\nimport time\n\nimport ray\n\nray.init()\n\nprint('''This cluster consists of\n    {} nodes in total\n    {} CPU resources in total\n'''.format(len(ray.nodes()), ray.cluster_resources()['CPU']))\n\n@ray.remote\ndef f():\n    time.sleep(0.001)\n    # Return IP address.\n    return socket.gethostbyname(\"localhost\")\n\nobject_ids = [f.remote() for _ in range(10000)]\nip_addresses = ray.get(object_ids)\n\nprint('Tasks executed')\nfor ip_address, num_tasks in Counter(ip_addresses).items():\n    print('    {} tasks on {}'.format(num_tasks, ip_address))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dataset Batch Formats in Ray Data\nDESCRIPTION: Shows how to use different batch formats (numpy and pandas) when iterating over batches in Ray Data. This example demonstrates retrieving the first batch of size 5 from a dataset of range(10) in both numpy and pandas formats.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-references/glossary.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import ray\n>>> dataset = ray.data.range(10)\n>>> next(iter(dataset.iter_batches(batch_format=\"numpy\", batch_size=5)))\n{'id': array([0, 1, 2, 3, 4])}\n>>> next(iter(dataset.iter_batches(batch_format=\"pandas\", batch_size=5)))\n   id\n0   0\n1   1\n2   2\n3   3\n4   4\n```\n\n----------------------------------------\n\nTITLE: Installing PettingZoo Environments\nDESCRIPTION: Bash command to install PettingZoo with comprehensive environment support\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pettingzoo[all]\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Google TPU Resources in Ray\nDESCRIPTION: Demonstrates how to initialize Ray with TPU resources, create TPU-aware actors and tasks, and access TPU IDs and environment variables. Shows how Ray automatically assigns different TPUs to concurrent workloads.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/accelerators.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\n\nray.init(resources={\"TPU\": 2})\n\n@ray.remote(resources={\"TPU\": 1})\nclass TPUActor:\n    def ping(self):\n        print(\"TPU IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"TPU\"]))\n        print(\"TPU_VISIBLE_CHIPS: {}\".format(os.environ[\"TPU_VISIBLE_CHIPS\"]))\n\n@ray.remote(resources={\"TPU\": 1})\ndef tpu_task():\n    print(\"TPU IDs: {}\".format(ray.get_runtime_context().get_accelerator_ids()[\"TPU\"]))\n    print(\"TPU_VISIBLE_CHIPS: {}\".format(os.environ[\"TPU_VISIBLE_CHIPS\"]))\n\ntpu_actor = TPUActor.remote()\nray.get(tpu_actor.ping.remote())\n# The actor uses the first TPU so the task uses the second one.\nray.get(tpu_task.remote())\n```\n\n----------------------------------------\n\nTITLE: Appending data to a non-numpy'ized Episode\nDESCRIPTION: This snippet demonstrates how to append data to a non-numpy'ized SingleAgentEpisode.  Data is stored in python lists before being converted to numpy arrays.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/single-agent-episode.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example of how to add data to a non-finalized (non-numpy'ized) episode.\n\"\"\"\n\nimport numpy as np\n\nfrom ray.rllib.env.single_agent_episode import SingleAgentEpisode\n\n# Create a dummy episode.\nepisode = SingleAgentEpisode()\n\n# The episode is initially non-numpy'ized, meaning that all data\n# is stored as plain python lists.\nassert episode.observations is None\n\nepisode.reset(obs=1.0)\nassert isinstance(episode.observations, list)\n\nepisode.add_action(action=0)\nepisode.add_reward(reward=1.0)\nepisode.add_observation(obs=2.0)\n\nepisode.add_action(action=1)\nepisode.add_reward(reward=-0.5)\nepisode.add_observation(obs=3.0)\n\n# Still in non-numpy'ized format.\nassert isinstance(episode.observations, list)\nassert len(episode.observations) == 3\n\nepisode.numpyize()\n\n# Data is now numpy'ized.\nassert isinstance(episode.observations, np.ndarray)\n\n```\n\n----------------------------------------\n\nTITLE: Deploying Ray Java to Maven Central Repository\nDESCRIPTION: This bash script demonstrates the process of deploying Ray Java packages to Maven Central. It sets necessary environment variables, checks out the release commit, builds multiplatform jars, and deploys them to Maven Central.\nSOURCE: https://github.com/ray-project/ray/blob/master/java/java-release-guide.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OSSRH_KEY=xxx\nexport OSSRH_TOKEN=xxx\nexport TRAVIS_BRANCH=releases/x.y.z  # Your releasing version number\nexport TRAVIS_COMMIT=xxxxxxxxxxx     # The commit id\ngit checkout $TRAVIS_COMMIT\nsh java/build-jar-multiplatform.sh multiplatform\nexport GPG_SKIP=false\ncd java && mvn versions:set -DnewVersion=x.y.z && cd -\nsh java/build-jar-multiplatform.sh deploy_jars\n```\n\n----------------------------------------\n\nTITLE: Handling File Reference Inconsistencies in a Bash and Python Ray Setup\nDESCRIPTION: Describes a situation where file references in Ray tasks or actors behave inconsistently due to local file availability. Using shared file systems like NFS or S3 is recommended to ensure consistent file accessibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/general-debugging.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n% touch /tmp/foo.txt\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\n\n@ray.remote\ndef check_file():\n    foo_exists = os.path.exists(\"/tmp/foo.txt\")\n    return foo_exists\n\nfutures = [check_file.remote() for _ in range(1000)]\nprint(ray.get(futures))\n```\n\n----------------------------------------\n\nTITLE: LLM Inference Response from vLLM Ray Service\nDESCRIPTION: JSON response from the LLM showing the generated completion. The response follows the OpenAI chat completion format with token usage statistics and the generated text describing the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/vllm-rayservice.md#2025-04-12_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"id\":\"cmpl-ce6585cd69ed47638b36ddc87930fded\",\"object\":\"chat.completion\",\"created\":1723161873,\"model\":\"meta-llama/Meta-Llama-3-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The Ray open-source project is a high-performance distributed computing framework that allows users to scale Python applications and machine learning models to thousands of nodes, supporting distributed data processing, distributed machine learning, and distributed analytics.\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":32,\"total_tokens\":74,\"completion_tokens\":42}}\n```\n\n----------------------------------------\n\nTITLE: Overloaded Actor Methods in Ray - Java\nDESCRIPTION: Demonstrates implementation of overloaded actor methods in Ray using Java, including inheritance and multiple method signatures. Shows how to properly cast and invoke overloaded actor methods.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/miscellaneous.rst#2025-04-12_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic static class Counter {\n  protected int value = 0;\n\n  public int increment() {\n    this.value += 1;\n    return this.value;\n  }\n}\n\npublic static class CounterOverloaded extends Counter {\n  public int increment(int diff) {\n    super.value += diff;\n    return super.value;\n  }\n\n  public int increment(int diff1, int diff2) {\n    super.value += diff1 + diff2;\n    return super.value;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Ax and Ray Tune\nDESCRIPTION: Installation commands for the required libraries: ax-platform version 0.2.4 and Ray with Tune extension.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# !pip install ray[tune]\n!pip install ax-platform==0.2.4\n```\n\n----------------------------------------\n\nTITLE: Validating KubeRay Operator Installation\nDESCRIPTION: Command to verify the successful installation of KubeRay operator by checking the pod status in the default namespace.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/kuberay-operator-installation.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get pods\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Cluster Deletion\nDESCRIPTION: Command to delete the entire Kubernetes cluster using kind.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nkind delete cluster\n```\n\n----------------------------------------\n\nTITLE: Package Index Configuration in Requirements File\nDESCRIPTION: Configuration options for package sources in the requirements file, specifying PyPI as the main index with additional sources for PyTorch CUDA 12.1 packages and PyG (PyTorch Geometric) wheel files.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cu121\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html\n```\n\n----------------------------------------\n\nTITLE: Installing RayService Configuration\nDESCRIPTION: Command to apply the RayService configuration for Stable Diffusion deployment using kubectl.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/stable-diffusion-rayservice.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-service.stable-diffusion.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Miniforge on MacOS\nDESCRIPTION: Executes the Miniforge installer and then cleans up the installer file. This sets up the conda environment necessary for installing Ray on MacOS.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n\"bash Miniforge3-MacOSX-arm64.sh\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n\"rm Miniforge3-MacOSX-arm64.sh # Cleanup.\"\n```\n\n----------------------------------------\n\nTITLE: Resource Management for Java Deployment\nDESCRIPTION: This snippet shows how to reserve resources, such as a GPU, for each deployment replica through the `rayActorOptions` parameter in the Java API.  It sets the resources using a dictionary-like structure. The deployment must already exist.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/managing-java-deployments.md#2025-04-12_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n    Serve.deployment(\"counter\")\n        .setRayActorOptions(Ray.actorOptions().setResource(\"GPU\", 1))\n        .deploy(true);\n    System.out.println(\"\\\"counter\\\" deployment now requires 1 GPU per replica.\");\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Incorrect Ray Job Submission Syntax\nDESCRIPTION: Shows an incorrect way of submitting a Ray job, which may result in a 'not found' error. This example wraps the entrypoint command in quotes, which should be avoided.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/cli.rst#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nray job submit --working-dir=\".\" -- \"python script.py\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ray with Cluster Launcher Support\nDESCRIPTION: Command to install Ray with default components using pip package manager.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/azure.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# install ray\npip install -U ray[default]\n```\n\n----------------------------------------\n\nTITLE: Ray Cluster Connection in Python\nDESCRIPTION: Python code snippet showing how to connect to a Ray cluster in the application script.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/yarn.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\"\n```\n\n----------------------------------------\n\nTITLE: Installing Fluent Bit Using Helm\nDESCRIPTION: Commands to add Fluent Bit Helm repository and install Fluent Bit with custom configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-operator-logs.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add fluent https://fluent.github.io/helm-charts\nhelm repo update\n\nhelm install fluent-bit fluent/fluent-bit --version 0.48.2 -f fluent-bit-config.yaml\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Worker Node\nDESCRIPTION: Command to connect a worker node to an existing Ray head node using the head node's address and port.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nray start --address=<head-node-address:port>\n```\n\n----------------------------------------\n\nTITLE: Kubernetes RayCluster Cleanup Commands\nDESCRIPTION: Shell commands for cleaning up a RayCluster and verifying Redis key deletion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# Step 9.1: Delete the RayCluster custom resource.\nkubectl delete raycluster raycluster-external-redis\n\n# Step 9.2: KubeRay operator deletes all Pods in the RayCluster.\n# Step 9.3: KubeRay operator creates a Kubernetes Job to delete the Redis key after the head Pod is terminated.\n\n# Step 9.4: Check whether the RayCluster has been deleted.\nkubectl get raycluster\n# [Expected output]: No resources found in default namespace.\n\n# Step 9.5: Check Redis keys after the Kubernetes Job finishes.\nexport REDIS_POD=$(kubectl get pods --selector=app=redis -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $REDIS_POD -- redis-cli -a \"5241590000000000\"\nKEYS *\n# [Expected output]: (empty list or set)\n```\n\n----------------------------------------\n\nTITLE: Configuring Fluent Bit for Ray Log Collection with Loki Integration\nDESCRIPTION: This YAML ConfigMap configures Fluent Bit to tail Ray logs from the session directory, add pod label metadata, and send logs to a Grafana Loki endpoint. It includes settings for refreshing file lists, adding labels for filtering, and specifying tenant information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentbit-config\ndata:\n  fluent-bit.conf: |\n    [INPUT]\n        Name tail\n        Path /tmp/ray/session_latest/logs/*\n        Tag ray\n        Path_Key true\n        Refresh_Interval 5\n    [FILTER]\n        Name modify\n        Match ray\n        Add POD_LABELS ${POD_LABELS}\n    [OUTPUT]\n        Name loki\n        Match *\n        Host loki-gateway\n        Port 80\n        Labels RayCluster=${POD_LABELS}\n        tenant_id test\n```\n\n----------------------------------------\n\nTITLE: Defining toctree and grid layout in reStructuredText\nDESCRIPTION: This RST code defines a hidden toctree for navigation and creates a grid layout with cards for different frameworks supported by Ray. Each card includes an image, link, and framework name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/more-frameworks.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _train-more-frameworks:\n\nMore Frameworks\n===============\n\n.. toctree::\n    :hidden:\n\n    Hugging Face Accelerate Guide <huggingface-accelerate>\n    DeepSpeed Guide <deepspeed>\n    TensorFlow and Keras Guide <distributed-tensorflow-keras>\n    XGBoost and LightGBM Guide <examples/xgboost/distributed-xgboost-lightgbm>\n    Horovod Guide <horovod>\n\n.. grid:: 1 2 3 4\n    :gutter: 1\n    :class-container: container pb-3\n\n    .. grid-item-card::\n        :img-top: /images/accelerate_logo.png\n        :class-img-top: mt-2 w-75 d-block mx-auto fixed-height-img\n        :link: huggingface-accelerate\n        :link-type: doc\n\n        Hugging Face Accelerate\n\n    .. grid-item-card::\n        :img-top: /images/deepspeed_logo.svg\n        :class-img-top: mt-2 w-75 d-block mx-auto fixed-height-img\n        :link: deepspeed\n        :link-type: doc\n\n        DeepSpeed\n\n    .. grid-item-card::\n        :img-top: /images/tf_logo.png\n        :class-img-top: mt-2 w-75 d-block mx-auto fixed-height-img\n        :link: distributed-tensorflow-keras\n        :link-type: doc\n\n        TensorFlow and Keras\n\n    .. grid-item-card::\n        :img-top: /images/xgboost_logo.png\n        :class-img-top: mt-2 w-75 d-block mx-auto fixed-height-img\n        :link: examples/xgboost/distributed-xgboost-lightgbm\n        :link-type: doc\n\n        XGBoost and LightGBM\n\n    .. grid-item-card::\n        :img-top: /images/horovod.png\n        :class-img-top: mt-2 w-75 d-block mx-auto fixed-height-img\n        :link: horovod\n        :link-type: doc\n\n\n        Horovod\n```\n\n----------------------------------------\n\nTITLE: Updating Ray BYOD Requirements for Python 3.12 using Bazel\nDESCRIPTION: Command instruction to update the requirements file for Ray's BYOD (Bring Your Own Dependencies) mode with Python 3.12 using Bazel. This shows how to regenerate this pip-compiled dependencies file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.12.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel run //release:requirements_byod_3.12.update\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Hash Verification\nDESCRIPTION: This snippet shows Python package dependencies with their exact versions and SHA-256 hash verifications. The file uses the pip hash verification format to ensure package integrity during installation. Comments indicate the source of each dependency requirement.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_40\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:2e8b55d8517a2fda8d95cb45d62a5a8bbf9dd0ad39c5b25c8833efea07b880ca \\\n    --hash=sha256:2fa4331c200c2521512595253f5bb70858b90f750d39b8cbfd67465f8d1b596d \\\n    --hash=sha256:3445e07bf2e8ecfeef6ef67ac83de670358abf2996916039b16a218e3d95e97e \\\n    --hash=sha256:3453e8d41fe5f17d1f8e9c383a7473cd46a63661628ec58e07777c2fff7196dc \\\n    --hash=sha256:378753b4a4de2a7b34063d6f95ae81bfa7b15f2c1a04a9518e8644e81807ebea \\\n    --hash=sha256:3af6e48651c4e0d2d166dc1b033b7042ea3f871504b6805ba5f4fe31581d8d38 \\\n    --hash=sha256:3dfcbc95bd7992b16f3f7ba05af8a64ca694331bd24f9157b49dadeeb287493b \\\n    --hash=sha256:3f21f0495edea7fdbaaa87e633a8689cd285f8f4af5c869f27bc8074638ad69c \\\n    --hash=sha256:4041711832360a9b75cfb11b25a6a97c8fb49c07b8bd43d0d02b45d0b499a4ff \\\n    --hash=sha256:44d61b4b7d0c2c9ac019c314e52d7cbda0ae31078aabd0f22e583af3e0d79723 \\\n    --hash=sha256:4617e1915a539a0d9a9567795023de41a87106522ff83fbfaf1f6baf8e85437e \\\n    --hash=sha256:4b232061ca880db21fa14defe219840ad9b74b6158adb52ddf0e87bead9e8493 \\\n    --hash=sha256:5246b14ca64a8675e0a7161f7af68fe3e910e6b90542b4bfb5439ba752191df6 \\\n    --hash=sha256:5725dd9cc02068996d4438d397e255dcb1df776b7ceea3b9cb972bdb11260a83 \\\n    --hash=sha256:583f6a1993ca3369e0f80ba99d796d8e6b1a3a2a442dd4e1a79e652116413091 \\\n    --hash=sha256:59259dc58e57b10e7e18ce02c311804c10c5a793e6568f8af4dead03264584d1 \\\n    --hash=sha256:593eba61ba0c3baae5bc9be2f5232430453fb4432048de28399ca7376de9c627 \\\n    --hash=sha256:59f4a79c19232a5774aee369a0c296712ad0e77f24e62cad53160312b1c1eaa1 \\\n    --hash=sha256:5f0e260eaf54380380ac3808aa4ebe2d8ca28b9087cf411649f96bad6900c728 \\\n    --hash=sha256:62d9cfcf4948683a18a9aff0ab7e1474d407b7bab2ca03116109f8464698ab16 \\\n    --hash=sha256:64607d4cbf1b7e3c3c8a14948b99345eda0e161b852e122c6bb71aab6d1d798c \\\n    --hash=sha256:655ca44a831ecb238d124e0402d98f6212ac527a0ba6c55ca26f616604e60a45 \\\n    --hash=sha256:666ecce376999bf619756a24ce15bb14c5bfaf04bf00abc7e663ce17c3f34fe7 \\\n    --hash=sha256:68049202f67380ff9aa52f12e92b1c30115f32e6895cd7198fa2a7961621fc5a \\\n    --hash=sha256:69803198097467ee7282750acb507fba35ca22cc3b85f16cf45fb01cb9097730 \\\n    --hash=sha256:6c7b99ca52c2c1752b544e310101b98a659b720b21db00e65edca34483259967 \\\n    --hash=sha256:6dd9412824c4ce1aca56c47b0991e65bebb7ac3f4edccfd3f156150c96a7bf25 \\\n    --hash=sha256:70eb60b3ae9245ddea20f8a4190bd79c705a22f8028aaf8bbdebe4716c3fab24 \\\n    --hash=sha256:70fb28128acbfd264eda9bf47015537ba3fe86e40d046eb2963d75024be4d055 \\\n    --hash=sha256:7b2513ba235829860b13faa931f3b6846548021846ac808455301c23a101689d \\\n    --hash=sha256:7ef9d9da710be50ff6809fed8f1963fecdfecc8b86656cadfca3bc24289414b0 \\\n    --hash=sha256:81e69b0a0e2537f26d73b4e43ad7bc8c8efb39621639b4434b76a3de50c6966e \\\n    --hash=sha256:8633e471c6207a039eff6aa116e35f69f3156b3989ea3e2d755f7bc41754a4a7 \\\n    --hash=sha256:8bd7c8cfc0b8247c8799080fbff54e0b9619e17cdfeb0478ba7295d43f635d7c \\\n    --hash=sha256:9253fc214112405f0afa7db88739294295f0e08466987f1d70e29930262b4c8f \\\n    --hash=sha256:99b37292234e61325e7a5bb9689e55e48c3f5f603af88b1642666277a81f1fbd \\\n    --hash=sha256:9bd7228827ec7bb817089e2eb301d907c0d9827a9e558f22f762bb690b131652 \\\n    --hash=sha256:9beeb01d8c190d7581a4d59522cd3d4b6887040dcfc744af99aa59fef3e041a8 \\\n    --hash=sha256:a63cbdd98acef6570c62b92a1e43266f9e8b21e699c363c0fef13bd530799c11 \\\n    --hash=sha256:a76e42402542b1fae59798fab64432b2d015ab9d0c8c47ba7addddbaf7952333 \\\n    --hash=sha256:ac0a03221cdb5058ce0167ecc92a8c89e8d0decdc9e99a2ec23380793c4dcb96 \\\n    --hash=sha256:b0b4136a252cadfa1adb705bb81524eee47d9f6aab4f2ee4fa1e9d3cd4581f64 \\\n    --hash=sha256:b25bc607423935079e05619d7de556c91fb6adeae9d5f80868dde3468657994b \\\n    --hash=sha256:b3d504047aba448d70cf6fa22e06cb09f7cbd761939fdd47604f5e007675c24e \\\n    --hash=sha256:bb47271f60660803ad11f4c61b42242b8c1312a31c98c578f79ef9387bbde21c \\\n    --hash=sha256:bbb232860e3d03d544bc03ac57855cd82ddf19c7a07651a7c0fdb95e9efea8b9 \\\n    --hash=sha256:bc27863442d388870c1809a87507727b799c8460573cfbb6dc0eeaef5a11b5ec \\\n    --hash=sha256:bc51abd01f08117283c5ebf64844a35144a0843ff7b2983e0648e4d3d9f10dbb \\\n    --hash=sha256:be2eb3f2495ba669d2a985f9b426c1797b7d48d6963899276d22f23e33d47e37 \\\n    --hash=sha256:bf9db5488121b596dbfc6718c76092fda77b703c1f7533a226a5a9f65248f8ad \\\n    --hash=sha256:c58e2339def52ef6b71b8f36d13c3688ea23fa093353f3a4fee2556e62086ec9 \\\n    --hash=sha256:cfbc454a2880389dbb9b5b398e50d439e2e58669160f27b60e5eca11f68ae17c \\\n    --hash=sha256:cff63a0272fcd259dcc3be1657b07c929c466b067ceb1c20060e8d10af56f5bf \\\n    --hash=sha256:d115bffdd417c6d806ea9069237a4ae02f513b778e3789a359bc5856e0404cc4 \\\n    --hash=sha256:d20cfb4e099748ea39e6f7b16c91ab057989712d31761d3300d43134e26e165f \\\n    --hash=sha256:d48424e39c2611ee1b84ad0f44fb3b2b53d473e65de061e3f460fc0be5f1939d \\\n    --hash=sha256:e0fa2d4ec53dc51cf7d3bb22e0aa0143966119f42a0c3e4998293a3dd2856b09 \\\n    --hash=sha256:e32fee8ab45d3c2db6da19a5323bc3362237c8b653c70194414b892fd06a080d \\\n    --hash=sha256:e35ba67d65d49080e8e5a1dd40101fccdd9798adb9b050ff670b7d74fa41c566 \\\n    --hash=sha256:e3fb866d9932a3d7d0c82da76d816996d1667c44891bd861a0f97ba27e84fc74 \\\n    --hash=sha256:e61b02c3f7a1e0b75e20c3978f7135fd13cb6cf551bf4a6d29b999a88830a338 \\\n    --hash=sha256:e67ba3c290821343c192f7eae1d8fd5999ca2dc99994114643e2f2d3e6138b15 \\\n    --hash=sha256:e79dd39f1e8c3504be0607e5fc6e86bb60fe3584bec8b782578c3b0fde8d932c \\\n    --hash=sha256:e89391e6d60251560f0a8f4bd32137b077a80d9b7dbe6d5cab1cd80d2746f648 \\\n    --hash=sha256:ea7433ce7e4bfc3a85654aeb6747babe3f66eaf9a1d0c1e7a4435bbdf27fea84 \\\n    --hash=sha256:eaf16ae9ae519a0e237a0f528fd9f0197b9bb70f40263ee57ae53c2b8d48aeb3 \\\n    --hash=sha256:eb0c341fa71df5a4595f9501df4ac5abfb5a09580081dffbd1ddd4654e6e9123 \\\n    --hash=sha256:f276b245347e6e36526cbd4a266a417796fc531ddf391e43574cf6466c492520 \\\n    --hash=sha256:f47ad3d5f3258bd7058d2d506852217865afefe6153a36eb4b6928758041d831 \\\n    --hash=sha256:f56a6b404f74ab372da986d240e2e002769a7d7102cc73eb238a4f72eec5284e \\\n    --hash=sha256:f5cf2a0c2bdadf3791b5c205d55a37a54025c6e18a71c71f82bb536cf9a454bf \\\n    --hash=sha256:f5d36399a1b96e1a5fdc91e0522544580dbebeb1f77f27b2b0ab25559e103b8b \\\n    --hash=sha256:f60bd8423be1d9d833f230fdbccf8f57af322d96bcad6599e5a771b151398eb2 \\\n    --hash=sha256:f612463ac081803f243ff13cccc648578e2279295048f2a8d5eb430af2bae6e3 \\\n    --hash=sha256:f73d3fef726b3243a811121de45193c0ca75f6407fe66f3f4e183c983573e130 \\\n    --hash=sha256:f82a116a1d03628a8ace4859556fb39fd1424c933341a08ea3ed6de1edb0283b \\\n    --hash=sha256:fb0ba113b4983beac1a2eb16faffd76cb41e176bf58c4afe3e14b9c681f702de \\\n    --hash=sha256:fb4f868f712b2dd4bcc538b0a0c1f63a2b1d584c925e69a224d759e7070a12d5 \\\n    --hash=sha256:fb6116dfb8d1925cbdb52595560584db42a7f664617a1f7d7f6e32f138cdf37d \\\n    --hash=sha256:fda7cb070f442bf80b642cd56483b5548e43d366fe3f39b98e67cce780cded00 \\\n    --hash=sha256:feea821ee2a9273771bae61194004ee2fc33f8ec7db08117ef9147d4bbcbca8e\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jsonschema\n    #   referencing\nrsa==4.7.2 \\\n    --hash=sha256:78f9a9bf4e7be0c5ded4583326e7461e3a3c5aae24073648b4bdfa797d78c9d2 \\\n    --hash=sha256:9d689e6ca1b3038bc82bf8d23e944b6b6037bc02301a574935b2dd946e0353b9\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-auth\n    #   oauth2client\ns3transfer==0.6.2 \\\n    --hash=sha256:b014be3a8a2aab98cfe1abc7229cc5a9a0cf05eb9c1f2b86b230fd8df3f78084 \\\n    --hash=sha256:cab66d3380cca3e70939ef2255d01cd8aece4a4907a9528740f668c4b0611861\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   boto3\nscikit-image==0.24.0 \\\n    --hash=sha256:18836a18d3a7b6aca5376a2d805f0045826bc6c9fc85331659c33b4813e0b563 \\\n    --hash=sha256:190ebde80b4470fe8838764b9b15f232a964f1a20391663e31008d76f0c696f7 \\\n    --hash=sha256:272909e02a59cea3ed4aa03739bb88df2625daa809f633f40b5053cf09241831 \\\n    --hash=sha256:39ee0af13435c57351a3397eb379e72164ff85161923eec0c38849fecf1b4764 \\\n    --hash=sha256:4688c18bd7ec33c08d7bf0fd19549be246d90d5f2c1d795a89986629af0a1e83 \\\n    --hash=sha256:56dab751d20b25d5d3985e95c9b4e975f55573554bd76b0aedf5875217c93e69 \\\n    --hash=sha256:59c98cc695005faf2b79904e4663796c977af22586ddf1b12d6af2fa22842dc2 \\\n    --hash=sha256:5d16efe95da8edbeb363e0c4157b99becbd650a60b77f6e3af5768b66cf007ab \\\n    --hash=sha256:5e37de6f4c1abcf794e13c258dc9b7d385d5be868441de11c180363824192ff7 \\\n    --hash=sha256:6fccceb54c9574590abcddc8caf6cefa57c13b5b8b4260ab3ff88ad8f3c252b3 \\\n    --hash=sha256:7ac7913b028b8aa780ffae85922894a69e33d1c0bf270ea1774f382fe8bf95e7 \\\n    --hash=sha256:82ab903afa60b2da1da2e6f0c8c65e7c8868c60a869464c41971da929b3e82bc \\\n    --hash=sha256:8579bda9c3f78cb3b3ed8b9425213c53a25fa7e994b7ac01f2440b395babf660 \\\n    --hash=sha256:93f46e6ce42e5409f4d09ce1b0c7f80dd7e4373bcec635b6348b63e3c886eac8 \\\n    --hash=sha256:9c7a52e20cdd760738da38564ba1fed7942b623c0317489af1a598a8dedf088b \\\n    --hash=sha256:cb3bc0264b6ab30b43c4179ee6156bc18b4861e78bb329dd8d16537b7bbf827a \\\n    --hash=sha256:ccc01e4760d655aab7601c1ba7aa4ddd8b46f494ac46ec9c268df6f33ccddf4c \\\n    --hash=sha256:dacf591ac0c272a111181afad4b788a27fe70d213cfddd631d151cbc34f8ca2c \\\n    --hash=sha256:e9aadb442360a7e76f0c5c9d105f79a83d6df0e01e431bd1d5757e2c5871a1f3 \\\n    --hash=sha256:ef04360eda372ee5cd60aebe9be91258639c86ae2ea24093fb9182118008d009 \\\n    --hash=sha256:fa27b3a0dbad807b966b8db2d78da734cb812ca4787f7fbb143764800ce2fa9c\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\nscipy==1.11.4 \\\n    --hash=sha256:00150c5eae7b610c32589dda259eacc7c4f1665aedf25d921907f4d08a951b1c \\\n    --hash=sha256:028eccd22e654b3ea01ee63705681ee79933652b2d8f873e7949898dda6d11b6 \\\n    --hash=sha256:1b7c3dca977f30a739e0409fb001056484661cb2541a01aba0bb0029f7b68db8 \\\n    --hash=sha256:2c6ff6ef9cc27f9b3db93a6f8b38f97387e6e0591600369a297a50a8e96e835d \\\n    --hash=sha256:36750b7733d960d7994888f0d148d31ea3017ac15eef664194b4ef68d36a4a97 \\\n    --hash=sha256:530f9ad26440e85766509dbf78edcfe13ffd0ab7fec2560ee5c36ff74d6269ff \\\n\n```\n\n----------------------------------------\n\nTITLE: Specifying Pexpect Package with Hash Verification in Bash\nDESCRIPTION: Defines the pexpect package version 4.8.0 with SHA256 hash verification for secure package installation. Only for non-Windows platforms, required by ipython.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\npexpect==4.8.0 ; sys_platform != 'win32' \\\n    --hash=sha256:0b48a55dcb3c05f3329815901ea4fc1537514d6ba867a152b581d69ae3710937 \\\n    --hash=sha256:fc65a43959d153d0114afe13997d439c22823a27cefceb5ff35c2178c6784c0c\n```\n\n----------------------------------------\n\nTITLE: Showing Turn-Based Hierarchical Action Timeline\nDESCRIPTION: A text diagram illustrating a turn-based hierarchical action pattern where low-level agents wait for the high-level agent to issue an action before taking multiple actions themselves.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/hierarchical-envs.rst#2025-04-12_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntop-level: action_0 -----------------------------------> action_1 ->\nlow-level: ---------> action_0 -> action_1 -> action_2 ------------>\n```\n\n----------------------------------------\n\nTITLE: Output with Custom Log Deduplication\nDESCRIPTION: Shows log output with specific messages repeated despite deduplication due to regex settings. Non-matching logs remain deduplicated. Demonstrates how regex affects log output patterns.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# 2024-10-10 17:54:19,095 INFO worker.py:1614 -- Connecting to existing Ray cluster at address: 172.31.13.10:6379...\n# 2024-10-10 17:54:19,102 INFO worker.py:1790 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265\n# (f pid=1574323) ABC\n# (f pid=1574323) DEF\n# (f pid=1574321) ABC\n# (f pid=1574318) ABC\n# (f pid=1574320) ABC\n# (f pid=1574322) ABC\n# (f pid=1574322) DEF [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n```\n\n----------------------------------------\n\nTITLE: Matching Ray Autoscaler Status Output Pattern\nDESCRIPTION: Regular expression pattern that matches the structured output of Ray's autoscaler status command. Captures node status information, active/pending nodes, resource usage metrics, and resource demands.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_status_v1.txt#2025-04-12_snippet_0\n\nLANGUAGE: regex\nCODE:\n```\n======== Autoscaler status: .+\\nNode status\\n---------------------------------------------------------------\\nActive:\\n 1 node_.+\\nPending:\\n \\(no pending nodes\\)\\nRecent failures:\\n \\(no failures\\)\\n\\nResources\\n---------------------------------------------------------------\\nUsage:\\n 0.0/3.0 CPU\\n 0.+\\n 0.+\\n\\nDemands:\\n \\(no resource demands\\)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Values\nDESCRIPTION: This code snippet demonstrates how to specify Python package dependencies with exact version numbers and hash values for security and reproducibility. It includes multiple packages like scipy, sentencepiece, and others.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_34\n\nLANGUAGE: Text\nCODE:\n```\nscipy==1.11.4 \\\n    --hash=sha256:00150c5eae7b610c32589dda259eacc7c4f1665aedf25d921907f4d08a951b1c \\\n    --hash=sha256:028eccd22e654b3ea01ee63705681ee79933652b2d8f873e7949898dda6d11b6 \\\n    --hash=sha256:1b7c3dca977f30a739e0409fb001056484661cb2541a01aba0bb0029f7b68db8 \\\n    # ... (additional hash values omitted for brevity)\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   scikit-image\n    #   vllm\nsentencepiece==0.2.0 \\\n    --hash=sha256:0461324897735512a32d222e3d886e24ad6a499761952b6bda2a9ee6e4313ea5 \\\n    --hash=sha256:0993dbc665f4113017892f1b87c3904a44d0640eda510abcacdfb07f74286d36 \\\n    # ... (additional hash values omitted for brevity)\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   mistral-common\n    #   vllm\n    #   xgrammar\nshellingham==1.5.4 \\\n    --hash=sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686 \\\n    --hash=sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   typer\nsix==1.16.0 \\\n    --hash=sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926 \\\n    --hash=sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   python-dateutil\nsmart-open==6.2.0 \\\n    --hash=sha256:088bf00f9327c71e549bc2f86567d3320df5d89667f009ce1c16568976068ef7 \\\n    --hash=sha256:1b4df5c8365218f3852c507451920ccad606c80b0acb4e67508e50ba9b5d2632\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\nsniffio==1.3.1 \\\n    --hash=sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2 \\\n    --hash=sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   anyio\n    #   openai\n```\n\n----------------------------------------\n\nTITLE: Inspecting Rows in Ray Datasets\nDESCRIPTION: This snippet illustrates how to inspect individual rows in a Ray Dataset using the take() method, which returns a list of rows represented as dictionaries.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/inspecting-data.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\nrows = ds.take(1)\nprint(rows)\n```\n\n----------------------------------------\n\nTITLE: Manually Starting Prometheus for Ray\nDESCRIPTION: Commands to manually start Prometheus with Ray's configuration file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/metrics.md#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# With default settings\n./prometheus --config.file=/tmp/ray/session_latest/metrics/prometheus/prometheus.yml\n\n# With specified --temp-dir\n./prometheus --config.file={your_temp_path}/session_latest/metrics/prometheus/prometheus.yml\n```\n\n----------------------------------------\n\nTITLE: Listing Hash Values for Python Package Dependencies\nDESCRIPTION: This snippet shows a portion of a requirements file or dependency specification, listing hash values for various Python packages. It includes multiple SHA256 hashes for each package version, ensuring integrity and security when installing dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_19\n\nLANGUAGE: Text\nCODE:\n```\n    --hash=sha256:3102045a10945173d38336f6e71a8dc71bcaeed55c3123ad4af82c52807b9375 \\\n    --hash=sha256:339894035d0ede518b16073bdc2feef4c991ee991a29774b33e515f1d308e08d \\\n    --hash=sha256:412444afb8c4c7a6cc11a47dade32982439925537e483be7c0ae0cf96c4f6a0b \\\n    --hash=sha256:4203efca580f0dd6f882ca211f923168548f7ba334c189e9eab1178ab840bf60 \\\n    --hash=sha256:45ebc7b45406febf07fef35d856f0293a92e7417ae7933207e90bf9090b70572 \\\n    --hash=sha256:4b5ec25d8b17217d635f8935dbc1b9aa5907962fae29dff220f2659487891cd3 \\\n    --hash=sha256:4c8e73e99da7db1b4cad7f8d682cf6abad7844da39834c288fbfa394a47bbced \\\n    --hash=sha256:4e6f7d1c414191c1199f8996d3f2282b9ebea0945693fb67392c75a3a320941f \\\n    --hash=sha256:4eaa22f0d22b1a7e93ff0a596d57fdede2e550aecffb5a1ef1106aaece48e96b \\\n    --hash=sha256:50b8eae8f7334ec826d6eeffaeeb00e36b5e24aa0b9df322c247539714c6df19 \\\n    --hash=sha256:50fd3f6b26e3441ae07b7c979309638b72abc1a25da31a81a7fbd9495713ef4f \\\n    --hash=sha256:51243f1ed5161b9945011a7360e997729776f6e5d7005ba0c6879267d4c5139d \\\n    --hash=sha256:5d512aafa1d32efa014fa041d38868fda85028e3f930a96f85d49c7d8ddc0383 \\\n    --hash=sha256:5f77cf66e96ae734717d341c145c5949c63180842a545c47a0ce7ae52ca83795 \\\n    --hash=sha256:6b02471b72526ab8a18c39cb7967b72d194ec53c1fd0a70b050565a0f366d355 \\\n    --hash=sha256:6fb1b30043271ec92dc65f6d9f0b7a830c210b8a96423074b15c7bc999975f57 \\\n    --hash=sha256:7161ec49ef0800947dc5570f86568a7bb36fa97dd09e9827dc02b718c5643f09 \\\n    --hash=sha256:72d622d262e463dfb7595202d229f5f3ab4b852289a1cd09650362db23b9eb0b \\\n    --hash=sha256:74d28c17412d9caa1066f7a31df8403ec23d5268ba46cd0ad2c50fb82ae40462 \\\n    --hash=sha256:78618cdbccaa74d3f88d0ad6cb8ac3007f1a6fa5c6f19af64b55ca170bfa1edf \\\n    --hash=sha256:793b4e24db2e8742ca6423d3fde8396db336698c55cd34b660663ee9e45ed37f \\\n    --hash=sha256:798232c92e7665fe82ac085f9d8e8ca98826f8e27859d9a96b41d519ecd2e49a \\\n    --hash=sha256:81d09caa7b27ef4e61cb7d8fbf1714f5aec1c6b6c5270ee53504981e6e9121ad \\\n    --hash=sha256:8ab74c06ffdab957d7670c2a5a6e1a70181cd10b727cd788c4dd9005b6a8acd9 \\\n    --hash=sha256:8eb0908e954d093b02a543dc963984d6e99ad2b5e36503d8a0aaf040505f747d \\\n    --hash=sha256:90b9e29824800e90c84e4022dd5cc16eb2d9605ee13f05d47641eb183cd73d45 \\\n    --hash=sha256:9797a6c8fe16f25749b371c02e2ade0efb51155e767a971c61734b1bf6293994 \\\n    --hash=sha256:9d2455fbf44c914840c793e89aa82d0e1763a14253a000743719ae5946814b2d \\\n    --hash=sha256:9d3bea1c75f8c53ee4d505c3e67d8c158ad4df0d83170605b50b64025917f338 \\\n    --hash=sha256:9e2ec1e921fd07c7cda7962bad283acc2f2a9ccc1b971ee4b216b75fad6f0463 \\\n    --hash=sha256:9e91179a242bbc99be65e139e30690e081fe6cb91a8e77faf4c409653de39451 \\\n    --hash=sha256:a0eaa93d054751ee9964afa21c06247779b90440ca41d184aeb5d410f20ff591 \\\n    --hash=sha256:a2c405445c79c3f5a124573a051062300936b0281fee57637e706453e452746c \\\n    --hash=sha256:aa7e402ce11f0885305bfb6afb3434b3cd8f53b563ac065452d9d5654c7b86fd \\\n    --hash=sha256:aff76a55a8aa8364d25400a210a65ff59d0168e0b4285ba6bf2bd83cf675ba32 \\\n    --hash=sha256:b09b86b27a064c9624d0a6c54da01c1beaf5b6cadfa609cf63789b1d08a797b9 \\\n    --hash=sha256:b14f16f94cbc61215115b9b1236f9c18403c15dd3c52cf629072afa9d54c1cbf \\\n    --hash=sha256:b50811d664d392f02f7761621303eba9d1b056fb1868c8cdf4231279645c25f5 \\\n    --hash=sha256:b7bc2176354defba3edc2b9a777744462da2f8e921fbaf61e52acb95bafa9828 \\\n    --hash=sha256:c78e1b00a87ce43bb37642c0812315b411e856a905d58d597750eb79802aaaa3 \\\n    --hash=sha256:c83341b89884e2b2e55886e8fbbf37c3fa5efd6c8907124aeb72f285ae5696e5 \\\n    --hash=sha256:ca2870d5d10d8726a27396d3ca4cf7976cec0f3cb706debe88e3a5bd4610f7d2 \\\n    --hash=sha256:ccce24b7ad89adb5a1e34a6ba96ac2530046763912806ad4c247356a8f33a67b \\\n    --hash=sha256:cd5e14fbf22a87321b24c88669aad3a51ec052eb145315b3da3b7e3cc105b9a2 \\\n    --hash=sha256:ce49c67f4ea0609933d01c0731b34b8695a7a748d6c8d186f95e7d085d2fe475 \\\n    --hash=sha256:d33891be6df59d93df4d846640f0e46f1a807339f09e79a8040bc887bdcd7ed3 \\\n    --hash=sha256:d3b2348a78bc939b4fed6552abfd2e7988e0f81443ef3911a4b8498ca084f6eb \\\n    --hash=sha256:d886f5d353333b4771d21267c7ecc75b710f1a73d72d03ca06df49b09015a9ef \\\n    --hash=sha256:d93480005693d247f8346bc8ee28c72a2191bdf1f6b5db469c096c0c867ac015 \\\n    --hash=sha256:dc1a390a82755a8c26c9964d457d4c9cbec5405896cba94cf51f36ea0d855002 \\\n    --hash=sha256:dd78700f5788ae180b5ee8902c6aea5a5726bac7c364b202b4b3e3ba2d293170 \\\n    --hash=sha256:e46f38133e5a060d46bd630faa4d9fa0202377495df1f068a8299fd78c84de84 \\\n    --hash=sha256:e4b878386c4bf293578b48fc570b84ecfe477d3b77ba39a6e87150af77f40c57 \\\n    --hash=sha256:f0d0591a0aeaefdaf9a5e545e7485f89910c977087e7de2b6c388aec32011e9f \\\n    --hash=sha256:fdcbb4068117dfd9ce0138d068ac512843c52295ed996ae6dd1faf537b6dbc27 \\\n    --hash=sha256:ff61bfd9253c3915e6d41c651d5f962da23eda633cf02262990094a18a55371a\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   imageio\n    #   scikit-image\nplatformdirs==3.11.0 \\\n    --hash=sha256:cf8ee52a3afdb965072dcc652433e0c7e3e40cf5ea1477cd4b3b1d2eb75495b3 \\\n    --hash=sha256:e9d171d00af68be50e9202731309c4e658fd8bc76f55c11c7dd760d023bda68e\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   virtualenv\nprometheus-client==0.19.0 \\\n    --hash=sha256:4585b0d1223148c27a225b10dbec5ae9bc4c81a99a3fa80774fa6209935324e1 \\\n    --hash=sha256:c88b1e6ecf6b41cd8fb5731c7ae919bf66df6ec6fafa555cd6c0e16ca169ae92\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\npropcache==0.3.0 \\\n    --hash=sha256:02df07041e0820cacc8f739510078f2aadcfd3fc57eaeeb16d5ded85c872c89e \\\n    --hash=sha256:03acd9ff19021bd0567582ac88f821b66883e158274183b9e5586f678984f8fe \\\n    --hash=sha256:03c091bb752349402f23ee43bb2bff6bd80ccab7c9df6b88ad4322258d6960fc \\\n    --hash=sha256:07700939b2cbd67bfb3b76a12e1412405d71019df00ca5697ce75e5ef789d829 \\\n    --hash=sha256:0c3e893c4464ebd751b44ae76c12c5f5c1e4f6cbd6fbf67e3783cd93ad221863 \\\n    --hash=sha256:119e244ab40f70a98c91906d4c1f4c5f2e68bd0b14e7ab0a06922038fae8a20f \\\n    --hash=sha256:11ae6a8a01b8a4dc79093b5d3ca2c8a4436f5ee251a9840d7790dccbd96cb649 \\\n    --hash=sha256:15010f29fbed80e711db272909a074dc79858c6d28e2915704cfc487a8ac89c6 \\\n    --hash=sha256:19d36bb351ad5554ff20f2ae75f88ce205b0748c38b146c75628577020351e3c \\\n    --hash=sha256:1c8f7d896a16da9455f882870a507567d4f58c53504dc2d4b1e1d386dfe4588a \\\n    --hash=sha256:2383a17385d9800b6eb5855c2f05ee550f803878f344f58b6e194de08b96352c \\\n    --hash=sha256:24c04f8fbf60094c531667b8207acbae54146661657a1b1be6d3ca7773b7a545 \\\n    --hash=sha256:2578541776769b500bada3f8a4eeaf944530516b6e90c089aa368266ed70c49e \\\n    --hash=sha256:26a67e5c04e3119594d8cfae517f4b9330c395df07ea65eab16f3d559b7068fe \\\n    --hash=sha256:2b975528998de037dfbc10144b8aed9b8dd5a99ec547f14d1cb7c5665a43f075 \\\n    --hash=sha256:2d15bc27163cd4df433e75f546b9ac31c1ba7b0b128bfb1b90df19082466ff57 \\\n    --hash=sha256:2d913d36bdaf368637b4f88d554fb9cb9d53d6920b9c5563846555938d5450bf \\\n    --hash=sha256:3302c5287e504d23bb0e64d2a921d1eb4a03fb93a0a0aa3b53de059f5a5d737d \\\n    --hash=sha256:36ca5e9a21822cc1746023e88f5c0af6fce3af3b85d4520efb1ce4221bed75cc \\\n    --hash=sha256:3b812b3cb6caacd072276ac0492d249f210006c57726b6484a1e1805b3cfeea0 \\\n    --hash=sha256:3c6ec957025bf32b15cbc6b67afe233c65b30005e4c55fe5768e4bb518d712f1 \\\n    --hash=sha256:41de3da5458edd5678b0f6ff66691507f9885f5fe6a0fb99a5d10d10c0fd2d64 \\\n    --hash=sha256:42924dc0c9d73e49908e35bbdec87adedd651ea24c53c29cac103ede0ea1d340 \\\n    --hash=sha256:4544699674faf66fb6b4473a1518ae4999c1b614f0b8297b1cef96bac25381db \\\n    --hash=sha256:46ed02532cb66612d42ae5c3929b5e98ae330ea0f3900bc66ec5f4862069519b \\\n    --hash=sha256:49ea05212a529c2caffe411e25a59308b07d6e10bf2505d77da72891f9a05641 \\\n    --hash=sha256:4fa0e7c9c3cf7c276d4f6ab9af8adddc127d04e0fcabede315904d2ff76db626 \\\n    --hash=sha256:507c5357a8d8b4593b97fb669c50598f4e6cccbbf77e22fa9598aba78292b4d7 \\\n    --hash=sha256:549722908de62aa0b47a78b90531c022fa6e139f9166be634f667ff45632cc92 \\\n    --hash=sha256:58e6d2a5a7cb3e5f166fd58e71e9a4ff504be9dc61b88167e75f835da5764d07 \\\n    --hash=sha256:5a16167118677d94bb48bfcd91e420088854eb0737b76ec374b91498fb77a70e \\\n    --hash=sha256:5d62c4f6706bff5d8a52fd51fec6069bef69e7202ed481486c0bc3874912c787 \\\n    --hash=sha256:5fa159dcee5dba00c1def3231c249cf261185189205073bde13797e57dd7540a \\\n    --hash=sha256:6032231d4a5abd67c7f71168fd64a47b6b451fbcb91c8397c2f7610e67683810 \\\n    --hash=sha256:63f26258a163c34542c24808f03d734b338da66ba91f410a703e505c8485791d \\\n    --hash=sha256:65a37714b8ad9aba5780325228598a5b16c47ba0f8aeb3dc0514701e4413d7c0 \\\n    --hash=sha256:67054e47c01b7b349b94ed0840ccae075449503cf1fdd0a1fdd98ab5ddc2667b \\\n    --hash=sha256:67dda3c7325691c2081510e92c561f465ba61b975f481735aefdfc845d2cd043 \\\n    --hash=sha256:6985a593417cdbc94c7f9c3403747335e450c1599da1647a5af76539672464d3 \\\n    --hash=sha256:6a1948df1bb1d56b5e7b0553c0fa04fd0e320997ae99689488201f19fa90d2e7 \\\n    --hash=sha256:6b5b7fd6ee7b54e01759f2044f936dcf7dea6e7585f35490f7ca0420fe723c0d \\\n    --hash=sha256:6c929916cbdb540d3407c66f19f73387f43e7c12fa318a66f64ac99da601bcdf \\\n    --hash=sha256:6f4d7a7c0aff92e8354cceca6fe223973ddf08401047920df0fcb24be2bd5138 \\\n    --hash=sha256:728af36011bb5d344c4fe4af79cfe186729efb649d2f8b395d1572fb088a996c \\\n    --hash=sha256:742840d1d0438eb7ea4280f3347598f507a199a35a08294afdcc560c3739989d \\\n    --hash=sha256:75e872573220d1ee2305b35c9813626e620768248425f58798413e9c39741f46 \\\n    --hash=sha256:794c3dd744fad478b6232289c866c25406ecdfc47e294618bdf1697e69bd64a6 \\\n    --hash=sha256:7c0fdbdf6983526e269e5a8d53b7ae3622dd6998468821d660d0daf72779aefa \\\n    --hash=sha256:7c5f5290799a3f6539cc5e6f474c3e5c5fbeba74a5e1e5be75587746a940d51e \\\n    --hash=sha256:7c6e7e4f9167fddc438cd653d826f2222222564daed4116a02a184b464d3ef05 \\\n    --hash=sha256:7cedd25e5f678f7738da38037435b340694ab34d424938041aa630d8bac42663 \\\n    --hash=sha256:7e2e068a83552ddf7a39a99488bcba05ac13454fb205c847674da0352602082f \\\n    --hash=sha256:8319293e85feadbbfe2150a5659dbc2ebc4afdeaf7d98936fb9a2f2ba0d4c35c \\\n\n```\n\n----------------------------------------\n\nTITLE: Sending Requests to the Text Summarizer Service\nDESCRIPTION: Commands to download and run a Python client script that sends a request to the deployed text summarizer model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/text-summarizer-rayservice.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# Step 5.1: Download `text_summarizer_req.py`\ncurl -LO https://raw.githubusercontent.com/ray-project/serve_config_examples/master/text_summarizer/text_summarizer_req.py\n\n# Step 5.2: Send a request to the Summarizer model.\npython text_summarizer_req.py\n# Check printed to console\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Pin Hashes for Ray Project\nDESCRIPTION: This is a Python requirements file that lists packages with their exact versions and hash values to ensure reproducibility. Each entry includes comments to indicate which parent package depends on it, helping to understand the dependency graph.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_10\n\nLANGUAGE: requirements.txt\nCODE:\n```\n--hash=sha256:ce9c432eda0dc91cf618a5cedf1a4e142651196bbcd2c80e89ed5a907e5cfaf1\n# via email-validator\ndocutils==0.19 \\\n    --hash=sha256:33995a6753c30b7f577febfc2c50411fec6aac7f7ffeb7c4cfe5991072dcf9e6 \\\n    --hash=sha256:5e1de4d849fee02c63b040a4a3fd567f4ab104defd8a5511fbbc24a8a017efbc\n    # via sphinx\neinops==0.8.1 \\\n    --hash=sha256:919387eb55330f5757c6bea9165c5ff5cfe63a642682ea788a6d472576d81737 \\\n    --hash=sha256:de5d960a7a761225532e0f1959e5315ebeafc0cd43394732f103ca44b9837e84\n    # via vllm\nemail-validator==2.2.0 \\\n    --hash=sha256:561977c2d73ce3611850a06fa56b414621e0c8faa9d66f2611407d87465da631 \\\n    --hash=sha256:cb690f344c617a714f22e66ae771445a1ceb46821152df8e165c5f9a364582b7\n    # via fastapi\nentrypoints==0.4 \\\n    --hash=sha256:b706eddaa9218a19ebcd67b56818f05bb27589b1ca9e8d797b74affad4ccacd4 \\\n    --hash=sha256:f174b5ff827504fd3cd97cc3f8649f3693f51538c7e4bdf3ef002c8429d42f9f\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jupyter-client\n    #   nbconvert\nexecuting==2.0.1 \\\n    --hash=sha256:35afe2ce3affba8ee97f2d69927fa823b08b472b7b994e36a52a964b93d16147 \\\n    --hash=sha256:eac49ca94516ccc753f9fb5ce82603156e590b27525a8bc32cce8ae302eb61bc\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   stack-data\nfarama-notifications==0.0.4 \\\n    --hash=sha256:13fceff2d14314cf80703c8266462ebf3733c7d165336eee998fc58e545efd18 \\\n    --hash=sha256:14de931035a41961f7c056361dc7f980762a143d05791ef5794a751a2caf05ae\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   gymnasium\nfastapi==0.115.0 \\\n    --hash=sha256:17ea427674467486e997206a5ab25760f6b09e069f099b96f5b55a32fb6f1631 \\\n    --hash=sha256:f93b4ca3529a8ebc6fc3fcf710e5efa8de3df9b41570958abf1d97d843138004\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   vllm\nfastapi-cli==0.0.5 \\\n    --hash=sha256:d30e1239c6f46fcb95e606f02cdda59a1e2fa778a54b64686b3ff27f6211ff9f \\\n    --hash=sha256:e94d847524648c748a5350673546bbf9bcaeb086b33c24f2e82e021436866a46\n    # via fastapi\nfastjsonschema==2.19.0 \\\n    --hash=sha256:b9fd1a2dd6971dbc7fee280a95bd199ae0dd9ce22beb91cc75e9c1c528a5170e \\\n    --hash=sha256:e25df6647e1bc4a26070b700897b07b542ec898dd4f1f6ea013e7f6a88417225\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   nbformat\nfastrlock==0.8.2 ; sys_platform != 'darwin' \\\n    --hash=sha256:067edb0a0805bf61e17a251d5046af59f6e9d2b8ad01222e0ef7a0b7937d5548 \\\n    --hash=sha256:07ed3c7b3867c05a3d6be4ced200c7767000f3431b9be6da66972822dd86e8be \\\n    --hash=sha256:08315bde19d0c2e6b06593d5a418be3dc8f9b1ee721afa96867b9853fceb45cf \\\n    --hash=sha256:11bbbbc526363955aeddb9eec4cee2a0012322b7b2f15b54f44454fcf4fd398a \\\n    --hash=sha256:17734e2e5af4c07ddb0fb10bd484e062c22de3be6b67940b9cc6ec2f18fa61ba \\\n    --hash=sha256:1b15430b93d7eb3d56f6ff690d2ebecb79ed0e58248427717eba150a508d1cd7 \\\n    --hash=sha256:1fed2f4797ad68e9982038423018cf08bec5f4ce9fed63a94a790773ed6a795c \\\n    --hash=sha256:2074548a335fcf7d19ebb18d9208da9e33b06f745754466a7e001d2b1c58dd19 \\\n    --hash=sha256:2587cedbb36c7988e707d83f0f1175c1f882f362b5ebbee25d70218ea33d220d \\\n    --hash=sha256:25945f962c7bd808415cfde3da624d4399d4ea71ed8918538375f16bceb79e1c \\\n    --hash=sha256:27786c62a400e282756ae1b090bcd7cfa35f28270cff65a9e7b27a5327a32561 \\\n    --hash=sha256:2c1719ddc8218b01e82fb2e82e8451bd65076cb96d7bef4477194bbb4305a968 \\\n    --hash=sha256:2d5595903444c854b99c42122b87edfe8a37cd698a4eae32f4fd1d2a7b6c115d \\\n    --hash=sha256:30bdbe4662992348132d03996700e1cf910d141d629179b967b146a22942264e \\\n    --hash=sha256:31a27a2edf482df72b91fe6c6438314d2c65290aa7becc55589d156c9b91f0da \\\n    --hash=sha256:320fd55bafee3eb069cfb5d6491f811a912758387ef2193840e2663e80e16f48 \\\n    --hash=sha256:33145acbad8317584cd64588131c7e1e286beef6280c0009b4544c91fce171d2 \\\n    --hash=sha256:43a241655e83e4603a152192cf022d5ca348c2f4e56dfb02e5c9c4c1a32f9cdb \\\n    --hash=sha256:4d63b6596368dab9e0cc66bf047e7182a56f33b34db141816a4f21f5bf958228 \\\n    --hash=sha256:4fb04442b6d1e2b36c774919c6bcbe3339c61b337261d4bd57e27932589095af \\\n    --hash=sha256:4fb2e77ff04bc4beb71d63c8e064f052ce5a6ea1e001d528d4d7f4b37d736f2e \\\n    --hash=sha256:5460c5ee6ced6d61ec8cd2324ebbe793a4960c4ffa2131ffff480e3b61c99ec5 \\\n    --hash=sha256:59344c1d46b7dec97d3f22f1cc930fafe8980b3c5bc9c9765c56738a5f1559e4 \\\n    --hash=sha256:5dfb78dd600a12f23fc0c3ec58f81336229fdc74501ecf378d1ce5b3f2f313ea \\\n    --hash=sha256:643e1e65b4f5b284427e61a894d876d10459820e93aa1e724dfb415117be24e0 \\\n    --hash=sha256:644ec9215cf9c4df8028d8511379a15d9c1af3e16d80e47f1b6fdc6ba118356a \\\n    --hash=sha256:66f2662c640bb71a1016a031eea6eef9d25c2bcdf7ffd1d1ddc5a58f9a1ced04 \\\n    --hash=sha256:685e656048b59d8dfde8c601f188ad53a4d719eb97080cafc8696cda6d75865e \\\n    --hash=sha256:7269bb3fc15587b0c191eecd95831d771a7d80f0c48929e560806b038ff3066c \\\n    --hash=sha256:73426f5eb2ecc10626c67cf86bd0af9e00d53e80e5c67d5ce8e18376d6abfa09 \\\n    --hash=sha256:75c07726c8b1a52147fd7987d6baaa318c5dced1416c3f25593e40f56e10755b \\\n    --hash=sha256:790fc19bccbd39426060047e53629f171a44745613bf360a045e9f9c8c4a2cea \\\n    --hash=sha256:7a2ccaf88ac0db153e84305d1ef0aa138cea82c6a88309066f6eaa3bc98636cd \\\n    --hash=sha256:87f4e01b042c84e6090dbc4fbe3415ddd69f6bc0130382323f9d3f1b8dd71b46 \\\n    --hash=sha256:88f079335e9da631efa64486c8207564a7bcd0c00526bb9e842e9d5b7e50a6cc \\\n    --hash=sha256:8c1c91a68926421f5ccbc82c85f83bd3ba593b121a46a1b9a554b3f0dd67a4bf \\\n    --hash=sha256:9121a894d74e65557e47e777060a495ab85f4b903e80dd73a3c940ba042920d7 \\\n    --hash=sha256:94e348c72a1fd1f8191f25ea056448e4f5a87b8fbf005b39d290dcb0581a48cd \\\n    --hash=sha256:98195866d3a9949915935d40a88e4f1c166e82e378f622c88025f2938624a90a \\\n    --hash=sha256:99dd6652bd6f730beadf74ef769d38c6bbd8ee6d1c15c8d138ea680b0594387f \\\n    --hash=sha256:9af691a9861027181d4de07ed74f0aee12a9650ac60d0a07f4320bff84b5d95f \\\n    --hash=sha256:a3b8b5d2935403f1b4b25ae324560e94b59593a38c0d2e7b6c9872126a9622ed \\\n    --hash=sha256:a3dcc876050b8f5cbc0ee84ef1e7f0c1dfe7c148f10098828bc4403683c33f10 \\\n    --hash=sha256:a74f5a92fa6e51c4f3c69b29c4662088b97be12f40652a21109605a175c81824 \\\n    --hash=sha256:ab91b0c36e95d42e1041a4907e3eefd06c482d53af3c7a77be7e214cc7cd4a63 \\\n    --hash=sha256:ad1bc61c7f6b0e58106aaab034916b6cb041757f708b07fbcdd9d6e1ac629225 \\\n    --hash=sha256:adcb9e77aa132cc6c9de2ffe7cf880a20aa8cdba21d367d1da1a412f57bddd5d \\\n    --hash=sha256:b22ea9bf5f9fad2b0077e944a7813f91593a4f61adf8faf734a70aed3f2b3a40 \\\n    --hash=sha256:b2a1c354f13f22b737621d914f3b4a8434ae69d3027a775e94b3e671756112f9 \\\n    --hash=sha256:b32fdf874868326351a75b1e4c02f97e802147119ae44c52d3d9da193ec34f5b \\\n    --hash=sha256:b3853ed4ce522598dc886160a7bab432a093051af85891fa2f5577c1dcac8ed6 \\\n    --hash=sha256:b443e73a4dfc7b6e0800ea4c13567b9694358e86f53bb2612a51c9e727cac67b \\\n    --hash=sha256:b4c9083ea89ab236b06e9ef2263971db3b4b507195fc7d5eecab95828dcae325 \\\n    --hash=sha256:b8ca0fe21458457077e4cb2d81e1ebdb146a00b3e9e2db6180a773f7ea905032 \\\n    --hash=sha256:c393af77c659a38bffbca215c0bcc8629ba4299568308dd7e4ff65d62cabed39 \\\n    --hash=sha256:c6bffa978793bea5e1b00e677062e53a62255439339591b70e209fa1552d5ee0 \\\n    --hash=sha256:ccf39ad5702e33e4d335b48ef9d56e21619b529b7f7471b5211419f380329b62 \\\n    --hash=sha256:cf81e0278b645004388873e0a1f9e3bc4c9ab8c18e377b14ed1a544be4b18c9a \\\n    --hash=sha256:d34546ad2e4a480b94b6797bcc5a322b3c705c4c74c3e4e545c4a3841c1b2d59 \\\n    --hash=sha256:d47713ffe6d4a627fbf078be9836a95ac106b4a0543e3841572c91e292a5d885 \\\n    --hash=sha256:d918dfe473291e8bfd8e13223ea5cb9b317bd9f50c280923776c377f7c64b428 \\\n    --hash=sha256:dbdce852e6bb66e1b8c36679d482971d69d93acf1785657522e51b7de30c3356 \\\n    --hash=sha256:dcc1bf0ac8a194313cf6e645e300a8a379674ceed8e0b1e910a2de3e3c28989e \\\n    --hash=sha256:dd961a32a7182c3891cdebca417fda67496d5d5de6ae636962254d22723bdf52 \\\n    --hash=sha256:ddf5d247f686aec853ddcc9a1234bfcc6f57b0a0670d2ad82fc25d8ae7e6a15f \\\n    --hash=sha256:e27c3cd27fbd25e5223c5c992b300cd4ee8f0a75c6f222ce65838138d853712c \\\n    --hash=sha256:e380ec4e6d8b26e389713995a43cb7fe56baea2d25fe073d4998c4821a026211 \\\n    --hash=sha256:e4bbde174a0aff5f6eeba75cf8c4c5d2a316316bc21f03a0bddca0fc3659a6f3 \\\n    --hash=sha256:e8b49b5743ede51e0bcf6805741f39f5e0e0fd6a172ba460cb39e3097ba803bb \\\n    --hash=sha256:e9904b5b37c3e5bb4a245c56bc4b7e497da57ffb8528f4fc39af9dcb168ee2e1 \\\n    --hash=sha256:ea96503b918fceaf40443182742b8964d47b65c5ebdea532893cb9479620000c \\\n    --hash=sha256:eb31fe390f03f7ae886dcc374f1099ec88526631a4cb891d399b68181f154ff0 \\\n    --hash=sha256:ebb32d776b61acd49f859a1d16b9e3d84e7b46d0d92aebd58acd54dc38e96664 \\\n    --hash=sha256:fb5363cf0fddd9b50525ddbf64a1e1b28ec4c6dfb28670a940cb1cf988a6786b \\\n    --hash=sha256:ff75c90663d6e8996610d435e71487daa853871ad1770dd83dc0f2fc4997241e\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   cupy-cuda12x\nfilelock==3.17.0 \\\n    --hash=sha256:533dc2f7ba78dc2f0f531fc6c4940addf7b70a481e269a5a3b93be94ffbe8338 \\\n    --hash=sha256:ee4e77401ef576ebb38cd7f13b9b28893194acc20a8e68e18730ba9c0e54660e\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   huggingface-hub\n    #   ray\n    #   torch\n    #   transformers\n    #   virtualenv\n    #   vllm\nfqdn==1.5.1 \\\n    --hash=sha256:105ed3677e767fb5ca086a0c1f4bb66ebc3c100be518f0e0d755d9eae164d89f \\\n    --hash=sha256:3a179af3761e4df6eb2e026ff9e1a3033d3587bf980a0b1b2e1e5d08d7358014\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jsonschema\nfrozenlist==1.4.1 \\\n    --hash=sha256:04ced3e6a46b4cfffe20f9ae482818e34eba9b5fb0ce4056e4cc9b6e212d09b7 \\\n    --hash=sha256:0633c8d5337cb5c77acbccc6357ac49a1770b8c487e5b3505c57b949b4b82e98 \\\n    --hash=sha256:068b63f23b17df8569b7fdca5517edef76171cf3897eb68beb01341131fbd2ad \\\n    --hash=sha256:0c250a29735d4f15321007fb02865f0e6b6a41a6b88f1f523ca1596ab5f50bd5\n```\n\n----------------------------------------\n\nTITLE: Retrieving Job Logs using Ray Jobs REST API in Python\nDESCRIPTION: This snippet demonstrates how to retrieve logs for a specific job using the Ray Jobs REST API. It sends a GET request to the job's logs endpoint and prints the retrieved logs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/rest.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresp = requests.get(\n    f\"http://127.0.0.1:8265/api/jobs/{job_id}/logs\"\n)\nrst = json.loads(resp.text)\nlogs = rst[\"logs\"]\nprint(logs)\n```\n\n----------------------------------------\n\nTITLE: Installing RayDP with pip\nDESCRIPTION: Command to install RayDP from PyPI, which supports PySpark 3.0 and 3.1. Requires Ray >= 1.2.0 and Java installed on head and worker nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/raydp.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install raydp\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Package dependency specifications with pinned versions and SHA256 hashes for secure installation verification. Includes core packages like charset-normalizer, click, cloudpickle and others with their exact versions and hash values.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\ncharset-normalizer==3.3.2 \\\n    --hash=sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027 \\\n    --hash=sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087\n\nclick==8.1.7 \\\n    --hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28 \\\n    --hash=sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de\n\ncloudpickle==2.2.0 \\\n    --hash=sha256:3f4219469c55453cfe4737e564b67c2a149109dabf7f242478948b895f61106f \\\n    --hash=sha256:7428798d5926d8fcbfd092d18d01a2a03daf8237d8fcdc8095d256b8490796f0\n```\n\n----------------------------------------\n\nTITLE: Fetching Object Data in Ray (Java)\nDESCRIPTION: Demonstrates how to retrieve object data from object references using Ray.get() in Java. It includes examples of getting single and multiple object references, setting a timeout, and handling timeout exceptions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects.rst#2025-04-12_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n// Get the value of one object ref.\nObjectRef<Integer> objRef = Ray.put(1);\nAssert.assertTrue(objRef.get() == 1);\n// You can also set a timeout(ms) to return early from a ``get`` that's blocking for too long.\nAssert.assertTrue(objRef.get(1000) == 1);\n\n// Get the values of multiple object refs in parallel.\nList<ObjectRef<Integer>> objectRefs = new ArrayList<>();\nfor (int i = 0; i < 3; i++) {\n  objectRefs.add(Ray.put(i));\n}\nList<Integer> results = Ray.get(objectRefs);\nAssert.assertEquals(results, ImmutableList.of(0, 1, 2));\n\n// Ray.get timeout example: Ray.get will throw an RayTimeoutException if time out.\npublic class MyRayApp {\n  public static int slowFunction() throws InterruptedException {\n    TimeUnit.SECONDS.sleep(10);\n    return 1;\n  }\n}\nAssert.assertThrows(RayTimeoutException.class,\n  () -> Ray.get(Ray.task(MyRayApp::slowFunction).remote(), 3000));\n```\n\n----------------------------------------\n\nTITLE: Setting Up Nsight Profiling for Ray Compiled Graph (Python)\nDESCRIPTION: Code snippet demonstrating how to set up Nsight profiling for Ray Compiled Graph by specifying the runtime environment for involved actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/profiling.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nruntime_env = {\n    \"env_vars\": {\n        \"NSYS_SESSION_TEMPLATE\": \"%q{SESSIONNAME}-%p-%h-%t{%y%m%d}\",\n        \"NSYS_TARGET_PROCESSES\": \"python3\",\n        \"NSYS_OUTPUT_DIR\": \"/tmp/ray/session_*/logs\",\n        \"NSYS_NODE_STRATEGY\": \"unique\",\n        \"NSYS_PROFILE_PYTHON\": \"true\",\n        \"NSYS_CUDABACKTRACE\": \"dwarf\",\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: View tuning results as a dataframe\nDESCRIPTION: Retrieves the results of the tuning run as a Pandas DataFrame and sorts them by evaluation loss.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntune_results.get_dataframe().sort_values(\"eval_loss\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Retrieving Global Named Actors in C++\nDESCRIPTION: Illustrates how to create an actor with a globally unique name and retrieve it later using Ray in C++. This allows accessing the actor from any job in the Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/named-actors.rst#2025-04-12_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n// Create an actor with a globally unique name\nActorHandle<Counter> counter = ray::Actor(CreateCounter).SetGlobalName(\"some_name\").Remote();\n\n...\n\n// Retrieve the actor later somewhere\nboost::optional<ray::ActorHandle<Counter>> counter = ray::GetGlobalActor(\"some_name\");\n```\n\n----------------------------------------\n\nTITLE: Starting a Local Ray Cluster\nDESCRIPTION: Command to start a local Ray cluster head node for development purposes\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/sdk.rst#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nray start --head\n```\n\n----------------------------------------\n\nTITLE: Configuring Number of Trials in Ray Tune\nDESCRIPTION: Example showing how to configure a specific number of trials for hyperparameter tuning experiments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/key-concepts.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    trainable,\n    param_space=param_space,\n    tune_config=tune.TuneConfig(\n        num_samples=10  # Number of times to run the experiment\n    )\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Downloading RayJob Configuration\nDESCRIPTION: This command downloads the RayJob configuration file that executes the PyTorch distributed training example.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/pytorch-resnet-image-classifier/ray-job.pytorch-image-classifier.yaml\n```\n\n----------------------------------------\n\nTITLE: Handling Databricks Configuration Error\nDESCRIPTION: This snippet addresses an error related to Databricks configuration, providing context on moving configuration files to shared locations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_10\n\nLANGUAGE: none\nCODE:\n```\ndatabricks_cli.utils.InvalidConfigurationError: You haven't configured the CLI yet!\n```\n\n----------------------------------------\n\nTITLE: Executing Commands on Ray Clusters\nDESCRIPTION: Examples of running shell commands on Ray clusters with various options including tmux sessions, port forwarding, and cluster lifecycle management.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-cli.rst#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Run a command on the cluster\n$ ray exec cluster.yaml 'echo \"hello world\"'\n\n# Run a command on the cluster, starting it if needed\n$ ray exec cluster.yaml 'echo \"hello world\"' --start\n\n# Run a command on the cluster, stopping the cluster after it finishes\n$ ray exec cluster.yaml 'echo \"hello world\"' --stop\n\n# Run a command on a new cluster called 'experiment-1', stopping it after\n$ ray exec cluster.yaml 'echo \"hello world\"' \\\n    --start --stop --cluster-name experiment-1\n\n# Run a command in a detached tmux session\n$ ray exec cluster.yaml 'echo \"hello world\"' --tmux\n\n# Run a command in a screen (experimental)\n$ ray exec cluster.yaml 'echo \"hello world\"' --screen\n```\n\n----------------------------------------\n\nTITLE: Including MNIST PyTorch Lightning Example with Literalinclude in RST\nDESCRIPTION: A reStructuredText directive that includes the content of an external Python file (`mnist_ptl_mini.py`) that demonstrates PyTorch Lightning with Ray Tune integration for MNIST dataset classification.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/mnist_ptl_mini.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. literalinclude:: /../../python/ray/tune/examples/mnist_ptl_mini.py\n```\n\n----------------------------------------\n\nTITLE: Basic RayService and Kubernetes Operations\nDESCRIPTION: Commands for applying RayService config, checking status, and sending requests to the service.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f ray-service.sample.yaml\n\nkubectl describe rayservices rayservice-sample\n\ncurl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/fruit/ -d '[\"MANGO\", 2]'\n```\n\n----------------------------------------\n\nTITLE: Submitting Ray Job with Authentication\nDESCRIPTION: Command to submit a Ray job with authentication headers, which should now succeed with the proper authorization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nray job submit --address http://localhost:8265  -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n```\n\n----------------------------------------\n\nTITLE: Full Azure Configuration (Autoscaler)\nDESCRIPTION: This YAML configuration represents a comprehensive Ray autoscaler setup for Azure, showcasing a detailed configuration with various options. It defines node types, resource allocation, and advanced settings for managing a Ray cluster on Azure.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/azure/example-full.yaml\n            :language: yaml\n\n```\n\n----------------------------------------\n\nTITLE: Seeding Random Number Generators in Python\nDESCRIPTION: This example shows how to seed the random number generators in Python's random and numpy modules for reproducibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport numpy as np\n\nrandom.seed(1234)\nnp.random.seed(5678)\n```\n\n----------------------------------------\n\nTITLE: Text Processing Pipeline Setup and Execution\nDESCRIPTION: Example setup and execution of the text processing pipeline including dataset creation, vocabulary building, and module specification configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nds = data.read_text(\"s3://anonymous@ray-example-data/this.txt\")\n\ntokens = []\nfor b in ds.iter_rows():\n    tokens.extend(b[\"text\"].split(\" \"))\nvocabulary = {token: idx for idx, token in enumerate(set(tokens), start=1)}\n\nmodule_spec = MultiRLModuleSpec(\n    rl_module_specs={\n        \"default_policy\": RLModuleSpec(\n            model_config=DefaultModelConfig(\n                conv_filters=[[16, 4, 2], [32, 4, 2], [64, 4, 2], [128, 4, 2]],\n                conv_activation=\"relu\",\n            ),\n            inference_only=False,\n            module_class=DefaultBCTorchRLModule,\n            catalog_class=BCCatalog,\n            action_space = gym.spaces.Discrete(len(vocabulary)),\n            observation_space=gym.spaces.Box(0.0, 1.0, (len(vocabulary), 1, 1), np.float32),\n        ),\n    },\n)\n\nbatch = ds.take_batch(10)\n\noplr = TextOfflinePreLearner(\n    config=AlgorithmConfig(),\n    spaces=(\n        gym.spaces.Discrete(len(vocabulary)),\n        gym.spaces.Box(0.0, 1.0, (len(vocabulary), 1, 1), np.float32)),\n    module_spec=module_spec,\n    vocabulary=vocabulary,\n)\n\ntransformed = oplr(batch)\n\nprint(f\"Batch: {batch}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray\nDESCRIPTION: This snippet initializes Ray with the configure_logging parameter set to False to suppress logging output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nray.init(configure_logging=False)\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Modin RayJob on Kubernetes\nDESCRIPTION: Command to apply the Modin example RayJob configuration to your Kubernetes cluster using kubectl. This references a sample YAML file from the KubeRay repository.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/modin-example.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-job.modin.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying RayJob Configuration\nDESCRIPTION: kubectl command to apply a RayJob configuration from a sample YAML file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-job.sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for GradioServer Example\nDESCRIPTION: Required imports for setting up a Gradio app with Ray Serve including GradioServer and transformers pipeline\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/gradio-integration.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import serve\nfrom ray.serve.gradio_integrations import GradioServer\nimport gradio as gr\nfrom transformers import pipeline\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenCV Python Headless Package with Hash Verification\nDESCRIPTION: Dependency specification for OpenCV Python headless package which provides computer vision capabilities without GUI components. Required by Mistral Common library for image processing tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_20\n\nLANGUAGE: pip\nCODE:\n```\nopencv-python-headless==4.11.0.86 \\\n    --hash=sha256:0e0a27c19dd1f40ddff94976cfe43066fbbe9dfbb2ec1907d66c19caef42a57b \\\n    --hash=sha256:48128188ade4a7e517237c8e1e11a9cdf5c282761473383e77beb875bb1e61ca \\\n    --hash=sha256:6c304df9caa7a6a5710b91709dd4786bf20a74d57672b3c31f7033cc638174ca \\\n    --hash=sha256:6efabcaa9df731f29e5ea9051776715b1bdd1845d7c9530065c7951d2a2899eb \\\n    --hash=sha256:996eb282ca4b43ec6a3972414de0e2331f5d9cda2b41091a49739c19fb843798 \\\n    --hash=sha256:a66c1b286a9de872c343ee7c3553b084244299714ebb50fbdcd76f07ebbe6c81 \\\n    --hash=sha256:f447d8acbb0b6f2808da71fddd29c1cdd448d2bc98f72d9bb78a7a898fc9621b\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   mistral-common\n```\n\n----------------------------------------\n\nTITLE: Initial Ray Tune Configuration Example\nDESCRIPTION: Example showing initial Tune experiment configuration with hyperparameter search space and training for 10 epochs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n__iter_experimentation_initial_start__\n```\n\n----------------------------------------\n\nTITLE: Running Ray Debugger CLI Command\nDESCRIPTION: Command to attach to active breakpoints in a Ray application. This starts the interactive debugger session that connects to any active breakpoints in the Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/ray-debugging.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython debugging.py\n```\n\nLANGUAGE: bash\nCODE:\n```\nray debug\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Ray Cluster Access\nDESCRIPTION: Shell command to set up port forwarding for accessing the Ray cluster's services.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/ml-example.md#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Run the following blocking command in a separate shell.\nkubectl port-forward service/raycluster-xgboost-benchmark-head-svc 8265:8265\n```\n\n----------------------------------------\n\nTITLE: Cutting a SingleAgentEpisode\nDESCRIPTION: This snippet shows how to cut a SingleAgentEpisode, creating a new episode chunk for continued data collection. The cut method is used when the EnvRunner needs to return collected data before an episode is fully terminated.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/single-agent-episode.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example of cutting a SingleAgentEpisode to create a new (continuation) chunk.\n\"\"\"\n\nimport numpy as np\n\nfrom ray.rllib.env.single_agent_episode import SingleAgentEpisode\n\n# Create a dummy episode.\nepisode = SingleAgentEpisode()\n\n# The episode is initially non-numpy'ized, meaning that all data\n# is stored as plain python lists.\nepisode.reset(obs=1.0)\nepisode.add_action(action=0)\nepisode.add_reward(reward=1.0)\nepisode.add_observation(obs=2.0)\nepisode.add_action(action=1)\nepisode.add_reward(reward=-0.5)\nepisode.add_observation(obs=3.0)\n\n# Cut the episode, thereby creating a new continuation episode.\nepisode_chunk = episode.cut()\n\n# The new chunk is also non-finalized (non-numpy'ized), but\n# holds no data yet.\nassert len(episode_chunk.get_observations()) == 0\n\n# The original episode still holds the data.\nassert len(episode.get_observations()) == 3\n\n```\n\n----------------------------------------\n\nTITLE: Example TLS Certificate Usage in Secure Communication\nDESCRIPTION: This code snippet demonstrates how TLS certificates are used in secure communication, showing how a client initiates a connection with a server using a certificate chain for authentication.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/tls/ca.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nClient                                      Server\n----------                                    ----------\n                                            Load private key and certificate chain\n                                            <-----------------+\nConnect to Server\n--------------->\nSend ClientHello with TLS version          Process ClientHello\n-------------------------------------->\n                                            Send ServerHello with TLS version\n                                            and selected cipher suite\n<----------------------------------------\n                                            Send Certificate message with\n                                            certificate chain\n<----------------------------------------\n                                            Send ServerHelloDone\n<----------------------------------------\nVerify server certificate\nGenerate premaster secret\nSend ClientKeyExchange\n-------------------------------------->\nSend ChangeCipherSpec\n-------------------------------------->\nSend Finished\n-------------------------------------->\n                                            Process ClientKeyExchange\n                                            Process ChangeCipherSpec\n                                            Process Finished\n                                            Send ChangeCipherSpec\n<----------------------------------------\n                                            Send Finished\n<----------------------------------------\nProcess ChangeCipherSpec\nProcess Finished\nApplication Data\n<------------------------------------->\nClose connection\n--------------->\n```\n\n----------------------------------------\n\nTITLE: Setting Up HuggingFace Pipeline for Text Generation with Fine-tuned Dolly-v2 Model in Python\nDESCRIPTION: This code sets up a HuggingFace Pipeline for text generation using the fine-tuned Dolly-v2 model. It loads the trained model from a checkpoint, initializes the tokenizer, and creates a text generation pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n\nckpt_path = os.path.join(result.checkpoint.path, \"checkpoint.ckpt\")\n\ndolly = DollyV2Model.load_from_checkpoint(ckpt_path, map_location=torch.device(\"cpu\"))\n\nnlp_pipeline = pipeline(\n    task=\"text-generation\", \n    model=dolly.model, \n    tokenizer=tokenizer, \n    device_map=\"auto\"\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary libraries for data processing, machine learning, and Ray Tune integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pickle\nfrom tempfile import TemporaryDirectory\n\nprint(f\"Number of CPUs in this system: {os.cpu_count()}\")\nfrom typing import Tuple, List, Union, Optional, Callable\nimport time\nimport pandas as pd\nimport numpy as np\n\nprint(f\"numpy: {np.__version__}\")\nimport pyarrow\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as pds\n\nprint(f\"pyarrow: {pyarrow.__version__}\")\n```\n\n----------------------------------------\n\nTITLE: Removing Resource Requests from Ray Autoscaler\nDESCRIPTION: This Python snippet shows how to remove previously made resource requests from the Ray autoscaler. By passing an empty list of bundles, it allows the GPU workers to scale down when idle.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gpu.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init()\nray.autoscaler.sdk.request_resources(bundles=[])\n```\n\n----------------------------------------\n\nTITLE: Configuring NGINX Ingress Annotation for Grafana on Kubernetes\nDESCRIPTION: YAML configuration snippet for Grafana ingress to allow proper embedding in Ray Dashboard when exposed with NGINX ingress on a Kubernetes cluster. The annotation ensures the proper X-Frame-Options header is set.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/configure-manage-dashboard.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nnginx.ingress.kubernetes.io/configuration-snippet: |\n    add_header X-Frame-Options SAMEORIGIN always;\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Fault Tolerance in RLlib with Python\nDESCRIPTION: Manages environment fault tolerance by turning on `restart_failed_sub_environments`. This allows individual environments to be restarted without affecting the entire worker. Important configurations include setting `num_envs_per_env_runner` and potentially disabling environment retries for more stable operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-fault-tolerance.rst#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n\"restart_failed_sub_environments\"\n```\n\n----------------------------------------\n\nTITLE: Ray Framework Performance Metrics Output\nDESCRIPTION: Benchmark results showing execution times for five different Ray operations: handling many arguments (10000), processing multiple returns (3000), Ray.get calls (10000), queued task processing (1M tasks), and handling large objects (100GB).\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.5.0/scalability/single_node.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nMany args time: 13.32423606399999 (10000 args)\nMany returns time: 5.455830246000005 (3000 returns)\nRay.get time: 30.079316559000006 (10000 args)\nQueued task time: 154.73578189300002 (1000000 tasks)\nRay.get large object time: 257.75048660999994 (107374182400 bytes)\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Tracking Refresh Period\nDESCRIPTION: TUNE_STATE_REFRESH_PERIOD sets how often (in seconds) the resource tracking information is updated from Ray. Defaults to 10 seconds.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_STATE_REFRESH_PERIOD=10\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Ray Tune and Optuna in Python\nDESCRIPTION: This snippet installs the necessary libraries for the tutorial. It uses pip to install Ray Tune and specifies a version constraint for Optuna to ensure compatibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# !pip install \"ray[tune]\"\n!pip install -q \"optuna>=3.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Verifying RayService Deployment Status on Kubernetes\nDESCRIPTION: This command checks the status of the deployed RayService custom resource, showing its service status and the number of Serve endpoints.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/tpu-serve-stable-diffusion.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get rayservice\n```\n\n----------------------------------------\n\nTITLE: Displaying Cluster-wide Spill Stats\nDESCRIPTION: This example shows the output of the 'ray memory' command, which displays aggregate object store statistics across all nodes, including spilling and restoring information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/object-spilling.rst#2025-04-12_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n--- Aggregate object store stats across all nodes ---\nPlasma memory usage 50 MiB, 1 objects, 50.0% full\nSpilled 200 MiB, 4 objects, avg write throughput 570 MiB/s\nRestored 150 MiB, 3 objects, avg read throughput 1361 MiB/s\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Ray Data Optimization Rule\nDESCRIPTION: Demonstrates how to create and register a custom optimization rule for Ray Data's logical plan optimizer.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/data-internals.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray.data._internal.logical.interfaces import Rule\n\nclass CustomRule(Rule):\n    def apply(self, plan):\n        ...\n\nray.data._internal.logical.optimizers.DEFAULT_LOGICAL_RULES.append(CustomRule)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Ray Tune\nDESCRIPTION: Installs the necessary Python packages for running Ray Tune with scikit-learn.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"ray[tune]\" scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Installing Google BigQuery Client Libraries\nDESCRIPTION: These commands install the necessary Python client libraries for Google BigQuery: `google-cloud-bigquery` and `google-cloud-bigquery-storage`. These are required to interact with BigQuery from Python.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_32\n\nLANGUAGE: console\nCODE:\n```\npip install google-cloud-bigquery\npip install google-cloud-bigquery-storage\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus for Ray Metrics\nDESCRIPTION: YAML configuration for Prometheus to scrape metrics from Ray nodes using file-based service discovery.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/metrics.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n# Scrape from each Ray node as defined in the service_discovery.json provided by Ray.\n- job_name: 'ray'\n  file_sd_configs:\n  - files:\n    - '/tmp/ray/prom_metrics_service_discovery.json' # or '${your_temp_path}/prom_metrics_service_discovery.json' if --temp-dir is specified\n```\n\n----------------------------------------\n\nTITLE: Local Testing Setup Commands\nDESCRIPTION: Commands for installing and running the test framework for documentation examples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/writing-code-snippets.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/ray-project/pytest-sphinx\n\npytest --doctest-modules python/ray/data/read_api.py\npytest --doctest-modules python/ray/data/read_api.py::ray.data.read_api.range\npytest --doctest-modules doc/source/data/getting-started.rst\n```\n\n----------------------------------------\n\nTITLE: Installing rfc3986-validator Package with Hash Verification\nDESCRIPTION: This snippet specifies the installation of rfc3986-validator package version 0.1.1 with SHA256 hash verification. It also lists the packages that depend on this validator in the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_47\n\nLANGUAGE: pip\nCODE:\n```\nrfc3986-validator==0.1.1 \\\n    --hash=sha256:2f235c432ef459970b4306369336b9d5dbdda31b510ca1e327636e01f528bfa9 \\\n    --hash=sha256:3d44bde7921b3b9ec3ae4e3adca370438eccebc676456449b145d533b240d055\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   jsonschema\n    #   jupyter-events\n```\n\n----------------------------------------\n\nTITLE: Identifying Ray Head Service\nDESCRIPTION: Retrieves information about the Kubernetes service for the Ray head node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl get service service-ray-cluster\n```\n\n----------------------------------------\n\nTITLE: Applying Kueue Resource Configuration\nDESCRIPTION: Command to apply the Kueue resource configuration YAML to the Kubernetes cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f kueue-resources.yaml\n```\n\n----------------------------------------\n\nTITLE: Managing Python Deployment with Java\nDESCRIPTION: This Java code demonstrates how to deploy and manage a Python deployment using the Java API. It sets the code search path, creates a deployment, deploys it, and calls a method on the deployment's handle.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/managing-java-deployments.md#2025-04-12_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nimport io.ray.api.Ray;\nimport io.ray.serve.api.Serve;\nimport io.ray.serve.deployment.Deployment;\nimport io.ray.serve.generated.DeploymentLanguage;\nimport java.io.File;\n\npublic class ManagePythonDeployment {\n\n  public static void main(String[] args) {\n\n    System.setProperty(\n        \"ray.job.code-search-path\",\n        System.getProperty(\"java.class.path\") + File.pathSeparator + \"/path/to/code/\");\n\n    Serve.start(true, false, null);\n\n    Deployment deployment =\n        Serve.deployment()\n            .setDeploymentLanguage(DeploymentLanguage.PYTHON)\n            .setName(\"counter\")\n            .setDeploymentDef(\"counter.Counter\")\n            .setNumReplicas(1)\n            .setInitArgs(new Object[] {\"1\"})\n            .create();\n    deployment.deploy(true);\n\n    System.out.println(Ray.get(deployment.getHandle().method(\"increase\").remote(\"2\")));\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Define the training function for PBT\nDESCRIPTION: This defines the `train_func` function, which represents the training loop for each trial in PBT.  It loads hyperparameters, initializes the model (resuming from a checkpoint if available), performs gradient ascent steps, calculates metrics, checkpoints the model periodically, and reports the results to Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"def train_func(config):\n    # Load the hyperparam config passed in by the Tuner\n    h0 = config.get(\\\"h0\\\")\n    h1 = config.get(\\\"h1\\\")\n    h = np.array([h0, h1]).astype(float)\n\n    lr = config.get(\\\"lr\\\")\n    train_step = 1\n    checkpoint_interval = config.get(\\\"checkpoint_interval\\\", 1)\n\n    # Initialize the model parameters\n    theta = get_init_theta()\n\n    # Load a checkpoint if it exists\n    # This checkpoint could be a trial's own checkpoint to resume,\n    # or another trial's checkpoint placed by PBT that we will exploit\n    checkpoint = tune.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            with open(os.path.join(checkpoint_dir, \\\"checkpoint.pkl\\\"), \\\"rb\\\") as f:\n                checkpoint_dict = pickle.load(f)\n        # Load in model (theta)\n        theta = checkpoint_dict[\\\"theta\\\"]\n        last_step = checkpoint_dict[\\\"train_step\\\"]\n        train_step = last_step + 1\n\n    # Main training loop (trial stopping is configured later)\n    while True:\n        # Perform gradient ascent steps\n        param_grads = grad_Qhat(theta, h)\n        theta_grad = np.asarray(param_grads[\\\"theta\\\"])\n        theta = theta + lr * theta_grad\n\n        # Define which custom metrics we want in our trial result\n        result = {\n            \\\"Q\\\": Q(theta),\n            \\\"theta0\\\": theta[0],\n            \\\"theta1\\\": theta[1],\n            \\\"h0\\\": h0,\n            \\\"h1\\\": h1,\n            \\\"train_step\\\": train_step,\n        }\n\n        # Checkpoint every `checkpoint_interval` steps\n        should_checkpoint = train_step % checkpoint_interval == 0\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            checkpoint = None\n            if should_checkpoint:\n                checkpoint_dict = {\n                    \\\"h\\\": h,\n                    \\\"train_step\\\": train_step,\n                    \\\"theta\\\": theta,\n                }\n                with open(\n                    os.path.join(temp_checkpoint_dir, \\\"checkpoint.pkl\\\"), \\\"wb\\\"\n                ) as f:\n                    pickle.dump(checkpoint_dict, f)\n                checkpoint = tune.Checkpoint.from_directory(temp_checkpoint_dir)\n\n            # Report metric for this training iteration, and include the\n            # trial checkpoint that contains the current parameters if we\n            # saved it this train step\n            tune.report(result, checkpoint=checkpoint)\n\n        train_step += 1\n\"\n```\n\n----------------------------------------\n\nTITLE: Setting Smoke Test Flag\nDESCRIPTION: A flag to control whether to run a quick test with reduced parameters or the full test. This helps with debugging and quick verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# If you want to run full test, please set SMOKE_TEST to False\nSMOKE_TEST = True\n```\n\n----------------------------------------\n\nTITLE: Classification Response Output\nDESCRIPTION: Example response from the text classification endpoint showing the predicted label and confidence score.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/text-classification.md#2025-04-12_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n200 [{'label': 'LABEL_1', 'score': 0.9994940757751465}]\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Jobs SDK with pip\nDESCRIPTION: Command to install Ray with all default components required for Ray Jobs\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/sdk.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install \"ray[default]\"\n```\n\n----------------------------------------\n\nTITLE: Specifying PyCurl Package Version with Hash Verification for Ray Project\nDESCRIPTION: Defines PyCurl version 7.45.3 with multiple SHA256 hashes for verification. This package is referenced via cloud requirements and is required for HTTP/FTP functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_35\n\nLANGUAGE: pip requirements\nCODE:\n```\npycurl==7.45.3 \\\n    --hash=sha256:0c41a172d5e8a5cdd8328cc8134f47b2a57960ac677f7cda8520eaa9fbe7d990 \\\n    --hash=sha256:0f0e1251a608ffd75fc502f4014442e554c67d3d7a1b0a839c35efb6ad2f8bf8 \\\n    --hash=sha256:13006b62c157bb4483c58e1abdced6df723c9399255a4f5f6bb7f8e425106679 \\\n    --hash=sha256:1610cc45b5bc8b39bc18b981d0473e59ef41226ee467eaa8fbfc7276603ef5af \\\n    --hash=sha256:1e0d32d6ed3a7ba13dbbd3a6fb50ca76c40c70e6bc6fe347f90677478d3422c7 \\\n    --hash=sha256:205983e87d6aa0b6e93ec7320060de44efaa905ecc5d13f70cbe38c65684c5c4 \\\n    --hash=sha256:27f4c5c20c86a9a823677316724306fb1ce3b25ec568efd52026dc6c563e5b29 \\\n    --hash=sha256:2c8a2ce568193f9f84763717d8961cec0db4ec1aa08c6bcf4d90da5eb72bec86 \\\n    --hash=sha256:2facab1c35600088cb82b5b093bd700bfbd1e3191deab24f7d1803d9dc5b76fc \\\n    --hash=sha256:3648ed9a57a6b704673faeab3dc64d1469cc69f2bc1ed8227ffa0f84e147c500 \\\n    --hash=sha256:3d07c5daef2d0d85949e32ec254ee44232bb57febb0634194379dd14d1ff4f87 \\\n    --hash=sha256:43c5e61a58783ddf78ef84949f6bb6e52e092a13ec67678e9a9e21071ecf5b80 \\\n    --hash=sha256:483f3aa5d1bc8cff5657ad96f68e1d89281f971a7b6aa93408a31e3199981ea9 \\\n    --hash=sha256:51a40a56c58e63dac6145829f9e9bd66e5867a9f0741bcb9ffefab619851d44f \\\n    --hash=sha256:5ebc6a0ac60c371a9efaf7d55dec5820f76fdafb43a3be1e390011339dc329ae \\\n    --hash=sha256:7cfca02d70579853041063e53ca713d31161b8831b98d4f68c3554dc0448beec \\\n    --hash=sha256:80ac7c17e69ca6b76ccccb4255f7c29a2a36e5b69eb10c2adba82135d43afe8c \\\n    --hash=sha256:8451e8475051f16eb4776380384699cb8ddd10ea8410bcbfaee5a6fc4c046de6 \\\n    --hash=sha256:86f66d334deaaab20a576fb785587566081407adc703318203fe26e43277ef12 \\\n    --hash=sha256:8c2471af9079ad798e1645ec0b0d3d4223db687379d17dd36a70637449f81d6b \\\n    --hash=sha256:921c9db0c3128481954f625b3b1bc10c730100aa944d54643528f716676439ee \\\n    --hash=sha256:936afd9c5ff7fe7457065e878a279811787778f472f9a4e8c5df79e7728358e2 \\\n    --hash=sha256:9f7afe5ef0e4750ac4515baebc251ee94aaefe5de6e2e8a24668473128d69904 \\\n    --hash=sha256:a0f920582b8713ca87d5a288a7532607bc4454275d733fc880650d602dbe3c67 \\\n    --hash=sha256:b129e9ee07f80b4af957607917af46ab517b0c4e746692f6d9e50e973edba8d8 \\\n    --hash=sha256:beaaa4450e23d41dd0c2f2f47a4f8a171210271543550c2c556090c7eeea88f5 \\\n    --hash=sha256:bf613844a1647fe3d2bba1f5c9c96a62a85280123a57a8a0c8d2f37d518bc10a \\\n    --hash=sha256:c0915ea139f66a289edc4f9de10cb45078af1bb950491c5612969864236a2e7e \\\n    --hash=sha256:c2c246bc29e8762ff4c8a833ac5b4da4c797d16ab138286e8aec9b0c0a0da2d4 \\\n    --hash=sha256:c7c13e4268550cde14a6f4743cc8bd8c035d4cd36514d58eff70276d68954b6f \\\n    --hash=sha256:c854885398410fa6e88fc29f7a420a3c13b88bae9b4e10a804437b582e24f58b \\\n    --hash=sha256:dbf816a6d0cb71e7fd06609246bbea4eaf100649d9decf49e4eb329594f70be7 \\\n    --hash=sha256:dd33fd9de8907a6275c70113124aeb7eea672c1324f5d5423f203738b341697d \\\n    --hash=sha256:e08a06802c8c8a9d04cf3319f9230ec09062c55d2550bd48f8ada1df1431adcf \\\n    --hash=sha256:fa7751b614d9aa82d7a0f49ca90924c29c6cedf85a2f8687fb6a772dbfe48711 \\\n    --hash=sha256:fbd4a6b8654b779089c5a44af1c65c1419c2cd60718780df6d8f354eb35d6d55\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing and Configuring Azure CLI\nDESCRIPTION: Commands to install Azure CLI and dependencies, followed by Azure login process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/azure.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install azure cli.\npip install azure-cli azure-identity\n\n# Login to azure. This will redirect you to your web browser.\naz login\n```\n\n----------------------------------------\n\nTITLE: Deploying RayCluster with Fluent Bit Sidecar in Kubernetes\nDESCRIPTION: Command to apply the Kubernetes configuration for a RayCluster with a Fluent Bit sidecar. This deploys the Fluent Bit ConfigMap and a single-pod RayCluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/refs/heads/master/ray-operator/config/samples/ray-cluster.fluentbit.yaml\n```\n\n----------------------------------------\n\nTITLE: Displaying Best Hyperparameters from Optimization\nDESCRIPTION: Prints the best hyperparameter configuration found during the optimization process, which minimizes the landscape function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Logging Metrics with PPO in Python\nDESCRIPTION: This snippet sets up a PPOConfig with an environment and custom callbacks, builds the configuration, and logs the mean of angle theta1 during training. It demonstrates how to extract and report specific metrics from training results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# Report with a sliding window of 50.\\nmetrics_logger.log_value(\\\"theta1_mean\\\", avg_theta1, reduce=\\\"mean\\\", window=50)\\n\\nconfig = (\\n    PPOConfig()\\n    .environment(\\\"Acrobot-v1\\\")\\n    .callbacks(\\n        callbacks_class=LogAcrobotAngle,\\n    )\\n)\\nppo = config.build()\\n\\n# Train n times. Expect `theta1_mean` to be found in the results under:\\n# `env_runners/theta1_mean`\\nfor i in range(10):\\n    results = ppo.train()\\n    print(\\n        f\\\"iter={i} \\\"\\n        f\\\"theta1_mean={results['env_runners']['theta1_mean']} \\\"\\n        f\\\"R={results['env_runners']['episode_return_mean']}\\\"\\n    )\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Cluster Cleanup Commands\nDESCRIPTION: Commands to clean up the Kubernetes cluster by deleting RayCluster, ConfigMap and uninstalling the KubeRay operator\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Delete RayCluster and ConfigMap\nkubectl delete -f https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-cluster.autoscaler.yaml\n\n# Uninstall the KubeRay operator\nhelm uninstall kuberay-operator\n```\n\n----------------------------------------\n\nTITLE: Specifying SemiDBM Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the semidbm package dependency with version 0.5.1 and SHA-256 hashes for verification. Comments indicate this package is directly required by the requirements_byod_3.9.in file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_38\n\nLANGUAGE: pip\nCODE:\n```\nsemidbm==0.5.1 \\\n    --hash=sha256:0dd74b5e9276eb5af186ace8b74165acec0c887e746bdae60340be91b99cffaf \\\n    --hash=sha256:add3e644dd6afcce83d1752b34ff80fa4e2b37b4ce6bce3289ad19d6f0bcd6ae\n    # via -r release/ray_release/byod/requirements_byod_3.9.in\n```\n\n----------------------------------------\n\nTITLE: Configuring Train Batch Size in Python\nDESCRIPTION: Shows how to set the train batch size per Learner actor.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconfig.training(train_batch_size_per_learner=256)\n```\n\n----------------------------------------\n\nTITLE: Creating Headless Service for RayCluster\nDESCRIPTION: Kubernetes manifest to create a headless service for the RayCluster with all required ports specified.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/istio.md#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl apply -f - <<EOF\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    ray.io/headless-worker-svc: raycluster-istio\n  name: raycluster-istio-headless-svc\n  namespace: default\nspec:\n  clusterIP: None\n  selector:\n    ray.io/cluster: raycluster-istio\n  publishNotReadyAddresses: true\n  ports:\n  - name: node-manager-port\n    port: 6380\n    appProtocol: grpc\n  - name: object-manager-port\n    port: 6381\n    appProtocol: grpc\n  - name: runtime-env-agent-port\n    port: 6382\n    appProtocol: grpc\n  - name: dashboard-agent-grpc-port\n    port: 6383\n    appProtocol: grpc\n  - name: dashboard-agent-listen-port\n    port: 52365\n    appProtocol: http\n  - name: metrics-export-port\n    port: 8080\n    appProtocol: http\n  - name: p10002\n    port: 10002\n    appProtocol: grpc\n  - name: p10003\n    port: 10003\n    appProtocol: grpc\n  - name: p10004\n    port: 10004\n    appProtocol: grpc\n  - name: p10005\n    port: 10005\n    appProtocol: grpc\n  - name: p10006\n    port: 10006\n    appProtocol: grpc\n  - name: p10007\n    port: 10007\n    appProtocol: grpc\n  - name: p10008\n    port: 10008\n    appProtocol: grpc\n  - name: p10009\n    port: 10009\n    appProtocol: grpc\n  - name: p10010\n    port: 10010\n    appProtocol: grpc\n  - name: p10011\n    port: 10011\n    appProtocol: grpc\n  - name: p10012\n    port: 10012\n    appProtocol: grpc\nEOF\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Hash Verification\nDESCRIPTION: This snippet defines the 'lz4' package dependency with version 4.3.3 and includes multiple SHA256 hash values for verification. It demonstrates how to specify exact package versions and ensure integrity when installing dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_12\n\nLANGUAGE: Text\nCODE:\n```\nlz4==4.3.3 \\\n    --hash=sha256:01fe674ef2889dbb9899d8a67361e0c4a2c833af5aeb37dd505727cf5d2a131e \\\n    --hash=sha256:054b4631a355606e99a42396f5db4d22046a3397ffc3269a348ec41eaebd69d2 \\\n    --hash=sha256:0a136e44a16fc98b1abc404fbabf7f1fada2bdab6a7e970974fb81cf55b636d0 \\\n    --hash=sha256:0e9c410b11a31dbdc94c05ac3c480cb4b222460faf9231f12538d0074e56c563 \\\n    --hash=sha256:222a7e35137d7539c9c33bb53fcbb26510c5748779364014235afc62b0ec797f \\\n    --hash=sha256:24b3206de56b7a537eda3a8123c644a2b7bf111f0af53bc14bed90ce5562d1aa \\\n    --hash=sha256:2b901c7784caac9a1ded4555258207d9e9697e746cc8532129f150ffe1f6ba0d \\\n    --hash=sha256:2f7b1839f795315e480fb87d9bc60b186a98e3e5d17203c6e757611ef7dcef61 \\\n    --hash=sha256:30e8c20b8857adef7be045c65f47ab1e2c4fabba86a9fa9a997d7674a31ea6b6 \\\n    --hash=sha256:31ea4be9d0059c00b2572d700bf2c1bc82f241f2c3282034a759c9a4d6ca4dc2 \\\n    --hash=sha256:337cb94488a1b060ef1685187d6ad4ba8bc61d26d631d7ba909ee984ea736be1 \\\n    --hash=sha256:33c9a6fd20767ccaf70649982f8f3eeb0884035c150c0b818ea660152cf3c809 \\\n    --hash=sha256:363ab65bf31338eb364062a15f302fc0fab0a49426051429866d71c793c23394 \\\n    --hash=sha256:43cf03059c0f941b772c8aeb42a0813d68d7081c009542301637e5782f8a33e2 \\\n    --hash=sha256:56f4fe9c6327adb97406f27a66420b22ce02d71a5c365c48d6b656b4aaeb7775 \\\n    --hash=sha256:5d35533bf2cee56f38ced91f766cd0038b6abf46f438a80d50c52750088be93f \\\n    --hash=sha256:6756212507405f270b66b3ff7f564618de0606395c0fe10a7ae2ffcbbe0b1fba \\\n    --hash=sha256:6cdc60e21ec70266947a48839b437d46025076eb4b12c76bd47f8e5eb8a75dcc \\\n    --hash=sha256:abc197e4aca8b63f5ae200af03eb95fb4b5055a8f990079b5bdf042f568469dd \\\n    --hash=sha256:b14d948e6dce389f9a7afc666d60dd1e35fa2138a8ec5306d30cd2e30d36b40c \\\n    --hash=sha256:b47839b53956e2737229d70714f1d75f33e8ac26e52c267f0197b3189ca6de24 \\\n    --hash=sha256:b6d9ec061b9eca86e4dcc003d93334b95d53909afd5a32c6e4f222157b50c071 \\\n    --hash=sha256:b891880c187e96339474af2a3b2bfb11a8e4732ff5034be919aa9029484cd201 \\\n    --hash=sha256:bca8fccc15e3add173da91be8f34121578dc777711ffd98d399be35487c934bf \\\n    --hash=sha256:c81703b12475da73a5d66618856d04b1307e43428a7e59d98cfe5a5d608a74c6 \\\n    --hash=sha256:d2507ee9c99dbddd191c86f0e0c8b724c76d26b0602db9ea23232304382e1f21 \\\n    --hash=sha256:e36cd7b9d4d920d3bfc2369840da506fa68258f7bb176b8743189793c055e43d \\\n    --hash=sha256:e7d84b479ddf39fe3ea05387f10b779155fc0990125f4fb35d636114e1c63a2e \\\n    --hash=sha256:eac9af361e0d98335a02ff12fb56caeb7ea1196cf1a49dbf6f17828a131da807 \\\n    --hash=sha256:edfd858985c23523f4e5a7526ca6ee65ff930207a7ec8a8f57a01eae506aaee7 \\\n    --hash=sha256:ee9ff50557a942d187ec85462bb0960207e7ec5b19b3b48949263993771c6205 \\\n    --hash=sha256:f0e822cd7644995d9ba248cb4b67859701748a93e2ab7fc9bc18c599a52e4604 \\\n    --hash=sha256:f180904f33bdd1e92967923a43c22899e303906d19b2cf8bb547db6653ea6e7d \\\n```\n\n----------------------------------------\n\nTITLE: Processing HTTP Request in Netty/Vert.x with JavaScript Execution\nDESCRIPTION: This stack trace shows the execution path of processing an HTTP request in a Netty-based server using Vert.x, which then invokes JavaScript code executed by Mozilla Rhino. It demonstrates the flow from network I/O through various layers of the application stack.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_75\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/ScriptRuntime:.createFunctionActivation_[j]\n```\n\n----------------------------------------\n\nTITLE: Package Hash Requirements\nDESCRIPTION: Lists SHA256 hashes for package dependencies including platformdirs, prometheus-client, and propcache. Used for verifying package integrity during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:339894035d0ede518b16073bdc2feef4c991ee991a29774b33e515f1d308e08d \\\n--hash=sha256:412444afb8c4c7a6cc11a47dade32982439925537e483be7c0ae0cf96c4f6a0b \\\n...\n```\n\n----------------------------------------\n\nTITLE: Package Hash Values - Multidict 6.0.5\nDESCRIPTION: SHA-256 hash values for multidict package version 6.0.5 verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_15\n\nLANGUAGE: text\nCODE:\n```\nmultidict==6.0.5 \\\n--hash=sha256:01265f5e40f5a17f8241d52656ed27192be03bfa8764d88e8220141d1e4b3556 \\\n--hash=sha256:0275e35209c27a3f7951e1ce7aaf93ce0d163b28948444bec61dd7badc6d3f8c\n```\n\n----------------------------------------\n\nTITLE: Setting Warning Threshold for Slow Experiment Checkpointing\nDESCRIPTION: TUNE_WARN_SLOW_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S sets the threshold (in seconds) for warning about slow experiment state synchronization. Defaults to 5 seconds.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_WARN_SLOW_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S=5\n```\n\n----------------------------------------\n\nTITLE: Setting Train Batch Size per Learner\nDESCRIPTION: This snippet demonstrates how to set the train batch size per learner using the `training()` method in the `AlgorithmConfig` object. It replaces the old stack's `train_batch_size` setting to accommodate the new Learner-based architecture.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"config.training(\n    train_batch_size_per_learner=512,\n)\"\n```\n\n----------------------------------------\n\nTITLE: Setting Bazel Build Mode\nDESCRIPTION: Configuration for setting default Bazel build mode in user-level settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nbuild --compilation_mode=fastbuild\n```\n\n----------------------------------------\n\nTITLE: Specifying protobuf Dependency with Hash Verification in requirements.txt\nDESCRIPTION: Defines the protobuf package dependency (version 3.20.3) with SHA256 hash verification for secure installation. This is a crucial dependency used in the Ray project for serializing structured data.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_26\n\nLANGUAGE: requirements.txt\nCODE:\n```\nprotobuf==3.20.3 \\\n    --hash=sha256:03038ac1cfbc41aa21f6afcbcd357281d7521b4157926f30ebecc8d4ea59dcb7 \\\n    --hash=sha256:28545383d61f55b57cf4df63eebd9827754fd2dc25f80c5253f9184235db242c \\\n    --hash=sha256:2e3427429c9cffebf259491be0af70189607f365c2f41c7c3764af6f337105f2 \\\n    --hash=sha256:398a9e0c3eaceb34ec1aee71894ca3299605fa8e761544934378bbc6c97de23b \\\n    --hash=sha256:44246bab5dd4b7fbd3c0c80b6f16686808fab0e4aca819ade6e8d294a29c7050 \\\n    --hash=sha256:447d43819997825d4e71bf5769d869b968ce96848b6479397e29fc24c4a5dfe9 \\\n    --hash=sha256:67a3598f0a2dcbc58d02dd1928544e7d88f764b47d4a286202913f0b2801c2e7 \\\n    --hash=sha256:74480f79a023f90dc6e18febbf7b8bac7508420f2006fabd512013c0c238f454 \\\n    --hash=sha256:819559cafa1a373b7096a482b504ae8a857c89593cf3a25af743ac9ecbd23480 \\\n    --hash=sha256:899dc660cd599d7352d6f10d83c95df430a38b410c1b66b407a6b29265d66469 \\\n    --hash=sha256:8c0c984a1b8fef4086329ff8dd19ac77576b384079247c770f29cc8ce3afa06c \\\n    --hash=sha256:9aae4406ea63d825636cc11ffb34ad3379335803216ee3a856787bcf5ccc751e \\\n    --hash=sha256:a7ca6d488aa8ff7f329d4c545b2dbad8ac31464f1d8b1c87ad1346717731e4db \\\n    --hash=sha256:b6cc7ba72a8850621bfec987cb72623e703b7fe2b9127a161ce61e61558ad905 \\\n    --hash=sha256:bf01b5720be110540be4286e791db73f84a2b721072a3711efff6c324cdf074b \\\n    --hash=sha256:c02ce36ec760252242a33967d51c289fd0e1c0e6e5cc9397e2279177716add86 \\\n    --hash=sha256:d9e4432ff660d67d775c66ac42a67cf2453c27cb4d738fc22cb53b5d84c135d4 \\\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry Semantic Conventions Package with Hash Verification\nDESCRIPTION: Defines the opentelemetry-semantic-conventions package version 0.20b0 with SHA256 hash verification and notes that it's required by opentelemetry-sdk.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_21\n\nLANGUAGE: text\nCODE:\n```\nopentelemetry-semantic-conventions==0.20b0 \\\n    --hash=sha256:ecae7367203e5204c70518e6d24b438480d6a6f1e5c8ee9dc2145f176ff4452e \\\n    --hash=sha256:fac014ac2098b1a05fe58af77cbe74c825ff869d6d53d316c393cc77f507ec15\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Job Deployment Command\nDESCRIPTION: Command to redeploy the Ray job using kubectl to verify automatic checkpoint recovery functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f ray-job.pytorch-image-classifier.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Ray and Jupyter for HPU Training\nDESCRIPTION: Command to install Ray with train support and Jupyter notebook inside the Gaudi container for running the training notebook.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install ray[train] notebook\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Pods for Ray Cluster\nDESCRIPTION: Lists the Kubernetes pods for the Ray head node and worker nodes after deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl get pods\n```\n\n----------------------------------------\n\nTITLE: Configuring Upscaling Speed\nDESCRIPTION: Controls cluster growth rate as a multiple of current size. Minimum value is 1.0 (100% growth), defaults to 1.0. Affects number of allowed pending node launches.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/configuring-autoscaling.rst#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nupscaling_speed: 1.0\n```\n\n----------------------------------------\n\nTITLE: Printing best hyperparameters\nDESCRIPTION: Prints the best hyperparameters found by the Tune experiment. These hyperparameters are the ones that minimized the mean loss of the objective function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"print(\\\"Best hyperparameters found were: \\\", results.get_best_result().config)\"\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network for Polynomial Approximation in Python\nDESCRIPTION: This snippet defines a neural network class 'Net' for polynomial approximations, allowing for different modes of operation (square or cubic functions) based on input parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/horovod_simple.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Net(torch.nn.Module):\n    def __init__(self, mode=\"sq\"):\n        super(Net, self).__init__()\n\n        if mode == \"square\":\n            self.mode = 0\n            self.param = torch.nn.Parameter(torch.FloatTensor([1.0, -1.0]))\n        else:\n            self.mode = 1\n            self.param = torch.nn.Parameter(torch.FloatTensor([1.0, -1.0, 1.0]))\n\n    def forward(self, x):\n        if ~self.mode:\n            return x * x + self.param[0] * x + self.param[1]\n        else:\n            return_val = 10 * x * x * x\n            return_val += self.param[0] * x * x\n            return_val += self.param[1] * x + self.param[2]\n            return return_val\n```\n\n----------------------------------------\n\nTITLE: Defining Natural Language to Code Test Cases in Python\nDESCRIPTION: Creates a list of test cases for evaluating the model's ability to convert natural language intents into code. Each test case consists of a textual description (intent) of a programming task involving Pandas, regex, or system signals.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntestcases = [\n    {\n        \"intent\": \"replace white spaces in colunm 'col' of dataframe `df` with '_'\",\n    },\n    {\n        \"intent\": \"search for occurrences of regex pattern '>.*<' in xml string `line`\",\n    },\n    {\n        \"intent\": \"send a signal `signal.SIGUSR1` to the current process\",\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Internal Actor Termination in Python\nDESCRIPTION: Shows how to terminate an actor from within its own method using ray.actor.exit_actor(). This approach allows graceful shutdown after completing pending tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/terminating-actors.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\nclass Actor:\n    def exit(self):\n        ray.actor.exit_actor()\n\nactor = Actor.remote()\nactor.exit.remote()\n```\n\n----------------------------------------\n\nTITLE: MultiAgentReplayBuffer with Alternative Underlying ReplayBuffer\nDESCRIPTION: This code snippet demonstrates how to create a :py:class:`~ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer` with an alternative underlying :py:class:`~ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer`. It shows how to specify a custom buffer config for the underlying buffers within the multi-agent buffer, allowing for customized behavior per policy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-replay-buffers.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\n\nfrom ray.rllib.algorithms.simple_q import SimpleQConfig\nfrom ray.rllib.env import EnvContext, PettingZooEnv\nfrom ray.rllib.examples.env.rock_paper_scissors import RockPaperScissorsEnv\nfrom ray.rllib.policy.sample_batch import SampleBatch\nfrom ray.rllib.utils.replay_buffers import (ReplayBuffer, StorageUnit, MultiAgentReplayBuffer)\n\n\nclass MyReplayBuffer(ReplayBuffer):\n    def __init__(self, capacity: int = 100, storage_unit: str = \"fragments\"):\n        super().__init__(capacity=capacity, storage_unit=storage_unit)\n\n    def add(self, **kwargs) -> None:\n        print(\"called add in MyReplayBuffer!\")\n        super().add(**kwargs)\n\n    def sample(self, num_items: int, **kwargs) -> SampleBatch:\n        print(\"called sample in MyReplayBuffer!\")\n        return super().sample(num_items=num_items, **kwargs)\n\n\nif __name__ == \"__main__\":\n    # This example shows how to configure a replay buffer when running an experiment.\n    # This is a toy example, but it shows how to use a custom replay buffer with SimpleQ.\n    env_config = {\"max_cycles\": 20}\n    # We set `disable_env_checking` to True because `gymnasium.make` will check\n    # the env (that we are wrapping with PettingZooEnv) and throw an error if the\n    # observation/action spaces do not match, but we do our own checking within\n    # PettingZooEnv already.\n    # Also note that we need to set render_mode=None as rendering is not supported.\n    rock_paper_scissors_env = gym.make(\"RockPaperScissors-v0\", **env_config, render_mode=None)\n    wrapped_env = PettingZooEnv(rock_paper_scissors_env, disable_env_checking=True)\n\n    config = (\n        SimpleQConfig()\n        .environment(wrapped_env, env_config=env_config)\n        .rollouts(num_rollout_workers=0)\n        .training(train_batch_size=64)\n        .replay_buffer_config(\n            type=MultiAgentReplayBuffer,\n            replay_buffer_config={\n                \"type\": MyReplayBuffer,\n                \"storage_unit\": \"episodes\",\n            },\n        )\n        .framework(\"torch\")\n    )\n\n    # Now execute the configuration using Tune.\n    from ray import tune\n\n    tune.Tuner(\"SimpleQ\", param_space=config.to_dict(), run_config=tune.RunConfig(stop={\"training_iteration\": 1})).fit()\n```\n\n----------------------------------------\n\nTITLE: Accessing Ray Head Pod and Running Sample Job\nDESCRIPTION: Commands to access the Ray head pod and run a sample job that demonstrates profiling capabilities with py-spy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/pyspy.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Log in to the head Pod\nkubectl exec -it ${YOUR_HEAD_POD} -- bash\n\n# (Head Pod) Run a sample job in the Pod\n# `long_running_task` includes a `while True` loop to ensure the task remains actively running indefinitely.\n# This allows you ample time to view the Stack Trace and CPU Flame Graph via Ray Dashboard.\npython3 samples/long_running_task.py\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Requirements with Hash Verification\nDESCRIPTION: This code snippet defines specific Python package requirements with their versions and hash verification data. Each package entry includes the exact version number, multiple hash verifications for different distributions, and comments indicating which requirement files reference them.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_10\n\nLANGUAGE: text\nCODE:\n```\njiter==0.8.2 \\\n    --hash=sha256:025337859077b41548bdcbabe38698bcd93cfe10b06ff66617a48ff92c9aec60 \\\n    --hash=sha256:03c9df035d4f8d647f8c210ddc2ae0728387275340668fb30d2421e17d9a0841 \\\n    --hash=sha256:08d4c92bf480e19fc3f2717c9ce2aa31dceaa9163839a311424b6862252c943e \\\n    --hash=sha256:0cf5dfa9956d96ff2efb0f8e9c7d055904012c952539a774305aaaf3abdf3d6c \\\n    --hash=sha256:14601dcac4889e0a1c75ccf6a0e4baf70dbc75041e51bcf8d0e9274519df6887 \\\n    --hash=sha256:180a8aea058f7535d1c84183c0362c710f4750bef66630c05f40c93c2b152a0f \\\n    --hash=sha256:1c0dfbd1be3cbefc7510102370d86e35d1d53e5a93d48519688b1bf0f761160a \\\n    --hash=sha256:2dd61c5afc88a4fda7d8b2cf03ae5947c6ac7516d32b7a15bf4b49569a5c076b \\\n    --hash=sha256:317b25e98a35ffec5c67efe56a4e9970852632c810d35b34ecdd70cc0e47b3b6 \\\n    --hash=sha256:32475a42b2ea7b344069dc1e81445cfc00b9d0e3ca837f0523072432332e9f74 \\\n    --hash=sha256:37b2998606d6dadbb5ccda959a33d6a5e853252d921fec1792fc902351bb4e2c \\\n    --hash=sha256:3ac9f578c46f22405ff7f8b1f5848fb753cc4b8377fbec8470a7dc3997ca7566 \\\n    --hash=sha256:3b94a33a241bee9e34b8481cdcaa3d5c2116f575e0226e421bed3f7a6ea71cff \\\n    --hash=sha256:4a9220497ca0cb1fe94e3f334f65b9b5102a0b8147646118f020d8ce1de70105 \\\n    --hash=sha256:4ab9a87f3784eb0e098f84a32670cfe4a79cb6512fd8f42ae3d0709f06405d18 \\\n    --hash=sha256:5127dc1abd809431172bc3fbe8168d6b90556a30bb10acd5ded41c3cfd6f43b6 \\\n    --hash=sha256:5672a86d55416ccd214c778efccf3266b84f87b89063b582167d803246354be4 \\\n    --hash=sha256:580ccf358539153db147e40751a0b41688a5ceb275e6f3e93d91c9467f42b2e3 \\\n    --hash=sha256:58dc9bc9767a1101f4e5e22db1b652161a225874d66f0e5cb8e2c7d1c438b587 \\\n    --hash=sha256:5a90a923338531b7970abb063cfc087eebae6ef8ec8139762007188f6bc69a9f \\\n    --hash=sha256:653cf462db4e8c41995e33d865965e79641ef45369d8a11f54cd30888b7e6ff1 \\\n    --hash=sha256:66227a2c7b575720c1871c8800d3a0122bb8ee94edb43a5685aa9aceb2782d44 \\\n    --hash=sha256:6e5337bf454abddd91bd048ce0dca5134056fc99ca0205258766db35d0a2ea43 \\\n    --hash=sha256:70bf4c43652cc294040dbb62256c83c8718370c8b93dd93d934b9a7bf6c4f53c \\\n    --hash=sha256:711e408732d4e9a0208008e5892c2966b485c783cd2d9a681f3eb147cf36c7ef \\\n    --hash=sha256:76e324da7b5da060287c54f2fabd3db5f76468006c811831f051942bf68c9d44 \\\n    --hash=sha256:789361ed945d8d42850f919342a8665d2dc79e7e44ca1c97cc786966a21f627a \\\n    --hash=sha256:79aec8172b9e3c6d05fd4b219d5de1ac616bd8da934107325a6c0d0e866a21b6 \\\n    --hash=sha256:7efe4853ecd3d6110301665a5178b9856be7e2a9485f49d91aa4d737ad2ae49e \\\n    --hash=sha256:7f22b16b35d5c1df9dfd58843ab2cd25e6bf15191f5a236bed177afade507bfc \\\n    --hash=sha256:83c0efd80b29695058d0fd2fa8a556490dbce9804eac3e281f373bbc99045f6c \\\n    --hash=sha256:859e8eb3507894093d01929e12e267f83b1d5f6221099d3ec976f0c995cb6bd9 \\\n    --hash=sha256:8b9931fd36ee513c26b5bf08c940b0ac875de175341cbdd4fa3be109f0492586 \\\n    --hash=sha256:8bd2a824d08d8977bb2794ea2682f898ad3d8837932e3a74937e93d62ecbb637 \\\n    --hash=sha256:8f2d5ed877f089862f4c7aacf3a542627c1496f972a34d0474ce85ee7d939c27 \\\n    --hash=sha256:8ffc86ae5e3e6a93765d49d1ab47b6075a9c978a2b3b80f0f32628f39caa0c88 \\\n    --hash=sha256:92249669925bc1c54fcd2ec73f70f2c1d6a817928480ee1c65af5f6b81cdf12d \\\n    --hash=sha256:99d9a1eded738299ba8e106c6779ce5c3893cffa0e32e4485d680588adae6db8 \\\n    --hash=sha256:9c63eaef32b7bebac8ebebf4dabebdbc6769a09c127294db6babee38e9f405b9 \\\n    --hash=sha256:9e1fa156ee9454642adb7e7234a383884452532bc9d53d5af2d18d98ada1d79c \\\n    --hash=sha256:a2ecaa3c23e7a7cf86d00eda3390c232f4d533cd9ddea4b04f5d0644faf642c5 \\\n    --hash=sha256:a6c710d657c8d1d2adbbb5c0b0c6bfcec28fd35bd6b5f016395f9ac43e878a15 \\\n    --hash=sha256:a9584de0cd306072635fe4b89742bf26feae858a0683b399ad0c2509011b9dc0 \\\n    --hash=sha256:ab7f43235d71e03b941c1630f4b6e3055d46b6cb8728a17663eaac9d8e83a865 \\\n    --hash=sha256:af102d3372e917cffce49b521e4c32c497515119dc7bd8a75665e90a718bbf08 \\\n    --hash=sha256:b25bd626bde7fb51534190c7e3cb97cee89ee76b76d7585580e22f34f5e3f393 \\\n    --hash=sha256:b2dd880785088ff2ad21ffee205e58a8c1ddabc63612444ae41e5e4b321b39c0 \\\n    --hash=sha256:b426f72cd77da3fec300ed3bc990895e2dd6b49e3bfe6c438592a3ba660e41ca \\\n    --hash=sha256:ba5bdf56969cad2019d4e8ffd3f879b5fdc792624129741d3d83fc832fef8c7d \\\n    --hash=sha256:bf55846c7b7a680eebaf9c3c48d630e1bf51bdf76c68a5f654b8524335b0ad29 \\\n    --hash=sha256:ca1f08b8e43dc3bd0594c992fb1fd2f7ce87f7bf0d44358198d6da8034afdf84 \\\n    --hash=sha256:ca29b6371ebc40e496995c94b988a101b9fbbed48a51190a4461fcb0a68b4a36 \\\n    --hash=sha256:ca8577f6a413abe29b079bc30f907894d7eb07a865c4df69475e868d73e71c7b \\\n    --hash=sha256:cadcc978f82397d515bb2683fc0d50103acff2a180552654bb92d6045dec2c49 \\\n    --hash=sha256:cd646c827b4f85ef4a78e4e58f4f5854fae0caf3db91b59f0d73731448a970c6 \\\n    --hash=sha256:cd73d3e740666d0e639f678adb176fad25c1bcbdae88d8d7b857e1783bb4212d \\\n    --hash=sha256:cde031d8413842a1e7501e9129b8e676e62a657f8ec8166e18a70d94d4682855 \\\n    --hash=sha256:ce0820f4a3a59ddced7fce696d86a096d5cc48d32a4183483a17671a61edfddc \\\n    --hash=sha256:d20be8b7f606df096e08b0b1b4a3c6f0515e8dac296881fe7461dfa0fb5ec817 \\\n    --hash=sha256:d21974d246ed0181558087cd9f76e84e8321091ebfb3a93d4c341479a736f099 \\\n    --hash=sha256:d33f94615fcaf872f7fd8cd98ac3b429e435c77619777e8a449d9d27e01134d1 \\\n    --hash=sha256:d35c864c2dff13dfd79fb070fc4fc6235d7b9b359efe340e1261deb21b9fcb66 \\\n    --hash=sha256:d5c826a221851a8dc028eb6d7d6429ba03184fa3c7e83ae01cd6d3bd1d4bd17d \\\n    --hash=sha256:e41e75344acef3fc59ba4765df29f107f309ca9e8eace5baacabd9217e52a5ee \\\n    --hash=sha256:e52bf98c7e727dd44f7c4acb980cb988448faeafed8433c867888268899b298b \\\n    --hash=sha256:e6ec2be506e7d6f9527dae9ff4b7f54e68ea44a0ef6b098256ddf895218a2f8f \\\n    --hash=sha256:e725edd0929fa79f8349ab4ec7f81c714df51dc4e991539a578e5018fa4a7152 \\\n    --hash=sha256:eaa58399c01db555346647a907b4ef6d4f584b123943be6ed5588c3f2359c9f4 \\\n    --hash=sha256:eb21aaa9a200d0a80dacc7a81038d2e476ffe473ffdd9c91eb745d623561de05 \\\n    --hash=sha256:ecff0dc14f409599bbcafa7e470c00b80f17abc14d1405d38ab02e4b42e55b57 \\\n    --hash=sha256:f557c55bc2b7676e74d39d19bcb8775ca295c7a028246175d6a8b431e70835e5 \\\n    --hash=sha256:f7200b8f7619d36aa51c803fd52020a2dfbea36ffec1b5e22cab11fd34d95a6d \\\n    --hash=sha256:f9d471356dc16f84ed48768b8ee79f29514295c7295cb41e1133ec0b2b8d637d \\\n    --hash=sha256:fc5adda618205bd4678b146612ce44c3cbfdee9697951f2c0ffdef1f26d72b63 \\\n    --hash=sha256:fc9043259ee430ecd71d178fccabd8c332a3bf1e81e50cae43cc2b28d19e4cb7 \\\n    --hash=sha256:ffd9fee7d0775ebaba131f7ca2e2d83839a62ad65e8e02fe2bd8fc975cedeb9e\n    # via openai\n```\n\n----------------------------------------\n\nTITLE: Setting Up Head Node Ray Commands for AWS\nDESCRIPTION: YAML configuration for starting Ray on the head node of an AWS cluster. It stops any existing Ray processes, sets file descriptor limits, and starts Ray in head mode with specified ports and configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nhead_start_ray_commands:\n  - ray stop\n  - ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting up RST Documentation Structure for Ray Tune Guides\nDESCRIPTION: RST configuration for Ray Tune documentation navigation and layout, including toctree setup and grid-based card layout for feature guides.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/overview.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _tune-guides:\n\n===========\nUser Guides\n===========\n\n.. toctree::\n    :hidden:\n\n    Running Basic Experiments <tune-run>\n    tune-output\n    Setting Trial Resources <tune-resources>\n    Using Search Spaces <tune-search-spaces>\n    tune-stopping\n    tune-trial-checkpoints\n    tune-storage\n    tune-fault-tolerance\n    Using Callbacks and Metrics <tune-metrics>\n    tune_get_data_in_and_out\n    ../examples/tune_analyze_results\n    ../examples/pbt_guide\n    Deploying Tune in the Cloud <tune-distributed>\n    Tune Architecture <tune-lifecycle>\n    Scalability Benchmarks <tune-scalability>\n```\n\n----------------------------------------\n\nTITLE: Configuring Structured Logging via Environment Variable\nDESCRIPTION: Set up structured logging using environment variables before Ray initialization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"RAY_LOGGING_CONFIG_ENCODING\"] = \"JSON\"\n\nimport ray\nimport logging\n\nray.init(log_to_driver=False)\n# Use the root logger to print log messages.\n```\n\n----------------------------------------\n\nTITLE: Managing Ray Cluster on Azure\nDESCRIPTION: Series of commands to download configuration, create, connect to, and tear down a Ray cluster on Azure.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/azure.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Download the example-full.yaml\nwget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/azure/example-full.yaml\n\n# Update the example-full.yaml to update resource_group, location, and subscription_id.\n# vi example-full.yaml\n\n# Create or update the cluster. When the command finishes, it will print\n# out the command that can be used to SSH into the cluster head node.\nray up example-full.yaml\n\n# Get a remote screen on the head node.\nray attach example-full.yaml\n# Try running a Ray program.\n\n# Tear down the cluster.\nray down example-full.yaml\n```\n\n----------------------------------------\n\nTITLE: Package Repository Configuration\nDESCRIPTION: Package source configuration specifying PyPI index URL and additional sources for PyTorch and PyG wheels\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cu121\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html\n```\n\n----------------------------------------\n\nTITLE: Initializing Model Pipeline with Accelerate\nDESCRIPTION: Sets up a model pipeline using Accelerate library to efficiently load and distribute model weights across devices. Implements memory optimization by using meta device initialization and device mapping.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport ray\nimport lightning.pytorch as pl\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\nfrom accelerate import (\n    init_empty_weights,\n    infer_auto_device_map,\n    load_checkpoint_and_dispatch,\n)\n\n# Initialize a model on meta device\nwith init_empty_weights():\n    config = AutoConfig.from_pretrained(MODEL_NAME)\n    meta_model = AutoModelForCausalLM.from_config(config)\nmeta_model.tie_weights()\n\n# Define the device mapping\ndevice_map = infer_auto_device_map(\n    meta_model,\n    max_memory={0: \"15GB\", \"cpu\": \"60GB\"},\n    no_split_module_classes=[\"LlamaDecoderLayer\"],\n)\n\n# Load the model parameters\nmodel = load_checkpoint_and_dispatch(\n    meta_model,\n    checkpoint=full_model_ckpt_path,\n    device_map=device_map,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up RLlib for Local Development\nDESCRIPTION: This bash snippet provides commands to clone the RLlib repository, navigate to the project directory, and execute the setup-dev.py script for development purposes. This setup allows developers to reflect changes immediately without needing to reinstall Ray. The script supports linking dependencies like Ray Tune or Train, which can be selected interactively.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-dev.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone your fork onto your local machine, e.g.:\ngit clone https://github.com/[your username]/ray.git\ncd ray\n# Only enter 'Y' at the first question on linking RLlib.\n# This leads to the most stable behavior and you won't have to re-install ray as often.\n# If you anticipate making changes to e.g. Tune or Train quite often, consider also symlinking Ray Tune or Train here\n# (say 'Y' when asked by the script about creating the Tune or Train symlinks).\npython python/ray/setup-dev.py\n```\n\n----------------------------------------\n\nTITLE: Starting Ray with Redis Configuration using ray start\nDESCRIPTION: Command to start Ray head node with Redis configuration for GCS fault tolerance. Requires setting RAY_REDIS_ADDRESS environment variable and redis password.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/gcs.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nRAY_REDIS_ADDRESS=redis_ip:port ray start --head --redis-password PASSWORD --redis-username default\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Train via pip\nDESCRIPTION: Command to install Ray Train package using pip. The installation includes Ray's train functionality with all required dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/train.rst#2025-04-12_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ pip install -U \"ray[train]\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Verification\nDESCRIPTION: A collection of Python package specifications with exact version pinning and SHA256 hash verification for packages like packaging, pandas, pandocfilters, parso, and pathspec. Each entry includes comments indicating where the dependencies are used in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   ipykernel\n    #   jupyter-server\n    #   jupyterlab\n    #   jupyterlab-server\n    #   lazy-loader\n    #   nbconvert\n    #   pytest\n    #   scikit-image\n    #   tensorboardx\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    --hash=sha256:41179ce559943d83a9b4bbacb736b04c928b095b5f25dd2b7389eda08f46f373 \\\n    --hash=sha256:478ff646ca42b20376e4ed3fa2e8d7341e8a63105586efe54fa2508ee087f328 \\\n    --hash=sha256:50869a35cbb0f2e0cd5ec04b191e7b12ed688874bd05dd777c19b28cbea90996 \\\n    --hash=sha256:565fa34a5434d38e9d250af3c12ff931abaf88050551d9fbcdfafca50d62babf \\\n    --hash=sha256:5f2b952406a1588ad4cad5b3f55f520e82e902388a6d5a4a91baa8d38d23c7f6 \\\n    --hash=sha256:5fbcb19d6fceb9e946b3e23258757c7b225ba450990d9ed63ccceeb8cae609f7 \\\n    --hash=sha256:6973549c01ca91ec96199e940495219c887ea815b2083722821f1d7abfa2b4dc \\\n    --hash=sha256:74a3fd7e5a7ec052f183273dc7b0acd3a863edf7520f5d3a1765c04ffdb3b0b1 \\\n    --hash=sha256:7a0a56cef15fd1586726dace5616db75ebcfec9179a3a55e78f72c5639fa2a23 \\\n    --hash=sha256:7cec0bee9f294e5de5bbfc14d0573f65526071029d036b753ee6507d2a21480a \\\n    --hash=sha256:87bd9c03da1ac870a6d2c8902a0e1fd4267ca00f13bc494c9e5a9020920e1d51 \\\n    --hash=sha256:972d8a45395f2a2d26733eb8d0f629b2f90bebe8e8eddbb8829b180c09639572 \\\n    --hash=sha256:9842b6f4b8479e41968eced654487258ed81df7d1c9b7b870ceea24ed9459b31 \\\n    --hash=sha256:9f69c4029613de47816b1bb30ff5ac778686688751a5e9c99ad8c7031f6508e5 \\\n    --hash=sha256:a50d9a4336a9621cab7b8eb3fb11adb82de58f9b91d84c2cd526576b881a0c5a \\\n    --hash=sha256:bc4c368f42b551bf72fac35c5128963a171b40dce866fb066540eeaf46faa003 \\\n    --hash=sha256:c39a8da13cede5adcd3be1182883aea1c925476f4e84b2807a46e2775306305d \\\n    --hash=sha256:c3ac844a0fe00bfaeb2c9b51ab1424e5c8744f89860b138434a363b1f620f354 \\\n    --hash=sha256:c4c00e0b0597c8e4f59e8d461f797e5d70b4d025880516a8261b2817c47759ee \\\n    --hash=sha256:c74a62747864ed568f5a82a49a23a8d7fe171d0c69038b38cedf0976831296fa \\\n    --hash=sha256:dd05f7783b3274aa206a1af06f0ceed3f9b412cf665b7247eacd83be41cf7bf0 \\\n    --hash=sha256:dfd681c5dc216037e0b0a2c821f5ed99ba9f03ebcf119c7dac0e9a7b960b9ec9 \\\n    --hash=sha256:e474390e60ed609cec869b0da796ad94f420bb057d86784191eefc62b65819ae \\\n    --hash=sha256:f76d097d12c82a535fda9dfe5e8dd4127952b45fea9b0276cb30cca5ea313fbc\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\npandocfilters==1.5.0 \\\n    --hash=sha256:0b679503337d233b4339a817bfc8c50064e2eff681314376a47cb582305a7a38 \\\n    --hash=sha256:33aae3f25fd1a026079f5d27bdd52496f0e0803b3469282162bafdcbdf6ef14f\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   nbconvert\nparso==0.8.3 \\\n    --hash=sha256:8c07be290bb59f03588915921e29e8a50002acaf2cdc5fa0e0114f91709fafa0 \\\n    --hash=sha256:c001d4636cd3aecdaf33cbb40aebb59b094be2a74c556778ef5576c175e19e75\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jedi\npathspec==0.11.2 \\\n    --hash=sha256:1d6ed233af05e679efb96b1851550ea95bbb64b7c490b0f5aa52996c11e92a20 \\\n    --hash=sha256:e0d8d0ac2f12da61956eb2306b69f9469b42f4deb0f3cb6ed47b9cce9996ced3\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure Definition\nDESCRIPTION: Defines the documentation structure for Ray Workflows using reStructuredText directives, including a table of contents tree and warning notice about deprecation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _workflows:\n\nRay Workflows: Durable Ray Task Graphs\n======================================\n\n.. toctree::\n    :hidden:\n\n    key-concepts\n    basics\n    management\n    metadata\n    events\n    comparison\n    advanced\n    api/api\n\n.. warning::\n\n  The experimental Ray Workflows library has been deprecated and will be removed in a\n  future version of Ray.\n```\n\n----------------------------------------\n\nTITLE: Configuring Traefik for Ray Dashboard Reverse Proxy\nDESCRIPTION: This YAML configuration demonstrates how to set up Traefik as a reverse proxy for the Ray Dashboard, including rules for proper URL handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/configure-manage-dashboard.md#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n[http]\n  [http.routers]\n    [http.routers.to-dashboard]\n      rule = \"PathPrefix(`/ray/dashboard`)\"\n      middlewares = [\"test-redirectregex\", \"strip\"]\n      service = \"dashboard\"\n  [http.middlewares]\n    [http.middlewares.test-redirectregex.redirectRegex]\n      regex = \"^(.*)/ray/dashboard$\"\n      replacement = \"${1}/ray/dashboard/\"\n    [http.middlewares.strip.stripPrefix]\n      prefixes = [\"/ray/dashboard\"]\n  [http.services]\n    [http.services.dashboard.loadBalancer]\n      [[http.services.dashboard.loadBalancer.servers]]\n        url = \"http://localhost:8265\"\n```\n\n----------------------------------------\n\nTITLE: Configuring DataWorker Concurrency in RLlib\nDESCRIPTION: Configuration for scaling post-processing by increasing DataWorker instances with specified CPU resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconfig = (\n    AlgorithmConfig()\n    .offline_data(\n        map_batches_kwargs={\n            \"concurrency\": 10,\n            \"num_cpus\": 4,\n        }\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Ray Initialization - C++\nDESCRIPTION: This C++ snippet demonstrates how to check if the Ray runtime has been initialized using `ray::IsInitialized()`. This functionality is essential for ensuring that the Ray runtime is ready for processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n#include <ray/api.h>\n\nint main(int argc, char **argv) {\n    ray::Init();\n    assert(ray::IsInitialized());\n\n    ray::Shutdown();\n    assert(!ray::IsInitialized());\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Version-Specific Dependencies for Ray Project in Python\nDESCRIPTION: This snippet defines version-specific and platform-specific dependencies for the Ray project, including different requirements for macOS and other platforms.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements.txt#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ngrpcio == 1.54.2; sys_platform == \"darwin\"\ngrpcio >= 1.54.2; sys_platform != \"darwin\"\nnumpy>=1.20\n\n# pyarrow 18 causes macos build failures.\n# See https://github.com/ray-project/ray/pull/48300\npyarrow >= 9.0.0\npyarrow <18; sys_platform == \"darwin\" and platform_machine == \"x86_64\"\n```\n\n----------------------------------------\n\nTITLE: Running TensorBoard on Remote Systems without sudo Access\nDESCRIPTION: Commands to configure TensorBoard to work on remote multi-user clusters where you don't have sudo access by setting a user-specific temp directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ export TMPDIR=/tmp/$USER; mkdir -p $TMPDIR; tensorboard --logdir=~/ray_results\n```\n\n----------------------------------------\n\nTITLE: Docker Installation Commands YAML Configuration\nDESCRIPTION: Commands to install Docker on systems where it's not available by default. Includes downloading the Docker installation script, executing it, adding user to Docker group, and restarting the Docker service.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ninitialization_commands:\n    - curl -fsSL https://get.docker.com -o get-docker.sh\n    - sudo sh get-docker.sh\n    - sudo usermod -aG docker $USER\n    - sudo systemctl restart docker -f\n```\n\n----------------------------------------\n\nTITLE: Building manylinux2014 aarch64 Ray Wheel using Docker\nDESCRIPTION: This command builds a Ray wheel for Python 3.9 on manylinux2014 aarch64 platform. It uses a Docker container and mounts the current directory to /ray. The resulting wheel files are placed in the .whl directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/README-building-wheels.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -ti --rm \\\n    -e BUILDKITE_COMMIT=\"$(git rev-parse HEAD)\" \\\n    -e BUILD_ONE_PYTHON_ONLY=py39 \\\n    -w /ray -v \"$(pwd)\":/ray \\\n    quay.io/pypa/manylinux2014_aarch64:2024-07-02-9ac04ee \\\n    /ray/python/build-wheel-manylinux2014.sh\n```\n\n----------------------------------------\n\nTITLE: Removing a Placement Group - C++\nDESCRIPTION: Illustrates the removal of a placement group using C++ Ray APIs. It includes getting the state of the placement group to confirm removal. Requires existing placement groups for execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nray::RemovePlacementGroup(placement_group.GetID());\n\nray::PlacementGroup removed_placement_group = ray::GetPlacementGroup(placement_group.GetID());\nassert(removed_placement_group.GetState(), ray::PlacementGroupState::REMOVED);\n```\n\n----------------------------------------\n\nTITLE: Ray Cluster Status and Broadcasting Progress\nDESCRIPTION: Terminal output showing autoscaler operations, node scaling from 22 to 50 target nodes, CPU resizing between 32 and 212 CPUs, and broadcasting of 1GB objects across 50 nodes with progress indicators.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/scalability/object_store.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n(autoscaler +2m27s) Resized to 212 CPUs.\nEnsure all actors have started.: 100%|| 50/50 [00:00<00:00, 68.17it/s]\nBroadcasting objects: 100%|| 50/50 [00:00<00:00, 5873.06it/s]\n\nBroadcast time: 310.718010085 (1073741824 B x 50 nodes)\n\nCurrent # nodes: 22, target: 50\nWaiting ...\n(autoscaler +2s) Tip: use `ray status` to view detailed autoscaling status.\n(autoscaler +2s) Resized to 32 CPUs.\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Kubernetes Cluster\nDESCRIPTION: This command deletes the Kubernetes cluster created using Kind. It's used for cleaning up resources after completing the verification process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/istio.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkind delete cluster\n```\n\n----------------------------------------\n\nTITLE: Installing HyperOpt Dependencies in Python\nDESCRIPTION: This snippet installs the HyperOpt package, version 0.2.5, required for running optimization experiments with Ray Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# install in a hidden cell\n# !pip install \"ray[tune]\"\n!pip install hyperopt==0.2.5\n```\n\n----------------------------------------\n\nTITLE: Checking Volcano PodGroup Status\nDESCRIPTION: Command to check the status of a RayCluster's Volcano PodGroup to verify if it's running correctly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get podgroup ray-test-cluster-0-pg -o yaml\n\n# apiVersion: scheduling.volcano.sh/v1beta1\n# kind: PodGroup\n# metadata:\n#   creationTimestamp: \"2022-12-01T04:43:30Z\"\n#   generation: 2\n#   name: ray-test-cluster-0-pg\n#   namespace: test\n#   ownerReferences:\n#   - apiVersion: ray.io/v1alpha1\n#     blockOwnerDeletion: true\n#     controller: true\n#     kind: RayCluster\n#     name: test-cluster-0\n#     uid: 7979b169-f0b0-42b7-8031-daef522d25cf\n#   resourceVersion: \"4427347\"\n#   uid: 78902d3d-b490-47eb-ba12-d6f8b721a579\n# spec:\n#   minMember: 3\n#   minResources:\n#     cpu: \"3\"\n#     memory: 4Gi\n#   queue: kuberay-test-queue\n# status:\n#   conditions:\n#   - lastTransitionTime: \"2022-12-01T04:43:31Z\"\n#     reason: tasks in the gang are ready to be scheduled\n#     status: \"True\"\n#     transitionID: f89f3062-ebd7-486b-8763-18ccdba1d585\n#     type: Scheduled\n#   phase: Running\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Detailed package requirements file specifying exact versions and SHA-256 hashes for Python dependencies. Used for secure and reproducible package installation. Includes dependencies referenced from ray-project test requirements.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\ncachetools==5.3.2 \\\n    --hash=sha256:086ee420196f7b2ab9ca2db2520aca326318b68fe5ba8bc4d49cca91add450f2 \\\n    --hash=sha256:861f35a13a451f94e301ce2bec7cac63e881232ccce7ed67fab9b5df4d3beaa1\ncertifi==2025.1.31 \\\n    --hash=sha256:3d5da6925056f6f18f119200434a4780a94263f10d1c21d032a6f6b2baa20651 \\\n    --hash=sha256:ca78db4565a652026a4db2bcdf68f2fb589ea80d0be70e03929ed730746b84fe\ncffi==1.16.0 ; platform_python_implementation != 'PyPy' \\\n    --hash=sha256:0c9ef6ff37e974b73c25eecc13952c55bceed9112be2d9d938ded8e856138bcc\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Settings in Python\nDESCRIPTION: Shows how to specify which RL environment to use for training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfig.environment(\"Humanoid-v5\")\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Ray Core Examples\nDESCRIPTION: ReStructuredText markup defining the documentation structure for Ray Core examples, including table of contents and categorized lists of examples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/overview.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _ray-core-examples-tutorial:\n\nRay Core Examples\n=================\n\n.. toctree::\n    :hidden:\n    :glob:\n\n    *\n\n.. Organize example .rst files in the same manner as the\n   .py files in ray/python/ray/train/examples.\n\nBelow are examples for using Ray Core for a variety use cases.\n\nBeginner\n--------\n\n.. list-table::\n\n  * - :doc:`A Gentle Introduction to Ray Core by Example <gentle_walkthrough>`\n  * - :doc:`Using Ray for Highly Parallelizable Tasks <highly_parallel>`\n  * - :doc:`Monte Carlo Estimation of  <monte_carlo_pi>`\n\n\nIntermediate\n------------\n\n.. list-table::\n\n  * - :doc:`Running a Simple MapReduce Example with Ray Core <map_reduce>`\n  * - :doc:`Speed Up Your Web Crawler by Parallelizing it with Ray <web-crawler>`\n\n\nAdvanced\n--------\n\n.. list-table::\n\n  * - :doc:`Build Simple AutoML for Time Series Using Ray <automl_for_time_series>`\n  * - :doc:`Build Batch Prediction Using Ray <batch_prediction>`\n  * - :doc:`Build a Simple Parameter Server Using Ray <plot_parameter_server>`\n  * - :doc:`Simple Parallel Model Selection <plot_hyperparameter>`\n  * - :doc:`Learning to Play Pong <plot_pong_example>`\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Ray Templates\nDESCRIPTION: Shows the expected directory structure for adding a new Ray template, including the main template file, README, and optional requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/README.md#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nray/\n    doc/source/templates/\n        <name-of-your-template>/\n            README.md\n            <name-of-your-template>.ipynb\n            requirements.txt  (Optional)\n        templates.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Ray Dependencies with Cryptographic Hashes in requirements.txt\nDESCRIPTION: This is a requirements.txt file that defines Python package dependencies with specific versions and SHA256 hashes for the Ray project. The format follows PEP 508 with additional hash verification to ensure package integrity and security during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:3e0153a805a98f5ada7e09826255ba99fb4f7524bb81bf6b47fb702666484ae1 \\\n--hash=sha256:410478a0c562d1a5bcc2f7ea448359fcb050ed48b3c6f6f4f18c313a9bdb1826 \\\n--hash=sha256:442acde1e068288a4ba7acfe05f5f343e19fac87bfc96d89eb886b0363e977ec \\\n--hash=sha256:48f6a4533887e189dae092f1cf981f2e3885175f7a0f33c91fb5b7b682b6bab6 \\\n--hash=sha256:4f57dab5fe3407b6c0c1cc907ac98e8a189f9e418f3b6e54d65a718aaafe3950 \\\n--hash=sha256:4f9c515e7914626b2a2e1e311794b4c35720a0be87af52b79ff8e1429fc25f19 \\\n--hash=sha256:55fdc093b5a3cb41d420884cdaf37a1e74c3c37a31f46e66286d9145d2063bd0 \\\n--hash=sha256:5667ed53d68d91920defdf4035d1cdaa3c3121dc0b113255124bcfada1cfa1b8 \\\n--hash=sha256:590344787a90ae57d62511dd7c736ed56b428f04cd8c161fcc5e7232c130c69a \\\n--hash=sha256:5a7d70357e7cee13f470c7883a063aae5fe209a493c57d86eb7f5a6f910fae09 \\\n--hash=sha256:5c3894db91f5a489fc8fa6a9991820f368f0b3cbdb9cd8849547ccfab3392d86 \\\n--hash=sha256:5c849d495bf5154cd8da18a9eb15db127d4dba2968d88831aff6f0331ea9bd4c \\\n--hash=sha256:64536573d0a2cb6e625cf309984e2d873979709f2cf22839bf2d61790b448ad5 \\\n--hash=sha256:693945278a31f2086d9bf3df0fe8254bbeaef1fe71e1351c3bd730aa7d31c41b \\\n--hash=sha256:6db4667b187a6742b33afbbaf05a7bc551ffcf1ced0000a571aedbb4aa42fc7b \\\n--hash=sha256:6eb73fa5426ea69ee0e012fb59cdc76a15b1283d6e32e4f8dc4482ec67d1194d \\\n--hash=sha256:722e1124aec435320ae01ee3ac7bec11a5d47f25d0ed6328f2273d287bc3abb0 \\\n--hash=sha256:7268252af60904bf52c26173cbadc3a071cece75f873705419c8681f24d3edea \\\n--hash=sha256:74fb4bee6880b529a0c6560885fce4dc95936920f9f20f53d99a213f7bf66776 \\\n--hash=sha256:780d3a35680ced9ce682fbcf4cb9c2bad3136eeff760ab33707b71db84664e3a \\\n--hash=sha256:82e8211d69a4f4bc360ea22cd6555f8e61a1bd211d1d5d39d3d228b48c83a897 \\\n--hash=sha256:89aa2c2eeb20957be2d950b85974b30a01a762f3308cd02bb15e1ad632e22dc7 \\\n--hash=sha256:8aefbba5f69d42246543407ed2461db31006b0f76c4e32dfd6f42215a2c41d09 \\\n--hash=sha256:96ec70beabbd3b10e8bfe52616a13561e58fe84c0101dd031dc78f250d5128b9 \\\n--hash=sha256:9750cc7fe1ae3b1611bb8cfc3f9ec11d532244235d75901fb6b8e42ce9229dfe \\\n--hash=sha256:9acbb16f06fe7f52f441bb6f413ebae6c37baa6ef9edd49cdd567216da8600cd \\\n--hash=sha256:9d3e0c25a2350080e9319724dede4f31f43a6c9779be48021a7f4ebde8b2d742 \\\n--hash=sha256:a06339f38e9ed3a64e4c4e43aec7f59084033647f908e4259d279a52d3757d09 \\\n--hash=sha256:a0cb6f11204443f27a1628b0e460f37fb30f624be6051d490fa7d7e26d4af3d0 \\\n--hash=sha256:a7496bfe1da7fb1a4e1cc23bb67c58fab69311cc7d32b5a99c2007b4b2a0e932 \\\n--hash=sha256:a828c57f00f729620a442881cc60e57cfcec6842ba38e1b19fd3e47ac0ff8dc1 \\\n--hash=sha256:a9b2de4cf0cdd5bd2dee4c4f63a653c61d2408055ab77b151c1957f221cabf2a \\\n--hash=sha256:b46c8ae3a8f1f41a0d2ef350c0b6e65822d80772fe46b653ab6b6274f61d4a49 \\\n--hash=sha256:b7e3ed87d4138356775346e6845cccbe66cd9e207f3cd11d2f0b9fd13681359d \\\n--hash=sha256:b7f2f9f912dca3934c1baec2e4585a674ef16fe00218d833856408c48d5beee7 \\\n--hash=sha256:ba60bb19387e13597fb059f32cd4d59445d7b18b69a745b8f8e5db0346f33480 \\\n--hash=sha256:beee944ae828747fd7cb216a70f120767fc9f4f00bacae8543c14a6831673f89 \\\n--hash=sha256:bfa4a17e17ce9abf47a74ae02f32d014c5e9404b6d9ac7f729e01562bbee601e \\\n--hash=sha256:c037a86e8513059a2613aaba4d817bb90b9d9b6b69aace3ce9c877e8c8ed402b \\\n--hash=sha256:c302220494f5c1ebeb0912ea782bcd5e2f8308037b3c7553fad0e48ebad6ad82 \\\n--hash=sha256:c6321c9efe29975232da3bd0af0ad216800a47e93d763ce64f291917a381b8eb \\\n--hash=sha256:c757a9dd70d72b076d6f68efdbb9bc943665ae954dad2801b874c8c69e185068 \\\n--hash=sha256:c99169d4ff810155ca50b4da3b075cbde79752443117d89429595c2e8e37fed8 \\\n--hash=sha256:c9c92be9fd329ac801cc420e08452b70e7aeab94ea4233a4804f0915c14eba9b \\\n--hash=sha256:cc7b01b3754ea68a62bd77ce6020afaffb44a590c2289089289363472d13aedb \\\n--hash=sha256:db9e724bebd621d9beca794f2a4ff1d26eed5965b004a97f1f1685a173b869c2 \\\n--hash=sha256:dca69045298ce5c11fd539682cff879cc1e664c245d1c64da929813e54241d11 \\\n--hash=sha256:dd9b1baec094d91bf36ec729445f7769d0d0cf6b64d04d86e45baf89e2b9059b \\\n--hash=sha256:e02a0e11cf6597299b9f3bbd3f93d79217cb90cfd1411aec33848b13f5c656cc \\\n--hash=sha256:e6a20a581f9ce92d389a8c7d7c3dd47c81fd5d6e655c8dddf341e14aa48659d0 \\\n--hash=sha256:e7004be74cbb7d9f34553a5ce5fb08be14fb33bc86f332fb71cbe5216362a497 \\\n--hash=sha256:e774d53b1a477a67838a904131c4b0eef6b3d8a651f8b138b04f748fccfefe17 \\\n--hash=sha256:edb678da49d9f72c9f6c609fbe41a5dfb9a9282f9e6a2253d5a91e0fc382d7c0 \\\n--hash=sha256:f146e0911cb2f1da549fc58fc7bcd2b836a44b79ef871980d605ec392ff6b0d2 \\\n--hash=sha256:f56e2333dda1fe0f909e7cc59f021eba0d2307bc6f012a1ccf2beca6ba362439 \\\n--hash=sha256:f9a3ea26252bd92f570600098783d1371354d89d5f6b7dfd87359d669f2109b5 \\\n--hash=sha256:f9aa1878d1083b276b0196f2dfbe00c9b7e752475ed3b682025ff20c1c1f51ac \\\n--hash=sha256:fb3c2db03683b5767dedb5769b8a40ebb47d6f7f45b1b3e3b4b51ec8ad9d9825 \\\n--hash=sha256:fbeb989b5cc29e8daf7f976b421c220f1b8c731cbf22b9130d8815418ea45887 \\\n--hash=sha256:fde5bd59ab5357e3853313127f4d3565fc7dad314a74d7b5d43c22c6a5ed2ced \\\n--hash=sha256:fe1a06da377e3a1062ae5fe0926e12b84eceb8a50b350ddca72dc85015873f74\n# via\n#   -c /tmp/ray-deps/requirements_compiled.txt\n#   aiohttp\n#   aiosignal\n```\n\n----------------------------------------\n\nTITLE: Creating a Kind Cluster for Local Kubernetes Testing\nDESCRIPTION: Creates a local Kubernetes cluster using Kind for testing and development purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n! kind create cluster\n```\n\n----------------------------------------\n\nTITLE: Displaying Habana PT Bridge Configuration in Ray Train Worker\nDESCRIPTION: This snippet shows the Habana PT Bridge configuration settings for a Ray Train worker, including various environment variables that control the behavior of the Habana AI accelerator integration with PyTorch.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n(RayTrainWorker pid=36561) ============================= HABANA PT BRIDGE CONFIGURATION =========================== \n(RayTrainWorker pid=36561)  PT_HPU_LAZY_MODE = 1\n(RayTrainWorker pid=36561)  PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024\n(RayTrainWorker pid=36561)  PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n(RayTrainWorker pid=36561)  PT_HPU_LAZY_ACC_PAR_MODE = 0\n(RayTrainWorker pid=36561)  PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n(RayTrainWorker pid=36561)  PT_HPU_EAGER_PIPELINE_ENABLE = 1\n(RayTrainWorker pid=36561)  PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n(RayTrainWorker pid=36561)  PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n```\n\n----------------------------------------\n\nTITLE: Creating a Java Deployment\nDESCRIPTION: This snippet demonstrates how to create a Ray Serve deployment using the Java API. It specifies the full class name to the `Serve.deployment()` method, which creates and deploys the deployment. It assumes the class is already defined and available in the classpath.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/managing-java-deployments.md#2025-04-12_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport io.ray.api.ActorHandle;\nimport io.ray.api.Ray;\nimport io.ray.serve.api.Serve;\nimport io.ray.serve.deployment.Deployment;\n\npublic class ManageDeployment {\n\n  public static void main(String[] args) throws Exception {\n    Serve.start(true, false, null);\n\n    Deployment d = Serve.deployment().setName(\"counter\").setDeploymentDef(Counter.class.getName()).create();\n    d.deploy(true);\n    System.out.println(\"\\\"counter\\\" deployment has been deployed.\");\n\n```\n\n----------------------------------------\n\nTITLE: Defining Kernel Specification for Python Notebook in Markdown\nDESCRIPTION: Shows how to define a kernel specification in a Markdown file to create a Python notebook. This YAML front matter is necessary for converting Markdown to a Jupyter notebook format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n---\njupytext:\n    text_representation:\n        extension: .md\n        format_name: myst\nkernelspec:\n    display_name: Python 3\n    language: python\n    name: python3\n---\n```\n\n----------------------------------------\n\nTITLE: Specifying Packaging Library Dependency\nDESCRIPTION: This snippet defines the packaging library dependency with hash verification. It's utilized by multiple components including Jupyter tools, pytest, and visualization libraries, and is referenced from both cloud and core requirements files.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   ipykernel\n    #   jupyter-server\n    #   jupyterlab\n    #   jupyterlab-server\n    #   lazy-loader\n    #   nbconvert\n    #   pytest\n    #   scikit-image\n    #   tensorboardx\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Ray Dashboard and Jupyter Notebook\nDESCRIPTION: SSH commands to forward both Jupyter Notebook (8888) and Ray Dashboard (8265) ports from an EC2 instance to localhost, enabling access to these interfaces from a local browser when Ray is running on a remote instance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/using-ray-with-jupyter.rst#2025-04-12_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nssh -i /path/my-key-pair.pem -N -f -L localhost:8888:localhost:8888 my-instance-user-name@my-instance-IPv6-address\nssh -i /path/my-key-pair.pem -N -f -L localhost:8265:localhost:8265 my-instance-user-name@my-instance-IPv6-address\n```\n\n----------------------------------------\n\nTITLE: Setting Authentication Token for Ray Job Submission\nDESCRIPTION: Command to create a token for the ray-user service account and export it as a header variable for Ray job submission.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport RAY_JOB_HEADERS=\"{\\\"Authorization\\\": \\\"Bearer $(kubectl create token ray-user --duration=1h)\\\"}\"\n```\n\n----------------------------------------\n\nTITLE: Cleanup Ray Processes\nDESCRIPTION: Commands to stop Ray and shutdown the Skein application.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/yarn.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nray stop\nskein application shutdown current\n```\n\n----------------------------------------\n\nTITLE: Example Queue Configuration in YAML\nDESCRIPTION: YAML snippet showing how to configure a RayCluster with Volcano scheduler and queue name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nlabels:\n  ray.io/scheduler-name: volcano\n  volcano.sh/queue-name: <replace with correct Queue resource name>\n```\n\n----------------------------------------\n\nTITLE: Building Resnet Docker Image\nDESCRIPTION: This Dockerfile sets up a Resnet application environment using the latest Ray CPU image. It installs 'torch' and 'torchvision', and configures the image by downloading necessary application code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/multi-app-container.md#2025-04-12_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use the latest Ray CPU image, `rayproject/ray:latest-py38-cpu`.\nFROM rayproject/ray:latest-py38-cpu\n\n# Install the packages `torch` and `torchvision`, which are dependencies for the ResNet model.\nRUN pip install torch==2.0.1 torchvision==0.15.2\nRUN sudo apt-get update && sudo apt-get install curl -y\n\n# Download the source code for the ResNet application into `resnet50_example.py`.\nRUN curl -O https://raw.githubusercontent.com/ray-project/ray/master/doc/source/serve/doc_code/resnet50_example.py\n\n# Add /home/ray path to PYTHONPATH avoid import module error\nENV PYTHONPATH \"${PYTHONPATH}:/home/ray\"\n```\n\n----------------------------------------\n\nTITLE: Text Classification Client Request\nDESCRIPTION: Python code to send a classification request to the deployed model endpoint. Makes an HTTP GET request and prints the response status and classification result.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/text-classification.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nprompt = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\ninput = \"%20\".join(prompt.split(\" \"))\nresp = requests.get(f\"http://127.0.0.1:8000/classify?sentence={prompt}\")\nprint(resp.status_code, resp.json())\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry Proto Package with Hash Verification\nDESCRIPTION: Defines the opentelemetry-proto package version 1.1.0 with SHA256 hash verification and notes that it's required by the opentelemetry-exporter-otlp package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_19\n\nLANGUAGE: text\nCODE:\n```\nopentelemetry-proto==1.1.0 \\\n    --hash=sha256:237ef4fdd7f752b2fe740352643f8ef82733bd8e0db8b46ed808125ac7c7f112 \\\n    --hash=sha256:ff1ad9a3c572075883c2af0053cefdfaba005d71eade783c4524d34660d53b60\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Operator via Kustomize\nDESCRIPTION: Alternative installation method using Kustomize to install the CRD and KubeRay operator version 1.3.0.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/kuberay-operator-installation.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# Install CRD and KubeRay operator.\nkubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v1.3.0\"\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry-SDK Dependency with Hash Verification\nDESCRIPTION: Defines the opentelemetry-sdk package dependency with version 1.1.0 and includes SHA256 hash validations. Used by opentelemetry-exporter-otlp-proto-grpc and referenced in the main requirements.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-sdk==1.1.0 \\\n    --hash=sha256:ba29274aab656572e97e0339afaad6f2bded4102324b1475ab7412079498df6e \\\n    --hash=sha256:da7dfa6188e8a39f34b99495260e6a1d398c86a9de064c7f0805db6f16733d94\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry Package Dependencies\nDESCRIPTION: Defines multiple OpenTelemetry-related package dependencies with specific versions and hashes. These packages are used for distributed tracing and metrics collection.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_16\n\nLANGUAGE: Text\nCODE:\n```\nopentelemetry-api==1.1.0 \\\n    --hash=sha256:38555cd773df903a2f7440778d6f8b48a86fd388604b171969bdbde4b746a558 \\\n    --hash=sha256:704a3b2a7511d2c9065013d362a8371bc452ae6c0521941de680af2a5ca94884\n\nopentelemetry-exporter-otlp==1.1.0 \\\n    --hash=sha256:2a2135f87cdad417408d34fc6131879d5cee1d7af7546b4a1f67fd178b262f4e \\\n    --hash=sha256:61ee0a6e9a12dd7191aedca34a8a3e7cc4e8e92504a71adf390b6d2bcc36d0d4\n\n# ... (additional OpenTelemetry packages omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Logger Callback in Ray Tune Python Configuration\nDESCRIPTION: This code demonstrates how to use a custom logger callback in Ray Tune configuration. It shows how to pass the custom logger to the Tuner's RunConfig, which will be used during the experiment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\n\ntuner = tune.Tuner(\n    MyTrainableClass,\n    run_config=tune.RunConfig(\n        name=\"experiment_name\", callbacks=[CustomLoggerCallback(\"log_test.txt\")]\n    )\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Minimal vSphere Configuration (Autoscaler)\nDESCRIPTION: This YAML configuration showcases a minimal Ray autoscaler setup for vSphere, defining the essential parameters needed to launch and manage a Ray cluster on vSphere. It includes node types and resource allocation, demonstrating a basic but functional configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/vsphere/example-minimal.yaml\n            :language: yaml\n\n```\n\n----------------------------------------\n\nTITLE: Shutting down Ray\nDESCRIPTION: This snippet shuts down the Ray runtime environment after the experiment is completed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nray.shutdown()\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Row Filtering in Parquet Data\nDESCRIPTION: Demonstrates how to apply row-level filtering using pyarrow dataset fields to filter specific agent IDs from the data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.dataset\n\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\nfrom ray.rllib.core.columns import Columns\n\nconfig = (\n    AlgorithmConfig()\n    .offline_data(\n        input_read_method_kwargs={\n            \"filter\": pyarrow.dataset.field(Columns.AGENT_ID) == \"agent_1\",\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Entropy Coefficient Schedule\nDESCRIPTION: This snippet shows how to configure an entropy coefficient schedule using the `training()` method in the `AlgorithmConfig` object. It uses a list of 2-tuples to define the entropy coefficient at different timesteps, including a sudden drop.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"config.training(\n    entropy_coeff=[\n        [0, 0.05],  # <- initial value at timestep 0\n        [1000000, 0.1],  # <- value at 1M timesteps\n        [1000001, 0.0],  # <- sudden drop to 0.0 right after 1M timesteps\n    ]\n)\"\n```\n\n----------------------------------------\n\nTITLE: Using tune list-trials with output flag\nDESCRIPTION: Example of using the tune list-trials command to list tabular information about trials within an experiment and save the output to a CSV file. The command displays a table of trials with their trainable names, experiment tags, and trial IDs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/cli.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ tune list-trials [EXPERIMENT_DIR] --output note.csv\n\n+------------------+-----------------------+------------+\n| trainable_name   | experiment_tag        | trial_id   |\n|------------------+-----------------------+------------|\n| MyTrainableClass | 0_height=40,width=37  | 87b54a1d   |\n| MyTrainableClass | 1_height=21,width=70  | 23b89036   |\n| MyTrainableClass | 2_height=99,width=90  | 518dbe95   |\n| MyTrainableClass | 3_height=54,width=21  | 7b99a28a   |\n| MyTrainableClass | 4_height=90,width=69  | ae4e02fb   |\n+------------------+-----------------------+------------+\nDropped columns: ['status', 'last_update_time']\nPlease increase your terminal size to view remaining columns.\nOutput saved at: note.csv\n```\n\n----------------------------------------\n\nTITLE: Specifying pycurl Dependency\nDESCRIPTION: Defines the pycurl package dependency with version 7.45.3 and multiple hash values for verification. This ensures a specific, verified version of pycurl is used in the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_30\n\nLANGUAGE: Text\nCODE:\n```\npycurl==7.45.3 \\\n    --hash=sha256:0c41a172d5e8a5cdd8328cc8134f47b2a57960ac677f7cda8520eaa9fbe7d990 \\\n    --hash=sha256:0f0e1251a608ffd75fc502f4014442e554c67d3d7a1b0a839c35efb6ad2f8bf8 \\\n    --hash=sha256:13006b62c157bb4483c58e1abdced6df723c9399255a4f5f6bb7f8e425106679\n```\n\n----------------------------------------\n\nTITLE: Policy RLModule Forward Methods Implementation\nDESCRIPTION: This code implements the forward methods for the VPGPolicyRLModule, including a standard forward method that processes observations through an encoder, and a specialized method that directly processes pre-computed embeddings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef forward(\n    self,\n    observations,\n    *,\n    mode=RLModule.Forward.INFERENCE,\n    timestep=None,\n    explore=None,\n    feature_callbacks=None,\n):\n    # Get the encoder's output: Embeddings to be passed to both policy modules.\n    encoder = VPGSharedEncoderRLModule(\n        observation_space=self.observation_space,\n        action_space=self.action_space,\n        model_config_dict=self.model_config_dict,\n    )\n    embedding = encoder.encode(observations, timestep=timestep)\n    return self.forward_given_embeddings(\n        embeddings=embedding,\n        mode=mode,\n        timestep=timestep,\n        explore=explore,\n        feature_callbacks=feature_callbacks,\n    )\n\ndef forward_given_embeddings(\n    self,\n    embeddings,\n    *,\n    mode=RLModule.Forward.INFERENCE,\n    timestep=None,\n    explore=None,\n    feature_callbacks=None,\n):\n    # Use shared embeddings to compute action logits and state values.\n    logits = self.logits_module(embeddings)\n    values = self.value_module(embeddings)\n\n    # Create a categorical distribution using the logits.\n    action_dist = TorchCategorical(logits)\n\n    # Note: Depending on whether this method is called by p1 or p2\n    # (via the p1/p2 module IDs in our multiagent mapping), the returned\n    # output will later automatically be packed under the agent's key in the\n    # collected SampleBatches. So no need to do that here.\n    if mode == RLModule.Forward.INFERENCE:\n        return action_dist.deterministic_sample()\n    elif mode == RLModule.Forward.EXPLORATION:\n        return action_dist.sample()\n    elif mode == RLModule.Forward.PREDICTION:\n        return {\n            \"action_dist\": action_dist,\n            \"vf\": values.squeeze(-1),\n        }\n```\n\n----------------------------------------\n\nTITLE: Monitoring Ray Cluster Startup\nDESCRIPTION: Shell command to observe the startup progress of the Ray head pod using kubectl and watch.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/ml-example.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# If you're on MacOS, first `brew install watch`.\nwatch -n 1 kubectl get pod\n```\n\n----------------------------------------\n\nTITLE: Documenting UV Autogeneration Command\nDESCRIPTION: This comment indicates that the file was automatically generated by a tool called 'uv'. The comment mentions that the command used for generation is expected but missing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# This file was autogenerated by uv via the following command:\n```\n\n----------------------------------------\n\nTITLE: Adding API References with Sphinx Autodoc in Markdown\nDESCRIPTION: Demonstrates how to use Sphinx's autodoc extension to generate API documentation from source code. This snippet shows how to add function and class references using autofunction and autoclass directives.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n.. autofunction:: ray.tune.integration.docker.DockerSyncer\n\n.. autoclass:: ray.tune.integration.keras.TuneReportCallback\n```\n\n----------------------------------------\n\nTITLE: Serve REST API Successful Response Example\nDESCRIPTION: This shows the response to a successful deployment via the Serve REST API. A `200 OK` response indicates that the request to deploy or update applications was successful.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/api/index.md#2025-04-12_snippet_1\n\nLANGUAGE: http\nCODE:\n```\n\"HTTP/1.1 200 OK\nContent-Type: application/json\"\n```\n\n----------------------------------------\n\nTITLE: Building Ray with Different Configurations\nDESCRIPTION: Commands for building Ray with different optimization levels using Bazel.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nbazel build -c fastbuild //:ray_pkg\n```\n\n----------------------------------------\n\nTITLE: Configuring Learner Iterations in RLlib's Offline RL API\nDESCRIPTION: This code shows how to modify the number of iterations each Learner instance runs per RLlib training iteration using the 'dataset_num_iters_per_learner' attribute.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .offline_data(\n        # Train on 20 batches from the substream in each learner.\n        dataset_num_iters_per_learner=20,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Submitting XGBoost Workload using Ray Job Python SDK\nDESCRIPTION: Python script to submit the XGBoost workload to the Ray cluster using the Ray Job Python SDK. Includes job configuration and submission.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/ml-example.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray.job_submission import JobSubmissionClient\n\nray.init(\"ray://127.0.0.1:8265\")\n\nclient = JobSubmissionClient(\"http://127.0.0.1:8265\")\njob_id = client.submit_job(\n    entrypoint=\"python xgboost_ray_train.py --data-size 100GB\",\n    runtime_env={\n        \"pip\": [\n            \"xgboost_ray==0.1.10\",\n            \"xgboost==1.7.4\",\n            \"pandas==1.5.3\",\n            \"pyarrow==11.0.0\",\n            \"dask==2023.2.0\",\n        ]\n    },\n)\n\nprint(f\"Job submitted: {job_id}.\")\nprint(f\"To follow the job's logs:\\nray job logs '{job_id}' --follow --address http://127.0.0.1:8265\")\n```\n\n----------------------------------------\n\nTITLE: Complete RayCluster CR Example in YAML\nDESCRIPTION: A condensed example of a RayCluster custom resource in YAML format showing the structure with head and worker group specifications, including resource allocation and Ray parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/config.md#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: ray.io/v1alpha1\nkind: RayCluster\nmetadata:\n  name: raycluster-complete\nspec:\n  rayVersion: \"2.3.0\"\n  enableInTreeAutoscaling: true\n  autoscalerOptions:\n     ...\n  headGroupSpec:\n    serviceType: ClusterIP # Options are ClusterIP, NodePort, and LoadBalancer\n    rayStartParams:\n      dashboard-host: \"0.0.0.0\"\n      ...\n    template: # Pod template\n        metadata: # Pod metadata\n        spec: # Pod spec\n            containers:\n            - name: ray-head\n              image: rayproject/ray-ml:2.3.0\n              resources:\n                limits:\n                  cpu: 14\n                  memory: 54Gi\n                requests:\n                  cpu: 14\n                  memory: 54Gi\n              ports: # Optional service port overrides\n              - containerPort: 6379\n                name: gcs\n              - containerPort: 8265\n                name: dashboard\n              - containerPort: 10001\n                name: client\n              - containerPort: 8000\n                name: serve\n                ...\n  workerGroupSpecs:\n  - groupName: small-group\n    replicas: 1\n    minReplicas: 1\n    maxReplicas: 5\n    rayStartParams:\n        ...\n    template: # Pod template\n      spec:\n        ...\n  # Another workerGroup\n  - groupName: medium-group\n    ...\n  # Yet another workerGroup, with access to special hardware perhaps.\n  - groupName: gpu-group\n    ...\n```\n\n----------------------------------------\n\nTITLE: Specifying HTTPS Remote Working Directory\nDESCRIPTION: Example of using a GitHub repository archive as a working directory with a remote HTTPS URI\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nruntime_env = {\"working_dir\": \"https://github.com/example_username/example_respository/archive/HEAD.zip\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Nsight Profiling Options in Ray\nDESCRIPTION: Python code showing how to specify custom Nsight profiling options through Ray's runtime environment. This example demonstrates tracking CUDA, cuDNN, and cuBLAS operations along with CUDA memory usage and graph tracing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/profiling.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport ray\n\nray.init()\n\n@ray.remote(\nnum_gpus=1, \nruntime_env={ \"nsight\": {\n    \"t\": \"cuda,cudnn,cublas\",\n    \"cuda-memory-usage\": \"true\",\n    \"cuda-graph-trace\": \"graph\",\n}})\nclass RayActor:\n    def run():\n    a = torch.tensor([1.0, 2.0, 3.0]).cuda()\n    b = torch.tensor([4.0, 5.0, 6.0]).cuda()\n    c = a * b\n\n    print(\"Result on GPU:\", c)\n\nray_actor = RayActor.remote()\n\n# The Actor or Task process runs with :\n# \"nsys profile -t cuda,cudnn,cublas --cuda-memory-usage=True --cuda-graph-trace=graph ...\"\nray.get(ray_actor.run.remote())\n```\n\n----------------------------------------\n\nTITLE: Manual Actor Termination via Handle in Java\nDESCRIPTION: Shows how to kill a Ray actor in Java using the actorHandle.kill() method. This bypasses normal Java shutdown hooks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/terminating-actors.rst#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nactorHandle.kill();\n```\n\n----------------------------------------\n\nTITLE: AWS Node Types Configuration YAML\nDESCRIPTION: Default configuration for AWS node types including head and worker nodes. Specifies instance types, volume configurations, and resource allocations for both head and worker nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\navailable_node_types:\n  ray.head.default:\n      node_config:\n        InstanceType: m5.large\n        BlockDeviceMappings:\n            - DeviceName: /dev/sda1\n              Ebs:\n                  VolumeSize: 140\n      resources: {\"CPU\": 2}\n  ray.worker.default:\n      node_config:\n        InstanceType: m5.large\n        InstanceMarketOptions:\n            MarketType: spot\n      resources: {\"CPU\": 2}\n      min_workers: 0\n```\n\n----------------------------------------\n\nTITLE: Applying RBAC Configuration\nDESCRIPTION: Command to apply the Role and RoleBinding YAML configuration to the Kubernetes cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f ray-cluster-rbac.yaml\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenCensus Package Dependency\nDESCRIPTION: Defines the opencensus package dependency with version 0.11.3 and SHA-256 hashes. This package is likely used for metrics and tracing in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_15\n\nLANGUAGE: Text\nCODE:\n```\nopencensus==0.11.3 \\\n    --hash=sha256:9c33d572059f0f0e874fc34c697a39a4193aa9cf3203f7e777df42e9edeea56a \\\n    --hash=sha256:af7a98bd51e63968144d772f346d696ed498a32dbdc4be267cd6011c4ce05da8\n```\n\n----------------------------------------\n\nTITLE: Checking Banned Words in Python & Docs\nDESCRIPTION: This shell script checks for banned words in Python and documentation files as part of the linting process. The script should be run in a Unix-like shell environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n./ci/lint/check-banned-words.sh\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Ray Developer Guides in reStructuredText\nDESCRIPTION: This snippet defines a table of contents structure for Ray developer guides using reStructuredText directives. It specifies a maximum depth of 2 and lists the included guide topics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    stability\n    api-policy\n    getting-involved\n    ../ray-core/configure\n    whitepaper\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Operator via Helm\nDESCRIPTION: Installs KubeRay operator version 1.3.0 using Helm package manager. This method adds the KubeRay repository, updates it, and installs both CRDs and the operator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/kuberay-operator-installation.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\n# Install both CRDs and KubeRay operator v1.3.0.\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.3.0\n```\n\n----------------------------------------\n\nTITLE: Checking Ray Cluster Status\nDESCRIPTION: Command to view the status of the Ray cluster, including node health, resource usage, and any pending demands.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/fake-autoscaler.rst#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ ray status\n```\n\n----------------------------------------\n\nTITLE: Package Requirement: markdown-it-py with Hashes\nDESCRIPTION: Specifies the markdown-it-py package version 2.2.0 with SHA-256 hashes for verification. This package is referenced in the rayllm test requirements file and is used by the rich package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nmarkdown-it-py==2.2.0 \\\n    --hash=sha256:5a35f8d1870171d9acc47b99612dc146129b631baf04970128b568f190d0cc30 \\\n    --hash=sha256:7c9a5e412688bc771c67432cbfebcdd686c93ce6484913dccf06cb5a0bea35a1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   rich\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Manually with Helm\nDESCRIPTION: Manually installs KubeRay v1.3.0 using Helm in a GKE cluster without the Ray Operator Addon. Adds the KubeRay repository and installs both CRDs and the operator.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-tpu-cluster.md#2025-04-12_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install both CRDs and KubeRay operator v1.3.0.\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.3.0\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Workers in Ray Train ScalingConfig\nDESCRIPTION: Configures the parallelism by setting the number of workers in the ScalingConfig, which controls how many training actors will be launched.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/using-gpus.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train import ScalingConfig\n\nscaling_config = ScalingConfig(\n    num_workers=8\n)\n```\n\n----------------------------------------\n\nTITLE: Cluster Troubleshooting\nDESCRIPTION: Command for restarting a Ray cluster without running installation commands when the program freezes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nray up CLUSTER.YAML --restart-only\n```\n\n----------------------------------------\n\nTITLE: MinIO S3-Compatible Storage Configuration\nDESCRIPTION: Shows how to configure MinIO storage with endpoint override in storage path\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/persistent-storage.rst#2025-04-12_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom ray import train\nfrom ray.train.torch import TorchTrainer\n\ntrainer = TorchTrainer(\n    ...,\n    run_config=train.RunConfig(\n        storage_path=\"s3://bucket-name/sub-path?endpoint_override=http://localhost:9000\",\n        name=\"unique-run-id\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Scheduling Actors in Placement Group - C++\nDESCRIPTION: Schedules a C++ actor using Ray, showcasing the actor class definition, a factory function, and custom placement within a GPU bundle of the placement group. Dependencies include Ray C++ APIs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nclass Counter {\npublic:\n  Counter(int init_value) : value(init_value){}\n  int GetValue() {return value;}\n  std::string Ping() {\n    return \"pong\";\n  }\nprivate:\n  int value;\n};\n\n// Factory function of Counter class.\nstatic Counter *CreateCounter() {\n  return new Counter();\n};\n\nRAY_REMOTE(&Counter::Ping, &Counter::GetValue, CreateCounter);\n\n// Create GPU actors on a gpu bundle.\nfor (int index = 0; index < 1; index++) {\n  ray::Actor(CreateCounter)\n    .SetPlacementGroup(pg, 0)\n    .Remote(1);\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Package Dependencies with SHA256 Hashes\nDESCRIPTION: This snippet shows a portion of the package dependencies for the Ray project. Each line represents a package with its corresponding SHA256 hash. The hashes are used to verify the integrity of the downloaded packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_19\n\nLANGUAGE: Text\nCODE:\n```\n--hash=sha256:0b3c92fa08759dbf12b3a59579a4096ba9af8dd344d9ec62fde2b591f879778a \\\n--hash=sha256:1a74a13a4c857a84a845505fd2d68e54826a2cd01935a96efb1e9d86c728e186 \\\n--hash=sha256:1d407181cfa6e70077df3377938c08012d18893f9f20e92f7d2f314a437c30b1 \\\n--hash=sha256:1dd4bdd05407ced96fed3d7f25dbbf88d2ffb045a0db60dbc247f5b3c5c25d50 \\\n--hash=sha256:25b411eddcfd56a2f0cd6a384e9f4f7aa3efee14b188de13048c25b5e91f1640\n```\n\n----------------------------------------\n\nTITLE: Configuring File Distribution in Skein YAML\nDESCRIPTION: YAML configuration showing how to specify files for distribution to YARN containers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/yarn.rst#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n    ray-head:\n        instances: 1\n        resources:\n            vcores: 1\n            memory: 2048\n        files:\n            example.py: example.py\n```\n\n----------------------------------------\n\nTITLE: Ray Runtime Environment API Reference Structure\nDESCRIPTION: ReStructuredText directive defining the API documentation structure for Ray's runtime environment components, specifically for RuntimeEnvConfig and RuntimeEnv classes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/runtime-env.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n    :toctree: doc/\n\n    ray.runtime_env.RuntimeEnvConfig\n    ray.runtime_env.RuntimeEnv\n```\n\n----------------------------------------\n\nTITLE: Generating Compiled Requirements with uv pip compile for Ray Project\nDESCRIPTION: Command used to generate compiled requirements file for Ray project with specific package constraints for testing environments. It includes safety flags, index URLs, and find-links for PyTorch and PyG packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cu121 --find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c /tmp/ray-deps/requirements_compiled.txt python/requirements.txt python/requirements/cloud-requirements.txt python/requirements/base-test-requirements.txt -o python/requirements_compiled_ray_test_py311_cu121.txt\n```\n\n----------------------------------------\n\nTITLE: Visualizing CPU Profile with pprof\nDESCRIPTION: Commands for generating and visualizing CPU profile data using google-pprof tool.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/profiling.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Use the appropriate path.\nRAYLET=ray/python/ray/core/src/ray/raylet/raylet\n\ngoogle-pprof -svg $RAYLET /tmp/pprof.out > /tmp/pprof.svg\n# Then open the .svg file with Chrome.\n\n# If you realize the call graph is too large, use -focus=<some function> to zoom\n# into subtrees.\ngoogle-pprof -focus=epoll_wait -svg $RAYLET /tmp/pprof.out > /tmp/pprof.svg\n```\n\n----------------------------------------\n\nTITLE: Installing Nevergrad\nDESCRIPTION: Installs the Nevergrad library using pip.  This is a necessary prerequisite for using Nevergrad with Ray Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"# !pip install ray[tune]\\n!pip install nevergrad==0.4.3.post7 \"\n```\n\n----------------------------------------\n\nTITLE: Parsing VOC-style XML Annotations\nDESCRIPTION: Function to decode XML annotations for object detection, extracting bounding boxes and labels for each object in an image.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_detection.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Tuple\nimport xmltodict\n\ndef decode_annotation(row: Dict[str, Any]) -> Dict[str, Any]:\n    text = row[\"bytes\"].decode(\"utf-8\")\n    annotation = xmltodict.parse(text)[\"annotation\"]\n\n    objects = annotation[\"object\"]\n    # If there's one object, `objects` is a `dict`; otherwise, it's a `list[dict]`.\n    if isinstance(objects, dict):\n        objects = [objects]\n\n    boxes: List[Tuple] = []\n    for obj in objects:\n        x1 = float(obj[\"bndbox\"][\"xmin\"])\n        y1 = float(obj[\"bndbox\"][\"ymin\"])\n        x2 = float(obj[\"bndbox\"][\"xmax\"])\n        y2 = float(obj[\"bndbox\"][\"ymax\"])\n        boxes.append((x1, y1, x2, y2))\n\n    labels: List[int] = [CLASS_TO_LABEL[obj[\"name\"]] for obj in objects]\n\n    filename = annotation[\"filename\"]\n\n    return {\n        \"boxes\": boxes,\n        \"labels\": labels,\n        \"filename\": filename,\n    }\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Function - Python\nDESCRIPTION: This snippet defines the evaluate function which simulates the evaluation of a machine learning model by sleeping for a specified duration. It takes in parameters like step, width, height, and activation to compute a score.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(step, width, height, activation):\\n    time.sleep(0.1)\\n    activation_boost = 10 if activation==\"relu\" else 1\\n    return (0.1 + width * step / 100) ** (-1) + height * 0.1 + activation_boost\n```\n\n----------------------------------------\n\nTITLE: AWS Configuration for Efficient S3 Transfers Bash\nDESCRIPTION: Configures AWS CLI for efficient S3 data transfer by setting maximum concurrent requests, preferred transfer client, target bandwidth, and multipart chunk size.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n!aws configure set s3.max_concurrent_requests 32\n!aws configure set default.s3.preferred_transfer_client crt\n!aws configure set default.s3.target_bandwidth 100Gb/s\n!aws configure set default.s3.multipart_chunksize 8MB\n```\n\n----------------------------------------\n\nTITLE: Running Ray Shuffle on Multi-Node Cluster with 8 CPUs per Node\nDESCRIPTION: Executes Ray's experimental shuffle operation across 4 nodes with 8 CPUs each, 200 partitions, 500MB partition size, and 5GB object store memory per node. Shows cluster connection and memory statistics.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.4.0/data_processing_tests/streaming_shuffle.txt#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m ray.experimental.shuffle --num-cpus=8 --num-partitions=200 --partition-size=500e6 --object-store-memory=5e9 --num-nodes=4\n```\n\n----------------------------------------\n\nTITLE: Implementing Stream Cancellation in Ray Serve\nDESCRIPTION: Shows how to implement a streaming response that detects client disconnection and terminates the stream appropriately. The code generates an infinite stream of numbers and terminates when the client disconnects, demonstrating proper cleanup behavior.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/http-guide.md#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n__begin_cancellation__\n:end-before: __end_cancellation__\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray Actor Creation Performance Metrics in JSON\nDESCRIPTION: This JSON object contains performance metrics for Ray actor creation. It includes the rate of actor creation, total number of actors, execution time, success status, runtime, session URL, commit URL, and stability flag.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/benchmarks/many_actors.txt#2025-04-12_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"actors_per_second\": 333.2797984180003,\n  \"num_actors\": 10000,\n  \"time\": 30.0048189163208,\n  \"success\": \"1\",\n  \"_runtime\": 43.551865577697754,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_han7mApDaGYvrbvhuLKBSGBz\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry Semantic Conventions Package with Hash Verification in Bash\nDESCRIPTION: Defines the OpenTelemetry semantic conventions package version 0.20b0 with SHA256 hash verification for secure package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-semantic-conventions==0.20b0 \\\n    --hash=sha256:ecae7367203e5204c70518e6d24b438480d6a6f1e5c8ee9dc2145f176ff4452e \\\n    --hash=sha256:fac014ac2098b1a05fe58af77cbe74c825ff869d6d53d316c393cc77f507ec15\n```\n\n----------------------------------------\n\nTITLE: KubeRay Documentation Table of Contents\nDESCRIPTION: Hidden table of contents directive listing all available KubeRay documentation pages and user guides, including configuration, deployment, observability, and infrastructure topics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n:hidden:\n\nDeploy Ray Serve Apps <user-guides/rayservice>\nuser-guides/rayservice-no-ray-serve-replica\nuser-guides/rayservice-high-availability\nuser-guides/observability\nuser-guides/upgrade-guide\nuser-guides/k8s-cluster-setup\nuser-guides/storage\nuser-guides/config\nuser-guides/configuring-autoscaling\nuser-guides/kuberay-gcs-ft\nuser-guides/kuberay-gcs-persistent-ft\nuser-guides/gke-gcs-bucket\nuser-guides/persist-kuberay-custom-resource-logs\nuser-guides/persist-kuberay-operator-logs\nuser-guides/gpu\nuser-guides/tpu\nuser-guides/rayserve-dev-doc\nuser-guides/pod-command\nuser-guides/helm-chart-rbac\nuser-guides/tls\nuser-guides/k8s-autoscaler\nuser-guides/static-ray-cluster-without-kuberay\nuser-guides/kubectl-plugin\nuser-guides/kuberay-auth\nuser-guides/reduce-image-pull-latency\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray Memory Information for Remote Task\nDESCRIPTION: Output of the 'ray memory' command showing memory usage information for a task running with a direct object reference, including local references and pinned memory details.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/memory-management.rst#2025-04-12_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n--- Summary for node address: 192.168.0.15 ---\nMem Used by Objects  Local References  Pinned Count  Pending Tasks  Captured in Objects  Actor Handles\n15 MiB               2                 0             1              0                    0\n\n--- Object references for node address: 192.168.0.15 ---\nIP Address    PID    Type    Object Ref                                                Size    Reference Type      Call Site\n192.168.0.15  7411   Worker  ffffffffffffffffffffffffffffffffffffffff0100000001000000  ?       LOCAL_REFERENCE     (deserialize task arg)\n                                                                                                                    __main__.f\n\n192.168.0.15  7373   Driver  a67dc375e60ddd1affffffffffffffffffffffff0100000001000000  ?       LOCAL_REFERENCE     (task call)\n                                                                                                                  | test.py:\n                                                                                                                  :<module>:38\n\n192.168.0.15  7373   Driver  ffffffffffffffffffffffffffffffffffffffff0100000001000000  15 MiB  USED_BY_PENDING_TASK  (put object)\n                                                                                                                  | test.py:\n                                                                                                                  <module>:37\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Dependencies with Hash Verification for Ray Project\nDESCRIPTION: This code snippet shows how to specify exact package dependencies with version pinning and hash verification for the Ray project. Each package includes version constraints and SHA-256 hashes for security verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_5\n\nLANGUAGE: pip\nCODE:\n```\n--hash=sha256:deb6be0ac38ece9ba87dea880e438f25ca3eddfac8b002a2ec3d9183a454e8ae \\\n--hash=sha256:e06ed3eb3218bc64786f7db41917d4e686cc4856944f53d5bdf83a6884432e12 \\\n--hash=sha256:e27ad930a842b4c5eb8ac0016b0a54f5aebbe679340c26101df33424142c143c \\\n--hash=sha256:e537484df0d8f426ce2afb2d0f8e1c3d0b114b83f8850e5f2fbea0e797bd82ae \\\n--hash=sha256:eb00ed941194665c332bf8e078baf037d6c35d7c4f3102ea2d4f16ca94a26dc8 \\\n--hash=sha256:eb6904c354526e758fda7167b33005998fb68c46fbc10e013ca97f21ca5c8887 \\\n--hash=sha256:eb8821e09e916165e160797a6c17edda0679379a4be5c716c260e836e122f54b \\\n--hash=sha256:efcb3f6676480691518c177e3b465bcddf57cea040302f9f4e6e191af91174d4 \\\n--hash=sha256:f27273b60488abe721a075bcca6d7f3964f9f6f067c8c4c605743023d7d3944f \\\n--hash=sha256:f30c3cb33b24454a82faecaf01b19c18562b1e89558fb6c56de4d9118a032fd5 \\\n--hash=sha256:fb69256e180cb6c8a894fee62b3afebae785babc1ee98b81cdf68bbca1987f33 \\\n--hash=sha256:fd1abc0d89e30cc4e02e4064dc67fcc51bd941eb395c502aac3ec19fab46b519 \\\n--hash=sha256:ff8fa367d09b717b2a17a052544193ad76cd49979c805768879cb63d9ca50561\n# via\n#   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n#   requests\nclick==8.1.7 \\\n--hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28 \\\n--hash=sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de\n# via\n#   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n#   -r python/requirements.txt\n#   ray\n#   typer\n#   uvicorn\ncloudpickle==2.2.0 \\\n--hash=sha256:3f4219469c55453cfe4737e564b67c2a149109dabf7f242478948b895f61106f \\\n--hash=sha256:7428798d5926d8fcbfd092d18d01a2a03daf8237d8fcdc8095d256b8490796f0\n# via\n#   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n#   gymnasium\n#   outlines\n#   vllm\n```\n\n----------------------------------------\n\nTITLE: Driver Deployment Configuration - Fixed Replica\nDESCRIPTION: This YAML configuration defines a Ray Serve deployment named 'Driver' with a fixed number of replicas (1) and a maximum number of ongoing requests (200). This setup is suitable when the driver deployment has low CPU usage and primarily makes asynchronous calls.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/advanced-autoscaling.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n\"- name: Driver\n  num_replicas: 1\n  max_ongoing_requests: 200\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray DAG Context\nDESCRIPTION: Reference to the DAGContext class for configuring the context of DAGs in Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/compiled-graph-api.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nray.dag.context.DAGContext\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Values\nDESCRIPTION: This snippet demonstrates how Python package dependencies are specified with exact version numbers and SHA256 hash values for security and reproducibility. It includes comments indicating the source of the requirements and relationships to other files.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_13\n\nLANGUAGE: Text\nCODE:\n```\nmarkdown-it-py==2.2.0 \\\n    --hash=sha256:5a35f8d1870171d9acc47b99612dc146129b631baf04970128b568f190d0cc30 \\\n    --hash=sha256:7c9a5e412688bc771c67432cbfebcdd686c93ce6484913dccf06cb5a0bea35a1\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   rich\n```\n\n----------------------------------------\n\nTITLE: Enabling Subreaper Feature in Python during Ray Initialization\nDESCRIPTION: This snippet shows how to enable the subreaper feature during ray.init() by adding a _system_config parameter that sets the kill_child_processes_on_worker_exit_with_raylet_subreaper flag to True.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/user-spawn-processes.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nray.init(_system_config={\"kill_child_processes_on_worker_exit_with_raylet_subreaper\":True})\n```\n\n----------------------------------------\n\nTITLE: Using the Scheduler Creation Shim Function\nDESCRIPTION: The create_scheduler shim function constructs a scheduler based on a provided string, useful when the scheduler implementation needs to change frequently based on configuration options.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune import create_scheduler\n```\n\n----------------------------------------\n\nTITLE: Specifying prometheus-client 0.19.0 Dependency with Hash Verification\nDESCRIPTION: Defines the prometheus-client package at version 0.19.0 with SHA256 hash verification values. This is imported from requirements_compiled_rayllm_test_py311_cu121.txt and requirements.txt, and used by prometheus-fastapi-instrumentator and vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_27\n\nLANGUAGE: plaintext\nCODE:\n```\nprometheus-client==0.19.0 \\\n    --hash=sha256:4585b0d1223148c27a225b10dbec5ae9bc4c81a99a3fa80774fa6209935324e1 \\\n    --hash=sha256:c88b1e6ecf6b41cd8fb5731c7ae919bf66df6ec6fafa555cd6c0e16ca169ae92\n```\n\n----------------------------------------\n\nTITLE: Setting OMP_NUM_THREADS Environment Variable for Ray\nDESCRIPTION: Example showing how to configure parallel processing by setting OMP_NUM_THREADS when starting Ray\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/resource-allocation.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nOMP_NUM_THREADS=12 ray start --head\nOMP_NUM_THREADS=12 ray start --address=$HEAD_NODE_ADDRESS\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray Experiment Results Table for Atari Breakout\nDESCRIPTION: This snippet shows a formatted table of results from running various reinforcement learning algorithms on the Atari Breakout game. It includes metrics such as trial name, status, iterations, total time, timesteps, reward, and episode statistics.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.0.1/rllib_regression.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n+-------------------------------------------+------------+-------+--------+------------------+---------+\n----------+----------------------+----------------------+--------------------+\n| Trial name                                | status     | loc   |   iter |   total time (s) |      ts |\n   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n|-------------------------------------------+------------+-------+--------+------------------+---------+\n----------+----------------------+----------------------+--------------------|\n```\n\n----------------------------------------\n\nTITLE: Peeking at Current Reduced Values in MetricsLogger - Python\nDESCRIPTION: This snippet shows how to use the peek method to retrieve the currently reduced result for a given key in the MetricsLogger without altering any of the underlying values. It illustrates the importance of checking reduced values without calling the reduce method directly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Peek at the current, reduced value.\n# Note that in the underlying structure, the internal values list still\n# contains all logged values: 0.01, 0.02, 0.03, 0.04, and 0.05.\nprint(logger.peek(\"loss\"))  # Expect: 0.045, which is the average over the last 2 values\n```\n\n----------------------------------------\n\nTITLE: Connecting to Ray Cluster via SSH Port Forwarding in Python\nDESCRIPTION: This Python code shows how to connect to a Ray cluster using Ray Client through an SSH port forwarding setup. It initializes the connection and defines a simple remote function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/ray-client.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\n# This will connect to the cluster via the open SSH session.\nray.init(\"ray://localhost:10001\")\n\n# Normal Ray code follows\n@ray.remote\ndef do_work(x):\n    return x ** x\n\ndo_work.remote(2)\n\n#....\n```\n\n----------------------------------------\n\nTITLE: Basic Python Task Distribution Example\nDESCRIPTION: Simple Python script demonstrating task execution and IP address tracking without Ray integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/getting-started.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import Counter\nimport socket\nimport time\n\ndef f():\n    time.sleep(0.001)\n    # Return IP address.\n    return socket.gethostbyname(\"localhost\")\n\nip_addresses = [f() for _ in range(10000)]\nprint(Counter(ip_addresses))\n```\n\n----------------------------------------\n\nTITLE: Starting a New Mars on Ray Runtime Locally\nDESCRIPTION: Code to initialize Ray, create a new Mars session on Ray, and perform a simple matrix computation using Mars tensor module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/mars-on-ray.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init()\nimport mars\nmars.new_ray_session()\nimport mars.tensor as mt\nmt.random.RandomState(0).rand(1000_0000, 5).sum().execute()\n```\n\n----------------------------------------\n\nTITLE: Git Commit Example with Microcheck Test Specification\nDESCRIPTION: Example showing how to manually add specific tests to microcheck by including @microcheck directives in the git commit message body. The example demonstrates adding documentation tests for serve components.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/ci.rst#2025-04-12_snippet_0\n\nLANGUAGE: git\nCODE:\n```\ngit commit -a -s\n\nrun other serve doc tests\n\n@microcheck //doc:source/serve/doc_code/distilbert //doc:source/serve/doc_code/object_detection //doc:source/serve/doc_code/stable_diffusion\n\nSigned-off-by: can <can@anyscale.com>\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Toctree for Ray Observability Documentation\nDESCRIPTION: Sphinx toctree configuration that defines the structure of the Ray observability documentation. It hides the direct table of contents while linking to API references, CLI documentation, and system metrics pages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/reference/index.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n\napi\ncli\nsystem-metrics\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for BERT Training\nDESCRIPTION: This Bash snippet installs necessary Python libraries to run the BERT training notebook, including Ray and Hugging Face transformers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/bert.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install ray[train] notebook transformers datasets evaluate\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Cluster Launcher\nDESCRIPTION: Command to install Ray with cluster launcher support using pip\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# install ray\npip install -U ray[default]\n```\n\n----------------------------------------\n\nTITLE: Local Testing Mode in Ray Serve\nDESCRIPTION: The snippet describes enabling a local testing mode in Ray Serve using `_local_testing_mode` in `serve.run`. This mode allows running deployments in a single-process environment, aiding unit testing and debugging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dev-workflow.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n```{literalinclude} ../doc_code/local_dev.py\n:start-after: __local_dev_testing_start__\n:end-before: __local_dev_testing_end__\n:language: python\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Building Whisper Docker Image\nDESCRIPTION: This Dockerfile defines the setup for a Whisper application environment using the Ray latest GPU image. It installs the 'faster_whisper' package and downloads the Whisper source code while adjusting the PYTHONPATH.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/multi-app-container.md#2025-04-12_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Use the latest Ray GPU image, `rayproject/ray:latest-py38-gpu`, so the Whisper model can run on GPUs.\nFROM rayproject/ray:latest-py38-gpu\n\n# Install the package `faster_whisper`, which is a dependency for the Whisper model.\nRUN pip install faster_whisper==0.10.0\nRUN sudo apt-get update && sudo apt-get install curl -y\n\n# Download the source code for the Whisper application into `whisper_example.py`.\nRUN curl -O https://raw.githubusercontent.com/ray-project/ray/master/doc/source/serve/doc_code/whisper_example.py\n\n# Add /home/ray path to PYTHONPATH avoid import module error\nENV PYTHONPATH \"${PYTHONPATH}:/home/ray\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Running Pods After Resource Availability\nDESCRIPTION: Command to check that the second RayCluster's Pods are now running after the first cluster was deleted, demonstrating YuniKorn's gang scheduling behavior.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl get pods\n\nNAME                                  READY   STATUS    RESTARTS   AGE\ntest-yunikorn-1-head-xl2r5            1/1     Running   0          3m34s\ntest-yunikorn-1-worker-worker-l6ttz   1/1     Running   0          3m34s\ntest-yunikorn-1-worker-worker-vjsts   1/1     Running   0          3m34s\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with SHA-256 Hashes\nDESCRIPTION: List of Python package dependencies with their exact versions and SHA-256 hash values. Each package includes a list of dependencies that require it. The file appears to be a compiled requirements file specific to Python 3.11 with CUDA 12.4 support.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_9\n\nLANGUAGE: txt\nCODE:\n```\ngymnasium==1.0.0 \\\n    --hash=sha256:9d2b66f30c1b34fe3c2ce7fae65ecf365d0e9982d2b3d860235e773328a3b403 \\\n    --hash=sha256:b6f40e1e24c5bd419361e1a5b86a9117d2499baecc3a660d44dfff4c465393ad\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Logging Format with Deployment Decorator\nDESCRIPTION: Demonstrates how to set JSON logging format using the deployment decorator configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@serve.deployment(logging_config={\"encoding\": \"JSON\"})\nclass Model:\n    def __call__(self):\n        return \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment using RLlib and Gymnasium - String\nDESCRIPTION: Setup an RL environment using a string name registered with Farama's Gymnasium within RLlib. This example shows configuration with the registered environment 'Acrobot-v1'.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-env.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\nconfig = (\n    PPOConfig()\n    # Configure the RL environment to use as a string (by name), which\n    # is registered with Farama's gymnasium.\n    .environment(\"Acrobot-v1\")\n)\nalgo = config.build()\nprint(algo.train())\n```\n\nLANGUAGE: Python\nCODE:\n```\nalgo.stop()\n```\n\n----------------------------------------\n\nTITLE: Configuring Nsight System Profiler\nDESCRIPTION: This snippet provides options for configuring the Nsight System Profiler in a Ray runtime environment. It includes examples for using the default configuration and customizing options.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n\"default\"\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Class Documentation Template in RST\nDESCRIPTION: A template that generates documentation for a Python class using Sphinx autodoc features. It includes the module context and automatically documents all class members. Uses Jinja templating variables for dynamic content insertion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/autosummary/class_without_init_args.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}()\n    :members:\n\n```\n\n----------------------------------------\n\nTITLE: Java Stack Trace with Attribute Setting in Rhino JavaScript Execution\nDESCRIPTION: This stack trace shows attribute setting in Mozilla Rhino's IdScriptableObject during JavaScript function activation. It demonstrates how property attributes (like readOnly, permanent, etc.) are configured when creating the execution context for JavaScript handlers in Vert.x.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_66\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.createFunctionActivation_[j];org/mozilla/javascript/IdScriptableObject:.setAttributes_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Submitting Ray Job with Default Environment (Bash)\nDESCRIPTION: This bash command submits a Ray job using the default environment. It demonstrates how to use the 'ray job submit' command to run a Python script in the cluster's default environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/quickstart.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ ray job submit -- python script.py\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing Function\nDESCRIPTION: Defining a function to preprocess images using torchvision transforms for the distributed pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/01_batch_inference/start.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom typing import Any, Dict\n\ndef preprocess_image(row: Dict[str, np.ndarray]):\n    return {\n        \"original_image\": row[\"image\"],\n        \"transformed_image\": transform(row[\"image\"]),\n    }\n```\n\n----------------------------------------\n\nTITLE: Installing pydantic-core with Pinned Version and Hashes\nDESCRIPTION: Specifies pydantic-core package with version 2.23.4 and multiple SHA256 hashes for verification. This is a dependency required by other packages in the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_28\n\nLANGUAGE: pip\nCODE:\n```\npydantic-core==2.23.4 \\\n    --hash=sha256:0a7df63886be5e270da67e0966cf4afbae86069501d35c8c1b3b6c168f42cb36 \\\n    --hash=sha256:0cb3da3fd1b6a5d0279a01877713dbda118a2a4fc6f0d821a57da2e464793f05 \\\n    --hash=sha256:0dbd8dbed2085ed23b5c04afa29d8fd2771674223135dc9bc937f3c09284d071 \\\n    --hash=sha256:0dff76e0602ca7d4cdaacc1ac4c005e0ce0dcfe095d5b5259163a80d3a10d327 \\\n    --hash=sha256:1278e0d324f6908e872730c9102b0112477a7f7cf88b308e4fc36ce1bdb6d58c \\\n    --hash=sha256:128585782e5bfa515c590ccee4b727fb76925dd04a98864182b22e89a4e6ed36 \\\n    --hash=sha256:1498bec4c05c9c787bde9125cfdcc63a41004ff167f495063191b863399b1a29 \\\n    --hash=sha256:19442362866a753485ba5e4be408964644dd6a09123d9416c54cd49171f50744 \\\n    --hash=sha256:1b84d168f6c48fabd1f2027a3d1bdfe62f92cade1fb273a5d68e621da0e44e6d \\\n    --hash=sha256:1e90d2e3bd2c3863d48525d297cd143fe541be8bbf6f579504b9712cb6b643ec \\\n    --hash=sha256:20152074317d9bed6b7a95ade3b7d6054845d70584216160860425f4fbd5ee9e \\\n    --hash=sha256:216f9b2d7713eb98cb83c80b9c794de1f6b7e3145eef40400c62e86cee5f4e1e \\\n    --hash=sha256:233710f069d251feb12a56da21e14cca67994eab08362207785cf8c598e74577 \\\n    --hash=sha256:255a8ef062cbf6674450e668482456abac99a5583bbafb73f9ad469540a3a232 \\\n    --hash=sha256:2584f7cf844ac4d970fba483a717dbe10c1c1c96a969bf65d61ffe94df1b2863 \\\n    --hash=sha256:2971bb5ffe72cc0f555c13e19b23c85b654dd2a8f7ab493c262071377bfce9f6 \\\n    --hash=sha256:29d2c342c4bc01b88402d60189f3df065fb0dda3654744d5a165a5288a657368 \\\n    --hash=sha256:2e203fdf807ac7e12ab59ca2bfcabb38c7cf0b33c41efeb00f8e5da1d86af480 \\\n    --hash=sha256:33e3d65a85a2a4a0dc3b092b938a4062b1a05f3a9abde65ea93b233bca0e03f2 \\\n    --hash=sha256:374a5e5049eda9e0a44c696c7ade3ff355f06b1fe0bb945ea3cac2bc336478a2 \\\n    --hash=sha256:37b0fe330e4a58d3c58b24d91d1eb102aeec675a3db4c292ec3928ecd892a9a6 \\\n```\n\n----------------------------------------\n\nTITLE: Listing Package Dependency Hashes for Ray Project\nDESCRIPTION: This snippet contains SHA-256 hashes for various Python package dependencies used in the Ray project. The format lists each package with its version number, followed by SHA-256 hashes for verification, and comments indicating which components of the Ray project depend on that package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_31\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:1dfc94946bc60ea375cc39cff0b8da6c7e5f8fcdc1d946beb8da5c216156ddd8 \\\n--hash=sha256:2034f6759a722da3a3dbd91a81148cf884e91d1b747992ca288ab88c1de15999 \\\n--hash=sha256:261ddb7ca91fcf71757979534fb4c128448b5b4c55cb6152d280312062f69599 \\\n--hash=sha256:2ed854e716a89b1afcedea551cd85f2eb2a807613752ab997b9974aaa0d56936 \\\n--hash=sha256:3102045a10945173d38336f6e71a8dc71bcaeed55c3123ad4af82c52807b9375 \\\n--hash=sha256:339894035d0ede518b16073bdc2feef4c991ee991a29774b33e515f1d308e08d \\\n--hash=sha256:412444afb8c4c7a6cc11a47dade32982439925537e483be7c0ae0cf96c4f6a0b \\\n--hash=sha256:4203efca580f0dd6f882ca211f923168548f7ba334c189e9eab1178ab840bf60 \\\n--hash=sha256:45ebc7b45406febf07fef35d856f0293a92e7417ae7933207e90bf9090b70572 \\\n--hash=sha256:4b5ec25d8b17217d635f8935dbc1b9aa5907962fae29dff220f2659487891cd3 \\\n--hash=sha256:4c8e73e99da7db1b4cad7f8d682cf6abad7844da39834c288fbfa394a47bbced \\\n--hash=sha256:4e6f7d1c414191c1199f8996d3f2282b9ebea0945693fb67392c75a3a320941f \\\n--hash=sha256:4eaa22f0d22b1a7e93ff0a596d57fdede2e550aecffb5a1ef1106aaece48e96b \\\n--hash=sha256:50b8eae8f7334ec826d6eeffaeeb00e36b5e24aa0b9df322c247539714c6df19 \\\n--hash=sha256:50fd3f6b26e3441ae07b7c979309638b72abc1a25da31a81a7fbd9495713ef4f \\\n--hash=sha256:51243f1ed5161b9945011a7360e997729776f6e5d7005ba0c6879267d4c5139d \\\n--hash=sha256:5d512aafa1d32efa014fa041d38868fda85028e3f930a96f85d49c7d8ddc0383 \\\n--hash=sha256:5f77cf66e96ae734717d341c145c5949c63180842a545c47a0ce7ae52ca83795 \\\n--hash=sha256:6b02471b72526ab8a18c39cb7967b72d194ec53c1fd0a70b050565a0f366d355 \\\n--hash=sha256:6fb1b30043271ec92dc65f6d9f0b7a830c210b8a96423074b15c7bc999975f57 \\\n--hash=sha256:7161ec49ef0800947dc5570f86568a7bb36fa97dd09e9827dc02b718c5643f09 \\\n--hash=sha256:72d622d262e463dfb7595202d229f5f3ab4b852289a1cd09650362db23b9eb0b \\\n--hash=sha256:74d28c17412d9caa1066f7a31df8403ec23d5268ba46cd0ad2c50fb82ae40462 \\\n--hash=sha256:78618cdbccaa74d3f88d0ad6cb8ac3007f1a6fa5c6f19af64b55ca170bfa1edf \\\n--hash=sha256:793b4e24db2e8742ca6423d3fde8396db336698c55cd34b660663ee9e45ed37f \\\n--hash=sha256:798232c92e7665fe82ac085f9d8e8ca98826f8e27859d9a96b41d519ecd2e49a \\\n--hash=sha256:81d09caa7b27ef4e61cb7d8fbf1714f5aec1c6b6c5270ee53504981e6e9121ad \\\n--hash=sha256:8ab74c06ffdab957d7670c2a5a6e1a70181cd10b727cd788c4dd9005b6a8acd9 \\\n--hash=sha256:8eb0908e954d093b02a543dc963984d6e99ad2b5e36503d8a0aaf040505f747d \\\n--hash=sha256:90b9e29824800e90c84e4022dd5cc16eb2d9605ee13f05d47641eb183cd73d45 \\\n--hash=sha256:9797a6c8fe16f25749b371c02e2ade0efb51155e767a971c61734b1bf6293994 \\\n--hash=sha256:9d2455fbf44c914840c793e89aa82d0e1763a14253a000743719ae5946814b2d \\\n--hash=sha256:9d3bea1c75f8c53ee4d505c3e67d8c158ad4df0d83170605b50b64025917f338 \\\n--hash=sha256:9e2ec1e921fd07c7cda7962bad283acc2f2a9ccc1b971ee4b216b75fad6f0463 \\\n--hash=sha256:9e91179a242bbc99be65e139e30690e081fe6cb91a8e77faf4c409653de39451 \\\n--hash=sha256:a0eaa93d054751ee9964afa21c06247779b90440ca41d184aeb5d410f20ff591 \\\n--hash=sha256:a2c405445c79c3f5a124573a051062300936b0281fee57637e706453e452746c \\\n--hash=sha256:aa7e402ce11f0885305bfb6afb3434b3cd8f53b563ac065452d9d5654c7b86fd \\\n--hash=sha256:aff76a55a8aa8364d25400a210a65ff59d0168e0b4285ba6bf2bd83cf675ba32 \\\n--hash=sha256:b09b86b27a064c9624d0a6c54da01c1beaf5b6cadfa609cf63789b1d08a797b9 \\\n--hash=sha256:b14f16f94cbc61215115b9b1236f9c18403c15dd3c52cf629072afa9d54c1cbf \\\n--hash=sha256:b50811d664d392f02f7761621303eba9d1b056fb1868c8cdf4231279645c25f5 \\\n--hash=sha256:b7bc2176354defba3edc2b9a777744462da2f8e921fbaf61e52acb95bafa9828 \\\n--hash=sha256:c78e1b00a87ce43bb37642c0812315b411e856a905d58d597750eb79802aaaa3 \\\n--hash=sha256:c83341b89884e2b2e55886e8fbbf37c3fa5efd6c8907124aeb72f285ae5696e5 \\\n--hash=sha256:ca2870d5d10d8726a27396d3ca4cf7976cec0f3cb706debe88e3a5bd4610f7d2 \\\n--hash=sha256:ccce24b7ad89adb5a1e34a6ba96ac2530046763912806ad4c247356a8f33a67b \\\n--hash=sha256:cd5e14fbf22a87321b24c88669aad3a51ec052eb145315b3da3b7e3cc105b9a2 \\\n--hash=sha256:ce49c67f4ea0609933d01c0731b34b8695a7a748d6c8d186f95e7d085d2fe475 \\\n--hash=sha256:d33891be6df59d93df4d846640f0e46f1a807339f09e79a8040bc887bdcd7ed3 \\\n--hash=sha256:d3b2348a78bc939b4fed6552abfd2e7988e0f81443ef3911a4b8498ca084f6eb \\\n--hash=sha256:d886f5d353333b4771d21267c7ecc75b710f1a73d72d03ca06df49b09015a9ef \\\n--hash=sha256:d93480005693d247f8346bc8ee28c72a2191bdf1f6b5db469c096c0c867ac015 \\\n--hash=sha256:dc1a390a82755a8c26c9964d457d4c9cbec5405896cba94cf51f36ea0d855002 \\\n--hash=sha256:dd78700f5788ae180b5ee8902c6aea5a5726bac7c364b202b4b3e3ba2d293170 \\\n--hash=sha256:e46f38133e5a060d46bd630faa4d9fa0202377495df1f068a8299fd78c84de84 \\\n--hash=sha256:e4b878386c4bf293578b48fc570b84ecfe477d3b77ba39a6e87150af77f40c57 \\\n--hash=sha256:f0d0591a0aeaefdaf9a5e545e7485f89910c977087e7de2b6c388aec32011e9f \\\n--hash=sha256:fdcbb4068117dfd9ce0138d068ac512843c52295ed996ae6dd1faf537b6dbc27 \\\n--hash=sha256:ff61bfd9253c3915e6d41c651d5f962da23eda633cf02262990094a18a55371a\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   -r python/requirements/llm/llm-test-requirements.txt\n#   imageio\n#   mistral-common\n#   scikit-image\n#   torchvision\n#   vllm\n```\n\n----------------------------------------\n\nTITLE: AI and ML Library Requirements for Ray Project\nDESCRIPTION: Package requirements for AI and ML libraries needed by the Ray project, including OpenAI, computer vision, and LLM-related dependencies. Each entry includes version constraints and SHA256 hashes.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_17\n\nLANGUAGE: requirements.txt\nCODE:\n```\nopenai==1.63.2 \\\n    --hash=sha256:1f38b27b5a40814c2b7d8759ec78110df58c4a614c25f182809ca52b080ff4d4 \\\n    --hash=sha256:aeabeec984a7d2957b4928ceaa339e2ead19c61cfcf35ae62b7c363368d26360\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\nopencensus==0.11.3 \\\n    --hash=sha256:9c33d572059f0f0e874fc34c697a39a4193aa9cf3203f7e777df42e9edeea56a \\\n    --hash=sha256:af7a98bd51e63968144d772f346d696ed498a32dbdc4be267cd6011c4ce05da8\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\nopencensus-context==0.1.3 \\\n    --hash=sha256:073bb0590007af276853009fac7e4bab1d523c3f03baf4cb4511ca38967c6039 \\\n    --hash=sha256:a03108c3c10d8c80bb5ddf5c8a1f033161fa61972a9917f9b9b3a18517f0088c\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   opencensus\nopencv-python-headless==4.11.0.86 \\\n    --hash=sha256:0e0a27c19dd1f40ddff94976cfe43066fbbe9dfbb2ec1907d66c19caef42a57b \\\n    --hash=sha256:48128188ade4a7e517237c8e1e11a9cdf5c282761473383e77beb875bb1e61ca \\\n    --hash=sha256:6c304df9caa7a6a5710b91709dd4786bf20a74d57672b3c31f7033cc638174ca \\\n    --hash=sha256:6efabcaa9df731f29e5ea9051776715b1bdd1845d7c9530065c7951d2a2899eb \\\n    --hash=sha256:996eb282ca4b43ec6a3972414de0e2331f5d9cda2b41091a49739c19fb843798 \\\n    --hash=sha256:a66c1b286a9de872c343ee7c3553b084244299714ebb50fbdcd76f07ebbe6c81 \\\n    --hash=sha256:f447d8acbb0b6f2808da71fddd29c1cdd448d2bc98f72d9bb78a7a898fc9621b\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   mistral-common\noutlines==0.1.11 \\\n    --hash=sha256:0997bd9da1cc050e430bd08995dc7d4bd855918bafa4531e49d3f37110a23aba \\\n    --hash=sha256:f5a5f2242ed9802d3aab7a92789bf4008d734c576be9258cc0a297f690124727\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\noutlines-core==0.1.26 \\\n    --hash=sha256:00f409f72c11f6ffadb57066950dd384d5388015028c1a1a615c9a64988dae3e \\\n    --hash=sha256:11ff56af56cb54c563b7f25d86cd9ee77f3fed825f1d4dccd9449bb1e4e89538 \\\n    --hash=sha256:15a3684fa29564da2db03934cf0097bef3e871f70d3af0ef2b52fdb886da2e09 \\\n    --hash=sha256:19f462f6b00935708677ad27cb4df55e0e17f6ffe713ab750f5f2683b090f95d \\\n    --hash=sha256:1e0ea28a76da31d25b6f53242bf13e1b59a0241badf82353c88f55e1cf81b128 \\\n    --hash=sha256:2f8641aab4a6bd84516907492ce82099503129da01b3c29c1dc9ad50320bae77 \\\n    --hash=sha256:3f59aeccea21ed6ff3cf52102fd163f26d279821c20e5127ddd18d4ea4d0c8d2 \\\n    --hash=sha256:481c4301341e77cc8f1832d616784adb4d461b4fec65878e7c0d2cba7163a189 \\\n    --hash=sha256:64e01c0cfa9ba371634d7c3f6ea1862397cef98e4509fe98e3f57faa721a72d6 \\\n    --hash=sha256:6a962a7452e7ac170fa04d405342cadae2d28fafa5b1830cef7aa610257ed32f \\\n    --hash=sha256:7b7849cf40028319ebb9d8ba0fe4c590ef5888eebe524a81b3af30aaa06ea21c \\\n    --hash=sha256:8cc8c87d89bd267356f8149c9066cbb98970425ec162997fbf195c3f1feb7009 \\\n    --hash=sha256:9525321b48700dcaaabf60bcdc951e45f9357ba3fb3e1bfc81b662d7d4170e7c \\\n    --hash=sha256:9b36bff12779e58883747116893a17b3551bbd10865878b951b03a44d112229a \\\n    --hash=sha256:9d792a43ed9d8a4e1b38f4d83fe99db442d57aad4404c2edf98b710892eda47e \\\n    --hash=sha256:a3c4196148e47f455f1ace78e329d5b97e531cbc406456d681592952adae7e17 \\\n    --hash=sha256:a84b7cd2fb6268bf990dd3d479ffb4fa0bace6f571cb85b15b6cdb44b84f5b69 \\\n    --hash=sha256:a8932044a3d9329be53a226118850638f85b4d7842f9b863d0a123f23de220cd \\\n    --hash=sha256:ad8564ecd7b64bcb840596c5049ff1c1a96346de494302ffcc0f2b188c15675e \\\n    --hash=sha256:b6787b07b7c673fc3087d2b537719ecac8e03b10a47d032dd1926985c32885b0 \\\n    --hash=sha256:bba56604efdbc5932c7a8a88c2b8b0d0c740ab883b0012fb5464a9736796802b \\\n    --hash=sha256:e86a1bb46adc5cbf6dfd7a7fe4105e0e2a4c6e041732a053126b41c521a1f223 \\\n    --hash=sha256:f19765c151abfc970996368080aeea6d2a19e927817fe4e2af6726e639be3de4 \\\n    --hash=sha256:f38d290a7f6e5e12cbfcaee03269dfc0dbda49b360024b4279d1aba251fdc346 \\\n    --hash=sha256:f54633bca50055d42ea4d94ae06dcbe52d3d76a9b621b75723b1177d0d952953\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   outlines\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n```\n\n----------------------------------------\n\nTITLE: Accessing Grafana via Port Forwarding\nDESCRIPTION: This command sets up port forwarding to access the Grafana web interface on localhost:3000. It forwards the port from the Grafana deployment in the prometheus-system namespace.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nkubectl port-forward deployment/prometheus-grafana -n prometheus-system 3000:3000\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Call Trace with Softirq and DMA Handling\nDESCRIPTION: Call stack trace showing Netty NIO event loop processing with kernel softirq handling and DMA operations. This trace demonstrates how network data is processed through the Linux kernel's interrupt handling mechanisms after being sent from the Java application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_11\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];dma_issue_pending_all_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Watching All Namespaces Configuration\nDESCRIPTION: Commands to set up and verify RBAC configuration for watching all namespaces in the Kubernetes cluster using ClusterRole and ClusterRoleBinding.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/helm-chart-rbac.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Create a Kubernetes cluster using Kind.\nkind create cluster --image=kindest/node:v1.26.0\n\n# Create namespaces.\nkubectl create ns n1\nkubectl create ns n2\n\n# Install a KubeRay operator. Use the default `values.yaml` file.\n# (path: helm-chart/kuberay-operator)\nhelm install kuberay-operator .\n\n# Check ClusterRole.\nkubectl get clusterrole | grep kuberay\n# kuberay-operator                  2023-10-15T04:54:28Z\n\n# Check Role.\nkubectl get role\n#NAME                               CREATED AT\n#kuberay-operator-leader-election   2023-10-15T04:54:28Z\n\n# Install RayCluster in the `default`, `n1`, and `n2` namespaces.\nhelm install raycluster kuberay/ray-cluster --version 1.3.0\nhelm install raycluster kuberay/ray-cluster --version 1.3.0 -n n1\nhelm install raycluster kuberay/ray-cluster --version 1.3.0 -n n2\n\n# You should create a RayCluster in these 3 namespaces.\nkubectl get raycluster -A\n# NAMESPACE   NAME                 DESIRED WORKERS   AVAILABLE WORKERS   STATUS   AGE\n# default     raycluster-kuberay   1                 1                   ready    73s\n# n1          raycluster-kuberay   1                 1                   ready    56s\n# n2          raycluster-kuberay   1                 1                   ready    52s\n```\n\n----------------------------------------\n\nTITLE: Modifying RayJob Configuration with GCS Bucket\nDESCRIPTION: This command uses sed to replace the GCS_BUCKET placeholder in the RayJob configuration with the actual GCS bucket name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/GCS_BUCKET/$BUCKET/g\" ray-job.pytorch-image-classifier.yaml\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray Autoscaler Status in Plaintext\nDESCRIPTION: This snippet shows the output of a Ray autoscaler status command. It includes information about active nodes, pending nodes, recent failures, and resource usage in the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_status_multinode_v1.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n======== Autoscaler status: .+\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_.+\n 1 node_.+\n 1 node_.+\n 1 node_.+\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/8.0 CPU\n 0.+\n 0.+\n\nDemands:\n (no resource demands)\n```\n\n----------------------------------------\n\nTITLE: Accessing Ray Pod Logs\nDESCRIPTION: Commands to access and check Ray logs within a pod's filesystem.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/observability.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec -it $RAY_POD -n $YOUR_NAMESPACE -- bash\n# Check the logs under /tmp/ray/session_latest/logs/\n```\n\n----------------------------------------\n\nTITLE: Testing Unauthorized Access to Ray Cluster\nDESCRIPTION: Commands to port-forward the Ray cluster service and attempt to submit a job without authentication, which should result in a 401 Unauthorized error.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward svc/ray-cluster-with-auth-head-svc 8265:8265 &\nray job submit --address http://localhost:8265  -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n```\n\n----------------------------------------\n\nTITLE: Launching and Tearing Down Ray Clusters using Shell Commands\nDESCRIPTION: Basic commands to create, update and tear down Ray clusters using backend-specific configuration files. Supports AWS, GCP, Kubernetes, or local backends.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-cli.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Replace '<your_backend>' with one of: 'aws', 'gcp', 'kubernetes', or 'local'.\n$ BACKEND=<your_backend>\n\n# Create or update the cluster.\n$ ray up ray/python/ray/autoscaler/$BACKEND/example-full.yaml\n\n# Tear down the cluster.\n$ ray down ray/python/ray/autoscaler/$BACKEND/example-full.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking KubeRay Operator Logs for Error Diagnosis\nDESCRIPTION: Command to extract logs from the KubeRay operator pod and save them to a file for analysis. This helps identify errors in the control plane that might affect RayService deployments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Column Filtering in RLlib\nDESCRIPTION: Example of setting up column filtering for Parquet data reading using AlgorithmConfig to specify required columns and avoid loading unnecessary data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\nfrom ray.rllib.core.columns import Columns\n\nconfig = (\n    AlgorithmConfig()\n    .offline_Data(\n        input_read_method_kwargs={\n            \"columns\": [\n                Columns.EPS_ID,\n                Columns.AGENT_ID,\n                Columns.OBS,\n                Columns.NEXT_OBS,\n                Columns.REWARDS,\n                Columns.ACTIONS,\n                Columns.TERMINATED,\n                Columns.TRUNCATED,\n            ],\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Full AWS Configuration (Autoscaler)\nDESCRIPTION: This YAML configuration represents a comprehensive Ray autoscaler setup for AWS, showcasing a detailed configuration with various options. It defines node types, resource allocation, and advanced settings for managing a Ray cluster on AWS.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/aws/example-full.yaml\n            :language: yaml\n\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Data Package\nDESCRIPTION: Command to install Ray Data using pip package manager. This installs Ray with the data processing components enabled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/data.rst#2025-04-12_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ pip install -U 'ray[data]'\n```\n\n----------------------------------------\n\nTITLE: Logging Output for Customized Raymond Worker Processes\nDESCRIPTION: Demonstrates log output from ray workers with customized logger settings, showing implementation detail of message formatting from modified loggers in both Tasks and Actors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n(Actor pid=179641) INFO:__main__:A log message for an actor.\n(f pid=177572) INFO:__main__:A log message for a task.\n```\n\n----------------------------------------\n\nTITLE: Increasing System-Wide Hard Limit for File Descriptors in Bash\nDESCRIPTION: This snippet outlines the process for increasing the hard limit for open file descriptors system-wide, which is necessary for robust performance in environments with extensive Ray tasks and workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-failures.rst#2025-04-12_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nsudo bash -c \"echo $USER hard nofile 65536 >> /etc/security/limits.conf\"\n* Logout and log back in.\n```\n\n----------------------------------------\n\nTITLE: Getting Head Service Info\nDESCRIPTION: Retrieves information about the RayCluster head service\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nkubectl get service raycluster-kuberay-head-svc\n```\n\n----------------------------------------\n\nTITLE: Specifying Numpy Package Dependency\nDESCRIPTION: Defines the numpy package dependency with version 1.26.4 and multiple SHA-256 hashes for verification. This ensures consistent and secure installation of the numpy package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_14\n\nLANGUAGE: Text\nCODE:\n```\nnumpy==1.26.4 \\\n    --hash=sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b \\\n    --hash=sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818 \\\n    --hash=sha256:1af303d6b2210eb850fcf03064d364652b7120803a0b872f5211f5234b399f20 \\\n    # ... (additional hashes omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Example Python Documentation Style\nDESCRIPTION: Demonstrates the canonical Ray documentation style for Python functions and classes\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef ray_canonical_doc_style(param1: int, param2: str) -> bool:\n    \"\"\"First sentence MUST be inline with the quotes and fit on one line.\n\n    Additional explanatory text can be added in paragraphs such as this one.\n    Do not introduce multi-line first sentences.\n\n    Examples:\n        .. doctest::\n\n            >>> # Provide code examples for key use cases, as possible.\n            >>> ray_canonical_doc_style(41, \"hello\")\n            True\n\n            >>> # A second example.\n            >>> ray_canonical_doc_style(72, \"goodbye\")\n            False\n\n    Args:\n        param1: The first parameter. Do not include the types in the\n            docstring. They should be defined only in the signature.\n            Multi-line parameter docs should be indented by four spaces.\n        param2: The second parameter.\n\n    Returns:\n        The return value. Do not include types here.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Commenting RestructuredText Hiring Message\nDESCRIPTION: A commented out RestructuredText admonition block containing a hiring message for RLlib team at Anyscale Inc, with links to job application and company website.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_includes/rllib/we_are_hiring.rst#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. TODO: we comment out the hiring message, as it's too much with the RL conf announcement.\n    uncomment again after the summit on March 29th.\n\n..\n    .. admonition:: We're hiring!\n\n        The RLlib team at `Anyscale Inc. <https://anyscale.com>`__, the company behind Ray,\n        is hiring interns and full-time **reinforcement learning engineers** to help advance and maintain RLlib.\n        If you have a background in ML/RL and are interested in making RLlib **the** industry-leading open-source RL library,\n        `apply here today <https://jobs.lever.co/anyscale/186d9b8d-3fee-4e07-bb8e-49e85cf33d6b>`__.\n        We'd be thrilled to welcome you on the team!\n```\n\n----------------------------------------\n\nTITLE: Installing Python Test Dependencies\nDESCRIPTION: Commands to install required Python dependencies for running Ray tests\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install -c python/requirements_compiled.txt -r python/requirements/test-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Detailed package requirements list with specific versions and SHA-256 hashes for security. Includes package dependencies for Ray project with their source references.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cpu\n--find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html\n\naiofiles==22.1.0 \\\n    --hash=sha256:1142fa8e80dbae46bb6339573ad4c8c0841358f79c6eb50a493dceca14621bad \\\n    --hash=sha256:9107f1ca0b2a5553987a94a3c9959fe5b491fdf731389aa5b7b1bd0733e32de6\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ypy-websocket\n[...additional packages truncated...]\n```\n\n----------------------------------------\n\nTITLE: Defining Pillow Image Processing Library Dependency\nDESCRIPTION: This code specifies the Pillow image processing library dependency with extensive hash verification for version 10.3.0. This is a critical dependency for image handling in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    --hash=sha256:0ea2a783a2bdf2a561808fe4a7a12e9aa3799b701ba305de596bc48b8bdfce9d \\\n    --hash=sha256:1530e8f3a4b965eb6a7785cf17a426c779333eb62c9a7d1bbcf3ffd5bf77a4aa \\\n    --hash=sha256:16563993329b79513f59142a6b02055e10514c1a8e86dca8b48a893e33cf91e3 \\\n    --hash=sha256:19aeb96d43902f0a783946a0a87dbdad5c84c936025b8419da0a0cd7724356b1 \\\n    --hash=sha256:1a1d1915db1a4fdb2754b9de292642a39a7fb28f1736699527bb649484fb966a \\\n    --hash=sha256:1b87bd9d81d179bd8ab871603bd80d8645729939f90b71e62914e816a76fc6bd \\\n    --hash=sha256:1dfc94946bc60ea375cc39cff0b8da6c7e5f8fcdc1d946beb8da5c216156ddd8 \\\n    --hash=sha256:2034f6759a722da3a3dbd91a81148cf884e91d1b747992ca288ab88c1de15999 \\\n    --hash=sha256:261ddb7ca91fcf71757979534fb4c128448b5b4c55cb6152d280312062f69599 \\\n    --hash=sha256:2ed854e716a89b1afcedea551cd85f2eb2a807613752ab997b9974aaa0d56936 \\\n    --hash=sha256:3102045a10945173d38336f6e71a8dc71bcaeed55c3123ad4af82c52807b9375 \\\n    --hash=sha256:339894035d0ede518b16073bdc2feef4c991ee991a29774b33e515f1d308e08d \\\n    --hash=sha256:412444afb8c4c7a6cc11a47dade32982439925537e483be7c0ae0cf96c4f6a0b \\\n    --hash=sha256:4203efca580f0dd6f882ca211f923168548f7ba334c189e9eab1178ab840bf60 \\\n    --hash=sha256:45ebc7b45406febf07fef35d856f0293a92e7417ae7933207e90bf9090b70572 \\\n    --hash=sha256:4b5ec25d8b17217d635f8935dbc1b9aa5907962fae29dff220f2659487891cd3 \\\n    --hash=sha256:4c8e73e99da7db1b4cad7f8d682cf6abad7844da39834c288fbfa394a47bbced \\\n    --hash=sha256:4e6f7d1c414191c1199f8996d3f2282b9ebea0945693fb67392c75a3a320941f \\\n    --hash=sha256:4eaa22f0d22b1a7e93ff0a596d57fdede2e550aecffb5a1ef1106aaece48e96b \\\n    --hash=sha256:50b8eae8f7334ec826d6eeffaeeb00e36b5e24aa0b9df322c247539714c6df19 \\\n    --hash=sha256:50fd3f6b26e3441ae07b7c979309638b72abc1a25da31a81a7fbd9495713ef4f \\\n    --hash=sha256:51243f1ed5161b9945011a7360e997729776f6e5d7005ba0c6879267d4c5139d \\\n    --hash=sha256:5d512aafa1d32efa014fa041d38868fda85028e3f930a96f85d49c7d8ddc0383 \\\n    --hash=sha256:5f77cf66e96ae734717d341c145c5949c63180842a545c47a0ce7ae52ca83795 \\\n    --hash=sha256:6b02471b72526ab8a18c39cb7967b72d194ec53c1fd0a70b050565a0f366d355 \\\n    --hash=sha256:6fb1b30043271ec92dc65f6d9f0b7a830c210b8a96423074b15c7bc999975f57 \\\n    --hash=sha256:7161ec49ef0800947dc5570f86568a7bb36fa97dd09e9827dc02b718c5643f09 \\\n    --hash=sha256:72d622d262e463dfb7595202d229f5f3ab4b852289a1cd09650362db23b9eb0b \\\n    --hash=sha256:74d28c17412d9caa1066f7a31df8403ec23d5268ba46cd0ad2c50fb82ae40462 \\\n    --hash=sha256:78618cdbccaa74d3f88d0ad6cb8ac3007f1a6fa5c6f19af64b55ca170bfa1edf \\\n    --hash=sha256:793b4e24db2e8742ca6423d3fde8396db336698c55cd34b660663ee9e45ed37f \\\n    --hash=sha256:798232c92e7665fe82ac085f9d8e8ca98826f8e27859d9a96b41d519ecd2e49a \\\n    --hash=sha256:81d09caa7b27ef4e61cb7d8fbf1714f5aec1c6b6c5270ee53504981e6e9121ad \\\n    --hash=sha256:8ab74c06ffdab957d7670c2a5a6e1a70181cd10b727cd788c4dd9005b6a8acd9 \\\n    --hash=sha256:8eb0908e954d093b02a543dc963984d6e99ad2b5e36503d8a0aaf040505f747d \\\n    --hash=sha256:90b9e29824800e90c84e4022dd5cc16eb2d9605ee13f05d47641eb183cd73d45 \\\n    --hash=sha256:9797a6c8fe16f25749b371c02e2ade0efb51155e767a971c61734b1bf6293994 \\\n    --hash=sha256:9d2455fbf44c914840c793e89aa82d0e1763a14253a000743719ae5946814b2d \\\n    --hash=sha256:9d3bea1c75f8c53ee4d505c3e67d8c158ad4df0d83170605b50b64025917f338 \\\n    --hash=sha256:9e2ec1e921fd07c7cda7962bad283acc2f2a9ccc1b971ee4b216b75fad6f0463 \\\n    --hash=sha256:9e91179a242bbc99be65e139e30690e081fe6cb91a8e77faf4c409653de39451 \\\n    --hash=sha256:a0eaa93d054751ee9964afa21c06247779b90440ca41d184aeb5d410f20ff591 \\\n    --hash=sha256:a2c405445c79c3f5a124573a051062300936b0281fee57637e706453e452746c \\\n    --hash=sha256:aa7e402ce11f0885305bfb6afb3434b3cd8f53b563ac065452d9d5654c7b86fd \\\n    --hash=sha256:aff76a55a8aa8364d25400a210a65ff59d0168e0b4285ba6bf2bd83cf675ba32 \\\n    --hash=sha256:b09b86b27a064c9624d0a6c54da01c1beaf5b6cadfa609cf63789b1d08a797b9 \\\n    --hash=sha256:b14f16f94cbc61215115b9b1236f9c18403c15dd3c52cf629072afa9d54c1cbf \\\n    --hash=sha256:b50811d664d392f02f7761621303eba9d1b056fb1868c8cdf4231279645c25f5 \\\n    --hash=sha256:b7bc2176354defba3edc2b9a777744462da2f8e921fbaf61e52acb95bafa9828 \\\n    --hash=sha256:c78e1b00a87ce43bb37642c0812315b411e856a905d58d597750eb79802aaaa3 \\\n\n```\n\n----------------------------------------\n\nTITLE: Listing Docker Images\nDESCRIPTION: Lists the available Docker images on the system. This helps verify the successful creation of the Ray Docker image.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n\"docker images\"\n```\n\n----------------------------------------\n\nTITLE: Running Documentation Tests\nDESCRIPTION: Command to execute tests for examples included in Python docstrings, ensuring code examples remain valid.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/README.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmake doctest\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding for Grafana in Kubernetes\nDESCRIPTION: Commands to set up port forwarding for accessing Grafana from a local machine. This involves getting the Grafana pod name and forwarding its port 3000 to the local machine.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.md#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl --namespace default port-forward $POD_NAME 3000\n```\n\n----------------------------------------\n\nTITLE: Applying Kueue Resource Configuration\nDESCRIPTION: Command to apply the Kueue resource configuration to the Kubernetes cluster. This step sets up the necessary Kueue components for priority scheduling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f kueue-resources.yaml\n```\n\n----------------------------------------\n\nTITLE: Documenting Ray Stack Command in reStructuredText\nDESCRIPTION: This snippet uses the click directive to generate documentation for the 'ray stack' command from the ray.scripts.scripts module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/cli.rst#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. click:: ray.scripts.scripts:stack\n   :prog: ray stack\n   :show-nested:\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry SDK Dependencies\nDESCRIPTION: This snippet specifies the OpenTelemetry SDK dependency with hash verification. The SDK is used by multiple components of the Ray project, including cloud requirements and core functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-sdk==1.1.0 \\\n    --hash=sha256:ba29274aab656572e97e0339afaad6f2bded4102324b1475ab7412079498df6e \\\n    --hash=sha256:da7dfa6188e8a39f34b99495260e6a1d398c86a9de064c7f0805db6f16733d94\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\n```\n\n----------------------------------------\n\nTITLE: Including HyperBand Example Code with reStructuredText Directive\nDESCRIPTION: A reStructuredText directive that includes the hyperband_example.py file from the Ray Tune examples directory. This directive is used in documentation to embed the full example code without duplicating it.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/hyperband_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. literalinclude:: /../../python/ray/tune/examples/hyperband_example.py\n```\n\n----------------------------------------\n\nTITLE: Submitting Ray Job via CLI\nDESCRIPTION: Command for submitting a Ray job using the Ray Jobs CLI. It specifies the Ray cluster address and executes a Python script within the current working directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_start_windows_osx.txt#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRAY_ADDRESS='http://[address]:8265' ray job submit --working-dir . -- python my_script.py\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows the format used to specify Python package dependencies with their versions and SHA256 hashes. It includes comments indicating the source of the requirement and any related packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_5\n\nLANGUAGE: Text\nCODE:\n```\nclick==8.1.7 \\\n    --hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28 \\\n    --hash=sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   ray\n    #   typer\n    #   uvicorn\n```\n\n----------------------------------------\n\nTITLE: Printing Requests Module Version in Ray Task (Python)\nDESCRIPTION: This Python script demonstrates how to use Ray to print the version of the 'requests' module in a remote task. It initializes Ray and defines a remote function to get the requests version.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/quickstart.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport requests\n\n@ray.remote\ndef get_requests_version():\n    return requests.__version__\n\n# Note: No need to specify the runtime_env in ray.init() in the driver script.\nray.init()\nprint(\"requests version:\", ray.get(get_requests_version.remote()))\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Training Parity Benchmark on GPU\nDESCRIPTION: This command runs a benchmark comparing Ray Train's TorchTrainer to native PyTorch Distributed on GPU, using 16 workers across multiple nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/benchmarks.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython workloads/torch_benchmark.py run --num-runs 3 --num-epochs 20 --num-workers 16 --cpus-per-worker 4 --use-gpu\n```\n\n----------------------------------------\n\nTITLE: Docker Image Pull and Run for Intel Gaudi Environment\nDESCRIPTION: Retrieves and runs the official Habana Labs Docker image for PyTorch with Gaudi support, enabling HPU-accelerated machine learning workflows\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n```\n\n----------------------------------------\n\nTITLE: Setting Weights and Biases API Key\nDESCRIPTION: WANDB_API_KEY sets the Weights and Biases API key for integration with Ray Tune. Alternative to using 'wandb login'.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nWANDB_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Running TensorFlow Training Parity Benchmark on CPU\nDESCRIPTION: This command runs a benchmark comparing Ray Train's TensorflowTrainer to native TensorFlow Distributed on CPU, using 16 workers across multiple nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/benchmarks.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython workloads/tensorflow_benchmark.py run --num-runs 3 --num-epochs 20 --num-workers 16 --cpus-per-worker 2\n```\n\n----------------------------------------\n\nTITLE: Deploying Applications using Serve CLI - Console\nDESCRIPTION: Console commands to deploy applications defined in a YAML config file using Serve CLI. Dependencies include a running Ray cluster. The deployment is triggered via 'serve deploy' and can be verified using 'serve status'.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/multi-app.md#2025-04-12_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ ray start --head\n\n$ serve deploy config.yaml\n> Sent deploy request successfully!\n```\n\n----------------------------------------\n\nTITLE: Generating Python Class Documentation with Sphinx and Jinja2\nDESCRIPTION: A template for creating Python class API documentation using Sphinx. The template includes the full class name as a heading, sets the current module context, and documents all class members.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/autosummary/class_without_autosummary_noinheritance.rst#2025-04-12_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n    :members:\n\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Requirements file specifying package versions with SHA256 hash verification for secure installation. Includes major ML frameworks and utilities needed for the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_49\n\nLANGUAGE: requirements.txt\nCODE:\n```\n--hash=sha256:91b51a507007d63a70b65be307d701088d15042a6399c0e2312b53072226e909 \\\n--hash=sha256:a49f8755c74a89553294a99ab25aa87ab1cddbfa40fe58387e09f64f0578cedc \\\n--hash=sha256:aa926114d1e13ffe5b2ea59c3f195216f26646d7fe36e9e5207b291e4b7902ff \\\n--hash=sha256:aaf3cfa290597ebbdf19d1a78729e3f555e459506cd58f8d7399359ac5e02a05 \\\n--hash=sha256:b75815b6a601edad52b4181e9805c8fcd04813a6ab1d5cd8127188dfd2788e20 \\\n--hash=sha256:bb0edd69103c154245c5f209f0507355cc68ba7e4de350084bc31edc562478e4 \\\n--hash=sha256:e73d43dbc68d8c711e70edecc4ac70472799a25ec4ec18a84d479ee18033d3c5 \\\n--hash=sha256:ea290e435464cf0794f657b48786e5fa413362abe55ed771c172c25980d070ce \\\n--hash=sha256:f8e85821317c9c0fbf1256e9f721cfb1400ba1e09becb844b3ddd91f744805fc\n```\n\n----------------------------------------\n\nTITLE: Listing Ray ML Library Integrations with reStructuredText\nDESCRIPTION: This code snippet creates a bulleted list of references to various Ray ML library integrations using reStructuredText format with internal document references.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/index.rst#2025-04-12_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n- :ref:`ray-joblib`\n- :ref:`ray-multiprocessing`\n- :ref:`ray-collective`\n- :ref:`dask-on-ray`\n- :ref:`spark-on-ray`\n- :ref:`mars-on-ray`\n- :ref:`modin-on-ray`\n- `daft <https://www.getdaft.io>`_\n```\n\n----------------------------------------\n\nTITLE: Process Creation Chain Diagram\nDESCRIPTION: This ASCII diagram illustrates the process hierarchy chain for Ray worker processes and how orphaned processes get reparented to the Raylet process when their parent dies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/user-spawn-processes.rst#2025-04-12_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nraylet -> the worker -> user process A -> user process B -> user process C\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with Hash Values - Ray Dependencies\nDESCRIPTION: Comprehensive list of Python package dependencies with their version constraints and SHA-256 hash values for verification. The file uses pip's hash verification format to ensure package integrity during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_5\n\nLANGUAGE: txt\nCODE:\n```\n--hash=sha256:1f71c10d1e88467126f0efd484bd44bca5e14c664ec2ede64c32f20875c0d413 \\\n--hash=sha256:2424ff4c4ac7f6b8177b53c17ed5d8fa74ae5955656867f5a8affaca36a27abb \\\n--hash=sha256:2bce03af1ce5a5567ab89bd90d11e7bbdff56b8af3acbbec1faded8f44cb06da \\\n# via\n#   -c /tmp/ray-deps/requirements_compiled.txt\n#   pyopenssl\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoSummary for Ray Data Classes in reStructuredText\nDESCRIPTION: This snippet sets up Sphinx autosummary directives to generate API documentation for specific classes in the ray.data module. It specifies the template to use and the classes to document.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/_autogen.rst#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: ray.data\n\n.. autosummary::\n    :nosignatures:\n    :template: autosummary/class_v2.rst\n    :toctree:\n\n    DataIterator\n    Dataset\n    Schema\n    grouped_data.GroupedData\n    aggregate.AggregateFn\n    aggregate.AggregateFnV2\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry-Exporter-OTLP Dependency with Hash Verification\nDESCRIPTION: Defines the opentelemetry-exporter-otlp package dependency with version 1.1.0 and includes SHA256 hash validations. Referenced from requirements_compiled_ray_test_py311_cpu.txt and the main requirements.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-exporter-otlp==1.1.0 \\\n    --hash=sha256:2a2135f87cdad417408d34fc6131879d5cee1d7af7546b4a1f67fd178b262f4e \\\n    --hash=sha256:61ee0a6e9a12dd7191aedca34a8a3e7cc4e8e92504a71adf390b6d2bcc36d0d4\n```\n\n----------------------------------------\n\nTITLE: Using the Default FIFOScheduler in Ray Tune\nDESCRIPTION: FIFOScheduler is the default scheduler in Ray Tune that processes trials in a first-in-first-out order.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/schedulers.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.schedulers import FIFOScheduler\n```\n\n----------------------------------------\n\nTITLE: Defining Additional Python Package Requirements with Dependencies\nDESCRIPTION: This snippet defines multiple Python package requirements with their versions, hash verification, and comments indicating which requirement files reference them. It includes packages like jmespath, json5, jsonpatch, jsonpointer, jsonref, jsonschema, and various jupyter components.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_11\n\nLANGUAGE: text\nCODE:\n```\njmespath==1.0.1 \\\n    --hash=sha256:02e2e4cc71b5bcab88332eebf907519190dd9e6e82107fa7f83b1003a6252980 \\\n    --hash=sha256:90261b206d6defd58fdd5e85f478bf633a2901798906be2ad389150c5c60edbe\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   boto3\n    #   botocore\njson5==0.9.14 \\\n    --hash=sha256:740c7f1b9e584a468dbb2939d8d458db3427f2c93ae2139d05f47e453eae964f \\\n    --hash=sha256:9ed66c3a6ca3510a976a9ef9b8c0787de24802724ab1860bc0153c7fdd589b02\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   jupyterlab-server\njsonpatch==1.32 \\\n    --hash=sha256:26ac385719ac9f54df8a2f0827bb8253aa3ea8ab7b3368457bcdb8c14595a397 \\\n    --hash=sha256:b6ddfe6c3db30d81a96aaeceb6baf916094ffa23d7dd5fa2c13e13f8b6e600c2\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/cloud-requirements.txt\njsonpointer==2.4 \\\n    --hash=sha256:15d51bba20eea3165644553647711d150376234112651b4f1811022aecad7d7a \\\n    --hash=sha256:585cee82b70211fa9e6043b7bb89db6e1aa49524340dde8ad6b63206ea689d88\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   jsonpatch\n    #   jsonschema\njsonref==1.1.0 \\\n    --hash=sha256:32fe8e1d85af0fdefbebce950af85590b22b60f9e95443176adbde4e1ecea552 \\\n    --hash=sha256:590dc7773df6c21cbf948b5dac07a72a251db28b0238ceecce0a2abfa8ec30a9\n    # via -r python/requirements/llm/llm-requirements.txt\njsonschema==4.23.0 \\\n    --hash=sha256:d71497fef26351a33265337fa77ffeb82423f3ea21283cd9467bb03999266bc4 \\\n    --hash=sha256:fbadb6f8b144a8f8cf9f0b89ba94501d143e50411a1278633f56a7acf7fd5566\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements/llm/llm-requirements.txt\n    #   -r python/requirements.txt\n    #   jupyter-events\n    #   jupyterlab-server\n    #   mistral-common\n    #   nbformat\n    #   outlines\n    #   outlines-core\n    #   ray\njsonschema-specifications==2024.10.1 \\\n    --hash=sha256:0f38b83639958ce1152d02a7f062902c41c8fd20d558b0c34344292d417ae272 \\\n    --hash=sha256:a09a0680616357d9a0ecf05c12ad234479f549239d0f5b55f3deea67475da9bf\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   jsonschema\njupyter-client==7.3.4 \\\n    --hash=sha256:17d74b0d0a7b24f1c8c527b24fcf4607c56bee542ffe8e3418e50b21e514b621 \\\n    --hash=sha256:aa9a6c32054b290374f95f73bb0cae91455c58dfb84f65c8591912b8f65e6d56\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   ipykernel\n    #   jupyter-server\n    #   nbclassic\n    #   nbclient\n    #   notebook\njupyter-core==5.5.0 \\\n    --hash=sha256:880b86053bf298a8724994f95e99b99130659022a4f7f45f563084b6223861d3 \\\n    --hash=sha256:e11e02cd8ae0a9de5c6c44abf5727df9f2581055afe00b22183f621ba3585805\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   ipykernel\n    #   jupyter-client\n    #   jupyter-server\n    #   jupyterlab\n    #   nbclassic\n    #   nbconvert\n    #   nbformat\n    #   notebook\njupyter-events==0.6.3 \\\n    --hash=sha256:57a2749f87ba387cd1bfd9b22a0875b889237dbf2edc2121ebb22bde47036c17 \\\n```\n\n----------------------------------------\n\nTITLE: Enabling Subreaper Feature via Ray Start Command in Bash\nDESCRIPTION: This command shows how to enable the subreaper feature when starting a Ray cluster by setting the environment variable RAY_kill_child_processes_on_worker_exit_with_raylet_subreaper to true.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/user-spawn-processes.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRAY_kill_child_processes_on_worker_exit_with_raylet_subreaper=true ray start --head\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version and Hash\nDESCRIPTION: Configuration for the fsspec, gguf, google-api-core, google-auth, googleapis-common-protos, and grpcio packages with their specific versions and hash verifications, along with their dependency relationships.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nfsspec==2023.5.0 \\\n    --hash=sha256:51a4ad01a5bb66fcc58036e288c0d53d3975a0df2a5dc59a93b59bade0391f2a \\\n    --hash=sha256:b3b56e00fb93ea321bc9e5d9cf6f8522a0198b20eb24e02774d329e9c6fb84ce\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   huggingface-hub\n    #   torch\ngguf==0.10.0 \\\n    --hash=sha256:52a30ef26328b419ffc47d9269fc580c238edf1c8a19b5ea143c323e04a038c1 \\\n    --hash=sha256:706089fba756a06913227841b4a6c8398360fa991569fd974e663a92b224e33f\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   vllm\ngoogle-api-core==1.34.0 \\\n    --hash=sha256:6fb380f49d19ee1d09a9722d0379042b7edb06c0112e4796c7a395078a043e71 \\\n    --hash=sha256:7421474c39d396a74dfa317dddbc69188f2336835f526087c7648f91105e32ff\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   opencensus\ngoogle-auth==2.23.4 \\\n    --hash=sha256:79905d6b1652187def79d491d6e23d0cbb3a21d3c7ba0dbaa9c8a01906b13ff3 \\\n    --hash=sha256:d4bbc92fe4b8bfd2f3e8d88e5ba7085935da208ee38a134fc280e7ce682a05f2\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   google-api-core\ngoogleapis-common-protos==1.61.0 \\\n    --hash=sha256:22f1915393bb3245343f6efe87f6fe868532efc12aa26b391b15132e1279f1c0 \\\n    --hash=sha256:8a64866a97f6304a7179873a465d6eee97b7a24ec6cfd78e0f575e96b821240b\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   google-api-core\n    #   opentelemetry-exporter-otlp-proto-grpc\ngrpcio==1.66.2 \\\n    --hash=sha256:02697eb4a5cbe5a9639f57323b4c37bcb3ab2d48cec5da3dc2f13334d72790dd \\\n    --hash=sha256:03b0b307ba26fae695e067b94cbb014e27390f8bc5ac7a3a39b7723fed085604 \\\n    --hash=sha256:05bc2ceadc2529ab0b227b1310d249d95d9001cd106aa4d31e8871ad3c428d73 \\\n    --hash=sha256:06de8ec0bd71be123eec15b0e0d457474931c2c407869b6c349bd9bed4adbac3 \\\n    --hash=sha256:0be4e0490c28da5377283861bed2941d1d20ec017ca397a5df4394d1c31a9b50 \\\n    --hash=sha256:12fda97ffae55e6526825daf25ad0fa37483685952b5d0f910d6405c87e3adb6 \\\n    --hash=sha256:1caa38fb22a8578ab8393da99d4b8641e3a80abc8fd52646f1ecc92bcb8dee34 \\\n    --hash=sha256:2018b053aa15782db2541ca01a7edb56a0bf18c77efed975392583725974b249 \\\n    --hash=sha256:20657d6b8cfed7db5e11b62ff7dfe2e12064ea78e93f1434d61888834bc86d75 \\\n    --hash=sha256:2335c58560a9e92ac58ff2bc5649952f9b37d0735608242973c7a8b94a6437d8 \\\n    --hash=sha256:31fd163105464797a72d901a06472860845ac157389e10f12631025b3e4d0453 \\\n    --hash=sha256:38b68498ff579a3b1ee8f93a05eb48dc2595795f2f62716e797dc24774c1aaa8 \\\n    --hash=sha256:3b00efc473b20d8bf83e0e1ae661b98951ca56111feb9b9611df8efc4fe5d55d \\\n\n```\n\n----------------------------------------\n\nTITLE: View Cluster Logs\nDESCRIPTION: Commands to view and stream log files from Ray cluster nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nray logs cluster\nray logs cluster gcs_server.out --node-id <NODE_ID>\nray logs raylet.out --node-ip <NODE_IP> --follow\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import list_logs, get_log\nlist_logs(node_id=<HEAD_NODE_ID>)\n\nfor line in get_log(filename=\"gcs_server.out\", node_id=<NODE_ID>):\n    print(line)\n```\n\n----------------------------------------\n\nTITLE: Running Template Validation Script\nDESCRIPTION: Command to run the validation script that checks template paths and YAML formatting.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/README.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python doc/source/templates/testing/validate.py\nSuccess!\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Ray Clusters for Example\nDESCRIPTION: Command to remove all existing Ray Clusters before running the provided example.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete raycluster --all\n```\n\n----------------------------------------\n\nTITLE: Managing Named Actors in Different Namespaces using Java\nDESCRIPTION: Shows how to create and retrieve named actors in different namespaces using Ray in Java. This demonstrates how named actors are scoped by namespace and how to access them across different jobs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/named-actors.rst#2025-04-12_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nimport ray\n\nclass Actor {\n}\n\n// Driver1.java\n// Job 1 creates an actor, \"orange\" in the \"colors\" namespace.\nSystem.setProperty(\"ray.job.namespace\", \"colors\");\nRay.init();\nRay.actor(Actor::new).setName(\"orange\").remote();\n\n// Driver2.java\n// Job 2 is now connecting to a different namespace.\nSystem.setProperty(\"ray.job.namespace\", \"fruits\");\nRay.init();\n// This fails because \"orange\" was defined in the \"colors\" namespace.\nOptional<ActorHandle<Actor>> actor = Ray.getActor(\"orange\");\nAssert.assertFalse(actor.isPresent());  // actor.isPresent() is false.\n\n// Driver3.java\nSystem.setProperty(\"ray.job.namespace\", \"colors\");\nRay.init();\n// This returns the \"orange\" actor we created in the first job.\nOptional<ActorHandle<Actor>> actor = Ray.getActor(\"orange\");\nAssert.assertTrue(actor.isPresent());  // actor.isPresent() is true.\n```\n\n----------------------------------------\n\nTITLE: Amazon EFS Setup Configuration\nDESCRIPTION: YAML configuration for setting up and mounting Amazon EFS in a Ray cluster\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsetup_commands:\n    - sudo kill -9 `sudo lsof /var/lib/dpkg/lock-frontend | awk '{print $2}' | tail -n 1`;\n        sudo pkill -9 apt-get;\n        sudo pkill -9 dpkg;\n        sudo dpkg --configure -a;\n        sudo apt-get -y install binutils;\n        cd $HOME;\n        git clone https://github.com/aws/efs-utils;\n        cd $HOME/efs-utils;\n        ./build-deb.sh;\n        sudo apt-get -y install ./build/amazon-efs-utils*deb;\n        cd $HOME;\n        mkdir efs;\n        sudo mount -t efs {{FileSystemId}}:/ efs;\n        sudo chmod 777 efs;\n```\n\n----------------------------------------\n\nTITLE: Defining ML Training Framework Dependencies for Ray Project\nDESCRIPTION: This snippet specifies the required versions of ML training frameworks used in the Ray project. It includes XGBoost and LightGBM, which are popular gradient boosting libraries for machine learning tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/core-requirements.txt#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# ML training frameworks\nxgboost==2.1.0\nlightgbm==3.3.5\n```\n\n----------------------------------------\n\nTITLE: Including Torch Regression Example Code\nDESCRIPTION: This snippet includes the content of a Python file focused on a PyTorch regression example meant for execution in Ray's framework. The purpose is to illustrate how to utilize Ray for distributed training with a regression model. The dependencies include PyTorch and Ray, which must be installed in the environment where the code is executed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_regression_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: /../../python/ray/train/examples/pytorch/torch_regression_example.py\n```\n\n----------------------------------------\n\nTITLE: Verifying Pod Status\nDESCRIPTION: Command to check the status of pods showing the preemption of lower priority jobs by the new high priority job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get pods\nNAME                                                      READY   STATUS    RESTARTS   AGE\nprod-pytorch-text-classifier-gkp9b-r9k5r                  1/1     Running   0          5s\ntorch-text-classifier-gkp9b-raycluster-s2f65-head-hfvht   1/1     Running   0          35s\n```\n\n----------------------------------------\n\nTITLE: LogQL Query for Viewing RayCluster Logs in Grafana\nDESCRIPTION: LogQL query to view logs for a specific RayCluster or RayJob in Grafana Loki. This query filters logs by the RayCluster name as set in the FluentBit ConfigMap.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n{RayCluster=\"raycluster-fluentbit-sidecar-logs\"}\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Object Store\nDESCRIPTION: Enhanced Ray task implementation using distributed object store for data sharing\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndb_object_ref = ray.put(database)\n\n\n@ray.remote\ndef retrieve_task(item, db):\n    time.sleep(item / 10.)\n    return item, db[item]\n```\n\n----------------------------------------\n\nTITLE: Rerunning Autoscaling Test\nDESCRIPTION: Command to rerun the autoscaling test without tearing down the operator, specifying the Ray image to use.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/kuberay/README.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nRAY_IMAGE=<registry>/<repo>:<tag> python test_autoscaling_e2e.py\n```\n\n----------------------------------------\n\nTITLE: UV Pip Compile Command\nDESCRIPTION: Command to compile Python package requirements with specific constraints and settings for Ray project\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cpu --find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c python/requirements_compiled_ray_test_py311_cpu.txt python/requirements.txt -o python/requirements_compiled_ray_py311_cpu.txt\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependency List with Hashes\nDESCRIPTION: Detailed list of Python package dependencies with their versions and SHA256 hashes for package verification and security. Includes dependencies for packages like jsonschema, referencing, google-auth, and transformers.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_33\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:1352ae4f7c717ae8cba93421a63373e582d19d55d2ee2cbb184344c82d2ae55a \\\n--hash=sha256:177c7c0fce2855833819c98e43c262007f42ce86651ffbb84f37883308cb0e7d \\\n# via\n#   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n#   jsonschema\n#   referencing\n```\n\n----------------------------------------\n\nTITLE: Creating a netrc File for Remote URI Authentication\nDESCRIPTION: Example content of a .netrc file used to store credentials for automatic authentication with remote servers like GitHub.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/runtime_env_auth.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# \"$HOME/.netrc\"\n\nmachine github.com\nlogin username\npassword personal_access_token\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with VertxHandler Channel Reading\nDESCRIPTION: This stack trace shows Netty's event processing flow reaching the Vert.x handler implementation. The trace illustrates how decoded messages are passed to application-specific handlers in the Vert.x framework after being processed by Netty's pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_44\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Deleting RayCluster Resources with kubectl-ray\nDESCRIPTION: Commands to delete RayCluster resources using the kubectl-ray plugin.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kubectl-plugin.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl ray delete raycluster-sample\nkubectl ray delete raycluster-sample-2\n```\n\n----------------------------------------\n\nTITLE: Printing Dataset Information in Ray\nDESCRIPTION: This code shows how to print detailed information about a Ray Dataset, including the number of rows and schema, by simply printing the dataset object.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/inspecting-data.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\nprint(ds)\n```\n\n----------------------------------------\n\nTITLE: Configuring Learners for GPU\nDESCRIPTION: This snippet shows how to configure the number of learners and GPUs per learner using the `learners()` method in the `AlgorithmConfig` object.  It demonstrates the equivalent configuration to the old stack's `config.resources(num_gpus=2)`.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"# The following setting is equivalent to the old stack's `config.resources(num_gpus=2)`.\nconfig.learners(\n    num_learners=2,\n    num_gpus_per_learner=1,\n)\"\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Ray Dashboard in Shell\nDESCRIPTION: This shell command shows how to use port forwarding to access the Ray Dashboard from outside a Kubernetes cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/configure-manage-dashboard.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl port-forward service/${RAYCLUSTER_NAME}-head-svc 8265:8265\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch Datasets for ResNet Training\nDESCRIPTION: Function to create PyTorch datasets for training and validation splits, applying the previously defined data transforms.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef build_datasets():\n    torch_datasets = {}\n    for split in [\"train\", \"val\"]:\n        torch_datasets[split] = datasets.ImageFolder(\n            os.path.join(\"./hymenoptera_data\", split), data_transforms[split]\n        )\n    return torch_datasets\n```\n\n----------------------------------------\n\nTITLE: Checking Modin RayJob Logs on Kubernetes\nDESCRIPTION: Command to view the logs from the Modin RayJob execution. The logs show the Ray cluster connection, Modin engine information, and computation time statistics for operations on the NYC Taxi Dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/modin-example.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl logs -l=job-name=rayjob-sample\n\n# [Example output]\n# 2024-07-05 10:01:00,945 INFO worker.py:1446 -- Using address 10.244.0.4:6379 set in the environment variable RAY_ADDRESS\n# 2024-07-05 10:01:00,945 INFO worker.py:1586 -- Connecting to existing Ray cluster at address: 10.244.0.4:6379...\n# 2024-07-05 10:01:00,948 INFO worker.py:1762 -- Connected to Ray cluster. View the dashboard at 10.244.0.4:8265\n# Modin Engine: Ray\n# FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n# Time to compute isnull: 0.065887747972738\n# Time to compute rounded_trip_distance: 0.34410698304418474\n# 2024-07-05 10:01:23,069 SUCC cli.py:60 -- -----------------------------------\n# 2024-07-05 10:01:23,069 SUCC cli.py:61 -- Job 'rayjob-sample-zt8wj' succeeded\n# 2024-07-05 10:01:23,069 SUCC cli.py:62 -- -----------------------------------\n```\n\n----------------------------------------\n\nTITLE: Verifying Python Package Integrity with SHA-256 Hashes\nDESCRIPTION: This snippet shows package dependency definitions with SHA-256 hash verification for security. The file appears to be part of a requirements file that ensures package integrity during installation with pip's hash verification feature.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nfsspec==2023.5.0 \\\n    --hash=sha256:51a4ad01a5bb66fcc58036e288c0d53d3975a0df2a5dc59a93b59bade0391f2a \\\n    --hash=sha256:b3b56e00fb93ea321bc9e5d9cf6f8522a0198b20eb24e02774d329e9c6fb84ce\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\ngoogle-api-core==1.34.0 \\\n    --hash=sha256:6fb380f49d19ee1d09a9722d0379042b7edb06c0112e4796c7a395078a043e71 \\\n    --hash=sha256:7421474c39d396a74dfa317dddbc69188f2336835f526087c7648f91105e32ff\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   opencensus\ngoogle-auth==2.23.4 \\\n    --hash=sha256:79905d6b1652187def79d491d6e23d0cbb3a21d3c7ba0dbaa9c8a01906b13ff3 \\\n    --hash=sha256:d4bbc92fe4b8bfd2f3e8d88e5ba7085935da208ee38a134fc280e7ce682a05f2\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   google-api-core\ngoogleapis-common-protos==1.61.0 \\\n    --hash=sha256:22f1915393bb3245343f6efe87f6fe868532efc12aa26b391b15132e1279f1c0 \\\n    --hash=sha256:8a64866a97f6304a7179873a465d6eee97b7a24ec6cfd78e0f575e96b821240b\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   google-api-core\n    #   opentelemetry-exporter-otlp-proto-grpc\ngrpcio==1.66.2 \\\n    --hash=sha256:02697eb4a5cbe5a9639f57323b4c37bcb3ab2d48cec5da3dc2f13334d72790dd \\\n    --hash=sha256:03b0b307ba26fae695e067b94cbb014e27390f8bc5ac7a3a39b7723fed085604 \\\n    --hash=sha256:05bc2ceadc2529ab0b227b1310d249d95d9001cd106aa4d31e8871ad3c428d73 \\\n    --hash=sha256:06de8ec0bd71be123eec15b0e0d457474931c2c407869b6c349bd9bed4adbac3 \\\n    --hash=sha256:0be4e0490c28da5377283861bed2941d1d20ec017ca397a5df4394d1c31a9b50 \\\n    --hash=sha256:12fda97ffae55e6526825daf25ad0fa37483685952b5d0f910d6405c87e3adb6 \\\n    --hash=sha256:1caa38fb22a8578ab8393da99d4b8641e3a80abc8fd52646f1ecc92bcb8dee34 \\\n    --hash=sha256:2018b053aa15782db2541ca01a7edb56a0bf18c77efed975392583725974b249 \\\n    --hash=sha256:20657d6b8cfed7db5e11b62ff7dfe2e12064ea78e93f1434d61888834bc86d75 \\\n    --hash=sha256:2335c58560a9e92ac58ff2bc5649952f9b37d0735608242973c7a8b94a6437d8 \\\n    --hash=sha256:31fd163105464797a72d901a06472860845ac157389e10f12631025b3e4d0453 \\\n    --hash=sha256:38b68498ff579a3b1ee8f93a05eb48dc2595795f2f62716e797dc24774c1aaa8 \\\n    --hash=sha256:3b00efc473b20d8bf83e0e1ae661b98951ca56111feb9b9611df8efc4fe5d55d \\\n    --hash=sha256:3ed71e81782966ffead60268bbda31ea3f725ebf8aa73634d5dda44f2cf3fb9c \\\n    --hash=sha256:45a3d462826f4868b442a6b8fdbe8b87b45eb4f5b5308168c156b21eca43f61c \\\n    --hash=sha256:49f0ca7ae850f59f828a723a9064cadbed90f1ece179d375966546499b8a2c9c \\\n    --hash=sha256:4e504572433f4e72b12394977679161d495c4c9581ba34a88d843eaf0f2fbd39 \\\n    --hash=sha256:4ea1d062c9230278793820146c95d038dc0f468cbdd172eec3363e42ff1c7d01 \\\n    --hash=sha256:563588c587b75c34b928bc428548e5b00ea38c46972181a4d8b75ba7e3f24231 \\\n    --hash=sha256:6001e575b8bbd89eee11960bb640b6da6ae110cf08113a075f1e2051cc596cae \\\n    --hash=sha256:66a0cd8ba6512b401d7ed46bb03f4ee455839957f28b8d61e7708056a806ba6a \\\n    --hash=sha256:6851de821249340bdb100df5eacfecfc4e6075fa85c6df7ee0eb213170ec8e5d \\\n    --hash=sha256:728bdf36a186e7f51da73be7f8d09457a03061be848718d0edf000e709418987 \\\n    --hash=sha256:73e3b425c1e155730273f73e419de3074aa5c5e936771ee0e4af0814631fb30a \\\n    --hash=sha256:73fc8f8b9b5c4a03e802b3cd0c18b2b06b410d3c1dcbef989fdeb943bd44aff7 \\\n    --hash=sha256:78fa51ebc2d9242c0fc5db0feecc57a9943303b46664ad89921f5079e2e4ada7 \\\n\n```\n\n----------------------------------------\n\nTITLE: Checking for Broken Links in Documentation\nDESCRIPTION: Command to scan the documentation for broken links. Note that this is not run in CI due to false positives.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/README.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake linkcheck\n```\n\n----------------------------------------\n\nTITLE: Executing Ray CLI Commands on KubeRay Cluster\nDESCRIPTION: Shows how to execute Ray CLI commands on a KubeRay cluster from outside the cluster using 'kubectl exec' and the RayCluster name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n# First, find the name of the Ray head service.\n$ kubectl get pod | grep <RayCluster name>-head\n# NAME                                             READY   STATUS    RESTARTS   AGE\n# <RayCluster name>-head-xxxxx                     2/2     Running   0          XXs\n\n# Then, use the name of the Ray head service to run `ray status`.\n$ kubectl exec <RayCluster name>-head-xxxxx -- ray status\n```\n\n----------------------------------------\n\nTITLE: Listing All Jobs using Ray Jobs REST API in Python\nDESCRIPTION: This code snippet shows how to list all jobs using the Ray Jobs REST API. It sends a GET request to the jobs endpoint and prints the JSON response containing job information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/rest.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresp = requests.get(\n    \"http://127.0.0.1:8265/api/jobs/\"\n)\nprint(resp.json())\n# {\"job_id\": {\"metadata\": ..., \"status\": ..., \"message\": ...}, ...}\n```\n\n----------------------------------------\n\nTITLE: Accessing Result Error in Ray Train\nDESCRIPTION: This snippet demonstrates how to access the error that occurred during training from a Ray Train `Result` object. The `result.error` attribute will contain the exception that was raised, if any error occurred during training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/results.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Access the error that occurred during training.\"\"\"\n# Get the error that occurred during training.\nerror = result.error\nprint(f\"Error: {error}\")\n\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up KubeRay Resources\nDESCRIPTION: Commands to remove the sample Ray cluster and uninstall the KubeRay operator after testing the py-spy profiling functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/pyspy.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.py-spy.yaml\nhelm uninstall kuberay-operator\n```\n\n----------------------------------------\n\nTITLE: Building Ray Docker Image\nDESCRIPTION: Navigates to the ray directory and executes the build-docker.sh script.  This script builds a Ray Docker image locally, allowing for customization and development.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n\"cd ray\\n./build-docker.sh\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Log Deduplication\nDESCRIPTION: This snippet shows how to set an environment variable to specify exceptions to Rays log deduplication feature. It illustrates the use of a regex pattern to allow specific log messages to bypass deduplication. Outputs show specific messages allowed multiple times due to regex configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"RAY_DEDUP_LOGS_ALLOW_REGEX\"] = \"ABC\"\n\nimport ray\n\n@ray.remote\ndef f():\n    print(\"ABC\")\n    print(\"DEF\")\n\nray.init()\nray.get([f.remote() for _ in range(5)])\n```\n\n----------------------------------------\n\nTITLE: Known Bugs and Limitations of Dynamic Generators in Ray\nDESCRIPTION: In this snippet, known issues with the exceptional handling of generated values are discussed, including scenarios leading to non-propagating exceptions when tasks yield unexpected amounts of values. Potential solutions are suggested along with recommendations for using max_retries.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tasks/generators.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../doc_code/generator.py\n    :language: python\n    :start-after: __generator_errors_unsupported_start__\n    :end-before: __generator_errors_unsupported_end__\n```\n\n----------------------------------------\n\nTITLE: Installing vSphere Automation SDK\nDESCRIPTION: Commands to install the VMware vSphere Automation SDK for Python, including options for specific version installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/vsphere.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install the VMware vSphere Automation SDK for Python.\npip install 'git+https://github.com/vmware/vsphere-automation-sdk-python.git'\n\n# Install the v8.0.1.0 version of the SDK.\npip install 'git+https://github.com/vmware/vsphere-automation-sdk-python.git@v8.0.1.0'\n```\n\n----------------------------------------\n\nTITLE: Defining Pandocfilters Dependency\nDESCRIPTION: This snippet specifies the Pandocfilters library dependency with hash verification. It's used by nbconvert for Jupyter notebook conversion functionality and is referenced from the requirements_compiled.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\npandocfilters==1.5.0 \\\n    --hash=sha256:0b679503337d233b4339a817bfc8c50064e2eff681314376a47cb582305a7a38 \\\n    --hash=sha256:33aae3f25fd1a026079f5d27bdd52496f0e0803b3469282162bafdcbdf6ef14f\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   nbconvert\n```\n\n----------------------------------------\n\nTITLE: Creating an IAM Service Account\nDESCRIPTION: Creates a Google Cloud IAM service account that will be used to access Google Cloud resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngcloud iam service-accounts create my-iam-sa\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenAI API Package with Hash Verification\nDESCRIPTION: Dependency specification for the OpenAI Python client library with specific version and hash verification. This package is required for integrating with OpenAI's API services for the VLLM component.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_18\n\nLANGUAGE: pip\nCODE:\n```\nopenai==1.63.2 \\\n    --hash=sha256:1f38b27b5a40814c2b7d8759ec78110df58c4a614c25f182809ca52b080ff4d4 \\\n    --hash=sha256:aeabeec984a7d2957b4928ceaa339e2ead19c61cfcf35ae62b7c363368d26360\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow Dependencies\nDESCRIPTION: Specifies TensorFlow and related package versions with platform-specific constraints. Includes special handling for macOS ARM64 architecture and Python version compatibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/dl-cpu-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\ntensorflow==2.15.1; python_version < '3.12' and (sys_platform != 'darwin' or platform_machine != 'arm64')\ntensorflow-macos==2.15.1; python_version < '3.12' and sys_platform == 'darwin' and platform_machine == 'arm64'\ntensorflow-probability==0.23.0; python_version < '3.12'\ntensorflow-io-gcs-filesystem==0.31.0; python_version < '3.12'\ntensorflow-datasets; python_version < '3.12'\narray-record==0.5.1; python_version < '3.12' and sys_platform != 'darwin' and platform_system != 'Windows'\netils==1.5.2; python_version < '3.12'\n```\n\n----------------------------------------\n\nTITLE: Listing Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how package dependencies are specified with version numbers and SHA256 hashes. It includes multiple hashes for each package to ensure integrity across different platforms and builds.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nclick==8.1.7 \\\n    --hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28 \\\n    --hash=sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   dask\n    #   distributed\n    #   flask\n    #   typer\ncloudpickle==2.2.0 \\\n    --hash=sha256:3f4219469c55453cfe4737e564b67c2a149109dabf7f242478948b895f61106f \\\n    --hash=sha256:7428798d5926d8fcbfd092d18d01a2a03daf8237d8fcdc8095d256b8490796f0\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   dask\n    #   distributed\n    #   gymnasium\ncmake==3.28.1 \\\n    --hash=sha256:0d4051d101d151d8387156c463aa45c8cd0e164f870e0ac0c8c91d3ff08528e1 \\\n    --hash=sha256:1be8f351271f8bcbe32288066e5add642d7c32f2f8fec3f135949c2cb13dfac2 \\\n    --hash=sha256:2ad22d897d2ed38544e5ef26ee21c4dccc38e938660cd07497fd6bdba0993ea6\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify Python package dependencies with their versions and hash values for secure installation. It includes entries for pyyaml, referencing, requests, rich, and rpds-py packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_22\n\nLANGUAGE: Text\nCODE:\n```\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    # ... (additional hashes omitted for brevity)\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\nreferencing==0.36.2 \\\n    --hash=sha256:df2e89862cd09deabbdba16944cc3f10feb6b3e6f18e902f7cc25609a34775aa \\\n    --hash=sha256:e8699adbbf8b5c7de96d8ffa0eb5c158b3beafce084968e2ea8bb08c6794dcd0\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jsonschema\n    #   jsonschema-specifications\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   google-api-core\nrich==13.3.2 \\\n    --hash=sha256:91954fe80cfb7985727a467ca98a7618e5dd15178cc2da10f553b36a93859001 \\\n    --hash=sha256:a104f37270bf677148d8acb07d33be1569eeee87e2d1beb286a4e9113caf6f2f\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   memray\n    #   typer\nrpds-py==0.22.3 \\\n    --hash=sha256:009de23c9c9ee54bf11303a966edf4d9087cd43a6003672e6aa7def643d06518 \\\n    --hash=sha256:02fbb9c288ae08bcb34fb41d516d5eeb0455ac35b5512d03181d755d80810059 \\\n    # ... (additional hashes omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Configuring Kueue Resources for Ray Cluster Scheduling\nDESCRIPTION: YAML configuration for Kueue resources including ResourceFlavor, ClusterQueue with preemption policy, LocalQueue, and two WorkloadPriorityClasses for production and development workloads.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# kueue-resources.yaml\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"default-flavor\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue\"\nspec:\n  preemption:\n    withinClusterQueue: LowerPriority\n  namespaceSelector: {} # Match all namespaces.\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\"]\n    flavors:\n    - name: \"default-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 3\n      - name: \"memory\"\n        nominalQuota: 6G\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: \"default\"\n  name: \"user-queue\"\nspec:\n  clusterQueue: \"cluster-queue\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: WorkloadPriorityClass\nmetadata:\n  name: prod-priority\nvalue: 1000\ndescription: \"Priority class for prod jobs\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: WorkloadPriorityClass\nmetadata:\n  name: dev-priority\nvalue: 100\ndescription: \"Priority class for development jobs\"\n```\n\n----------------------------------------\n\nTITLE: Managing Completed RayJob Instances\nDESCRIPTION: Commands to check the status of RayJob resources and delete them after completion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get rayjobs.ray.io\nNAME                  JOB STATUS   DEPLOYMENT STATUS   START TIME             END TIME               AGE\nrayjob-sample-ckvq4   SUCCEEDED    Complete            xxxxx                  xxxxx                  xxx\nrayjob-sample-p5msp   SUCCEEDED    Complete            xxxxx                  xxxxx                  xxx\n\n$ kubectl delete rayjob rayjob-sample-ckvq4\n$ kubectl delete rayjob rayjob-sample-p5msp\n```\n\n----------------------------------------\n\nTITLE: Tokenizing the Shakespeare Dataset\nDESCRIPTION: Processes the split sentences by tokenizing them using the model's tokenizer. This converts the text into token IDs that can be used as input for the language model during fine-tuning.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Then tokenize the dataset.\ntrain_ds = train_ds.map_batches(tokenize, batch_format=\"pandas\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Grafana Admin Password\nDESCRIPTION: Command to retrieve the Grafana admin password from Kubernetes secret.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-operator-logs.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get secret --namespace default grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n```\n\n----------------------------------------\n\nTITLE: Defining Core Dependencies for Ray Project in Python\nDESCRIPTION: This code snippet lists the core dependencies required for the Ray project. It includes packages for various functionalities such as JSON schema validation, messaging, and file watching.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclick>=7.0\ncupy-cuda12x; sys_platform != 'darwin'\nfilelock\njsonschema\nmsgpack<2.0.0,>=1.0.0\npackaging\nprotobuf!=3.19.5,>=3.15.3\npyyaml\nrequests\nwatchfiles\n```\n\n----------------------------------------\n\nTITLE: Defining pycparser Dependency\nDESCRIPTION: This snippet defines the pycparser package dependency with version 2.21 and hash verification. The pycparser package is a parser for the C language implemented in Python, used as a dependency for various cryptographic and security-related functions in Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_34\n\nLANGUAGE: pip\nCODE:\n```\npycparser==2.21 \\\n    --hash=sha256:8ee45429555515e1f6b185e78100aea234072576aa43ab53aefcae078162fca9\n```\n\n----------------------------------------\n\nTITLE: Implementing Load Task in Airflow TaskFlow API\nDESCRIPTION: Defines a load task that takes the transformed data and prints the total order value formatted to two decimal places.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/workflow/examples/comparisons/airflow/etl_airflow.py.txt#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@task()\ndef load(total_order_value: float):\n    \"\"\"\n    #### Load task\n    A simple Load task which takes in the result of the Transform task and\n    instead of saving it to end user review, just prints it out.\n    \"\"\"\n\n    print(f\"Total order value is: {total_order_value:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Documenting Ray Disable Usage Stats Command in reStructuredText\nDESCRIPTION: This snippet uses the click directive to generate documentation for the 'ray disable-usage-stats' command from the ray.scripts.scripts module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/cli.rst#2025-04-12_snippet_5\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. click:: ray.scripts.scripts:disable_usage_stats\n   :prog: ray disable-usage-stats\n   :show-nested:\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenCensus Dependencies\nDESCRIPTION: Defines the OpenCensus packages required by the Ray project with exact version and hash verification. Includes both the main package and the context package with their dependencies listed in comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nopencensus==0.11.3 \\\n    --hash=sha256:9c33d572059f0f0e874fc34c697a39a4193aa9cf3203f7e777df42e9edeea56a \\\n    --hash=sha256:af7a98bd51e63968144d772f346d696ed498a32dbdc4be267cd6011c4ce05da8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\nopencensus-context==0.1.3 \\\n    --hash=sha256:073bb0590007af276853009fac7e4bab1d523c3f03baf4cb4511ca38967c6039 \\\n    --hash=sha256:a03108c3c10d8c80bb5ddf5c8a1f033161fa61972a9917f9b9b3a18517f0088c\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   opencensus\n```\n\n----------------------------------------\n\nTITLE: Accessing Relative Filepaths in Tune Training Functions with Python\nDESCRIPTION: Shows how to access relative filepaths in Tune training functions by setting the RAY_CHDIR_TO_TRIAL_DIR environment variable and using the tune.get_context() API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom ray import tune\n\nos.environ[\"RAY_CHDIR_TO_TRIAL_DIR\"] = \"0\"\n\ndef train(config):\n    # Access files relative to the original working directory\n    with open(\"my_file.txt\", \"r\") as f:\n        data = f.read()\n\n    # Use the trial dir for saving outputs\n    output_path = tune.get_context().get_trial_dir()\n    with open(os.path.join(output_path, \"output.txt\"), \"w\") as f:\n        f.write(\"Hello, World!\")\n\ntune.run(\n    train,\n    config={\"param\": tune.grid_search([1, 2, 3])}\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies\nDESCRIPTION: Command to install the required dependencies for building Ray documentation using pip.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements-doc.txt\n```\n\n----------------------------------------\n\nTITLE: Submitting Ray Job to Cluster\nDESCRIPTION: Submits a Ray job to the cluster using the Ray CLI, which prints the cluster resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n! ray job submit --address http://localhost:8265 -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n```\n\n----------------------------------------\n\nTITLE: Using Ray Actors in Tasks (Python)\nDESCRIPTION: Demonstrates how to incorporate a Ray actor into a task function and use it to track state across multiple task executions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef retrieve_tracker_task(item, tracker, db):\n    time.sleep(item / 10.)\n    tracker.increment.remote()\n    return item, db[item]\n\n\ntracker = DataTracker.remote()\n\nobject_references = [\n    retrieve_tracker_task.remote(item, tracker, db_object_ref) for item in range(8)\n]\ndata = ray.get(object_references)\n\nprint(data)\nprint(ray.get(tracker.counts.remote()))\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Result Buffer Time in Ray Tune\nDESCRIPTION: TUNE_RESULT_BUFFER_MAX_TIME_S sets the maximum time (in seconds) to buffer results. Defaults to 100 seconds.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_RESULT_BUFFER_MAX_TIME_S=100\n```\n\n----------------------------------------\n\nTITLE: Ray Performance Metrics Output\nDESCRIPTION: Performance timing results for different Ray operations: creation of 10000 actors, execution of 10000 tasks, creation of 1000 placement groups, and launching of 250 nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/scalability/distributed.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nActor time: 37.22384708500087 (10000 actors)\nTask time: 403.85309777899885 (10000 tasks)\nPG time: 30.63472470399972 (1000 placement groups)\nNode launch time: 651.0706797980001 (250 nodes)\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Cluster on GCP\nDESCRIPTION: Shell commands to create a Kubernetes cluster on Google Cloud Platform (GCP) using Google Kubernetes Engine (GKE). Includes options for standard and autopilot cluster creation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/ml-example.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Set up a cluster on Google Kubernetes Engine (GKE)\ngcloud container clusters create autoscaler-ray-cluster \\\n    --num-nodes=10 --zone=us-central1-c --machine-type e2-standard-16 --disk-size 1000GB\n\n# (Optional) Set up a cluster with autopilot on Google Kubernetes Engine (GKE).\n# The following command creates an autoscaling node pool with a 1 node minimum and a 10 node maximum.\n# The 1 static node will be used to run the Ray head pod. This node may also host the KubeRay\n# operator and Kubernetes system components. After the workload is submitted, 9 additional nodes will\n# scale up to accommodate Ray worker pods. These nodes will scale back down after the workload is complete.\ngcloud container clusters create autoscaler-ray-cluster \\\n    --num-nodes=1 --min-nodes 1 --max-nodes 10 --enable-autoscaling \\\n    --zone=us-central1-c --machine-type e2-standard-16 --disk-size 1000GB\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Checkpoint Interval in Ray Tune\nDESCRIPTION: TUNE_GLOBAL_CHECKPOINT_S sets how often (in seconds) the experiment state is checkpointed. Can be set to a fixed value or 'auto' which adapts based on snapshot time.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_GLOBAL_CHECKPOINT_S=60\n```\n\n----------------------------------------\n\nTITLE: Ray Performance Test Configuration JSON\nDESCRIPTION: JSON configuration containing test metrics including broadcast time (611ms), object size (1GB), node count (50), and environment details including session URL and commit reference\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/scalability/object_store.txt#2025-04-12_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"broadcast_time\": 611.015479593,\n  \"object_size\": 1073741824,\n  \"num_nodes\": 50,\n  \"success\": \"1\",\n  \"_runtime\": 620.4363269805908,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_Chj4PHZqrEjbzc8Ni4RY1Fev\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Ray Buildkite Requirements with Bazel\nDESCRIPTION: Command to update the Buildkite requirements file using Bazel. This ensures that the dependencies are kept up-to-date for the CI pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nbazel run //release:requirements_buildkite.update\n```\n\n----------------------------------------\n\nTITLE: Starting a Fake Ray Cluster for Autoscaling Tests\nDESCRIPTION: Commands to stop any existing Ray processes, then start a fake Ray cluster using a specified autoscaling configuration file. This allows testing of autoscaling behavior locally.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/fake-autoscaler.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ ray stop --force\n$ RAY_FAKE_CLUSTER=1 ray start \\\n    --autoscaling-config=./python/ray/autoscaler/_private/fake_multi_node/example.yaml \\\n    --head --block\n```\n\n----------------------------------------\n\nTITLE: Specifying Search Algorithm Dependencies for Ray\nDESCRIPTION: A requirements file that lists all the necessary Python packages for different search algorithms in Ray. It includes version constraints and conditional dependencies based on Python version compatibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/tune-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# Searchers\nax-platform==0.3.2\n\nbayesian-optimization==1.4.3\n\n# BOHB\nConfigSpace==0.7.1; python_version < \"3.12\"\nhpbandster==0.7.4; python_version < \"3.12\"\n\nhyperopt @ git+https://github.com/hyperopt/hyperopt.git@2504ee61419737e814e2dec2961b15d12775529c\nfuture\nnevergrad==0.4.3.post7\noptuna==4.1.0\n```\n\n----------------------------------------\n\nTITLE: List Alive Actors\nDESCRIPTION: Command to list all alive actors in the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nray list actors -f state=ALIVE\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import list_actors\nlist_actors(filters=[(\"state\", \"=\", \"ALIVE\")])\n```\n\n----------------------------------------\n\nTITLE: Matching Ray Autoscaler Status Output with Regex\nDESCRIPTION: A regular expression pattern designed to match the output from Ray's autoscaler status command. The pattern captures sections for node status (active, idle, pending, failures) and resource usage information with proper escaping for special characters.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_status.txt#2025-04-12_snippet_0\n\nLANGUAGE: regex\nCODE:\n```\n======== Autoscaler status: .+\nNode status\n---------------------------------------------------------------\nActive:\n \\(no active nodes\\)\nIdle:\n 1 node_.+\nPending:\n \\(no pending nodes\\)\nRecent failures:\n \\(no failures\\)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/3.0 CPU\n 0.+\n 0.+\n\nDemands:\n \\(no resource demands\\)\n```\n\n----------------------------------------\n\nTITLE: Setting Warning Threshold for Insufficient Resources\nDESCRIPTION: TUNE_WARN_INSUFFICENT_RESOURCE_THRESHOLD_S sets the time threshold (in seconds) before warning about no active trials in RUNNING state, likely due to insufficient resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_WARN_INSUFFICENT_RESOURCE_THRESHOLD_S=60\n```\n\n----------------------------------------\n\nTITLE: Ray Network Processing with Xen-specific Time Retrieval\nDESCRIPTION: This stack trace extends the previous one by including Xen-specific operations for retrieving the current time. It shows how Ray's network processing interacts with virtualization-specific time management on Xen-based systems.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_22\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];tcp_v4_do_rcv_[k];tcp_rcv_established_[k];tcp_ack_[k];tcp_clean_rtx_queue_[k];ktime_get_real_[k];getnstimeofday_[k];xen_clocksource_get_cycles_[k];xen_clocksource_read_[k]\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Batches with TensorFlow in Ray Data\nDESCRIPTION: Demonstrates how to convert a Ray Dataset to a TensorFlow dataset and iterate over batches using Ray Data's to_tf() method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/iterating-over-data.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\ntf_dataset = ds.to_tf(\n    feature_columns=\"sepal length (cm)\",\n    label_columns=\"target\",\n    batch_size=2\n)\nfor features, labels in tf_dataset:\n    print(features, labels)\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Dashboard with pip\nDESCRIPTION: Command to install Ray with the dashboard component included using pip. This is required to access the dashboard functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/getting-started.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"ray[default]\"\n```\n\n----------------------------------------\n\nTITLE: Package Hash List Format\nDESCRIPTION: Dependencies list showing package versions and their corresponding SHA256 hashes for verification purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_5\n\nLANGUAGE: txt\nCODE:\n```\n--hash=sha256:068b63f23b17df8569b7fdca5517edef76171cf3897eb68beb01341131fbd2ad \\\n--hash=sha256:0c250a29735d4f15321007fb02865f0e6b6a41a6b88f1f523ca1596ab5f50bd5 \\\n--hash=sha256:1979bc0aeb89b33b588c51c54ab0161791149f2461ea7c7c946d95d5f93b56ae\n```\n\n----------------------------------------\n\nTITLE: Disabling IPython Caching for Ray Memory Management\nDESCRIPTION: Command to disable IPython caching, preventing Jupyter from storing cell outputs indefinitely which can cause Ray to pin memory objects unnecessarily. This helps reduce memory usage when running Ray in notebooks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/using-ray-with-jupyter.rst#2025-04-12_snippet_0\n\nLANGUAGE: console\nCODE:\n```\necho 'c = get_config()\\nc.InteractiveShell.cache_size = 0 # disable cache\\n' >>  ~/.ipython/profile_default/ipython_config.py\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements List\nDESCRIPTION: List of Python package dependencies with version constraints and source annotations. Each entry specifies a package name, version requirement, and which parent packages depend on it.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_compiled.txt#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ndnspython==2.4.2\ndocker==6.1.3\ndocker-pycreds==0.4.0\ndocstring-parser==0.15\ndocutils==0.19\ndulwich==0.21.6\necdsa==0.18.0\nentrypoints==0.4\net-xmlfile==1.1.0\netils==1.5.2\nevaluate==0.4.3\neverett==3.1.0\nexecnet==2.1.1\nexecuting==2.0.1\nface==22.0.0\nfairscale==0.4.6\nfarama-notifications==0.0.4\nfastapi==0.115.0\nfastavro==1.9.4\nfasteners==0.19\nfastjsonschema==2.19.0\nfastrlock==0.8.2\n```\n\n----------------------------------------\n\nTITLE: Creating GPU Node Pool in GKE for Ray Batch Inference\nDESCRIPTION: Command to create a GKE node pool with four Nvidia T4 GPUs, using the n1-standard-64 machine type which provides 64 vCPUs and 240 GB RAM. The command enables autoscaling with a minimum of 0 nodes and maximum of 1 node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-batch-inference-example.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-tesla-t4,count=4,gpu-driver-version=default \\\n  --zone us-west1-b \\\n  --cluster kuberay-gpu-cluster \\\n  --num-nodes 1 \\\n  --min-nodes 0 \\\n  --max-nodes 1 \\\n  --enable-autoscaling \\\n  --machine-type n1-standard-64\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Algorithm-Specific Config Objects in Python\nDESCRIPTION: Demonstrates importing and configuring an algorithm-specific config class (IMPALA) with environment and learning rate settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.impala import IMPALAConfig\n\nconfig = (\n    # Create an `IMPALAConfig` instance.\n    IMPALAConfig()\n    # Specify the RL environment.\n    .environment(\"CartPole-v1\")\n    # Change the learning rate.\n    .training(lr=0.0004)\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Grafana Server with Ray Configurations\nDESCRIPTION: Shell command to start a Grafana server using Ray's built-in configuration files located in the metrics folder of the latest Ray session.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/metrics.md#2025-04-12_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./bin/grafana-server --config /tmp/ray/session_latest/metrics/grafana/grafana.ini web\n```\n\n----------------------------------------\n\nTITLE: Log Rotation Configuration\nDESCRIPTION: Example of configuring log rotation settings using environment variables.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/configure-logging.md#2025-04-12_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nRAY_ROTATION_MAX_BYTES=1024; ray start --head # Start a ray instance with maxBytes 1KB.\nRAY_ROTATION_BACKUP_COUNT=1; ray start --head # Start a ray instance with backupCount 1.\n```\n\n----------------------------------------\n\nTITLE: Rendering Class Documentation with Sphinx and Jinja Templates\nDESCRIPTION: This template defines the documentation structure for a Python class in Sphinx. It includes directives to display the class name as a section header, set the current module context, and render the class documentation with all its members and inheritance information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/autosummary/class_without_autosummary_noindex.rst#2025-04-12_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n    :members:\n    :noindex:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Table of Contents for Ray Documentation\nDESCRIPTION: ReStructuredText directive that creates a table of contents linking to deployment guides for different cloud platforms. Uses toctree with maxdepth of 2 to organize the documentation structure.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    aws.md\n    gcp.md\n    azure.md\n    vsphere.md\n    on-premises.md\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify exact versions and hash values for Python package dependencies. It includes entries for pyyaml, referencing, requests, rich, and rpds-py, along with their respective version numbers and SHA256 hashes.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_24\n\nLANGUAGE: Text\nCODE:\n```\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    # ... (additional hashes omitted for brevity)\n\nreferencing==0.36.2 \\\n    --hash=sha256:df2e89862cd09deabbdba16944cc3f10feb6b3e6f18e902f7cc25609a34775aa \\\n    --hash=sha256:e8699adbbf8b5c7de96d8ffa0eb5c158b3beafce084968e2ea8bb08c6794dcd0\n\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n\nrich==13.3.2 \\\n    --hash=sha256:91954fe80cfb7985727a467ca98a7618e5dd15178cc2da10f553b36a93859001 \\\n    --hash=sha256:a104f37270bf677148d8acb07d33be1569eeee87e2d1beb286a4e9113caf6f2f\n\nrpds-py==0.22.3 \\\n    --hash=sha256:009de23c9c9ee54bf11303a966edf4d9087cd43a6003672e6aa7def643d06518 \\\n    --hash=sha256:02fbb9c288ae08bcb34fb41d516d5eeb0455ac35b5512d03181d755d80810059 \\\n    # ... (additional hashes omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Configuring Idle Timeout\nDESCRIPTION: Sets how long (in minutes) a worker node must be idle before removal. Idle means no active tasks, actors, or referenced objects. Defaults to 5 minutes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/configuring-autoscaling.rst#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nidle_timeout_minutes: 5\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Sources for Ray Project\nDESCRIPTION: Configuration of package sources including PyPI, PyTorch, and PyG (PyTorch Geometric) for the Ray project dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_2\n\nLANGUAGE: Pip\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cpu\n--find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay and Setting Up ALB Ingress on AWS EKS\nDESCRIPTION: Bash commands for installing KubeRay operator, deploying a RayCluster, and configuring ALB Ingress on AWS EKS. Includes steps for checking and managing the ingress setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Step 1: Install KubeRay operator and CRD\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.3.0\n\n# Step 2: Install a RayCluster\nhelm install raycluster kuberay/ray-cluster --version 1.3.0\n\n# Step 3: Edit the `ray-operator/config/samples/ray-cluster-alb-ingress.yaml`\n#\n# (1) Annotation `alb.ingress.kubernetes.io/subnets`\n#   1. Please include at least two subnets.\n#   2. One Availability Zone (ex: us-west-2a) can only have at most 1 subnet.\n#   3. In this example, you need to select public subnets (subnets that \"Auto-assign public IPv4 address\" is Yes on AWS dashboard)\n#\n# (2) Set the name of head pod service to `spec...backend.service.name`\neksctl get cluster ${YOUR_EKS_CLUSTER} # Check subnets on the EKS cluster\n\n# Step 4: Check ingress created by Step 4.\nkubectl describe ingress ray-cluster-ingress\n\n# Step 6: Check ALB on AWS (EC2 -> Load Balancing -> Load Balancers)\n#        The name of the ALB should be like \"k8s-default-rayclust-......\".\n\n# Step 7: Check Ray Dashboard by ALB DNS Name. The name of the DNS Name should be like\n#        \"k8s-default-rayclust-.....us-west-2.elb.amazonaws.com\"\n\n# Step 8: Delete the ingress, and AWS Load Balancer controller will remove ALB.\n#        Check ALB on AWS to make sure it is removed.\nkubectl delete ingress ray-cluster-ingress\n```\n\n----------------------------------------\n\nTITLE: Checking KubeRay Operator Logs\nDESCRIPTION: Command to retrieve and save logs from the KubeRay operator pod for error analysis.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/observability.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n```\n\n----------------------------------------\n\nTITLE: Defining Image Preprocessing Logic\nDESCRIPTION: Creates a function to preprocess images, applying transformations like tensor conversion and random horizontal flipping.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/torch_detection.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\nfrom torchvision import transforms\n\ndef preprocess_image(row: Dict[str, Any]) -> Dict[str, Any]:\n    transform = transforms.Compose([transforms.ToTensor(), transforms.RandomHorizontalFlip(p=0.5)])\n    row[\"image\"] = transform(row[\"image\"])\n    return row\n    \n# The following transform operation is lazy.\n# It will be re-run every epoch.\ntrain_dataset = train_dataset.map(preprocess_image)\n```\n\n----------------------------------------\n\nTITLE: Deploying RayCluster with YuniKorn Gang Scheduling\nDESCRIPTION: Shell commands to download and apply a RayCluster configuration file with YuniKorn gang scheduling settings. The configuration sets necessary labels for YuniKorn's gang scheduling feature.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Path: kuberay/ray-operator/config/samples\n# Configure the necessary labels on the RayCluster custom resource for Apache YuniKorn scheduler's gang scheduling:\n# - `ray.io/gang-scheduling-enabled`: Set to `true` to enable gang scheduling.\n# - `yunikorn.apache.org/app-id`: Set to a unique identifier for the application in Kubernetes, even across different namespaces.\n# - `yunikorn.apache.org/queue`: Set to the name of one of the queues in Apache YuniKorn.\nwget https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.yunikorn-scheduler.yaml\nkubectl apply -f ray-cluster.yunikorn-scheduler.yaml\n```\n\n----------------------------------------\n\nTITLE: Ray Project Comment Header\nDESCRIPTION: Comment header indicating the file was autogenerated using uv tool\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# This file was autogenerated by uv via the following command:\n```\n\n----------------------------------------\n\nTITLE: Platform-specific Package Dependency for xgrammar\nDESCRIPTION: Defines a platform-specific dependency for the xgrammar package, limited to aarch64 or x86_64 architectures, with multiple SHA256 hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_39\n\nLANGUAGE: text\nCODE:\n```\nxgrammar==0.1.16 ; platform_machine == 'aarch64' or platform_machine == 'x86_64' \\\n    --hash=sha256:027c6937748d22b1c2db2850c99e3cdca6b9532817ad2013b6fb646f07cc8448 \\\n    --hash=sha256:04e361b22926f431ae82fad3c4463e0d3c8f653fe15ebe3d7059edf73e348565 \\\n    --hash=sha256:19807de8dbe3772f8b6aaa979541183fe6c6bafa3680d52d8289c7b4439eff2c \\\n    --hash=sha256:1d898e3dc04ea7d81a0e9cd10b632c22707fcc9ce02d7be3c0aa6c38067af97f \\\n    --hash=sha256:2301413a374f11add07843dfb49050f27beae89a4be7e0ffd454c08cf302412c \\\n    --hash=sha256:23d016b09b22ad77a0cc7de49e2a7152d8cd221734aa6d41b5fd7827dfb1a4d3 \\\n    --hash=sha256:2d75e6501f55368462b4d61ce0fb6a65c587782faa7319f48f49a8c444b4245f \\\n    --hash=sha256:46e52514479056418495d68413c2ea18798b95dcdc36d25f48b281ca7d203ce1 \\\n    --hash=sha256:4ddd5128a82d0a9c800c03df25c610368ca630704ad20a6bb7a3629f24ced442 \\\n    --hash=sha256:51565f8e6eb17fefe7ce90aa4598cf8216b4ee801a33d58d8439242d3d18cfa6 \\\n    --hash=sha256:54a3d4386b538fe0a6b6399de2592dd57756e31c1def812cf9653b8f91f827d8 \\\n    --hash=sha256:5bf4754f53c7d3fb1a5b011d8948e90c6cb022caff73b0f74cc70cb319ec728a \\\n    --hash=sha256:60967ad8435448c183ad911c9c5252e5cb0b032b37f86dcfc16cdd07c35954f6 \\\n    --hash=sha256:773ae9b9d34456a3ade17d2c4e363eb149419d7f84bb014921c223290854aaf3 \\\n    --hash=sha256:854e2b23d0099c590cbc8bb83ab7de7d7ba3acb8aab65d64fa1436af0639f80c \\\n    --hash=sha256:90fae6c9256753f9816aacddf8c37176eded8b4164024d28d6342ea4b9182ae9 \\\n    --hash=sha256:97322341c29185b31482459325160dc2fb3eeb99bdf52cfeb57ae61a7e76c9d1 \\\n    --hash=sha256:ab1850ffb1615c1370e4ba3d4dafb2c116a03a06683b9fcf309982c49b8c2f87 \\\n    --hash=sha256:ae561d74bcfacfe96970e3ec847cdeeda7fe2cb3ad38ff44ad370de75cef5615 \\\n    --hash=sha256:bd151867c7007c1af27c901d3fd9dd178e41468775b782e083d0d125228a915f \\\n    --hash=sha256:cc0a1859332167c29a30ccf65c12bc56c3e4524244ac36df04c126a6d7711959 \\\n    --hash=sha256:d3c4fbc944bc2c0529da3efe0c5accab20df6c99aef7adfd17e3d0fecd10a80a \\\n    --hash=sha256:eb381bc5a1b8f17477700447a6cc676f22e91cc54a96f45dabe803f7fb0aec4d \\\n    --hash=sha256:eb59626839181af5918b7f82d179dd50501383c3d4d0d3ebf30f4d8be81485f6\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Cluster Max Workers\nDESCRIPTION: Sets the maximum number of worker nodes that can be launched in the cluster, excluding the head node. Must be greater than or equal to 0, defaults to 2.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/configuring-autoscaling.rst#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmax_workers: 2\n```\n\n----------------------------------------\n\nTITLE: Creating Kind Cluster\nDESCRIPTION: Command to create a local Kubernetes cluster using Kind.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/istio.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkind create cluster\n```\n\n----------------------------------------\n\nTITLE: Configuration Settings for Image Generation\nDESCRIPTION: Defines configuration parameters for image generation including number of images and image size.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/start.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nNUM_IMAGES_PER_PROMPT: int = NUM_REPLICAS\n\n# Control the output size: (IMAGE_SIZE, IMAGE_SIZE)\n# The stable diffusion model requires `IMAGE_SIZE` to be a multiple of 8.\n# NOTE: Generated image quality degrades rapidly if you reduce the size too much.\nIMAGE_SIZE: int = 640\n\nINTERACTIVE: bool = False\nPROMPT = \"twin peaks sf in basquiat painting style\"\n```\n\n----------------------------------------\n\nTITLE: Running KubeRay Autoscaling Test Script\nDESCRIPTION: Executes the KubeRay autoscaling test using a bash script. This script automates the process of setting up the environment, building and loading the Docker image, and running the test.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/kuberay/README.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./rune2e.sh\n```\n\n----------------------------------------\n\nTITLE: Checking Running Pods After Provisioning\nDESCRIPTION: Shows how to verify the running status of Ray cluster pods after successful GPU provisioning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get pods\nNAME                                                      READY   STATUS    RESTARTS        AGE\npytorch-text-classifier-nv77q-g6z57                       1/1     Running   0               13s\ntorch-text-classifier-nv77q-raycluster-gstrk-head-phnfl   1/1     Running   0               6m43s\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Head Node with Custom IP\nDESCRIPTION: Command to start a Ray head node with a specified external IP address and port for worker node connections.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/faq.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nray start --head --node-ip-address xx.xx.xx.xx --port nnnn\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Ray Project\nDESCRIPTION: This snippet lists the required Python packages for the Ray project. It includes version constraints for gym and black, and specifies other essential libraries like scikit-image, pandas, requests, and tensorflow.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tune/requirements-dev.txt#2025-04-12_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ngym>=0.21.0,<0.24.0\nscikit-image\npandas\nrequests\ntensorflow\nblack==22.10.0\nyq\n```\n\n----------------------------------------\n\nTITLE: Specifying gitdb Package Dependency with Hash Values\nDESCRIPTION: Definition of the gitdb package dependency at version 4.0.11 with associated hash values for verification. The comment indicates this package is required by gitpython.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_8\n\nLANGUAGE: text\nCODE:\n```\ngitdb==4.0.11 \\\n    --hash=sha256:81a3407ddd2ee8df444cbacea00e2d038e40150acfa3001696fe0dcf1d3adfa4 \\\n    --hash=sha256:bf5421126136d6d0af55bc1e7c1af1c397a34f5b7bd79e776cd3e89785c2b04b\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   gitpython\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Commands to install Ray Serve and required dependencies for the chatbot implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/streaming.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"ray[serve]\" transformers torch\n```\n\n----------------------------------------\n\nTITLE: Converting SampleBatch Recordings to Episodes\nDESCRIPTION: Shows how to convert RLlib's old API stack SampleBatch recordings into SingleAgentEpisode format using OfflinePreLearner.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport msgpack\nimport msgpack_numpy as mnp\n\nfrom ray import data\nfrom ray.rllib.offline.offline_prelearner import OfflinePreLearner\n\n# Set up the data path to your `SampleBatch` expert data.\ndata_path = ...\n# Set up the write path for the Parquet episode data.\noutput_data_path = \"/tmp/sample_batch_data\"\n\n# Load the `SampleBatch` recordings.\nds = data.read_json(data_path)\n\n# Iterate over batches (of `SampleBatch`es) and convert them to episodes.\nfor i, batch in enumerate(ds.iter_batches(batch_size=100, prefetch_batches=2)):\n    # Use the RLlib's `OfflinePreLearner` to convert `SampleBatch`es to episodes.\n    episodes = OfflinePreLearner._map_sample_batch_to_episode(False, batch)[\"episodes\"]\n\n    # Create a dataset from the episodes. Note, for storing episodes you need to\n    # serialize them through `msgpack-numpy`.\n    episode_ds = data.from_items([msgpack.packb(eps.get_state(), default=mnp.encode) for eps in episodes])\n    # Write the batch of episodes to local disk.\n    episode_ds.write_parquet(output_data_path + f\"/file-{i}\".zfill(6), compression=\"gzip\")\n\nprint(\"Finished converting `SampleBatch` data to episode data.\")\n```\n\n----------------------------------------\n\nTITLE: Running Development Tools\nDESCRIPTION: Commands for running TensorBoard and Jupyter Lab on the cluster with port forwarding to local machine.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nray exec CLUSTER.YAML 'tensorboard --logdir ~/ray_results/ --port 6006' --port-forward 6006\n\nray exec CLUSTER.YAML 'jupyter lab --port 6006' --port-forward 6006\n```\n\n----------------------------------------\n\nTITLE: Displaying Interactive Development Diagram in ReStructuredText\nDESCRIPTION: This code snippet uses ReStructuredText directives to display an image illustrating the interactive development setup for Ray on Kubernetes. It includes an image alignment directive and a comment about the source of the diagram.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/storage.md#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: ../images/interactive-dev.png\n    :align: center\n..\n    Find the source document here (https://whimsical.com/clusters-P5Y6R23riCuNb6xwXVXN72)\n```\n\n----------------------------------------\n\nTITLE: Ray Performance Test Results\nDESCRIPTION: Log output showing timing measurements for key Ray operations including passing many arguments (10000), handling multiple returns (3000), Ray.get calls (10000), queued tasks (1M), and transferring large objects (100GB)\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.2.0/scalability/single_node.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nMany args time: 11.433474627000002 (10000 args)\nMany returns time: 4.487700554 (3000 returns)\nRay.get time: 21.957432587999996 (10000 args)\nQueued task time: 124.148238013 (1000000 tasks)\nRay.get large object time: 35.118229127000006 (107374182400 bytes)\n```\n\n----------------------------------------\n\nTITLE: Mocking Comet Logger Callback for Testing\nDESCRIPTION: This snippet mocks the CometLoggerCallback for testing purposes.  It prevents the logger from running in the test environment. It uses `unittest.mock.MagicMock` to replace the actual logger process class. This is useful for ensuring that tests don't depend on external services.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-comet.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# This cell is hidden from the rendered notebook. It makes the \nfrom unittest.mock import MagicMock\nfrom ray.air.integrations.comet import CometLoggerCallback\n\nCometLoggerCallback._logger_process_cls = MagicMock\napi_key = \"abc\"\nproject_name = \"test\"\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies Declaration\nDESCRIPTION: Detailed listing of Python package dependencies with versions and source information. Each entry specifies the package name, version, and the requirement file or dependent package that requires it.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled.txt#2025-04-12_snippet_4\n\nLANGUAGE: text\nCODE:\n```\npytest-repeat==0.9.3\npytest-rerunfailures==11.1.2\npytest-shutil==1.7.0\npytest-sphinx @ git+https://github.com/ray-project/pytest-sphinx\npytest-sugar==0.9.5\npytest-timeout==2.1.0\npytest-virtualenv==1.7.0 ; python_version < \"3.12\"\npython-box==6.1.0\npython-dateutil==2.8.2\npython-jose==3.3.0\npython-json-logger==2.0.7\npython-lsp-jsonrpc==1.0.0\npython-multipart==0.0.6\npython-snappy==0.7.2\npytorch-lightning==1.8.6\npytorch-ranger==0.1.1\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Out-of-Band ObjectRef Serialization in Ray (Anti-pattern)\nDESCRIPTION: This code snippet demonstrates the anti-pattern of serializing a Ray ObjectRef using cloudpickle outside of Ray's managed environment.  This causes Ray to be unable to properly garbage collect the underlying object, potentially leading to memory leaks and disk spilling. The snippet highlights the incorrect way to handle ObjectRefs when passing them between processes or storing them.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/out-of-band-object-ref-serialization.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example demonstrating out-of-band object ref serialization.\n\nThis is an anti-pattern because Ray won't be able to garbage collect the underlying object\nwhen ray.ObjectRef is serialized by cloudpickle.\n\"\"\"\n\nimport ray\nimport cloudpickle\n\n# This test is skipped by default because it's expected to fail.  Set\n# RAY_ALLOW_OUT_OF_BAND_OBJECT_REF_SERIALIZATION=0 to trigger this exception.\n\n# __anti_pattern_start__\n@ray.remote\ndef f():\n    return 1\n\nobj_ref = f.remote()\n# BAD: Serializing an object ref like this prevents the object from getting\n# garbage collected.\nserialized_obj_ref = cloudpickle.dumps(obj_ref)\n# __anti_pattern_end__\n\n# This test is skipped by default because it's expected to fail.  Set\n# RAY_ALLOW_OUT_OF_BAND_OBJECT_REF_SERIALIZATION=0 to trigger this exception.\n```\n\n----------------------------------------\n\nTITLE: Listing SHA256 Hashes for Python Package Dependencies\nDESCRIPTION: This snippet lists SHA256 hashes for various Python package dependencies. Each line represents a unique hash for a specific package version. The hashes are used to verify the integrity of downloaded packages during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_26\n\nLANGUAGE: Text\nCODE:\n```\n--hash=sha256:c1e1cc06da1491e6734f0ea1e6294ce00792193c463350626571c287c9a704db \\\n--hash=sha256:c654d5207c78e0bd6d749f6dae1dcbbfde3403ad3a4b11f3c5544d9906969dde \\\n--hash=sha256:c69697d3adff5aa4f874b19c0e4ed65180ceed6318ec856ebc423aa5850d84f7 \\\n--hash=sha256:c7d79f7d9aabd6011004e33b22bc13056a3e3fb54794d138af57f5ee9d9032cb \\\n--hash=sha256:ccaa3a4b521b780a7e771cc336a2dba389a0861592bbce09a476190bb0c8b4b3 \\\n--hash=sha256:ccd17349166b1bee6e529b4add61727d3f55edb7babbe4069b5764c9587a8cc6 \\\n--hash=sha256:ce1af883b94304f493698b00d0f006d56aea98aeb49d75ec7d98cd4a777e9285 \\\n--hash=sha256:d0e883008013c0e4aef84dcfe2a0b172c4d23c2669412cf5b3371003941f72bb \\\n--hash=sha256:d980e0325b6eddc81331d3f4551e2a333999fb176fd153e075c6d1c2530aa8a8 \\\n--hash=sha256:e17c9361d46a4d5addf777c6dd5eab0715a7684c2f11b88c67ac37edfba6c482 \\\n--hash=sha256:e2c08cc9b16f4f4bc522771d96734c7901e7ebef70c6c5c35dd0f10845270bcd \\\n--hash=sha256:e35ef8683211db69ffe129a25d5634319a677570ab6b2eba4afa860f54eeaf75 \\\n--hash=sha256:e3b9fd71836999aad54084906f8663dffcd2a7fb5cdafd6c37713b2e72be1760 \\\n--hash=sha256:ef9f7768395923c3039055c14334ba4d926f3baf7b776c923c93d80195624782 \\\n--hash=sha256:f52a265001d830bc425f82ca9eabda94a64a4d753b07d623a9f2863fde532b53 \\\n--hash=sha256:f91c4803173928a25e1a55b943c81f55b8872f0018be83e3ad4938adffb77dd2 \\\n--hash=sha256:fbd6748e8ab9b41171bb95c6142faf068f5ef1511935a0aa07025438dd9a9bc1 \\\n--hash=sha256:fe57328fbc1bfd0bd0514470ac692630f3901c0ee39052ae47acd1d90a436719 \\\n--hash=sha256:fea09ca13323376a2fdfb353a5fa2e59f90cd18d7ca4eaa1fd31f0a8b4f91e62\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch CPU Dependencies\nDESCRIPTION: Configuration for PyTorch CPU-only packages and related libraries. Includes specific index URLs for CPU versions and torch-geometric ecosystem dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/dl-cpu-requirements.txt#2025-04-12_snippet_1\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cpu\n--find-links https://data.pyg.org/whl/torch-2.3.0+cpu.html\n\ntorch==2.3.0\ntorchmetrics==0.10.3\ntorchtext==0.18.0\ntorchvision==0.18.0\n\ntorch-scatter==2.1.2\ntorch-sparse==0.6.18\ntorch-cluster==1.6.3\ntorch-spline-conv==1.2.2\ntorch-geometric==2.5.3\n\ncupy-cuda12x==13.1.0; sys_platform != 'darwin'\n```\n\n----------------------------------------\n\nTITLE: Updating Ray ML BYOD Requirements with Bazel\nDESCRIPTION: Command to update the requirements file for Ray ML BYOD dependencies using Bazel and pip-compile with Python 3.9\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbazel run //release:requirements_ml_byod_3.9.update\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with SHA256 Hashes\nDESCRIPTION: A requirements file listing Python package dependencies with their exact versions and SHA256 hash values for secure installation. The file includes comments indicating which parent packages or requirements files require these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_5\n\nLANGUAGE: pip-requirements\nCODE:\n```\n--hash=sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6 \\\n--hash=sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8 \\\n--hash=sha256:37e55c8e51c236f95b033f6fb391d7d7970ba5fe7ff453dad675e88cf303377a \\\n--hash=sha256:3d47fa203a7bd9c5b6cee4736ee84ca03b8ef23193c0d1ca99b5089f72645c73 \\\n--hash=sha256:3e4d1f6587322d2788836a99c69062fbb091331ec940e02d12d179c1d53e25fc \\\n--hash=sha256:42cb296636fcc8b0644486d15c12376cb9fa75443e00fb25de0b8602e64c1714 \\\n--hash=sha256:45485e01ff4d3630ec0d9617310448a8702f70e9c01906b0d0118bdf9d124cf2 \\\n--hash=sha256:4a78b2b446bd7c934f5dcedc588903fb2f5eec172f3d29e52a9096a43722adfc \\\n--hash=sha256:4ab2fe47fae9e0f9dee8c04187ce5d09f48eabe611be8259444906793ab7cbce \\\n--hash=sha256:4d0d1650369165a14e14e1e47b372cfcb31d6ab44e6e33cb2d4e57265290044d \\\n--hash=sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e \\\n--hash=sha256:55086ee1064215781fff39a1af09518bc9255b50d6333f2e4c74ca09fac6a8f6 \\\n--hash=sha256:572c3763a264ba47b3cf708a44ce965d98555f618ca42c926a9c1616d8f34269 \\\n--hash=sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96 \\\n--hash=sha256:5b4c145409bef602a690e7cfad0a15a55c13320ff7a3ad7ca59c13bb8ba4d45d \\\n--hash=sha256:6463effa3186ea09411d50efc7d85360b38d5f09b870c48e4600f63af490e56a \\\n--hash=sha256:65f6f63034100ead094b8744b3b97965785388f308a64cf8d7c34f2f2e5be0c4 \\\n--hash=sha256:663946639d296df6a2bb2aa51b60a2454ca1cb29835324c640dafb5ff2131a77 \\\n--hash=sha256:6897af51655e3691ff853668779c7bad41579facacf5fd7253b0133308cf000d \\\n--hash=sha256:68d1f8a9e9e37c1223b656399be5d6b448dea850bed7d0f87a8311f1ff3dabb0 \\\n--hash=sha256:6ac7ffc7ad6d040517be39eb591cac5ff87416c2537df6ba3cba3bae290c0fed \\\n--hash=sha256:6b3251890fff30ee142c44144871185dbe13b11bab478a88887a639655be1068 \\\n--hash=sha256:6c4caeef8fa63d06bd437cd4bdcf3ffefe6738fb1b25951440d80dc7df8c03ac \\\n--hash=sha256:6ef1d82a3af9d3eecdba2321dc1b3c238245d890843e040e41e470ffa64c3e25 \\\n--hash=sha256:753f10e867343b4511128c6ed8c82f7bec3bd026875576dfd88483c5c73b2fd8 \\\n--hash=sha256:7cd13a2e3ddeed6913a65e66e94b51d80a041145a026c27e6bb76c31a853c6ab \\\n--hash=sha256:7ed9e526742851e8d5cc9e6cf41427dfc6068d4f5a3bb03659444b4cabf6bc26 \\\n--hash=sha256:7f04c839ed0b6b98b1a7501a002144b76c18fb1c1850c8b98d458ac269e26ed2 \\\n--hash=sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db \\\n--hash=sha256:80402cd6ee291dcb72644d6eac93785fe2c8b9cb30893c1af5b8fdd753b9d40f \\\n--hash=sha256:8465322196c8b4d7ab6d1e049e4c5cb460d0394da4a27d23cc242fbf0034b6b5 \\\n--hash=sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99 \\\n--hash=sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c \\\n--hash=sha256:8bdb58ff7ba23002a4c5808d608e4e6c687175724f54a5dade5fa8c67b604e4d \\\n--hash=sha256:8c622a5fe39a48f78944a87d4fb8a53ee07344641b0562c540d840748571b811 \\\n--hash=sha256:8d756e44e94489e49571086ef83b2bb8ce311e730092d2c34ca8f7d925cb20aa \\\n--hash=sha256:8f4a014bc36d3c57402e2977dada34f9c12300af536839dc38c0beab8878f38a \\\n--hash=sha256:9063e24fdb1e498ab71cb7419e24622516c4a04476b17a2dab57e8baa30d6e03 \\\n--hash=sha256:90d558489962fd4918143277a773316e56c72da56ec7aa3dc3dbbe20fdfed15b \\\n--hash=sha256:923c0c831b7cfcb071580d3f46c4baf50f174be571576556269530f4bbd79d04 \\\n--hash=sha256:95f2a5796329323b8f0512e09dbb7a1860c46a39da62ecb2324f116fa8fdc85c \\\n--hash=sha256:96b02a3dc4381e5494fad39be677abcb5e6634bf7b4fa83a6dd3112607547001 \\\n--hash=sha256:9f96df6923e21816da7e0ad3fd47dd8f94b2a5ce594e00677c0013018b813458 \\\n--hash=sha256:a10af20b82360ab00827f916a6058451b723b4e65030c5a18577c8b2de5b3389 \\\n--hash=sha256:a50aebfa173e157099939b17f18600f72f84eed3049e743b68ad15bd69b6bf99 \\\n--hash=sha256:a981a536974bbc7a512cf44ed14938cf01030a99e9b3a06dd59578882f06f985 \\\n--hash=sha256:a9a8e9031d613fd2009c182b69c7b2c1ef8239a0efb1df3f7c8da66d5dd3d537 \\\n--hash=sha256:ae5f4161f18c61806f411a13b0310bea87f987c7d2ecdbdaad0e94eb2e404238 \\\n--hash=sha256:aed38f6e4fb3f5d6bf81bfa990a07806be9d83cf7bacef998ab1a9bd660a581f \\\n--hash=sha256:b01b88d45a6fcb69667cd6d2f7a9aeb4bf53760d7fc536bf679ec94fe9f3ff3d \\\n--hash=sha256:b261ccdec7821281dade748d088bb6e9b69e6d15b30652b74cbbac25e280b796 \\\n--hash=sha256:b2b0a0c0517616b6869869f8c581d4eb2dd83a4d79e0ebcb7d373ef9956aeb0a \\\n--hash=sha256:b4a23f61ce87adf89be746c8a8974fe1c823c891d8f86eb218bb957c924bb143 \\\n--hash=sha256:bd8f7df7d12c2db9fab40bdd87a7c09b1530128315d047a086fa3ae3435cb3a8 \\\n--hash=sha256:beb58fe5cdb101e3a055192ac291b7a21e3b7ef4f67fa1d74e331a7f2124341c \\\n--hash=sha256:c002b4ffc0be611f0d9da932eb0f704fe2602a9a949d1f738e4c34c75b0863d5 \\\n--hash=sha256:c083af607d2515612056a31f0a8d9e0fcb5876b7bfc0abad3ecd275bc4ebc2d5 \\\n--hash=sha256:c180f51afb394e165eafe4ac2936a14bee3eb10debc9d9e4db8958fe36afe711 \\\n--hash=sha256:c235ebd9baae02f1b77bcea61bce332cb4331dc3617d254df3323aa01ab47bd4 \\\n--hash=sha256:cd70574b12bb8a4d2aaa0094515df2463cb429d8536cfb6c7ce983246983e5a6 \\\n--hash=sha256:d0eccceffcb53201b5bfebb52600a5fb483a20b61da9dbc885f8b103cbe7598c \\\n--hash=sha256:d965bba47ddeec8cd560687584e88cf699fd28f192ceb452d1d7ee807c5597b7 \\\n--hash=sha256:db364eca23f876da6f9e16c9da0df51aa4f104a972735574842618b8c6d999d4 \\\n--hash=sha256:ddbb2551d7e0102e7252db79ba445cdab71b26640817ab1e3e3648dad515003b \\\n--hash=sha256:deb6be0ac38ece9ba87dea880e438f25ca3eddfac8b002a2ec3d9183a454e8ae \\\n--hash=sha256:e06ed3eb3218bc64786f7db41917d4e686cc4856944f53d5bdf83a6884432e12 \\\n--hash=sha256:e27ad930a842b4c5eb8ac0016b0a54f5aebbe679340c26101df33424142c143c \\\n--hash=sha256:e537484df0d8f426ce2afb2d0f8e1c3d0b114b83f8850e5f2fbea0e797bd82ae \\\n--hash=sha256:eb00ed941194665c332bf8e078baf037d6c35d7c4f3102ea2d4f16ca94a26dc8 \\\n--hash=sha256:eb6904c354526e758fda7167b33005998fb68c46fbc10e013ca97f21ca5c8887 \\\n--hash=sha256:eb8821e09e916165e160797a6c17edda0679379a4be5c716c260e836e122f54b \\\n--hash=sha256:efcb3f6676480691518c177e3b465bcddf57cea040302f9f4e6e191af91174d4 \\\n--hash=sha256:f27273b60488abe721a075bcca6d7f3964f9f6f067c8c4c605743023d7d3944f \\\n--hash=sha256:f30c3cb33b24454a82faecaf01b19c18562b1e89558fb6c56de4d9118a032fd5 \\\n--hash=sha256:fb69256e180cb6c8a894fee62b3afebae785babc1ee98b81cdf68bbca1987f33 \\\n--hash=sha256:fd1abc0d89e30cc4e02e4064dc67fcc51bd941eb395c502aac3ec19fab46b519 \\\n--hash=sha256:ff8fa367d09b717b2a17a052544193ad76cd49979c805768879cb63d9ca50561\n# via\n#   -c python/requirements_compiled_ray_test_py311_cpu.txt\n#   requests\nclick==8.1.7 \\\n    --hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28 \\\n    --hash=sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   ray\n    #   typer\n    #   uvicorn\ncloudpickle==2.2.0 \\\n    --hash=sha256:3f4219469c55453cfe4737e564b67c2a149109dabf7f242478948b895f61106f \\\n    --hash=sha256:7428798d5926d8fcbfd092d18d01a2a03daf8237d8fcdc8095d256b8490796f0\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   gymnasium\n    #   outlines\n    #   vllm\ncolorama==0.4.6 \\\n    --hash=sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44 \\\n    --hash=sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   halo\n    #   log-symbols\ncolorful==0.5.5 \\\n    --hash=sha256:62c187e27c1433db9463ff93b1451898d1e7e23a7e553583fd9daeb6325182e4 \\\n    --hash=sha256:66f8c1264b2a26f7293b96a03bb7a76c4bc8b9634369a0bffdcd12d618056a1d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\ncomm==0.2.0 \\\n    --hash=sha256:2da8d9ebb8dd7bfc247adaff99f24dce705638a8042b85cb995066793e391001 \\\n    --hash=sha256:a517ea2ca28931c7007a7a99c562a0fa5883cfb48963140cf642c41c948498be\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   ipykernel\n    #   ipywidgets\ncompressed-tensors==0.9.2 \\\n    --hash=sha256:18c5627a7324a75cd4c7d984799269e0ddef592b6fb3b9a81c16754d5c4b56ff \\\n    --hash=sha256:fbc5d188ee43f93eccd6df566e8eccbb1eba907560b2b81ca85153335df55dd9\n    # via vllm\ncryptography==42.0.5 \\\n    --hash=sha256:0270572b8bd2c833c3981724b8ee9747b3ec96f699a9665470018594301439ee \\\n    --hash=sha256:111a0d8553afcf8eb02a4fea6ca4f59d48ddb34497aa8706a6cf536f1a5ec576 \\\n    --hash=sha256:16a48c23a62a2f4a285699dba2e4ff2d1cff3115b9df052cdd976a18856d8e3d \\\n    --hash=sha256:1b95b98b0d2af784078fa69f637135e3c317091b615cd0905f8b8a087e86fa30 \\\n    --hash=sha256:1f71c10d1e88467126f0efd484bd44bca5e14c664ec2ede64c32f20875c0d413 \\\n    --hash=sha256:2424ff4c4ac7f6b8177b53c17ed5d8fa74ae5955656867f5a8affaca36a27abb \\\n    --hash=sha256:2bce03af1ce5a5567ab89bd90d11e7bbdff56b8af3acbbec1faded8f44cb06da \\\n    --hash=sha256:329906dcc7b20ff3cad13c069a78124ed8247adcac44b10bea1130e36caae0b4 \\\n    --hash=sha256:37dd623507659e08be98eec89323469e8c7b4c1407c85112634ae3dbdb926fdd \\\n    --hash=sha256:3eaafe47ec0d0ffcc9349e1708be2aaea4c6dd4978d76bf6eb0cb2c13636c6fc \\\n    --hash=sha256:5e6275c09d2badf57aea3afa80d975444f4be8d3bc58f7f80d2a484c6f9485c8 \\\n    --hash=sha256:6fe07eec95dfd477eb9530aef5bead34fec819b3aaf6c5bd6d20565da607bfe1 \\\n    --hash=sha256:7367d7b2eca6513681127ebad53b2582911d1736dc2ffc19f2c3ae49997496bc \\\n    --hash=sha256:7cde5f38e614f55e28d831754e8a3bacf9ace5d1566235e39d91b35502d6936e \\\n    --hash=sha256:9481ffe3cf013b71b2428b905c4f7a9a4f76ec03065b05ff499bb5682a8d9ad8 \\\n    --hash=sha256:98d8dc6d012b82287f2c3d26ce1d2dd130ec200c8679b6213b3c73c08b2b7940 \\\n    --hash=sha256:a011a644f6d7d03736214d38832e030d8268bcff4a41f728e6030325fea3e400\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding and Testing the Application\nDESCRIPTION: Commands for port forwarding and testing the Ray Serve application from a local machine. The initial test reveals a missing python-multipart dependency required for form parsing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md#2025-04-12_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# (On your local machine) Forward the serve port of the head Pod\nkubectl port-forward $HEAD_POD 8000\n\n# Clone the repository on your local machine\ngit clone https://github.com/ray-project/serve_config_examples.git\ncd serve_config_examples/mobilenet\n\n# Prepare a sample image file. `stable_diffusion_example.png` is a cat image generated by the Stable Diffusion model.\ncurl -O https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/docs/images/stable_diffusion_example.png\n\n# Update `image_path` in `mobilenet_req.py` to the path of `stable_diffusion_example.png`\n# Send a request to the Ray Serve application.\npython3 mobilenet_req.py\n```\n\n----------------------------------------\n\nTITLE: Starting Intel Gaudi Docker Container\nDESCRIPTION: Commands to pull and run the Intel Gaudi Docker container with the necessary runtime and device configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/sd.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\ndocker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n```\n\n----------------------------------------\n\nTITLE: Using Ray State CLI\nDESCRIPTION: Commands to access the Ray State CLI for monitoring Ray Serve applications and actor status.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/observability.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- ray summary actors\n```\n\n----------------------------------------\n\nTITLE: Disabling Exploration Behavior in Python\nDESCRIPTION: Demonstrates how to disable exploration behavior for environment runners.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Disable exploration behavior.\n# When False, the EnvRunner calls `forward_inference()` on the RLModule to compute\n# actions instead of `forward_exploration()`.\nconfig.env_runners(explore=False)\n```\n\n----------------------------------------\n\nTITLE: Specifying System-Dependent and Additional Python Package Dependencies\nDESCRIPTION: Requirements for system-dependent packages like pexpect (for non-Windows systems) and additional Python packages like pickleshare and pillow with version pinning and hash verification for the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npexpect==4.8.0 ; sys_platform != 'win32' \\\n    --hash=sha256:0b48a55dcb3c05f3329815901ea4fc1537514d6ba867a152b581d69ae3710937 \\\n    --hash=sha256:fc65a43959d153d0114afe13997d439c22823a27cefceb5ff35c2178c6784c0c\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ipython\npickleshare==0.7.5 \\\n    --hash=sha256:87683d47965c1da65cdacaf31c8441d12b8044cdec9aca500cd78fc2c683afca \\\n    --hash=sha256:9649af414d74d4df115d5d718f82acb59c9d418196b7b4290ed47a12ce62df56\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ipython\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    --hash=sha256:0ea2a783a2bdf2a561808fe4a7a12e9aa3799b701ba305de596bc48b8bdfce9d \\\n    --hash=sha256:1530e8f3a4b965eb6a7785cf17a426c779333eb62c9a7d1bbcf3ffd5bf77a4aa \\\n    --hash=sha256:16563993329b79513f59142a6b02055e10514c1a8e86dca8b48a893e33cf91e3 \\\n    --hash=sha256:19aeb96d43902f0a783946a0a87dbdad5c84c936025b8419da0a0cd7724356b1 \\\n    --hash=sha256:1a1d1915db1a4fdb2754b9de292642a39a7fb28f1736699527bb649484fb966a \\\n    --hash=sha256:1b87bd9d81d179bd8ab871603bd80d8645729939f90b71e62914e816a76fc6bd \\\n    --hash=sha256:1dfc94946bc60ea375cc39cff0b8da6c7e5f8fcdc1d946beb8da5c216156ddd8 \\\n    --hash=sha256:2034f6759a722da3a3dbd91a81148cf884e91d1b747992ca288ab88c1de15999 \\\n    --hash=sha256:261ddb7ca91fcf71757979534fb4c128448b5b4c55cb6152d280312062f69599 \\\n    --hash=sha256:2ed854e716a89b1afcedea551cd85f2eb2a807613752ab997b9974aaa0d56936 \\\n    --hash=sha256:3102045a10945173d38336f6e71a8dc71bcaeed55c3123ad4af82c52807b9375 \\\n    --hash=sha256:339894035d0ede518b16073bdc2feef4c991ee991a29774b33e515f1d308e08d \\\n    --hash=sha256:412444afb8c4c7a6cc11a47dade32982439925537e483be7c0ae0cf96c4f6a0b \\\n    --hash=sha256:4203efca580f0dd6f882ca211f923168548f7ba334c189e9eab1178ab840bf60 \\\n    --hash=sha256:45ebc7b45406febf07fef35d856f0293a92e7417ae7933207e90bf9090b70572 \\\n    --hash=sha256:4b5ec25d8b17217d635f8935dbc1b9aa5907962fae29dff220f2659487891cd3 \\\n    --hash=sha256:4c8e73e99da7db1b4cad7f8d682cf6abad7844da39834c288fbfa394a47bbced \\\n    --hash=sha256:4e6f7d1c414191c1199f8996d3f2282b9ebea0945693fb67392c75a3a320941f \\\n    --hash=sha256:4eaa22f0d22b1a7e93ff0a596d57fdede2e550aecffb5a1ef1106aaece48e96b \\\n    --hash=sha256:50b8eae8f7334ec826d6eeffaeeb00e36b5e24aa0b9df322c247539714c6df19 \\\n    --hash=sha256:50fd3f6b26e3441ae07b7c979309638b72abc1a25da31a81a7fbd9495713ef4f \\\n    --hash=sha256:51243f1ed5161b9945011a7360e997729776f6e5d7005ba0c6879267d4c5139d \\\n    --hash=sha256:5d512aafa1d32efa014fa041d38868fda85028e3f930a96f85d49c7d8ddc0383 \\\n    --hash=sha256:5f77cf66e96ae734717d341c145c5949c63180842a545c47a0ce7ae52ca83795 \\\n    --hash=sha256:6b02471b72526ab8a18c39cb7967b72d194ec53c1fd0a70b050565a0f366d355 \\\n    --hash=sha256:6fb1b30043271ec92dc65f6d9f0b7a830c210b8a96423074b15c7bc999975f57 \\\n    --hash=sha256:7161ec49ef0800947dc5570f86568a7bb36fa97dd09e9827dc02b718c5643f09 \\\n    --hash=sha256:72d622d262e463dfb7595202d229f5f3ab4b852289a1cd09650362db23b9eb0b \\\n    --hash=sha256:74d28c17412d9caa1066f7a31df8403ec23d5268ba46cd0ad2c50fb82ae40462 \\\n    --hash=sha256:78618cdbccaa74d3f88d0ad6cb8ac3007f1a6fa5c6f19af64b55ca170bfa1edf \\\n    --hash=sha256:793b4e24db2e8742ca6423d3fde8396db336698c55cd34b660663ee9e45ed37f \\\n    --hash=sha256:798232c92e7665fe82ac085f9d8e8ca98826f8e27859d9a96b41d519ecd2e49a \\\n    --hash=sha256:81d09caa7b27ef4e61cb7d8fbf1714f5aec1c6b6c5270ee53504981e6e9121ad \\\n    --hash=sha256:8ab74c06ffdab957d7670c2a5a6e1a70181cd10b727cd788c4dd9005b6a8acd9 \\\n    --hash=sha256:8eb0908e954d093b02a543dc963984d6e99ad2b5e36503d8a0aaf040505f747d \\\n    --hash=sha256:90b9e29824800e90c84e4022dd5cc16eb2d9605ee13f05d47641eb183cd73d45 \\\n    --hash=sha256:9797a6c8fe16f25749b371c02e2ade0efb51155e767a971c61734b1bf6293994 \\\n    --hash=sha256:9d2455fbf44c914840c793e89aa82d0e1763a14253a000743719ae5946814b2d \\\n    --hash=sha256:9d3bea1c75f8c53ee4d505c3e67d8c158ad4df0d83170605b50b64025917f338 \\\n    --hash=sha256:9e2ec1e921fd07c7cda7962bad283acc2f2a9ccc1b971ee4b216b75fad6f0463 \\\n    --hash=sha256:9e91179a242bbc99be65e139e30690e081fe6cb91a8e77faf4c409653de39451 \\\n    --hash=sha256:a0eaa93d054751ee9964afa21c06247779b90440ca41d184aeb5d410f20ff591 \\\n    --hash=sha256:a2c405445c79c3f5a124573a051062300936b0281fee57637e706453e452746c \\\n    --hash=sha256:aa7e402ce11f0885305bfb6afb3434b3cd8f53b563ac065452d9d5654c7b86fd \\\n    --hash=sha256:aff76a55a8aa8364d25400a210a65ff59d0168e0b4285ba6bf2bd83cf675ba32 \\\n    --hash=sha256:b09b86b27a064c9624d0a6c54da01c1beaf5b6cadfa609cf63789b1d08a797b9 \\\n    --hash=sha256:b14f16f94cbc61215115b9b1236f9c18403c15dd3c52cf629072afa9d54c1cbf \\\n    --hash=sha256:b50811d664d392f02f7761621303eba9d1b056fb1868c8cdf4231279645c25f5 \\\n    --hash=sha256:b7bc2176354defba3edc2b9a777744462da2f8e921fbaf61e52acb95bafa9828 \\\n    --hash=sha256:c78e1b00a87ce43bb37642c0812315b411e856a905d58d597750eb79802aaaa3 \\\n\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: List of Python package dependencies with their exact versions, SHA256 hashes for verification, and dependency relationship comments. This ensures reproducible builds and secure package verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\npyasn1==0.5.1 \\\n    --hash=sha256:4439847c58d40b1d0a573d07e3856e95333f1976294494c325775aeca506eb58 \\\n    --hash=sha256:6d391a96e59b23130a5cfa74d6fd7f388dbbe26cc8f1edf39fdddf08d9d6676c\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   pyasn1-modules\n    #   rsa\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with SHA256 Hashes\nDESCRIPTION: List of Python package dependencies with exact versions and their corresponding SHA256 hash values used for package verification. Includes packages like bleach, boto3, botocore, cachetools, certifi and cffi along with their transitive dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:a760153f4e66edd6214df0a69e7eb90206c8ddd8083734ac430e852453a58e06 \\\n--hash=sha256:a764b697fd1cb01b92a18240f9afd291b1f33ede3c9cdc59dd92ba87a5f4f8f3 \\\n--hash=sha256:af18fcd2a37aa51c24cedbb82f4934f39a9a4ea11a84d34c1ab63df94a28fdd1\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Verification\nDESCRIPTION: A requirements file format that lists specific package versions along with their SHA256 hashes for security verification. It includes comments that indicate which parent packages require each dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nfilelock==3.17.0 \\\n    --hash=sha256:533dc2f7ba78dc2f0f531fc6c4940addf7b70a481e269a5a3b93be94ffbe8338 \\\n    --hash=sha256:ee4e77401ef576ebb38cd7f13b9b28893194acc20a8e68e18730ba9c0e54660e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   huggingface-hub\n    #   ray\n    #   torch\n    #   transformers\n    #   virtualenv\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Specifying google-auth Package Dependency with Hash Values\nDESCRIPTION: Definition of the google-auth package dependency at version 2.23.4 with associated hash values for verification. The comment indicates this package is required by multiple Google Cloud components and the cloud requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_11\n\nLANGUAGE: text\nCODE:\n```\ngoogle-auth==2.23.4 \\\n    --hash=sha256:79905d6b1652187def79d491d6e23d0cbb3a21d3c7ba0dbaa9c8a01906b13ff3 \\\n    --hash=sha256:d4bbc92fe4b8bfd2f3e8d88e5ba7085935da208ee38a134fc280e7ce682a05f2\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   google-api-core\n    #   google-cloud-core\n    #   google-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Serve Maven Dependency\nDESCRIPTION: Maven dependency configuration required to use Ray Serve in a Java project. Uses provided scope to avoid version conflicts when deploying to clusters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/java.md#2025-04-12_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>io.ray</groupId>\n  <artifactId>ray-serve</artifactId>\n  <version>${ray.version}</version>\n  <scope>provided</scope>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Commands to install Ray Data and vLLM packages needed for the implementation\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/llm/examples/batch/vllm-with-lora.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -qU \"ray[data]\" \"vllm==0.7.2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Data Block Split Factor\nDESCRIPTION: Sets the maximum block size multiplier that triggers block splitting in Ray Data. Helps control memory usage by splitting large blocks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/data-internals.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.data.context.MAX_SAFE_BLOCK_SIZE_FACTOR = 1.5\n```\n\n----------------------------------------\n\nTITLE: Resetting SIGCHLD Signal Handler in Python for Process Waiting\nDESCRIPTION: This code snippet demonstrates how to reset the SIGCHLD signal handler from SIG_IGN back to the default handler (SIG_DFL) when you need to wait for child processes to exit.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/user-spawn-processes.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport signal\nsignal.signal(signal.SIGCHLD, signal.SIG_DFL)\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Package requirements list with pinned versions and SHA256 hashes for secure verification. Includes dependencies like protobuf 3.20.3, psutil 5.9.6, ptyprocess 0.7.0 and others with platform-specific conditions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_38\n\nLANGUAGE: txt\nCODE:\n```\n--hash=sha256:67dda3c7325691c2081510e92c561f465ba61b975f481735aefdfc845d2cd043 \\\n--hash=sha256:6985a593417cdbc94c7f9c3403747335e450c1599da1647a5af76539672464d3 \\\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   aiohttp\n#   yarl\nprotobuf==3.20.3 \\\npsutil==5.9.6 \\\nptyprocess==0.7.0 ; os_name != 'nt' or sys_platform != 'win32' \\\npure-eval==0.2.2 \\\npy-cpuinfo==9.0.0 \\\npy-spy==0.4.0 ; python_full_version < '3.12'\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing for Model Inference\nDESCRIPTION: Demonstrates image preprocessing using torchvision transforms before model inference.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/working-with-images.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\nfrom torchvision import transforms\nimport ray\n\ndef transform_image(row: Dict[str, Any]) -> Dict[str, Any]:\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((32, 32))\n    ])\n    row[\"image\"] = transform(row[\"image\"])\n    return row\n\nds = (\n    ray.data.read_images(\"s3://anonymous@ray-example-data/batoidea/JPEGImages\")\n    .map(transform_image)\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying PyCountry Package Dependency with Hash Verification\nDESCRIPTION: Defines the pycountry package dependency at version 24.6.1 with SHA256 hash verification. This package is required by the outlines component to provide country-related data.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\npycountry==24.6.1 \\\n    --hash=sha256:b61b3faccea67f87d10c1f2b0fc0be714409e8fcdcc1315613174f6466c10221 \\\n    --hash=sha256:f1a4fb391cd7214f8eefd39556d740adcc233c778a27f8942c8dca351d6ce06f\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Ray Cluster\nDESCRIPTION: Shell command to delete the Ray cluster after job completion.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/ml-example.md#2025-04-12_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete raycluster raycluster-xgboost-benchmark\n```\n\n----------------------------------------\n\nTITLE: Ray Operation Timing Results\nDESCRIPTION: Performance metrics showing execution times in seconds for core Ray operations: creating 10000 actors (258s), executing 10000 tasks (619s), creating 1000 placement groups (24s), and launching 250 nodes (797s).\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.4.0/scalability/distributed.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nActor time: 258.023392103 (10000 actors)\nTask time: 619.2078768060001 (10000 tasks)\nPG time: 24.422072359999902 (1000 placement groups)\nNode launch time: 797.4376067439998 (250 nodes)\n```\n\n----------------------------------------\n\nTITLE: Defining User Guides Table of Contents in reStructuredText\nDESCRIPTION: A reStructuredText directive that defines the table of contents for Ray's user guides documentation. It specifies a nested structure with a maximum depth of 4 levels and lists all the main sections of the user guides.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/user-guide.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 4\n\n    tasks\n    actors\n    objects\n    handling-dependencies\n    scheduling/index.rst\n    fault-tolerance\n    patterns/index.rst\n    compiled-graph/ray-compiled-graph\n    advanced-topics\n```\n\n----------------------------------------\n\nTITLE: Pinning protobuf Package Version with Hash Verification\nDESCRIPTION: This snippet defines the protobuf package requirement with version 3.20.3 along with multiple SHA-256 hash values for verification. It includes comments indicating which packages depend on this dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_20\n\nLANGUAGE: text\nCODE:\n```\nprotobuf==3.20.3 \\\n    --hash=sha256:03038ac1cfbc41aa21f6afcbcd357281d7521b4157926f30ebecc8d4ea59dcb7 \\\n    --hash=sha256:28545383d61f55b57cf4df63eebd9827754fd2dc25f80c5253f9184235db242c \\\n    --hash=sha256:2e3427429c9cffebf259491be0af70189607f365c2f41c7c3764af6f337105f2 \\\n    --hash=sha256:398a9e0c3eaceb34ec1aee71894ca3299605fa8e761544934378bbc6c97de23b \\\n    --hash=sha256:44246bab5dd4b7fbd3c0c80b6f16686808fab0e4aca819ade6e8d294a29c7050 \\\n    --hash=sha256:447d43819997825d4e71bf5769d869b968ce96848b6479397e29fc24c4a5dfe9 \\\n    --hash=sha256:67a3598f0a2dcbc58d02dd1928544e7d88f764b47d4a286202913f0b2801c2e7 \\\n    --hash=sha256:74480f79a023f90dc6e18febbf7b8bac7508420f2006fabd512013c0c238f454 \\\n    --hash=sha256:819559cafa1a373b7096a482b504ae8a857c89593cf3a25af743ac9ecbd23480 \\\n    --hash=sha256:899dc660cd599d7352d6f10d83c95df430a38b410c1b66b407a6b29265d66469 \\\n    --hash=sha256:8c0c984a1b8fef4086329ff8dd19ac77576b384079247c770f29cc8ce3afa06c \\\n    --hash=sha256:9aae4406ea63d825636cc11ffb34ad3379335803216ee3a856787bcf5ccc751e \\\n    --hash=sha256:a7ca6d488aa8ff7f329d4c545b2dbad8ac31464f1d8b1c87ad1346717731e4db \\\n    --hash=sha256:b6cc7ba72a8850621bfec987cb72623e703b7fe2b9127a161ce61e61558ad905 \\\n    --hash=sha256:bf01b5720be110540be4286e791db73f84a2b721072a3711efff6c324cdf074b \\\n    --hash=sha256:c02ce36ec760252242a33967d51c289fd0e1c0e6e5cc9397e2279177716add86 \\\n    --hash=sha256:d9e4432ff660d67d775c66ac42a67cf2453c27cb4d738fc22cb53b5d84c135d4 \\\n    --hash=sha256:daa564862dd0d39c00f8086f88700fdbe8bc717e993a21e90711acfed02f2402 \\\n    --hash=sha256:de78575669dddf6099a8a0f46a27e82a1783c557ccc38ee620ed8cc96d3be7d7 \\\n    --hash=sha256:e64857f395505ebf3d2569935506ae0dfc4a15cb80dc25261176c784662cdcc4 \\\n    --hash=sha256:f4bd856d702e5b0d96a00ec6b307b0f51c1982c2bf9c0052cf9019e9a544ba99 \\\n    --hash=sha256:f4c42102bc82a51108e449cbb32b19b180022941c727bac0cfd50170341f16ee\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   google-api-core\n    #   googleapis-common-protos\n    #   opentelemetry-proto\n    #   tensorboardx\n```\n\n----------------------------------------\n\nTITLE: Disabling Ray Tune's Automatic Ray Initialization\nDESCRIPTION: Setting TUNE_DISABLE_AUTO_INIT to 1 prevents Ray Tune from automatically calling ray.init() when not attached to a Ray session.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_DISABLE_AUTO_INIT=1\n```\n\n----------------------------------------\n\nTITLE: Setting SMOKE_TEST flag for Pytorch ResNet Finetuning\nDESCRIPTION: Defines a boolean flag SMOKE_TEST to control whether the full example or a reduced version is run for demonstration purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# To run full example, set SMOKE_TEST as False\nSMOKE_TEST = True\n```\n\n----------------------------------------\n\nTITLE: Specifying Lark, Lazy-loader, LLGuidance, and LLVMLite Dependencies for Ray Project\nDESCRIPTION: Defines several package dependencies: Lark parser version 1.2.2, Lazy-loader version 0.4, LLGuidance version 0.7.10 (with platform constraints), and the beginning of LLVMLite version 0.43.0, each with verification hashes. These dependencies support various components including Outlines, VLLM, and Scikit-image.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_12\n\nLANGUAGE: pip\nCODE:\n```\nlark==1.2.2 \\\n    --hash=sha256:c2276486b02f0f1b90be155f2c8ba4a8e194d42775786db622faccd652d8e80c \\\n    --hash=sha256:ca807d0162cd16cef15a8feecb862d7319e7a09bdb13aef927968e45040fed80\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   outlines\n    #   vllm\nlazy-loader==0.4 \\\n    --hash=sha256:342aa8e14d543a154047afb4ba8ef17f5563baad3fc610d7b15b213b0f119efc \\\n    --hash=sha256:47c75182589b91a4e1a85a136c074285a5ad4d9f39c63e0d7fb76391c4574cd1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   scikit-image\nllguidance==0.7.10 ; platform_machine == 'aarch64' or platform_machine == 'arm64' or platform_machine == 'x86_64' \\\n    --hash=sha256:09deaad060797d87242925c99f6cb6f3ab0b3a70456f0654604e40f0d0cbf740 \\\n    --hash=sha256:0ed278c9bb5ac7553ea6303984c749b01a58f88e406e2239de5dbf3dfc1bbb9d \\\n    --hash=sha256:3a8299972e09d4f4353b61c1ad4d8443e4518b9338ccdaf37806f82949ed0815 \\\n    --hash=sha256:4d85fa4919bfc72368441612f5de53bf8781cfa9091fc77c60580a04018e83c2 \\\n    --hash=sha256:a5c641f7c7aa888b7776684828245cc69dffdf8e05c45ae1e636870e7fef640f \\\n    --hash=sha256:bf84873a7078fabfcb7eb83840f1b56698020f4ae64a0a1cba43724939c216f2 \\\n    --hash=sha256:c38bb403d81e249039cdf82743586ded98e4233ab8a4b2207d1e1bce2f63b498 \\\n    --hash=sha256:f74871b9bb40c593b88396c2d6c88b9b8cf668f0348a822668953708f10bdd97\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   vllm\nllvmlite==0.43.0 \\\n    --hash=sha256:14f0e4bf2fd2d9a75a3534111e8ebeb08eda2f33e9bdd6dfa13282afacdde0ed \\\n    --hash=sha256:18e9953c748b105668487b7c81a3e97b046d8abf95c4ddc0cd3c94f4e4651ae8 \\\n    --hash=sha256:35d80d61d0cda2d767f72de99450766250560399edc309da16937b93d3b676e7 \\\n    --hash=sha256:3e8d0618cb9bfe40ac38a9633f2493d4d4e9fcc2f438d39a4e854f39cc0f5f98 \\\n\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset API Structure in reStructuredText\nDESCRIPTION: This snippet outlines the structure of the Dataset API documentation using reStructuredText directives. It includes sections for the main API, developer API, and deprecated API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/dataset.rst#2025-04-12_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _dataset-api:\n\nDataset API\n==============\n\n.. include:: ray.data.Dataset.rst\n.. include:: ray.data.Schema.rst\n\nDeveloper API\n-------------\n\n.. currentmodule:: ray.data\n\n.. autosummary::\n  :nosignatures:\n  :toctree: doc/\n\n  Dataset.to_pandas_refs\n  Dataset.to_numpy_refs\n  Dataset.to_arrow_refs\n  Dataset.iter_internal_ref_bundles\n  block.Block\n  block.BlockExecStats\n  block.BlockMetadata\n  block.BlockAccessor\n\nDeprecated API\n--------------\n\n.. currentmodule:: ray.data\n\n.. autosummary::\n  :nosignatures:\n  :toctree: doc/\n\n  Dataset.iter_tf_batches\n  Dataset.to_torch\n```\n\n----------------------------------------\n\nTITLE: Dependency Hash Verification for Ray Project Dependencies\nDESCRIPTION: This snippet contains a list of package dependencies with their version numbers and SHA256 hashes. Each entry includes the package name, version, and multiple hash values for verification. Comments indicate which requirements files or components require each dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_47\n\nLANGUAGE: Text\nCODE:\n```\n--hash=sha256:65d98939f1a2e74b58839f8c4dab3b6b3c1ce84972ae712be02845e65391ac7c \\\n--hash=sha256:66324e4e1beede9ac79e60f88de548da58b1f8ab4b2f1354d8375774f997e6c0 \\\n--hash=sha256:6c77c9937962577a6a76917845d06af6ab9197702a42e1346d8ae2e76b5e3675 \\\n--hash=sha256:70dec29e8ac485dbf57481baee40781c63e381bebea080991893cd297742b8fd \\\n--hash=sha256:7250a3fa399f08ec9cb3f7b1b987955d17e044f1ade821b32e5f435130250d7f \\\n--hash=sha256:748290bf9112b581c525e6e6d3820621ff020ed95af6f17fedef416b27ed564c \\\n--hash=sha256:7da13da6f985aab7f6f28debab00c67ff9cbacd588e8477034c0652ac141feea \\\n--hash=sha256:8f959b26f2634a091bb42241c3ed8d3cedb506e7c27b8dd5c7b9f745318ddbb6 \\\n--hash=sha256:9de9e5188a782be6b1ce866e8a51bc76a0fbaa0e16613823fc38e4fc2556ad05 \\\n--hash=sha256:a48900ecea1cbb71b8c71c620dee15b62f85f7c14189bdeee54966fbd9a0c5bd \\\n--hash=sha256:b87936fd2c317b6ee08a5741ea06b9d11a6074ef4cc42e031bc6403f82a32575 \\\n--hash=sha256:c77da1263aa361938476f04c4b6c8916001b90b2c2fdd92d8d535e1af48fba5a \\\n--hash=sha256:cb5ec8eead331e3bb4ce8066cf06d2dfef1bfb1b2a73082dfe8a161301b76e37 \\\n--hash=sha256:cc0ee35043162abbf717b7df924597ade8e5395e7b66d18270116f8745ceb795 \\\n--hash=sha256:d14d30e7f46a0476efb0deb5b61343b1526f73ebb5ed84f23dc794bdb88f9d9f \\\n--hash=sha256:d371e811d6b156d82aa5f9a4e08b58debf97c302a35714f6f45e35139c332e32 \\\n--hash=sha256:d3d20ea5782ba63ed13bc2b8c291a053c8d807a8fa927d941bd718468f7b950c \\\n--hash=sha256:d3f7594930c423fd9f5d1a76bee85a2c36fd8b4b16921cae7e965f22575e9c01 \\\n--hash=sha256:dcef026f608f678c118779cd6591c8af6e9b4155c44e0d1bc0c87c036fb8c8c4 \\\n--hash=sha256:e0791ac58d91ac58f694d8d2957884df8e4e2f6687cdf367ef7eb7497f79eaa2 \\\n--hash=sha256:e385b637ac3acaae8022e7e47dfa7b83d3620e432e3ecb9a3f7f58f150e50921 \\\n--hash=sha256:e519d64089b0876c7b467274468709dadf11e41d65f63bba207e04217f47c085 \\\n--hash=sha256:e7229e60ac41a1202444497ddde70a48d33909e484f96eb0da9baf8dc68541df \\\n--hash=sha256:ed3ad863b1b40cd1d4bd21e7498329ccaece75db5a5bf58cd3c9f130843e7102 \\\n--hash=sha256:f0ba29bafd8e7e22920567ce0d232c26d4d47c8b5cf4ed7b562b5db39fa199c5 \\\n--hash=sha256:fa2ba70284fa42c2a5ecb35e322e68823288a4251f9ba9cc77be04ae15eada68 \\\n--hash=sha256:fba85b6cd9c39be262fcd23865652920832b61583de2a2ca907dbd8e8a8c81e5\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   ipykernel\n#   jupyter-client\n#   jupyter-server\n#   jupyterlab\n#   nbclassic\n#   notebook\n#   terminado\ntqdm==4.64.1 \\\n--hash=sha256:5f4f682a004951c1b450bc753c710e9280c5746ce6ffedee253ddbcbf54cf1e4 \\\n--hash=sha256:6fee160d6ffcd1b1c68c65f14c829c22832bc401726335ce92c52d395944a6a1\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   -r python/requirements/cloud-requirements.txt\n#   gguf\n#   huggingface-hub\n#   openai\n#   outlines\n#   transformers\n#   vllm\ntraitlets==5.14.3 \\\n--hash=sha256:9ed0579d3502c94b4b3732ac120375cda96f923114522847de4b3bb98b96b6b7 \\\n--hash=sha256:b74e89e397b1ed28cc831db7aea759ba6640cb3de13090ca145426688ff1ac4f\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   comm\n#   ipykernel\n#   ipython\n#   ipywidgets\n#   jupyter-client\n#   jupyter-core\n#   jupyter-events\n#   jupyter-server\n#   matplotlib-inline\n#   nbclassic\n#   nbclient\n#   nbconvert\n#   nbformat\n#   notebook\ntransformers==4.49.0 \\\n--hash=sha256:6b4fded1c5fee04d384b1014495b4235a2b53c87503d7d592423c06128cbbe03 \\\n--hash=sha256:7e40e640b5b8dc3f48743f5f5adbdce3660c82baafbd3afdfc04143cdbd2089e\n# via\n#   compressed-tensors\n#   vllm\n#   xgrammar\ntriton==3.2.0 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n--hash=sha256:0fc1217eed33c7695272f981f5a8874ce3cb0195bbb2bfed16d58edd0aefef04 \\\n--hash=sha256:142dd3a9ac2fc3433768eeb4a4cd120655e2f658f4bf42726d2ea7f3748abffa \\\n--hash=sha256:30ceed0eff2c4a73b14eb63e052992f44bbdf175f3fad21e1ac8097a772de7ee \\\n--hash=sha256:468a01c9aa6e18fe2bba49c5e5002c1fd5f61b1af891c0594eaf446fe1aaae10 \\\n--hash=sha256:8009a1fb093ee8546495e96731336a33fb8856a38e45bb4ab6affd6dbc3ba220 \\\n--hash=sha256:8d9b215efc1c26fa7eefb9a157915c92d52e000d2bf83e5f69704047e63f125c \\\n--hash=sha256:b3e54983cd51875855da7c68ec05c05cf8bb08df361b1d5b69e05e40b0c9bd62 \\\n--hash=sha256:d528960c898f74596d5a8af1d70a7f0899c05a0781205eab51407b67f1644652 \\\n--hash=sha256:dd88c7a4255991bf034e1e381e26636f43d2f01a0f244c27b9c7dceae5656eb9 \\\n--hash=sha256:e5dfa23ba84541d7c0a531dfce76d8bcd19159d50a4a8b14ad01e91734a5c1b0 \\\n--hash=sha256:f1679fde231fb04c96cb5a01b160c8d0294ce6f7c122565d8b33ad8a910422d7 \\\n--hash=sha256:f24212d12744266f6229f90f820f34c43a538a69d6511b8e92ee392d2dc0d38b\n# via torch\ntyper==0.12.3 \\\n--hash=sha256:070d7ca53f785acbccba8e7d28b08dcd88f79f1fbda035ade0aecec71ca5c914 \\\n--hash=sha256:49e73131481d804288ef62598d97a1ceef3058905aa536a1134f90891ba35482\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   -r python/requirements/llm/llm-requirements.txt\n#   -r python/requirements.txt\n#   fastapi-cli\ntypes-python-dateutil==2.9.0.20240316 \\\n--hash=sha256:5d2f2e240b86905e40944dd787db6da9263f0deabef1076ddaed797351ec0202 \\\n--hash=sha256:6b8cb66d960771ce5ff974e9dd45e38facb81718cc1e208b10b1baccbfdbee3b\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   arrow\ntyping-extensions==4.12.2 \\\n--hash=sha256:04e5ca0351e0f3f85c6853954072df659d0d13fac324d0072316b67d7794700d \\\n--hash=sha256:1a7ead55c7e559dd4dee8856e3a88b41225abfe1ce8df57b7c13915fe121ffb8\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   fastapi\n#   gymnasium\n#   huggingface-hub\n#   mistral-common\n#   openai\n#   outlines\n#   pydantic\n#   pydantic-core\n#   referencing\n#   torch\n#   typer\n#   vllm\ntzlocal==5.3 \\\n--hash=sha256:2fafbfc07e9d8b49ade18f898d6bcd37ae88ce3ad6486842a2e4f03af68323d2 \\\n--hash=sha256:3814135a1bb29763c6e4f08fd6e41dbb435c7a60bfbb03270211bcc537187d8c\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   -r python/requirements/cloud-requirements.txt\nuri-template==1.3.0 \\\n--hash=sha256:0e00f8eb65e18c7de20d595a14336e9f337ead580c70934141624b6d1ffdacc7 \\\n--hash=sha256:a44a133ea12d44a0c0f06d7d42a52d71282e77e2f937d8abd5655b8d56fc1363\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   jsonschema\nurllib3==1.26.19 \\\n--hash=sha256:37a0344459b199fce0e80b0d3569837ec6b6937435c5244e7fd73fa6006830f3 \\\n--hash=sha256:3e3d753a8618b86d7de333b4223005f68720bcd6a7d2bcb9fbd2229ec7c1e429\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   -r python/requirements/cloud-requirements.txt\n#   botocore\n#   requests\nuvicorn==0.22.0 \\\n--hash=sha256:79277ae03db57ce7d9aa0567830bbb51d7a612f54d6e1e3e92da3ef24c2c8ed8 \\\n--hash=sha256:e9434d3bbf05f310e762147f769c9f21235ee118ba2d2bf1155a7196448bd996\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   -r python/requirements.txt\n#   fastapi\n#   fastapi-cli\nuvloop==0.21.0 ; platform_python_implementation != 'PyPy' and sys_platform != 'cygwin' and sys_platform != 'win32' \\\n--hash=sha256:0878c2640cf341b269b7e128b1a5fed890adc4455513ca710d77d5e93aa6d6a0 \\\n--hash=sha256:10d66943def5fcb6e7b37310eb6b5639fd2ccbc38df1177262b0640c3ca68c1f \\\n--hash=sha256:10da8046cc4a8f12c91a1c39d1dd1585c41162a15caaef165c2174db9ef18bdc \\\n--hash=sha256:17df489689befc72c39a08359efac29bbee8eee5209650d4b9f34df73d22e414 \\\n--hash=sha256:183aef7c8730e54c9a3ee3227464daed66e37ba13040bb3f350bc2ddc040f22f \\\n--hash=sha256:196274f2adb9689a289ad7d65700d37df0c0930fd8e4e743fa4834e850d7719d \\\n--hash=sha256:221f4f2a1f46032b403bf3be628011caf75428ee3cc204a22addf96f586b19fd \\\n--hash=sha256:2d1f581393673ce119355d56da84fe1dd9d2bb8b3d13ce792524e1607139feff \\\n--hash=sha256:359ec2c888397b9e592a889c4d72ba3d6befba8b2bb01743f72fffbde663b59c \\\n--hash=sha256:3bf12b0fda68447806a7ad847bfa591613177275d35b6724b1ee573faa3704e3 \\\n--hash=sha256:4509360fcc4c3bd2c70d87573ad472de40c13387f5fda8cb58350a1d7475e58d \\\n--hash=sha256:460def4412e473896ef179a1671b40c039c7012184b627898eea5072ef6f017a \\\n--hash=sha256:461d9ae6660fbbafedd07559c6a2e57cd553b34b0065b6550685f6653a98c1cb \\\n--hash=sha256:46923b0b5ee7fc0020bef24afe7836cb068f5050ca04caf6b487c513dc1a20b2 \\\n--hash=sha256:53e420a3afe22cdcf2a0f4846e377d16e718bc70103d7088a4f7623567ba5fb0 \\\n--hash=sha256:5ee4d4ef48036ff6e5cfffb09dd192c7a5027153948d85b8da7ff705065bacc6 \\\n--hash=sha256:67dd654b8ca23aed0a8e99010b4c34aca62f4b7fce88f39d452ed7622c94845c \\\n--hash=sha256:787ae31ad8a2856fc4e7c095341cccc7209bd657d0e71ad0dc2ea83c4a6fa8af \\\n--hash=sha256:86975dca1c773a2c9864f4c52c5a55631038e387b47eaf56210f873887b6c8dc \\\n--hash=sha256:87c43e0f13022b998eb9b973b5e97200c8b90823454d4bc06ab33829e09fb9bb \\\n--hash=sha256:88cb67cdbc0e483da00af0b2c3cdad4b7c61ceb1ee0f33fe00e09c81e3a6cb75 \\\n--hash=sha256:8a375441696e2eda1c43c44ccb66e04d61ceeffcd76e4929e527b7fa401b90fb \\\n--hash=sha256:a5c39f217ab3c663dc699c04cbd50c13813e31d917642d459fdcec07555cc553 \\\n--hash=sha256:b9fb766bb57b7388745d8bcc53a359b116b8a04c83a2288069809d2b3466c37e \\\n--hash=sha256:baa0e6291d91649c6ba4ed4b2f982f9fa165b5bbd50a9e203c416a2797bab3c6 \\\n--hash=sha256:baa4dcdbd9ae0a372f2167a207cd98c9f9a1ea1188a8a526431eef2f8116cc8d \\\n--hash=sha256:bc09f0ff191e61c2d592a752423c767b4ebb2986daa9ed62908e2b1b9a9ae206 \\\n--hash=sha256:bd53ecc9a0f3d87ab847503c2e1552b690362e005ab54e8a48ba97da3924c0dc \\\n--hash=sha256:bfd55dfcc2a512316e65f16e503e9e450cab148ef11df4e4e679b5e8253a5281 \\\n--hash=sha256:c097078b8031190c934ed0ebfee8cc5f9ba9642e6eb88322b9958b649750f72b \\\n--hash=sha256:c0f3fa6200b3108919f8bdabb9a7f87f20e7097ea3c543754cabc7d717d95cf8 \\\n```\n\n----------------------------------------\n\nTITLE: Ray Cluster Start/Stop Time Measurement\nDESCRIPTION: Command to measure the time taken to start and stop a Ray cluster, demonstrating the overhead of cluster initialization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/testing-tips.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -c 'import ray; ray.init(); ray.shutdown()'  3.93s user 1.23s system 116% cpu 4.420 total\n```\n\n----------------------------------------\n\nTITLE: Disabling SIGINT Handler in Ray Tune\nDESCRIPTION: Setting TUNE_DISABLE_SIGINT_HANDLER to 1 disables Ray Tune's ability to catch SIGINT signals (e.g., Ctrl+C) for graceful shutdown and final checkpointing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_DISABLE_SIGINT_HANDLER=1\n```\n\n----------------------------------------\n\nTITLE: Setting Up Grafana Port Forwarding\nDESCRIPTION: Commands to set up port forwarding and retrieve Grafana admin password.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-operator-logs.md#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl --namespace default port-forward $POD_NAME 3000\n```\n\n----------------------------------------\n\nTITLE: Specifying Ray Project Dependencies\nDESCRIPTION: This snippet lists various Python package dependencies with their version numbers and SHA256 hash values. It includes comments indicating which packages are required by other dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_15\n\nLANGUAGE: Plain Text\nCODE:\n```\ngoogle-resumable-media==2.6.0 \\\n    --hash=sha256:972852f6c65f933e15a4a210c2b96930763b47197cdf4aa5f5bea435efb626e7 \\\n    --hash=sha256:fc03d344381970f79eebb632a3c18bb1828593a2dc5572b5f90115ef7d11e81b\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-cloud-storage\ngooglapis-common-protos==1.61.0 \\\n    --hash=sha256:22f1915393bb3245343f6efe87f6fe868532efc12aa26b391b15132e1279f1c0 \\\n    --hash=sha256:8a64866a97f6304a7179873a465d6eee97b7a24ec6cfd78e0f575e96b821240b\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-api-core\n    #   opentelemetry-exporter-otlp-proto-grpc\ngrpcio==1.66.2 \\\n    --hash=sha256:02697eb4a5cbe5a9639f57323b4c37bcb3ab2d48cec5da3dc2f13334d72790dd \\\n    --hash=sha256:03b0b307ba26fae695e067b94cbb014e27390f8bc5ac7a3a39b7723fed085604\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   grpcio-tools\n    #   opentelemetry-exporter-otlp-proto-grpc\ngymnasium==1.0.0 \\\n    --hash=sha256:9d2b66f30c1b34fe3c2ce7fae65ecf365d0e9982d2b3d860235e773328a3b403 \\\n    --hash=sha256:b6f40e1e24c5bd419361e1a5b86a9117d2499baecc3a660d44dfff4c465393ad\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\nh11==0.14.0 \\\n    --hash=sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d \\\n    --hash=sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   uvicorn\nhalo==0.0.31 \\\n    --hash=sha256:5350488fb7d2aa7c31a1344120cee67a872901ce8858f60da7946cef96c208ab \\\n    --hash=sha256:7b67a3521ee91d53b7152d4ee3452811e1d2a6321975137762eb3d70063cc9d6\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\nhttplib2==0.20.4 \\\n    --hash=sha256:58a98e45b4b1a48273073f905d2961666ecf0fbac4250ea5b47aef259eb5c585 \\\n    --hash=sha256:8b6a905cb1c79eefd03f8669fd993c36dc341f7c558f056cb5a33b5c2f458543\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure with Sphinx TOC Tree\nDESCRIPTION: Sphinx documentation table of contents tree defining the structure of VM cluster guides, including sections for launching clusters, best practices, autoscaling, logging, and community-supported cluster managers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/index.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n:hidden:\n\nlaunching-clusters/index\nlarge-cluster-best-practices\nconfiguring-autoscaling\nlogging\nCommunity-supported Cluster Managers <community/index>\n```\n\n----------------------------------------\n\nTITLE: Specifying google-cloud-core Package Dependency with Hash Values\nDESCRIPTION: Definition of the google-cloud-core package dependency at version 2.4.1 with associated hash values for verification. The comment indicates this package is required by google-cloud-storage.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_12\n\nLANGUAGE: text\nCODE:\n```\ngoogle-cloud-core==2.4.1 \\\n    --hash=sha256:9b7749272a812bde58fff28868d0c5e2f585b82f37e09a1f6ed2d4d10f134073 \\\n    --hash=sha256:a9e6a4422b9ac5c29f79a0ede9485473338e2ce78d91f2370c01e730eab22e61\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Adding GPU Tolerations to Pod Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates how to manually add GPU tolerations to a Ray worker pod configuration. This is necessary when the Kubernetes ExtendedResourceToleration admission controller is not enabled in your cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gpu.rst#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Pod\nmetadata:\n generateName: example-cluster-ray-worker\n spec:\n ...\n tolerations:\n - effect: NoSchedule\n   key: nvidia.com/gpu\n   operator: Exists\n ...\n containers:\n - name: ray-node\n   image: rayproject/ray:nightly-gpu\n   ...\n```\n\n----------------------------------------\n\nTITLE: Cloning AWS Neuron Samples Repository\nDESCRIPTION: Command to clone the AWS Neuron EKS samples repository containing the fine-tuning examples.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/aws-trainium/llama3.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/aws-neuron/aws-neuron-eks-samples.git\n```\n\n----------------------------------------\n\nTITLE: Retrieving Grafana Admin Password in Kubernetes\nDESCRIPTION: Command to retrieve the admin password for Grafana from a Kubernetes secret. The password is base64 decoded for immediate use.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.md#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get secret --namespace default grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n```\n\n----------------------------------------\n\nTITLE: Java Stack Trace - Netty/Vert.x Request Processing\nDESCRIPTION: Stack trace showing the full execution path from Java thread creation through Netty NIO event processing, Vert.x HTTP server handling, and Mozilla JavaScript runtime operations. Includes slot creation in ScriptableObject.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_79\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/IdScriptableObject:.put_[j];org/mozilla/javascript/ScriptableObject:.getSlot_[j];org/mozilla/javascript/ScriptableObject:.createSlot_[j]\n```\n\n----------------------------------------\n\nTITLE: DDP Training Logs for Llama-2 on HPUs\nDESCRIPTION: Training logs from Distributed Data Parallel implementation showing per-epoch metrics including loss, gradient norm, learning rate and memory usage. Running on 4 HPUs with LoRA fine-tuning of Llama-2-70b-chat-hf model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n(RayTrainWorker pid=123181) {'loss': 1.8051, 'grad_norm': 0.6015625, 'learning_rate': 9.938441702975689e-05, 'epoch': 0.16, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.6754, 'grad_norm': 0.408203125, 'learning_rate': 9.567727288213005e-05, 'epoch': 0.32, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.568, 'grad_norm': 0.4453125, 'learning_rate': 8.885729807284856e-05, 'epoch': 0.48, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.4934, 'grad_norm': 0.4609375, 'learning_rate': 7.938926261462366e-05, 'epoch': 0.65, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.3965, 'grad_norm': 0.3515625, 'learning_rate': 6.7918397477265e-05, 'epoch': 0.81, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.3461, 'grad_norm': 0.34765625, 'learning_rate': 5.522642316338268e-05, 'epoch': 0.97, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.2924, 'grad_norm': 0.32421875, 'learning_rate': 4.2178276747988446e-05, 'epoch': 1.13, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.2643, 'grad_norm': 0.33203125, 'learning_rate': 2.9663167846209998e-05, 'epoch': 1.29, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.263, 'grad_norm': 0.318359375, 'learning_rate': 1.8533980447508137e-05, 'epoch': 1.45, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.2502, 'grad_norm': 0.275390625, 'learning_rate': 9.549150281252633e-06, 'epoch': 1.61, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.2161, 'grad_norm': 0.2734375, 'learning_rate': 3.3209786751399187e-06, 'epoch': 1.77, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=123181) {'loss': 1.2517, 'grad_norm': 0.294921875, 'learning_rate': 2.7390523158633554e-07, 'epoch': 1.94, 'memory_allocated (GB)': 13.64, 'max_memory_allocated (GB)': 48.92, 'total_memory_available (GB)': 94.62}\n```\n\n----------------------------------------\n\nTITLE: Pinned Python Dependencies with Integrity Hashes\nDESCRIPTION: This snippet shows pinned Python package dependencies with their SHA-256 hashes for security verification. The file lists packages like websockets, xformers, and xgrammar with their specific versions and platform constraints.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_16\n\nLANGUAGE: plain\nCODE:\n```\n--hash=sha256:5569fc7f967429d4bc87e355cdfdcee6aabe4b620801e2cf5805ea245c06097c \\\n    --hash=sha256:68dce92b29575dda0f8d30c11742a8e2b9b8ec768ae414b54f7453f27bdf9545 \\\n    --hash=sha256:79c533ff593db861ae23436541f481ec896ee3da4e5db8962429b441bbaae16e \\\n    --hash=sha256:7f3920b1285a7d3ce898e303d84791b7bf40d57b7695ad549dc04e6a44c9f120 \\\n    --hash=sha256:91633e64712df3051ca454ca7d1b976baf842d7a3640b87622b323c55f3345e7 \\\n    --hash=sha256:945be0baa3e2440151eb3718fd8846751e8b51d8de7b884c90b17d271d34cae8 \\\n    --hash=sha256:9afd0d69429172c796164fd7fe8e821ade9be983f51c659a38da3faaaaac44dc \\\n    --hash=sha256:9c75eff897786ee262c9f17a48886f4e98e6cfd335e011c591c305e5d083c056 \\\n    --hash=sha256:b538014a87f94d92f98f34d3e6d2635478e6be6423a9ea53e4dd96210065e193 \\\n    --hash=sha256:b6577b8c6c8701ba8642ea9335a129836347894b666dd1ec2226830e263909d3 \\\n    --hash=sha256:c0376deac92377817e4fb8f347bf559b7d44ff556d9bc6f6208dd3f79f104aaf \\\n    --hash=sha256:cae3dde0b4b2078f31527acff6f486e23abed307ba4d3932466ba7cdd5ecec79 \\\n    --hash=sha256:cb5d45c4143c1dd60f98a16187fd123eda7248f84ef22244818c18d531a249d1 \\\n    --hash=sha256:d9b073073e048081e502b6c6b0b88714c026a1a4c890569238d04aca5f9ca74b \\\n    --hash=sha256:fac19dc9cbc34052394dbe81e149411a62e71999c0a19e1e09ce537867f95ae0\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   uvicorn\n    #   vllm\nwebsockets==15.0 \\\n    --hash=sha256:0e389efe46ccb25a1f93d08c7a74e8123a2517f7b7458f043bd7529d1a63ffeb \\\n    --hash=sha256:0f2205cdb444a42a7919690238fb5979a05439b9dbb73dd47c863d39640d85ab \\\n    --hash=sha256:10552fed076757a70ba2c18edcbc601c7637b30cdfe8c24b65171e824c7d6081 \\\n    --hash=sha256:110a847085246ab8d4d119632145224d6b49e406c64f1bbeed45c6f05097b680 \\\n    --hash=sha256:1206432cc6c644f6fc03374b264c5ff805d980311563202ed7fef91a38906276 \\\n    --hash=sha256:1657a9eecb29d7838e3b415458cc494e6d1b194f7ac73a34aa55c6fb6c72d1f3 \\\n    --hash=sha256:17f2854c6bd9ee008c4b270f7010fe2da6c16eac5724a175e75010aacd905b31 \\\n    --hash=sha256:190bc6ef8690cd88232a038d1b15714c258f79653abad62f7048249b09438af3 \\\n    --hash=sha256:1caf951110ca757b8ad9c4974f5cac7b8413004d2f29707e4d03a65d54cedf2b \\\n    --hash=sha256:24d5333a9b2343330f0f4eb88546e2c32a7f5c280f8dd7d3cc079beb0901781b \\\n    --hash=sha256:26ba70fed190708551c19a360f9d7eca8e8c0f615d19a574292b7229e0ae324c \\\n    --hash=sha256:2bd8ef197c87afe0a9009f7a28b5dc613bfc585d329f80b7af404e766aa9e8c7 \\\n    --hash=sha256:2ea4f210422b912ebe58ef0ad33088bc8e5c5ff9655a8822500690abc3b1232d \\\n    --hash=sha256:30cff3ef329682b6182c01c568f551481774c476722020b8f7d0daacbed07a17 \\\n    --hash=sha256:327adab7671f3726b0ba69be9e865bba23b37a605b585e65895c428f6e47e766 \\\n    --hash=sha256:32e02a2d83f4954aa8c17e03fe8ec6962432c39aca4be7e8ee346b05a3476904 \\\n    --hash=sha256:37d66646f929ae7c22c79bc73ec4074d6db45e6384500ee3e0d476daf55482a9 \\\n    --hash=sha256:3a302241fbe825a3e4fe07666a2ab513edfdc6d43ce24b79691b45115273b5e7 \\\n    --hash=sha256:3abd670ca7ce230d5a624fd3d55e055215d8d9b723adee0a348352f5d8d12ff4 \\\n    --hash=sha256:4095a1f2093002c2208becf6f9a178b336b7572512ee0a1179731acb7788e8ad \\\n    --hash=sha256:45535fead66e873f411c1d3cf0d3e175e66f4dd83c4f59d707d5b3e4c56541c4 \\\n    --hash=sha256:45d464622314973d78f364689d5dbb9144e559f93dca11b11af3f2480b5034e1 \\\n    --hash=sha256:4f7290295794b5dec470867c7baa4a14182b9732603fd0caf2a5bf1dc3ccabf3 \\\n    --hash=sha256:4ff380aabd7a74a42a760ee76c68826a8f417ceb6ea415bd574a035a111fd133 \\\n    --hash=sha256:51ffd53c53c4442415b613497a34ba0aa7b99ac07f1e4a62db5dcd640ae6c3c3 \\\n    --hash=sha256:5294fcb410ed0a45d5d1cdedc4e51a60aab5b2b3193999028ea94afc2f554b05 \\\n    --hash=sha256:56e3efe356416bc67a8e093607315951d76910f03d2b3ad49c4ade9207bf710d \\\n    --hash=sha256:5d3cc75ef3e17490042c47e0523aee1bcc4eacd2482796107fd59dd1100a44bc \\\n    --hash=sha256:5e6ee18a53dd5743e6155b8ff7e8e477c25b29b440f87f65be8165275c87fef0 \\\n    --hash=sha256:67a04754d121ea5ca39ddedc3f77071651fb5b0bc6b973c71c515415b44ed9c5 \\\n    --hash=sha256:7394c0b7d460569c9285fa089a429f58465db930012566c03046f9e3ab0ed181 \\\n    --hash=sha256:789c43bf4a10cd067c24c321238e800b8b2716c863ddb2294d2fed886fa5a689 \\\n    --hash=sha256:7ac67b542505186b3bbdaffbc303292e1ee9c8729e5d5df243c1f20f4bb9057e \\\n    --hash=sha256:8561c48b0090993e3b2a54db480cab1d23eb2c5735067213bb90f402806339f5 \\\n    --hash=sha256:86bfb52a9cfbcc09aba2b71388b0a20ea5c52b6517c0b2e316222435a8cdab72 \\\n    --hash=sha256:8711682a629bbcaf492f5e0af72d378e976ea1d127a2d47584fa1c2c080b436b \\\n    --hash=sha256:89da58e4005e153b03fe8b8794330e3f6a9774ee9e1c3bd5bc52eb098c3b0c4f \\\n    --hash=sha256:89f72524033abbfde880ad338fd3c2c16e31ae232323ebdfbc745cbb1b3dcc03 \\\n    --hash=sha256:8bf1ab71f9f23b0a1d52ec1682a3907e0c208c12fef9c3e99d2b80166b17905f \\\n    --hash=sha256:8d7bbbe2cd6ed80aceef2a14e9f1c1b61683194c216472ed5ff33b700e784e37 \\\n    --hash=sha256:94c4a9b01eede952442c088d415861b0cf2053cbd696b863f6d5022d4e4e2453 \\\n    --hash=sha256:98dcf978d4c6048965d1762abd534c9d53bae981a035bfe486690ba11f49bbbb \\\n    --hash=sha256:a4cc73a6ae0a6751b76e69cece9d0311f054da9b22df6a12f2c53111735657c8 \\\n    --hash=sha256:a9f8e33747b1332db11cf7fcf4a9512bef9748cb5eb4d3f7fbc8c30d75dc6ffc \\\n    --hash=sha256:ace960769d60037ca9625b4c578a6f28a14301bd2a1ff13bb00e824ac9f73e55 \\\n    --hash=sha256:ae721bcc8e69846af00b7a77a220614d9b2ec57d25017a6bbde3a99473e41ce8 \\\n    --hash=sha256:aea01f40995fa0945c020228ab919b8dfc93fc8a9f2d3d705ab5b793f32d9e99 \\\n    --hash=sha256:b499caef4bca9cbd0bd23cd3386f5113ee7378094a3cb613a2fa543260fe9506 \\\n    --hash=sha256:b89504227a5311610e4be16071465885a0a3d6b0e82e305ef46d9b064ce5fb72 \\\n    --hash=sha256:bd66b4865c8b853b8cca7379afb692fc7f52cf898786537dfb5e5e2d64f0a47f \\\n    --hash=sha256:bfcd3acc1a81f106abac6afd42327d2cf1e77ec905ae11dc1d9142a006a496b6 \\\n    --hash=sha256:c24ba103ecf45861e2e1f933d40b2d93f5d52d8228870c3e7bf1299cd1cb8ff1 \\\n    --hash=sha256:c348abc5924caa02a62896300e32ea80a81521f91d6db2e853e6b1994017c9f6 \\\n    --hash=sha256:c53f97032b87a406044a1c33d1e9290cc38b117a8062e8a8b285175d7e2f99c9 \\\n    --hash=sha256:c7cd4b1015d2f60dfe539ee6c95bc968d5d5fad92ab01bb5501a77393da4f596 \\\n    --hash=sha256:c86dc2068f1c5ca2065aca34f257bbf4f78caf566eb230f692ad347da191f0a1 \\\n    --hash=sha256:c8c5c8e1bac05ef3c23722e591ef4f688f528235e2480f157a9cfe0a19081375 \\\n    --hash=sha256:ca36151289a15b39d8d683fd8b7abbe26fc50be311066c5f8dcf3cb8cee107ab \\\n    --hash=sha256:cc8821a03bcfb36e4e4705316f6b66af28450357af8a575dc8f4b09bf02a3dee \\\n    --hash=sha256:cccc18077acd34c8072578394ec79563664b1c205f7a86a62e94fafc7b59001f \\\n    --hash=sha256:d2244d8ab24374bed366f9ff206e2619345f9cd7fe79aad5225f53faac28b6b1 \\\n    --hash=sha256:d4c22992e24f12de340ca5f824121a5b3e1a37ad4360b4e1aaf15e9d1c42582d \\\n    --hash=sha256:dd24c4d256558429aeeb8d6c24ebad4e982ac52c50bc3670ae8646c181263965 \\\n    --hash=sha256:e413352a921f5ad5d66f9e2869b977e88d5103fc528b6deb8423028a2befd842 \\\n    --hash=sha256:ee06405ea2e67366a661ed313e14cf2a86e84142a3462852eb96348f7219cee3 \\\n    --hash=sha256:f83eca8cbfd168e424dfa3b3b5c955d6c281e8fc09feb9d870886ff8d03683c7 \\\n    --hash=sha256:fb915101dfbf318486364ce85662bb7b020840f68138014972c08331458d41f3 \\\n    --hash=sha256:ffc02b159b65c05f2ed9ec176b715b66918a674bd4daed48a9a7a590dd4be1aa \\\n    --hash=sha256:ffc5ae23ada6515f31604f700009e2df90b091b67d463a8401c1d8a37f76c1d7\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   uvicorn\nxformers==0.0.29.post2 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:0d0eb14db56cf08ec3fb9cb36ed5e98de1303411571539ca4dc080c5861e2744 \\\n    --hash=sha256:2eed954ce0491d379f19ea38796027d367e259a90d1fcc9f4166331c1c27ce87 \\\n    --hash=sha256:6ca3d1a6db6f2abff25c1154adee96987f77f4dfd5141771805afa5fc13e9395 \\\n    --hash=sha256:a3ddb47abce3810d3928e8f48b290c0423c7939764a217c2b35ac8124a3cf641 \\\n    --hash=sha256:bbf0e9505f6b2e2b7738eeb3c22e94c45e6297fbdae66626febb0dbfe28c5050 \\\n    --hash=sha256:c3e19aa15de0242c27096e2cb72636123c4475096a9397f4f331eb08c67d193b \\\n    --hash=sha256:eb1db57f05b595ed9f1d0f8cc83a8e54d2c0737a16982238a01e93bdd0f2a4f5 \\\n    --hash=sha256:eb73626de82953fa7673a19ddcff3ef37d5de5f4e3230fe18dfd99c52460c55d \\\n    --hash=sha256:f4379dda52efd4e7beb9a3bdae183f6c9857a77f04d58ed2e000ce92b05f5d92\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   vllm\nxgrammar==0.1.16 ; platform_machine == 'aarch64' or platform_machine == 'x86_64' \\\n    --hash=sha256:027c6937748d22b1c2db2850c99e3cdca6b9532817ad2013b6fb646f07cc8448 \\\n    --hash=sha256:04e361b22926f431ae82fad3c4463e0d3c8f653fe15ebe3d7059edf73e348565 \\\n    --hash=sha256:19807de8dbe3772f8b6aaa979541183fe6c6bafa3680d52d8289c7b4439eff2c \\\n    --hash=sha256:1d898e3dc04ea7d81a0e9cd10b632c22707fcc9ce02d7be3c0aa6c38067af97f \\\n    --hash=sha256:2301413a374f11add07843dfb49050f27beae89a4be7e0ffd454c08cf302412c \\\n    --hash=sha256:23d016b09b22ad77a0cc7de49e2a7152d8cd221734aa6d41b5fd7827dfb1a4d3 \\\n    --hash=sha256:2d75e6501f55368462b4d61ce0fb6a65c587782faa7319f48f49a8c444b4245f \\\n    --hash=sha256:46e52514479056418495d68413c2ea18798b95dcdc36d25f48b281ca7d203ce1 \\\n    --hash=sha256:4ddd5128a82d0a9c800c03df25c610368ca630704ad20a6bb7a3629f24ced442 \\\n    --hash=sha256:51565f8e6eb17fefe7ce90aa4598cf8216b4ee801a33d58d8439242d3d18cfa6 \\\n    --hash=sha256:54a3d4386b538fe0a6b6399de2592dd57756e31c1def812cf9653b8f91f827d8 \\\n    --hash=sha256:5bf4754f53c7d3fb1a5b011d8948e90c6cb022caff73b0f74cc70cb319ec728a \\\n    --hash=sha256:60967ad8435448c183ad911c9c5252e5cb0b032b37f86dcfc16cdd07c35954f6 \\\n    --hash=sha256:773ae9b9d34456a3ade17d2c4e363eb149419d7f84bb014921c223290854aaf3 \\\n    --hash=sha256:854e2b23d0099c590cbc8bb83ab7de7d7ba3acb8aab65d64fa1436af0639f80c \\\n    --hash=sha256:90fae6c9256753f9816aacddf8c37176eded8b4164024d28d6342ea4b9182ae9 \\\n    --hash=sha256:97322341c29185b31482459325160dc2fb3eeb99bdf52cfeb57ae61a7e76c9d1 \\\n    --hash=sha256:ab1850ffb1615c1370e4ba3d4dafb2c116a03a06683b9fcf309982c49b8c2f87 \\\n```\n\n----------------------------------------\n\nTITLE: Specifying propcache Package Dependency with Extensive Hash Verification\nDESCRIPTION: This snippet shows the start of the propcache package dependency specification with version 0.3.0 and numerous SHA-256 hashes for verification. The large number of hashes suggests this package is available in multiple formats or platforms.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_37\n\nLANGUAGE: plaintext\nCODE:\n```\npropcache==0.3.0 \\\n    --hash=sha256:02df07041e0820cacc8f739510078f2aadcfd3fc57eaeeb16d5ded85c872c89e \\\n    --hash=sha256:03acd9ff19021bd0567582ac88f821b66883e158274183b9e5586f678984f8fe \\\n    --hash=sha256:03c091bb752349402f23ee43bb2bff6bd80ccab7c9df6b88ad4322258d6960fc \\\n    --hash=sha256:07700939b2cbd67bfb3b76a12e1412405d71019df00ca5697ce75e5ef789d829 \\\n    --hash=sha256:0c3e893c4464ebd751b44ae76c12c5f5c1e4f6cbd6fbf67e3783cd93ad221863 \\\n    --hash=sha256:119e244ab40f70a98c91906d4c1f4c5f2e68bd0b14e7ab0a06922038fae8a20f \\\n    --hash=sha256:11ae6a8a01b8a4dc79093b5d3ca2c8a4436f5ee251a9840d7790dccbd96cb649 \\\n    --hash=sha256:15010f29fbed80e711db272909a074dc79858c6d28e2915704cfc487a8ac89c6 \\\n    --hash=sha256:19d36bb351ad5554ff20f2ae75f88ce205b0748c38b146c75628577020351e3c \\\n    --hash=sha256:1c8f7d896a16da9455f882870a507567d4f58c53504dc2d4b1e1d386dfe4588a \\\n    --hash=sha256:2383a17385d9800b6eb5855c2f05ee550f803878f344f58b6e194de08b96352c \\\n    --hash=sha256:24c04f8fbf60094c531667b8207acbae54146661657a1b1be6d3ca7773b7a545 \\\n    --hash=sha256:2578541776769b500bada3f8a4eeaf944530516b6e90c089aa368266ed70c49e \\\n    --hash=sha256:26a67e5c04e3119594d8cfae517f4b9330c395df07ea65eab16f3d559b7068fe \\\n    --hash=sha256:2b975528998de037dfbc10144b8aed9b8dd5a99ec547f14d1cb7c5665a43f075 \\\n    --hash=sha256:2d15bc27163cd4df433e75f546b9ac31c1ba7b0b128bfb1b90df19082466ff57 \\\n    --hash=sha256:2d913d36bdaf368637b4f88d554fb9cb9d53d6920b9c5563846555938d5450bf \\\n    --hash=sha256:3302c5287e504d23bb0e64d2a921d1eb4a03fb93a0a0aa3b53de059f5a5d737d \\\n    --hash=sha256:36ca5e9a21822cc1746023e88f5c0af6fce3af3b85d4520efb1ce4221bed75cc \\\n    --hash=sha256:3b812b3cb6caacd072276ac0492d249f210006c57726b6484a1e1805b3cfeea0 \\\n    --hash=sha256:3c6ec957025bf32b15cbc6b67afe233c65b30005e4c55fe5768e4bb518d712f1 \\\n    --hash=sha256:41de3da5458edd5678b0f6ff66691507f9885f5fe6a0fb99a5d10d10c0fd2d64 \\\n    --hash=sha256:42924dc0c9d73e49908e35bbdec87adedd651ea24c53c29cac103ede0ea1d340 \\\n    --hash=sha256:4544699674faf66fb6b4473a1518ae4999c1b614f0b8297b1cef96bac25381db \\\n    --hash=sha256:46ed02532cb66612d42ae5c3929b5e98ae330ea0f3900bc66ec5f4862069519b \\\n    --hash=sha256:49ea05212a529c2caffe411e25a59308b07d6e10bf2505d77da72891f9a05641 \\\n    --hash=sha256:4fa0e7c9c3cf7c276d4f6ab9af8adddc127d04e0fcabede315904d2ff76db626 \\\n    --hash=sha256:507c5357a8d8b4593b97fb669c50598f4e6cccbbf77e22fa9598aba78292b4d7 \\\n    --hash=sha256:549722908de62aa0b47a78b90531c022fa6e139f9166be634f667ff45632cc92 \\\n    --hash=sha256:58e6d2a5a7cb3e5f166fd58e71e9a4ff504be9dc61b88167e75f835da5764d07 \\\n    --hash=sha256:5a16167118677d94bb48bfcd91e420088854eb0737b76ec374b91498fb77a70e \\\n    --hash=sha256:5d62c4f6706bff5d8a52fd51fec6069bef69e7202ed481486c0bc3874912c787 \\\n    --hash=sha256:5fa159dcee5dba00c1def3231c249cf261185189205073bde13797e57dd7540a \\\n    --hash=sha256:6032231d4a5abd67c7f71168fd64a47b6b451fbcb91c8397c2f7610e67683810 \\\n    --hash=sha256:63f26258a163c34542c24808f03d734b338da66ba91f410a703e505c8485791d \\\n    --hash=sha256:65a37714b8ad9aba5780325228598a5b16c47ba0f8aeb3dc0514701e4413d7c0 \\\n    --hash=sha256:67054e47c01b7b349b94ed0840ccae075449503cf1fdd0a1fdd98ab5ddc2667b \\\n```\n\n----------------------------------------\n\nTITLE: Examples of Conflict Resolution\nDESCRIPTION: This snippet outlines two examples showcasing how Ray handles conflicts in environment settings when merging runtime environments specified by both a job and a driver.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\"pip\": [\"requests\", \"chess\"], \"env_vars\": {\"C\": \"a\", \"B\": \"b\"}}\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies: grpcio Configuration\nDESCRIPTION: Definition of the grpcio package dependency with version 1.66.2 and a large number of corresponding SHA256 hashes. This contains only a partial list of the hashes as the full list appears to be truncated in the source.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\ngrpcio==1.66.2 \\\n    --hash=sha256:02697eb4a5cbe5a9639f57323b4c37bcb3ab2d48cec5da3dc2f13334d72790dd \\\n    --hash=sha256:03b0b307ba26fae695e067b94cbb014e27390f8bc5ac7a3a39b7723fed085604 \\\n    --hash=sha256:05bc2ceadc2529ab0b227b1310d249d95d9001cd106aa4d31e8871ad3c428d73 \\\n    --hash=sha256:06de8ec0bd71be123eec15b0e0d457474931c2c407869b6c349bd9bed4adbac3 \\\n    --hash=sha256:0be4e0490c28da5377283861bed2941d1d20ec017ca397a5df4394d1c31a9b50 \\\n    --hash=sha256:12fda97ffae55e6526825daf25ad0fa37483685952b5d0f910d6405c87e3adb6 \\\n    --hash=sha256:1caa38fb22a8578ab8393da99d4b8641e3a80abc8fd52646f1ecc92bcb8dee34 \\\n    --hash=sha256:2018b053aa15782db2541ca01a7edb56a0bf18c77efed975392583725974b249 \\\n    --hash=sha256:20657d6b8cfed7db5e11b62ff7dfe2e12064ea78e93f1434d61888834bc86d75 \\\n    --hash=sha256:2335c58560a9e92ac58ff2bc5649952f9b37d0735608242973c7a8b94a6437d8 \\\n    --hash=sha256:31fd163105464797a72d901a06472860845ac157389e10f12631025b3e4d0453 \\\n    --hash=sha256:38b68498ff579a3b1ee8f93a05eb48dc2595795f2f62716e797dc24774c1aaa8 \\\n    --hash=sha256:3b00efc473b20d8bf83e0e1ae661b98951ca56111feb9b9611df8efc4fe5d55d \\\n    --hash=sha256:3ed71e81782966ffead60268bbda31ea3f725ebf8aa73634d5dda44f2cf3fb9c \\\n    --hash=sha256:45a3d462826f4868b442a6b8fdbe8b87b45eb4f5b5308168c156b21eca43f61c \\\n    --hash=sha256:49f0ca7ae850f59f828a723a9064cadbed90f1ece179d375966546499b8a2c9c \\\n    --hash=sha256:4e504572433f4e72b12394977679161d495c4c9581ba34a88d843eaf0f2fbd39 \\\n    --hash=sha256:4ea1d062c9230278793820146c95d038dc0f468cbdd172eec3363e42ff1c7d01 \\\n    --hash=sha256:563588c587b75c34b928bc428548e5b00ea38c46972181a4d8b75ba7e3f24231 \\\n    --hash=sha256:6001e575b8bbd89eee11960bb640b6da6ae110cf08113a075f1e2051cc596cae \\\n    --hash=sha256:66a0cd8ba6512b401d7ed46bb03f4ee455839957f28b8d61e7708056a806ba6a \\\n    --hash=sha256:6851de821249340bdb100df5eacfecfc4e6075fa85c6df7ee0eb213170ec8e5d \\\n    --hash=sha256:728bdf36a186e7f51da73be7f8d09457a03061be848718d0edf000e709418987 \\\n    --hash=sha256:73e3b425c1e155730273f73e419de3074aa5c5e936771ee0e4af0814631fb30a \\\n    --hash=sha256:73fc8f8b9b5c4a03e802b3cd0c18b2b06b410d3c1dcbef989fdeb943bd44aff7 \\\n    --hash=sha256:78fa51ebc2d9242c0fc5db0feecc57a9943303b46664ad89921f5079e2e4ada7 \\\n\n```\n\n----------------------------------------\n\nTITLE: Installing Bazel on Windows\nDESCRIPTION: Script to install Bazel build tool, with special instructions for Windows users regarding PATH and BAZEL_SH configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Install Bazel.\nray/ci/env/install-bazel.sh\n# (Windows users: please manually place Bazel in your PATH, and point\n# BAZEL_SH to MSYS2's Bash: ``set BAZEL_SH=C:\\Program Files\\Git\\bin\\bash.exe``)\n```\n\n----------------------------------------\n\nTITLE: Creating a Kind Cluster for Testing\nDESCRIPTION: Command to create a local Kubernetes cluster using Kind for testing the py-spy profiling setup with KubeRay.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/pyspy.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkind create cluster\n```\n\n----------------------------------------\n\nTITLE: Inspecting PodGroup Status in Kubernetes\nDESCRIPTION: Command and output showing PodGroup configuration and status after resources become available\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl get podgroup ray-test-cluster-1-pg -o yaml\n\n# apiVersion: scheduling.volcano.sh/v1beta1\n# kind: PodGroup\n# metadata:\n#   creationTimestamp: \"2022-12-01T04:48:18Z\"\n#   generation: 9\n#   name: ray-test-cluster-1-pg\n#   namespace: test\n#   ownerReferences:\n#   - apiVersion: ray.io/v1alpha1\n#     blockOwnerDeletion: true\n#     controller: true\n#     kind: RayCluster\n#     name: test-cluster-1\n#     uid: b3cf83dc-ef3a-4bb1-9c42-7d2a39c53358\n#   resourceVersion: \"4428864\"\n#   uid: 9087dd08-8f48-4592-a62e-21e9345b0872\n# spec:\n#   minMember: 3\n#   minResources:\n#     cpu: \"3\"\n#     memory: 4Gi\n#   queue: kuberay-test-queue\n# status:\n#   conditions:\n#   - lastTransitionTime: \"2022-12-01T04:54:04Z\"\n#     message: '3/3 tasks in gang unschedulable: pod group is not ready, 3 Pending,\n#       3 minAvailable; Pending: 3 Undetermined'\n#     reason: NotEnoughResources\n#     status: \"True\"\n#     transitionID: db90bbf0-6845-441b-8992-d0e85f78db77\n#     type: Unschedulable\n#   - lastTransitionTime: \"2022-12-01T04:55:10Z\"\n#     reason: tasks in the gang are ready to be scheduled\n#     status: \"True\"\n#     transitionID: 72bbf1b3-d501-4528-a59d-479504f3eaf5\n#     type: Scheduled\n#   phase: Running\n#   running: 3\n```\n\n----------------------------------------\n\nTITLE: Measuring Ray Task Invocation Overhead\nDESCRIPTION: A utility snippet for measuring the overhead of Ray's remote task invocation. This can help determine the minimum task size needed to benefit from parallelization with Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/tips-for-first-time.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef no_work(x):\n    return x\n\nstart = time.time()\nnum_calls = 1000\n[ray.get(no_work.remote(x)) for x in range(num_calls)]\n```\n\n----------------------------------------\n\nTITLE: Installing Linting Dependencies and Running Pre-commit\nDESCRIPTION: Commands to set up and run code linting and formatting checks\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install -c python/requirements_compiled.txt -r python/requirements/lint-requirements.txt\npip install -U pre-commit==3.5.0\npre-commit install\npre-commit run ruff -a\n```\n\n----------------------------------------\n\nTITLE: Cleanup Commands for Kubernetes Resources\nDESCRIPTION: Commands to clean up the Ray cluster, KubeRay operator, and associated resources after completing the training workload.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/gpu-training-example.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete raycluster raycluster\n\n# Please make sure the ray cluster has already been removed before delete the operator.\nhelm uninstall kuberay-operator\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Hash Verification\nDESCRIPTION: A requirements file listing Python package dependencies with specific versions and SHA256 hash values for verification. The file includes packages like pygments, pynvml, pyopenssl, pyparsing, pytest, and various other dependencies needed for the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_40\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:29d2c342c4bc01b88402d60189f3df065fb0dda3654744d5a165a5288a657368 \\\n    --hash=sha256:2e203fdf807ac7e12ab59ca2bfcabb38c7cf0b33c41efeb00f8e5da1d86af480 \\\n    --hash=sha256:33e3d65a85a2a4a0dc3b092b938a4062b1a05f3a9abde65ea93b233bca0e03f2 \\\n    --hash=sha256:374a5e5049eda9e0a44c696c7ade3ff355f06b1fe0bb945ea3cac2bc336478a2 \\\n    --hash=sha256:37b0fe330e4a58d3c58b24d91d1eb102aeec675a3db4c292ec3928ecd892a9a6 \\\n    --hash=sha256:3d5639516376dce1940ea36edf408c554475369f5da2abd45d44621cb616f769 \\\n    --hash=sha256:42c6dcb030aefb668a2b7009c85b27f90e51e6a3b4d5c9bc4c57631292015b0d \\\n    --hash=sha256:4a7cd62e831afe623fbb7aabbb4fe583212115b3ef38a9f6b71869ba644624a2 \\\n    --hash=sha256:4ba762ed58e8d68657fc1281e9bb72e1c3e79cc5d464be146e260c541ec12d84 \\\n    --hash=sha256:4fc714bdbfb534f94034efaa6eadd74e5b93c8fa6315565a222f7b6f42ca1166 \\\n    --hash=sha256:4ffa2ebd4c8530079140dd2d7f794a9d9a73cbb8e9d59ffe24c63436efa8f271 \\\n    --hash=sha256:5a1504ad17ba4210df3a045132a7baeeba5a200e930f57512ee02909fc5c4cb5 \\\n    --hash=sha256:5c364564d17da23db1106787675fc7af45f2f7b58b4173bfdd105564e132e6fb \\\n    --hash=sha256:5e11661ce0fd30a6790e8bcdf263b9ec5988e95e63cf901972107efc49218b13 \\\n    --hash=sha256:5f54b118ce5de9ac21c363d9b3caa6c800341e8c47a508787e5868c6b79c9323 \\\n    --hash=sha256:5f5ff8d839f4566a474a969508fe1c5e59c31c80d9e140566f9a37bba7b8d556 \\\n    --hash=sha256:61817945f2fe7d166e75fbfb28004034b48e44878177fc54d81688e7b85a3665 \\\n    --hash=sha256:624e278a7d29b6445e4e813af92af37820fafb6dcc55c012c834f9e26f9aaaef \\\n    --hash=sha256:63e46b3169866bd62849936de036f901a9356e36376079b05efa83caeaa02ceb \\\n    --hash=sha256:6531b7ca5f951d663c339002e91aaebda765ec7d61b7d1e3991051906ddde119 \\\n    --hash=sha256:68665f4c17edcceecc112dfed5dbe6f92261fb9d6054b47d01bf6371a6196126 \\\n    --hash=sha256:696dd8d674d6ce621ab9d45b205df149399e4bb9aa34102c970b721554828510 \\\n    --hash=sha256:6f783e0ec4803c787bcea93e13e9932edab72068f68ecffdf86a99fd5918878b \\\n    --hash=sha256:723314c1d51722ab28bfcd5240d858512ffd3116449c557a1336cbe3919beb87 \\\n    --hash=sha256:74b9127ffea03643e998e0c5ad9bd3811d3dac8c676e47db17b0ee7c3c3bf35f \\\n    --hash=sha256:7530e201d10d7d14abce4fb54cfe5b94a0aefc87da539d0346a484ead376c3cc \\\n    --hash=sha256:77733e3892bb0a7fa797826361ce8a9184d25c8dffaec60b7ffe928153680ba8 \\\n    --hash=sha256:78ddaaa81421a29574a682b3179d4cf9e6d405a09b99d93ddcf7e5239c742e21 \\\n    --hash=sha256:7c9129eb40958b3d4500fa2467e6a83356b3b61bfff1b414c7361d9220f9ae8f \\\n    --hash=sha256:7d32706badfe136888bdea71c0def994644e09fff0bfe47441deaed8e96fdbc6 \\\n    --hash=sha256:81965a16b675b35e1d09dd14df53f190f9129c0202356ed44ab2728b1c905658 \\\n    --hash=sha256:8394d940e5d400d04cad4f75c0598665cbb81aecefaca82ca85bd28264af7f9b \\\n    --hash=sha256:86d2f57d3e1379a9525c5ab067b27dbb8a0642fb5d454e17a9ac434f9ce523e3 \\\n    --hash=sha256:883a91b5dd7d26492ff2f04f40fbb652de40fcc0afe07e8129e8ae779c2110eb \\\n    --hash=sha256:88ad334a15b32a791ea935af224b9de1bf99bcd62fabf745d5f3442199d86d59 \\\n    --hash=sha256:9261d3ce84fa1d38ed649c3638feefeae23d32ba9182963e465d58d62203bd24 \\\n    --hash=sha256:97df63000f4fea395b2824da80e169731088656d1818a11b95f3b173747b6cd9 \\\n    --hash=sha256:98d134c954828488b153d88ba1f34e14259284f256180ce659e8d83e9c05eaa3 \\\n    --hash=sha256:996a38a83508c54c78a5f41456b0103c30508fed9abcad0a59b876d7398f25fd \\\n    --hash=sha256:9a5bce9d23aac8f0cf0836ecfc033896aa8443b501c58d0602dbfd5bd5b37753 \\\n    --hash=sha256:9a6b5099eeec78827553827f4c6b8615978bb4b6a88e5d9b93eddf8bb6790f55 \\\n    --hash=sha256:9d18368b137c6295db49ce7218b1a9ba15c5bc254c96d7c9f9e924a9bc7825ad \\\n    --hash=sha256:a4fa4fc04dff799089689f4fd502ce7d59de529fc2f40a2c8836886c03e0175a \\\n    --hash=sha256:a5c7ba8ffb6d6f8f2ab08743be203654bb1aaa8c9dcb09f82ddd34eadb695605 \\\n    --hash=sha256:aea443fffa9fbe3af1a9ba721a87f926fe548d32cab71d188a6ede77d0ff244e \\\n    --hash=sha256:b10bd51f823d891193d4717448fab065733958bdb6a6b351967bd349d48d5c9b \\\n    --hash=sha256:ba1a0996f6c2773bd83e63f18914c1de3c9dd26d55f4ac302a7efe93fb8e7433 \\\n    --hash=sha256:bb2802e667b7051a1bebbfe93684841cc9351004e2badbd6411bf357ab8d5ac8 \\\n    --hash=sha256:cfdd16ab5e59fc31b5e906d1a3f666571abc367598e3e02c83403acabc092e07 \\\n    --hash=sha256:d06b0c8da4f16d1d1e352134427cb194a0a6e19ad5db9161bf32b2113409e728 \\\n    --hash=sha256:d0776dea117cf5272382634bd2a5c1b6eb16767c223c6a5317cd3e2a757c61a0 \\\n    --hash=sha256:d18ca8148bebe1b0a382a27a8ee60350091a6ddaf475fa05ef50dc35b5df6327 \\\n    --hash=sha256:d4488a93b071c04dc20f5cecc3631fc78b9789dd72483ba15d423b5b3689b555 \\\n    --hash=sha256:d5f7a395a8cf1621939692dba2a6b6a830efa6b3cee787d82c7de1ad2930de64 \\\n    --hash=sha256:d7a80d21d613eec45e3d41eb22f8f94ddc758a6c4720842dc74c0581f54993d6 \\\n    --hash=sha256:d97683ddee4723ae8c95d1eddac7c192e8c552da0c73a925a89fa8649bf13eea \\\n    --hash=sha256:dcedcd19a557e182628afa1d553c3895a9f825b936415d0dbd3cd0bbcfd29b4b \\\n    --hash=sha256:de6d1d1b9e5101508cb37ab0d972357cac5235f5c6533d1071964c47139257df \\\n    --hash=sha256:df49e7a0861a8c36d089c1ed57d308623d60416dab2647a4a17fe050ba85de0e \\\n    --hash=sha256:df933278128ea1cd77772673c73954e53a1c95a4fdf41eef97c2b779271bd0bd \\\n    --hash=sha256:e08277a400de01bc72436a0ccd02bdf596631411f592ad985dcee21445bd0068 \\\n    --hash=sha256:e38e63e6f3d1cec5a27e0afe90a085af8b6806ee208b33030e65b6516353f1a3 \\\n    --hash=sha256:e55541f756f9b3ee346b840103f32779c695a19826a4c442b7954550a0972040 \\\n    --hash=sha256:ec4e55f79b1c4ffb2eecd8a0cfba9955a2588497d96851f4c8f99aa4a1d39b12 \\\n    --hash=sha256:ed1a53de42fbe34853ba90513cea21673481cd81ed1be739f7f2efb931b24916 \\\n    --hash=sha256:ed541d70698978a20eb63d8c5d72f2cc6d7079d9d90f6b50bad07826f1320f5f \\\n    --hash=sha256:f09e2ff1f17c2b51f2bc76d1cc33da96298f0a036a137f5440ab3ec5360b624f \\\n    --hash=sha256:f220b0eea5965dec25480b6333c788fb72ce5f9129e8759ef876a1d805d00801 \\\n    --hash=sha256:f3e0da4ebaef65158d4dfd7d3678aad692f7666877df0002b8a522cdf088f231 \\\n    --hash=sha256:f455ee30a9d61d3e1a15abd5068827773d6e4dc513e795f380cdd59932c782d5 \\\n    --hash=sha256:f5ef8f42bec47f21d07668a043f077d507e5bf4e668d5c6dfe6aaba89de1a5b8 \\\n    --hash=sha256:f69a8e0b033b747bb3e36a44e7732f0c99f7edd5cea723d45bc0d6e95377ffee \\\n    --hash=sha256:ff02b6d461a6de369f07ec15e465a88895f3223eb75073ffea56b84d9331f607\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   pydantic\npygments==2.18.0 \\\n    --hash=sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199 \\\n    --hash=sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   ipython\n    #   nbconvert\n    #   rich\n    #   sphinx\npynvml==12.0.0 \\\n    --hash=sha256:299ce2451a6a17e6822d6faee750103e25b415f06f59abb8db65d30f794166f5 \\\n    --hash=sha256:fdff84b62a27dbe98e08e1a647eb77342bef1aebe0878bcd15e99a83fcbecb9e\n    # via -r python/requirements/llm/llm-test-requirements.txt\npyopenssl==24.2.1 \\\n    --hash=sha256:4247f0dbe3748d560dcbb2ff3ea01af0f9a1a001ef5f7c4c647956ed8cbf0e95 \\\n    --hash=sha256:967d5719b12b243588573f39b0c677637145c7a1ffedcd495a487e58177fbb8d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\npyparsing==3.1.1 \\\n    --hash=sha256:32c7c0b711493c72ff18a981d24f28aaf9c1fb7ed5e9667c9e84e3db623bdbfb \\\n    --hash=sha256:ede28a1a32462f5a9705e07aea48001a08f7cf81a021585011deba701581a0db\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   httplib2\npytest==7.4.4 \\\n    --hash=sha256:2cf0005922c6ace4a3e2ec8b4080eb0d9753fdc93107415332f50ce9e7994280 \\\n    --hash=sha256:b090cdf5ed60bf4c45261be03239c2c1c22df034fbffe691abe93cd80cea01d8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/base-test-requirements.txt\n    #   -r python/requirements/llm/llm-test-requirements.txt\n    #   pytest-aiohttp\n    #   pytest-asyncio\npytest-aiohttp==1.1.0 \\\n    --hash=sha256:147de8cb164f3fc9d7196967f109ab3c0b93ea3463ab50631e56438eab7b5adc \\\n    --hash=sha256:f39a11693a0dce08dd6c542d241e199dd8047a6e6596b2bcfa60d373f143456d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/base-test-requirements.txt\npytest-asyncio==0.17.2 \\\n    --hash=sha256:6d895b02432c028e6957d25fc936494e78c6305736e785d9fee408b1efbc7ff4 \\\n    --hash=sha256:e0fe5dbea40516b661ef1bcfe0bd9461c2847c4ef4bb40012324f2454fb7d56d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/base-test-requirements.txt\n    #   pytest-aiohttp\npython-dateutil==2.8.2 \\\n    --hash=sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86 \\\n    --hash=sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   arrow\n    #   botocore\n    #   jupyter-client\n    #   pandas\npython-dotenv==1.0.1 \\\n    --hash=sha256:e324ee90a023d808f1959c46bcbc04446a10ced277783dc6ee09987c37ec10ca \\\n    --hash=sha256:f7b63ef50f1b690dddf550d03497b66d609393b40b564ed0d674909a68ebf16a\n    # via uvicorn\npython-json-logger==2.0.7 \\\n    --hash=sha256:23e7ec02d34237c5aa1e29a070193a4ea87583bb4e7f8fd06d3de8264c4b2e1c \\\n    --hash=sha256:f380b826a991ebbe3de4d897aeec42760035ac760345e57b812938dc8b35e2bd\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   jupyter-events\n    #   vllm\npython-multipart==0.0.20 \\\n    --hash=sha256:8a62d3a8335e06589fe01f2a3e178cdcc632f3fbe0d492ad9ee0ec35aab1f104 \\\n    --hash=sha256:8dd0cab45b8e23064ae09147625994d090fa46f5b0d1e13af944c331a7fa9d13\n    # via fastapi\npytz==2022.7.1 \\\n    --hash=sha256:01a0681c4b9684a28304615eba55d1ab31ae00bf68ec157ec3708a8182dbbcd0 \\\n    --hash=sha256:78f4f37d8198e0627c5f1143240bb0206b8691d8d7ac6d78fee88b78733f8c4a\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   pandas\n```\n\n----------------------------------------\n\nTITLE: Specifying Pyarrow Package Version and Hashes\nDESCRIPTION: Defines the required version of the pyarrow package (14.0.2) along with its hash values for verification. This ensures consistent and secure installation of the pyarrow package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_17\n\nLANGUAGE: Text\nCODE:\n```\npyarrow==14.0.2 \\\n    --hash=sha256:059bd8f12a70519e46cd64e1ba40e97eae55e0cbe1695edd95384653d7626b23 \\\n    --hash=sha256:06ff1264fe4448e8d02073f5ce45a9f934c0f3db0a04460d0b01ff28befc3696 \\\n    --hash=sha256:1e6987c5274fb87d66bb36816afb6f65707546b3c45c44c28e3c4133c010a881 \\\n    --hash=sha256:209bac546942b0d8edc8debda248364f7f668e4aad4741bae58e67d40e5fcf75 \\\n    --hash=sha256:20e003a23a13da963f43e2b432483fdd8c38dc8882cd145f09f21792e1cf22a1 \\\n    --hash=sha256:22a768987a16bb46220cef490c56c671993fbee8fd0475febac0b3e16b00a10e \\\n    --hash=sha256:2cc61593c8e66194c7cdfae594503e91b926a228fba40b5cf25cc593563bcd07 \\\n    --hash=sha256:2dbba05e98f247f17e64303eb876f4a80fcd32f73c7e9ad975a83834d81f3fda \\\n    --hash=sha256:32356bfb58b36059773f49e4e214996888eeea3a08893e7dbde44753799b2a02 \\\n    --hash=sha256:36cef6ba12b499d864d1def3e990f97949e0b79400d08b7cf74504ffbd3eb025 \\\n    --hash=sha256:37c233ddbce0c67a76c0985612fef27c0c92aef9413cf5aa56952f359fcb7379 \\\n    --hash=sha256:3c0fa3bfdb0305ffe09810f9d3e2e50a2787e3a07063001dcd7adae0cee3601a \\\n    --hash=sha256:3f16111f9ab27e60b391c5f6d197510e3ad6654e73857b4e394861fc79c37200 \\\n    --hash=sha256:52809ee69d4dbf2241c0e4366d949ba035cbcf48409bf404f071f624ed313a2b \\\n    --hash=sha256:5c1da70d668af5620b8ba0a23f229030a4cd6c5f24a616a146f30d2386fec422 \\\n    --hash=sha256:63ac901baec9369d6aae1cbe6cca11178fb018a8d45068aaf5bb54f94804a866 \\\n    --hash=sha256:64df2bf1ef2ef14cee531e2dfe03dd924017650ffaa6f9513d7a1bb291e59c15 \\\n    --hash=sha256:66e986dc859712acb0bd45601229021f3ffcdfc49044b64c6d071aaf4fa49e98 \\\n    --hash=sha256:6dd4f4b472ccf4042f1eab77e6c8bce574543f54d2135c7e396f413046397d5a \\\n    --hash=sha256:75ee0efe7a87a687ae303d63037d08a48ef9ea0127064df18267252cfe2e9541 \\\n    --hash=sha256:76fc257559404ea5f1306ea9a3ff0541bf996ff3f7b9209fc517b5e83811fa8e \\\n    --hash=sha256:78ea56f62fb7c0ae8ecb9afdd7893e3a7dbeb0b04106f5c08dbb23f9c0157591 \\\n    --hash=sha256:87482af32e5a0c0cce2d12eb3c039dd1d853bd905b04f3f953f147c7a196915b \\\n    --hash=sha256:87e879323f256cb04267bb365add7208f302df942eb943c93a9dfeb8f44840b1 \\\n    --hash=sha256:a01d0052d2a294a5f56cc1862933014e696aa08cc7b620e8c0cce5a5d362e976 \\\n    --hash=sha256:a25eb2421a58e861f6ca91f43339d215476f4fe159eca603c55950c14f378cc5 \\\n    --hash=sha256:a51fee3a7db4d37f8cda3ea96f32530620d43b0489d169b285d774da48ca9785 \\\n    --hash=sha256:a898d134d00b1eca04998e9d286e19653f9d0fcb99587310cd10270907452a6b \\\n    --hash=sha256:b0c4a18e00f3a32398a7f31da47fefcd7a927545b396e1f15d0c85c2f2c778cd \\\n    --hash=sha256:ba9fe808596c5dbd08b3aeffe901e5f81095baaa28e7d5118e01354c64f22807 \\\n    --hash=sha256:c65bf4fd06584f058420238bc47a316e80dda01ec0dfb3044594128a6c2db794 \\\n    --hash=sha256:c87824a5ac52be210d32906c715f4ed7053d0180c1060ae3ff9b7e560f53f944 \\\n    --hash=sha256:e354fba8490de258be7687f341bc04aba181fc8aa1f71e4584f9890d9cb2dec2 \\\n    --hash=sha256:e4b123ad0f6add92de898214d404e488167b87b5dd86e9a434126bc2b7a5578d \\\n    --hash=sha256:f7d029f20ef56673a9730766023459ece397a05001f4e4d13805111d7c2108c0 \\\n    --hash=sha256:fc0de7575e841f1595ac07e5bc631084fd06ca8b03c0f2ecece733d23cd5102a\n```\n\n----------------------------------------\n\nTITLE: Configuring Trial Error Printing in Ray Tune\nDESCRIPTION: TUNE_PRINT_ALL_TRIAL_ERRORS controls whether all trial errors are printed as they occur. When set to 0, errors are only saved to the trial directory. Defaults to 1.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_PRINT_ALL_TRIAL_ERRORS=1\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Pip Dependencies - Python\nDESCRIPTION: Shows how to initialize Ray with a runtime environment that includes pip package dependencies. Uses the requests package as an example.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nray.init(runtime_env={\"pip\": [\"requests\"]})\n```\n\n----------------------------------------\n\nTITLE: Full Parameter Fine-tuning Launch\nDESCRIPTION: Command to start full parameter fine-tuning for 7B model\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./run_llama_ft.sh --size=7b\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Batches with pandas in Ray Data\nDESCRIPTION: Demonstrates iterating over batches of data in pandas DataFrame format using Ray Data's iter_batches() method with the 'pandas' batch format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/iterating-over-data.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n\nfor batch in ds.iter_batches(batch_size=2, batch_format=\"pandas\"):\n    print(batch)\n```\n\n----------------------------------------\n\nTITLE: Package Dependency Hashes for Python Ray Project\nDESCRIPTION: A collection of cryptographic hash values for Python packages required by the Ray project. Each entry contains the package name, version, and multiple SHA-256 hash values to verify package integrity during installation. Comments indicate which requirements files reference these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_20\n\nLANGUAGE: plaintext\nCODE:\n```\n    --hash=sha256:5ed82f5a7af3697b1c4786053736f24a0efd0a1b8a130d4c7bfee4b9ded0f08f \\\n    --hash=sha256:6d4c80667de2e36970ebf74f42d1088cc9ee7ef5f4e8c35eee1b40eafd33ca5b \\\n    --hash=sha256:730076207cb816138cf1af7f7237b208340a2c5e749707457d70705715c93b93 \\\n    --hash=sha256:7687e22a31e976a0e7fc99c2f4d11ca45eff652a81eb8c8085e9609298916dcf \\\n    --hash=sha256:822ea70dc4018c7e6223f13affd1c5c30c0f5c12ac1f96cd8e9949acddb48a61 \\\n    --hash=sha256:84b0daf226913133f899ea9b30618722d45feffa67e4fe867b0b5ae83a34060c \\\n    --hash=sha256:85765fdf4b27eb5086f05ac0491090fc76f4f2b28e09d9350c31aac25a5aaff8 \\\n    --hash=sha256:8dd178c4c80706546702c59529ffc005681bd6dc2ea234c450661b205445a34d \\\n    --hash=sha256:8f5b234f567cf76ee489502ceb7165c2a5cecec081db2b37e35332b537f8157c \\\n    --hash=sha256:98bbd754a422a0b123c66a4c341de0474cad4a5c10c164ceed6ea090f3563db4 \\\n    --hash=sha256:993584fc821c58d5993521bfdcd31a4adf025c7d745bbd4d12ccfecf695af5ba \\\n    --hash=sha256:a40821a89dc373d6427e2b44b572efc36a2778d3f543299e2f24eb1a5de65415 \\\n    --hash=sha256:b291f0ee7961a597cbbcc77709374087fa2a9afe7bdb6a40dbbd9b127e79afee \\\n    --hash=sha256:b573a43ef7c368ba4ea06050a957c2a7550f729c31f11dd616d2ac4aba99888d \\\n    --hash=sha256:b610ff0f24e9f11c9ae653c67ff8cc03c075131401b3e5ef4b82570d1728f8a9 \\\n    --hash=sha256:bdf38ba2d393c7911ae989c3bbba510ebbcdf4ecbdbfec36272abe350c454075 \\\n    --hash=sha256:bfef2bb6ef068827bbd021017a107194956918ab43ce4d6dc945ffa13efbc25f \\\n    --hash=sha256:cab3db8bab4b7e635c1c97270d7a4b2a90c070b33cbc00c99ef3f9be03d3e1f7 \\\n    --hash=sha256:cb70766519500281815dfd7a87d3a178acf7ce95390544b8c90587d76b227681 \\\n    --hash=sha256:cca1b62fe70d761a282496b96a5e51c44c213e410a964bdffe0928e611368329 \\\n    --hash=sha256:ccf9a39706b604d884d2cb1e27fe973bc55f2890c52f38df742bc1d79ab9f5e1 \\\n    --hash=sha256:dc43f1ec66eb8440567186ae2f8c447d91e0372d793dfe8c222aec857b81a8cf \\\n    --hash=sha256:dd632777ff3beaaf629f1ab4396caf7ba0bdd075d948a69460d13d44357aca4c \\\n    --hash=sha256:e45ae4927759289c30ccba8d9fdce62bb414977ba158286b5ddaf8df2cddb5c5 \\\n    --hash=sha256:e50ebce52f41370707f1e21a59514e3375e3edd6e1832f5e5235237db933c98b \\\n    --hash=sha256:ebbbba226f0a108a7366bf4b59bf0f30a12fd5e75100c630267d94d7f0ad20e5 \\\n    --hash=sha256:ec79ff6159dffcc30853b2ad612ed572af86c92b5168aa3fc01a67b0fa40665e \\\n    --hash=sha256:f0936e08e0003f66bfd97e74ee530427707297b0d0361247e9b4f59ab78ddc8b \\\n    --hash=sha256:f26a07a6e877c76a88e3cecac8531908d980d3d5067ff69213653649ec0f60ad \\\n    --hash=sha256:f64e376cd20d3f030190e8c32e1c64582eba56ac6dc7d5b0b49a9d44021b52fd \\\n    --hash=sha256:f6ffbc252eb0d229aeb2f9ad051200668fc3a9aaa8994e49f0cb2ffe2b7867e7 \\\n    --hash=sha256:f9a7c509542db4eceed3dcf21ee5267ab565a83555c9b88a8109dcecc4709002 \\\n    --hash=sha256:ff1d0899f104f3921d94579a5638847f783c9b04f2d5f229392ca77fba5b82fc\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   ray\n```\n\n----------------------------------------\n\nTITLE: Logging Ray Cluster Autoscaling and Object Broadcasting\nDESCRIPTION: This log snippet shows the autoscaler resizing the cluster, progress of actor startup, and metrics for object broadcasting across the cluster. It includes information on CPU count, progress percentages, and timing.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.4.0/scalability/object_store.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n(autoscaler +2m20s) Resized to 212 CPUs.\nEnsure all actors have started.: 100%|| 50/50 [00:02<00:00, 22.15it/s]\nBroadcasting objects: 100%|| 50/50 [00:00<00:00, 10586.33it/s]\nBroadcast time: 308.624902904 (1073741824 B x 50 nodes)\n```\n\n----------------------------------------\n\nTITLE: Checking Dashboard Agent Process and Logs in Kubernetes\nDESCRIPTION: This snippet demonstrates how to log into a Kubernetes pod, check the dashboard agent process, kill it, and examine the logs to troubleshoot connection issues.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Step 1: Log in to the head Pod\nkubectl exec -it $HEAD_POD -n $YOUR_NAMESPACE -- bash\n\n# Step 2: Check the PID of the dashboard agent process\nps aux\n# [Example output]\n# ray          156 ... 0:03 /.../python -u /.../ray/dashboard/agent.py --\n\n# Step 3: Kill the dashboard agent process\nkill 156\n\n# Step 4: Check the logs\ncat /tmp/ray/session_latest/logs/dashboard_agent.log\n\n# [Example output]\n# 2023-07-10 11:24:31,962 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:31 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n# 2023-07-10 11:24:34,001 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:33 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n# 2023-07-10 11:24:36,043 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:36 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n# 2023-07-10 11:24:38,082 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:38 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n# 2023-07-10 11:24:38,590 WARNING agent.py:531 -- Exiting with SIGTERM immediately...\n\n# Step 5: Open a new terminal and check the logs of the KubeRay operator\nkubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n\n# [Example output]\n# Get \\\"http://rayservice-sample-raycluster-rqlsl-head-svc.default.svc.cluster.local:52365/api/serve/applications/\\\": dial tcp 10.96.7.154:52365: connect: connection refused\n```\n\n----------------------------------------\n\nTITLE: Compiled Python Package Dependencies with Hashes for Ray Project\nDESCRIPTION: Sample of compiled Python package dependencies with pinned versions and cryptographic hashes for security. Each package includes hashes for verification and comments indicating which requirement files reference them, ensuring reproducible builds and dependency resolution.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_3\n\nLANGUAGE: text\nCODE:\n```\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   aiohttp\naiohttp==3.11.16 \\\n    --hash=sha256:004511d3413737700835e949433536a2fe95a7d0297edd911a1e9705c5b5ea43 \\\n    --hash=sha256:0902e887b0e1d50424112f200eb9ae3dfed6c0d0a19fc60f633ae5a57c809656 \\\n    --hash=sha256:09b00dd520d88eac9d1768439a59ab3d145065c91a8fab97f900d1b5f802895e \\\n    --hash=sha256:0a2f451849e6b39e5c226803dcacfa9c7133e9825dcefd2f4e837a2ec5a3bb98 \\\n    --hash=sha256:0a950c2eb8ff17361abd8c85987fd6076d9f47d040ebffce67dce4993285e973 \\\n    --hash=sha256:0ad1fb47da60ae1ddfb316f0ff16d1f3b8e844d1a1e154641928ea0583d486ed \\\n    --hash=sha256:13ceac2c5cdcc3f64b9015710221ddf81c900c5febc505dbd8f810e770011540 \\\n    --hash=sha256:14461157d8426bcb40bd94deb0450a6fa16f05129f7da546090cebf8f3123b0f \\\n    --hash=sha256:16f8a2c9538c14a557b4d309ed4d0a7c60f0253e8ed7b6c9a2859a7582f8b1b8 \\\n    --hash=sha256:17ae4664031aadfbcb34fd40ffd90976671fa0c0286e6c4113989f78bebab37a \\\n    --hash=sha256:1ce63ae04719513dd2651202352a2beb9f67f55cb8490c40f056cea3c5c355ce \\\n    --hash=sha256:23a15727fbfccab973343b6d1b7181bfb0b4aa7ae280f36fd2f90f5476805682 \\\n    --hash=sha256:2540ddc83cc724b13d1838026f6a5ad178510953302a49e6d647f6e1de82bc34 \\\n    --hash=sha256:37dcee4906454ae377be5937ab2a66a9a88377b11dd7c072df7a7c142b63c37c \\\n    --hash=sha256:38bea84ee4fe24ebcc8edeb7b54bf20f06fd53ce4d2cc8b74344c5b9620597fd \\\n    --hash=sha256:3ab3367bb7f61ad18793fea2ef71f2d181c528c87948638366bf1de26e239183 \\\n    --hash=sha256:3ad1d59fd7114e6a08c4814983bb498f391c699f3c78712770077518cae63ff7 \\\n    --hash=sha256:3b4e6db8dc4879015b9955778cfb9881897339c8fab7b3676f8433f849425913 \\\n    --hash=sha256:3e061b09f6fa42997cf627307f220315e313ece74907d35776ec4373ed718b86 \\\n    --hash=sha256:42864e70a248f5f6a49fdaf417d9bc62d6e4d8ee9695b24c5916cb4bb666c802 \\\n    --hash=sha256:493910ceb2764f792db4dc6e8e4b375dae1b08f72e18e8f10f18b34ca17d0979 \\\n    --hash=sha256:4d0c970c0d602b1017e2067ff3b7dac41c98fef4f7472ec2ea26fd8a4e8c2149 \\\n    --hash=sha256:54eb3aead72a5c19fad07219acd882c1643a1027fbcdefac9b502c267242f955 \\\n    --hash=sha256:56a3443aca82abda0e07be2e1ecb76a050714faf2be84256dae291182ba59049 \\\n    --hash=sha256:576f5ca28d1b3276026f7df3ec841ae460e0fc3aac2a47cbf72eabcfc0f102e1 \\\n    --hash=sha256:58ede86453a6cf2d6ce40ef0ca15481677a66950e73b0a788917916f7e35a0bb \\\n    --hash=sha256:61c721764e41af907c9d16b6daa05a458f066015abd35923051be8705108ed17 \\\n    --hash=sha256:634d96869be6c4dc232fc503e03e40c42d32cfaa51712aee181e922e61d74814 \\\n    --hash=sha256:696ef00e8a1f0cec5e30640e64eca75d8e777933d1438f4facc9c0cdf288a810 \\\n    --hash=sha256:69a2cbd61788d26f8f1e626e188044834f37f6ae3f937bd9f08b65fc9d7e514e \\\n    --hash=sha256:6a792ce34b999fbe04a7a71a90c74f10c57ae4c51f65461a411faa70e154154e \\\n    --hash=sha256:6ac13b71761e49d5f9e4d05d33683bbafef753e876e8e5a7ef26e937dd766713 \\\n    --hash=sha256:6fdec0213244c39973674ca2a7f5435bf74369e7d4e104d6c7473c81c9bcc8c4 \\\n    --hash=sha256:72b1b03fb4655c1960403c131740755ec19c5898c82abd3961c364c2afd59fe7 \\\n    --hash=sha256:745f1ed5e2c687baefc3c5e7b4304e91bf3e2f32834d07baaee243e349624b24 \\\n    --hash=sha256:776c8e959a01e5e8321f1dec77964cb6101020a69d5a94cd3d34db6d555e01f7 \\\n    --hash=sha256:780df0d837276276226a1ff803f8d0fa5f8996c479aeef52eb040179f3156cbd \\\n    --hash=sha256:78e6e23b954644737e385befa0deb20233e2dfddf95dd11e9db752bdd2a294d3 \\\n    --hash=sha256:7951decace76a9271a1ef181b04aa77d3cc309a02a51d73826039003210bdc86 \\\n    --hash=sha256:7ba92a2d9ace559a0a14b03d87f47e021e4fa7681dc6970ebbc7b447c7d4b7cd \\\n    --hash=sha256:7f6428fee52d2bcf96a8aa7b62095b190ee341ab0e6b1bcf50c615d7966fd45b \\\n    --hash=sha256:87944bd16b7fe6160607f6a17808abd25f17f61ae1e26c47a491b970fb66d8cb \\\n    --hash=sha256:87a6e922b2b2401e0b0cf6b976b97f11ec7f136bfed445e16384fbf6fd5e8602 \\\n    --hash=sha256:8cb0688a8d81c63d716e867d59a9ccc389e97ac7037ebef904c2b89334407180 \\\n    --hash=sha256:8df6612df74409080575dca38a5237282865408016e65636a76a2eb9348c2567 \\\n    --hash=sha256:911a6e91d08bb2c72938bc17f0a2d97864c531536b7832abee6429d5296e5b27 \\\n    --hash=sha256:92b7ee222e2b903e0a4b329a9943d432b3767f2d5029dbe4ca59fb75223bbe2e \\\n    --hash=sha256:938f756c2b9374bbcc262a37eea521d8a0e6458162f2a9c26329cc87fdf06534 \\\n    --hash=sha256:9756d9b9d4547e091f99d554fbba0d2a920aab98caa82a8fb3d3d9bee3c9ae85 \\\n    --hash=sha256:98b88a2bf26965f2015a771381624dd4b0839034b70d406dc74fd8be4cc053e3 \\\n    --hash=sha256:9b751a6306f330801665ae69270a8a3993654a85569b3469662efaad6cf5cc50 \\\n    --hash=sha256:a2a450bcce4931b295fc0848f384834c3f9b00edfc2150baafb4488c27953de6 \\\n    --hash=sha256:a3814760a1a700f3cfd2f977249f1032301d0a12c92aba74605cfa6ce9f78489 \\\n    --hash=sha256:a5abcbba9f4b463a45c8ca8b7720891200658f6f46894f79517e6cd11f3405ca \\\n    --hash=sha256:a6db7458ab89c7d80bc1f4e930cc9df6edee2200127cfa6f6e080cf619eddfbd \\\n    --hash=sha256:ad497f38a0d6c329cb621774788583ee12321863cd4bd9feee1effd60f2ad133 \\\n    --hash=sha256:ad9509ffb2396483ceacb1eee9134724443ee45b92141105a4645857244aecc8 \\\n    --hash=sha256:bbcba75fe879ad6fd2e0d6a8d937f34a571f116a0e4db37df8079e738ea95c71 \\\n    --hash=sha256:c10d85e81d0b9ef87970ecbdbfaeec14a361a7fa947118817fcea8e45335fa46 \\\n    --hash=sha256:c15b2271c44da77ee9d822552201180779e5e942f3a71fb74e026bf6172ff287 \\\n    --hash=sha256:ca37057625693d097543bd88076ceebeb248291df9d6ca8481349efc0b05dcd0 \\\n    --hash=sha256:cc3a145479a76ad0ed646434d09216d33d08eef0d8c9a11f5ae5cdc37caa3540 \\\n    --hash=sha256:ccf10f16ab498d20e28bc2b5c1306e9c1512f2840f7b6a67000a517a4b37d5ee \\\n    --hash=sha256:cd464ba806e27ee24a91362ba3621bfc39dbbb8b79f2e1340201615197370f7c \\\n    --hash=sha256:d007aa39a52d62373bd23428ba4a2546eed0e7643d7bf2e41ddcefd54519842c \\\n    --hash=sha256:d0666afbe984f6933fe72cd1f1c3560d8c55880a0bdd728ad774006eb4241ecd \\\n    --hash=sha256:d07502cc14ecd64f52b2a74ebbc106893d9a9717120057ea9ea1fd6568a747e7 \\\n    --hash=sha256:d489d9778522fbd0f8d6a5c6e48e3514f11be81cb0a5954bdda06f7e1594b321 \\\n    --hash=sha256:df7db76400bf46ec6a0a73192b14c8295bdb9812053f4fe53f4e789f3ea66bbb \\\n    --hash=sha256:e3538bc9fe1b902bef51372462e3d7c96fce2b566642512138a480b7adc9d508 \\\n    --hash=sha256:e87fd812899aa78252866ae03a048e77bd11b80fb4878ce27c23cade239b42b2 \\\n    --hash=sha256:ecdb8173e6c7aa09eee342ac62e193e6904923bd232e76b4157ac0bfa670609f \\\n    --hash=sha256:f244b8e541f414664889e2c87cac11a07b918cb4b540c36f7ada7bfa76571ea2 \\\n    --hash=sha256:f4065145bf69de124accdd17ea5f4dc770da0a6a6e440c53f6e0a8c27b3e635c \\\n    --hash=sha256:f420bfe862fb357a6d76f2065447ef6f484bc489292ac91e29bc65d2d7a2c84d \\\n    --hash=sha256:f6ddd90d9fb4b501c97a4458f1c1720e42432c26cb76d28177c5b5ad4e332601 \\\n    --hash=sha256:fa73e8c2656a3653ae6c307b3f4e878a21f87859a9afab228280ddccd7369d71 \\\n    --hash=sha256:fadbb8f1d4140825069db3fedbbb843290fd5f5bc0a5dbd7eaf81d91bf1b003b \\\n    --hash=sha256:fb3d0cc5cdb926090748ea60172fa8a213cec728bd6c54eae18b96040fcd6227 \\\n    --hash=sha256:fb46bb0f24813e6cede6cc07b1961d4b04f331f7112a23b5e21f567da4ee50aa \\\n    --hash=sha256:fd36c119c5d6551bce374fcb5c19269638f8d09862445f85a5a48596fd59f4bb\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   aiohttp-cors\naiohttp-cors==0.7.0 \\\n    --hash=sha256:0451ba59fdf6909d0e2cd21e4c0a43752bc0703d33fc78ae94d9d9321710193e \\\n    --hash=sha256:4d39c6d7100fd9764ed1caf8cebf0eb01bf5e3f24e2e073fda6234bc48b19f5d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\naiorwlock==1.3.0 \\\n    --hash=sha256:45baf8e4fa9a23e0bb325fbd67da80de1fd7ae1d4f59a6381754c60cec7b289b \\\n    --hash=sha256:83f12d87df4b9728a0b8fda1756585ab0d652b107bab59c6084e1b1ad692ab45\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\naiosignal==1.3.1 \\\n    --hash=sha256:54cd96e15e1649b75d6c87526a6ff0b6c1b0dd3459f43d9ca11d48c339b68cfc \\\n    --hash=sha256:f8376fb07dd1e86a584e4fcdec80b36b7f81aac666ebc724e2c090300dd83b17\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   aiohttp\nannotated-types==0.6.0 \\\n    --hash=sha256:0641064de18ba7a25dee8f96403ebc39113d0cb953a01429249d5c7564666a43 \\\n    --hash=sha256:563339e807e53ffd9c267e99fc6d9ea23eb8443c08f112651963e24e22f84a5d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   pydantic\nanyio==3.7.1 \\\n    --hash=sha256:44a3c9aba0f5defa43261a8b3efb97891f2bd7d804e0e1f56419befa1adfc780 \\\n    --hash=sha256:91dee416e570e92c64041bd18b900d1d6fa78dff7048769ce5ac5ddad004fbb5\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   starlette\n    #   watchfiles\nattrs==25.1.0 \\\n    --hash=sha256:1c97078a80c814273a76b2a298a932eb681c87415c11dee0a6921de7f1b02c3e \\\n    --hash=sha256:c75a69e28a550a7e93789579c22aa26b0f5b83b75dc4e08fe092980051e1090a\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   aiohttp\n    #   jsonschema\n    #   referencing\nbackoff==1.10.0 \\\n    --hash=sha256:5e73e2cbe780e1915a204799dba0a01896f45f4385e636bcca7a0614d879d0cd \\\n    --hash=sha256:b8fba021fac74055ac05eb7c7bfce4723aedde6cd0a504e5326bcb0bdd6d19a4\n```\n\n----------------------------------------\n\nTITLE: Updating BYOD Requirements with Bazel for Python 3.11\nDESCRIPTION: Command to update requirements for bring-your-own-dependencies (BYOD) environment using Bazel run command. This needs to be executed to regenerate the requirements file with Python 3.11.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.11.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel run //release:requirements_byod_3.11.update\n```\n\n----------------------------------------\n\nTITLE: Defining Package Dependencies with Hash Verification in Python Requirements File\nDESCRIPTION: This snippet defines Python package dependencies using pinned versions and SHA256 hash verification for security. Each package includes comments indicating which components require it. The file uses a format where each package is defined with its version and corresponding hash values, followed by comments showing dependency relationships.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n--hash=sha256:f7e301075edaf50500f0b341543c41194d8df3ae5caf4702f2095f3ca73dd8da \\\n    --hash=sha256:fb616be3538599e797a2017cccca78e354c767165e8858ab5116813146041a24 \\\n    --hash=sha256:fce28b3c8a81b6b36dfac9feb1de115bab619b3c13905b419ec71d03a3fc1423 \\\n    --hash=sha256:fe5d7785250541f7f5019ab9cba2c71169dc7d74d0f45253f8313f436458a4ef\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   aiohttp\n    #   yarl\nnest-asyncio==1.5.8 \\\n    --hash=sha256:25aa2ca0d2a5b5531956b9e273b45cf664cae2b145101d73b86b199978d48fdb \\\n    --hash=sha256:accda7a339a70599cb08f9dd09a67e0c2ef8d8d6f4c07f96ab203f2ae254e48d\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   outlines\nnetworkx==3.2.1 \\\n    --hash=sha256:9f1bb5cf3409bf324e0a722c20bdb4c20ee39bf1c30ce8ae499c8502b0b5e0c6 \\\n    --hash=sha256:f18c69adc97877c42332c170849c96cefa91881c99a7cb3e95b7c659ebdc1ec2\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   scikit-image\n    #   torch\nninja==1.11.1.3 \\\n    --hash=sha256:04d48d14ea7ba11951c156599ab526bdda575450797ff57c6fdf99b2554d09c7 \\\n    --hash=sha256:114ed5c61c8474df6a69ab89097a20749b769e2c219a452cb2fadc49b0d581b0 \\\n    --hash=sha256:17978ad611d8ead578d83637f5ae80c2261b033db0b493a7ce94f88623f29e1b \\\n    --hash=sha256:1ad2112c2b0159ed7c4ae3731595191b1546ba62316fc40808edecd0306fefa3 \\\n    --hash=sha256:2883ea46b3c5079074f56820f9989c6261fcc6fd873d914ee49010ecf283c3b2 \\\n    --hash=sha256:28aea3c1c280cba95b8608d50797169f3a34280e3e9a6379b6e340f0c9eaeeb0 \\\n    --hash=sha256:2b4879ea3f1169f3d855182c57dcc84d1b5048628c8b7be0d702b81882a37237 \\\n    --hash=sha256:53409151da081f3c198bb0bfc220a7f4e821e022c5b7d29719adda892ddb31bb \\\n    --hash=sha256:56ada5d33b8741d298836644042faddebc83ee669782d661e21563034beb5aba \\\n    --hash=sha256:7fa2247fce98f683bc712562d82b22b8a0a5c000738a13147ca2d1b68c122298 \\\n    --hash=sha256:8c4bdb9fd2d0c06501ae15abfd23407660e95659e384acd36e013b6dd7d8a8e4 \\\n    --hash=sha256:a27e78ca71316c8654965ee94b286a98c83877bfebe2607db96897bbfe458af0 \\\n    --hash=sha256:a38c6c6c8032bed68b70c3b065d944c35e9f903342875d3a3218c1607987077c \\\n    --hash=sha256:a4a3b71490557e18c010cbb26bd1ea9a0c32ee67e8f105e9731515b6e0af792e \\\n    --hash=sha256:b6966f83064a88a51693073eea3decd47e08c3965241e09578ef7aa3a7738329 \\\n    --hash=sha256:bc3ebc8b2e47716149f3541742b5cd8e0b08f51013b825c05baca3e34854370d \\\n    --hash=sha256:edfa0d2e9d7ead1635b03e40a32ad56cc8f56798b6e2e9848d8300b174897076\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements/llm/llm-requirements.txt\n    #   vllm\n    #   xgrammar\nnumba==0.60.0 \\\n    --hash=sha256:01ef4cd7d83abe087d644eaa3d95831b777aa21d441a23703d649e06b8e06b74 \\\n    --hash=sha256:0b983bd6ad82fe868493012487f34eae8bf7dd94654951404114f23c3466d34b \\\n    --hash=sha256:0ebaa91538e996f708f1ab30ef4d3ddc344b64b5227b67a57aa74f401bb68b9d \\\n    --hash=sha256:1527dc578b95c7c4ff248792ec33d097ba6bef9eda466c948b68dfc995c25781 \\\n    --hash=sha256:159e618ef213fba758837f9837fb402bbe65326e60ba0633dbe6c7f274d42c1b \\\n    --hash=sha256:19407ced081d7e2e4b8d8c36aa57b7452e0283871c296e12d798852bc7d7f198 \\\n    --hash=sha256:3031547a015710140e8c87226b4cfe927cac199835e5bf7d4fe5cb64e814e3ab \\\n    --hash=sha256:38d6ea4c1f56417076ecf8fc327c831ae793282e0ff51080c5094cb726507b1c \\\n    --hash=sha256:3fb02b344a2a80efa6f677aa5c40cd5dd452e1b35f8d1c2af0dfd9ada9978e4b \\\n    --hash=sha256:4142d7ac0210cc86432b818338a2bc368dc773a2f5cf1e32ff7c5b378bd63ee8 \\\n    --hash=sha256:5d761de835cd38fb400d2c26bb103a2726f548dc30368853121d66201672e651 \\\n    --hash=sha256:5df6158e5584eece5fc83294b949fd30b9f1125df7708862205217e068aabf16 \\\n    --hash=sha256:5f4fde652ea604ea3c86508a3fb31556a6157b2c76c8b51b1d45eb40c8598703 \\\n    --hash=sha256:62908d29fb6a3229c242e981ca27e32a6e606cc253fc9e8faeb0e48760de241e \\\n    --hash=sha256:819a3dfd4630d95fd574036f99e47212a1af41cbcb019bf8afac63ff56834449 \\\n    --hash=sha256:a17b70fc9e380ee29c42717e8cc0bfaa5556c416d94f9aa96ba13acb41bdece8 \\\n    --hash=sha256:c151748cd269ddeab66334bd754817ffc0cabd9433acb0f551697e5151917d25 \\\n    --hash=sha256:cac02c041e9b5bc8cf8f2034ff6f0dbafccd1ae9590dc146b3a02a45e53af4e2 \\\n    --hash=sha256:d7da4098db31182fc5ffe4bc42c6f24cd7d1cb8a14b59fd755bfee32e34b8404 \\\n    --hash=sha256:f75262e8fe7fa96db1dca93d53a194a38c46da28b112b8a4aca168f0df860347 \\\n    --hash=sha256:fe0b28abb8d70f8160798f4de9d486143200f34458d34c4a214114e445d7124e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\nnumpy==1.26.4 \\\n    --hash=sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b \\\n    --hash=sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818 \\\n    --hash=sha256:1af303d6b2210eb850fcf03064d364652b7120803a0b872f5211f5234b399f20 \\\n    --hash=sha256:1dda2e7b4ec9dd512f84935c5f126c8bd8b9f2fc001e9f54af255e8c5f16b0e0 \\\n    --hash=sha256:2a02aba9ed12e4ac4eb3ea9421c420301a0c6460d9830d74a9df87efa4912010 \\\n    --hash=sha256:2e4ee3380d6de9c9ec04745830fd9e2eccb3e6cf790d39d7b98ffd19b0dd754a \\\n    --hash=sha256:3373d5d70a5fe74a2c1bb6d2cfd9609ecf686d47a2d7b1d37a8f3b6bf6003aea \\\n    --hash=sha256:47711010ad8555514b434df65f7d7b076bb8261df1ca9bb78f53d3b2db02e95c \\\n    --hash=sha256:4c66707fabe114439db9068ee468c26bbdf909cac0fb58686a42a24de1760c71 \\\n    --hash=sha256:50193e430acfc1346175fcbdaa28ffec49947a06918b7b92130744e81e640110 \\\n    --hash=sha256:52b8b60467cd7dd1e9ed082188b4e6bb35aa5cdd01777621a1658910745b90be \\\n    --hash=sha256:60dedbb91afcbfdc9bc0b1f3f402804070deed7392c23eb7a7f07fa857868e8a \\\n    --hash=sha256:62b8e4b1e28009ef2846b4c7852046736bab361f7aeadeb6a5b89ebec3c7055a \\\n    --hash=sha256:666dbfb6ec68962c033a450943ded891bed2d54e6755e35e5835d63f4f6931d5 \\\n    --hash=sha256:675d61ffbfa78604709862923189bad94014bef562cc35cf61d3a07bba02a7ed \\\n    --hash=sha256:679b0076f67ecc0138fd2ede3a8fd196dddc2ad3254069bcb9faf9a79b1cebcd \\\n    --hash=sha256:7349ab0fa0c429c82442a27a9673fc802ffdb7c7775fad780226cb234965e53c \\\n    --hash=sha256:7ab55401287bfec946ced39700c053796e7cc0e3acbef09993a9ad2adba6ca6e \\\n    --hash=sha256:7e50d0a0cc3189f9cb0aeb3a6a6af18c16f59f004b866cd2be1c14b36134a4a0 \\\n    --hash=sha256:95a7476c59002f2f6c590b9b7b998306fba6a5aa646b1e22ddfeaf8f78c3a29c \\\n    --hash=sha256:96ff0b2ad353d8f990b63294c8986f1ec3cb19d749234014f4e7eb0112ceba5a \\\n    --hash=sha256:9fad7dcb1aac3c7f0584a5a8133e3a43eeb2fe127f47e3632d43d677c66c102b \\\n    --hash=sha256:9ff0f4f29c51e2803569d7a51c2304de5554655a60c5d776e35b4a41413830d0 \\\n    --hash=sha256:a354325ee03388678242a4d7ebcd08b5c727033fcff3b2f536aea978e15ee9e6 \\\n    --hash=sha256:a4abb4f9001ad2858e7ac189089c42178fcce737e4169dc61321660f1a96c7d2 \\\n    --hash=sha256:ab47dbe5cc8210f55aa58e4805fe224dac469cde56b9f731a4c098b91917159a \\\n    --hash=sha256:afedb719a9dcfc7eaf2287b839d8198e06dcd4cb5d276a3df279231138e83d30 \\\n    --hash=sha256:b3ce300f3644fb06443ee2222c2201dd3a89ea6040541412b8fa189341847218 \\\n    --hash=sha256:b97fe8060236edf3662adfc2c633f56a08ae30560c56310562cb4f95500022d5 \\\n    --hash=sha256:bfe25acf8b437eb2a8b2d49d443800a5f18508cd811fea3181723922a8a82b07 \\\n    --hash=sha256:cd25bcecc4974d09257ffcd1f098ee778f7834c3ad767fe5db785be9a4aa9cb2 \\\n    --hash=sha256:d209d8969599b27ad20994c8e41936ee0964e6da07478d6c35016bc386b66ad4 \\\n    --hash=sha256:d5241e0a80d808d70546c697135da2c613f30e28251ff8307eb72ba696945764 \\\n    --hash=sha256:edd8b5fe47dab091176d21bb6de568acdd906d1887a4584a15a9a96a1dca06ef \\\n    --hash=sha256:f870204a840a60da0b12273ef34f7051e98c3b5961b61b0c2c1be6dfd64fbcd3 \\\n    --hash=sha256:ffa75af20b44f8dba823498024771d5ac50620e6915abac414251bd971b4529f\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   cupy-cuda12x\n    #   gguf\n    #   gymnasium\n    #   imageio\n    #   mistral-common\n    #   numba\n    #   opencv-python-headless\n    #   outlines\n    #   pandas\n    #   pyarrow\n    #   scikit-image\n    #   scipy\n    #   tensorboardx\n    #   tifffile\n    #   torchvision\n    #   transformers\n    #   vllm\n    #   xformers\nnvidia-cublas-cu12==12.4.5.8 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:0f8aa1706812e00b9f19dfe0cdb3999b092ccb8ca168c0db5b8ea712456fd9b3 \\\n    --hash=sha256:2fc8da60df463fdefa81e323eef2e36489e1c94335b5358bcb38360adf75ac9b \\\n    --hash=sha256:5a796786da89203a0657eda402bcdcec6180254a8ac22d72213abc42069522dc\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   nvidia-cudnn-cu12\n    #   nvidia-cusolver-cu12\n    #   torch\nnvidia-cuda-cupti-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:5688d203301ab051449a2b1cb6690fbe90d2b372f411521c86018b950f3d7922 \\\n    --hash=sha256:79279b35cf6f91da114182a5ce1864997fd52294a87a16179ce275773799458a \\\n    --hash=sha256:9dec60f5ac126f7bb551c055072b69d85392b13311fcc1bcda2202d172df30fb\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\nnvidia-cuda-nvrtc-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:0eedf14185e04b76aa05b1fea04133e59f465b6f960c0cbf4e37c3cb6b0ea198 \\\n    --hash=sha256:a178759ebb095827bd30ef56598ec182b85547f1508941a3d560eb7ea1fbf338 \\\n    --hash=sha256:a961b2f1d5f17b14867c619ceb99ef6fcec12e46612711bcec78eb05068a60ec\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\nnvidia-cuda-runtime-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:09c2e35f48359752dfa822c09918211844a3d93c100a715d79b59591130c5e1e \\\n    --hash=sha256:64403288fa2136ee8e467cdc9c9427e0434110899d07c779f25b5c068934faa5 \\\n    --hash=sha256:961fe0e2e716a2a1d967aab7caee97512f71767f852f67432d572e36cb3a11f3\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\nnvidia-cudnn-cu12==9.1.0.70 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Dependencies with CUDA Support\nDESCRIPTION: Configuration for installing PyTorch and related packages with CUDA 12.1 support, including specific versions of torch-scatter, torch-sparse, torch-cluster, and torch-spline-conv.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/dl-gpu-requirements.txt#2025-04-12_snippet_1\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cu121\n--find-links https://data.pyg.org/whl/torch-2.3.0+cu121.html\ntorch==2.3.0+cu121\ntorchvision==0.18.0+cu121\ntorch-scatter==2.1.2+pt23cu121\ntorch-sparse==0.6.18+pt23cu121\ntorch-cluster==1.6.3+pt23cu121\ntorch-spline-conv==1.2.2+pt23cu121\n```\n\n----------------------------------------\n\nTITLE: Specifying oauthlib Package Requirement\nDESCRIPTION: Defines the required version and hash values for the oauthlib package. This ensures a specific, verified version is installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_15\n\nLANGUAGE: Text\nCODE:\n```\noauthlib==3.2.2 \\\n    --hash=sha256:8139f29aac13e25d502680e9e19963e83f16838d48a0d71c287fe40e7067fbca \\\n    --hash=sha256:9859c40929662bec5d64f34d01c99e093149682a3f38915dc0655d5a633dd918\n```\n\n----------------------------------------\n\nTITLE: Defining JupyterLab Package Requirement\nDESCRIPTION: Specifies the version and hash values for the jupyterlab package, along with a comment indicating its source in the requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\njupyterlab==3.6.1 \\\n    --hash=sha256:ad6707dd0149b629d0ed5b56916cfcdb816b376c6af3190337faba09e27ea29e \\\n    --hash=sha256:aee98c174180e98a30470297d10b959e8e64f2288970c0de65f0a6d2b4807034\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Importing Ray and RLlib Libraries in Python\nDESCRIPTION: This code snippet shows how to import Ray, Ray RLlib's PPO module, and Ray Serve. It's marked with tags to hide the cell in the rendered output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/template.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# This can be useful if you don't want to clutter the page with details.\n\nimport ray\nimport ray.rllib.agents.ppo as ppo\nfrom ray import serve\n```\n\n----------------------------------------\n\nTITLE: Creating Azure Resource Group\nDESCRIPTION: Creates a new Azure resource group in the East US region for KubeRay deployment\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/azure-aks-gpu-cluster.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naz group create -l eastus -n kuberay-rg\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Version Requirements\nDESCRIPTION: Lists required Python packages and their version constraints. Includes pytest for testing, aiohttp/httpx for HTTP operations, pillow for image processing, pynvml for NVIDIA GPU monitoring, xgrammar for grammar processing, jupytext for Jupyter notebook conversion, and sphinx for documentation generation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/llm/llm-test-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npytest\naiohttp\npillow\nhttpx>=0.27.2\npynvml>=12.0.0\nxgrammar>=0.1.11, !=0.1.13, !=0.1.12\njupytext>1.13.6\nsphinx==6.2.1\n```\n\n----------------------------------------\n\nTITLE: Ray Training Status Output\nDESCRIPTION: Status output showing memory usage, resource allocation, and training results for 12 trials across different reinforcement learning algorithms. Includes detailed metrics like rewards, episode lengths and training iterations.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/rllib_tests/regression_tests_torch.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n== Status ==\nMemory usage on this node: 11.0/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/64 CPUs, 0/8 GPUs, 0.0/326.39 GiB heap, 0.0/143.87 GiB objects (0.0/1.0 accelerator_type:V100)\nResult logdir: /home/ray/ray_results/a2c-torch-atari\nResult logdir: /home/ray/ray_results/apex-dqn-torch-atari\nResult logdir: /home/ray/ray_results/dqn-torch-atari\nResult logdir: /home/ray/ray_results/impala-torch-atari\nResult logdir: /home/ray/ray_results/ppo-torch-atari\nResult logdir: /home/ray/ray_results/sac-torch-halfcheetah-pybullet\nNumber of trials: 12/12 (12 TERMINATED)\n```\n\n----------------------------------------\n\nTITLE: Specifying Py-spy Package Version and Hashes\nDESCRIPTION: Defines the required version of the py-spy package (0.4.0) along with its hash values for verification. This package is only required for Python versions below 3.12.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_16\n\nLANGUAGE: Text\nCODE:\n```\npy-spy==0.4.0 ; python_full_version < '3.12' \\\n    --hash=sha256:47cdda4c34d9b6cb01f3aaeceb2e88faf57da880207fe72ff6ff97e9bb6cc8a9 \\\n    --hash=sha256:77d8f637ade38367d944874776f45b703b7ac5938b1f7be8891f3a5876ddbb96 \\\n    --hash=sha256:806602ce7972782cc9c1e383f339bfc27bfb822d42485e6a3e0530ae5040e1f0 \\\n    --hash=sha256:87573e64dbfdfc89ba2e0f5e2f525aa84e0299c7eb6454b47ea335fde583a7a0 \\\n    --hash=sha256:8bf2f3702cef367a489faa45177b41a6c31b2a3e5bd78c978d44e29340152f5a \\\n    --hash=sha256:c5f06ffce4c9c98b7fc9f5e67e5e7db591173f1351837633f3f23d9378b1d18a \\\n    --hash=sha256:eee3d0bde85ca5cf4f01f012d461180ca76c24835a96f7b5c4ded64eb6a008ab \\\n    --hash=sha256:f2cf3f7130e7d780471faa5957441d3b4e0ec39a79b2c00f4c33d494f7728428\n```\n\n----------------------------------------\n\nTITLE: Package Dependency Hash Declarations\nDESCRIPTION: Lists Python package dependencies with their specific versions and SHA-256 hash values for security verification. Each package includes version constraints and hash values for package authenticity verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:8ed3564abed97c806db122c2d3e1a2b64c74a63debe9903aad795167cc301368 \\\n--hash=sha256:94d3f0826311f45ee19b75f5b48c99466e4218a0489e81c0f0167bda50cacf22\n```\n\n----------------------------------------\n\nTITLE: Specifying grpcio Package with Hash Values\nDESCRIPTION: Requirement specification for grpcio version 1.66.2 with numerous SHA-256 hash values for verification. This is an extensive list of hash values for different binary distributions of the grpcio package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_10\n\nLANGUAGE: text\nCODE:\n```\ngrpcio==1.66.2 \\\n    --hash=sha256:02697eb4a5cbe5a9639f57323b4c37bcb3ab2d48cec5da3dc2f13334d72790dd \\\n    --hash=sha256:03b0b307ba26fae695e067b94cbb014e27390f8bc5ac7a3a39b7723fed085604 \\\n    --hash=sha256:05bc2ceadc2529ab0b227b1310d249d95d9001cd106aa4d31e8871ad3c428d73 \\\n    --hash=sha256:06de8ec0bd71be123eec15b0e0d457474931c2c407869b6c349bd9bed4adbac3 \\\n    --hash=sha256:0be4e0490c28da5377283861bed2941d1d20ec017ca397a5df4394d1c31a9b50 \\\n    --hash=sha256:12fda97ffae55e6526825daf25ad0fa37483685952b5d0f910d6405c87e3adb6 \\\n    --hash=sha256:1caa38fb22a8578ab8393da99d4b8641e3a80abc8fd52646f1ecc92bcb8dee34 \\\n    --hash=sha256:2018b053aa15782db2541ca01a7edb56a0bf18c77efed975392583725974b249 \\\n    --hash=sha256:20657d6b8cfed7db5e11b62ff7dfe2e12064ea78e93f1434d61888834bc86d75 \\\n    --hash=sha256:2335c58560a9e92ac58ff2bc5649952f9b37d0735608242973c7a8b94a6437d8 \\\n    --hash=sha256:31fd163105464797a72d901a06472860845ac157389e10f12631025b3e4d0453 \\\n    --hash=sha256:38b68498ff579a3b1ee8f93a05eb48dc2595795f2f62716e797dc24774c1aaa8 \\\n    --hash=sha256:3b00efc473b20d8bf83e0e1ae661b98951ca56111feb9b9611df8efc4fe5d55d \\\n    --hash=sha256:3ed71e81782966ffead60268bbda31ea3f725ebf8aa73634d5dda44f2cf3fb9c \\\n    --hash=sha256:45a3d462826f4868b442a6b8fdbe8b87b45eb4f5b5308168c156b21eca43f61c \\\n    --hash=sha256:49f0ca7ae850f59f828a723a9064cadbed90f1ece179d375966546499b8a2c9c \\\n    --hash=sha256:4e504572433f4e72b12394977679161d495c4c9581ba34a88d843eaf0f2fbd39 \\\n    --hash=sha256:4ea1d062c9230278793820146c95d038dc0f468cbdd172eec3363e42ff1c7d01 \\\n    --hash=sha256:563588c587b75c34b928bc428548e5b00ea38c46972181a4d8b75ba7e3f24231 \\\n    --hash=sha256:6001e575b8bbd89eee11960bb640b6da6ae110cf08113a075f1e2051cc596cae \\\n    --hash=sha256:66a0cd8ba6512b401d7ed46bb03f4ee455839957f28b8d61e7708056a806ba6a \\\n    --hash=sha256:6851de821249340bdb100df5eacfecfc4e6075fa85c6df7ee0eb213170ec8e5d \\\n    --hash=sha256:728bdf36a186e7f51da73be7f8d09457a03061be848718d0edf000e709418987 \\\n    --hash=sha256:73e3b425c1e155730273f73e419de3074aa5c5e936771ee0e4af0814631fb30a \\\n    --hash=sha256:73fc8f8b9b5c4a03e802b3cd0c18b2b06b410d3c1dcbef989fdeb943bd44aff7 \\\n    --hash=sha256:78fa51ebc2d9242c0fc5db0feecc57a9943303b46664ad89921f5079e2e4ada7 \\\n    --hash=sha256:7b2c86457145ce14c38e5bf6bdc19ef88e66c5fee2c3d83285c5aef026ba93b3 \\\n    --hash=sha256:7d69ce1f324dc2d71e40c9261d3fdbe7d4c9d60f332069ff9b2a4d8a257c7b2b \\\n    --hash=sha256:802d84fd3d50614170649853d121baaaa305de7b65b3e01759247e768d691ddf \\\n    --hash=sha256:80fd702ba7e432994df208f27514280b4b5c6843e12a48759c9255679ad38db8 \\\n    --hash=sha256:8ac475e8da31484efa25abb774674d837b343afb78bb3bcdef10f81a93e3d6bf \\\n    --hash=sha256:950da58d7d80abd0ea68757769c9db0a95b31163e53e5bb60438d263f4bed7b7 \\\n    --hash=sha256:99a641995a6bc4287a6315989ee591ff58507aa1cbe4c2e70d88411c4dcc0839 \\\n    --hash=sha256:9c3a99c519f4638e700e9e3f83952e27e2ea10873eecd7935823dab0c1c9250e \\\n    --hash=sha256:9c509a4f78114cbc5f0740eb3d7a74985fd2eff022971bc9bc31f8bc93e66a3b \\\n    --hash=sha256:a18e20d8321c6400185b4263e27982488cb5cdd62da69147087a76a24ef4e7e3 \\\n    --hash=sha256:a917d26e0fe980b0ac7bfcc1a3c4ad6a9a4612c911d33efb55ed7833c749b0ee \\\n    --hash=sha256:a9539f01cb04950fd4b5ab458e64a15f84c2acc273670072abe49a3f29bbad54 \\\n    --hash=sha256:ad2efdbe90c73b0434cbe64ed372e12414ad03c06262279b104a029d1889d13e \\\n    --hash=sha256:b672abf90a964bfde2d0ecbce30f2329a47498ba75ce6f4da35a2f4532b7acbc \\\n    --hash=sha256:bbd27c24a4cc5e195a7f56cfd9312e366d5d61b86e36d46bbe538457ea6eb8dd \\\n    --hash=sha256:c400ba5675b67025c8a9f48aa846f12a39cf0c44df5cd060e23fda5b30e9359d \\\n    --hash=sha256:c408f5ef75cfffa113cacd8b0c0e3611cbfd47701ca3cdc090594109b9fcbaed \\\n    --hash=sha256:c806852deaedee9ce8280fe98955c9103f62912a5b2d5ee7e3eaa284a6d8d8e7 \\\n    --hash=sha256:ce89f5876662f146d4c1f695dda29d4433a5d01c8681fbd2539afff535da14d4 \\\n    --hash=sha256:d25a14af966438cddf498b2e338f88d1c9706f3493b1d73b93f695c99c5f0e2a \\\n    --hash=sha256:d8d4732cc5052e92cea2f78b233c2e2a52998ac40cd651f40e398893ad0d06ec \\\n    --hash=sha256:d9a9724a156c8ec6a379869b23ba3323b7ea3600851c91489b871e375f710bc8 \\\n    --hash=sha256:e636ce23273683b00410f1971d209bf3689238cf5538d960adc3cdfe80dd0dbd \\\n    --hash=sha256:e88264caad6d8d00e7913996030bac8ad5f26b7411495848cc218bd3a9040b6c \\\n    --hash=sha256:f145cc21836c332c67baa6fc81099d1d27e266401565bf481948010d6ea32d46 \\\n    --hash=sha256:fb57870449dfcfac428afbb5a877829fcb0d6db9d9baa1148705739e9083880e \\\n```\n\n----------------------------------------\n\nTITLE: Specifying fsspec Package Dependency with Hash Values\nDESCRIPTION: Definition of the fsspec package dependency at version 2023.5.0 with associated hash values for verification. The comment indicates this package is required by the Ray project's main requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nfsspec==2023.5.0 \\\n    --hash=sha256:51a4ad01a5bb66fcc58036e288c0d53d3975a0df2a5dc59a93b59bade0391f2a \\\n    --hash=sha256:b3b56e00fb93ea321bc9e5d9cf6f8522a0198b20eb24e02774d329e9c6fb84ce\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation TOC Tree in Markdown\nDESCRIPTION: Sphinx/MyST documentation configuration showing a hidden table of contents tree entry for a machine learning example page.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/examples/index.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n:hidden:\n\nml-example\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: A requirements.txt style file that lists Python package dependencies with exact version pins and SHA256 hash verification codes. Each entry includes comments showing the dependency chain.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_11\n\nLANGUAGE: pip\nCODE:\n```\nhumanize==4.12.1 \\\n    --hash=sha256:1338ba97415c96556758a6e2f65977ed406dddf4620d4c6db9bbdfd07f0f1232 \\\n    --hash=sha256:86014ca5c52675dffa1d404491952f1f5bf03b07c175a51891a343daebf01fea\n\nidna==3.7 \\\n    --hash=sha256:028ff3aadf0609c1fd278d8ea3089299412a7a8b9bd005dd08b9f8285bcb5cfc \\\n    --hash=sha256:82fee1fc78add43492d3a1898bfa6d8a904cc97d8427f683ed8e798d07761aa0\n\nimageio==2.34.2 \\\n    --hash=sha256:5c0c0ee8faa018a1c42f649b90395dd4d3bb6187c09053a0cd6f1fdd51bbff5e \\\n    --hash=sha256:a0bb27ec9d5bab36a9f4835e51b21d2cb099e1f78451441f94687ff3404b79f8\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding for Ray Dashboard\nDESCRIPTION: Command to set up port forwarding to access the Ray dashboard for visualizing the Ray Serve application state and debugging deployment issues.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.md#2025-04-12_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nkubectl port-forward svc/rayservice-no-ray-serve-replica-head-svc 8265:8265\n```\n\n----------------------------------------\n\nTITLE: Package Hash Requirements for Ray Dependencies\nDESCRIPTION: SHA256 hash requirements for Python package dependencies including multidict version 6.0.5. Used for dependency verification and security in the Ray project's Python environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:15c1e86fff77184c20a2932cd9742bf33fe23125fa3fcf332df9ad2f7d483044 \\\n--hash=sha256:19746b50be214a54239aab822964f2ac81e38b0055cca94808359d779338c10e \\\n--hash=sha256:2719647625320b60e2d8af06b35f5b12d4f4d281db30a15a1df22adb2295f633\n```\n\n----------------------------------------\n\nTITLE: Implementing Negative Indexing for Lookback Buffers - Python\nDESCRIPTION: This snippet introduces the functionality of the `neg_index_as_lookback` argument, which alters the interpretation of negative indices for lookback buffers, allowing backward iteration over a specified range of timesteps in a reinforcement learning context.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/single-agent-episode.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: doc_code/sa_episode.py\n    :language: python\n    :start-after: rllib-sa-episode-06-begin\n    :end-before: rllib-sa-episode-06-end\n```\n\n----------------------------------------\n\nTITLE: Package Source and Dependency Information\nDESCRIPTION: This snippet provides information about the source of the package hashes and its dependencies. It indicates that the hashes are sourced from a specific requirements file for the 'ray' project, and that this package is a dependency of 'aiohttp'.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_25\n\nLANGUAGE: Text\nCODE:\n```\n# via\n#   -c python/requirements_compiled_ray_test_py311_cpu.txt\n#   aiohttp\n```\n\n----------------------------------------\n\nTITLE: Custom OfflineData Class for Image Processing in RLlib\nDESCRIPTION: Example of a custom OfflineData class that handles reading and preprocessing image data, converting binary encoded images into numpy arrays.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n# This code is referenced but not shown in the snippet\n```\n\n----------------------------------------\n\nTITLE: Specifying PTYProcess Package Dependency with Platform Condition\nDESCRIPTION: Defines the ptyprocess package dependency at version 0.7.0 with a platform-specific condition that excludes Windows systems. This package is a dependency for pexpect and terminado packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_20\n\nLANGUAGE: plaintext\nCODE:\n```\nptyprocess==0.7.0 ; os_name != 'nt' or sys_platform != 'win32' \\\n    --hash=sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35 \\\n    --hash=sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220\n```\n\n----------------------------------------\n\nTITLE: JVM Parallel Garbage Collection with Thread Signaling\nDESCRIPTION: Thread stack trace showing JVM parallel garbage collection with condition variable signaling to coordinate GC threads. This trace captures the inter-thread communication mechanisms used during garbage collection.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_116\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;VMThread::run;VMThread::loop;VMThread::evaluate_operation;VM_Operation::evaluate;VM_ParallelGCFailedAllocation::doit;ParallelScavengeHeap::failed_mem_allocate;PSScavenge::invoke;PSScavenge::invoke_no_policy;pthread_cond_signal@@GLIBC_2.3.2;system_call_fastpath_[k];sys_futex_[k];do_futex_[k];futex_wake_op_[k]\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Hashes\nDESCRIPTION: This snippet shows the format used to specify Python package names, versions, and their corresponding SHA256 hashes. It includes multiple hash values for each package to support different platforms or build variants.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_16\n\nLANGUAGE: Text\nCODE:\n```\nmatplotlib-inline==0.1.6 \\\n    --hash=sha256:f1f41aab5328aa5aaea9b16d083b128102f8712542f819fe7e6a420ff581b311 \\\n    --hash=sha256:f887e5f10ba98e8d2b150ddcf4702c1e5f8b3a20005eb0f74bfdbd360ee6f304\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   ipykernel\n    #   ipython\n```\n\n----------------------------------------\n\nTITLE: Displaying Production Environment Diagram in ReStructuredText\nDESCRIPTION: This code snippet uses ReStructuredText directives to display an image illustrating the production environment setup for Ray on Kubernetes. It includes an image alignment directive and a comment about the source of the diagram.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/storage.md#2025-04-12_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: ../images/production.png\n    :align: center\n..\n    Find the source document here (https://whimsical.com/clusters-P5Y6R23riCuNb6xwXVXN72)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Values\nDESCRIPTION: This snippet demonstrates how to specify Python package dependencies with exact versions and hash values for integrity verification. It includes packages like gymnasium, h11, idna, and imageio.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_7\n\nLANGUAGE: Text\nCODE:\n```\ngymnasium==1.0.0 \\\n    --hash=sha256:9d2b66f30c1b34fe3c2ce7fae65ecf365d0e9982d2b3d860235e773328a3b403 \\\n    --hash=sha256:b6f40e1e24c5bd419361e1a5b86a9117d2499baecc3a660d44dfff4c465393ad\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\nh11==0.14.0 \\\n    --hash=sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d \\\n    --hash=sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   uvicorn\nidna==3.7 \\\n    --hash=sha256:028ff3aadf0609c1fd278d8ea3089299412a7a8b9bd005dd08b9f8285bcb5cfc \\\n    --hash=sha256:82fee1fc78add43492d3a1898bfa6d8a904cc97d8427f683ed8e798d07761aa0\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   anyio\n    #   requests\n    #   yarl\nimageio==2.34.2 \\\n    --hash=sha256:5c0c0ee8faa018a1c42f649b90395dd4d3bb6187c09053a0cd6f1fdd51bbff5e \\\n    --hash=sha256:a0bb27ec9d5bab36a9f4835e51b21d2cb099e1f78451441f94687ff3404b79f8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   scikit-image\n```\n\n----------------------------------------\n\nTITLE: Extending Ray Docker Image with Dockerfile\nDESCRIPTION: Dockerfile that extends the official Ray image by adding the Faker package and setting up the working directory for a Serve application.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/docker.md#2025-04-12_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM rayproject/ray:2.9.0\n\nRUN pip install Faker==18.13.0\n\nWORKDIR /serve_app\n\nCOPY fake.py /serve_app/fake.py\n```\n\n----------------------------------------\n\nTITLE: Specifying Pandas Package Dependency\nDESCRIPTION: Defines the pandas package dependency with version 1.5.3 and multiple SHA-256 hashes. Pandas is a popular data manipulation library used in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_17\n\nLANGUAGE: Text\nCODE:\n```\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    # ... (additional hashes omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Resources\nDESCRIPTION: Command to delete the second RayCluster and clean up the resources used in the demonstration of KubeRay with YuniKorn integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete raycluster test-yunikorn-1\n```\n\n----------------------------------------\n\nTITLE: Avoiding ObjectRef Outliving Owner Task in Ray (Python)\nDESCRIPTION: Shows an example of creating an ObjectRef that outlives its owner task, which can lead to OwnerDiedError if the worker process fails. This is not a recommended practice for fault-tolerant applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault-tolerance.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef a():\n    x = ray.put(1)\n    return x\n\nx_ref = ray.get(a.remote())\n# If the worker for `a` dies here, this will raise OwnerDiedError:\nprint(ray.get(x_ref))\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables and Starting LLaMa Training on Habana Gaudi\nDESCRIPTION: This code sets up necessary environment variables for controlling HPU device visibility and execution mode before launching the training process. It demonstrates how to configure lazy, eager, or eager.compile execution modes for Habana Gaudi processors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# set some environment variables\nos.environ[\"RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES\"] = \"0\"\n# if using RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES env var\n# you must set HABANA_VISIBLE_MODULES, such as\n# os.environ[\"HABANA_VISIBLE_MODULES\"] = \"0,1,2,3\"\n\n# execution_mode are [\"lazy\", \"eager\", \"eager.compile\"]\nexecution_mode = \"lazy\"\nos.environ[\"PT_HPU_LAZY_MODE\"] = \"1\" if execution_mode == \"lazy\" else \"0\"\n\nmain(num_workers=4, execution_mode=execution_mode)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Creating RayJob\nDESCRIPTION: Commands to download and deploy a PyTorch text classifier RayJob configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/pytorch-text-classifier/ray-job.pytorch-distributed-training.yaml\n$ kubectl create -f ray-job.pytorch-distributed-training.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Ray Data API Documentation in RST\nDESCRIPTION: This RST code defines the structure of the Ray Data API documentation. It sets up a table of contents with maxdepth 2 and lists various rst files that comprise different sections of the API documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/api.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _data-api:\n\nRay Data API\n================\n\n.. toctree::\n    :maxdepth: 2\n\n    input_output.rst\n    dataset.rst\n    data_iterator.rst\n    execution_options.rst\n    aggregate.rst\n    grouped_data.rst\n    data_context.rst\n    preprocessor.rst\n    llm.rst\n    from_other_data_libs.rst\n```\n\n----------------------------------------\n\nTITLE: NVIDIA CUDA Dependencies for Ray Project\nDESCRIPTION: A collection of NVIDIA CUDA dependencies with specific version constraints, platform limitations, and SHA256 hashes for security verification. These packages support GPU acceleration for Ray's machine learning capabilities on Linux x86_64 systems.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_18\n\nLANGUAGE: pip\nCODE:\n```\nnvidia-cublas-cu12==12.4.5.8 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:0f8aa1706812e00b9f19dfe0cdb3999b092ccb8ca168c0db5b8ea712456fd9b3 \\\n    --hash=sha256:2fc8da60df463fdefa81e323eef2e36489e1c94335b5358bcb38360adf75ac9b \\\n    --hash=sha256:5a796786da89203a0657eda402bcdcec6180254a8ac22d72213abc42069522dc\n    # via\n    #   nvidia-cudnn-cu12\n    #   nvidia-cusolver-cu12\n    #   torch\n```\n\n----------------------------------------\n\nTITLE: Configuring HyperOpt Search for Conditional Spaces in Python\nDESCRIPTION: Initializes HyperOptSearch with a provided conditional search space and applies concurrency limits. Designed for optimizing hyperparameters within said conditional spaces using Ray Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nalgo = HyperOptSearch(space=conditional_space, metric=\"mean_loss\", mode=\"min\")\nalgo = ConcurrencyLimiter(algo, max_concurrent=4)\n```\n\n----------------------------------------\n\nTITLE: Creating a Progress Actor Instance in Ray\nDESCRIPTION: Instantiates the ProgressActor to track overall progress of the Monte Carlo simulation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/monte_carlo_pi.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 1_000_000_000\nprogress_actor = ProgressActor.remote(num_samples)\n```\n\n----------------------------------------\n\nTITLE: Building MacOS Ray Wheel using Shell Script\nDESCRIPTION: This command runs a shell script to build Ray wheels for MacOS. It should be executed from the root directory of the Ray project. The script may require sudo access, so the user might need to enter their password.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/README-building-wheels.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./python/build-wheel-macos.sh\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Operator\nDESCRIPTION: Installs the KubeRay operator using a utility script\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n../scripts/doctest-utils.sh install_kuberay_operator\n```\n\n----------------------------------------\n\nTITLE: Setting Warning Threshold for Autoscaler Resource Issues\nDESCRIPTION: TUNE_WARN_INSUFFICENT_RESOURCE_THRESHOLD_S_AUTOSCALER sets the time threshold (in seconds) for warnings about no running trials when autoscaler is enabled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_WARN_INSUFFICENT_RESOURCE_THRESHOLD_S_AUTOSCALER=60\n```\n\n----------------------------------------\n\nTITLE: Ray Training Status Output with Trial Results Table\nDESCRIPTION: Comprehensive status output showing node memory usage, resource allocation, result directories, and a detailed table of training results across multiple RL algorithms. Includes metrics like reward, episode length, and training iterations for each trial.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.2.0/rllib_regression_tf.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n== Status ==\nMemory usage on this node: 8.8/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/64 CPUs, 0.0/8 GPUs, 0.0/325.83 GiB heap, 0.0/99.07 GiB objects (0/1.0 accelerator_type:V100)\nResult logdir: /home/ray/ray_results/a2c-tf-atari\nResult logdir: /home/ray/ray_results/apex-dqn-tf-atari\nResult logdir: /home/ray/ray_results/dqn-tf-atari\nResult logdir: /home/ray/ray_results/impala-tf-atari\nResult logdir: /home/ray/ray_results/ppo-tf-atari\nResult logdir: /home/ray/ray_results/sac-tf-halfcheetah-pybullet\nNumber of trials: 12/12 (12 TERMINATED)\n+-------------------------------------------+------------+-------+--------+------------------+---------+-----------+----------------------+----------------------+--------------------+\n| Trial name                                | status     | loc   |   iter |   total time (s) |      ts |    reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n|-------------------------------------------+------------+-------+--------+------------------+---------+-----------+----------------------+----------------------+--------------------|\n| A2C_BreakoutNoFrameskip-v4_e6509_00000    | TERMINATED |       |    355 |          3604.01 | 4137500 |   1.86    |               10     |                0     |            815.78  |\n| A2C_BreakoutNoFrameskip-v4_e6509_00001    | TERMINATED |       |    354 |          3601.32 | 4067500 |   1.79    |               10     |                0     |            803.07  |\n| APEX_BreakoutNoFrameskip-v4_e6509_00002   | TERMINATED |       |     98 |          3626.91 | 7297440 |   1.4     |                9     |                0     |            739.886 |\n| APEX_BreakoutNoFrameskip-v4_e6509_00003   | TERMINATED |       |     97 |          3607.18 | 7222240 |   1.17816 |                5     |                0     |            702.362 |\n| DQN_BreakoutNoFrameskip-v4_e6509_00004    | TERMINATED |       |     35 |          3636.53 |  360000 |   1.25    |                6     |                0     |            710.49  |\n| DQN_BreakoutNoFrameskip-v4_e6509_00005    | TERMINATED |       |     35 |          3631.05 |  360000 |   1.36    |                9     |                0     |            723.54  |\n| IMPALA_BreakoutNoFrameskip-v4_e6509_00006 | TERMINATED |       |    350 |          3607.49 | 3024500 |   1.87    |                9     |                0     |            816.3   |\n| IMPALA_BreakoutNoFrameskip-v4_e6509_00007 | TERMINATED |       |    349 |          3601.95 | 3025500 |   1.21    |                6     |                0     |            716.7   |\n| PPO_BreakoutNoFrameskip-v4_e6509_00008    | TERMINATED |       |   1858 |          3600.41 | 9290000 |   1.69    |               10     |                0     |            792.13  |\n| PPO_BreakoutNoFrameskip-v4_e6509_00009    | TERMINATED |       |   1851 |          3601.2  | 9255000 |   1.6     |               11     |                0     |            770.95  |\n| SAC_HalfCheetahBulletEnv-v0_e6509_00010   | TERMINATED |       |     45 |          3670.33 |   54000 | 269.06    |              622.238 |             -454.818 |           1000     |\n| SAC_HalfCheetahBulletEnv-v0_e6509_00011   | TERMINATED |       |     45 |          3654.38 |   54000 | 473.166   |              628.875 |              156.264 |           1000     |\n+-------------------------------------------+------------+-------+--------+------------------+---------+-----------+----------------------+----------------------+--------------------+\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with Hashes\nDESCRIPTION: A requirements.txt style listing of Python package dependencies including websockets, xformers, xgrammar and yarl with their specific versions and SHA256 hashes for package integrity verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_39\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:9c75eff897786ee262c9f17a48886f4e98e6cfd335e011c591c305e5d083c056 \\\n--hash=sha256:b538014a87f94d92f98f34d3e6d2635478e6be6423a9ea53e4dd96210065e193 \\\n--hash=sha256:b6577b8c6c8701ba8642ea9335a129836347894b666dd1ec2226830e263909d3 \\\n--hash=sha256:c0376deac92377817e4fb8f347bf559b7d44ff556d9bc6f6208dd3f79f104aaf \\\n--hash=sha256:cae3dde0b4b2078f31527acff6f486e23abed307ba4d3932466ba7cdd5ecec79 \\\n--hash=sha256:cb5d45c4143c1dd60f98a16187fd123eda7248f84ef22244818c18d531a249d1 \\\n--hash=sha256:d9b073073e048081e502b6c6b0b88714c026a1a4c890569238d04aca5f9ca74b \\\n--hash=sha256:fac19dc9cbc34052394dbe81e149411a62e71999c0a19e1e09ce537867f95ae0\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Cluster for 70B Language Model Training (YAML)\nDESCRIPTION: This YAML configuration defines the cluster setup for training a 70B parameter language model. It specifies a head node and GPU worker nodes with their instance types and scaling limits.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nhead_node_type:\n  name: head_node_type\n  instance_type: m5.xlarge\n\nworker_node_types:\n- name: gpu_worker\n  instance_type: g5.48xlarge\n  min_workers: 0\n  max_workers: 4\n  use_spot: false\n```\n\n----------------------------------------\n\nTITLE: Defining Pandas Library Dependency\nDESCRIPTION: This code specifies the Pandas library dependency with extensive hash verification. It's referenced from both the requirements_compiled.txt and the core requirements.txt files, indicating its importance for the Ray project's data manipulation capabilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    --hash=sha256:41179ce559943d83a9b4bbacb736b04c928b095b5f25dd2b7389eda08f46f373 \\\n    --hash=sha256:478ff646ca42b20376e4ed3fa2e8d7341e8a63105586efe54fa2508ee087f328 \\\n    --hash=sha256:50869a35cbb0f2e0cd5ec04b191e7b12ed688874bd05dd777c19b28cbea90996 \\\n    --hash=sha256:565fa34a5434d38e9d250af3c12ff931abaf88050551d9fbcdfafca50d62babf \\\n    --hash=sha256:5f2b952406a1588ad4cad5b3f55f520e82e902388a6d5a4a91baa8d38d23c7f6 \\\n    --hash=sha256:5fbcb19d6fceb9e946b3e23258757c7b225ba450990d9ed63ccceeb8cae609f7 \\\n    --hash=sha256:6973549c01ca91ec96199e940495219c887ea815b2083722821f1d7abfa2b4dc \\\n    --hash=sha256:74a3fd7e5a7ec052f183273dc7b0acd3a863edf7520f5d3a1765c04ffdb3b0b1 \\\n    --hash=sha256:7a0a56cef15fd1586726dace5616db75ebcfec9179a3a55e78f72c5639fa2a23 \\\n    --hash=sha256:7cec0bee9f294e5de5bbfc14d0573f65526071029d036b753ee6507d2a21480a \\\n    --hash=sha256:87bd9c03da1ac870a6d2c8902a0e1fd4267ca00f13bc494c9e5a9020920e1d51 \\\n    --hash=sha256:972d8a45395f2a2d26733eb8d0f629b2f90bebe8e8eddbb8829b180c09639572 \\\n    --hash=sha256:9842b6f4b8479e41968eced654487258ed81df7d1c9b7b870ceea24ed9459b31 \\\n    --hash=sha256:9f69c4029613de47816b1bb30ff5ac778686688751a5e9c99ad8c7031f6508e5 \\\n    --hash=sha256:a50d9a4336a9621cab7b8eb3fb11adb82de58f9b91d84c2cd526576b881a0c5a \\\n    --hash=sha256:bc4c368f42b551bf72fac35c5128963a171b40dce866fb066540eeaf46faa003 \\\n    --hash=sha256:c39a8da13cede5adcd3be1182883aea1c925476f4e84b2807a46e2775306305d \\\n    --hash=sha256:c3ac844a0fe00bfaeb2c9b51ab1424e5c8744f89860b138434a363b1f620f354 \\\n    --hash=sha256:c4c00e0b0597c8e4f59e8d461f797e5d70b4d025880516a8261b2817c47759ee \\\n    --hash=sha256:c74a62747864ed568f5a82a49a23a8d7fe171d0c69038b38cedf0976831296fa \\\n    --hash=sha256:dd05f7783b3274aa206a1af06f0ceed3f9b412cf665b7247eacd83be41cf7bf0 \\\n    --hash=sha256:dfd681c5dc216037e0b0a2c821f5ed99ba9f03ebcf119c7dac0e9a7b960b9ec9 \\\n    --hash=sha256:e474390e60ed609cec869b0da796ad94f420bb057d86784191eefc62b65819ae \\\n    --hash=sha256:f76d097d12c82a535fda9dfe5e8dd4127952b45fea9b0276cb30cca5ea313fbc\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Lists required Python packages with their versions and SHA256 hashes for security verification. Includes dependency annotations showing which packages require these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:cbebcd5bcaf1eaf302617c114aa67569dd3f090dd0ce8ba9e35e9985b41ac35b \\\n--hash=sha256:cd6c8fca38178e12c00418de737aef1261576bd1b6e8c6134d3e729a4e858b38 \\\n--hash=sha256:ceb3b7e6a0135e092de86110c5a74e46bda4bd4fbfeeb3a3bcec79c0f861e450\n```\n\n----------------------------------------\n\nTITLE: Ray Project Execution Statistics\nDESCRIPTION: Performance metrics showing total execution time and statistics about iteration durations including average, maximum and minimum times.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.0/stress_tests/test_dead_actors.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nFinished in: 220.28446006774902s\nAverage iteration time: 2.2028422021865843s\nMax iteration time: 4.1706767082214355s\nMin iteration time: 1.1767382621765137s\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Live Rendering\nDESCRIPTION: Command to build documentation with incremental updates and live rendering support.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmake local\n```\n\n----------------------------------------\n\nTITLE: Building Resnet Podman Image\nDESCRIPTION: This Bash script details building and optionally pushing a Podman image for the Resnet application, specifying its Dockerfile and using a placeholder registry path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/multi-app-container.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Build the image from the Dockerfile using Podman\nexport IMG2=alice/resnet_image:latest\npodman build -t $IMG2 -f resnet.Dockerfile .\n# Push to a registry. This step is unnecessary if you are deploying Serve locally.\npodman push $IMG2\n```\n\n----------------------------------------\n\nTITLE: Vert.x HTTP Handler Write Operation Stack Trace\nDESCRIPTION: Thread stack trace showing HTTP response generation through Vert.x HTTP handlers. This trace demonstrates the complete path from JavaScript request handling to HTTP response generation via VertxHttpHandler.write() in a Vert.x application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_93\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];org/vertx/java/core/http/impl/VertxHttpHandler:.write_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Ray Training Status Table Output\nDESCRIPTION: System status output showing memory usage (24.9/480.3 GiB), resource allocation, and training results for 4 IMPALA trials across different Atari environments. Each trial shows status, environment name, iterations, total time, timesteps, and reward metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.4/stress_tests/application_stress_test.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n== Status ==\nMemory usage on this node: 24.9/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/640 CPUs, 0/8 GPUs, 0.0/1905.71 GiB heap, 0.0/566.21 GiB objects\nResult logdir: /home/ubuntu/ray_results/atari-impala\nNumber of trials: 4 (4 TERMINATED)\n+------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------+\n| Trial name                               | status     | loc   | env                         |   iter |   total time (s) |       ts |   reward |\n|------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_00000      | TERMINATED |       | BreakoutNoFrameskip-v4      |    379 |          6695.67 | 30028000 |   547.94 |\n| IMPALA_BeamRiderNoFrameskip-v4_00001     | TERMINATED |       | BeamRiderNoFrameskip-v4     |    382 |          6742.5  | 30068000 |  2520.02 |\n| IMPALA_QbertNoFrameskip-v4_00002         | TERMINATED |       | QbertNoFrameskip-v4         |    374 |          6623.08 | 30024500 |  9815    |\n| IMPALA_SpaceInvadersNoFrameskip-v4_00003 | TERMINATED |       | SpaceInvadersNoFrameskip-v4 |    380 |          6711.43 | 30058500 |   762.9  |\n+------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------+\n```\n\n----------------------------------------\n\nTITLE: Specifying Pandas Dependency with Hash Verification\nDESCRIPTION: Defines the pandas package dependency with version 1.5.3 and includes multiple SHA256 hash validations for security. This is referenced from requirements_compiled_ray_test_py311_cpu.txt and the main requirements.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    --hash=sha256:41179ce559943d83a9b4bbacb736b04c928b095b5f25dd2b7389eda08f46f373 \\\n    --hash=sha256:478ff646ca42b20376e4ed3fa2e8d7341e8a63105586efe54fa2508ee087f328 \\\n    --hash=sha256:50869a35cbb0f2e0cd5ec04b191e7b12ed688874bd05dd777c19b28cbea90996 \\\n    --hash=sha256:565fa34a5434d38e9d250af3c12ff931abaf88050551d9fbcdfafca50d62babf \\\n    --hash=sha256:5f2b952406a1588ad4cad5b3f55f520e82e902388a6d5a4a91baa8d38d23c7f6 \\\n    --hash=sha256:5fbcb19d6fceb9e946b3e23258757c7b225ba450990d9ed63ccceeb8cae609f7 \\\n    --hash=sha256:6973549c01ca91ec96199e940495219c887ea815b2083722821f1d7abfa2b4dc \\\n    --hash=sha256:74a3fd7e5a7ec052f183273dc7b0acd3a863edf7520f5d3a1765c04ffdb3b0b1 \\\n    --hash=sha256:7a0a56cef15fd1586726dace5616db75ebcfec9179a3a55e78f72c5639fa2a23 \\\n    --hash=sha256:7cec0bee9f294e5de5bbfc14d0573f65526071029d036b753ee6507d2a21480a \\\n    --hash=sha256:87bd9c03da1ac870a6d2c8902a0e1fd4267ca00f13bc494c9e5a9020920e1d51 \\\n    --hash=sha256:972d8a45395f2a2d26733eb8d0f629b2f90bebe8e8eddbb8829b180c09639572 \\\n    --hash=sha256:9842b6f4b8479e41968eced654487258ed81df7d1c9b7b870ceea24ed9459b31 \\\n    --hash=sha256:9f69c4029613de47816b1bb30ff5ac778686688751a5e9c99ad8c7031f6508e5 \\\n    --hash=sha256:a50d9a4336a9621cab7b8eb3fb11adb82de58f9b91d84c2cd526576b881a0c5a \\\n    --hash=sha256:bc4c368f42b551bf72fac35c5128963a171b40dce866fb066540eeaf46faa003 \\\n    --hash=sha256:c39a8da13cede5adcd3be1182883aea1c925476f4e84b2807a46e2775306305d \\\n    --hash=sha256:c3ac844a0fe00bfaeb2c9b51ab1424e5c8744f89860b138434a363b1f620f354 \\\n    --hash=sha256:c4c00e0b0597c8e4f59e8d461f797e5d70b4d025880516a8261b2817c47759ee \\\n    --hash=sha256:c74a62747864ed568f5a82a49a23a8d7fe171d0c69038b38cedf0976831296fa \\\n    --hash=sha256:dd05f7783b3274aa206a1af06f0ceed3f9b412cf665b7247eacd83be41cf7bf0 \\\n    --hash=sha256:dfd681c5dc216037e0b0a2c821f5ed99ba9f03ebcf119c7dac0e9a7b960b9ec9 \\\n    --hash=sha256:e474390e60ed609cec869b0da796ad94f420bb057d86784191eefc62b65819ae \\\n    --hash=sha256:f76d097d12c82a535fda9dfe5e8dd4127952b45fea9b0276cb30cca5ea313fbc\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx autopydantic Template for Ray Project\nDESCRIPTION: A reStructuredText template that sets up documentation for a Pydantic model using the autopydantic_model directive. It excludes certain elements like Config and validators while inheriting from BaseModel.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/autosummary/autopydantic.rst#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autopydantic_model:: {{ fullname }}\n    :inherited-members: BaseModel\n    :exclude-members: Config\n    :model-show-config-summary: False\n    :model-show-validator-summary: False\n    :model-show-field-summary: False\n    :field-list-validators: False\n    :model-show-json: False\n    :undoc-members: \n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies\nDESCRIPTION: Command to install the required dependencies for building the Ray documentation using pip.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/README.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements-doc.txt\n```\n\n----------------------------------------\n\nTITLE: Using ARM-based Docker Images for Apple Silicon MacBooks\nDESCRIPTION: Command showing how to specify an ARM-compatible Ray Docker image for use with Apple Silicon MacBooks (M1/M2). These platform-specific images are required until Ray implements multi-architecture images.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting/troubleshooting.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nimage: rayproject/ray:2.41.0-aarch64\n```\n\n----------------------------------------\n\nTITLE: Displaying Training Status and Resource Usage ASCII Table\nDESCRIPTION: ASCII-formatted status table showing system resource usage and training results for multiple RL algorithms. Includes memory usage, CPU/GPU allocation, and detailed metrics for each trial including rewards, runtime, and training progress.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.2/rllib_regression.txt#2025-04-12_snippet_0\n\nLANGUAGE: ascii\nCODE:\n```\n== Status ==\nMemory usage on this node: 43.4/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/64 CPUs, 0.0/8 GPUs, 0.0/440.23 GiB heap, 0.0/12.84 GiB objects\nResult logdir: /home/ubuntu/ray_results/apex\nResult logdir: /home/ubuntu/ray_results/atari-a2c\nResult logdir: /home/ubuntu/ray_results/atari-basic-dqn\nResult logdir: /home/ubuntu/ray_results/atari-impala\nResult logdir: /home/ubuntu/ray_results/atari-ppo-tf\nResult logdir: /home/ubuntu/ray_results/atari-ppo-torch\nNumber of trials: 24 (24 TERMINATED)\nTable truncated to 20 rows. 4 trials (4 TERMINATED) not shown.\n+--------------------------------------+------------+-------+----------+------------------+---------+--------+\n| Trial name                           | status     | loc   |   reward |   total time (s) |      ts |   iter |\n|--------------------------------------+------------+-------+----------+------------------+---------+--------|\n| A2C_BreakoutNoFrameskip-v4_c8ad5a48  | TERMINATED |       |   139.19 |          3606.77 | 3686000 |    352 |\n| A2C_BreakoutNoFrameskip-v4_c8ad1c54  | TERMINATED |       |    75.56 |          3601.57 | 2932000 |    349 |\n| A2C_BreakoutNoFrameskip-v4_c8acd28a  | TERMINATED |       |   131.97 |          3603.39 | 2928000 |    349 |\n| A2C_BreakoutNoFrameskip-v4_c8ac8d16  | TERMINATED |       |   105.42 |          3601.03 | 2901500 |    349 |\n| DQN_BreakoutNoFrameskip-v4_c8af8a02  | TERMINATED |       |    15.81 |          3665.65 |  270000 |     27 |\n| DQN_BreakoutNoFrameskip-v4_c8af079e  | TERMINATED |       |    11.32 |          3612.1  |  270000 |     27 |\n| APEX_BreakoutNoFrameskip-v4_c8ac4694 | TERMINATED |       |    50.56 |          3627.89 | 5786880 |    115 |\n| DQN_BreakoutNoFrameskip-v4_c8ae61ae  | TERMINATED |       |     7.14 |          3620.61 |  270000 |     27 |\n| DQN_BreakoutNoFrameskip-v4_c8adbcea  | TERMINATED |       |    11.24 |          3640.35 |  270000 |     27 |\n| APEX_BreakoutNoFrameskip-v4_c8abef3c | TERMINATED |       |    94.5  |          3625.19 | 5820800 |    115 |\n| PPO_BreakoutNoFrameskip-v4_c8ab0572  | TERMINATED |       |    25.26 |          3603.23 | 1335000 |    267 |\n| PPO_BreakoutNoFrameskip-v4_c8aabf36  | TERMINATED |       |    18.2  |          3603.36 | 1300000 |    260 |\n| APEX_BreakoutNoFrameskip-v4_c8abaa86 | TERMINATED |       |    90.98 |          3627.03 | 7350400 |    116 |\n| PPO_BreakoutNoFrameskip-v4_c8aa6f5e  | TERMINATED |       |    17.01 |          3611.01 | 1555000 |    311 |\n| PPO_BreakoutNoFrameskip-v4_c8aa27e2  | TERMINATED |       |    22.41 |          3609.64 | 1545000 |    309 |\n| PPO_BreakoutNoFrameskip-v4_c8a9e39a  | TERMINATED |       |    61.25 |          3602.17 | 4475000 |    895 |\n| PPO_BreakoutNoFrameskip-v4_c8a97978  | TERMINATED |       |    28.19 |          3601.33 | 4415000 |    883 |\n| PPO_BreakoutNoFrameskip-v4_c8a904ca  | TERMINATED |       |    41.3  |          3600.42 | 4515000 |    903 |\n| APEX_BreakoutNoFrameskip-v4_c8ab5108 | TERMINATED |       |    62.46 |          3626.37 | 5091840 |    114 |\n| PPO_BreakoutNoFrameskip-v4_c8a88004  | TERMINATED |       |    60.44 |          3602.52 | 3380000 |    676 |\n+--------------------------------------+------------+-------+----------+------------------+---------+--------+\n```\n\n----------------------------------------\n\nTITLE: Verifying TLS Authentication in Ray Worker Pod\nDESCRIPTION: This snippet contains a sequence of commands to verify TLS authentication in a Ray worker Pod. It checks the connection to the Ray head service using ray health-check and demonstrates the impact of DNS entries in certificate alt_names on successful connections. Requires access to the Kubernetes cluster and permissions to execute into the worker Pod.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/configure.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec -it ${WORKER_POD} -- bash\nray health-check --address service-ray-head.default.svc.cluster.local:6379\nray health-check --address service-ray-head:6379\n```\n\n----------------------------------------\n\nTITLE: Invalid Multiple Output Paths Example in RLlib\nDESCRIPTION: Example showing unsupported configuration of multiple output paths for cloud storage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nconfig= (\n    AlgorithmConfig()\n    .offline_data(\n        output=[\"gs://<your-bucket>/dir1\", \"gs://<your-bucket>/dir2\"],\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Ray List Actors Command\nDESCRIPTION: Displays the output format of the 'ray list actors' command, showing statistics on the total number of actors and a table with details about each actor including ID, class name, name, PID and state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/key-concepts.rst#2025-04-12_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n======== List: 2022-07-23 21:29:39.323925 ========\nStats:\n------------------------------\nTotal: 2\n\nTable:\n------------------------------\n    ACTOR_ID                          CLASS_NAME    NAME      PID  STATE\n0  31405554844820381c2f0f8501000000  Actor                 96956  ALIVE\n1  f36758a9f8871a9ca993b1d201000000  Actor                 96955  ALIVE\n```\n\n----------------------------------------\n\nTITLE: Configuring lz4 package dependency with hash verification\nDESCRIPTION: Specification for the lz4 package (version 4.3.3) with SHA256 hash values for verification. The file includes comments indicating this dependency is required by the Ray project's requirements.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_14\n\nLANGUAGE: plain\nCODE:\n```\nlz4==4.3.3 \\\n    --hash=sha256:01fe674ef2889dbb9899d8a67361e0c4a2c833af5aeb37dd505727cf5d2a131e \\\n    --hash=sha256:054b4631a355606e99a42396f5db4d22046a3397ffc3269a348ec41eaebd69d2 \\\n    --hash=sha256:0a136e44a16fc98b1abc404fbabf7f1fada2bdab6a7e970974fb81cf55b636d0 \\\n    --hash=sha256:0e9c410b11a31dbdc94c05ac3c480cb4b222460faf9231f12538d0074e56c563 \\\n    --hash=sha256:222a7e35137d7539c9c33bb53fcbb26510c5748779364014235afc62b0ec797f \\\n    --hash=sha256:24b3206de56b7a537eda3a8123c644a2b7bf111f0af53bc14bed90ce5562d1aa \\\n    --hash=sha256:2b901c7784caac9a1ded4555258207d9e9697e746cc8532129f150ffe1f6ba0d \\\n    --hash=sha256:2f7b1839f795315e480fb87d9bc60b186a98e3e5d17203c6e757611ef7dcef61 \\\n    --hash=sha256:30e8c20b8857adef7be045c65f47ab1e2c4fabba86a9fa9a997d7674a31ea6b6 \\\n    --hash=sha256:31ea4be9d0059c00b2572d700bf2c1bc82f241f2c3282034a759c9a4d6ca4dc2 \\\n    --hash=sha256:337cb94488a1b060ef1685187d6ad4ba8bc61d26d631d7ba909ee984ea736be1 \\\n    --hash=sha256:33c9a6fd20767ccaf70649982f8f3eeb0884035c150c0b818ea660152cf3c809 \\\n    --hash=sha256:363ab65bf31338eb364062a15f302fc0fab0a49426051429866d71c793c23394 \\\n    --hash=sha256:43cf03059c0f941b772c8aeb42a0813d68d7081c009542301637e5782f8a33e2 \\\n    --hash=sha256:56f4fe9c6327adb97406f27a66420b22ce02d71a5c365c48d6b656b4aaeb7775 \\\n    --hash=sha256:5d35533bf2cee56f38ced91f766cd0038b6abf46f438a80d50c52750088be93f \\\n    --hash=sha256:6756212507405f270b66b3ff7f564618de0606395c0fe10a7ae2ffcbbe0b1fba \\\n    --hash=sha256:6cdc60e21ec70266947a48839b437d46025076eb4b12c76bd47f8e5eb8a75dcc \\\n    --hash=sha256:abc197e4aca8b63f5ae200af03eb95fb4b5055a8f990079b5bdf042f568469dd \\\n    --hash=sha256:b14d948e6dce389f9a7afc666d60dd1e35fa2138a8ec5306d30cd2e30d36b40c \\\n    --hash=sha256:b47839b53956e2737229d70714f1d75f33e8ac26e52c267f0197b3189ca6de24 \\\n    --hash=sha256:b6d9ec061b9eca86e4dcc003d93334b95d53909afd5a32c6e4f222157b50c071 \\\n    --hash=sha256:b891880c187e96339474af2a3b2bfb11a8e4732ff5034be919aa9029484cd201 \\\n    --hash=sha256:bca8fccc15e3add173da91be8f34121578dc777711ffd98d399be35487c934bf \\\n    --hash=sha256:c81703b12475da73a5d66618856d04b1307e43428a7e59d98cfe5a5d608a74c6 \\\n    --hash=sha256:d2507ee9c99dbddd191c86f0e0c8b724c76d26b0602db9ea23232304382e1f21 \\\n    --hash=sha256:e36cd7b9d4d920d3bfc2369840da506fa68258f7bb176b8743189793c055e43d \\\n    --hash=sha256:e7d84b479ddf39fe3ea05387f10b779155fc0990125f4fb35d636114e1c63a2e \\\n    --hash=sha256:eac9af361e0d98335a02ff12fb56caeb7ea1196cf1a49dbf6f17828a131da807 \\\n    --hash=sha256:edfd858985c23523f4e5a7526ca6ee65ff930207a7ec8a8f57a01eae506aaee7 \\\n    --hash=sha256:ee9ff50557a942d187ec85462bb0960207e7ec5b19b3b48949263993771c6205 \\\n    --hash=sha256:f0e822cd7644995d9ba248cb4b67859701748a93e2ab7fc9bc18c599a52e4604 \\\n    --hash=sha256:f180904f33bdd1e92967923a43c22899e303906d19b2cf8bb547db6653ea6e7d \\\n    --hash=sha256:f1d18718f9d78182c6b60f568c9a9cec8a7204d7cb6fad4e511a2ef279e4cb05 \\\n    --hash=sha256:f4c7bf687303ca47d69f9f0133274958fd672efaa33fb5bcde467862d6c621f0\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Sources for Ray Dependencies\nDESCRIPTION: This section defines the primary and additional package sources for Ray dependencies, including PyPI and specific URLs for PyTorch and PyG wheels compatible with CUDA 12.4.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cu124\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html\n```\n\n----------------------------------------\n\nTITLE: Configuring FSDP Strategy for Distributed Training\nDESCRIPTION: Sets up the Fully Sharded Data Parallel (FSDP) strategy for distributed training, including sharding policy, prefetching, and activation checkpointing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport lightning.pytorch as pl \n\nfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\nfrom torch.distributed.fsdp import ShardingStrategy, BackwardPrefetch\nfrom transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXLayer\n\nfrom ray.train.lightning import RayFSDPStrategy\n\n\n# Define the model sharding policy:\n# Wrap every GPTNeoXLayer as its own FSDP instance\nauto_wrap_policy = functools.partial(\n    transformer_auto_wrap_policy,\n    transformer_layer_cls = {GPTNeoXLayer}\n)\n\nfsdp_strategy = RayFSDPStrategy(\n    sharding_strategy=ShardingStrategy.FULL_SHARD,\n    backward_prefetch=BackwardPrefetch.BACKWARD_PRE,\n    forward_prefetch=True,\n    auto_wrap_policy=auto_wrap_policy,\n    limit_all_gathers=True,\n    activation_checkpointing=[GPTNeoXLayer],\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying googleapis-common-protos Package with Hash Values\nDESCRIPTION: Requirement specification for googleapis-common-protos version 1.61.0 with SHA-256 hash values for verification. This package is required by google-api-core and opentelemetry-exporter-otlp-proto-grpc in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_9\n\nLANGUAGE: text\nCODE:\n```\ngoogleapis-common-protos==1.61.0 \\\n    --hash=sha256:22f1915393bb3245343f6efe87f6fe868532efc12aa26b391b15132e1279f1c0 \\\n    --hash=sha256:8a64866a97f6304a7179873a465d6eee97b7a24ec6cfd78e0f575e96b821240b\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   google-api-core\n    #   opentelemetry-exporter-otlp-proto-grpc\n```\n\n----------------------------------------\n\nTITLE: Listing Package Dependencies with SHA-256 Hash Values for Ray Project\nDESCRIPTION: A requirements file listing Python package dependencies with their corresponding SHA-256 hash values for security and integrity verification. The file includes packages like google-resumable-media, googleapis-common-protos, and grpcio with their exact versions and hash values.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:2e920d506ec85eb4ba50cd4228c2bec05642894d4c73c59b3a2fe20346bd00ee \\\n--hash=sha256:3359fc442a743e870f4588fcf5dcbc1bf929df1fad8fb9905cd94e5edb02e84c \\\n--hash=sha256:37933ec6e693e51a5b07505bd05de57eee12f3e8c32b07da7e73669398e6630a \\\n--hash=sha256:398af5e3ba9cf768787eef45c803ff9614cc3e22a5b2f7d7ae116df8b11e3314 \\\n--hash=sha256:3b747a674c20a67343cb61d43fdd9207ce5da6a99f629c6e2541aa0e89215bcd \\\n--hash=sha256:461665ff58895f508e2866824a47bdee72497b091c730071f2b7575d5762ab65 \\\n--hash=sha256:4c6fdd4fccbec90cc8a01fc00773fcd5fa28db683c116ee3cb35cd5da9ef6c37 \\\n--hash=sha256:5829b792bf5822fd0a6f6eb34c5f81dd074f01d570ed7f36aa101d6fc7a0a6e4 \\\n--hash=sha256:596d1f98fc70232fcb6590c439f43b350cb762fb5d61ce7b0e9db4539654cc13 \\\n--hash=sha256:5ae44e10a8e3407dbe138984f21e536583f2bba1be9491239f942c2464ac0894 \\\n--hash=sha256:635f5d4dd18758a1fbd1049a8e8d2fee4ffed124462d837d1a02a0e009c3ab31 \\\n--hash=sha256:64e52e2b3970bd891309c113b54cf0e4384762c934d5ae56e283f9a0afcd953e \\\n--hash=sha256:66741ef4ee08ea0b2cc3c86916ab66b6aef03768525627fd6a1b34968b4e3709 \\\n--hash=sha256:67b741654b851abafb7bc625b6d1cdd520a379074e64b6a128e3b688c3c04740 \\\n--hash=sha256:6ac08d24c1f16bd2bf5eca8eaf8304812f44af5cfe5062006ec676e7e1d50afc \\\n--hash=sha256:6f998db4e71b645350b9ac28a2167e6632c239963ca9da411523bb439c5c514d \\\n--hash=sha256:72218785ce41b9cfd2fc1d6a017dc1ff7acfc4c17d01053265c41a2c0cc39b8c \\\n--hash=sha256:74dea7751d98034887dbd821b7aae3e1d36eda111d6ca36c206c44478035709c \\\n--hash=sha256:759ce4851a4bb15ecabae28f4d2e18983c244eddd767f560165563bf9aefbc8d \\\n--hash=sha256:77e2fd3057c9d78e225fa0a2160f96b64a824de17840351b26825b0848022906 \\\n--hash=sha256:7c074fece789b5034b9b1404a1f8208fc2d4c6ce9decdd16e8220c5a793e6f61 \\\n--hash=sha256:7c42c70cd1d362284289c6273adda4c6af8039a8ae12dc451dcd61cdabb8ab57 \\\n--hash=sha256:7f57f14606cd1dd0f0de396e1e53824c371e9544a822648cd76c034d209b559c \\\n--hash=sha256:83c681c526a3439b5cf94f7420471705bbf96262f49a6fe546a6db5f687a3d4a \\\n--hash=sha256:8485b340a6a9e76c62a7dce3c98e5f102c9219f4cfbf896a00cf48caf078d438 \\\n--hash=sha256:84e6e8cd997930fc66d5bb4fde61e2b62ba19d62b7abd7a69920406f9ecca946 \\\n--hash=sha256:89284716bc6a5a415d4eaa11b1726d2d60a0cd12aadf5439828353662ede9dd7 \\\n--hash=sha256:8b87e1a59c38f275c0e3676fc2ab6d59eccecfd460be267ac360cc31f7bcde96 \\\n--hash=sha256:8f24ed114432de109aa9fd317278518a5af2d31ac2ea6b952b2f7782b43da091 \\\n--hash=sha256:98cb4d057f285bd80d8778ebc4fde6b4d509ac3f331758fb1528b733215443ae \\\n--hash=sha256:998679bf62b7fb599d2878aa3ed06b9ce688b8974893e7223c60db155f26bd8d \\\n--hash=sha256:9ba053c5f50430a3fcfd36f75aff9caeba0440b2d076afdb79a318d6ca245f88 \\\n--hash=sha256:9c99616c853bb585301df6de07ca2cadad344fd1ada6d62bb30aec05219c45d2 \\\n--hash=sha256:a1fd716e7a01f8e717490fbe2e431d2905ab8aa598b9b12f8d10abebb36b04dd \\\n--hash=sha256:a2355cba1f4ad8b6988a4ca3feed5bff33f6af2d7f134852cf279c2aebfde541 \\\n--hash=sha256:b1f8133c9a275df5613a451e73f36c2aea4fe13c5c8997e22cf355ebd7bd0728 \\\n--hash=sha256:b8667b48e7a7ef66afba2c81e1094ef526388d35b873966d8a9a447974ed9178 \\\n--hash=sha256:ba1eb1843304b1e5537e1fca632fa894d6f6deca8d6389636ee5b4797affb968 \\\n--hash=sha256:be82c3c8cfb15b30f36768797a640e800513793d6ae1724aaaafe5bf86f8f346 \\\n--hash=sha256:c02ec1c5856179f171e032a31d6f8bf84e5a75c45c33b2e20a3de353b266ebd8 \\\n--hash=sha256:c672d99a345849301784604bfeaeba4db0c7aae50b95be04dd651fd2a7310b93 \\\n--hash=sha256:c6c777a480337ac14f38564ac88ae82d4cd238bf293f0a22295b66eb89ffced7 \\\n--hash=sha256:cae0274952c079886567f3f4f685bcaf5708f0a23a5f5216fdab71f81a6c0273 \\\n--hash=sha256:cd67cf24a553339d5062eff51013780a00d6f97a39ca062781d06b3a73b15462 \\\n--hash=sha256:d3515f198eaa2f0ed49f8819d5732d70698c3fa37384146079b3799b97667a94 \\\n--hash=sha256:d5280312b9af0976231f9e317c20e4a61cd2f9629b7bfea6a693d1878a264ebd \\\n--hash=sha256:de06adc872bcd8c2a4e0dc51250e9e65ef2ca91be023b9d13ebd67c2ba552e1e \\\n--hash=sha256:e1674e4307fa3024fc897ca774e9c7562c957af85df55efe2988ed9056dc4e57 \\\n--hash=sha256:e2096eddb4e7c7bdae4bd69ad364e55e07b8316653234a56552d9c988bd2d61b \\\n--hash=sha256:e560628513ed34759456a416bf86b54b2476c59144a9138165c9a1575801d0d9 \\\n--hash=sha256:edfedb64740750e1a3b16152620220f51d58ff1b4abceb339ca92e934775c27a \\\n--hash=sha256:f13cae8cc389a440def0c8c52057f37359014ccbc9dc1f0827936bcd367c6100 \\\n--hash=sha256:f314013e7dcd5cf45ab1945d92e713eec788166262ae8deb2cfacd53def27325 \\\n--hash=sha256:f583edb943cf2e09c60441b910d6a20b4d9d626c75a36c8fcac01a6c96c01183 \\\n--hash=sha256:fd8536e902db7e365f49e7d9029283403974ccf29b13fc7028b97e2295b33556 \\\n--hash=sha256:fe70e325aa68fa4b5edf7d1a4b6f691eb04bbccac0ace68e34820d283b5f80d4\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   google-cloud-storage\n#   google-resumable-media\n```\n\n----------------------------------------\n\nTITLE: Checking Ray Version and Installation Path in Bash\nDESCRIPTION: This command imports Ray and prints its version and installation path. It's executed within a virtual environment set up for the Ray runtime.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\n\"/tmp/ray/session_2022-02-28_14-12-29_909064_87908/runtime_resources/pip/0cc818a054853c3841171109300436cad4dcf594/virtualenv/bin/python -c 'import ray; print(ray.__version__, ray.__path__[0])'\"\n```\n\n----------------------------------------\n\nTITLE: Python Package Hash Verification for zipp Package\nDESCRIPTION: SHA-256 hash verification entries for the zipp package version 3.18.1, which is required by importlib-metadata. The file also includes a warning about unpinned packages that require pinning when using hash verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nzipp==3.18.1 \\\n    --hash=sha256:206f5a15f2af3dbaee80769fb7dc6f249695e940acca08dfb2a4769fe61e538b \\\n    --hash=sha256:2884ed22e7d8961de1c9a05142eb69a247f120291bc0206a00a7642f09b5b715\n    # via importlib-metadata\n\n# WARNING: The following packages were not pinned, but pip requires them to be\n# pinned when the requirements file includes hashes. Consider using the --allow-unsafe flag.\n# setuptools\n```\n\n----------------------------------------\n\nTITLE: Switching AlgorithmConfig API Stack in Python\nDESCRIPTION: This snippet demonstrates how to switch to the new API stack in AlgorithmConfig, which is used to enable RLlib's RLModule and EnvRunnerConduct environments. This migration is important for compatibility with the newer versions of the API stack.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .api_stack(\n        enable_rl_module_and_learner=True,\n        enable_env_runner_and_connector_v2=True,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Package Requirement: markupsafe with Hashes\nDESCRIPTION: Specifies the markupsafe package version 2.1.3 with multiple SHA-256 hashes for verification. This package is essential for safely handling HTML and XML markup in templates and web applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nmarkupsafe==2.1.3 \\\n    --hash=sha256:05fb21170423db021895e1ea1e1f3ab3adb85d1c2333cbc2310f2a26bc77272e \\\n    --hash=sha256:0a4e4a1aff6c7ac4cd55792abf96c915634c2b97e3cc1c7129578aa68ebd754e \\\n    --hash=sha256:10bbfe99883db80bdbaff2dcf681dfc6533a614f700da1287707e8a5d78a8431 \\\n    --hash=sha256:134da1eca9ec0ae528110ccc9e48041e0828d79f24121a1a146161103c76e686 \\\n    --hash=sha256:14ff806850827afd6b07a5f32bd917fb7f45b046ba40c57abdb636674a8b559c \\\n    --hash=sha256:1577735524cdad32f9f694208aa75e422adba74f1baee7551620e43a3141f559 \\\n    --hash=sha256:1b40069d487e7edb2676d3fbdb2b0829ffa2cd63a2ec26c4938b2d34391b4ecc \\\n    --hash=sha256:1b8dd8c3fd14349433c79fa8abeb573a55fc0fdd769133baac1f5e07abf54aeb \\\n    --hash=sha256:1f67c7038d560d92149c060157d623c542173016c4babc0c1913cca0564b9939 \\\n    --hash=sha256:282c2cb35b5b673bbcadb33a585408104df04f14b2d9b01d4c345a3b92861c2c \\\n    --hash=sha256:2c1b19b3aaacc6e57b7e25710ff571c24d6c3613a45e905b1fde04d691b98ee0 \\\n    --hash=sha256:2ef12179d3a291be237280175b542c07a36e7f60718296278d8593d21ca937d4 \\\n    --hash=sha256:338ae27d6b8745585f87218a3f23f1512dbf52c26c28e322dbe54bcede54ccb9 \\\n    --hash=sha256:3c0fae6c3be832a0a0473ac912810b2877c8cb9d76ca48de1ed31e1c68386575 \\\n    --hash=sha256:3fd4abcb888d15a94f32b75d8fd18ee162ca0c064f35b11134be77050296d6ba \\\n    --hash=sha256:42de32b22b6b804f42c5d98be4f7e5e977ecdd9ee9b660fda1a3edf03b11792d \\\n    --hash=sha256:47d4f1c5f80fc62fdd7777d0d40a2e9dda0a05883ab11374334f6c4de38adffd \\\n    --hash=sha256:504b320cd4b7eff6f968eddf81127112db685e81f7e36e75f9f84f0df46041c3 \\\n    --hash=sha256:525808b8019e36eb524b8c68acdd63a37e75714eac50e988180b169d64480a00 \\\n    --hash=sha256:56d9f2ecac662ca1611d183feb03a3fa4406469dafe241673d521dd5ae92a155 \\\n    --hash=sha256:5bbe06f8eeafd38e5d0a4894ffec89378b6c6a625ff57e3028921f8ff59318ac \\\n    --hash=sha256:65c1a9bcdadc6c28eecee2c119465aebff8f7a584dd719facdd9e825ec61ab52 \\\n    --hash=sha256:68e78619a61ecf91e76aa3e6e8e33fc4894a2bebe93410754bd28fce0a8a4f9f \\\n    --hash=sha256:69c0f17e9f5a7afdf2cc9fb2d1ce6aabdb3bafb7f38017c0b77862bcec2bbad8 \\\n    --hash=sha256:6b2b56950d93e41f33b4223ead100ea0fe11f8e6ee5f641eb753ce4b77a7042b \\\n    --hash=sha256:715d3562f79d540f251b99ebd6d8baa547118974341db04f5ad06d5ea3eb8007 \\\n    --hash=sha256:787003c0ddb00500e49a10f2844fac87aa6ce977b90b0feaaf9de23c22508b24 \\\n    --hash=sha256:7ef3cb2ebbf91e330e3bb937efada0edd9003683db6b57bb108c4001f37a02ea \\\n    --hash=sha256:8023faf4e01efadfa183e863fefde0046de576c6f14659e8782065bcece22198 \\\n    --hash=sha256:8758846a7e80910096950b67071243da3e5a20ed2546e6392603c096778d48e0 \\\n    --hash=sha256:8afafd99945ead6e075b973fefa56379c5b5c53fd8937dad92c662da5d8fd5ee \\\n    --hash=sha256:8c41976a29d078bb235fea9b2ecd3da465df42a562910f9022f1a03107bd02be \\\n    --hash=sha256:8e254ae696c88d98da6555f5ace2279cf7cd5b3f52be2b5cf97feafe883b58d2 \\\n    --hash=sha256:8f9293864fe09b8149f0cc42ce56e3f0e54de883a9de90cd427f191c346eb2e1 \\\n    --hash=sha256:9402b03f1a1b4dc4c19845e5c749e3ab82d5078d16a2a4c2cd2df62d57bb0707 \\\n    --hash=sha256:962f82a3086483f5e5f64dbad880d31038b698494799b097bc59c2edf392fce6 \\\n    --hash=sha256:9aad3c1755095ce347e26488214ef77e0485a3c34a50c5a5e2471dff60b9dd9c \\\n    --hash=sha256:9dcdfd0eaf283af041973bff14a2e143b8bd64e069f4c383416ecd79a81aab58 \\\n    --hash=sha256:aa57bd9cf8ae831a362185ee444e15a93ecb2e344c8e52e4d721ea3ab6ef1823 \\\n    --hash=sha256:aa7bd130efab1c280bed0f45501b7c8795f9fdbeb02e965371bbef3523627779 \\\n    --hash=sha256:ab4a0df41e7c16a1392727727e7998a467472d0ad65f3ad5e6e765015df08636 \\\n    --hash=sha256:ad9e82fb8f09ade1c3e1b996a6337afac2b8b9e365f926f5a61aacc71adc5b3c \\\n    --hash=sha256:af598ed32d6ae86f1b747b82783958b1a4ab8f617b06fe68795c7f026abbdcad \\\n    --hash=sha256:b076b6226fb84157e3f7c971a47ff3a679d837cf338547532ab866c57930dbee \\\n    --hash=sha256:b7ff0f54cb4ff66dd38bebd335a38e2c22c41a8ee45aa608efc890ac3e3931bc \\\n    --hash=sha256:bfce63a9e7834b12b87c64d6b155fdd9b3b96191b6bd334bf37db7ff1fe457f2 \\\n    --hash=sha256:c011a4149cfbcf9f03994ec2edffcb8b1dc2d2aede7ca243746df97a5d41ce48 \\\n    --hash=sha256:c9c804664ebe8f83a211cace637506669e7890fec1b4195b505c214e50dd4eb7 \\\n    --hash=sha256:ca379055a47383d02a5400cb0d110cef0a776fc644cda797db0c5696cfd7e18e \\\n    --hash=sha256:cb0932dc158471523c9637e807d9bfb93e06a95cbf010f1a38b98623b929ef2b \\\n    --hash=sha256:cd0f502fe016460680cd20aaa5a76d241d6f35a1c3350c474bac1273803893fa \\\n    --hash=sha256:ceb01949af7121f9fc39f7d27f91be8546f3fb112c608bc4029aef0bab86a2a5 \\\n    --hash=sha256:d080e0a5eb2529460b30190fcfcc4199bd7f827663f858a226a81bc27beaa97e \\\n    --hash=sha256:dd15ff04ffd7e05ffcb7fe79f1b98041b8ea30ae9234aed2a9168b5797c3effb \\\n    --hash=sha256:df0be2b576a7abbf737b1575f048c23fb1d769f267ec4358296f31c2479db8f9 \\\n\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry Protocol Packages with Version Pinning\nDESCRIPTION: This code snippet demonstrates how to pin specific versions of OpenTelemetry protocol packages with hash verification, ensuring reproducible builds. The comments indicate dependency relationships between packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_24\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-exporter-otlp-proto-grpc==1.1.0 \\\n    --hash=sha256:281e9bbce73b08c1c93781cf7f4282396f74895987fdc051bea335f7dd086199 \\\n    --hash=sha256:5a4a86becf4f9fdf2910a5b869fc40ec9978044f93045fdce240fecb6c64681a\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   opentelemetry-exporter-otlp\nopentelemetry-proto==1.1.0 \\\n    --hash=sha256:237ef4fdd7f752b2fe740352643f8ef82733bd8e0db8b46ed808125ac7c7f112 \\\n    --hash=sha256:ff1ad9a3c572075883c2af0053cefdfaba005d71eade783c4524d34660d53b60\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Package dependency specifications with SHA256 hash verification. Contains version-pinned dependencies like mdurl, memray, mistral-common, mpmath, msgpack and msgspec with their corresponding hash values for security verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:e09031c87a1e51556fdcb46e5bd4f59dfb743061cf93c4d6831bf894f125eb57 \\\n--hash=sha256:e4dd52d80b8c83fdce44e12478ad2e85c64ea965e75d66dbeafb0a3e77308fcc \\\n--hash=sha256:f698de3fd0c4e6972b92290a45bd9b1536bffe8c6759c62471efaa8acb4c37bc \\\n--hash=sha256:fec21693218efe39aa7f8599346e90c705afa52c5b31ae019b2e57e8f6542bb2 \\\n--hash=sha256:ffcc3f7c66b5f5b7931a5aa68fc9cecc51e685ef90282f4a82f0f5e9b704ad11\n```\n\n----------------------------------------\n\nTITLE: Installing widgetsnbextension Package with Hash Verification\nDESCRIPTION: This snippet specifies the installation requirements for the widgetsnbextension package version 4.0.11 with SHA256 hash verification. It also includes comments indicating this dependency is required by ipywidgets and is constrained by a compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_47\n\nLANGUAGE: text\nCODE:\n```\nwidgetsnbextension==4.0.11 \\\n    --hash=sha256:55d4d6949d100e0d08b94948a42efc3ed6dfdc0e9468b2c4b128c9a2ce3a7a36 \\\n    --hash=sha256:8b22a8f1910bfd188e596fe7fc05dcbd87e810c8a4ba010bdb3da86637398474\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   ipywidgets\n```\n\n----------------------------------------\n\nTITLE: Defining Machine Learning Package Dependencies for Ray Project\nDESCRIPTION: This snippet shows machine learning-specific package dependencies like dm-tree with exact version numbers and SHA256 hash verification for secure installation. The extensive list of hashes ensures package integrity when installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_9\n\nLANGUAGE: pip\nCODE:\n```\ndm-tree==0.1.8 \\\n    --hash=sha256:054b461f8176f4bce7a21f7b1870f873a1ced3bdbe1282c816c550bb43c71fa6 \\\n    --hash=sha256:09964470f76a5201aff2e8f9b26842976de7889300676f927930f6285e256760 \\\n    --hash=sha256:0d3172394079a86c3a759179c65f64c48d1a42b89495fcf38976d11cc3bb952c \\\n    --hash=sha256:0e9620ccf06393eb6b613b5e366469304622d4ea96ae6540b28a33840e6c89cf \\\n    --hash=sha256:0fcaabbb14e7980377439e7140bd05552739ca5e515ecb3119f234acee4b9430 \\\n    --hash=sha256:1607ce49aa42f010d1e5e616d92ce899d66835d4d8bea49679582435285515de \\\n    --hash=sha256:181c35521d480d0365f39300542cb6cd7fd2b77351bb43d7acfda15aef63b317 \\\n    --hash=sha256:1d7c26e431fc93cc7e0cba867eb000db6a05f6f2b25af11ac4e9dada88fc5bca \\\n    --hash=sha256:1fe962015b2fe1282892b28ebe962faed53c7f98d942da9a4625cbf27baef913 \\\n    --hash=sha256:250b692fb75f45f02e2f58fbef9ab338904ef334b90557565621fa251df267cf \\\n    --hash=sha256:2869228d9c619074de501a3c10dc7f07c75422f8fab36ecdcb859b6f1b1ec3ef \\\n    --hash=sha256:28c52cbf4f8b3dbd0beaedf44f69fa85eec5e9dede612e08035e06ada6ec9426 \\\n    --hash=sha256:2f7915660f59c09068e428613c480150180df1060561fd0d1470684ae7007bd1 \\\n    --hash=sha256:343a4a4ebaa127451ff971254a4be4084eb4bdc0b2513c32b46f6f728fd03f9e \\\n    --hash=sha256:35cc164a79336bfcfafb47e5f297898359123bbd3330c1967f0c4994f9cf9f60 \\\n    --hash=sha256:378cc8ad93c5fe3590f405a309980721f021c790ca1bdf9b15bb1d59daec57f5 \\\n    --hash=sha256:39070ba268c0491af9fe7a58644d99e8b4f2cde6e5884ba3380bddc84ed43d5f \\\n    --hash=sha256:435227cf3c5dc63f4de054cf3d00183790bd9ead4c3623138c74dde7f67f521b \\\n    --hash=sha256:5483dca4d7eb1a0d65fe86d3b6a53ae717face83c1f17e0887b1a4a64ae5c410 \\\n    --hash=sha256:694c3654cfd2a81552c08ec66bb5c4a3d48fa292b9a181880fb081c36c5b9134 \\\n    --hash=sha256:75c5d528bb992981c20793b6b453e91560784215dffb8a5440ba999753c14ceb \\\n    --hash=sha256:803bfc53b4659f447ac694dbd04235f94a73ef7c1fd1e0df7c84ac41e0bc963b \\\n    --hash=sha256:81fce77f22a302d7a5968aebdf4efafef4def7ce96528719a354e6990dcd49c7 \\\n    --hash=sha256:83b7764de0d855338abefc6e3ee9fe40d301668310aa3baea3f778ff051f4393 \\\n    --hash=sha256:8c60a7eadab64c2278861f56bca320b2720f163dca9d7558103c3b77f2416571 \\\n    --hash=sha256:8ed3564abed97c806db122c2d3e1a2b64c74a63debe9903aad795167cc301368 \\\n    --hash=sha256:94d3f0826311f45ee19b75f5b48c99466e4218a0489e81c0f0167bda50cacf22 \\\n    --hash=sha256:96a548a406a6fb15fe58f6a30a57ff2f2aafbf25f05afab00c8f5e5977b6c715 \\\n    --hash=sha256:a5d819c38c03f0bb5b3b3703c60e4b170355a0fc6b5819325bf3d4ceb3ae7e80 \\\n    --hash=sha256:ad16ceba90a56ec47cf45b21856d14962ac314787975ef786efb5e6e9ca75ec7 \\\n    --hash=sha256:af4b3d372f2477dcd89a6e717e4a575ca35ccc20cc4454a8a4b6f8838a00672d \\\n    --hash=sha256:b095ba4f8ca1ba19350fd53cf1f8f3eb0bd406aa28af64a6dfc86707b32a810a \\\n    --hash=sha256:b9bd9b9ccb59409d33d51d84b7668010c04c2af7d4a371632874c1ca356cff3d \\\n    --hash=sha256:b9f89a454e98806b44fe9d40ec9eee61f848388f7e79ac2371a55679bd5a3ac6 \\\n    --hash=sha256:bb2d109f42190225112da899b9f3d46d0d5f26aef501c61e43529fe9322530b5 \\\n    --hash=sha256:c0a94aba18a35457a1b5cd716fd7b46c5dafdc4cf7869b4bae665b91c4682a8e \\\n    --hash=sha256:c5c8c12e3fda754ef6af94161bacdaeda816d941995fac415d6855c6c386af68 \\\n    --hash=sha256:d1612fcaecd79023dbc6a6ae48d51a80beb5c385d6f3f6d71688e57bc8d07de8 \\\n    --hash=sha256:d16e1f2a073604cfcc09f7131ae8d534674f43c3aef4c25742eae295bc60d04f \\\n    --hash=sha256:d20f2faa3672b52e5013f4077117bfb99c4cfc0b445d3bde1584c34032b57436 \\\n    --hash=sha256:d40fa4106ca6edc66760246a08f500ec0c85ef55c762fb4a363f6ee739ba02ee \\\n    --hash=sha256:de287fabc464b8734be251e46e06aa9aa1001f34198da2b6ce07bd197172b9cb \\\n    --hash=sha256:e4d714371bb08839e4e5e29024fc95832d9affe129825ef38836b143028bd144 \\\n    --hash=sha256:ea9e59e0451e7d29aece402d9f908f2e2a80922bcde2ebfd5dcb07750fcbfee8 \\\n    --hash=sha256:f7ac31b9aecccb2c6e1ab29706f6ded3eba0c2c69c770322c9c685929c3d6afb \\\n    --hash=sha256:fa42a605d099ee7d41ba2b5fb75e21423951fd26e5d50583a00471238fb3021d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies: google-api-core Configuration\nDESCRIPTION: Definition of the google-api-core package dependency with version 1.34.0 and corresponding SHA256 hashes. The comment indicates this package is required via python/requirements_compiled_ray_test_py311_cu121.txt and opencensus.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\ngoogle-api-core==1.34.0 \\\n    --hash=sha256:6fb380f49d19ee1d09a9722d0379042b7edb06c0112e4796c7a395078a043e71 \\\n    --hash=sha256:7421474c39d396a74dfa317dddbc69188f2336835f526087c7648f91105e32ff\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   opencensus\n```\n\n----------------------------------------\n\nTITLE: Ray Performance Metrics JSON Configuration\nDESCRIPTION: JSON object containing execution statistics, timing metrics, and deployment configuration for a Ray project. Includes success status, iteration timing details, runtime metrics, session URL, commit information, and stability flag.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/stress_tests/dead_actors.txt#2025-04-12_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": 1,\n  \"total_time\": 130.34314274787903,\n  \"avg_iteration_time\": 1.303428828716278,\n  \"max_iteration_time\": 3.651247501373291,\n  \"min_iteration_time\": 0.09438443183898926,\n  \"_runtime\": 902.0143933296204,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_pxDnaxYFzDNsyifjJNV1qhqs\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Check RayCluster Provisioning Status\nDESCRIPTION: Waits for the RayCluster associated with the rayjob-sample-shutdown job to be provisioned with a 500-second timeout.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nkubectl wait --for=condition=RayClusterProvisioned raycluster/$(kubectl get rayjob rayjob-sample-shutdown -o jsonpath='{.status.rayClusterName}') --timeout=500s\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and file hashes. It includes cachetools, certifi, and the beginning of cffi specification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\ncachetools==5.3.2 \\\n    --hash=sha256:086ee420196f7b2ab9ca2db2520aca326318b68fe5ba8bc4d49cca91add450f2 \\\n    --hash=sha256:861f35a13a451f94e301ce2bec7cac63e881232ccce7ed67fab9b5df4d3beaa1\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   google-auth\ncertifi==2025.1.31 \\\n    --hash=sha256:3d5da6925056f6f18f119200434a4780a94263f10d1c21d032a6f6b2baa20651 \\\n    --hash=sha256:ca78db4565a652026a4db2bcdf68f2fb589ea80d0be70e03929ed730746b84fe\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   requests\ncffi==1.16.0 ; platform_python_implementation != 'PyPy' \\\n    --hash=sha256:0c9ef6ff37e974b73c25eecc13952c55bceed9112be2d9d938ded8e856138bcc \\\n    --hash=sha256:131fd094d1065b19540c3d72594260f118b231090295d8c34e19a7bbcf2e860a \\\n    --hash=sha256:1b8ebc27c014c59692bb2664c7d13ce7a6e9a629be20e54e7271fa696ff2b417 \\\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA-Specific Package Dependency with Conditional Platform Check\nDESCRIPTION: This snippet shows how to specify a CUDA-specific package dependency (cupy-cuda12x) with platform-specific conditions that exclude macOS systems. Each package includes exact version numbers and SHA256 hash verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_7\n\nLANGUAGE: pip\nCODE:\n```\ncupy-cuda12x==13.1.0 ; sys_platform != 'darwin' \\\n    --hash=sha256:230f8a8e99c81a653baa0ed00819990c0ed1f0cf0298214786b5e323461dc61a \\\n    --hash=sha256:2d16eaa2d086e416ac13467d4ff3184b9a081fe76b761ce51d4a46ec1c4bd28a \\\n    --hash=sha256:432273fd4b61a284f7d705d08b8291403548fd422bcbd945635cc155bc6a923d \\\n    --hash=sha256:4c51a1062a3c5a826b0425952d229ffe73b1791656a31de95b318117e67a9576 \\\n    --hash=sha256:4c8e9fdb1f3ffc3151808f8bb8c871518d2783e1be8b53792b698a840543d60c \\\n    --hash=sha256:51b1d6cb83d82dfa306c9efaeb4d57f24bad3041ebd8716d61072676abbcf67b \\\n    --hash=sha256:52185a2cf95d3bac2c3fda95c9c8e06a985b5a00cd2e587d3caace337db33899 \\\n    --hash=sha256:5afb6658faa22f21479ae2c0a07254df31c0aebc36907a64a1f6be4ecc9e96da \\\n    --hash=sha256:d3dc91ef9c4104652195eea4b282d343ecad653021efe20d1c8dd8dfe8ccfd86 \\\n    --hash=sha256:d60d1e124592cb82a5f3f45b3e7bee7bda7b72a743029f275e9d6b125f338c60 \\\n    --hash=sha256:dac0284fecb90b5731f514e569a6fcf6674a730ae95b9490781a713b60a34423 \\\n    --hash=sha256:e7a25ef1b44ae6276b5105affc2289edb34f1aa6676babd5bcd80907348c4cfa\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   ray\n```\n\n----------------------------------------\n\nTITLE: Defining smmap dependency with version pinning and hash verification\nDESCRIPTION: Specifies smmap version 5.0.1 with SHA-256 hashes for verification. Comments indicate it's required by compiled ray test requirements and gitdb.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_37\n\nLANGUAGE: plaintext\nCODE:\n```\nsmmap==5.0.1 \\\n    --hash=sha256:dceeb6c0028fdb6734471eb07c0cd2aae706ccaecab45965ee83f11c8d3b1f62 \\\n    --hash=sha256:e6d8668fa5f93e706934a62d7b4db19c8d9eb8cf2adbb75ef1b675aa332b69da\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   gitdb\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Version and Hash\nDESCRIPTION: This snippet demonstrates how to specify a Python package dependency in a requirements file, including the package name, version, and SHA256 hash for verification. It also includes comments indicating which other packages or files depend on this package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nattrs==25.1.0 \\\n    --hash=sha256:1c97078a80c814273a76b2a298a932eb681c87415c11dee0a6921de7f1b02c3e \\\n    --hash=sha256:c75a69e28a550a7e93789579c22aa26b0f5b83b75dc4e08fe092980051e1090a\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   aiohttp\n    #   jsonschema\n    #   referencing\n```\n\n----------------------------------------\n\nTITLE: Specifying Packaging Dependency with Hash Verification\nDESCRIPTION: Defines the packaging package dependency with version 23.0 and includes SHA256 hash validations. Used by lazy-loader, scikit-image, and tensorboardx packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n```\n\n----------------------------------------\n\nTITLE: Example Ray Java 1.4.0 Release Command Sequence\nDESCRIPTION: This example demonstrates the actual commands used to release Ray Java version 1.4.0. It shows the specific commit ID, branch name, and version number used in a real release.\nSOURCE: https://github.com/ray-project/ray/blob/master/java/java-release-guide.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport TRAVIS_BRANCH=releases/1.4.0\nexport TRAVIS_COMMIT=3a09c82fbfce8f00533234844729e6d99fb0f24c\ngit checkout $TRAVIS_COMMIT\nsh java/build-jar-multiplatform.sh multiplatform\nexport GPG_SKIP=false\ncd java && mvn versions:set -DnewVersion=1.4.0 && cd -\nsh java/build-jar-multiplatform.sh deploy_jars\n```\n\n----------------------------------------\n\nTITLE: Specifying PSUtil Package Dependency with Hash Verification\nDESCRIPTION: Defines the psutil package dependency at version 5.9.6 with SHA256 hash verification. This package is used for process and system monitoring functionality and is required by ipykernel and vllm components.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\npsutil==5.9.6 \\\n    --hash=sha256:10e8c17b4f898d64b121149afb136c53ea8b68c7531155147867b7b1ac9e7e28 \\\n    --hash=sha256:18cd22c5db486f33998f37e2bb054cc62fd06646995285e02a51b1e08da97017 \\\n    --hash=sha256:3ebf2158c16cc69db777e3c7decb3c0f43a7af94a60d72e87b2823aebac3d602 \\\n    --hash=sha256:51dc3d54607c73148f63732c727856f5febec1c7c336f8f41fcbd6315cce76ac \\\n    --hash=sha256:6e5fb8dc711a514da83098bc5234264e551ad980cec5f85dabf4d38ed6f15e9a \\\n    --hash=sha256:70cb3beb98bc3fd5ac9ac617a327af7e7f826373ee64c80efd4eb2856e5051e9 \\\n    --hash=sha256:748c9dd2583ed86347ed65d0035f45fa8c851e8d90354c122ab72319b5f366f4 \\\n    --hash=sha256:91ecd2d9c00db9817a4b4192107cf6954addb5d9d67a969a4f436dbc9200f88c \\\n    --hash=sha256:92e0cc43c524834af53e9d3369245e6cc3b130e78e26100d1f63cdb0abeb3d3c \\\n    --hash=sha256:a6f01f03bf1843280f4ad16f4bde26b817847b4c1a0db59bf6419807bc5ce05c \\\n    --hash=sha256:c69596f9fc2f8acd574a12d5f8b7b1ba3765a641ea5d60fb4736bf3c08a8214a \\\n    --hash=sha256:ca2780f5e038379e520281e4c032dddd086906ddff9ef0d1b9dcf00710e5071c \\\n    --hash=sha256:daecbcbd29b289aac14ece28eca6a3e60aa361754cf6da3dfb20d4d32b6c7f57 \\\n    --hash=sha256:e4b92ddcd7dd4cdd3f900180ea1e104932c7bce234fb88976e2a3b296441225a \\\n    --hash=sha256:fb8a697f11b0f5994550555fcfe3e69799e5b060c8ecf9e2f75c69302cc35c0d \\\n    --hash=sha256:ff18b8d1a784b810df0b0fff3bcb50ab941c3b8e2c8de5726f9c71c601c611aa\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry-API Dependency with Hash Verification\nDESCRIPTION: Defines the opentelemetry-api package dependency with version 1.1.0 and includes SHA256 hash validations. This is used by opentelemetry-exporter-otlp-proto-grpc and opentelemetry-sdk packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-api==1.1.0 \\\n    --hash=sha256:38555cd773df903a2f7440778d6f8b48a86fd388604b171969bdbde4b746a558 \\\n    --hash=sha256:704a3b2a7511d2c9065013d362a8371bc452ae6c0521941de680af2a5ca94884\n```\n\n----------------------------------------\n\nTITLE: Customizing CLI Output for Ray Tune with CLIReporter\nDESCRIPTION: Configures a CLIReporter to specify which parameter and metric columns should be displayed in the command line output during hyperparameter tuning. This helps focus on relevant information like model architecture parameters, learning rate, batch size, and performance metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nreporter = CLIReporter(\n    parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n    metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Pandocfilters Package with Hash Verification in Bash\nDESCRIPTION: Defines the pandocfilters package version 1.5.0 with SHA256 hash verification for secure package installation. Required by nbconvert.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\npandocfilters==1.5.0 \\\n    --hash=sha256:0b679503337d233b4339a817bfc8c50064e2eff681314376a47cb582305a7a38 \\\n    --hash=sha256:33aae3f25fd1a026079f5d27bdd52496f0e0803b3469282162bafdcbdf6ef14f\n```\n\n----------------------------------------\n\nTITLE: Configuring DataContext in Ray Data (Python)\nDESCRIPTION: Shows how to modify DataContext attributes for general Ray Data options. This example sets the verbose_stats_logs option to True.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/execution-configurations.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nctx = ray.data.DataContext.get_current()\nctx.verbose_stats_logs = True\n```\n\n----------------------------------------\n\nTITLE: Defining sphinxcontrib-devhelp dependency with version pinning and hash verification\nDESCRIPTION: Specifies sphinxcontrib-devhelp version 2.0.0 with SHA-256 hashes for verification. Comments indicate it's required by sphinx.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_43\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinxcontrib-devhelp==2.0.0 \\\n    --hash=sha256:411f5d96d445d1d73bb5d52133377b4248ec79db5c793ce7dbe59e074b4dd1ad \\\n    --hash=sha256:aefb8b83854e4b0998877524d1029fd3e6879210422ee3780459e28a1f03a8a2\n```\n\n----------------------------------------\n\nTITLE: Java Netty NIO Event Processing with JavaScript Generated File Stack Trace\nDESCRIPTION: Stack trace showing the execution path through Netty event handling to a generated JavaScript file execution. This trace shows how the system processes HTTP requests through to the JavaScript module handling code.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_52\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Installing xgrammar Package with Hash Verification\nDESCRIPTION: This snippet specifies the installation requirements for xgrammar version 0.1.16 with multiple SHA256 hash verification options. The file appears to be truncated at the end.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_50\n\nLANGUAGE: text\nCODE:\n```\nxgrammar==0.1.16 \\\n    --hash=sha256:027c6937748d22b1c2db2850c99e3cdca6b9532817ad2013b6fb646f07cc8448 \\\n    --hash=sha256:04e361b22926f431ae82fad3c4463e0d3c8f653fe15ebe3d7059edf73e348565 \\\n    --hash=sha256:19807de8dbe3772f8b6aaa979541183fe6c6bafa3680d52d8289c7b4439eff2c \\\n    --hash=sha256:1d898e3dc04ea7d81a0e9cd10b632c22707fcc9ce02d7be3c0aa6c38067af97f \\\n    --hash=sha256:2301413a374f11add07843dfb49050f27beae89a4be7e0ffd454c08cf302412c \\\n    --hash=sha256:23d016b09b22ad77a0cc7de49e2a7152d8cd221734aa6d41b5fd7827dfb1a4d3 \\\n    --hash=sha256:2d75e6501f55368462b4d61ce0fb6a65c587782faa7319f48f49a8c444b4245f \\\n    --hash=sha256:46e52514479056418495d68413c2ea18798b95dcdc36d25f48b281ca7d203ce1 \\\n    --hash=sha256:4ddd5128a82d0a9c800c03df25c610368ca630704ad20a6bb7a3629f24ced442 \\\n    --hash=sha256:51565f8e6eb17fefe7ce90aa4598cf8216b4ee801a33d58d8439242d3d18cfa6 \\\n\n```\n\n----------------------------------------\n\nTITLE: Configure Serve Applications with Custom Images\nDESCRIPTION: This YAML configuration outlines how to specify custom container images in the runtime environment for Ray Serve applications, assigning each application to its designated image.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/multi-app-container.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napplications:\n  - name: whisper\n    import_path: whisper_example:entrypoint\n    route_prefix: /whisper\n    runtime_env:\n      image_uri: {IMG1}\n  - name: resnet\n    import_path: resnet50_example:app\n    route_prefix: /resnet\n    runtime_env:\n      image_uri: {IMG2}\n```\n\n----------------------------------------\n\nTITLE: Specifying Pillow Dependency with Hash Verification\nDESCRIPTION: Defines the pillow package dependency with version 10.3.0 and includes multiple SHA256 hash validations for security. This is a Python Imaging Library that provides image processing capabilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_20\n\nLANGUAGE: plaintext\nCODE:\n```\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    --hash=sha256:0ea2a783a2bdf2a561808fe4a7a12e9aa3799b701ba305de596bc48b8bdfce9d \\\n    --hash=sha256:1530e8f3a4b965eb6a7785cf17a426c779333eb62c9a7d1bbcf3ffd5bf77a4aa \\\n    --hash=sha256:16563993329b79513f59142a6b02055e10514c1a8e86dca8b48a893e33cf91e3 \\\n    --hash=sha256:19aeb96d43902f0a783946a0a87dbdad5c84c936025b8419da0a0cd7724356b1 \\\n    --hash=sha256:1a1d1915db1a4fdb2754b9de292642a39a7fb28f1736699527bb649484fb966a \\\n    --hash=sha256:1b87bd9d81d179bd8ab871603bd80d8645729939f90b71e62914e816a76fc6bd \\\n    --hash=sha256:1dfc94946bc60ea375cc39cff0b8da6c7e5f8fcdc1d946beb8da5c216156ddd8 \\\n    --hash=sha256:2034f6759a722da3a3dbd91a81148cf884e91d1b747992ca288ab88c1de15999 \\\n    --hash=sha256:261ddb7ca91fcf71757979534fb4c128448b5b4c55cb6152d280312062f69599 \\\n    --hash=sha256:2ed854e716a89b1afcedea551cd85f2eb2a807613752ab997b9974aaa0d56936 \\\n    --hash=sha256:3102045a10945173d38336f6e71a8dc71bcaeed55c3123ad4af82c52807b9375\n```\n\n----------------------------------------\n\nTITLE: Specifying Scientific Computing Dependencies with Hash Verification in pip\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and hash verification for secure installation. Each package includes its version, SHA-256 hashes for verification, and comments indicating which requirements files or packages depend on it.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_32\n\nLANGUAGE: pip\nCODE:\n```\n--hash=sha256:f82a116a1d03628a8ace4859556fb39fd1424c933341a08ea3ed6de1edb0283b \\\n    --hash=sha256:fb0ba113b4983beac1a2eb16faffd76cb41e176bf58c4afe3e14b9c681f702de \\\n    --hash=sha256:fb4f868f712b2dd4bcc538b0a0c1f63a2b1d584c925e69a224d759e7070a12d5 \\\n    --hash=sha256:fb6116dfb8d1925cbdb52595560584db42a7f664617a1f7d7f6e32f138cdf37d \\\n    --hash=sha256:fda7cb070f442bf80b642cd56483b5548e43d366fe3f39b98e67cce780cded00 \\\n    --hash=sha256:feea821ee2a9273771bae61194004ee2fc33f8ec7db08117ef9147d4bbcbca8e\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   jsonschema\n    #   referencing\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Ray Project\nDESCRIPTION: This snippet defines the required versions and hash values for ypy-websocket and zipp packages. It also includes comments indicating the packages that depend on these and mentions excluded packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_52\n\nLANGUAGE: Text\nCODE:\n```\nypy-websocket==0.8.4 \\\n    --hash=sha256:43a001473f5c8abcf182f603049cf305cbc855ad8deaa9dfa0f3b5a7cea9d0ff \\\n    --hash=sha256:b1ba0dfcc9762f0ca168d2378062d3ca1299d39076b0f145d961359121042be5\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   jupyter-server-ydoc\nzipp==3.21.0 \\\n    --hash=sha256:2c9958f6430a2040341a52eb608ed6dd93ef4392e02ffe219417c1b28b5dd1f4 \\\n    --hash=sha256:ac1bbe05fd2991f160ebce24ffbac5f6d11d83dc90891255885223d42b3cd931\n    # via importlib-metadata\n\n# The following packages were excluded from the output:\n# ray\n# grpcio-tools\n# setuptools\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Ray\nDESCRIPTION: Terminates the Ray framework to clean up resources after the optimization is complete.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/ax_example.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Setting Training Batch Size and Limiting Dataset\nDESCRIPTION: Configures the batch size per worker and limits the dataset size for training. This is used to set up the training parameters for the Ray TorchTrainer, with each worker processing 10 batches.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnum_workers = 16\nbatch_size_per_worker = 10\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Volume Claim for Redis Storage in Kubernetes\nDESCRIPTION: Kubernetes configuration for provisioning persistent storage for Redis. Creates an 8GB ReadWriteOnce volume using the standard-rwo storage class.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-persistent-ft.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: redis-data\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 8Gi\n  storageClassName: standard-rwo\n```\n\n----------------------------------------\n\nTITLE: Running Vale on a specific documentation file\nDESCRIPTION: Command to run Vale on a specific RST file in the Ray project. This will check the file for style and grammar issues according to the configured rules.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nvale doc/source/data/overview.rst\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies Declaration\nDESCRIPTION: Requirements file listing package dependencies with version constraints and dependency relationships. Includes packages for testing, ML, cloud integration and other functionalities.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled.txt#2025-04-12_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n# via -r python/requirements.txt\nopencensus-context==0.1.3\n    # via opencensus\nopencensus-proto==0.1.0\n    # via opentelemetry-exporter-opencensus\nopencv-python-headless==4.8.1.78\n    # via -r python/requirements/ml/rllib-test-requirements.txt\nopenpyxl==3.0.10\n    # via -r python/requirements/test-requirements.txt\nopentelemetry-api==1.1.0\n```\n\n----------------------------------------\n\nTITLE: Handling W&B Missing Credentials Error\nDESCRIPTION: This snippet outlines a common error when missing W&B credentials, explaining how to resolve it by logging in correctly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/experiment-tracking.rst#2025-04-12_snippet_9\n\nLANGUAGE: none\nCODE:\n```\nwandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key]).\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Serve Service Setup\nDESCRIPTION: Checks the status of the RayService and Service endpoints to ensure the proper setup of the RayService with high availability. The service should have three endpoints, including the Ray head and two Ray workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-high-availability.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# Step 4.1: Wait until the RayService is ready to serve requests.\nkubectl describe rayservices.ray.io rayservice-ha\n\n# [Example output]\n#   Conditions:\n#     Last Transition Time:  2025-02-13T21:36:18Z\n#     Message:               Number of serve endpoints is greater than 0\n#     Observed Generation:   1\n#     Reason:                NonZeroServeEndpoints\n#     Status:                True\n#     Type:                  Ready \n\n# Step 4.2: `rayservice-ha-serve-svc` should have 3 endpoints, including the Ray head and two Ray workers.\nkubectl describe svc rayservice-ha-serve-svc\n\n# [Example output]\n# Endpoints:         10.244.0.29:8000,10.244.0.30:8000,10.244.0.32:8000\n```\n\n----------------------------------------\n\nTITLE: Specifying Parso Package with Hash Verification\nDESCRIPTION: Defines the parso package version 0.8.3 with SHA256 hash verification and notes that it's required by jedi.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_25\n\nLANGUAGE: text\nCODE:\n```\nparso==0.8.3 \\\n    --hash=sha256:8c07be290bb59f03588915921e29e8a50002acaf2cdc5fa0e0114f91709fafa0 \\\n    --hash=sha256:c001d4636cd3aecdaf33cbb40aebb59b094be2a74c556778ef5576c175e19e75\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jedi\n```\n\n----------------------------------------\n\nTITLE: Customizing Ray Tune Storage Path and Experiment Name\nDESCRIPTION: Demonstrates how to specify a custom storage path and experiment name for Ray Tune results. This allows for better organization of experiment outputs in specified locations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# This logs to 2 different trial folders:\n# ./results/test_experiment/trial_name_1 and ./results/test_experiment/trial_name_2\n# Only trial_name is autogenerated.\ntuner = tune.Tuner(trainable,\n    tune_config=tune.TuneConfig(num_samples=2),\n    run_config=RunConfig(storage_path=\"./results\", name=\"test_experiment\"))\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry-Semantic-Conventions Dependency with Hash Verification\nDESCRIPTION: Defines the opentelemetry-semantic-conventions package dependency with version 0.20b0 and includes SHA256 hash validations. This is a dependency of opentelemetry-sdk.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-semantic-conventions==0.20b0 \\\n    --hash=sha256:ecae7367203e5204c70518e6d24b438480d6a6f1e5c8ee9dc2145f176ff4452e \\\n    --hash=sha256:fac014ac2098b1a05fe58af77cbe74c825ff869d6d53d316c393cc77f507ec15\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Verification\nDESCRIPTION: This code block lists Python package dependencies with specific versions and SHA256 hash values for verification. The format follows pip's requirements file pattern with hash verification to ensure package integrity and security.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:f7e301075edaf50500f0b341543c41194d8df3ae5caf4702f2095f3ca73dd8da \\\n    --hash=sha256:fb616be3538599e797a2017cccca78e354c767165e8858ab5116813146041a24 \\\n    --hash=sha256:fce28b3c8a81b6b36dfac9feb1de115bab619b3c13905b419ec71d03a3fc1423 \\\n    --hash=sha256:fe5d7785250541f7f5019ab9cba2c71169dc7d74d0f45253f8313f436458a4ef\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   aiohttp\n    #   yarl\nnest-asyncio==1.5.8 \\\n    --hash=sha256:25aa2ca0d2a5b5531956b9e273b45cf664cae2b145101d73b86b199978d48fdb \\\n    --hash=sha256:accda7a339a70599cb08f9dd09a67e0c2ef8d8d6f4c07f96ab203f2ae254e48d\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   outlines\nnetworkx==3.2.1 \\\n    --hash=sha256:9f1bb5cf3409bf324e0a722c20bdb4c20ee39bf1c30ce8ae499c8502b0b5e0c6 \\\n    --hash=sha256:f18c69adc97877c42332c170849c96cefa91881c99a7cb3e95b7c659ebdc1ec2\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   scikit-image\n    #   torch\nninja==1.11.1.3 \\\n    --hash=sha256:04d48d14ea7ba11951c156599ab526bdda575450797ff57c6fdf99b2554d09c7 \\\n    --hash=sha256:114ed5c61c8474df6a69ab89097a20749b769e2c219a452cb2fadc49b0d581b0 \\\n    --hash=sha256:17978ad611d8ead578d83637f5ae80c2261b033db0b493a7ce94f88623f29e1b \\\n    --hash=sha256:1ad2112c2b0159ed7c4ae3731595191b1546ba62316fc40808edecd0306fefa3 \\\n    --hash=sha256:2883ea46b3c5079074f56820f9989c6261fcc6fd873d914ee49010ecf283c3b2 \\\n    --hash=sha256:28aea3c1c280cba95b8608d50797169f3a34280e3e9a6379b6e340f0c9eaeeb0 \\\n    --hash=sha256:2b4879ea3f1169f3d855182c57dcc84d1b5048628c8b7be0d702b81882a37237 \\\n    --hash=sha256:53409151da081f3c198bb0bfc220a7f4e821e022c5b7d29719adda892ddb31bb \\\n    --hash=sha256:56ada5d33b8741d298836644042faddebc83ee669782d661e21563034beb5aba \\\n    --hash=sha256:7fa2247fce98f683bc712562d82b22b8a0a5c000738a13147ca2d1b68c122298 \\\n    --hash=sha256:8c4bdb9fd2d0c06501ae15abfd23407660e95659e384acd36e013b6dd7d8a8e4 \\\n    --hash=sha256:a27e78ca71316c8654965ee94b286a98c83877bfebe2607db96897bbfe458af0 \\\n    --hash=sha256:a38c6c6c8032bed68b70c3b065d944c35e9f903342875d3a3218c1607987077c \\\n    --hash=sha256:a4a3b71490557e18c010cbb26bd1ea9a0c32ee67e8f105e9731515b6e0af792e \\\n    --hash=sha256:b6966f83064a88a51693073eea3decd47e08c3965241e09578ef7aa3a7738329 \\\n    --hash=sha256:bc3ebc8b2e47716149f3541742b5cd8e0b08f51013b825c05baca3e34854370d \\\n    --hash=sha256:edfa0d2e9d7ead1635b03e40a32ad56cc8f56798b6e2e9848d8300b174897076\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements/llm/llm-requirements.txt\n    #   vllm\n    #   xgrammar\nnumba==0.60.0 \\\n    --hash=sha256:01ef4cd7d83abe087d644eaa3d95831b777aa21d441a23703d649e06b8e06b74 \\\n    --hash=sha256:0b983bd6ad82fe868493012487f34eae8bf7dd94654951404114f23c3466d34b \\\n    --hash=sha256:0ebaa91538e996f708f1ab30ef4d3ddc344b64b5227b67a57aa74f401bb68b9d \\\n    --hash=sha256:1527dc578b95c7c4ff248792ec33d097ba6bef9eda466c948b68dfc995c25781 \\\n    --hash=sha256:159e618ef213fba758837f9837fb402bbe65326e60ba0633dbe6c7f274d42c1b \\\n    --hash=sha256:19407ced081d7e2e4b8d8c36aa57b7452e0283871c296e12d798852bc7d7f198 \\\n    --hash=sha256:3031547a015710140e8c87226b4cfe927cac199835e5bf7d4fe5cb64e814e3ab \\\n    --hash=sha256:38d6ea4c1f56417076ecf8fc327c831ae793282e0ff51080c5094cb726507b1c \\\n    --hash=sha256:3fb02b344a2a80efa6f677aa5c40cd5dd452e1b35f8d1c2af0dfd9ada9978e4b \\\n    --hash=sha256:4142d7ac0210cc86432b818338a2bc368dc773a2f5cf1e32ff7c5b378bd63ee8 \\\n    --hash=sha256:5d761de835cd38fb400d2c26bb103a2726f548dc30368853121d66201672e651 \\\n    --hash=sha256:5df6158e5584eece5fc83294b949fd30b9f1125df7708862205217e068aabf16 \\\n    --hash=sha256:5f4fde652ea604ea3c86508a3fb31556a6157b2c76c8b51b1d45eb40c8598703 \\\n    --hash=sha256:62908d29fb6a3229c242e981ca27e32a6e606cc253fc9e8faeb0e48760de241e \\\n    --hash=sha256:819a3dfd4630d95fd574036f99e47212a1af41cbcb019bf8afac63ff56834449 \\\n    --hash=sha256:a17b70fc9e380ee29c42717e8cc0bfaa5556c416d94f9aa96ba13acb41bdece8 \\\n    --hash=sha256:c151748cd269ddeab66334bd754817ffc0cabd9433acb0f551697e5151917d25 \\\n    --hash=sha256:cac02c041e9b5bc8cf8f2034ff6f0dbafccd1ae9590dc146b3a02a45e53af4e2 \\\n    --hash=sha256:d7da4098db31182fc5ffe4bc42c6f24cd7d1cb8a14b59fd755bfee32e34b8404 \\\n    --hash=sha256:f75262e8fe7fa96db1dca93d53a194a38c46da28b112b8a4aca168f0df860347 \\\n    --hash=sha256:fe0b28abb8d70f8160798f4de9d486143200f34458d34c4a214114e445d7124e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   vllm\nnumpy==1.26.4 \\\n    --hash=sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b \\\n    --hash=sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818 \\\n    --hash=sha256:1af303d6b2210eb850fcf03064d364652b7120803a0b872f5211f5234b399f20 \\\n    --hash=sha256:1dda2e7b4ec9dd512f84935c5f126c8bd8b9f2fc001e9f54af255e8c5f16b0e0 \\\n    --hash=sha256:2a02aba9ed12e4ac4eb3ea9421c420301a0c6460d9830d74a9df87efa4912010 \\\n    --hash=sha256:2e4ee3380d6de9c9ec04745830fd9e2eccb3e6cf790d39d7b98ffd19b0dd754a \\\n    --hash=sha256:3373d5d70a5fe74a2c1bb6d2cfd9609ecf686d47a2d7b1d37a8f3b6bf6003aea \\\n    --hash=sha256:47711010ad8555514b434df65f7d7b076bb8261df1ca9bb78f53d3b2db02e95c \\\n    --hash=sha256:4c66707fabe114439db9068ee468c26bbdf909cac0fb58686a42a24de1760c71 \\\n    --hash=sha256:50193e430acfc1346175fcbdaa28ffec49947a06918b7b92130744e81e640110 \\\n    --hash=sha256:52b8b60467cd7dd1e9ed082188b4e6bb35aa5cdd01777621a1658910745b90be \\\n    --hash=sha256:60dedbb91afcbfdc9bc0b1f3f402804070deed7392c23eb7a7f07fa857868e8a \\\n    --hash=sha256:62b8e4b1e28009ef2846b4c7852046736bab361f7aeadeb6a5b89ebec3c7055a \\\n    --hash=sha256:666dbfb6ec68962c033a450943ded891bed2d54e6755e35e5835d63f4f6931d5 \\\n    --hash=sha256:675d61ffbfa78604709862923189bad94014bef562cc35cf61d3a07bba02a7ed \\\n    --hash=sha256:679b0076f67ecc0138fd2ede3a8fd196dddc2ad3254069bcb9faf9a79b1cebcd \\\n    --hash=sha256:7349ab0fa0c429c82442a27a9673fc802ffdb7c7775fad780226cb234965e53c \\\n    --hash=sha256:7ab55401287bfec946ced39700c053796e7cc0e3acbef09993a9ad2adba6ca6e \\\n    --hash=sha256:7e50d0a0cc3189f9cb0aeb3a6a6af18c16f59f004b866cd2be1c14b36134a4a0 \\\n    --hash=sha256:95a7476c59002f2f6c590b9b7b998306fba6a5aa646b1e22ddfeaf8f78c3a29c \\\n    --hash=sha256:96ff0b2ad353d8f990b63294c8986f1ec3cb19d749234014f4e7eb0112ceba5a \\\n    --hash=sha256:9fad7dcb1aac3c7f0584a5a8133e3a43eeb2fe127f47e3632d43d677c66c102b \\\n    --hash=sha256:9ff0f4f29c51e2803569d7a51c2304de5554655a60c5d776e35b4a41413830d0 \\\n    --hash=sha256:a354325ee03388678242a4d7ebcd08b5c727033fcff3b2f536aea978e15ee9e6 \\\n    --hash=sha256:a4abb4f9001ad2858e7ac189089c42178fcce737e4169dc61321660f1a96c7d2 \\\n    --hash=sha256:ab47dbe5cc8210f55aa58e4805fe224dac469cde56b9f731a4c098b91917159a \\\n    --hash=sha256:afedb719a9dcfc7eaf2287b839d8198e06dcd4cb5d276a3df279231138e83d30 \\\n    --hash=sha256:b3ce300f3644fb06443ee2222c2201dd3a89ea6040541412b8fa189341847218 \\\n    --hash=sha256:b97fe8060236edf3662adfc2c633f56a08ae30560c56310562cb4f95500022d5 \\\n    --hash=sha256:bfe25acf8b437eb2a8b2d49d443800a5f18508cd811fea3181723922a8a82b07 \\\n    --hash=sha256:cd25bcecc4974d09257ffcd1f098ee778f7834c3ad767fe5db785be9a4aa9cb2 \\\n    --hash=sha256:d209d8969599b27ad20994c8e41936ee0964e6da07478d6c35016bc386b66ad4 \\\n    --hash=sha256:d5241e0a80d808d70546c697135da2c613f30e28251ff8307eb72ba696945764 \\\n    --hash=sha256:edd8b5fe47dab091176d21bb6de568acdd906d1887a4584a15a9a96a1dca06ef \\\n    --hash=sha256:f870204a840a60da0b12273ef34f7051e98c3b5961b61b0c2c1be6dfd64fbcd3 \\\n    --hash=sha256:ffa75af20b44f8dba823498024771d5ac50620e6915abac414251bd971b4529f\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   cupy-cuda12x\n    #   gguf\n    #   gymnasium\n    #   imageio\n    #   mistral-common\n    #   numba\n    #   opencv-python-headless\n    #   outlines\n    #   pandas\n    #   pyarrow\n    #   scikit-image\n    #   scipy\n    #   tensorboardx\n    #   tifffile\n    #   torchvision\n    #   transformers\n    #   vllm\n    #   xformers\nnvidia-cublas-cu12==12.4.5.8 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:0f8aa1706812e00b9f19dfe0cdb3999b092ccb8ca168c0db5b8ea712456fd9b3 \\\n    --hash=sha256:2fc8da60df463fdefa81e323eef2e36489e1c94335b5358bcb38360adf75ac9b \\\n    --hash=sha256:5a796786da89203a0657eda402bcdcec6180254a8ac22d72213abc42069522dc\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   nvidia-cudnn-cu12\n    #   nvidia-cusolver-cu12\n    #   torch\nnvidia-cuda-cupti-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:5688d203301ab051449a2b1cb6690fbe90d2b372f411521c86018b950f3d7922 \\\n    --hash=sha256:79279b35cf6f91da114182a5ce1864997fd52294a87a16179ce275773799458a \\\n    --hash=sha256:9dec60f5ac126f7bb551c055072b69d85392b13311fcc1bcda2202d172df30fb\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   torch\nnvidia-cuda-nvrtc-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:0eedf14185e04b76aa05b1fea04133e59f465b6f960c0cbf4e37c3cb6b0ea198 \\\n    --hash=sha256:a178759ebb095827bd30ef56598ec182b85547f1508941a3d560eb7ea1fbf338 \\\n    --hash=sha256:a961b2f1d5f17b14867c619ceb99ef6fcec12e46612711bcec78eb05068a60ec\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   torch\nnvidia-cuda-runtime-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:09c2e35f48359752dfa822c09918211844a3d93c100a715d79b59591130c5e1e \\\n    --hash=sha256:64403288fa2136ee8e467cdc9c9427e0434110899d07c779f25b5c068934faa5 \\\n    --hash=sha256:961fe0e2e716a2a1d967aab7caee97512f71767f852f67432d572e36cb3a11f3\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   torch\nnvidia-cudnn-cu12==9.1.0.70 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n```\n\n----------------------------------------\n\nTITLE: Storing Ray Performance Metrics in JSON\nDESCRIPTION: This JSON object stores performance metrics and metadata for a Ray project execution. It includes task execution rate, total tasks, runtime, success status, session URL, and commit URL. The data is useful for analyzing and comparing Ray job performance across different runs or configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/benchmarks/many_tasks.txt#2025-04-12_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tasks_per_second\": 27.508657888123608,\n  \"num_tasks\": 10000,\n  \"time\": 663.5219151973724,\n  \"success\": \"1\",\n  \"_runtime\": 674.2678966522217,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_XCJkRqS4HkuHLXehx7i6Fwvc\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding and Log Retrieval with kubectl-ray\nDESCRIPTION: Commands to forward ports for Ray resources and retrieve logs from RayClusters using the kubectl-ray plugin.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kubectl-plugin.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl ray session raycluster-sample\nkubectl ray log raycluster-sample\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Dependencies with Hashes\nDESCRIPTION: This section lists the required packages with their specific versions and corresponding hashes. It includes comments indicating the source of each requirement and any constraints.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_3\n\nLANGUAGE: pip\nCODE:\n```\naiofiles==22.1.0 \\\n    --hash=sha256:1142fa8e80dbae46bb6339573ad4c8c0841358f79c6eb50a493dceca14621bad \\\n    --hash=sha256:9107f1ca0b2a5553987a94a3c9959fe5b491fdf731389aa5b7b1bd0733e32de6\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   ypy-websocket\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   aiohttp\naiohttp==3.11.16 \\\n    --hash=sha256:004511d3413737700835e949433536a2fe95a7d0297edd911a1e9705c5b5ea43 \\\n    --hash=sha256:0902e887b0e1d50424112f200eb9ae3dfed6c0d0a19fc60f633ae5a57c809656 \\\n    --hash=sha256:09b00dd520d88eac9d1768439a59ab3d145065c91a8fab97f900d1b5f802895e \\\n    --hash=sha256:0a2f451849e6b39e5c226803dcacfa9c7133e9825dcefd2f4e837a2ec5a3bb98 \\\n    --hash=sha256:0a950c2eb8ff17361abd8c85987fd6076d9f47d040ebffce67dce4993285e973 \\\n    --hash=sha256:0ad1fb47da60ae1ddfb316f0ff16d1f3b8e844d1a1e154641928ea0583d486ed \\\n    --hash=sha256:13ceac2c5cdcc3f64b9015710221ddf81c900c5febc505dbd8f810e770011540 \\\n    --hash=sha256:14461157d8426bcb40bd94deb0450a6fa16f05129f7da546090cebf8f3123b0f \\\n    --hash=sha256:16f8a2c9538c14a557b4d309ed4d0a7c60f0253e8ed7b6c9a2859a7582f8b1b8 \\\n    --hash=sha256:17ae4664031aadfbcb34fd40ffd90976671fa0c0286e6c4113989f78bebab37a \\\n    --hash=sha256:1ce63ae04719513dd2651202352a2beb9f67f55cb8490c40f056cea3c5c355ce \\\n    --hash=sha256:23a15727fbfccab973343b6d1b7181bfb0b4aa7ae280f36fd2f90f5476805682 \\\n    --hash=sha256:2540ddc83cc724b13d1838026f6a5ad178510953302a49e6d647f6e1de82bc34 \\\n    --hash=sha256:37dcee4906454ae377be5937ab2a66a9a88377b11dd7c072df7a7c142b63c37c \\\n    --hash=sha256:38bea84ee4fe24ebcc8edeb7b54bf20f06fd53ce4d2cc8b74344c5b9620597fd \\\n    --hash=sha256:3ab3367bb7f61ad18793fea2ef71f2d181c528c87948638366bf1de26e239183 \\\n    --hash=sha256:3ad1d59fd7114e6a08c4814983bb498f391c699f3c78712770077518cae63ff7 \\\n    --hash=sha256:3b4e6db8dc4879015b9955778cfb9881897339c8fab7b3676f8433f849425913 \\\n    --hash=sha256:3e061b09f6fa42997cf627307f220315e313ece74907d35776ec4373ed718b86 \\\n    --hash=sha256:42864e70a248f5f6a49fdaf417d9bc62d6e4d8ee9695b24c5916cb4bb666c802 \\\n    --hash=sha256:493910ceb2764f792db4dc6e8e4b375dae1b08f72e18e8f10f18b34ca17d0979 \\\n    --hash=sha256:4d0c970c0d602b1017e2067ff3b7dac41c98fef4f7472ec2ea26fd8a4e8c2149 \\\n    --hash=sha256:54eb3aead72a5c19fad07219acd882c1643a1027fbcdefac9b502c267242f955 \\\n    --hash=sha256:56a3443aca82abda0e07be2e1ecb76a050714faf2be84256dae291182ba59049 \\\n    --hash=sha256:576f5ca28d1b3276026f7df3ec841ae460e0fc3aac2a47cbf72eabcfc0f102e1 \\\n    --hash=sha256:58ede86453a6cf2d6ce40ef0ca15481677a66950e73b0a788917916f7e35a0bb \\\n    --hash=sha256:61c721764e41af907c9d16b6daa05a458f066015abd35923051be8705108ed17 \\\n    --hash=sha256:634d96869be6c4dc232fc503e03e40c42d32cfaa51712aee181e922e61d74814 \\\n    --hash=sha256:696ef00e8a1f0cec5e30640e64eca75d8e777933d1438f4facc9c0cdf288a810 \\\n    --hash=sha256:69a2cbd61788d26f8f1e626e188044834f37f6ae3f937bd9f08b65fc9d7e514e \\\n    --hash=sha256:6a792ce34b999fbe04a7a71a90c74f10c57ae4c51f65461a411faa70e154154e \\\n    --hash=sha256:6ac13b71761e49d5f9e4d05d33683bbafef753e876e8e5a7ef26e937dd766713 \\\n    --hash=sha256:6fdec0213244c39973674ca2a7f5435bf74369e7d4e104d6c7473c81c9bcc8c4 \\\n    --hash=sha256:72b1b03fb4655c1960403c131740755ec19c5898c82abd3961c364c2afd59fe7 \\\n    --hash=sha256:745f1ed5e2c687baefc3c5e7b4304e91bf3e2f32834d07baaee243e349624b24 \\\n    --hash=sha256:776c8e959a01e5e8321f1dec77964cb6101020a69d5a94cd3d34db6d555e01f7 \\\n    --hash=sha256:780df0d837276276226a1ff803f8d0fa5f8996c479aeef52eb040179f3156cbd \\\n    --hash=sha256:78e6e23b954644737e385befa0deb20233e2dfddf95dd11e9db752bdd2a294d3 \\\n    --hash=sha256:7951decace76a9271a1ef181b04aa77d3cc309a02a51d73826039003210bdc86 \\\n    --hash=sha256:7ba92a2d9ace559a0a14b03d87f47e021e4fa7681dc6970ebbc7b447c7d4b7cd \\\n    --hash=sha256:7f6428fee52d2bcf96a8aa7b62095b190ee341ab0e6b1bcf50c615d7966fd45b \\\n    --hash=sha256:87944bd16b7fe6160607f6a17808abd25f17f61ae1e26c47a491b970fb66d8cb \\\n    --hash=sha256:87a6e922b2b2401e0b0cf6b976b97f11ec7f136bfed445e16384fbf6fd5e8602 \\\n    --hash=sha256:8cb0688a8d81c63d716e867d59a9ccc389e97ac7037ebef904c2b89334407180 \\\n    --hash=sha256:8df6612df74409080575dca38a5237282865408016e65636a76a2eb9348c2567 \\\n    --hash=sha256:911a6e91d08bb2c72938bc17f0a2d97864c531536b7832abee6429d5296e5b27 \\\n    --hash=sha256:92b7ee222e2b903e0a4b329a9943d432b3767f2d5029dbe4ca59fb75223bbe2e \\\n    --hash=sha256:938f756c2b9374bbcc262a37eea521d8a0e6458162f2a9c26329cc87fdf06534 \\\n    --hash=sha256:9756d9b9d4547e091f99d554fbba0d2a920aab98caa82a8fb3d3d9bee3c9ae85 \\\n    --hash=sha256:98b88a2bf26965f2015a771381624dd4b0839034b70d406dc74fd8be4cc053e3 \\\n    --hash=sha256:9b751a6306f330801665ae69270a8a3993654a85569b3469662efaad6cf5cc50 \\\n    --hash=sha256:a2a450bcce4931b295fc0848f384834c3f9b00edfc2150baafb4488c27953de6 \\\n    --hash=sha256:a3814760a1a700f3cfd2f977249f1032301d0a12c92aba74605cfa6ce9f78489 \\\n    --hash=sha256:a5abcbba9f4b463a45c8ca8b7720891200658f6f46894f79517e6cd11f3405ca \\\n    --hash=sha256:a6db7458ab89c7d80bc1f4e930cc9df6edee2200127cfa6f6e080cf619eddfbd \\\n    --hash=sha256:ad497f38a0d6c329cb621774788583ee12321863cd4bd9feee1effd60f2ad133 \\\n    --hash=sha256:ad9509ffb2396483ceacb1eee9134724443ee45b92141105a4645857244aecc8 \\\n    --hash=sha256:bbcba75fe879ad6fd2e0d6a8d937f34a571f116a0e4db37df8079e738ea95c71 \\\n    --hash=sha256:c10d85e81d0b9ef87970ecbdbfaeec14a361a7fa947118817fcea8e45335fa46 \\\n    --hash=sha256:c15b2271c44da77ee9d822552201180779e5e942f3a71fb74e026bf6172ff287 \\\n    --hash=sha256:ca37057625693d097543bd88076ceebeb248291df9d6ca8481349efc0b05dcd0 \\\n    --hash=sha256:cc3a145479a76ad0ed646434d09216d33d08eef0d8c9a11f5ae5cdc37caa3540 \\\n    --hash=sha256:ccf10f16ab498d20e28bc2b5c1306e9c1512f2840f7b6a67000a517a4b37d5ee \\\n    --hash=sha256:cd464ba806e27ee24a91362ba3621bfc39dbbb8b79f2e1340201615197370f7c \\\n    --hash=sha256:d007aa39a52d62373bd23428ba4a2546eed0e7643d7bf2e41ddcefd54519842c \\\n    --hash=sha256:d0666afbe984f6933fe72cd1f1c3560d8c55880a0bdd728ad774006eb4241ecd \\\n    --hash=sha256:d07502cc14ecd64f52b2a74ebbc106893d9a9717120057ea9ea1fd6568a747e7 \\\n    --hash=sha256:d489d9778522fbd0f8d6a5c6e48e3514f11be81cb0a5954bdda06f7e1594b321 \\\n    --hash=sha256:df7db76400bf46ec6a0a73192b14c8295bdb9812053f4fe53f4e789f3ea66bbb \\\n    --hash=sha256:e3538bc9fe1b902bef51372462e3d7c96fce2b566642512138a480b7adc9d508 \\\n    --hash=sha256:e87fd812899aa78252866ae03a048e77bd11b80fb4878ce27c23cade239b42b2 \\\n    --hash=sha256:ecdb8173e6c7aa09eee342ac62e193e6904923bd232e76b4157ac0bfa670609f \\\n    --hash=sha256:f244b8e541f414664889e2c87cac11a07b918cb4b540c36f7ada7bfa76571ea2 \\\n    --hash=sha256:f4065145bf69de124accdd17ea5f4dc770da0a6a6e440c53f6e0a8c27b3e635c \\\n    --hash=sha256:f420bfe862fb357a6d76f2065447ef6f484bc489292ac91e29bc65d2d7a2c84d \\\n    --hash=sha256:f6ddd90d9fb4b501c97a4458f1c1720e42432c26cb76d28177c5b5ad4e332601 \\\n    --hash=sha256:fa73e8c2656a3653ae6c307b3f4e878a21f87859a9afab228280ddccd7369d71 \\\n    --hash=sha256:fadbb8f1d4140825069db3fedbbb843290fd5f5bc0a5dbd7eaf81d91bf1b003b \\\n    --hash=sha256:fb3d0cc5cdb926090748ea60172fa8a213cec728bd6c54eae18b96040fcd6227 \\\n    --hash=sha256:fb46bb0f24813e6cede6cc07b1961d4b04f331f7112a23b5e21f567da4ee50aa \\\n    --hash=sha256:fd36c119c5d6551bce374fcb5c19269638f8d09862445f85a5a48596fd59f4bb\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements/llm/llm-test-requirements.txt\n    #   -r python/requirements.txt\n    #   aiohttp-cors\n    #   pytest-aiohttp\n    #   vllm\naiohttp-cors==0.7.0 \\\n    --hash=sha256:0451ba59fdf6909d0e2cd21e4c0a43752bc0703d33fc78ae94d9d9321710193e \\\n    --hash=sha256:4d39c6d7100fd9764ed1caf8cebf0eb01bf5e3f24e2e073fda6234bc48b19f5d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\naiorwlock==1.3.0 \\\n    --hash=sha256:45baf8e4fa9a23e0bb325fbd67da80de1fd7ae1d4f59a6381754c60cec7b289b \\\n    --hash=sha256:83f12d87df4b9728a0b8fda1756585ab0d652b107bab59c6084e1b1ad692ab45\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\naiosignal==1.3.1 \\\n    --hash=sha256:54cd96e15e1649b75d6c87526a6ff0b6c1b0dd3459f43d9ca11d48c339b68cfc \\\n    --hash=sha256:f8376fb07dd1e86a584e4fcdec80b36b7f81aac666ebc724e2c090300dd83b17\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   aiohttp\n    #   ray\naiosqlite==0.19.0 \\\n    --hash=sha256:95ee77b91c8d2808bd08a59fbebf66270e9090c3d92ffbf260dc0db0b979577d \\\n    --hash=sha256:edba222e03453e094a3ce605db1b970c4b3376264e56f32e2a4959f948d66a96\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   ypy-websocket\nairportsdata==20241001 \\\n    --hash=sha256:67d71cf2c5378cc17ff66b62b1e11aa2444043949c894543ac8fd8dafce192fd \\\n```\n\n----------------------------------------\n\nTITLE: Configuring Node Type Specific Max Workers\nDESCRIPTION: Sets maximum workers for a specific node type. Must be less than or equal to cluster max_workers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/configuring-autoscaling.rst#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\navailable_node_types:\n  <node_type_name>:\n    max_workers: <value>\n```\n\n----------------------------------------\n\nTITLE: RST Grid Layout Configuration\nDESCRIPTION: Restructured Text configuration for creating a grid layout with four cards describing different documentation sections including Getting Started, Examples, User Guides, and API Reference.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/index.md#2025-04-12_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{eval-rst}\n.. grid:: 1 2 2 2\n    :gutter: 1\n    :class-container: container pb-3\n\n    .. grid-item-card::\n\n        **Getting Started**\n        ^^^\n\n        Learn how to start a Ray cluster and deploy Ray applications in the cloud.\n\n        +++\n        .. button-ref:: vm-cluster-quick-start\n            :color: primary\n            :outline:\n            :expand:\n\n            Get Started with Ray on Cloud VMs\n\n    .. grid-item-card::\n\n        **Examples**\n        ^^^\n\n        Try example Ray workloads in the Cloud\n\n        +++\n        .. button-ref:: vm-cluster-examples\n            :color: primary\n            :outline:\n            :expand:\n\n            Try example workloads\n\n    .. grid-item-card::\n\n        **User Guides**\n        ^^^\n\n        Learn best practices for configuring cloud clusters\n\n        +++\n        .. button-ref:: vm-cluster-guides\n            :color: primary\n            :outline:\n            :expand:\n\n            Read the User Guides\n\n    .. grid-item-card::\n\n        **API Reference**\n        ^^^\n\n        Find API references for cloud clusters\n\n        +++\n        .. button-ref:: vm-cluster-api-references\n            :color: primary\n            :outline:\n            :expand:\n\n            Check API references\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependency Specification with Hashes for y-py in Ray Project\nDESCRIPTION: This section specifies the y-py package at version 0.6.2 with comprehensive SHA-256 hashes for verification. The comment indicates this package is required by jupyter-ydoc and ypy-websocket packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_51\n\nLANGUAGE: pip\nCODE:\n```\ny-py==0.6.2 \\\n    --hash=sha256:015f7f6c1ce8a83d57955d1dc7ddd57cb633ae00576741a4fc9a0f72ed70007d \\\n    --hash=sha256:032365dfe932bfab8e80937ad6093b4c22e67d63ad880096b5fa8768f8d829ba \\\n    --hash=sha256:0649a41cd3c98e290c16592c082dbe42c7ffec747b596172eebcafb7fd8767b0 \\\n    --hash=sha256:0787e85645bb4986c27e271715bc5ce21bba428a17964e5ec527368ed64669bc \\\n    --hash=sha256:0cd6213c3cf2b9eee6f2c9867f198c39124c557f4b3b77d04a73f30fd1277a59 \\\n    --hash=sha256:0f2d881f0f8bf5674f8fe4774a438c545501e40fa27320c73be4f22463af4b05 \\\n    --hash=sha256:17bce637a89f6e75f0013be68becac3e38dc082e7aefaf38935e89215f0aa64a \\\n    --hash=sha256:17edd21eef863d230ea00004ebc6d582cc91d325e7132deb93f0a90eb368c855 \\\n    --hash=sha256:1d5b544e79ace93fdbd0b36ed329c86e346898153ac7ba2ec62bc9b4c6b745c9 \\\n    --hash=sha256:1f798165158b76365a463a4f8aa2e3c2a12eb89b1fc092e7020e93713f2ad4dc \\\n    --hash=sha256:266ec46ab9f9cb40fbb5e649f55c329fc4620fa0b1a8117bdeefe91595e182dc \\\n    --hash=sha256:26cb1307c3ca9e21a3e307ab2c2099677e071ae9c26ec10ddffb3faceddd76b3 \\\n    --hash=sha256:2a497ebe617bec6a420fc47378856caae40ab0652e756f3ed40c5f1fe2a12220 \\\n    --hash=sha256:2b4fac4ea2ce27b86d173ae45765ced7f159120687d4410bb6d0846cbdb170a3 \\\n    --hash=sha256:2cf817a72ffec4295def5c5be615dd8f1e954cdf449d72ebac579ff427951328 \\\n    --hash=sha256:2d2b054a1a5f4004967532a4b82c6d1a45421ef2a5b41d35b6a8d41c7142aabe \\\n    --hash=sha256:316e5e1c40259d482883d1926fd33fa558dc87b2bd2ca53ce237a6fe8a34e473 \\\n    --hash=sha256:35fcb9def6ce137540fdc0e91b08729677548b9c393c0151a6359fd199da3bd7 \\\n    --hash=sha256:376c5cc0c177f03267340f36aec23e5eaf19520d41428d87605ca2ca3235d845 \\\n    --hash=sha256:3ba99d0bdbd9cabd65f914cd07b4fb2e939ce199b54ae5ace1639ce1edf8e0a2 \\\n    --hash=sha256:3c011303eb2b360695d2bd4bd7ca85f42373ae89fcea48e7fa5b8dc6fc254a98 \\\n    --hash=sha256:4757a82a50406a0b3a333aa0122019a331bd6f16e49fed67dca423f928b3fd4d \\\n    --hash=sha256:47fcc19158150dc4a6ae9a970c5bc12f40b0298a2b7d0c573a510a7b6bead3f3 \\\n    --hash=sha256:4c28d977f516d4928f6bc0cd44561f6d0fdd661d76bac7cdc4b73e3c209441d9 \\\n    --hash=sha256:5415083f7f10eac25e1c434c87f07cb9bfa58909a6cad6649166fdad21119fc5 \\\n    --hash=sha256:613f83713714972886e81d71685403098a83ffdacf616f12344b52bc73705107 \\\n    --hash=sha256:69cfbcbe0a05f43e780e6a198080ba28034bf2bb4804d7d28f71a0379bfd1b19 \\\n    --hash=sha256:6c2f2831c5733b404d2f2da4bfd02bb4612ae18d0822e14ae79b0b92436b816d \\\n    --hash=sha256:7227f232f2daf130ba786f6834548f2cfcfa45b7ec4f0d449e72560ac298186c \\\n    --hash=sha256:72875641a907523d37f4619eb4b303611d17e0a76f2ffc423b62dd1ca67eef41 \\\n    --hash=sha256:7c7302619fc962e53093ba4a94559281491c045c925e5c4defec5dac358e0568 \\\n    --hash=sha256:7cbefd4f1060f05768227ddf83be126397b1d430b026c64e0eb25d3cf50c5734 \\\n    --hash=sha256:80a827e173372682959a57e6b8cc4f6468b1a4495b4bc7a775ef6ca05ae3e8e8 \\\n    --hash=sha256:82f2e5b31678065e7a7fa089ed974af5a4f076673cf4f414219bdadfc3246a21 \\\n    --hash=sha256:82f5ca62bedbf35aaf5a75d1f53b4457a1d9b6ff033497ca346e2a0cedf13d14 \\\n    --hash=sha256:8448da4092265142662bbd3fc46cb8b0796b1e259189c020bc8f738899abd0b5 \\\n    --hash=sha256:863e175ce5585f9ff3eba2aa16626928387e2a576157f02c8eb247a218ecdeae \\\n    --hash=sha256:86422c6090f34906c062fd3e4fdfdccf3934f2922021e979573ae315050b4288 \\\n    --hash=sha256:898fede446ca1926b8406bdd711617c2aebba8227ee8ec1f0c2f8568047116f7 \\\n    --hash=sha256:8f5c14d25611b263b876e9ada1701415a13c3e9f02ea397224fbe4ca9703992b \\\n    --hash=sha256:8f6071328aad06fdcc0a4acc2dc4839396d645f5916de07584af807eb7c08407 \\\n    --hash=sha256:932abb560fe739416b50716a72ba6c6c20b219edded4389d1fc93266f3505d4b \\\n    --hash=sha256:9b7cafbe946b4cafc1e5709957e6dd5c6259d241d48ed75713ded42a5e8a4663 \\\n    --hash=sha256:9b8822a5c0fd9a8cffcabfcc0cd7326bad537ee614fc3654e413a03137b6da1a \\\n    --hash=sha256:a21148b8ea09a631b752d975f9410ee2a31c0e16796fdc113422a6d244be10e5 \\\n    --hash=sha256:a3932f53418b408fa03bd002e6dc573a74075c2c092926dde80657c39aa2e054 \\\n    --hash=sha256:a70aee572da3994238c974694767365f237fc5949a550bee78a650fe16f83184 \\\n    --hash=sha256:ae80d505aee7b3172cdcc2620ca6e2f85586337371138bb2b71aa377d2c31e9a \\\n    --hash=sha256:b2686d7d8ca31531458a48e08b0344a8eec6c402405446ce7d838e2a7e43355a \\\n    --hash=sha256:bae1b1ad8d2b8cf938a60313f8f7461de609621c5dcae491b6e54975f76f83c5 \\\n    --hash=sha256:bd302c6d46a3be57664571a5f0d4224646804be9890a01d73a0b294f2d3bbff1 \\\n    --hash=sha256:beea5ad9bd9e56aa77a6583b6f4e347d66f1fe7b1a2cb196fff53b7634f9dc84 \\\n    --hash=sha256:bf6020560584671e76375b7a0539e0d5388fc70fa183c99dc769895f7ef90233 \\\n    --hash=sha256:c011997f62d0c3b40a617e61b7faaaf6078e4eeff2e95ce4c45838db537816eb \\\n    --hash=sha256:c08311db17647a47d4898fc6f8d9c1f0e58b927752c894877ff0c38b3db0d6e1 \\\n    --hash=sha256:c26bada6cd109095139237a46f50fc4308f861f0d304bc9e70acbc6c4503d158 \\\n    --hash=sha256:c31240e30d5636ded02a54b7280aa129344fe8e964fd63885e85d9a8a83db206 \\\n    --hash=sha256:ce0ae49879d10610cf3c40f4f376bb3cc425b18d939966ac63a2a9c73eb6f32a \\\n    --hash=sha256:ce15a842c2a0bf46180ae136743b561fa276300dd7fa61fe76daf00ec7dc0c2d \\\n    --hash=sha256:ce7c20b9395696d3b5425dccf2706d374e61ccf8f3656bff9423093a6df488f5 \\\n    --hash=sha256:cfc8381df1f0f873da8969729974f90111cfb61a725ef0a2e0e6215408fe1217 \\\n    --hash=sha256:d1dca48687f41efd862355e58b0aa31150586219324901dbea2989a506e291d4 \\\n    --hash=sha256:d3bbe2f925cc587545c8d01587b4523177408edd252a32ce6d61b97113fe234d \\\n    --hash=sha256:d917f5bc27b85611ceee4eb85f0e4088b0a03b4eed22c472409933a94ee953cf \\\n    --hash=sha256:dab84c52f64e10adc79011a08673eb80286c159b14e8fb455524bf2994f0cb38 \\\n    --hash=sha256:de9cfafe97c75cd3ea052a24cd4aabf9fb0cfc3c0f9f810f00121cdf123db9e4 \\\n    --hash=sha256:df35ea436592eb7e30e59c5403ec08ec3a5e7759e270cf226df73c47b3e739f5 \\\n    --hash=sha256:e13cba03c7af8c8a846c4495875a09d64362cc4caeed495ada5390644411bbe7 \\\n    --hash=sha256:e1935d12e503780b859d343161a80df65205d23cad7b4f6c3df6e50321e188a3 \\\n    --hash=sha256:e42258f66ad9f16d9b62e9c9642742982acb1f30b90f5061522048c1cb99814f \\\n    --hash=sha256:e794e44fa260300b8850246c6371d94014753c73528f97f6ccb42f5e7ce698ae \\\n    --hash=sha256:e8638355ae2f996356f7f281e03a3e3ce31f1259510f9d551465356532e0302c \\\n    --hash=sha256:e92878cc05e844c8da937204bc34c2e6caf66709ce5936802fbfb35f04132892 \\\n    --hash=sha256:ff32548e45e45bf3280ac1d28b3148337a5c6714c28db23aeb0693e33eba257e\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jupyter-ydoc\n    #   ypy-websocket\n```\n\n----------------------------------------\n\nTITLE: Installing kubectl-ray Plugin on Linux\nDESCRIPTION: Commands to download and install the kubectl-ray plugin binary for Linux amd64 systems. This installs version 1.3.0 of the plugin.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kubectl-plugin.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://github.com/ray-project/kuberay/releases/download/v1.3.0/kubectl-ray_v1.3.0_linux_amd64.tar.gz\ntar -xvf kubectl-ray_v1.3.0_linux_amd64.tar.gz\ncp kubectl-ray ~/.local/bin\n```\n\n----------------------------------------\n\nTITLE: Streaming Actor Logs with Python SDK\nDESCRIPTION: Illustrates streaming logs from a specific actor using the Ray Python SDK. It uses the get_log function with the actor ID to continuously print log lines.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import get_log\n\n# Get the Actor's ID from the output of `ray list actors`.\n# The loop blocks with `follow=True`\nfor line in get_log(actor_id=<ACTOR_ID>, follow=True):\n    print(line)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Security Group for Ray Client in YAML\nDESCRIPTION: This YAML configuration sets up an AWS security group to allow inbound access to the Ray Client server port. It's used in the cluster configuration file for the Ray cluster launcher.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/ray-client.rst#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# An unique identifier for the head node and workers of this cluster.\ncluster_name: minimal_security_group\n\n# Cloud-provider specific configuration.\nprovider:\n    type: aws\n    region: us-west-2\n    security_group:\n        GroupName: ray_client_security_group\n        IpPermissions:\n              - FromPort: 10001\n                ToPort: 10001\n                IpProtocol: TCP\n                IpRanges:\n                    # Allow traffic only from your local IP address.\n                    - CidrIp: <YOUR_IP_ADDRESS>/32\n```\n\n----------------------------------------\n\nTITLE: Example Python Class Documentation Style\nDESCRIPTION: Demonstrates the canonical Ray documentation style for Python classes\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass RayClass:\n    \"\"\"The summary line for a class docstring should fit on one line.\n\n    Additional explanatory text can be added in paragraphs such as this one.\n    Do not introduce multi-line first sentences.\n\n    The __init__ method is documented here in the class level docstring.\n\n    All the public methods and attributes should have docstrings.\n\n    Examples:\n        .. testcode::\n\n            obj = RayClass(12, \"world\")\n            obj.increment_attr1()\n\n    Args:\n        param1: The first parameter. Do not include the types in the\n            docstring. They should be defined only in the signature.\n            Multi-line parameter docs should be indented by four spaces.\n        param2: The second parameter.\n    \"\"\"\n\n    def __init__(self, param1: int, param2: str):\n        #: Public attribute is documented here.\n        self.attr1 = param1\n        #: Public attribute is documented here.\n        self.attr2 = param2\n\n    @property\n    def attr3(self) -> str:\n        \"\"\"Public property of the class.\n\n        Properties created with the @property decorator\n        should be documented here.\n        \"\"\"\n        return \"hello\"\n\n    def increment_attr1(self) -> None:\n        \"\"\"Class methods are similar to regular functions.\n\n        See above about how to document functions.\n        \"\"\"\n\n        self.attr1 = self.attr1 + 1\n```\n\n----------------------------------------\n\nTITLE: Package Dependency Hash Listing\nDESCRIPTION: Comprehensive list of SHA256 hashes for Python package dependencies used in the ray project. Each hash corresponds to a specific package version and is used for security verification during package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:2e920d506ec85eb4ba50cd4228c2bec05642894d4c73c59b3a2fe20346bd00ee \\\n    --hash=sha256:3359fc442a743e870f4588fcf5dcbc1bf929df1fad8fb9905cd94e5edb02e84c \\\n    --hash=sha256:37933ec6e693e51a5b07505bd05de57eee12f3e8c32b07da7e73669398e6630a\n```\n\n----------------------------------------\n\nTITLE: Package Hash Verification Config\nDESCRIPTION: Configuration block containing SHA-256 hash values for verifying package dependencies. These hashes are used to ensure package integrity during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:3102045a10945173d38336f6e71a8dc71bcaeed55c3123ad4af82c52807b9375 \\\n--hash=sha256:339894035d0ede518b16073bdc2feef4c991ee991a29774b33e515f1d308e08d \\\n--hash=sha256:412444afb8c4c7a6cc11a47dade32982439925537e483be7c0ae0cf96c4f6a0b\n```\n\n----------------------------------------\n\nTITLE: Visualizing Grid Search Results for Comparison\nDESCRIPTION: Creates visualization plots for grid search results to compare with PBT. The plots show parameter history and Q value history, illustrating how grid search is limited by fixed hyperparameter values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw=dict(width_ratios=[1.5, 1]))\n\ncolors = [\"red\", \"black\"]\nlabels = [\"h = [1, 0]\", \"h = [0, 1]\"]\n\nplot_parameter_history(\n    grid_results,\n    colors,\n    labels,\n    perturbation_interval=perturbation_interval,\n    fig=fig,\n    ax=axs[0],\n)\nplot_Q_history(grid_results, colors, labels, ax=axs[1])\n```\n\n----------------------------------------\n\nTITLE: Ray Data Execution Progress Log Format\nDESCRIPTION: Example log output showing execution progress of Ray Data operators, including active tasks, queued tasks, and block completion status.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/monitoring-your-workload.rst#2025-04-12_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nExecution Progress:\n0: - Input: 0 active, 0 queued, 0.0 MiB objects, Blocks Outputted: 200/200\n1: - ReadRange->MapBatches(<lambda>): 10 active, 190 queued, 381.47 MiB objects, Blocks Outputted: 100/200\n```\n\n----------------------------------------\n\nTITLE: Sample Ray application with tasks and actors\nDESCRIPTION: A simple Ray application that creates two long-running tasks and two actors. This example is used to demonstrate the monitoring capabilities of Ray's CLI and SDK.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport time\n\nray.init(num_cpus=4)\n\n@ray.remote\ndef task_running_300_seconds():\n    time.sleep(300)\n\n@ray.remote\nclass Actor:\n    def __init__(self):\n        pass\n\n# Create 2 tasks\ntasks = [task_running_300_seconds.remote() for _ in range(2)]\n\n# Create 2 actors\nactors = [Actor.remote() for _ in range(2)]\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how package dependencies are specified with exact versions and SHA256 hash values. It includes entries for packages like 'requests' and 'rich', along with their required versions and hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_14\n\nLANGUAGE: Text\nCODE:\n```\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   google-api-core\n    #   huggingface-hub\n    #   mistral-common\n    #   outlines\n    #   ray\n    #   tiktoken\n    #   transformers\n    #   vllm\nrich==13.3.2 \\\n    --hash=sha256:91954fe80cfb7985727a467ca98a7618e5dd15178cc2da10f553b36a93859001 \\\n    --hash=sha256:a104f37270bf677148d8acb07d33be1569eeee87e2d1beb286a4e9113caf6f2f\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   memray\n    #   typer\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry Dependencies with Hash Verification in Requirements File\nDESCRIPTION: A series of OpenTelemetry-related package specifications with pinned versions and SHA256 hash verification. Each entry includes comments indicating which components require these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-proto==1.1.0 \\\n    --hash=sha256:237ef4fdd7f752b2fe740352643f8ef82733bd8e0db8b46ed808125ac7c7f112 \\\n    --hash=sha256:ff1ad9a3c572075883c2af0053cefdfaba005d71eade783c4524d34660d53b60\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\nopentelemetry-sdk==1.1.0 \\\n    --hash=sha256:ba29274aab656572e97e0339afaad6f2bded4102324b1475ab7412079498df6e \\\n    --hash=sha256:da7dfa6188e8a39f34b99495260e6a1d398c86a9de064c7f0805db6f16733d94\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\nopentelemetry-semantic-conventions==0.20b0 \\\n    --hash=sha256:ecae7367203e5204c70518e6d24b438480d6a6f1e5c8ee9dc2145f176ff4452e \\\n    --hash=sha256:fac014ac2098b1a05fe58af77cbe74c825ff869d6d53d316c393cc77f507ec15\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Referencing Package Dependency for RayLLM Tests\nDESCRIPTION: Specifies the referencing package (version 0.36.2) with hash signatures for dependency verification and comments indicating it's used through jsonschema and outlines.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_28\n\nLANGUAGE: txt\nCODE:\n```\nreferencing==0.36.2 \\\n    --hash=sha256:df2e89862cd09deabbdba16944cc3f10feb6b3e6f18e902f7cc25609a34775aa \\\n    --hash=sha256:e8699adbbf8b5c7de96d8ffa0eb5c158b3beafce084968e2ea8bb08c6794dcd0\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   jsonschema\n    #   jsonschema-specifications\n    #   outlines\n```\n\n----------------------------------------\n\nTITLE: Installing CuPy with CUDA Support\nDESCRIPTION: Configuration for installing CuPy with CUDA 12.x support, excluding Darwin platform.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/dl-gpu-requirements.txt#2025-04-12_snippet_2\n\nLANGUAGE: pip\nCODE:\n```\ncupy-cuda12x==13.1.0; sys_platform != 'darwin'\n```\n\n----------------------------------------\n\nTITLE: Downloading and Running XGBoost Submission Script\nDESCRIPTION: Shell commands to download the XGBoost submission script from the Ray repository and execute it to submit the workload to the Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/examples/ml-example.md#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# Download the above script.\ncurl https://raw.githubusercontent.com/ray-project/ray/releases/2.0.0/doc/source/cluster/doc_code/xgboost_submit.py -o xgboost_submit.py\n# Run the script.\npython xgboost_submit.py\n```\n\n----------------------------------------\n\nTITLE: Installing wrapt Package with Hash Verification\nDESCRIPTION: This snippet defines the installation requirements for the wrapt package version 1.14.1 with extensive SHA256 hash verification options. The package is constrained by compiled requirements and is needed for cloud-requirements.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_48\n\nLANGUAGE: text\nCODE:\n```\nwrapt==1.14.1 \\\n    --hash=sha256:00b6d4ea20a906c0ca56d84f93065b398ab74b927a7a3dbd470f6fc503f95dc3 \\\n    --hash=sha256:01c205616a89d09827986bc4e859bcabd64f5a0662a7fe95e0d359424e0e071b \\\n    --hash=sha256:02b41b633c6261feff8ddd8d11c711df6842aba629fdd3da10249a53211a72c4 \\\n    --hash=sha256:07f7a7d0f388028b2df1d916e94bbb40624c59b48ecc6cbc232546706fac74c2 \\\n    --hash=sha256:11871514607b15cfeb87c547a49bca19fde402f32e2b1c24a632506c0a756656 \\\n    --hash=sha256:1b376b3f4896e7930f1f772ac4b064ac12598d1c38d04907e696cc4d794b43d3 \\\n    --hash=sha256:2020f391008ef874c6d9e208b24f28e31bcb85ccff4f335f15a3251d222b92d9 \\\n    --hash=sha256:21ac0156c4b089b330b7666db40feee30a5d52634cc4560e1905d6529a3897ff \\\n    --hash=sha256:240b1686f38ae665d1b15475966fe0472f78e71b1b4903c143a842659c8e4cb9 \\\n    --hash=sha256:257fd78c513e0fb5cdbe058c27a0624c9884e735bbd131935fd49e9fe719d310 \\\n    --hash=sha256:26046cd03936ae745a502abf44dac702a5e6880b2b01c29aea8ddf3353b68224 \\\n    --hash=sha256:2b39d38039a1fdad98c87279b48bc5dce2c0ca0d73483b12cb72aa9609278e8a \\\n    --hash=sha256:2cf71233a0ed05ccdabe209c606fe0bac7379fdcf687f39b944420d2a09fdb57 \\\n    --hash=sha256:2fe803deacd09a233e4762a1adcea5db5d31e6be577a43352936179d14d90069 \\\n    --hash=sha256:2feecf86e1f7a86517cab34ae6c2f081fd2d0dac860cb0c0ded96d799d20b335 \\\n    --hash=sha256:3232822c7d98d23895ccc443bbdf57c7412c5a65996c30442ebe6ed3df335383 \\\n    --hash=sha256:34aa51c45f28ba7f12accd624225e2b1e5a3a45206aa191f6f9aac931d9d56fe \\\n    --hash=sha256:358fe87cc899c6bb0ddc185bf3dbfa4ba646f05b1b0b9b5a27c2cb92c2cea204 \\\n    --hash=sha256:36f582d0c6bc99d5f39cd3ac2a9062e57f3cf606ade29a0a0d6b323462f4dd87 \\\n    --hash=sha256:380a85cf89e0e69b7cfbe2ea9f765f004ff419f34194018a6827ac0e3edfed4d \\\n    --hash=sha256:40e7bc81c9e2b2734ea4bc1aceb8a8f0ceaac7c5299bc5d69e37c44d9081d43b \\\n    --hash=sha256:43ca3bbbe97af00f49efb06e352eae40434ca9d915906f77def219b88e85d907 \\\n    --hash=sha256:49ef582b7a1152ae2766557f0550a9fcbf7bbd76f43fbdc94dd3bf07cc7168be \\\n    --hash=sha256:4fcc4649dc762cddacd193e6b55bc02edca674067f5f98166d7713b193932b7f \\\n    --hash=sha256:5a0f54ce2c092aaf439813735584b9537cad479575a09892b8352fea5e988dc0 \\\n    --hash=sha256:5a9a0d155deafd9448baff28c08e150d9b24ff010e899311ddd63c45c2445e28 \\\n    --hash=sha256:5b02d65b9ccf0ef6c34cba6cf5bf2aab1bb2f49c6090bafeecc9cd81ad4ea1c1 \\\n    --hash=sha256:60db23fa423575eeb65ea430cee741acb7c26a1365d103f7b0f6ec412b893853 \\\n    --hash=sha256:642c2e7a804fcf18c222e1060df25fc210b9c58db7c91416fb055897fc27e8cc \\\n    --hash=sha256:6447e9f3ba72f8e2b985a1da758767698efa72723d5b59accefd716e9e8272bf \\\n    --hash=sha256:6a9a25751acb379b466ff6be78a315e2b439d4c94c1e99cb7266d40a537995d3 \\\n    --hash=sha256:6b1a564e6cb69922c7fe3a678b9f9a3c54e72b469875aa8018f18b4d1dd1adf3 \\\n    --hash=sha256:6d323e1554b3d22cfc03cd3243b5bb815a51f5249fdcbb86fda4bf62bab9e164 \\\n    --hash=sha256:6e743de5e9c3d1b7185870f480587b75b1cb604832e380d64f9504a0535912d1 \\\n    --hash=sha256:709fe01086a55cf79d20f741f39325018f4df051ef39fe921b1ebe780a66184c \\\n    --hash=sha256:7b7c050ae976e286906dd3f26009e117eb000fb2cf3533398c5ad9ccc86867b1 \\\n    --hash=sha256:7d2872609603cb35ca513d7404a94d6d608fc13211563571117046c9d2bcc3d7 \\\n    --hash=sha256:7ef58fb89674095bfc57c4069e95d7a31cfdc0939e2a579882ac7d55aadfd2a1 \\\n    --hash=sha256:80bb5c256f1415f747011dc3604b59bc1f91c6e7150bd7db03b19170ee06b320 \\\n    --hash=sha256:81b19725065dcb43df02b37e03278c011a09e49757287dca60c5aecdd5a0b8ed \\\n    --hash=sha256:833b58d5d0b7e5b9832869f039203389ac7cbf01765639c7309fd50ef619e0b1 \\\n    --hash=sha256:88bd7b6bd70a5b6803c1abf6bca012f7ed963e58c68d76ee20b9d751c74a3248 \\\n    --hash=sha256:8ad85f7f4e20964db4daadcab70b47ab05c7c1cf2a7c1e51087bfaa83831854c \\\n    --hash=sha256:8c0ce1e99116d5ab21355d8ebe53d9460366704ea38ae4d9f6933188f327b456 \\\n    --hash=sha256:8d649d616e5c6a678b26d15ece345354f7c2286acd6db868e65fcc5ff7c24a77 \\\n    --hash=sha256:903500616422a40a98a5a3c4ff4ed9d0066f3b4c951fa286018ecdf0750194ef \\\n    --hash=sha256:9736af4641846491aedb3c3f56b9bc5568d92b0692303b5a305301a95dfd38b1 \\\n    --hash=sha256:988635d122aaf2bdcef9e795435662bcd65b02f4f4c1ae37fbee7401c440b3a7 \\\n    --hash=sha256:9cca3c2cdadb362116235fdbd411735de4328c61425b0aa9f872fd76d02c4e86 \\\n    --hash=sha256:9e0fd32e0148dd5dea6af5fee42beb949098564cc23211a88d799e434255a1f4 \\\n    --hash=sha256:9f3e6f9e05148ff90002b884fbc2a86bd303ae847e472f44ecc06c2cd2fcdb2d \\\n    --hash=sha256:a85d2b46be66a71bedde836d9e41859879cc54a2a04fad1191eb50c2066f6e9d \\\n    --hash=sha256:a9008dad07d71f68487c91e96579c8567c98ca4c3881b9b113bc7b33e9fd78b8 \\\n    --hash=sha256:a9a52172be0b5aae932bef82a79ec0a0ce87288c7d132946d645eba03f0ad8a8 \\\n    --hash=sha256:aa31fdcc33fef9eb2552cbcbfee7773d5a6792c137b359e82879c101e98584c5 \\\n    --hash=sha256:acae32e13a4153809db37405f5eba5bac5fbe2e2ba61ab227926a22901051c0a \\\n    --hash=sha256:b014c23646a467558be7da3d6b9fa409b2c567d2110599b7cf9a0c5992b3b471 \\\n    --hash=sha256:b21bb4c09ffabfa0e85e3a6b623e19b80e7acd709b9f91452b8297ace2a8ab00 \\\n    --hash=sha256:b5901a312f4d14c59918c221323068fad0540e34324925c8475263841dbdfe68 \\\n    --hash=sha256:b9b7a708dd92306328117d8c4b62e2194d00c365f18eff11a9b53c6f923b01e3 \\\n    --hash=sha256:d1967f46ea8f2db647c786e78d8cc7e4313dbd1b0aca360592d8027b8508e24d \\\n    --hash=sha256:d52a25136894c63de15a35bc0bdc5adb4b0e173b9c0d07a2be9d3ca64a332735 \\\n    --hash=sha256:d77c85fedff92cf788face9bfa3ebaa364448ebb1d765302e9af11bf449ca36d \\\n    --hash=sha256:d79d7d5dc8a32b7093e81e97dad755127ff77bcc899e845f41bf71747af0c569 \\\n    --hash=sha256:dbcda74c67263139358f4d188ae5faae95c30929281bc6866d00573783c422b7 \\\n    --hash=sha256:ddaea91abf8b0d13443f6dac52e89051a5063c7d014710dcb4d4abb2ff811a59 \\\n    --hash=sha256:dee0ce50c6a2dd9056c20db781e9c1cfd33e77d2d569f5d1d9321c641bb903d5 \\\n    --hash=sha256:dee60e1de1898bde3b238f18340eec6148986da0455d8ba7848d50470a7a32fb \\\n    --hash=sha256:e2f83e18fe2f4c9e7db597e988f72712c0c3676d337d8b101f6758107c42425b \\\n    --hash=sha256:e3fb1677c720409d5f671e39bac6c9e0e422584e5f518bfd50aa4cbbea02433f \\\n    --hash=sha256:ecee4132c6cd2ce5308e21672015ddfed1ff975ad0ac8d27168ea82e71413f55 \\\n    --hash=sha256:ee2b1b1769f6707a8a445162ea16dddf74285c3964f605877a20e38545c3c462 \\\n    --hash=sha256:ee6acae74a2b91865910eef5e7de37dc6895ad96fa23603d1d27ea69df545015 \\\n    --hash=sha256:ef3f72c9666bba2bab70d2a8b79f2c6d2c1a42a7f7e2b0ec83bb2f9e383950af\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Hash Verification\nDESCRIPTION: Detailed package version specifications with SHA256 hash verification for Python dependencies. Includes specific version pins and multiple hash verifications for each package to ensure security and reproducibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406\n```\n\n----------------------------------------\n\nTITLE: Defining Ray LLM Dependencies in requirements.txt\nDESCRIPTION: Lists the required Python packages for the Ray LLM module, with minimum version specifications where applicable. The file includes vllm for LLM functionality, jsonref and jsonschema for JSON mode support, ninja for compilation, async-timeout for Python versions below 3.11, and typer for command-line interfaces.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/llm/llm-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Keep this in sync with the definition in setup.py for ray[llm]\nvllm>=0.8.2\n# For json mode\njsonref>=1.1.0\njsonschema\nninja\n# async-timeout is a backport of asyncio.timeout for python < 3.11\nasync-timeout; python_version < '3.11'\ntyper\n```\n\n----------------------------------------\n\nTITLE: Generating Ray Project Requirements File with pip-compile\nDESCRIPTION: This command uses pip-compile to generate a comprehensive requirements file for the Ray project. It includes various flags to specify package sources, handle unsafe packages, and include additional requirement files for different project components.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cpu --find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c python/requirements_compiled_ray_test_py311_cpu.txt python/requirements.txt python/requirements/cloud-requirements.txt python/requirements/base-test-requirements.txt python/requirements/llm/llm-requirements.txt python/requirements/llm/llm-test-requirements.txt -o python/requirements_compiled_rayllm_test_py311_cpu.txt\n```\n\n----------------------------------------\n\nTITLE: Visualizing an Image from the Batch using PIL\nDESCRIPTION: Converts the first image in the batch from a Numpy array to a PIL Image object for visualization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/huggingface_vit_batch_prediction.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n\nimg = Image.fromarray(single_batch[\"image\"][0])\nimg\n```\n\n----------------------------------------\n\nTITLE: Pinned Pandas Dependency with Hash Verification\nDESCRIPTION: Specifies pandas version 1.5.3 with SHA256 hash verification for secure installation. Includes multiple hash values to verify package integrity across different distributions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    --hash=sha256:41179ce559943d83a9b4bbacb736b04c928b095b5f25dd2b7389eda08f46f373 \\\n    --hash=sha256:478ff646ca42b20376e4ed3fa2e8d7341e8a63105586efe54fa2508ee087f328 \\\n    --hash=sha256:50869a35cbb0f2e0cd5ec04b191e7b12ed688874bd05dd777c19b28cbea90996 \\\n    --hash=sha256:565fa34a5434d38e9d250af3c12ff931abaf88050551d9fbcdfafca50d62babf \\\n    --hash=sha256:5f2b952406a1588ad4cad5b3f55f520e82e902388a6d5a4a91baa8d38d23c7f6 \\\n    --hash=sha256:5fbcb19d6fceb9e946b3e23258757c7b225ba450990d9ed63ccceeb8cae609f7 \\\n    --hash=sha256:6973549c01ca91ec96199e940495219c887ea815b2083722821f1d7abfa2b4dc \\\n    --hash=sha256:74a3fd7e5a7ec052f183273dc7b0acd3a863edf7520f5d3a1765c04ffdb3b0b1 \\\n    --hash=sha256:7a0a56cef15fd1586726dace5616db75ebcfec9179a3a55e78f72c5639fa2a23 \\\n    --hash=sha256:7cec0bee9f294e5de5bbfc14d0573f65526071029d036b753ee6507d2a21480a \\\n    --hash=sha256:87bd9c03da1ac870a6d2c8902a0e1fd4267ca00f13bc494c9e5a9020920e1d51 \\\n    --hash=sha256:972d8a45395f2a2d26733eb8d0f629b2f90bebe8e8eddbb8829b180c09639572 \\\n    --hash=sha256:9842b6f4b8479e41968eced654487258ed81df7d1c9b7b870ceea24ed9459b31 \\\n    --hash=sha256:9f69c4029613de47816b1bb30ff5ac778686688751a5e9c99ad8c7031f6508e5 \\\n    --hash=sha256:a50d9a4336a9621cab7b8eb3fb11adb82de58f9b91d84c2cd526576b881a0c5a \\\n    --hash=sha256:bc4c368f42b551bf72fac35c5128963a171b40dce866fb066540eeaf46faa003 \\\n    --hash=sha256:c39a8da13cede5adcd3be1182883aea1c925476f4e84b2807a46e2775306305d \\\n    --hash=sha256:c3ac844a0fe00bfaeb2c9b51ab1424e5c8744f89860b138434a363b1f620f354 \\\n    --hash=sha256:c4c00e0b0597c8e4f59e8d461f797e5d70b4d025880516a8261b2817c47759ee \\\n    --hash=sha256:c74a62747864ed568f5a82a49a23a8d7fe171d0c69038b38cedf0976831296fa \\\n    --hash=sha256:dd05f7783b3274aa206a1af06f0ceed3f9b412cf665b7247eacd83be41cf7bf0 \\\n    --hash=sha256:dfd681c5dc216037e0b0a2c821f5ed99ba9f03ebcf119c7dac0e9a7b960b9ec9 \\\n    --hash=sha256:e474390e60ed609cec869b0da796ad94f420bb057d86784191eefc62b65819ae \\\n    --hash=sha256:f76d097d12c82a535fda9dfe5e8dd4127952b45fea9b0276cb30cca5ea313fbc\n```\n\n----------------------------------------\n\nTITLE: Specifying propcache Dependency with Hash Verification\nDESCRIPTION: Defines the propcache package dependency at version 0.3.0 with cryptographic hash verification for package integrity. The comment indicates this dependency is referenced from other requirement files in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_29\n\nLANGUAGE: plaintext\nCODE:\n```\npropcache==0.3.0 \\\n    --hash=sha256:02df07041e0820cacc8f739510078f2aadcfd3fc57eaeeb16d5ded85c872c89e \\\n    --hash=sha256:03acd9ff19021bd0567582ac88f821b66883e158274183b9e5586f678984f8fe \\\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   aiohttp\n    #   yarl\n```\n\n----------------------------------------\n\nTITLE: Listing Package Requirements with SHA-256 Hashes\nDESCRIPTION: This snippet shows Python package dependencies with SHA-256 hash verification to ensure package integrity and security. The file includes entries for multiple packages with their version constraints, hash values, and associated dependency relationships.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:47e147cdda9037f94b399bf03bfd8a6b6b1f2f90be94a454e3386f006455a9b4 \\\n--hash=sha256:6912a87782acdff6eb8bf01675ed01d60ca1f2551f8176a300a886f09e836a6a \\\n--hash=sha256:6d4fd101f571a31acb1559ae1af30f30b1dc4b3186669f92ad780e17c81e91bc \\\n--hash=sha256:74937acd22dc11b33946b67dca7680e6d103d6e90eeaaaf932603bec6fe7b03a \\\n--hash=sha256:7a2872ee80dcf6b5dbdc838763d26554c2a18aa833d31a2635bff16aafefb9c9 \\\n--hash=sha256:7d434ec7e2ce3cc8f452d1cd9a28591745de022f931d67be688a737320dfcead \\\n--hash=sha256:977525a1e5f4059316b183fb4fd34fa858c9eade31f165427a3977c95e3ee749 \\\n--hash=sha256:9cd2a7376f7b3367019b664c21f0c61766219faa3b03731113ead75107f3b66c \\\n--hash=sha256:a289af9a1687c6cf463478f0fa8e8aa3b6fb813317b0d70bf1ed0759eab6f761 \\\n--hash=sha256:ae2b5b5c3ef67354824fb75517c8db5fbe93bc02cd9671f3c62271626bc041d5 \\\n--hash=sha256:bc9efc739cc6ed760f795806f67889923f7274276f0eb45092a1473e40d9b867 \\\n--hash=sha256:c1da416ab53e4f7f3bc8d4eeba36d801cc1894b9fbfbf2022b29b6bad34a7df2 \\\n--hash=sha256:d5bd550001d26450bd90777736c69d68c487d17bf371438f975229b2b8241a91 \\\n--hash=sha256:df6509e1507ca0760787a199d19439cc887bfd82226f5af746d6977bd9f66844 \\\n--hash=sha256:e0a9a1a39d4bf3517f2af9d23d479b4175ead205c592ceeb8b89af48a327ea57 \\\n--hash=sha256:eccce86bba940bae0d8d48ed925f21dbb813519169246e2ab292b5092aba121f \\\n--hash=sha256:f99b600aa7f65235a5a05d0b9a9f31150c390f31261f2a0ba678e26823ec38f7\n# via\n#   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n#   numba\n```\n\n----------------------------------------\n\nTITLE: Specifying CUDA-specific Package Requirements\nDESCRIPTION: This snippet demonstrates how CUDA-specific packages are specified with version and platform constraints. It includes hash values and platform-specific installation conditions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_5\n\nLANGUAGE: Text\nCODE:\n```\ncupy-cuda12x==13.1.0 ; sys_platform != 'darwin' \\\n    --hash=sha256:230f8a8e99c81a653baa0ed00819990c0ed1f0cf0298214786b5e323461dc61a \\\n    --hash=sha256:2d16eaa2d086e416ac13467d4ff3184b9a081fe76b761ce51d4a46ec1c4bd28a \\\n    --hash=sha256:432273fd4b61a284f7d705d08b8291403548fd422bcbd945635cc155bc6a923d \\\n    --hash=sha256:4c51a1062a3c5a826b0425952d229ffe73b1791656a31de95b318117e67a9576 \\\n    --hash=sha256:4c8e9fdb1f3ffc3151808f8bb8c871518d2783e1be8b53792b698a840543d60c \\\n    --hash=sha256:51b1d6cb83d82dfa306c9efaeb4d57f24bad3041ebd8716d61072676abbcf67b \\\n    --hash=sha256:52185a2cf95d3bac2c3fda95c9c8e06a985b5a00cd2e587d3caace337db33899 \\\n    --hash=sha256:5afb6658faa22f21479ae2c0a07254df31c0aebc36907a64a1f6be4ecc9e96da \\\n    --hash=sha256:d3dc91ef9c4104652195eea4b282d343ecad653021efe20d1c8dd8dfe8ccfd86 \\\n    --hash=sha256:d60d1e124592cb82a5f3f45b3e7bee7bda7b72a743029f275e9d6b125f338c60 \\\n    --hash=sha256:dac0284fecb90b5731f514e569a6fcf6674a730ae95b9490781a713b60a34423 \\\n    --hash=sha256:e7a25ef1b44ae6276b5105affc2289edb34f1aa6676babd5bcd80907348c4cfa\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with SHA-256 Hashes\nDESCRIPTION: Detailed requirements file containing Python package dependencies with their versions and SHA-256 hash values for security verification. Includes dependency relationships indicated by comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:fa0bd143b4f4be3557cb892fa0612ef210fd91a92bd720b4d8221de576a4fa00\n# via outlines\nalabaster==0.7.16 \\\n    --hash=sha256:75a8b99c28a5dad50dd7f8ccdd447a121ddb3892da9e53d1ca5cca3106d58d65 \\\n    --hash=sha256:b46733c07dce03ae4e150330b975c75737fa60f0a7c591b6c8bf4928a28e2c92\n    # via sphinx\nannotated-types==0.6.0 \\\n    --hash=sha256:0641064de18ba7a25dee8f96403ebc39113d0cb953a01429249d5c7564666a43 \\\n    --hash=sha256:563339e807e53ffd9c267e99fc6d9ea23eb8443c08f112651963e24e22f84a5d\n```\n\n----------------------------------------\n\nTITLE: Multidict 6.0.5 SHA-256 Hash Verification\nDESCRIPTION: SHA-256 hash verification values for the multidict package version 6.0.5. Multidict is likely a dependency for the Ray project, with these hashes ensuring package integrity during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nmultidict==6.0.5 \\\n    --hash=sha256:01265f5e40f5a17f8241d52656ed27192be03bfa8764d88e8220141d1e4b3556 \\\n    --hash=sha256:0275e35209c27a3f7951e1ce7aaf93ce0d163b28948444bec61dd7badc6d3f8c \\\n    --hash=sha256:04bde7a7b3de05732a4eb39c94574db1ec99abb56162d6c520ad26f83267de29 \\\n    --hash=sha256:04da1bb8c8dbadf2a18a452639771951c662c5ad03aefe4884775454be322c9b \\\n    --hash=sha256:09a892e4a9fb47331da06948690ae38eaa2426de97b4ccbfafbdcbe5c8f37ff8 \\\n    --hash=sha256:0d63c74e3d7ab26de115c49bffc92cc77ed23395303d496eae515d4204a625e7 \\\n    --hash=sha256:107c0cdefe028703fb5dafe640a409cb146d44a6ae201e55b35a4af8e95457dd \\\n    --hash=sha256:141b43360bfd3bdd75f15ed811850763555a251e38b2405967f8e25fb43f7d40 \\\n    --hash=sha256:14c2976aa9038c2629efa2c148022ed5eb4cb939e15ec7aace7ca932f48f9ba6 \\\n    --hash=sha256:19fe01cea168585ba0f678cad6f58133db2aa14eccaf22f88e4a6dccadfad8b3 \\\n    --hash=sha256:1d147090048129ce3c453f0292e7697d333db95e52616b3793922945804a433c \\\n    --hash=sha256:1d9ea7a7e779d7a3561aade7d596649fbecfa5c08a7674b11b423783217933f9 \\\n    --hash=sha256:215ed703caf15f578dca76ee6f6b21b7603791ae090fbf1ef9d865571039ade5 \\\n    --hash=sha256:21fd81c4ebdb4f214161be351eb5bcf385426bf023041da2fd9e60681f3cebae \\\n    --hash=sha256:220dd781e3f7af2c2c1053da9fa96d9cf3072ca58f057f4c5adaaa1cab8fc442 \\\n    --hash=sha256:228b644ae063c10e7f324ab1ab6b548bdf6f8b47f3ec234fef1093bc2735e5f9 \\\n    --hash=sha256:29bfeb0dff5cb5fdab2023a7a9947b3b4af63e9c47cae2a10ad58394b517fddc \\\n    --hash=sha256:2f4848aa3baa109e6ab81fe2006c77ed4d3cd1e0ac2c1fbddb7b1277c168788c \\\n    --hash=sha256:2faa5ae9376faba05f630d7e5e6be05be22913782b927b19d12b8145968a85ea \\\n    --hash=sha256:2ffc42c922dbfddb4a4c3b438eb056828719f07608af27d163191cb3e3aa6cc5 \\\n    --hash=sha256:37b15024f864916b4951adb95d3a80c9431299080341ab9544ed148091b53f50 \\\n    --hash=sha256:3cc2ad10255f903656017363cd59436f2111443a76f996584d1077e43ee51182 \\\n    --hash=sha256:3d25f19500588cbc47dc19081d78131c32637c25804df8414463ec908631e453 \\\n    --hash=sha256:403c0911cd5d5791605808b942c88a8155c2592e05332d2bf78f18697a5fa15e \\\n    --hash=sha256:411bf8515f3be9813d06004cac41ccf7d1cd46dfe233705933dd163b60e37600 \\\n    --hash=sha256:425bf820055005bfc8aa9a0b99ccb52cc2f4070153e34b701acc98d201693733 \\\n    --hash=sha256:435a0984199d81ca178b9ae2c26ec3d49692d20ee29bc4c11a2a8d4514c67eda \\\n    --hash=sha256:4a6a4f196f08c58c59e0b8ef8ec441d12aee4125a7d4f4fef000ccb22f8d7241 \\\n    --hash=sha256:4cc0ef8b962ac7a5e62b9e826bd0cd5040e7d401bc45a6835910ed699037a461 \\\n    --hash=sha256:51d035609b86722963404f711db441cf7134f1889107fb171a970c9701f92e1e \\\n    --hash=sha256:53689bb4e102200a4fafa9de9c7c3c212ab40a7ab2c8e474491914d2305f187e \\\n    --hash=sha256:55205d03e8a598cfc688c71ca8ea5f66447164efff8869517f175ea632c7cb7b \\\n    --hash=sha256:5c0631926c4f58e9a5ccce555ad7747d9a9f8b10619621f22f9635f069f6233e \\\n    --hash=sha256:5cb241881eefd96b46f89b1a056187ea8e9ba14ab88ba632e68d7a2ecb7aadf7 \\\n    --hash=sha256:60d698e8179a42ec85172d12f50b1668254628425a6bd611aba022257cac1386 \\\n    --hash=sha256:612d1156111ae11d14afaf3a0669ebf6c170dbb735e510a7438ffe2369a847fd \\\n    --hash=sha256:6214c5a5571802c33f80e6c84713b2c79e024995b9c5897f794b43e714daeec9 \\\n    --hash=sha256:6939c95381e003f54cd4c5516740faba40cf5ad3eeff460c3ad1d3e0ea2549bf \\\n    --hash=sha256:69db76c09796b313331bb7048229e3bee7928eb62bab5e071e9f7fcc4879caee \\\n    --hash=sha256:6bf7a982604375a8d49b6cc1b781c1747f243d91b81035a9b43a2126c04766f5 \\\n    --hash=sha256:766c8f7511df26d9f11cd3a8be623e59cca73d44643abab3f8c8c07620524e4a \\\n    --hash=sha256:76c0de87358b192de7ea9649beb392f107dcad9ad27276324c24c91774ca5271 \\\n    --hash=sha256:76f067f5121dcecf0d63a67f29080b26c43c71a98b10c701b0677e4a065fbd54 \\\n    --hash=sha256:7901c05ead4b3fb75113fb1dd33eb1253c6d3ee37ce93305acd9d38e0b5f21a4 \\\n    --hash=sha256:79660376075cfd4b2c80f295528aa6beb2058fd289f4c9252f986751a4cd0496 \\\n    --hash=sha256:79a6d2ba910adb2cbafc95dad936f8b9386e77c84c35bc0add315b856d7c3abb \\\n    --hash=sha256:7afcdd1fc07befad18ec4523a782cde4e93e0a2bf71239894b8d61ee578c1319 \\\n    --hash=sha256:7be7047bd08accdb7487737631d25735c9a04327911de89ff1b26b81745bd4e3 \\\n    --hash=sha256:7c6390cf87ff6234643428991b7359b5f59cc15155695deb4eda5c777d2b880f \\\n    --hash=sha256:7df704ca8cf4a073334e0427ae2345323613e4df18cc224f647f251e5e75a527 \\\n    --hash=sha256:85f67aed7bb647f93e7520633d8f51d3cbc6ab96957c71272b286b2f30dc70ed \\\n    --hash=sha256:896ebdcf62683551312c30e20614305f53125750803b614e9e6ce74a96232604 \\\n    --hash=sha256:92d16a3e275e38293623ebf639c471d3e03bb20b8ebb845237e0d3664914caef \\\n    --hash=sha256:99f60d34c048c5c2fabc766108c103612344c46e35d4ed9ae0673d33c8fb26e8 \\\n    --hash=sha256:9fe7b0653ba3d9d65cbe7698cca585bf0f8c83dbbcc710db9c90f478e175f2d5 \\\n    --hash=sha256:a3145cb08d8625b2d3fee1b2d596a8766352979c9bffe5d7833e0503d0f0b5e5 \\\n    --hash=sha256:aeaf541ddbad8311a87dd695ed9642401131ea39ad7bc8cf3ef3967fd093b626 \\\n    --hash=sha256:b55358304d7a73d7bdf5de62494aaf70bd33015831ffd98bc498b433dfe5b10c \\\n    --hash=sha256:b82cc8ace10ab5bd93235dfaab2021c70637005e1ac787031f4d1da63d493c1d \\\n    --hash=sha256:c0868d64af83169e4d4152ec612637a543f7a336e4a307b119e98042e852ad9c \\\n    --hash=sha256:c1c1496e73051918fcd4f58ff2e0f2f3066d1c76a0c6aeffd9b45d53243702cc \\\n    --hash=sha256:c9bf56195c6bbd293340ea82eafd0071cb3d450c703d2c93afb89f93b8386ccc \\\n\n```\n\n----------------------------------------\n\nTITLE: Specifying Pydantic-Core Package Version with Hash Verification for Ray Project\nDESCRIPTION: Defines Pydantic-Core version 2.23.4 with multiple SHA256 hashes for verification. This is a dependency used by Pydantic for core validation functionality and optimized performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_37\n\nLANGUAGE: pip requirements\nCODE:\n```\npydantic-core==2.23.4 \\\n    --hash=sha256:0a7df63886be5e270da67e0966cf4afbae86069501d35c8c1b3b6c168f42cb36 \\\n    --hash=sha256:0cb3da3fd1b6a5d0279a01877713dbda118a2a4fc6f0d821a57da2e464793f05 \\\n    --hash=sha256:0dbd8dbed2085ed23b5c04afa29d8fd2771674223135dc9bc937f3c09284d071 \\\n    --hash=sha256:0dff76e0602ca7d4cdaacc1ac4c005e0ce0dcfe095d5b5259163a80d3a10d327 \\\n    --hash=sha256:1278e0d324f6908e872730c9102b0112477a7f7cf88b308e4fc36ce1bdb6d58c \\\n    --hash=sha256:128585782e5bfa515c590ccee4b727fb76925dd04a98864182b22e89a4e6ed36 \\\n    --hash=sha256:1498bec4c05c9c787bde9125cfdcc63a41004ff167f495063191b863399b1a29 \\\n    --hash=sha256:19442362866a753485ba5e4be408964644dd6a09123d9416c54cd49171f50744 \\\n    --hash=sha256:1b84d168f6c48fabd1f2027a3d1bdfe62f92cade1fb273a5d68e621da0e44e6d \\\n    --hash=sha256:1e90d2e3bd2c3863d48525d297cd143fe541be8bbf6f579504b9712cb6b643ec \\\n    --hash=sha256:20152074317d9bed6b7a95ade3b7d6054845d70584216160860425f4fbd5ee9e \\\n    --hash=sha256:216f9b2d7713eb98cb83c80b9c794de1f6b7e3145eef40400c62e86cee5f4e1e \\\n    --hash=sha256:233710f069d251feb12a56da21e14cca67994eab08362207785cf8c598e74577 \\\n    --hash=sha256:255a8ef062cbf6674450e668482456abac99a5583bbafb73f9ad469540a3a232 \\\n    --hash=sha256:2584f7cf844ac4d970fba483a717dbe10c1c1c96a969bf65d61ffe94df1b2863 \\\n    --hash=sha256:2971bb5ffe72cc0f555c13e19b23c85b654dd2a8f7ab493c262071377bfce9f6 \\\n    --hash=sha256:29d2c342c4bc01b88402d60189f3df065fb0dda3654744d5a165a5288a657368 \\\n    --hash=sha256:2e203fdf807ac7e12ab59ca2bfcabb38c7cf0b33c41efeb00f8e5da1d86af480 \\\n    --hash=sha256:33e3d65a85a2a4a0dc3b092b938a4062b1a05f3a9abde65ea93b233bca0e03f2 \\\n    --hash=sha256:374a5e5049eda9e0a44c696c7ade3ff355f06b1fe0bb945ea3cac2bc336478a2 \\\n    --hash=sha256:37b0fe330e4a58d3c58b24d91d1eb102aeec675a3db4c292ec3928ecd892a9a6 \\\n    --hash=sha256:3d5639516376dce1940ea36edf408c554475369f5da2abd45d44621cb616f769 \\\n    --hash=sha256:42c6dcb030aefb668a2b7009c85b27f90e51e6a3b4d5c9bc4c57631292015b0d \\\n    --hash=sha256:4a7cd62e831afe623fbb7aabbb4fe583212115b3ef38a9f6b71869ba644624a2 \\\n    --hash=sha256:4ba762ed58e8d68657fc1281e9bb72e1c3e79cc5d464be146e260c541ec12d84 \\\n    --hash=sha256:4fc714bdbfb534f94034efaa6eadd74e5b93c8fa6315565a222f7b6f42ca1166 \\\n    --hash=sha256:4ffa2ebd4c8530079140dd2d7f794a9d9a73cbb8e9d59ffe24c63436efa8f271 \\\n    --hash=sha256:5a1504ad17ba4210df3a045132a7baeeba5a200e930f57512ee02909fc5c4cb5 \\\n    --hash=sha256:5c364564d17da23db1106787675fc7af45f2f7b58b4173bfdd105564e132e6fb \\\n    --hash=sha256:5e11661ce0fd30a6790e8bcdf263b9ec5988e95e63cf901972107efc49218b13 \\\n    --hash=sha256:5f54b118ce5de9ac21c363d9b3caa6c800341e8c47a508787e5868c6b79c9323 \\\n    --hash=sha256:5f5ff8d839f4566a474a969508fe1c5e59c31c80d9e140566f9a37bba7b8d556 \\\n    --hash=sha256:61817945f2fe7d166e75fbfb28004034b48e44878177fc54d81688e7b85a3665 \\\n    --hash=sha256:624e278a7d29b6445e4e813af92af37820fafb6dcc55c012c834f9e26f9aaaef \\\n    --hash=sha256:63e46b3169866bd62849936de036f901a9356e36376079b05efa83caeaa02ceb \\\n    --hash=sha256:6531b7ca5f951d663c339002e91aaebda765ec7d61b7d1e3991051906ddde119 \\\n    --hash=sha256:68665f4c17edcceecc112dfed5dbe6f92261fb9d6054b47d01bf6371a6196126 \\\n    --hash=sha256:696dd8d674d6ce621ab9d45b205df149399e4bb9aa34102c970b721554828510 \\\n    --hash=sha256:6f783e0ec4803c787bcea93e13e9932edab72068f68ecffdf86a99fd5918878b \\\n    --hash=sha256:723314c1d51722ab28bfcd5240d858512ffd3116449c557a1336cbe3919beb87 \\\n    --hash=sha256:74b9127ffea03643e998e0c5ad9bd3811d3dac8c676e47db17b0ee7c3c3bf35f \\\n    --hash=sha256:7530e201d10d7d14abce4fb54cfe5b94a0aefc87da539d0346a484ead376c3cc \\\n    --hash=sha256:77733e3892bb0a7fa797826361ce8a9184d25c8dffaec60b7ffe928153680ba8 \\\n    --hash=sha256:78ddaaa81421a29574a682b3179d4cf9e6d405a09b99d93ddcf7e5239c742e21 \\\n    --hash=sha256:7c9129eb40958b3d4500fa2467e6a83356b3b61bfff1b414c7361d9220f9ae8f \\\n    --hash=sha256:7d32706badfe136888bdea71c0def994644e09fff0bfe47441deaed8e96fdbc6 \\\n    --hash=sha256:81965a16b675b35e1d09dd14df53f190f9129c0202356ed44ab2728b1c905658 \\\n    --hash=sha256:8394d940e5d400d04cad4f75c0598665cbb81aecefaca82ca85bd28264af7f9b \\\n    --hash=sha256:86d2f57d3e1379a9525c5ab067b27dbb8a0642fb5d454e17a9ac434f9ce523e3 \\\n    --hash=sha256:883a91b5dd7d26492ff2f04f40fbb652de40fcc0afe07e8129e8ae779c2110eb \\\n    --hash=sha256:88ad334a15b32a791ea935af224b9de1bf99bcd62fabf745d5f3442199d86d59 \\\n    --hash=sha256:9261d3ce84fa1d38ed649c3638feefeae23d32ba9182963e465d58d62203bd24 \\\n    --hash=sha256:97df63000f4fea395b2824da80e169731088656d1818a11b95f3b173747b6cd9 \\\n    --hash=sha256:98d134c954828488b153d88ba1f34e14259284f256180ce659e8d83e9c05eaa3 \\\n    --hash=sha256:996a38a83508c54c78a5f41456b0103c30508fed9abcad0a59b876d7398f25fd \\\n    --hash=sha256:9a5bce9d23aac8f0cf0836ecfc033896aa8443b501c58d0602dbfd5bd5b37753 \\\n    --hash=sha256:9a6b5099eeec78827553827f4c6b8615978bb4b6a88e5d9b93eddf8bb6790f55 \\\n    --hash=sha256:9d18368b137c6295db49ce7218b1a9ba15c5bc254c96d7c9f9e924a9bc7825ad \\\n    --hash=sha256:a4fa4fc04dff799089689f4fd502ce7d59de529fc2f40a2c8836886c03e0175a \\\n    --hash=sha256:a5c7ba8ffb6d6f8f2ab08743be203654bb1aaa8c9dcb09f82ddd34eadb695605 \\\n    --hash=sha256:aea443fffa9fbe3af1a9ba721a87f926fe548d32cab71d188a6ede77d0ff244e \\\n    --hash=sha256:b10bd51f823d891193d4717448fab065733958bdb6a6b351967bd349d48d5c9b \\\n    --hash=sha256:ba1a0996f6c2773bd83e63f18914c1de3c9dd26d55f4ac302a7efe93fb8e7433 \\\n    --hash=sha256:bb2802e667b7051a1bebbfe93684841cc9351004e2badbd6411bf357ab8d5ac8 \\\n    --hash=sha256:cfdd16ab5e59fc31b5e906d1a3f666571abc367598e3e02c83403acabc092e07 \\\n    --hash=sha256:d06b0c8da4f16d1d1e352134427cb194a0a6e19ad5db9161bf32b2113409e728 \\\n    --hash=sha256:d0776dea117cf5272382634bd2a5c1b6eb16767c223c6a5317cd3e2a757c61a0 \\\n    --hash=sha256:d18ca8148bebe1b0a382a27a8ee60350091a6ddaf475fa05ef50dc35b5df6327 \\\n    --hash=sha256:d4488a93b071c04dc20f5cecc3631fc78b9789dd72483ba15d423b5b3689b555 \\\n    --hash=sha256:d5f7a395a8cf1621939692dba2a6b6a830efa6b3cee787d82c7de1ad2930de64 \\\n    --hash=sha256:d7a80d21d613eec45e3d41eb22f8f94ddc758a6c4720842dc74c0581f54993d6 \\\n    --hash=sha256:d97683ddee4723ae8c95d1eddac7c192e8c552da0c73a925a89fa8649bf13eea \\\n    --hash=sha256:dcedcd19a557e182628afa1d553c3895a9f825b936415d0dbd3cd0bbcfd29b4b \\\n    --hash=sha256:de6d1d1b9e5101508cb37ab0d972357cac5235f5c6533d1071964c47139257df \\\n\n```\n\n----------------------------------------\n\nTITLE: Starting Gaudi Container for PyTorch Training\nDESCRIPTION: Commands to pull and run a Docker container with Gaudi support for PyTorch training. It sets up the necessary environment variables and runtime for Gaudi devices.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\ndocker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n```\n\n----------------------------------------\n\nTITLE: Package Dependency Specification with Hashes for Ray Project\nDESCRIPTION: This snippet contains package dependencies including typer, typing-extensions, urllib3, uvicorn, uvloop, virtualenv, vllm, watchfiles, and websockets. Each package is specified with its exact version and secure hash values for verification. The file includes comments indicating which component requires each dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_36\n\nLANGUAGE: pip requirements\nCODE:\n```\ntyper==0.12.3 \\\n    --hash=sha256:070d7ca53f785acbccba8e7d28b08dcd88f79f1fbda035ade0aecec71ca5c914 \\\n    --hash=sha256:49e73131481d804288ef62598d97a1ceef3058905aa536a1134f90891ba35482\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements/llm/llm-requirements.txt\n    #   -r python/requirements.txt\n    #   fastapi-cli\ntyping-extensions==4.12.2 \\\n    --hash=sha256:04e5ca0351e0f3f85c6853954072df659d0d13fac324d0072316b67d7794700d \\\n    --hash=sha256:1a7ead55c7e559dd4dee8856e3a88b41225abfe1ce8df57b7c13915fe121ffb8\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   fastapi\n    #   gymnasium\n    #   huggingface-hub\n    #   mistral-common\n    #   openai\n    #   outlines\n    #   pydantic\n    #   pydantic-core\n    #   referencing\n    #   torch\n    #   typer\n    #   vllm\nurllib3==1.26.19 \\\n    --hash=sha256:37a0344459b199fce0e80b0d3569837ec6b6937435c5244e7fd73fa6006830f3 \\\n    --hash=sha256:3e3d753a8618b86d7de333b4223005f68720bcd6a7d2bcb9fbd2229ec7c1e429\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   requests\nuvicorn==0.22.0 \\\n    --hash=sha256:79277ae03db57ce7d9aa0567830bbb51d7a612f54d6e1e3e92da3ef24c2c8ed8 \\\n    --hash=sha256:e9434d3bbf05f310e762147f769c9f21235ee118ba2d2bf1155a7196448bd996\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   fastapi\n    #   fastapi-cli\nuvloop==0.21.0 ; platform_python_implementation != 'PyPy' and sys_platform != 'cygwin' and sys_platform != 'win32' \\\n    --hash=sha256:0878c2640cf341b269b7e128b1a5fed890adc4455513ca710d77d5e93aa6d6a0 \\\n    --hash=sha256:10d66943def5fcb6e7b37310eb6b5639fd2ccbc38df1177262b0640c3ca68c1f \\\n    --hash=sha256:10da8046cc4a8f12c91a1c39d1dd1585c41162a15caaef165c2174db9ef18bdc \\\n    --hash=sha256:17df489689befc72c39a08359efac29bbee8eee5209650d4b9f34df73d22e414 \\\n    --hash=sha256:183aef7c8730e54c9a3ee3227464daed66e37ba13040bb3f350bc2ddc040f22f \\\n    --hash=sha256:196274f2adb9689a289ad7d65700d37df0c0930fd8e4e743fa4834e850d7719d \\\n    --hash=sha256:221f4f2a1f46032b403bf3be628011caf75428ee3cc204a22addf96f586b19fd \\\n    --hash=sha256:2d1f581393673ce119355d56da84fe1dd9d2bb8b3d13ce792524e1607139feff \\\n    --hash=sha256:359ec2c888397b9e592a889c4d72ba3d6befba8b2bb01743f72fffbde663b59c \\\n    --hash=sha256:3bf12b0fda68447806a7ad847bfa591613177275d35b6724b1ee573faa3704e3 \\\n    --hash=sha256:4509360fcc4c3bd2c70d87573ad472de40c13387f5fda8cb58350a1d7475e58d \\\n    --hash=sha256:460def4412e473896ef179a1671b40c039c7012184b627898eea5072ef6f017a \\\n    --hash=sha256:461d9ae6660fbbafedd07559c6a2e57cd553b34b0065b6550685f6653a98c1cb \\\n    --hash=sha256:46923b0b5ee7fc0020bef24afe7836cb068f5050ca04caf6b487c513dc1a20b2 \\\n    --hash=sha256:53e420a3afe22cdcf2a0f4846e377d16e718bc70103d7088a4f7623567ba5fb0 \\\n    --hash=sha256:5ee4d4ef48036ff6e5cfffb09dd192c7a5027153948d85b8da7ff705065bacc6 \\\n    --hash=sha256:67dd654b8ca23aed0a8e99010b4c34aca62f4b7fce88f39d452ed7622c94845c \\\n    --hash=sha256:787ae31ad8a2856fc4e7c095341cccc7209bd657d0e71ad0dc2ea83c4a6fa8af \\\n    --hash=sha256:86975dca1c773a2c9864f4c52c5a55631038e387b47eaf56210f873887b6c8dc \\\n    --hash=sha256:87c43e0f13022b998eb9b973b5e97200c8b90823454d4bc06ab33829e09fb9bb \\\n    --hash=sha256:88cb67cdbc0e483da00af0b2c3cdad4b7c61ceb1ee0f33fe00e09c81e3a6cb75 \\\n    --hash=sha256:8a375441696e2eda1c43c44ccb66e04d61ceeffcd76e4929e527b7fa401b90fb \\\n    --hash=sha256:a5c39f217ab3c663dc699c04cbd50c13813e31d917642d459fdcec07555cc553 \\\n    --hash=sha256:b9fb766bb57b7388745d8bcc53a359b116b8a04c83a2288069809d2b3466c37e \\\n    --hash=sha256:baa0e6291d91649c6ba4ed4b2f982f9fa165b5bbd50a9e203c416a2797bab3c6 \\\n    --hash=sha256:baa4dcdbd9ae0a372f2167a207cd98c9f9a1ea1188a8a526431eef2f8116cc8d \\\n    --hash=sha256:bc09f0ff191e61c2d592a752423c767b4ebb2986daa9ed62908e2b1b9a9ae206 \\\n    --hash=sha256:bd53ecc9a0f3d87ab847503c2e1552b690362e005ab54e8a48ba97da3924c0dc \\\n    --hash=sha256:bfd55dfcc2a512316e65f16e503e9e450cab148ef11df4e4e679b5e8253a5281 \\\n    --hash=sha256:c097078b8031190c934ed0ebfee8cc5f9ba9642e6eb88322b9958b649750f72b \\\n    --hash=sha256:c0f3fa6200b3108919f8bdabb9a7f87f20e7097ea3c543754cabc7d717d95cf8 \\\n    --hash=sha256:e678ad6fe52af2c58d2ae3c73dc85524ba8abe637f134bf3564ed07f555c5e79 \\\n    --hash=sha256:ec7e6b09a6fdded42403182ab6b832b71f4edaf7f37a9a0e371a01db5f0cb45f \\\n    --hash=sha256:f0ce1b49560b1d2d8a2977e3ba4afb2414fb46b86a1b64056bc4ab929efdafbe \\\n    --hash=sha256:f38b2e090258d051d68a5b14d1da7203a3c3677321cf32a95a6f4db4dd8b6f26 \\\n    --hash=sha256:f3df876acd7ec037a3d005b3ab85a7e4110422e4d9c1571d4fc89b0fc41b6816 \\\n    --hash=sha256:f7089d2dc73179ce5ac255bdf37c236a9f914b264825fdaacaded6990a7fb4c2\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   uvicorn\nvirtualenv==20.29.1 \\\n    --hash=sha256:4e4cb403c0b0da39e13b46b1b2476e505cb0046b25f242bee80f62bf990b2779 \\\n    --hash=sha256:b8b8970138d32fb606192cb97f6cd4bb644fa486be9308fb9b63f81091b5dc35\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\nvllm==0.8.2 \\\n    --hash=sha256:32442b686c5dad8e6ddcf5a8b0cf3f741359fed6a9e9e940009f1daf80ae15de \\\n    --hash=sha256:9b337b1c4072ccb94b1bf2b716593fadbe2dcb8d091f9bcbd6b5c6d37f9842ac\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements/llm/llm-requirements.txt\nwatchfiles==0.19.0 \\\n    --hash=sha256:0089c6dc24d436b373c3c57657bf4f9a453b13767150d17284fc6162b2791911 \\\n    --hash=sha256:09ea3397aecbc81c19ed7f025e051a7387feefdb789cf768ff994c1228182fda \\\n    --hash=sha256:176a9a7641ec2c97b24455135d58012a5be5c6217fc4d5fef0b2b9f75dbf5154 \\\n    --hash=sha256:18b28f6ad871b82df9542ff958d0c86bb0d8310bb09eb8e87d97318a3b5273af \\\n    --hash=sha256:20b44221764955b1e703f012c74015306fb7e79a00c15370785f309b1ed9aa8d \\\n    --hash=sha256:3d7d267d27aceeeaa3de0dd161a0d64f0a282264d592e335fff7958cc0cbae7c \\\n    --hash=sha256:5471582658ea56fca122c0f0d0116a36807c63fefd6fdc92c71ca9a4491b6b48 \\\n    --hash=sha256:5569fc7f967429d4bc87e355cdfdcee6aabe4b620801e2cf5805ea245c06097c \\\n    --hash=sha256:68dce92b29575dda0f8d30c11742a8e2b9b8ec768ae414b54f7453f27bdf9545 \\\n    --hash=sha256:79c533ff593db861ae23436541f481ec896ee3da4e5db8962429b441bbaae16e \\\n    --hash=sha256:7f3920b1285a7d3ce898e303d84791b7bf40d57b7695ad549dc04e6a44c9f120 \\\n    --hash=sha256:91633e64712df3051ca454ca7d1b976baf842d7a3640b87622b323c55f3345e7 \\\n    --hash=sha256:945be0baa3e2440151eb3718fd8846751e8b51d8de7b884c90b17d271d34cae8 \\\n    --hash=sha256:9afd0d69429172c796164fd7fe8e821ade9be983f51c659a38da3faaaaac44dc \\\n    --hash=sha256:9c75eff897786ee262c9f17a48886f4e98e6cfd335e011c591c305e5d083c056 \\\n    --hash=sha256:b538014a87f94d92f98f34d3e6d2635478e6be6423a9ea53e4dd96210065e193 \\\n    --hash=sha256:b6577b8c6c8701ba8642ea9335a129836347894b666dd1ec2226830e263909d3 \\\n    --hash=sha256:c0376deac92377817e4fb8f347bf559b7d44ff556d9bc6f6208dd3f79f104aaf \\\n    --hash=sha256:cae3dde0b4b2078f31527acff6f486e23abed307ba4d3932466ba7cdd5ecec79 \\\n    --hash=sha256:cb5d45c4143c1dd60f98a16187fd123eda7248f84ef22244818c18d531a249d1 \\\n    --hash=sha256:d9b073073e048081e502b6c6b0b88714c026a1a4c890569238d04aca5f9ca74b \\\n    --hash=sha256:fac19dc9cbc34052394dbe81e149411a62e71999c0a19e1e09ce537867f95ae0\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   uvicorn\n    #   vllm\nwebsockets==15.0 \\\n    --hash=sha256:0e389efe46ccb25a1f93d08c7a74e8123a2517f7b7458f043bd7529d1a63ffeb \\\n    --hash=sha256:0f2205cdb444a42a7919690238fb5979a05439b9dbb73dd47c863d39640d85ab \\\n    --hash=sha256:10552fed076757a70ba2c18edcbc601c7637b30cdfe8c24b65171e824c7d6081 \\\n    --hash=sha256:110a847085246ab8d4d119632145224d6b49e406c64f1bbeed45c6f05097b680 \\\n    --hash=sha256:1206432cc6c644f6fc03374b264c5ff805d980311563202ed7fef91a38906276 \\\n    --hash=sha256:1657a9eecb29d7838e3b415458cc494e6d1b194f7ac73a34aa55c6fb6c72d1f3 \\\n    --hash=sha256:17f2854c6bd9ee008c4b270f7010fe2da6c16eac5724a175e75010aacd905b31 \\\n    --hash=sha256:190bc6ef8690cd88232a038d1b15714c258f79653abad62f7048249b09438af3 \\\n    --hash=sha256:1caf951110ca757b8ad9c4974f5cac7b8413004d2f29707e4d03a65d54cedf2b \\\n    --hash=sha256:24d5333a9b2343330f0f4eb88546e2c32a7f5c280f8dd7d3cc079beb0901781b \\\n    --hash=sha256:26ba70fed190708551c19a360f9d7eca8e8c0f615d19a574292b7229e0ae324c \\\n    --hash=sha256:2bd8ef197c87afe0a9009f7a28b5dc613bfc585d329f80b7af404e766aa9e8c7 \\\n    --hash=sha256:2ea4f210422b912ebe58ef0ad33088bc8e5c5ff9655a8822500690abc3b1232d \\\n    --hash=sha256:30cff3ef329682b6182c01c568f551481774c476722020b8f7d0daacbed07a17 \\\n    --hash=sha256:327adab7671f3726b0ba69be9e865bba23b37a605b585e65895c428f6e47e766 \\\n    --hash=sha256:32e02a2d83f4954aa8c17e03fe8ec6962432c39aca4be7e8ee346b05a3476904 \\\n    --hash=sha256:37d66646f929ae7c22c79bc73ec4074d6db45e6384500ee3e0d476daf55482a9 \\\n    --hash=sha256:3a302241fbe825a3e4fe07666a2ab513edfdc6d43ce24b79691b45115273b5e7 \\\n    --hash=sha256:3abd670ca7ce230d5a624fd3d55e055215d8d9b723adee0a348352f5d8d12ff4 \\\n    --hash=sha256:4095a1f2093002c2208becf6f9a178b336b7572512ee0a1179731acb7788e8ad \\\n    --hash=sha256:45535fead66e873f411c1d3cf0d3e175e66f4dd83c4f59d707d5b3e4c56541c4 \\\n    --hash=sha256:45d464622314973d78f364689d5dbb9144e559f93dca11b11af3f2480b5034e1 \\\n    --hash=sha256:4f7290295794b5dec470867c7baa4a14182b9732603fd0caf2a5bf1dc3ccabf3 \\\n    --hash=sha256:4ff380aabd7a74a42a760ee76c68826a8f417ceb6ea415bd574a035a111fd133\n```\n\n----------------------------------------\n\nTITLE: Testing Generated Pandas Code for String Replacement in Python\nDESCRIPTION: Validates a generated code snippet that replaces spaces with underscores in a Pandas DataFrame column. The code creates a sample DataFrame with spaces, applies the string replacement operation, and prints both the original and modified DataFrames for comparison.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({\"col\": [\"abc def ghi\", \" 12 3 456\", \"     \"]})\nprint(\"Before\\n\", df)\n\ndf[\"col\"] = df[\"col\"].str.replace(\" \", \"_\")\nprint(\"After\\n\", df)\n```\n\n----------------------------------------\n\nTITLE: Specifying Scikit-learn Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the scikit-learn package dependency with version 1.3.2 and multiple SHA-256 hashes for verification. Comments indicate this package is required by multiple dependencies including lightgbm and is constrained by requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_36\n\nLANGUAGE: pip\nCODE:\n```\nscikit-learn==1.3.2 \\\n    --hash=sha256:0402638c9a7c219ee52c94cbebc8fcb5eb9fe9c773717965c1f4185588ad3107 \\\n    --hash=sha256:0ee107923a623b9f517754ea2f69ea3b62fc898a3641766cb7deb2f2ce450161 \\\n    --hash=sha256:1215e5e58e9880b554b01187b8c9390bf4dc4692eedeaf542d3273f4785e342c \\\n    --hash=sha256:15e1e94cc23d04d39da797ee34236ce2375ddea158b10bee3c343647d615581d \\\n    --hash=sha256:18424efee518a1cde7b0b53a422cde2f6625197de6af36da0b57ec502f126157 \\\n    --hash=sha256:1d08ada33e955c54355d909b9c06a4789a729977f165b8bae6f225ff0a60ec4a \\\n    --hash=sha256:3271552a5eb16f208a6f7f617b8cc6d1f137b52c8a1ef8edf547db0259b2c9fb \\\n    --hash=sha256:35a22e8015048c628ad099da9df5ab3004cdbf81edc75b396fd0cff8699ac58c \\\n    --hash=sha256:535805c2a01ccb40ca4ab7d081d771aea67e535153e35a1fd99418fcedd1648a \\\n    --hash=sha256:5b2de18d86f630d68fe1f87af690d451388bb186480afc719e5f770590c2ef6c \\\n    --hash=sha256:61a6efd384258789aa89415a410dcdb39a50e19d3d8410bd29be365bcdd512d5 \\\n    --hash=sha256:64381066f8aa63c2710e6b56edc9f0894cc7bf59bd71b8ce5613a4559b6145e0 \\\n    --hash=sha256:67f37d708f042a9b8d59551cf94d30431e01374e00dc2645fa186059c6c5d78b \\\n    --hash=sha256:6c43290337f7a4b969d207e620658372ba3c1ffb611f8bc2b6f031dc5c6d1d03 \\\n    --hash=sha256:6fb6bc98f234fda43163ddbe36df8bcde1d13ee176c6dc9b92bb7d3fc842eb66 \\\n    --hash=sha256:763f0ae4b79b0ff9cca0bf3716bcc9915bdacff3cebea15ec79652d1cc4fa5c9 \\\n    --hash=sha256:785a2213086b7b1abf037aeadbbd6d67159feb3e30263434139c98425e3dcfcf \\\n    --hash=sha256:8db94cd8a2e038b37a80a04df8783e09caac77cbe052146432e67800e430c028 \\\n    --hash=sha256:a19f90f95ba93c1a7f7924906d0576a84da7f3b2282ac3bfb7a08a32801add93 \\\n    --hash=sha256:a2f54c76accc15a34bfb9066e6c7a56c1e7235dda5762b990792330b52ccfb05 \\\n    --hash=sha256:b8692e395a03a60cd927125eef3a8e3424d86dde9b2370d544f0ea35f78a8073 \\\n    --hash=sha256:cb06f8dce3f5ddc5dee1715a9b9f19f20d295bed8e3cd4fa51e1d050347de525 \\\n    --hash=sha256:dc9002fc200bed597d5d34e90c752b74df516d592db162f756cc52836b38fe0e \\\n    --hash=sha256:e326c0eb5cf4d6ba40f93776a20e9a7a69524c4db0757e7ce24ba222471ee8a1 \\\n    --hash=sha256:ed932ea780517b00dae7431e031faae6b49b20eb6950918eb83bd043237950e0 \\\n    --hash=sha256:fc4144a5004a676d5022b798d9e573b05139e77f271253a4703eed295bde0433\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n    #   lightgbm\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Runtime - Java\nDESCRIPTION: This Java snippet shows how to initialize the Ray runtime by calling `Ray.init()`. Similar to Python, this is the first step before any other Ray API calls can be made.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/starting-ray.rst#2025-04-12_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport io.ray.api.Ray;\n\npublic class MyRayApp {\n\n  public static void main(String[] args) {\n    // Other Ray APIs will not work until `Ray.init()` is called.\n    Ray.init();\n    ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Platform-Specific Python Package Dependencies\nDESCRIPTION: This snippet demonstrates how to specify a Python package dependency (memray) that is only applicable to non-Windows platforms. It includes multiple hash values for different versions or builds of the package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_18\n\nLANGUAGE: Text\nCODE:\n```\nmemray==1.10.0 ; sys_platform != 'win32' \\\n    --hash=sha256:0a21745fb516b7a6efcd40aa7487c59e9313fcfc782d0193fcfcf00b48426874 \\\n    --hash=sha256:22f2a47871c172a0539bd72737bb6b294fc10c510464066b825d90fcd3bb4916 \\\n    --hash=sha256:23e8c402625cfb32d0e9edb5ec0945f3e5e54bc6b0c5699f6284302082b80bd4 \\\n    --hash=sha256:2ce59ef485db3634de98b3a026d2450fc0a875e3a58a9ea85f7a89098841defe \\\n    --hash=sha256:322ed0b69014a0969b777768d461a785203f81f9864386b666b5b26645d9c294 \\\n    --hash=sha256:38322e052b882790993412f1840517a51818aa55c47037f69915b2007f2c4cee \\\n    --hash=sha256:38393c86ce6d0a08e6ec0eb1401d49803b7c0c950c2565386751cdc81568cba8 \\\n    --hash=sha256:391aac6c9f744528d3186bc82d708a1acc83525778f804045d7c96f860f8ec98 \\\n    --hash=sha256:3a8bb7fbd8303c4f0017ba7faef6b88f904cda2931ed667cbf3b98f024b3bc44 \\\n    --hash=sha256:3c401c57f49c4c5f1fecaee1e746f537cdc6680da05fb963dc143bd08ee109bf \\\n    --hash=sha256:4eba29179772b4a2e440a065b320b03bc2e73fe2648bdf7936aa3b9a086fab4a \\\n    --hash=sha256:53a8f66af18b1f3bcf5c9f3c95ae4134dd675903a38f9d0e6341b7bca01b63d0 \\\n    --hash=sha256:566602b2143e06b3d592901d98c52ce4599e71aa2555146eeb5cec03506f9498 \\\n    --hash=sha256:663d463e89a64bae4a6b2f8c837d11a3d094834442d536a4165e1d31899a3500 \\\n    --hash=sha256:68bd8df023c8a32f44c11d997e5c536837e27c0955daf557d3a377edd55a1dd3 \\\n    --hash=sha256:6937d7ef67d18ccc01c3250cdf3b4ef1445b859ee8756f09e3d11bd3ff0c7d67 \\\n    --hash=sha256:6b311e91203be71e1a0ce5e4f978137765bcb1045f3bf5646129c83c5b96ab3c \\\n    --hash=sha256:6fd13ef666c7fced9768d1cfabf71dc6dfa6724935a8dff463495ac2dc5e13a4 \\\n    --hash=sha256:8196c684f1be8fe423e5cdd2356d4255a2cb482a1f3e89612b70d2a2862cf5bb \\\n    --hash=sha256:843a688877691746f9d1835cfa8a65139948471bdd78720435808d20bc30a1cc \\\n    --hash=sha256:85c32d6613d81b075f740e398c4d653e0803cd48e82c33dcd584c109d6782666 \\\n    --hash=sha256:898acd60f57a10dc5aaf1fd64aa2f821f0420114f3f60c3058083788603f173a \\\n    --hash=sha256:8d56f37a34125684746c13d24bd7a3fb17549b0bb355eb50969eb11e05e3ba62 \\\n    --hash=sha256:92c372cb262eddd23049f945ca9527f0e4cc7c40a070aade1802d066f680885b \\\n    --hash=sha256:95e563d9c976e429ad597ad2720d95cebbe8bac891a3082465439143e2740772 \\\n    --hash=sha256:9627184c926252c8f719c301f1fefe970f0d033c643a6448b93fed2889d1ea94 \\\n    --hash=sha256:a9e985fb7646b0475c303919d19211d2aa54e5a9e2cd2a102472299be5dbebd3 \\\n    --hash=sha256:b681519357d94f5f0857fbc6029e7c44d3f41436109e955a14fd312d8317bc35 \\\n    --hash=sha256:b75040f28e8678d0e9c4907d55c95cf26db8ef5adc9941a228f1b280a9efd9c0 \\\n    --hash=sha256:c3a14960838d89a91747885897d34134afb65883cc3b0ed7ff30fe1af00f9fe6 \\\n    --hash=sha256:c7aeb47174c42e99740a8e2b3b6fe0932c95d987258d48a746974ead19176c26 \\\n    --hash=sha256:ce22a887a585ef5020896de89ffc793e531b65ccc81fbafcc7886010c2c562b3 \\\n    --hash=sha256:cf6d683c4f8d25c6ad06ae18715f218983c5eb86803953615e902d632fdf6ec1 \\\n    --hash=sha256:e356af93e3b031c83957e9ac1a653f5aaba5df1e357dd17142f5ed19bb3dc660 \\\n    --hash=sha256:f16c5c8730b616613dc8bafe32649ca6bd7252606251eb00148582011758d0b5\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing a RayService Sample\nDESCRIPTION: Applies a sample RayService YAML configuration from the KubeRay GitHub repository. This creates a RayService that includes a RayCluster and Ray Serve applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-service.sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Analyzing Ray Performance Metrics in JSON\nDESCRIPTION: This JSON object contains various performance metrics for Ray operations, including timings for arguments, returns, gets, queued operations, and large object handling. It also includes metadata about the test environment and Ray version used.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/scalability/single_node.txt#2025-04-12_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"args_time\": 17.256289814000013,\n  \"num_args\": 10000,\n  \"returns_time\": 5.854934190999984,\n  \"num_returns\": 3000,\n  \"get_time\": 25.88724605799996,\n  \"queued_time\": 140.99555420300004,\n  \"num_queued\": 1000000,\n  \"large_object_time\": 294.249499343,\n  \"large_object_size\": 107374182400,\n  \"success\": \"1\",\n  \"_runtime\": 528.4356288909912,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_ELgpggWSHiqhksawLcz4urEP\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Task State Transition Diagram in ASCII\nDESCRIPTION: This ASCII diagram illustrates the transitions between different task states in Ray. It shows how tasks move between states like Placeable, Waiting, Ready, Running, and others based on various conditions such as resource availability, actor creation, and worker status.\nSOURCE: https://github.com/ray-project/ray/blob/master/src/ray/design_docs/task_states.rst#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n                                    ---------------------------------\n                                   |                                 |\n                                   |     forward                     | forward\n                                   |----------------                 |\n  node with                  ------|                |   arguments    |\n  resources          forward|      |   resource     |     local      |   actor/worker\n  joins                     |      v  available     |    -------->   |    available\n    ---------------------- Placeable ----------> Waiting           Ready ---------> Running\n  |                       | |  ^                    ^    <--------   ^               |   ^\n  |             |---------  |  |                    |    local arg   |               |   |\n  |             |           |  |                    |     evicted    |        worker |   | worker\n  |             |     actor |  |                    |                |       blocked |   | unblocked\n  |   resources |   created |  | actor              | ---------------                |   |\n  |  infeasible |           |  | created            | actor                          |   |\n  |             |           |  | (remote)           | created                        v   |\n  |             |           v  |                    | (local)                              Blocked\n  |             |     WaitForActorCreation----------\n  |             v\n   ----Infeasible\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows the format used to specify Python package dependencies with their version numbers and SHA256 hashes. It includes multiple hash values for different versions or builds of each package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\nmsgspec==0.19.0 \\\n    --hash=sha256:00e87ecfa9795ee5214861eab8326b0e75475c2e68a384002aa135ea2a27d909 \\\n    --hash=sha256:047cfa8675eb3bad68722cfe95c60e7afabf84d1bd8938979dd2b92e9e4a9551 \\\n    --hash=sha256:0553bbc77662e5708fe66aa75e7bd3e4b0f209709c48b299afd791d711a93c36 \\\n    # ... more hashes ...\n    # via vllm\nmultidict==6.0.5 \\\n    --hash=sha256:01265f5e40f5a17f8241d52656ed27192be03bfa8764d88e8220141d1e4b3556 \\\n    --hash=sha256:0275e35209c27a3f7951e1ce7aaf93ce0d163b28948444bec61dd7badc6d3f8c \\\n    --hash=sha256:04bde7a7b3de05732a4eb39c94574db1ec99abb56162d6c520ad26f83267de29 \\\n    # ... more hashes ...\n```\n\n----------------------------------------\n\nTITLE: Allocating GPU Resources Based on Config in Ray Tune\nDESCRIPTION: This snippet demonstrates how to allocate GPU resources to Ray Tune trials based on a setting in the parameter space using a lambda function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrainable_with_conditional_gpus = tune.with_resources(\n    trainable,\n    lambda config: {\"GPU\": 0.2 if config[\"use_gpu\"] else 0}\n)\n```\n\n----------------------------------------\n\nTITLE: Defining pyzmq Package Requirements with Hash Verification\nDESCRIPTION: Specifies pyzmq package version 26.0.3 with SHA256 hash verification for secure package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_42\n\nLANGUAGE: text\nCODE:\n```\npyzmq==26.0.3 \\\n    --hash=sha256:01fbfbeb8249a68d257f601deb50c70c929dc2dfe683b754659569e502fbd3aa \\\n    --hash=sha256:0270b49b6847f0d106d64b5086e9ad5dc8a902413b5dbbb15d12b60f9c1747a4\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: List of Python package dependencies with exact versions and SHA256 hashes for security verification. Includes package relationships and source information via comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_31\n\nLANGUAGE: text\nCODE:\n```\nreferencing==0.36.2 \\\n    --hash=sha256:df2e89862cd09deabbdba16944cc3f10feb6b3e6f18e902f7cc25609a34775aa \\\n    --hash=sha256:e8699adbbf8b5c7de96d8ffa0eb5c158b3beafce084968e2ea8bb08c6794dcd0\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   jsonschema\n    #   jsonschema-specifications\n```\n\nLANGUAGE: text\nCODE:\n```\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n```\n\nLANGUAGE: text\nCODE:\n```\nrequests-oauthlib==2.0.0 \\\n    --hash=sha256:7dd8a5c40426b779b0868c404bdef9768deccf22749cde15852df527e6269b36 \\\n    --hash=sha256:b3dffaebd884d8cd778494369603a9e7b58d29111bf6b41bdc2dcd87203af4e9\n```\n\nLANGUAGE: text\nCODE:\n```\nretry-decorator==1.1.1 \\\n    --hash=sha256:e1e8ad02e518fe11073f2ea7d80b6b8be19daa27a60a1838aff7c731ddcf2ebe\n```\n\nLANGUAGE: text\nCODE:\n```\nrich==13.3.2 \\\n    --hash=sha256:91954fe80cfb7985727a467ca98a7618e5dd15178cc2da10f553b36a93859001 \\\n    --hash=sha256:a104f37270bf677148d8acb07d33be1569eeee87e2d1beb286a4e9113caf6f2f\n```\n\nLANGUAGE: text\nCODE:\n```\nroundrobin==0.0.4 \\\n    --hash=sha256:7e9d19a5bd6123d99993fb935fa86d25c88bb2096e493885f61737ed0f5e9abd\n```\n\nLANGUAGE: text\nCODE:\n```\nrpds-py==0.22.3 \\\n    --hash=sha256:009de23c9c9ee54bf11303a966edf4d9087cd43a6003672e6aa7def643d06518 \\\n    # [Multiple additional hashes omitted for brevity]\n```\n\n----------------------------------------\n\nTITLE: Alternative ResNet50 Autoscaling Configuration (YAML)\nDESCRIPTION: This YAML snippet provides an alternative configuration for deploying a ResNet50 workload with autoscaling in Ray Serve. It specifies the application and its deployment, with automatic scaling based on traffic.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/autoscaling-guide.md#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napplications:\n  - name: default\n    import_path: resnet:app\n    deployments:\n    - name: Model\n      num_replicas: auto\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Hugging Face and Ray\nDESCRIPTION: This code snippet installs the necessary Python packages for working with Hugging Face Transformers, datasets, PyTorch, and MLflow.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#! pip install \"datasets\" \"transformers>=4.19.0\" \"torch>=1.10.0\" \"mlflow\"\n```\n\n----------------------------------------\n\nTITLE: Ray Config JSON Structure\nDESCRIPTION: JSON configuration structure in ~/.ray/config.json for disabling usage statistics collection\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/usage-stats.rst#2025-04-12_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"usage_stats\": true}\n```\n\n----------------------------------------\n\nTITLE: Ray Project Configuration Table\nDESCRIPTION: Markdown table defining the project specifications including compute requirements, runtime estimates, and cluster environment details. The template uses the anyscale/ray-ml:latest-py39-gpu image and requires 4 nodes with NVIDIA T4 GPUs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/01_batch_inference/README.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Template Specification | Description |\n| ---------------------- | ----------- |\n| Summary | This template walks through GPU batch inference on an image dataset. |\n| Time to Run | Less than 5 minutes to compute predictions on the dataset. |\n| Minimum Compute Requirements | No hard requirements. The default is 4 nodes, each with 1 NVIDIA T4 GPU. |\n| Cluster Environment | This template uses the latest Anyscale-provided Ray ML image using Python 3.9: [`anyscale/ray-ml:latest-py39-gpu`](https://docs.anyscale.com/reference/base-images/overview?utm_source=ray_docs&utm_medium=docs&utm_campaign=01_batch_inference). If you want to change to a different cluster environment, make sure that it's based on this image.|\n```\n\n----------------------------------------\n\nTITLE: Installing ptyprocess with Pinned Version and Hashes\nDESCRIPTION: Specifies ptyprocess package with version 0.7.0 and SHA256 hashes for verification. Comments indicate this is required by the terminado package.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_21\n\nLANGUAGE: pip\nCODE:\n```\nptyprocess==0.7.0 \\\n    --hash=sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35 \\\n    --hash=sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   terminado\n```\n\n----------------------------------------\n\nTITLE: Creating GKE Cluster for KubeRay Benchmark\nDESCRIPTION: Creates a Google Kubernetes Engine (GKE) cluster named 'kuberay-benchmark-cluster' with autoscaling enabled, up to 16 nodes of type e2-highcpu-16, each with 16 CPUs and 16 GB of memory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container clusters create kuberay-benchmark-cluster \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 16 --enable-autoscaling \\\n    --zone=us-west1-b --machine-type e2-highcpu-16\n```\n\n----------------------------------------\n\nTITLE: HeavyLoad Deployment Configuration - Autoscaling (Attempt 2)\nDESCRIPTION: This YAML configuration defines a Ray Serve deployment named 'HeavyLoad' with autoscaling enabled for the second attempt. It sets the target number of ongoing requests to 1, minimum replicas to 0, initial replicas to 0, and a maximum replicas to 200. It also defines the upscale and downscale delays, as well as the upscaling and downscaling factors.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/advanced-autoscaling.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n\"- name: HeavyLoad\n  max_ongoing_requests: 3\n  autoscaling_config:\n    target_ongoing_requests: 1\n    min_replicas: 0\n    initial_replicas: 0\n    max_replicas: 200\n    upscale_delay_s: 3\n    downscale_delay_s: 60\n    upscaling_factor: 0.3\n    downscaling_factor: 0.3\n    metrics_interval_s: 2\n    look_back_period_s: 10\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Regex Package Dependency with Extensive Hash Verification\nDESCRIPTION: A requirements entry for the regex package (version 2024.11.6) with an extensive list of SHA256 hashes for verification. The entry includes references to parent packages that require this dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_33\n\nLANGUAGE: plaintext\nCODE:\n```\nregex==2024.11.6 \\\n    --hash=sha256:02a02d2bb04fec86ad61f3ea7f49c015a0681bf76abb9857f945d26159d2968c \\\n    --hash=sha256:02e28184be537f0e75c1f9b2f8847dc51e08e6e171c6bde130b2687e0c33cf60 \\\n    --hash=sha256:040df6fe1a5504eb0f04f048e6d09cd7c7110fef851d7c567a6b6e09942feb7d \\\n    --hash=sha256:068376da5a7e4da51968ce4c122a7cd31afaaec4fccc7856c92f63876e57b51d \\\n    --hash=sha256:06eb1be98df10e81ebaded73fcd51989dcf534e3c753466e4b60c4697a003b67 \\\n    --hash=sha256:072623554418a9911446278f16ecb398fb3b540147a7828c06e2011fa531e773 \\\n    --hash=sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0 \\\n    --hash=sha256:08986dce1339bc932923e7d1232ce9881499a0e02925f7402fb7c982515419ef \\\n    --hash=sha256:0a86e7eeca091c09e021db8eb72d54751e527fa47b8d5787caf96d9831bd02ad \\\n    --hash=sha256:0c32f75920cf99fe6b6c539c399a4a128452eaf1af27f39bce8909c9a3fd8cbe \\\n    --hash=sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3 \\\n    --hash=sha256:1062b39a0a2b75a9c694f7a08e7183a80c63c0d62b301418ffd9c35f55aaa114 \\\n    --hash=sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4 \\\n    --hash=sha256:149f5008d286636e48cd0b1dd65018548944e495b0265b45e1bffecce1ef7f39 \\\n    --hash=sha256:164d8b7b3b4bcb2068b97428060b2a53be050085ef94eca7f240e7947f1b080e \\\n    --hash=sha256:167ed4852351d8a750da48712c3930b031f6efdaa0f22fa1933716bfcd6bf4a3 \\\n    --hash=sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7 \\\n    --hash=sha256:202eb32e89f60fc147a41e55cb086db2a3f8cb82f9a9a88440dcfc5d37faae8d \\\n    --hash=sha256:220902c3c5cc6af55d4fe19ead504de80eb91f786dc102fbd74894b1551f095e \\\n    --hash=sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a \\\n    --hash=sha256:2c89a8cc122b25ce6945f0423dc1352cb9593c68abd19223eebbd4e56612c5b7 \\\n    --hash=sha256:2d548dafee61f06ebdb584080621f3e0c23fff312f0de1afc776e2a2ba99a74f \\\n    --hash=sha256:2e34b51b650b23ed3354b5a07aab37034d9f923db2a40519139af34f485f77d0 \\\n    --hash=sha256:32f9a4c643baad4efa81d549c2aadefaeba12249b2adc5af541759237eee1c54 \\\n    --hash=sha256:3a51ccc315653ba012774efca4f23d1d2a8a8f278a6072e29c7147eee7da446b \\\n    --hash=sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c \\\n    --hash=sha256:40291b1b89ca6ad8d3f2b82782cc33807f1406cf68c8d440861da6304d8ffbbd \\\n    --hash=sha256:41758407fc32d5c3c5de163888068cfee69cb4c2be844e7ac517a52770f9af57 \\\n    --hash=sha256:4181b814e56078e9b00427ca358ec44333765f5ca1b45597ec7446d3a1ef6e34 \\\n    --hash=sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d \\\n    --hash=sha256:50153825ee016b91549962f970d6a4442fa106832e14c918acd1c8e479916c4f \\\n    --hash=sha256:5056b185ca113c88e18223183aa1a50e66507769c9640a6ff75859619d73957b \\\n    --hash=sha256:5071b2093e793357c9d8b2929dfc13ac5f0a6c650559503bb81189d0a3814519 \\\n    --hash=sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4 \\\n    --hash=sha256:52fb28f528778f184f870b7cf8f225f5eef0a8f6e3778529bdd40c7b3920796a \\\n    --hash=sha256:5478c6962ad548b54a591778e93cd7c456a7a29f8eca9c49e4f9a806dcc5d638 \\\n    --hash=sha256:5670bce7b200273eee1840ef307bfa07cda90b38ae56e9a6ebcc9f50da9c469b \\\n    --hash=sha256:5704e174f8ccab2026bd2f1ab6c510345ae8eac818b613d7d73e785f1310f839 \\\n    --hash=sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07 \\\n    --hash=sha256:5e7e351589da0850c125f1600a4c4ba3c722efefe16b297de54300f08d734fbf \\\n    --hash=sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff \\\n    --hash=sha256:658f90550f38270639e83ce492f27d2c8d2cd63805c65a13a14d36ca126753f0 \\\n    --hash=sha256:684d7a212682996d21ca12ef3c17353c021fe9de6049e19ac8481ec35574a70f \\\n    --hash=sha256:69ab78f848845569401469da20df3e081e6b5a11cb086de3eed1d48f5ed57c95 \\\n    --hash=sha256:6f44ec28b1f858c98d3036ad5d7d0bfc568bdd7a74f9c24e25f41ef1ebfd81a4 \\\n    --hash=sha256:70b7fa6606c2881c1db9479b0eaa11ed5dfa11c8d60a474ff0e095099f39d98e \\\n    --hash=sha256:764e71f22ab3b305e7f4c21f1a97e1526a25ebdd22513e251cf376760213da13 \\\n    --hash=sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519 \\\n    --hash=sha256:805e6b60c54bf766b251e94526ebad60b7de0c70f70a4e6210ee2891acb70bf2 \\\n    --hash=sha256:8447d2d39b5abe381419319f942de20b7ecd60ce86f16a23b0698f22e1b70008 \\\n    --hash=sha256:86fddba590aad9208e2fa8b43b4c098bb0ec74f15718bb6a704e3c63e2cef3e9 \\\n    --hash=sha256:89d75e7293d2b3e674db7d4d9b1bee7f8f3d1609428e293771d1a962617150cc \\\n    --hash=sha256:93c0b12d3d3bc25af4ebbf38f9ee780a487e8bf6954c115b9f015822d3bb8e48 \\\n    --hash=sha256:94d87b689cdd831934fa3ce16cc15cd65748e6d689f5d2b8f4f4df2065c9fa20 \\\n    --hash=sha256:9714398225f299aa85267fd222f7142fcb5c769e73d7733344efc46f2ef5cf89 \\\n    --hash=sha256:982e6d21414e78e1f51cf595d7f321dcd14de1f2881c5dc6a6e23bbbbd68435e \\\n    --hash=sha256:997d6a487ff00807ba810e0f8332c18b4eb8d29463cfb7c820dc4b6e7562d0cf \\\n    --hash=sha256:a03e02f48cd1abbd9f3b7e3586d97c8f7a9721c436f51a5245b3b9483044480b \\\n    --hash=sha256:a36fdf2af13c2b14738f6e973aba563623cb77d753bbbd8d414d18bfaa3105dd \\\n    --hash=sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84 \\\n    --hash=sha256:a7c2155f790e2fb448faed6dd241386719802296ec588a8b9051c1f5c481bc29 \\\n    --hash=sha256:a93c194e2df18f7d264092dc8539b8ffb86b45b899ab976aa15d48214138e81b \\\n    --hash=sha256:abfa5080c374a76a251ba60683242bc17eeb2c9818d0d30117b4486be10c59d3 \\\n    --hash=sha256:ac10f2c4184420d881a3475fb2c6f4d95d53a8d50209a2500723d831036f7c45 \\\n    --hash=sha256:ad182d02e40de7459b73155deb8996bbd8e96852267879396fb274e8700190e3 \\\n    --hash=sha256:b2837718570f95dd41675328e111345f9b7095d821bac435aac173ac80b19983 \\\n    --hash=sha256:b489578720afb782f6ccf2840920f3a32e31ba28a4b162e13900c3e6bd3f930e \\\n    --hash=sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7 \\\n    --hash=sha256:b85c2530be953a890eaffde05485238f07029600e8f098cdf1848d414a8b45e4 \\\n    --hash=sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e \\\n    --hash=sha256:ba9b72e5643641b7d41fa1f6d5abda2c9a263ae835b917348fc3c928182ad467 \\\n    --hash=sha256:bb26437975da7dc36b7efad18aa9dd4ea569d2357ae6b783bf1118dabd9ea577 \\\n    --hash=sha256:bb8f74f2f10dbf13a0be8de623ba4f9491faf58c24064f32b65679b021ed0001 \\\n    --hash=sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0 \\\n    --hash=sha256:bec9931dfb61ddd8ef2ebc05646293812cb6b16b60cf7c9511a832b6f1854b55 \\\n    --hash=sha256:c36f9b6f5f8649bb251a5f3f66564438977b7ef8386a52460ae77e6070d309d9 \\\n    --hash=sha256:cdf58d0e516ee426a48f7b2c03a332a4114420716d55769ff7108c37a09951bf \\\n    --hash=sha256:d1cee317bfc014c2419a76bcc87f071405e3966da434e03e13beb45f8aced1a6 \\\n    --hash=sha256:d22326fcdef5e08c154280b71163ced384b428343ae16a5ab2b3354aed12436e \\\n    --hash=sha256:d3660c82f209655a06b587d55e723f0b813d3a7db2e32e5e7dc64ac2a9e86fde \\\n    --hash=sha256:da8f5fc57d1933de22a9e23eec290a0d8a5927a5370d24bda9a6abe50683fe62 \\\n    --hash=sha256:df951c5f4a1b1910f1a99ff42c473ff60f8225baa1cdd3539fe2819d9543e9df \\\n    --hash=sha256:e5364a4502efca094731680e80009632ad6624084aff9a23ce8c8c6820de3e51 \\\n    --hash=sha256:ea1bfda7162605f6e8178223576856b3d791109f15ea99a9f95c16a7636fb5 \\\n    --hash=sha256:f02f93b92358ee3f78660e43b4b0091229260c5d5c408d17d60bf26b6c900e86 \\\n    --hash=sha256:f056bf21105c2515c32372bbc057f43eb02aae2fda61052e2f7622c801f0b4e2 \\\n    --hash=sha256:f1ac758ef6aebfc8943560194e9fd0fa18bcb34d89fd8bd2af18183afd8da3a2 \\\n    --hash=sha256:f2a19f302cd1ce5dd01a9099aaa19cae6173306d1302a43b627f62e21cf18ac0 \\\n    --hash=sha256:f654882311409afb1d780b940234208a252322c24a93b442ca714d119e68086c \\\n    --hash=sha256:f65557897fc977a44ab205ea871b690adaef6b9da6afda4790a2484b04293a5f \\\n    --hash=sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6 \\\n    --hash=sha256:fdabbfc59f2c6edba2a6622c647b716e34e8e3867e0ab975412c5c2f79b82da2 \\\n    --hash=sha256:fdd6028445d2460f33136c55eeb1f601ab06d74cb3347132e1c24250187500d9 \\\n    --hash=sha256:ff590880083d60acc0433f9c3f713c51f7ac6ebb9adf889c79a261ecf541aa91\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   tiktoken\n    #   transformers\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Serve Deployment for Llama2-7b Model on HPU\nDESCRIPTION: This bash command starts the Ray Serve deployment for the Llama2-7b model on an HPU. It runs the Python script that defines the model deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nserve run intel_gaudi_inference_serve:entrypoint\n```\n\n----------------------------------------\n\nTITLE: Setting the number of samples\nDESCRIPTION: Sets the number of hyperparameter combinations that will be tried out during the Tune run.  This determines the extent of the hyperparameter search.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"num_samples = 1000\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Package\nDESCRIPTION: Command to install Ray with default components using pip.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/quickstart.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install \"ray[default]\"\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with SHA256 Hashes\nDESCRIPTION: Requirements file listing Python package dependencies with their exact versions and corresponding SHA256 hash values for package verification. Includes dependencies for ML frameworks, API servers, and utility packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_38\n\nLANGUAGE: plaintext\nCODE:\n```\ntqdm==4.64.1 \\\n    --hash=sha256:5f4f682a004951c1b450bc753c710e9280c5746ce6ffedee253ddbcbf54cf1e4 \\\n    --hash=sha256:6fee160d6ffcd1b1c68c65f14c829c22832bc401726335ce92c52d395944a6a1\ntransformers==4.49.0 \\\n    --hash=sha256:6b4fded1c5fee04d384b1014495b4235a2b53c87503d7d592423c06128cbbe03 \\\n    --hash=sha256:7e40e640b5b8dc3f48743f5f5adbdce3660c82baafbd3afdfc04143cdbd2089e\n```\n\n----------------------------------------\n\nTITLE: Starting Ray with Metrics Export\nDESCRIPTION: Command to start Ray and set up the metrics export port for monitoring.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/monitoring.md#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nray start --head --metrics-export-port=8080\n```\n\n----------------------------------------\n\nTITLE: Installing PyYAML 6.0.1 with Hash Verification\nDESCRIPTION: Package specification for PyYAML 6.0.1 with SHA-256 hashes for secure installation. The package is required by multiple Ray project components including huggingface-hub, ray, transformers, and vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_42\n\nLANGUAGE: plaintext\nCODE:\n```\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \\\n    --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \\\n    --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \\\n    --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \\\n    --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \\\n    --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \\\n    --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \\\n    --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \\\n    --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \\\n    --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \\\n    --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \\\n    --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \\\n    --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \\\n    --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \\\n    --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \\\n    --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \\\n    --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \\\n    --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \\\n    --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \\\n    --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \\\n    --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \\\n    --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \\\n    --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \\\n    --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \\\n    --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \\\n    --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \\\n    --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \\\n    --hash=sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef \\\n    --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \\\n    --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \\\n    --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \\\n    --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \\\n    --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \\\n    --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \\\n    --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \\\n    --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \\\n    --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \\\n    --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \\\n    --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \\\n    --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \\\n    --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \\\n    --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \\\n    --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \\\n    --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \\\n    --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \\\n    --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \\\n    --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \\\n    --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \\\n    --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   gguf\n    #   huggingface-hub\n    #   jupyter-events\n    #   jupytext\n    #   lm-format-enforcer\n    #   ray\n    #   transformers\n    #   uvicorn\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Specifying ypy-websocket Package Dependency\nDESCRIPTION: This snippet shows the dependency specification for the ypy-websocket package. It includes the package version and SHA256 hashes for verification. The comment indicates that this package is required by other dependencies in the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_20\n\nLANGUAGE: Text\nCODE:\n```\nypy-websocket==0.8.4 \\\n    --hash=sha256:43a001473f5c8abcf182f603049cf305cbc855ad8deaa9dfa0f3b5a7cea9d0ff \\\n    --hash=sha256:b1ba0dfcc9762f0ca168d2378062d3ca1299d39076b0f145d961359121042be5\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jupyter-server-ydoc\n```\n\n----------------------------------------\n\nTITLE: Ray Task Execution Metrics Output\nDESCRIPTION: Console output showing the successful execution of 1000 Ray tasks, including total execution time and tasks per second rate.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.6.0/benchmarks/many_nodes.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nSuccess! Started 1000 tasks in 652.3915314674377s. (2.837752643588723 tasks/s)\n```\n\n----------------------------------------\n\nTITLE: Package Dependency Specification with SHA-256 Hashes in Requirements File\nDESCRIPTION: This snippet shows package dependencies with their version constraints and SHA-256 hashes for security verification. The file uses pip's hash-checking mode to ensure package integrity during installation. Comments indicate the source of requirements (via -c and -r directives).\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n    --hash=sha256:a7d568eb07473d9bc6fb413a4d3248265212c537b80d494ab884cc5316589110 \\\n    --hash=sha256:ad57917650af59c989b62184fc4604d6c5066fc030ced4c6e07a596000f1ab86 \\\n    --hash=sha256:ad83e4c78379cc3e22b760e9874bc57f91a9cfb85107ccba1c6442bc1a2e2a1c \\\n    --hash=sha256:b04c44ad7cde9c21ad426bdfa675ba7039db82a6961c99690f9d2ff2f034c892 \\\n    --hash=sha256:b917b73d810bcdbcd1461978ba55038dcf2bbc3b56704b0082d2f9b0d5edc7ad \\\n    --hash=sha256:c04a27ba3cbc7a9e34c77f402bd3a83442a2c7acd3897d2539b1a3321ed28a6a \\\n    --hash=sha256:c59c6ea67ab927b2ab958c7b01a6b17c9cad882e7a1da51b9c35fbc9874ff46a \\\n    --hash=sha256:c74d81a00972cbe65e27e99838b44ed5e04bced971e5bfa01c27a4bd17138442 \\\n    --hash=sha256:ca03d8d5b35a26e0d3eb8c7121de3e37a59042735029eabcf1c4b15343f82cdd \\\n    --hash=sha256:cea0fe7053e36a4809e5bf95989552f52c98bbc94dca9062fb5b8c976daa0f32 \\\n    --hash=sha256:d27116037f97a02f1a123ca82008ee993c28afe8590e047a6cd86aca33653cca \\\n    --hash=sha256:d82fa5bb0661a7a508e62730d4d9045f53d4ab6a9211b560a014f1d58a8337cb \\\n    --hash=sha256:dce1deda03c6dbe0f5ae6e3e0f8671caead64075fd19a61b1700d42a88af97c8 \\\n    --hash=sha256:dd9bc7e5599f5970fff1f9aa551639336a76d1bb1fb00f0b87704049df8ba035 \\\n    --hash=sha256:df19ab6ab3884a237388c7720b1fe617dd4893305f62383d0f96fc7980dfdf7c \\\n    --hash=sha256:e14f4d57e004fa5a6100ea3aeb9574bee6f95965a96a382154fa40aee1fdeb5e \\\n    --hash=sha256:e6e16d57b8103fee9fdecb38e908d9ceb70d2196bb932dba64bf7b570f44c0b9 \\\n    --hash=sha256:ed14214fcc1416e0dc63be4c88aad7f58e0f0cb2c22d578b861e8fc19d1b2d2f \\\n    --hash=sha256:ef1165f7f36edaae03fcf03f1ca3bdbf196a5255d656bfb17959ba0405a2c8ee \\\n    --hash=sha256:f1679f7f700f2aec3dbee4e357a2fdde53e2ec151dde4e0b52a9205fac273a90 \\\n    --hash=sha256:f524fd202472d041b9bddb4a51b5fff28767a9c69953dbcdeecc67ef65707c07 \\\n    --hash=sha256:f641a9bd24a309637cca6c119b8aabdfe6d41bab5ea630124ee9be7891e36ba1 \\\n    --hash=sha256:f9a070dbe10dac29c2f591a59300c37448e3c7a747b6ea18d4826b7c94a956bd \\\n    --hash=sha256:fac1b4248625acd65985378f6b34a00b73cfc9db5b8ccc73101744de2e3dfa66 \\\n    --hash=sha256:fddf16ed92dcb8ee34a12bd0757d5719d3c750a9dc813d82972477885b114339\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n```\n\n----------------------------------------\n\nTITLE: Testing Ray Serve Applications\nDESCRIPTION: Demonstrates how to send requests to the Ray Serve applications using curl. Includes examples for both the fruit stand and calculator applications defined in the RayService sample.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# Step 6.1: Run a curl Pod.\n# If you already have a curl Pod, you can use `kubectl exec -it <curl-pod> -- sh` to access the Pod.\nkubectl run curl --image=radial/busyboxplus:curl -i --tty\n\n# Step 6.2: Send a request to the fruit stand app.\ncurl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/fruit/ -d '[\"MANGO\", 2]'\n# [Expected output]: 6\n\n# Step 6.3: Send a request to the calculator app.\ncurl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/calc/ -d '[\"MUL\", 3]'\n# [Expected output]: \"15 pizzas please!\"\n```\n\n----------------------------------------\n\nTITLE: Ray Training Results Table\nDESCRIPTION: Tabular results showing performance metrics for each trial including status, iterations, time, rewards and episode statistics across different RL algorithms.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/rllib_tests/regression_tests_torch.txt#2025-04-12_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n+-------------------------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n| Trial name                                | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n|-------------------------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n| A2C_BreakoutNoFrameskip-v4_89dd2_00000    | TERMINATED |       |    350 |          3605.64 | 2574500 |   45.54  |              273     |                 8    |            2489.29 |\n| A2C_BreakoutNoFrameskip-v4_89dd2_00001    | TERMINATED |       |    349 |          3609.91 | 2531000 |   20.22  |               40     |                 9    |            2869.39 |\n| APEX_BreakoutNoFrameskip-v4_89dd2_00002   | TERMINATED |       |     79 |          3603.33 | 3973120 |   11.56  |               34     |                 2    |            1897.13 |\n| APEX_BreakoutNoFrameskip-v4_89dd2_00003   | TERMINATED |       |     81 |          3626.75 | 4075040 |   14.6   |               36     |                 4    |            2166.04 |\n| DQN_BreakoutNoFrameskip-v4_89dd2_00004    | TERMINATED |       |     20 |          3705.37 |  210000 |    9.89  |               27     |                 2    |            1613.49 |\n| DQN_BreakoutNoFrameskip-v4_89dd2_00005    | TERMINATED |       |     21 |          3680.85 |  220000 |   15.7   |              234     |                 3    |            2003.28 |\n| IMPALA_BreakoutNoFrameskip-v4_89dd2_00006 | TERMINATED |       |    356 |          3607.74 | 6490250 |  350.07  |              435     |                27    |            8428.66 |\n| IMPALA_BreakoutNoFrameskip-v4_89dd2_00007 | TERMINATED |       |    356 |          3604.6  | 6510000 |  333.94  |              435     |                60    |            8422.51 |\n| PPO_BreakoutNoFrameskip-v4_89dd2_00008    | TERMINATED |       |    580 |          3600.36 | 2900000 |   21.8   |               41     |                 9    |            3029.82 |\n| PPO_BreakoutNoFrameskip-v4_89dd2_00009    | TERMINATED |       |    578 |          3602.76 | 2890000 |   24.63  |               69     |                 7    |            2980.77 |\n| SAC_HalfCheetahBulletEnv-v0_89dd2_00010   | TERMINATED |       |     17 |          3695.49 |   26000 | -241.653 |              389.052 |             -1042.96 |            1000    |\n| SAC_HalfCheetahBulletEnv-v0_89dd2_00011   | TERMINATED |       |     36 |          3619.25 |   45000 | -607.206 |              367.676 |             -1331.25 |            1000    |\n+-------------------------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n```\n\n----------------------------------------\n\nTITLE: Specifying google-resumable-media Package with Hash Values\nDESCRIPTION: Requirement specification for google-resumable-media version 2.6.0 with SHA-256 hash values for verification. This package is used by google-cloud-storage and is referenced in the Ray project's compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_8\n\nLANGUAGE: text\nCODE:\n```\ngoogle-resumable-media==2.6.0 \\\n    --hash=sha256:972852f6c65f933e15a4a210c2b96930763b47197cdf4aa5f5bea435efb626e7 \\\n    --hash=sha256:fc03d344381970f79eebb632a3c18bb1828593a2dc5572b5f90115ef7d11e81b\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   google-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Ray in Python\nDESCRIPTION: This snippet gracefully shuts down the Ray runtime after completing the experiments to free up resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Version Pinning and Hashes\nDESCRIPTION: A pip requirements file that specifies exact versions and hash verification for Python package dependencies. Each package has pinned versions, SHA256 hashes for verification, and comments indicating which parts of the project require them.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_1\n\nLANGUAGE: pip\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cpu\n--find-links https://data.pyg.org/whl/torch-2.3.0+cpu.html\n\nabsl-py==1.4.0 \\\n    --hash=sha256:0d3fe606adfa4f7db64792dd4c7aee4ee0c38ab75dfd353b7a83ed3e957fcb47 \\\n    --hash=sha256:d2c244d01048ba476e7c080bd2c6df5e141d211de80223460d5b3b8a2a58433d\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   rouge-score\n```\n\n----------------------------------------\n\nTITLE: Rich Terminal Formatting Library Dependency\nDESCRIPTION: Defines the rich package (version 13.3.2) with hash signatures. The comments show it's used by memray and typer dependencies in the Ray project requirements.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_31\n\nLANGUAGE: txt\nCODE:\n```\nrich==13.3.2 \\\n    --hash=sha256:91954fe80cfb7985727a467ca98a7618e5dd15178cc2da10f553b36a93859001 \\\n    --hash=sha256:a104f37270bf677148d8acb07d33be1569eeee87e2d1beb286a4e9113caf6f2f\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   memray\n    #   typer\n```\n\n----------------------------------------\n\nTITLE: Documenting Ray Workflow Management API in reStructuredText\nDESCRIPTION: This code snippet defines the structure for the Ray Workflow Management API documentation. It sets up the current module and provides an autosummary of the available functions with links to their respective documentation pages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/api/management.rst#2025-04-12_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: ray.workflow\n\n.. autosummary::\n   :nosignatures:\n   :toctree: doc/\n\n   resume\n   resume_async\n   resume_all\n   list_all\n   get_status\n   get_output\n   get_output_async\n   get_metadata\n   cancel\n```\n\n----------------------------------------\n\nTITLE: Testing Ray Serve Endpoints with a Curl Pod\nDESCRIPTION: Commands to create a curl Pod and send HTTP requests to the Ray Serve application to verify that it's working correctly despite having one worker Pod in an unready state.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.md#2025-04-12_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n# Step 6.1: Run a curl Pod.\n# If you already have a curl Pod, you can use `kubectl exec -it <curl-pod> -- sh` to access the Pod.\nkubectl run curl --image=radial/busyboxplus:curl -i --tty\n\n# Step 6.2: Send a request to the simple_app.\ncurl -X POST -H 'Content-Type: application/json' rayservice-no-ray-serve-replica-serve-svc:8000/basic\n# [Expected output]: hello world\n```\n\n----------------------------------------\n\nTITLE: File Synchronization with Ray Clusters\nDESCRIPTION: Commands for uploading and downloading files between local machine and Ray cluster head node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-cli.rst#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ ray rsync_down cluster.yaml '/path/on/cluster' '/local/path'\n$ ray rsync_up cluster.yaml '/local/path' '/path/on/cluster'\n```\n\n----------------------------------------\n\nTITLE: Project Configuration Table in Markdown\nDESCRIPTION: Markdown table defining the project specifications including summary, runtime expectations, compute requirements, and cluster environment details. The configuration uses the anyscale/ray-ml:latest-py39-gpu image and requires packages from requirements.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/README.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Template Specification | Description |\n| ---------------------- | ----------- |\n| Summary | This template demonstrates how to parallelize the training of hundreds of time-series forecasting models with [Ray Tune](https://docs.ray.io/en/latest/tune/index.html). The template uses the `statsforecast` library to fit models to partitions of the M4 forecasting competition dataset. |\n| Time to Run | Around 5 minutes to train all models. |\n| Minimum Compute Requirements | No hard requirements. The default is 8 nodes with 8 CPUs each. |\n| Cluster Environment | This template uses the latest Anyscale-provided Ray ML image using Python 3.9, [`anyscale/ray-ml:latest-py39-gpu`](https://docs.anyscale.com/reference/base-images/overview?utm_source=ray_docs&utm_medium=docs&utm_campaign=many_model_training_readme), with some extra requirements from `requirements.txt` installed on top. If you want to change to a different cluster environment, make sure that it's based on this image and includes all packages listed in the `requirements.txt` file. |\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Package dependency specifications including version numbers and SHA256 hashes for verification. Each entry specifies a package name, version, and corresponding hash values to ensure package integrity.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_7\n\nLANGUAGE: plain\nCODE:\n```\n--hash=sha256:b4c34b7d10b51bcc3a5071e7b8dee77939f1e878477eeecc965e9835f63c6c86 \\\n--hash=sha256:ce9c432eda0dc91cf618a5cedf1a4e142651196bbcd2c80e89ed5a907e5cfaf1\n# via email-validator\ndocutils==0.19 \\\n--hash=sha256:33995a6753c30b7f577febfc2c50411fec6aac7f7ffeb7c4cfe5991072dcf9e6 \\\n--hash=sha256:5e1de4d849fee02c63b040a4a3fd567f4ab104defd8a5511fbbc24a8a017efbc\n# via sphinx\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Libraries with Hash Verification in Python\nDESCRIPTION: This snippet shows how to specify OpenTelemetry and other Python dependencies with exact versions and hash verification for secure installation. The file includes comments showing which requirement files reference each package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-exporter-otlp==1.1.0 \\\n    --hash=sha256:2a2135f87cdad417408d34fc6131879d5cee1d7af7546b4a1f67fd178b262f4e \\\n    --hash=sha256:61ee0a6e9a12dd7191aedca34a8a3e7cc4e8e92504a71adf390b6d2bcc36d0d4\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Result Delimiter for ExperimentAnalysis\nDESCRIPTION: TUNE_RESULT_DELIM sets the delimiter used for nested entries in ExperimentAnalysis dataframes. Currently defaults to '.' but will change to '/' in future Ray versions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_RESULT_DELIM=.\n```\n\n----------------------------------------\n\nTITLE: Checking ProvisioningRequest Status - Completed\nDESCRIPTION: Demonstrates checking the ProvisioningRequest status after GPU resources have been successfully provisioned.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get provisioningrequest\nNAME                                                      ACCEPTED   PROVISIONED   FAILED   AGE\nrayjob-pytorch-text-classifier-nv77q-e95ec-rayjob-gpu-1   True       True          False    57s\n```\n\n----------------------------------------\n\nTITLE: Requirements File with Package Dependencies and Hashes\nDESCRIPTION: A pip requirements file listing Python package dependencies with exact versions and SHA256 hash values for security verification. Each package includes source information and dependency chain through comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-exporter-otlp-proto-grpc==1.1.0 \\\n    --hash=sha256:281e9bbce73b08c1c93781cf7f4282396f74895987fdc051bea335f7dd086199 \\\n    --hash=sha256:5a4a86becf4f9fdf2910a5b869fc40ec9978044f93045fdce240fecb6c64681a\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   opentelemetry-exporter-otlp\nopentelemetry-proto==1.1.0 \\\n    --hash=sha256:237ef4fdd7f752b2fe740352643f8ef82733bd8e0db8b46ed808125ac7c7f112 \\\n    --hash=sha256:ff1ad9a3c572075883c2af0053cefdfaba005d71eade783c4524d34660d53b60\n```\n\n----------------------------------------\n\nTITLE: Pinned Partial-JSON-Parser Dependency with Hash Verification\nDESCRIPTION: Specifies partial-json-parser version 0.2.1.1.post5 with SHA256 hash verification for secure installation. Required by the vllm package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_20\n\nLANGUAGE: plaintext\nCODE:\n```\npartial-json-parser==0.2.1.1.post5 \\\n    --hash=sha256:627715aaa3cb3fb60a65b0d62223243acaa6c70846520a90326fef3a2f0b61ca \\\n    --hash=sha256:992710ac67e90b367921d52727698928040f7713ba7ecb33b96371ea7aec82ca\n```\n\n----------------------------------------\n\nTITLE: Specifying propcache Dependency with Hash Verification in requirements.txt\nDESCRIPTION: Defines the propcache package dependency (version 0.3.0) with comprehensive SHA256 hash verification for secure installation. The package is referenced by other requirements files in the Ray project, specifically for RayLLM testing with Python 3.11 and CUDA 12.4.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_25\n\nLANGUAGE: requirements.txt\nCODE:\n```\npropcache==0.3.0 \\\n    --hash=sha256:02df07041e0820cacc8f739510078f2aadcfd3fc57eaeeb16d5ded85c872c89e \\\n    --hash=sha256:03acd9ff19021bd0567582ac88f821b66883e158274183b9e5586f678984f8fe \\\n    --hash=sha256:03c091bb752349402f23ee43bb2bff6bd80ccab7c9df6b88ad4322258d6960fc \\\n    --hash=sha256:07700939b2cbd67bfb3b76a12e1412405d71019df00ca5697ce75e5ef789d829 \\\n    --hash=sha256:0c3e893c4464ebd751b44ae76c12c5f5c1e4f6cbd6fbf67e3783cd93ad221863 \\\n    --hash=sha256:119e244ab40f70a98c91906d4c1f4c5f2e68bd0b14e7ab0a06922038fae8a20f \\\n    --hash=sha256:11ae6a8a01b8a4dc79093b5d3ca2c8a4436f5ee251a9840d7790dccbd96cb649 \\\n    --hash=sha256:15010f29fbed80e711db272909a074dc79858c6d28e2915704cfc487a8ac89c6 \\\n    --hash=sha256:19d36bb351ad5554ff20f2ae75f88ce205b0748c38b146c75628577020351e3c \\\n    --hash=sha256:1c8f7d896a16da9455f882870a507567d4f58c53504dc2d4b1e1d386dfe4588a \\\n    --hash=sha256:2383a17385d9800b6eb5855c2f05ee550f803878f344f58b6e194de08b96352c \\\n    --hash=sha256:24c04f8fbf60094c531667b8207acbae54146661657a1b1be6d3ca7773b7a545 \\\n    --hash=sha256:2578541776769b500bada3f8a4eeaf944530516b6e90c089aa368266ed70c49e \\\n    --hash=sha256:26a67e5c04e3119594d8cfae517f4b9330c395df07ea65eab16f3d559b7068fe \\\n    --hash=sha256:2b975528998de037dfbc10144b8aed9b8dd5a99ec547f14d1cb7c5665a43f075 \\\n    --hash=sha256:2d15bc27163cd4df433e75f546b9ac31c1ba7b0b128bfb1b90df19082466ff57 \\\n    --hash=sha256:2d913d36bdaf368637b4f88d554fb9cb9d53d6920b9c5563846555938d5450bf \\\n    --hash=sha256:3302c5287e504d23bb0e64d2a921d1eb4a03fb93a0a0aa3b53de059f5a5d737d \\\n    --hash=sha256:36ca5e9a21822cc1746023e88f5c0af6fce3af3b85d4520efb1ce4221bed75cc \\\n    --hash=sha256:3b812b3cb6caacd072276ac0492d249f210006c57726b6484a1e1805b3cfeea0 \\\n    --hash=sha256:3c6ec957025bf32b15cbc6b67afe233c65b30005e4c55fe5768e4bb518d712f1 \\\n    --hash=sha256:41de3da5458edd5678b0f6ff66691507f9885f5fe6a0fb99a5d10d10c0fd2d64 \\\n    --hash=sha256:42924dc0c9d73e49908e35bbdec87adedd651ea24c53c29cac103ede0ea1d340 \\\n    --hash=sha256:4544699674faf66fb6b4473a1518ae4999c1b614f0b8297b1cef96bac25381db \\\n    --hash=sha256:46ed02532cb66612d42ae5c3929b5e98ae330ea0f3900bc66ec5f4862069519b \\\n    --hash=sha256:49ea05212a529c2caffe411e25a59308b07d6e10bf2505d77da72891f9a05641 \\\n    --hash=sha256:4fa0e7c9c3cf7c276d4f6ab9af8adddc127d04e0fcabede315904d2ff76db626 \\\n    --hash=sha256:507c5357a8d8b4593b97fb669c50598f4e6cccbbf77e22fa9598aba78292b4d7 \\\n    --hash=sha256:549722908de62aa0b47a78b90531c022fa6e139f9166be634f667ff45632cc92 \\\n    --hash=sha256:58e6d2a5a7cb3e5f166fd58e71e9a4ff504be9dc61b88167e75f835da5764d07 \\\n    --hash=sha256:5a16167118677d94bb48bfcd91e420088854eb0737b76ec374b91498fb77a70e \\\n    --hash=sha256:5d62c4f6706bff5d8a52fd51fec6069bef69e7202ed481486c0bc3874912c787 \\\n    --hash=sha256:5fa159dcee5dba00c1def3231c249cf261185189205073bde13797e57dd7540a \\\n    --hash=sha256:6032231d4a5abd67c7f71168fd64a47b6b451fbcb91c8397c2f7610e67683810 \\\n    --hash=sha256:63f26258a163c34542c24808f03d734b338da66ba91f410a703e505c8485791d \\\n    --hash=sha256:65a37714b8ad9aba5780325228598a5b16c47ba0f8aeb3dc0514701e4413d7c0 \\\n    --hash=sha256:67054e47c01b7b349b94ed0840ccae075449503cf1fdd0a1fdd98ab5ddc2667b \\\n    --hash=sha256:67dda3c7325691c2081510e92c561f465ba61b975f481735aefdfc845d2cd043 \\\n    --hash=sha256:6985a593417cdbc94c7f9c3403747335e450c1599da1647a5af76539672464d3 \\\n    --hash=sha256:6a1948df1bb1d56b5e7b0553c0fa04fd0e320997ae99689488201f19fa90d2e7 \\\n    --hash=sha256:6b5b7fd6ee7b54e01759f2044f936dcf7dea6e7585f35490f7ca0420fe723c0d \\\n    --hash=sha256:6c929916cbdb540d3407c66f19f73387f43e7c12fa318a66f64ac99da601bcdf \\\n    --hash=sha256:6f4d7a7c0aff92e8354cceca6fe223973ddf08401047920df0fcb24be2bd5138 \\\n    --hash=sha256:728af36011bb5d344c4fe4af79cfe186729efb649d2f8b395d1572fb088a996c \\\n    --hash=sha256:742840d1d0438eb7ea4280f3347598f507a199a35a08294afdcc560c3739989d \\\n    --hash=sha256:75e872573220d1ee2305b35c9813626e620768248425f58798413e9c39741f46 \\\n    --hash=sha256:794c3dd744fad478b6232289c866c25406ecdfc47e294618bdf1697e69bd64a6 \\\n    --hash=sha256:7c0fdbdf6983526e269e5a8d53b7ae3622dd6998468821d660d0daf72779aefa \\\n    --hash=sha256:7c5f5290799a3f6539cc5e6f474c3e5c5fbeba74a5e1e5be75587746a940d51e \\\n    --hash=sha256:7c6e7e4f9167fddc438cd653d826f2222222564daed4116a02a184b464d3ef05 \\\n    --hash=sha256:7cedd25e5f678f7738da38037435b340694ab34d424938041aa630d8bac42663 \\\n    --hash=sha256:7e2e068a83552ddf7a39a99488bcba05ac13454fb205c847674da0352602082f \\\n    --hash=sha256:8319293e85feadbbfe2150a5659dbc2ebc4afdeaf7d98936fb9a2f2ba0d4c35c \\\n    --hash=sha256:8526b0941ec5a40220fc4dfde76aed58808e2b309c03e9fa8e2260083ef7157f \\\n    --hash=sha256:8884ba1a0fe7210b775106b25850f5e5a9dc3c840d1ae9924ee6ea2eb3acbfe7 \\\n    --hash=sha256:8cb625bcb5add899cb8ba7bf716ec1d3e8f7cdea9b0713fa99eadf73b6d4986f \\\n    --hash=sha256:8d663fd71491dde7dfdfc899d13a067a94198e90695b4321084c6e450743b8c7 \\\n    --hash=sha256:8ee1983728964d6070ab443399c476de93d5d741f71e8f6e7880a065f878e0b9 \\\n    --hash=sha256:997e7b8f173a391987df40f3b52c423e5850be6f6df0dcfb5376365440b56667 \\\n    --hash=sha256:9be90eebc9842a93ef8335291f57b3b7488ac24f70df96a6034a13cb58e6ff86 \\\n    --hash=sha256:9ddd49258610499aab83b4f5b61b32e11fce873586282a0e972e5ab3bcadee51 \\\n    --hash=sha256:9ecde3671e62eeb99e977f5221abcf40c208f69b5eb986b061ccec317c82ebd0 \\\n    --hash=sha256:9ff4e9ecb6e4b363430edf2c6e50173a63e0820e549918adef70515f87ced19a \\\n    --hash=sha256:a254537b9b696ede293bfdbc0a65200e8e4507bc9f37831e2a0318a9b333c85c \\\n    --hash=sha256:a2b9bf8c79b660d0ca1ad95e587818c30ccdb11f787657458d6f26a1ea18c568 \\\n    --hash=sha256:a61a68d630e812b67b5bf097ab84e2cd79b48c792857dc10ba8a223f5b06a2af \\\n    --hash=sha256:a7080b0159ce05f179cfac592cda1a82898ca9cd097dacf8ea20ae33474fbb25 \\\n    --hash=sha256:a8fd93de4e1d278046345f49e2238cdb298589325849b2645d4a94c53faeffc5 \\\n    --hash=sha256:a94ffc66738da99232ddffcf7910e0f69e2bbe3a0802e54426dbf0714e1c2ffe \\\n    --hash=sha256:aa806bbc13eac1ab6291ed21ecd2dd426063ca5417dd507e6be58de20e58dfcf \\\n    --hash=sha256:b0c1a133d42c6fc1f5fbcf5c91331657a1ff822e87989bf4a6e2e39b818d0ee9 \\\n    --hash=sha256:b58229a844931bca61b3a20efd2be2a2acb4ad1622fc026504309a6883686fbf \\\n    --hash=sha256:bb2f144c6d98bb5cbc94adeb0447cfd4c0f991341baa68eee3f3b0c9c0e83767 \\\n    --hash=sha256:be90c94570840939fecedf99fa72839aed70b0ced449b415c85e01ae67422c90 \\\n    --hash=sha256:bf0d9a171908f32d54f651648c7290397b8792f4303821c42a74e7805bfb813c \\\n    --hash=sha256:bf15fc0b45914d9d1b706f7c9c4f66f2b7b053e9517e40123e137e8ca8958b3d \\\n    --hash=sha256:bf4298f366ca7e1ad1d21bbb58300a6985015909964077afd37559084590c929 \\\n    --hash=sha256:c441c841e82c5ba7a85ad25986014be8d7849c3cfbdb6004541873505929a74e \\\n    --hash=sha256:cacea77ef7a2195f04f9279297684955e3d1ae4241092ff0cfcef532bb7a1c32 \\\n    --hash=sha256:cd54895e4ae7d32f1e3dd91261df46ee7483a735017dc6f987904f194aa5fd14 \\\n    --hash=sha256:d1323cd04d6e92150bcc79d0174ce347ed4b349d748b9358fd2e497b121e03c8 \\\n    --hash=sha256:d383bf5e045d7f9d239b38e6acadd7b7fdf6c0087259a84ae3475d18e9a2ae8b \\\n    --hash=sha256:d3e7420211f5a65a54675fd860ea04173cde60a7cc20ccfbafcccd155225f8bc \\\n    --hash=sha256:d8074c5dd61c8a3e915fa8fc04754fa55cfa5978200d2daa1e2d4294c1f136aa \\\n    --hash=sha256:df03cd88f95b1b99052b52b1bb92173229d7a674df0ab06d2b25765ee8404bce \\\n    --hash=sha256:e45377d5d6fefe1677da2a2c07b024a6dac782088e37c0b1efea4cfe2b1be19b \\\n    --hash=sha256:e53d19c2bf7d0d1e6998a7e693c7e87300dd971808e6618964621ccd0e01fe4e \\\n    --hash=sha256:e560fd75aaf3e5693b91bcaddd8b314f4d57e99aef8a6c6dc692f935cc1e6bbf \\\n    --hash=sha256:ec5060592d83454e8063e487696ac3783cc48c9a329498bafae0d972bc7816c9 \\\n    --hash=sha256:ecc2920630283e0783c22e2ac94427f8cca29a04cfdf331467d4f661f4072dac \\\n    --hash=sha256:ed7161bccab7696a473fe7ddb619c1d75963732b37da4618ba12e60899fefe4f \\\n    --hash=sha256:ee0bd3a7b2e184e88d25c9baa6a9dc609ba25b76daae942edfb14499ac7ec374 \\\n    --hash=sha256:ee25f1ac091def37c4b59d192bbe3a206298feeb89132a470325bf76ad122a1e \\\n    --hash=sha256:efa44f64c37cc30c9f05932c740a8b40ce359f51882c70883cc95feac842da4d \\\n    --hash=sha256:f47d52fd9b2ac418c4890aad2f6d21a6b96183c98021f0a48497a904199f006e \\\n    --hash=sha256:f857034dc68d5ceb30fb60afb6ff2103087aea10a01b613985610e007053a121 \\\n    --hash=sha256:fb91d20fa2d3b13deea98a690534697742029f4fb83673a3501ae6e3746508b5 \\\n    --hash=sha256:fddb8870bdb83456a489ab67c6b3040a8d5a55069aa6f72f9d872235fbc52f54\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   aiohttp\n    #   yarl\n```\n\n----------------------------------------\n\nTITLE: Converting Between Mars DataFrames and Ray Datasets\nDESCRIPTION: Example showing how to create a Mars DataFrame, convert it to a Ray Dataset, perform operations on the dataset, and convert it back to a Mars DataFrame.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/mars-on-ray.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mars.tensor as mt\nimport mars.dataframe as md\ndf = md.DataFrame(\n    mt.random.rand(1000_0000, 4),\n    columns=list('abcd'))\n# Convert mars dataframe to ray dataset\nimport ray\n# ds = md.to_ray_dataset(df)\nds = ray.data.from_mars(df)\nprint(ds.schema(), ds.count())\nds.filter(lambda row: row[\"a\"] > 0.5).show(5)\n# Convert ray dataset to mars dataframe\n# df2 = md.read_ray_dataset(ds)\ndf2 = ds.to_mars()\nprint(df2.head(5).execute())\n```\n\n----------------------------------------\n\nTITLE: Installing pyasn1 with Pinned Version and Hashes\nDESCRIPTION: Specifies pyasn1 package with version 0.5.1 and SHA256 hashes for verification. Comments indicate this is required by oauth2client, pyasn1-modules, and rsa packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_24\n\nLANGUAGE: pip\nCODE:\n```\npyasn1==0.5.1 \\\n    --hash=sha256:4439847c58d40b1d0a573d07e3856e95333f1976294494c325775aeca506eb58 \\\n    --hash=sha256:6d391a96e59b23130a5cfa74d6fd7f388dbbe26cc8f1edf39fdddf08d9d6676c\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   oauth2client\n    #   pyasn1-modules\n    #   rsa\n```\n\n----------------------------------------\n\nTITLE: Package Hash Requirements Block\nDESCRIPTION: Lists package dependencies with their corresponding SHA-256 hash values for secure package verification. The file specifically shows requirements for packages like aiohttp and ypy-websocket with multiple hash values for different distributions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_37\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:0b3c92fa08759dbf12b3a59579a4096ba9af8dd344d9a813fc7f5070d86bbab1 \\\n--hash=sha256:0fb2171a4486bb075316ee754c6d8382ea6eb8b399d4ec62fde2b591f879778a \\\n--hash=sha256:1a74a13a4c857a84a845505fd2d68e54826a2cd01935a96efb1e9d86c728e186\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Documentation Tree for Ray Core API\nDESCRIPTION: RST toctree directive that defines the documentation structure for Ray Core API with maxdepth set to 2 levels. Lists key documentation files covering core functionality, scheduling, runtime environment, utilities, exceptions, CLI and observability.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    core.rst\n    scheduling.rst\n    runtime-env.rst\n    utility.rst\n    exceptions.rst\n    cli.rst\n    ../../ray-observability/reference/cli.rst\n    ../../ray-observability/reference/api.rst\n```\n\n----------------------------------------\n\nTITLE: Specifying jiter Package Version and Hashes\nDESCRIPTION: Defines the version and hash values for the jiter package. It specifies version 0.8.2 and includes multiple hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_12\n\nLANGUAGE: Text\nCODE:\n```\njiter==0.8.2 \\\n    --hash=sha256:025337859077b41548bdcbabe38698bcd93cfe10b06ff66617a48ff92c9aec60 \\\n    --hash=sha256:03c9df035d4f8d647f8c210ddc2ae0728387275340668fb30d2421e17d9a0841 \\\n    --hash=sha256:08d4c92bf480e19fc3f2717c9ce2aa31dceaa9163839a311424b6862252c943e \\\n    # ... (additional hash values omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Package Hash Requirements List\nDESCRIPTION: List of package dependencies with their SHA-256 hash values for verification, including platform-specific constraints and version pinning\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_51\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:60db23fa423575eeb65ea430cee741acb7c26a1365d103f7b0f6ec412b893853 \\\n--hash=sha256:642c2e7a804fcf18c222e1060df25fc210b9c58db7c91416fb055897fc27e8cc \\\n--hash=sha256:6447e9f3ba72f8e2b985a1da758767698efa72723d5b59accefd716e9e8272bf\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Index and Find Links\nDESCRIPTION: This section specifies the package index URLs and find-links for the project. It includes the main PyPI index, an extra index for PyTorch wheels, and find-links for specific torch versions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_2\n\nLANGUAGE: pip\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cu121\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html\n```\n\n----------------------------------------\n\nTITLE: Pinned Prometheus FastAPI Instrumentator with Hash Verification\nDESCRIPTION: Specifies prometheus-fastapi-instrumentator version 7.0.2 with SHA256 hash verification for secure installation. Required by the vllm package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_24\n\nLANGUAGE: plaintext\nCODE:\n```\nprometheus-fastapi-instrumentator==7.0.2 \\\n    --hash=sha256:8a4d8fb13dbe19d2882ac6af9ce236e4e1f98dc48e3fa44fe88d8e23ac3c953f \\\n    --hash=sha256:975e39992acb7a112758ff13ba95317e6c54d1bbf605f9156f31ac9f2800c32d\n```\n\n----------------------------------------\n\nTITLE: Ray Data Operator Completion Log Format\nDESCRIPTION: Example log output showing detailed metrics when a Ray Data operator completes, including input/output statistics and resource usage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/monitoring-your-workload.rst#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nOperator InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(<lambda>)] completed. Operator Metrics:\n{'num_inputs_received': 20, 'bytes_inputs_received': 46440, 'num_task_inputs_processed': 20, 'bytes_task_inputs_processed': 46440, 'num_task_outputs_generated': 20, 'bytes_task_outputs_generated': 800, 'rows_task_outputs_generated': 100, 'num_outputs_taken': 20, 'bytes_outputs_taken': 800, 'num_outputs_of_finished_tasks': 20, 'bytes_outputs_of_finished_tasks': 800, 'num_tasks_submitted': 20, 'num_tasks_running': 0, 'num_tasks_have_outputs': 20, 'num_tasks_finished': 20, 'obj_store_mem_freed': 46440, 'obj_store_mem_spilled': 0, 'block_generation_time': 1.191296085, 'cpu_usage': 0, 'gpu_usage': 0, 'ray_remote_args': {'num_cpus': 1, 'scheduling_strategy': 'SPREAD'}}\n```\n\n----------------------------------------\n\nTITLE: Package Hash Verification with Source Comment\nDESCRIPTION: Hash verification data for an unnamed package with a comment indicating it's sourced from a compiled requirements file and used via nbconvert. This is likely part of a larger requirements file with pinned dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n    --hash=sha256:23d891e5bdc12e2e506e7d225d6aa929e0a0368c9916c1fddefab88166e98b20 \\\n    --hash=sha256:266f655d1baff9c47b52f529b5f6bec33f66042f65f7c56adde3fcf2ed62ae8b \\\n    --hash=sha256:273473d34462ae6e97c0f4e517bd1bf9588aa67a1d47d93f760a1282640e24ac \\\n    --hash=sha256:2bd9ac6e44f2db368ef8986f3989a4cad3de4cd55dbdda536e253000c801bcc7 \\\n    --hash=sha256:33714fcf5af4ff7e70a49731a7cc8fd9ce910b9ac194f66eaa18c3cc0a4c02be \\\n    --hash=sha256:359a8b09d712df27849e0bcb62c6a3404e780b274b0b7e4c39a88826d1926c28 \\\n    --hash=sha256:365005e8b0718ea6d64b374423e870648ab47c3a905356ab6e5a5ff03962b9a9 \\\n    --hash=sha256:389d2b2e543b27962990ab529ac6720c3dded588cc6d0f6557eec153305a3622 \\\n    --hash=sha256:3b505f2bbff50d261176e67be24e8909e54b5d9d08b12d4946344066d66b3e43 \\\n    --hash=sha256:3d74d4a3c4b8f7a1f676cedf8e84bcc57705a6d7925e6daef7a1e54ae543a197 \\\n    --hash=sha256:3f3f00a9061605725df1816f5713d10cd94636347ed651abdbc75828df302b20 \\\n    --hash=sha256:43498ea734ccdfb92e1886dfedaebeb81178a241d39a79d5351ba2b671bff2b2 \\\n    --hash=sha256:4855161013dfb2b762e02b3f4d4a21cc7c6aec13c69e3bffbf5022b3e708dd97 \\\n    --hash=sha256:4d973729ce04784906a19108054e1fd476bc85279a403ea1a72fdb051c76fa48 \\\n    --hash=sha256:4ece9cca4cd1c8ba889bfa67eae7f21d0d1a2e715b4d5045395113361e8c533d \\\n    --hash=sha256:506becdf2ecaebaf7f7995f776394fcc8bd8a78022772de66677c84fb02dd33d \\\n    --hash=sha256:520486f27f1d4ce9654154b4494cf9307b495527f3a2908ad4cb48e4f7ed7ef7 \\\n    --hash=sha256:5557461f83bb7cc718bc9ee1f7156d50e31747e5b38d79cf40f79ab1447afd2d \\\n    --hash=sha256:562778586949be7e0d7435fcb24aca4810913771f845d99145a6cee64d5b67ca \\\n    --hash=sha256:59bb5979f9941c61e907ee571732219fa4774d5a18f3fa5ff2df963f5dfaa6bc \\\n    --hash=sha256:606d445feeb0856c2b424405236a01c71af7c97e5fe42fbc778634faef2b47e4 \\\n    --hash=sha256:6197c3f3c0b960ad033b9b7d611db11285bb461fc6b802c1dd50d04ad715c225 \\\n    --hash=sha256:647459b23594f370c1c01768edaa0ba0959afc39caeeb793b43158bb9bb6a663 \\\n    --hash=sha256:647bfe88b1997d7ae8d45dabc7c868d8cb0c8412a6e730a7651050b8c7289cf2 \\\n    --hash=sha256:6bee9c2e501d835f91460b2c904bc359f8433e96799f5c2ff20feebd9bb1e590 \\\n    --hash=sha256:6dbdacf5752fbd78ccdb434698230c4f0f95df7dd956d5f205b5ed6911a1367c \\\n    --hash=sha256:701847a7aaefef121c5c0d855b2affa5f9bd45196ef00266724a80e439220e46 \\\n    --hash=sha256:786d6b57026e7e04d184313c1359ac3d68002c33e4b1042ca58c362f1d09ff58 \\\n    --hash=sha256:7b378847a09d6bd46047f5f3599cdc64fcb4cc5a5a2dd0a2af610361fbe77b16 \\\n    --hash=sha256:7d1d6c9e74c70ddf524e3c09d9dc0522aba9370708c2cb58680ea40174800013 \\\n    --hash=sha256:857d6565f9aa3464764c2cb6a2e3c2e75e1970e877c188f4aeae45954a314e0c \\\n    --hash=sha256:8671622256a0859f5089cbe0ce4693c2af407bc053dcc99aadff7f5310b4aa02 \\\n    --hash=sha256:88f7c383071981c74ec1998ba9b437659e4fd02a3c4a4d3efc16774eb108d0ec \\\n    --hash=sha256:8aecb5a7f6f7f8fe9cac0bcadd39efaca8bbf8d1bf242e9f175cbe4c925116c3 \\\n    --hash=sha256:91bbf398ac8bb7d65a5a52127407c05f75a18d7015a270fdd94bbcb04e65d573 \\\n    --hash=sha256:936e8880cc00f839aa4173f94466a8406a96ddce814651075f95837316369899 \\\n    --hash=sha256:953dd5481bd6252bd480d6ec431f61d7d87fdcbbb71b0d2bdcfc6ae00bb6fb10 \\\n    --hash=sha256:95ae6c5a196e2f239150aa4a479967351df7f44800c93e5a975ec726fef005e2 \\\n    --hash=sha256:9a2b5915c333e4364367140443b59f09feae42184459b913f0f41b9fed55794a \\\n    --hash=sha256:9ae6c3363261021144121427b1552b29e7b59de9d6a75bf51e03bc072efb3c37 \\\n    --hash=sha256:9b556596c49fa1232b0fff4b0e69b9d4083a502e60e404b44341e2f8fb7187f5 \\\n    --hash=sha256:9c131447768ed7bc05a02553d939e7f0e807e533441901dd504e217b76307745 \\\n    --hash=sha256:9d9d5726474cbbef279fd709008f91a49c4f758bec9c062dfbba88eab00e3ff9 \\\n    --hash=sha256:a1bdcbebd4e13446a14de4dd1825f1e778e099f17f79718b4aeaf2403624b0f7 \\\n    --hash=sha256:a602ed9bd2c7d85bd58592c28e101bd9ff9c718fbde06545a70945ffd5d11868 \\\n    --hash=sha256:a8edae5253efa75c2fc79a90068fe540b197d1c7ab5803b800fccfe240eed33c \\\n    --hash=sha256:a905affe76f1802edcac554e3ccf68188bea16546071d7583fb1b693f9cf756b \\\n    --hash=sha256:a9e7c6d89c77bb2770c9491d988f26a4b161d05c8ca58f63fb1f1b6b9a74be45 \\\n    --hash=sha256:aa9b5abd07f71b081a33115d9758ef6077924082055005808f68feccb27616bd \\\n    --hash=sha256:aaa5c173a26960fe67daa69aa93d6d6a1cd714a6eb13802d4e4bd1d24a530644 \\\n    --hash=sha256:ac7674d1638df129d9cb4503d20ffc3922bd463c865ef3cb412f2c926108e9a4 \\\n    --hash=sha256:b1541e50b78e15fa06a2670157a1962ef06591d4c998b998047fff5e3236880e \\\n    --hash=sha256:b1980dbcaad634fe78e710c8587383e6e3f61dbe146bcbfd13a9c8ab2d7b1192 \\\n    --hash=sha256:bafa65e3acae612a7799ada439bd202403414ebe23f52e5b17f6ffc2eb98c2be \\\n    --hash=sha256:bb5bd6212eb0edfd1e8f254585290ea1dadc3687dd8fd5e2fd9a87c31915cdab \\\n    --hash=sha256:bbdd69e20fe2943b51e2841fc1e6a3c1de460d630f65bde12452d8c97209464d \\\n    --hash=sha256:bc354b1393dce46026ab13075f77b30e40b61b1a53e852e99d3cc5dd1af4bc85 \\\n    --hash=sha256:bcee502c649fa6351b44bb014b98c09cb00982a475a1912a9881ca28ab4f9cd9 \\\n    --hash=sha256:bdd9abccd0927673cffe601d2c6cdad1c9321bf3437a2f507d6b037ef91ea307 \\\n    --hash=sha256:c42ae7e010d7d6bc51875d768110c10e8a59494855c3d4c348b068f5fb81fdcd \\\n    --hash=sha256:c71b5b860c5215fdbaa56f715bc218e45a98477f816b46cfde4a84d25b13274e \\\n    --hash=sha256:c7721a3ef41591341388bb2265395ce522aba52f969d33dacd822da8f018aff8 \\\n    --hash=sha256:ca8e44b5ba3edb682ea4e6185b49661fc22b230cf811b9c13963c9f982d1d964 \\\n    --hash=sha256:cb53669442895763e61df5c995f0e8361b61662f26c1b04ee82899c2789c8f69 \\\n    --hash=sha256:cc02c06e9e320869d7d1bd323df6dd4281e78ac2e7f8526835d3d48c69060683 \\\n    --hash=sha256:d3caa09e613ece43ac292fbed513a4bce170681a447d25ffcbc1b647d45a39c5 \\\n    --hash=sha256:d82411dbf4d3127b6cde7da0f9373e37ad3a43e89ef374965465928f01c2b979 \\\n    --hash=sha256:dbcb2dc07308453db428a95a4d03259bd8caea97d7f0776842299f2d00c72fc8 \\\n    --hash=sha256:dd4fda67f5faaef4f9ee5383435048ee3e11ad996901225ad7615bc92245bc8e \\\n    --hash=sha256:ddd92e18b783aeb86ad2132d84a4b795fc5ec612e3545c1b687e7747e66e2b53 \\\n    --hash=sha256:de362ac8bc962408ad8fae28f3967ce1a262b5d63ab8cefb42662566737f1dc7 \\\n    --hash=sha256:e214025e23db238805a600f1f37bf9f9a15413c7bf5f9d6ae194f84980c78722 \\\n    --hash=sha256:e8f9f93a23634cfafbad6e46ad7d09e0f4a25a2400e4a64b1b7b7c0fbaa06d9d \\\n    --hash=sha256:e96a1788f24d03e8d61679f9881a883ecdf9c445a38f9ae3f3f193ab6c591c66 \\\n    --hash=sha256:ec53a09aee61d45e7dbe7e91252ff0491b6b5fee3d85b2d45b173d8ab453efc1 \\\n    --hash=sha256:f10250bb190fb0742e3e1958dd5c100524c2cc5096c67c8da51233f7448dc137 \\\n    --hash=sha256:f1faee2a831fe249e1bae9cbc68d3cd8a30f7e37851deee4d7962b17c410dd56 \\\n    --hash=sha256:f610d980e3fccf4394ab3806de6065682982f3d27c12d4ce3ee46a8183d64a6a \\\n    --hash=sha256:f6c35b2f87c004270fa2e703b872fcc984d714d430b305145c39d53074e1ffe0 \\\n    --hash=sha256:f836f39678cb47c9541f04d8ed4545719dc31ad850bf1832d6b4171e30d65d23 \\\n    --hash=sha256:f99768232f036b4776ce419d3244a04fe83784bce871b16d2c2e984c7fcea847 \\\n    --hash=sha256:fd814847901df6e8de13ce69b84c31fc9b3fb591224d6762d0b256d510cbf382 \\\n    --hash=sha256:fdb325b7fba1e2c40b9b1db407f85642e32404131c08480dd652110fc908561b\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   nbconvert\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with SHA256 Hashes\nDESCRIPTION: Package requirements listing with SHA256 hashes for package verification. Each package includes version number and multiple hash values for different package distributions to ensure integrity during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_53\n\nLANGUAGE: plaintext\nCODE:\n```\nypy-websocket==0.8.4 \\\n    --hash=sha256:43a001473f5c8abcf182f603049cf305cbc855ad8deaa9dfa0f3b5a7cea9d0ff \\\n    --hash=sha256:b1ba0dfcc9762f0ca168d2378062d3ca1299d39076b0f145d961359121042be5\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jupyter-server-ydoc\nzipp==3.21.0 \\\n    --hash=sha256:2c9958f6430a2040341a52eb608ed6dd93ef4392e02ffe219417c1b28b5dd1f4 \\\n    --hash=sha256:ac1bbe05fd2991f160ebce24ffbac5f6d11d83dc90891255885223d42b3cd931\n    # via importlib-metadata\n```\n\n----------------------------------------\n\nTITLE: Package Hash Specifications\nDESCRIPTION: Detailed package dependency specifications with SHA256 hashes for package verification. Includes dependencies for various Python packages like ipykernel, jupyter-client, requests, and other Ray project dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_39\n\nLANGUAGE: txt\nCODE:\n```\n--hash=sha256:2cc4e280098c1b192c42a849de8de2c8e0f3a84086a76ec5b07bfee29bda7d18 \\\n--hash=sha256:2ed8357f4c6e0daa4f3baf31832df8a33334e0fe5b020a61bc8b345a3db7a606 \\\n--hash=sha256:3191d312c73e3cfd0f0afdf51df8405aafeb0bad71e7ed8f68b24b63c4f36500\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with SHA256 Hashes\nDESCRIPTION: Package dependency specifications with version pins and SHA256 hash verification values. Includes packages like click, cloudpickle, colorful, cryptography, and others with their exact versions and hash values for secure installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_3\n\nLANGUAGE: txt\nCODE:\n```\n--hash=sha256:9f96df6923e21816da7e0ad3fd47dd8f94b2a5ce594e00677c0013018b813458 \\\n    --hash=sha256:a10af20b82360ab00827f916a6058451b723b4e65030c5a18577c8b2de5b3389 \\\n    --hash=sha256:a50aebfa173e157099939b17f18600f72f84eed3049e743b68ad15bd69b6bf99\n```\n\n----------------------------------------\n\nTITLE: Reflection-Based Method Invocation in Vert.x\nDESCRIPTION: Thread stack trace showing reflection-based method invocation in Vert.x. This trace demonstrates how Java reflection is used to invoke methods from JavaScript code in a Vert.x HTTP server.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_91\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j] 3\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Version Pinning and Hashes\nDESCRIPTION: List of Python package dependencies with exact versions specified and SHA256 hashes for security verification. Includes packages for Jupyter notebooks, scientific computing, cloud services and utilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_8\n\nLANGUAGE: pip\nCODE:\n```\nhumanize==4.12.1 \\\n    --hash=sha256:1338ba97415c96556758a6e2f65977ed406dddf4620d4c6db9bbdfd07f0f1232 \\\n    --hash=sha256:86014ca5c52675dffa1d404491952f1f5bf03b07c175a51891a343daebf01fea\n\nidna==3.7 \\\n    --hash=sha256:028ff3aadf0609c1fd278d8ea3089299412a7a8b9bd005dd08b9f8285bcb5cfc \\\n    --hash=sha256:82fee1fc78add43492d3a1898bfa6d8a904cc97d8427f683ed8e798d07761aa0\n```\n\n----------------------------------------\n\nTITLE: Defining NVIDIA CUDA Libraries Dependencies with Platform Constraints for Ray Project\nDESCRIPTION: Package specifications for NVIDIA CUDA libraries with specific version requirements, platform constraints (x86_64 Linux), and hash verification. These packages are core dependencies for GPU-accelerated computation in PyTorch.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_17\n\nLANGUAGE: pip\nCODE:\n```\nnvidia-cufft-cu12==11.2.1.3 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:5dad8008fc7f92f5ddfa2101430917ce2ffacd86824914c82e28990ad7f00399 \\\n    --hash=sha256:d802f4954291101186078ccbe22fc285a902136f974d369540fd4a5333d1440b \\\n    --hash=sha256:f083fc24912aa410be21fa16d157fed2055dab1cc4b6934a0e03cba69eb242b9\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   torch\n```\n\n----------------------------------------\n\nTITLE: Specifying Pure-Eval Package Dependency with Hash Verification\nDESCRIPTION: Defines the pure-eval package dependency at version 0.2.2 with SHA256 hash verification. This package is required by the stack-data component.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\npure-eval==0.2.2 \\\n    --hash=sha256:01eaab343580944bc56080ebe0a674b39ec44a945e6d09ba7db3cb8cec289350 \\\n    --hash=sha256:2b45320af6dfaa1750f543d714b6d1c520a1688dec6fd24d339063ce0aaa9ac3\n```\n\n----------------------------------------\n\nTITLE: Network Connectivity Test Commands\nDESCRIPTION: Commands to test network connectivity to the Ray head node using nmap and netcat tools.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ nmap -sV --reason -p $PORT $HEAD_ADDRESS\nNmap scan report for compute04.berkeley.edu (123.456.78.910)\nHost is up, received echo-reply ttl 60 (0.00087s latency).\nrDNS record for 123.456.78.910: compute04.berkeley.edu\nPORT     STATE SERVICE REASON         VERSION\n6379/tcp open  redis?  syn-ack\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\n$ nc -vv -z $HEAD_ADDRESS $PORT\nConnection to compute04.berkeley.edu 6379 port [tcp/*] succeeded!\n```\n\n----------------------------------------\n\nTITLE: Dependency Hash Verification for Ray Project in requirements.txt\nDESCRIPTION: This snippet shows package dependencies with their SHA-256 hash values used for verification in the Ray project. The format includes package names, versions, hash values, and comments indicating which files require these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_30\n\nLANGUAGE: requirements.txt\nCODE:\n```\n--hash=sha256:44d61b4b7d0c2c9ac019c314e52d7cbda0ae31078aabd0f22e583af3e0d79723 \\\n    --hash=sha256:4617e1915a539a0d9a9567795023de41a87106522ff83fbfaf1f6baf8e85437e \\\n    --hash=sha256:4b232061ca880db21fa14defe219840ad9b74b6158adb52ddf0e87bead9e8493 \\\n    --hash=sha256:5246b14ca64a8675e0a7161f7af68fe3e910e6b90542b4bfb5439ba752191df6 \\\n    --hash=sha256:5725dd9cc02068996d4438d397e255dcb1df776b7ceea3b9cb972bdb11260a83 \\\n    --hash=sha256:583f6a1993ca3369e0f80ba99d796d8e6b1a3a2a442dd4e1a79e652116413091 \\\n    --hash=sha256:59259dc58e57b10e7e18ce02c311804c10c5a793e6568f8af4dead03264584d1 \\\n    --hash=sha256:593eba61ba0c3baae5bc9be2f5232430453fb4432048de28399ca7376de9c627 \\\n    --hash=sha256:59f4a79c19232a5774aee369a0c296712ad0e77f24e62cad53160312b1c1eaa1 \\\n    --hash=sha256:5f0e260eaf54380380ac3808aa4ebe2d8ca28b9087cf411649f96bad6900c728 \\\n    --hash=sha256:62d9cfcf4948683a18a9aff0ab7e1474d407b7bab2ca03116109f8464698ab16 \\\n    --hash=sha256:64607d4cbf1b7e3c3c8a14948b99345eda0e161b852e122c6bb71aab6d1d798c \\\n    --hash=sha256:655ca44a831ecb238d124e0402d98f6212ac527a0ba6c55ca26f616604e60a45 \\\n    --hash=sha256:666ecce376999bf619756a24ce15bb14c5bfaf04bf00abc7e663ce17c3f34fe7 \\\n    --hash=sha256:68049202f67380ff9aa52f12e92b1c30115f32e6895cd7198fa2a7961621fc5a \\\n    --hash=sha256:69803198097467ee7282750acb507fba35ca22cc3b85f16cf45fb01cb9097730 \\\n    --hash=sha256:6c7b99ca52c2c1752b544e310101b98a659b720b21db00e65edca34483259967 \\\n    --hash=sha256:6dd9412824c4ce1aca56c47b0991e65bebb7ac3f4edccfd3f156150c96a7bf25 \\\n    --hash=sha256:70eb60b3ae9245ddea20f8a4190bd79c705a22f8028aaf8bbdebe4716c3fab24 \\\n    --hash=sha256:70fb28128acbfd264eda9bf47015537ba3fe86e40d046eb2963d75024be4d055 \\\n    --hash=sha256:7b2513ba235829860b13faa931f3b6846548021846ac808455301c23a101689d \\\n    --hash=sha256:7ef9d9da710be50ff6809fed8f1963fecdfecc8b86656cadfca3bc24289414b0 \\\n    --hash=sha256:81e69b0a0e2537f26d73b4e43ad7bc8c8efb39621639b4434b76a3de50c6966e \\\n    --hash=sha256:8633e471c6207a039eff6aa116e35f69f3156b3989ea3e2d755f7bc41754a4a7 \\\n    --hash=sha256:8bd7c8cfc0b8247c8799080fbff54e0b9619e17cdfeb0478ba7295d43f635d7c \\\n    --hash=sha256:9253fc214112405f0afa7db88739294295f0e08466987f1d70e29930262b4c8f \\\n    --hash=sha256:99b37292234e61325e7a5bb9689e55e48c3f5f603af88b1642666277a81f1fbd \\\n    --hash=sha256:9bd7228827ec7bb817089e2eb301d907c0d9827a9e558f22f762bb690b131652 \\\n    --hash=sha256:9beeb01d8c190d7581a4d59522cd3d4b6887040dcfc744af99aa59fef3e041a8 \\\n    --hash=sha256:a63cbdd98acef6570c62b92a1e43266f9e8b21e699c363c0fef13bd530799c11 \\\n    --hash=sha256:a76e42402542b1fae59798fab64432b2d015ab9d0c8c47ba7addddbaf7952333 \\\n    --hash=sha256:ac0a03221cdb5058ce0167ecc92a8c89e8d0decdc9e99a2ec23380793c4dcb96 \\\n    --hash=sha256:b0b4136a252cadfa1adb705bb81524eee47d9f6aab4f2ee4fa1e9d3cd4581f64 \\\n    --hash=sha256:b25bc607423935079e05619d7de556c91fb6adeae9d5f80868dde3468657994b \\\n    --hash=sha256:b3d504047aba448d70cf6fa22e06cb09f7cbd761939fdd47604f5e007675c24e \\\n    --hash=sha256:bb47271f60660803ad11f4c61b42242b8c1312a31c98c578f79ef9387bbde21c \\\n    --hash=sha256:bbb232860e3d03d544bc03ac57855cd82ddf19c7a07651a7c0fdb95e9efea8b9 \\\n    --hash=sha256:bc27863442d388870c1809a87507727b799c8460573cfbb6dc0eeaef5a11b5ec \\\n    --hash=sha256:bc51abd01f08117283c5ebf64844a35144a0843ff7b2983e0648e4d3d9f10dbb \\\n    --hash=sha256:be2eb3f2495ba669d2a985f9b426c1797b7d48d6963899276d22f23e33d47e37 \\\n    --hash=sha256:bf9db5488121b596dbfc6718c76092fda77b703c1f7533a226a5a9f65248f8ad \\\n    --hash=sha256:c58e2339def52ef6b71b8f36d13c3688ea23fa093353f3a4fee2556e62086ec9 \\\n    --hash=sha256:cfbc454a2880389dbb9b5b398e50d439e2e58669160f27b60e5eca11f68ae17c \\\n    --hash=sha256:cff63a0272fcd259dcc3be1657b07c929c466b067ceb1c20060e8d10af56f5bf \\\n    --hash=sha256:d115bffdd417c6d806ea9069237a4ae02f513b778e3789a359bc5856e0404cc4 \\\n    --hash=sha256:d20cfb4e099748ea39e6f7b16c91ab057989712d31761d3300d43134e26e165f \\\n    --hash=sha256:d48424e39c2611ee1b84ad0f44fb3b2b53d473e65de061e3f460fc0be5f1939d \\\n    --hash=sha256:e0fa2d4ec53dc51cf7d3bb22e0aa0143966119f42a0c3e4998293a3dd2856b09 \\\n    --hash=sha256:e32fee8ab45d3c2db6da19a5323bc3362237c8b653c70194414b892fd06a080d \\\n    --hash=sha256:e35ba67d65d49080e8e5a1dd40101fccdd9798adb9b050ff670b7d74fa41c566 \\\n    --hash=sha256:e3fb866d9932a3d7d0c82da76d816996d1667c44891bd861a0f97ba27e84fc74 \\\n    --hash=sha256:e61b02c3f7a1e0b75e20c3978f7135fd13cb6cf551bf4a6d29b999a88830a338 \\\n    --hash=sha256:e67ba3c290821343c192f7eae1d8fd5999ca2dc99994114643e2f2d3e6138b15 \\\n    --hash=sha256:e79dd39f1e8c3504be0607e5fc6e86bb60fe3584bec8b782578c3b0fde8d932c \\\n    --hash=sha256:e89391e6d60251560f0a8f4bd32137b077a80d9b7dbe6d5cab1cd80d2746f648 \\\n    --hash=sha256:ea7433ce7e4bfc3a85654aeb6747babe3f66eaf9a1d0c1e7a4435bbdf27fea84 \\\n    --hash=sha256:eaf16ae9ae519a0e237a0f528fd9f0197b9bb70f40263ee57ae53c2b8d48aeb3 \\\n    --hash=sha256:eb0c341fa71df5a4595f9501df4ac5abfb5a09580081dffbd1ddd4654e6e9123 \\\n    --hash=sha256:f276b245347e6e36526cbd4a266a417796fc531ddf391e43574cf6466c492520 \\\n    --hash=sha256:f47ad3d5f3258bd7058d2d506852217865afefe6153a36eb4b6928758041d831 \\\n    --hash=sha256:f56a6b404f74ab372da986d240e2e002769a7d7102cc73eb238a4f72eec5284e \\\n    --hash=sha256:f5cf2a0c2bdadf3791b5c205d55a37a54025c6e18a71c71f82bb536cf9a454bf \\\n    --hash=sha256:f5d36399a1b96e1a5fdc91e0522544580dbebeb1f77f27b2b0ab25559e103b8b \\\n    --hash=sha256:f60bd8423be1d9d833f230fdbccf8f57af322d96bcad6599e5a771b151398eb2 \\\n    --hash=sha256:f612463ac081803f243ff13cccc648578e2279295048f2a8d5eb430af2bae6e3 \\\n    --hash=sha256:f73d3fef726b3243a811121de45193c0ca75f6407fe66f3f4e183c983573e130 \\\n    --hash=sha256:f82a116a1d03628a8ace4859556fb39fd1424c933341a08ea3ed6de1edb0283b \\\n    --hash=sha256:fb0ba113b4983beac1a2eb16faffd76cb41e176bf58c4afe3e14b9c681f702de \\\n    --hash=sha256:fb4f868f712b2dd4bcc538b0a0c1f63a2b1d584c925e69a224d759e7070a12d5 \\\n    --hash=sha256:fb6116dfb8d1925cbdb52595560584db42a7f664617a1f7d7f6e32f138cdf37d \\\n    --hash=sha256:fda7cb070f442bf80b642cd56483b5548e43d366fe3f39b98e67cce780cded00 \\\n    --hash=sha256:feea821ee2a9273771bae61194004ee2fc33f8ec7db08117ef9147d4bbcbca8e\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jsonschema\n    #   referencing\nrsa==4.7.2 \\\n    --hash=sha256:78f9a9bf4e7be0c5ded4583326e7461e3a3c5aae24073648b4bdfa797d78c9d2 \\\n    --hash=sha256:9d689e6ca1b3038bc82bf8d23e944b6b6037bc02301a574935b2dd946e0353b9\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   google-auth\n    #   oauth2client\ns3transfer==0.6.2 \\\n    --hash=sha256:b014be3a8a2aab98cfe1abc7229cc5a9a0cf05eb9c1f2b86b230fd8df3f78084 \\\n    --hash=sha256:cab66d3380cca3e70939ef2255d01cd8aece6a4907a9528740f668c4b0611861\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   boto3\nsafetensors==0.5.2 \\\n    --hash=sha256:03c937100f38c9ff4c1507abea9928a6a9b02c9c1c9c3609ed4fb2bf413d4975 \\\n    --hash=sha256:1506e4c2eda1431099cebe9abf6c76853e95d0b7a95addceaa74c6019c65d8cf \\\n    --hash=sha256:3ab696dfdc060caffb61dbe4066b86419107a24c804a4e373ba59be699ebd8d5 \\\n    --hash=sha256:3dfa7c2f3fe55db34eba90c29df94bcdac4821043fc391cb5d082d9922013869 \\\n    --hash=sha256:45b6092997ceb8aa3801693781a71a99909ab9cc776fbc3fa9322d29b1d3bef2 \\\n    --hash=sha256:46ff2116150ae70a4e9c490d2ab6b6e1b1b93f25e520e540abe1b81b48560c3a \\\n    --hash=sha256:5c5b5d9da594f638a259fca766046f44c97244cc7ab8bef161b3e80d04becc76 \\\n    --hash=sha256:6d0d6a8ee2215a440e1296b843edf44fd377b055ba350eaba74655a2fe2c4bae \\\n    --hash=sha256:78abdddd03a406646107f973c7843276e7b64e5e32623529dc17f3d94a20f589 \\\n    --hash=sha256:86016d40bcaa3bcc9a56cd74d97e654b5f4f4abe42b038c71e4f00a089c4526c \\\n    --hash=sha256:990833f70a5f9c7d3fc82c94507f03179930ff7d00941c287f73b6fcbf67f19e \\\n    --hash=sha256:a00e737948791b94dad83cf0eafc09a02c4d8c2171a239e8c8572fe04e25960e \\\n    --hash=sha256:cb4a8d98ba12fa016f4241932b1fc5e702e5143f5374bba0bbcf7ddc1c4cf2b8 \\\n    --hash=sha256:d3a06fae62418ec8e5c635b61a8086032c9e281f16c63c3af46a6efbab33156f \\\n    --hash=sha256:fe55c039d97090d1f85277d402954dd6ad27f63034fa81985a9cc59655ac3ee2\n    # via transformers\nscikit-image==0.24.0 \\\n    --hash=sha256:18836a18d3a7b6aca5376a2d805f0045826bc6c9fc85331659c33b4813e0b563 \\\n    --hash=sha256:190ebde80b4470fe8838764b9b15f232a964f1a20391663e31008d76f0c696f7 \\\n    --hash=sha256:272909e02a59cea3ed4aa03739bb88df2625daa809f633f40b5053cf09241831 \\\n    --hash=sha256:39ee0af13435c57351a3397eb379e72164ff85161923eec0c38849fecf1b4764 \\\n    --hash=sha256:4688c18bd7ec33c08d7bf0fd19549be246d90d5f2c1d795a89986629af0a1e83 \\\n    --hash=sha256:56dab751d20b25d5d3985e95c9b4e975f55573554bd76b0aedf5875217c93e69 \\\n    --hash=sha256:59c98cc695005faf2b79904e4663796c977af22586ddf1b12d6af2fa22842dc2 \\\n    --hash=sha256:5d16efe95da8edbeb363e0c4157b99becbd650a60b77f6e3af5768b66cf007ab \\\n    --hash=sha256:5e37de6f4c1abcf794e13c258dc9b7d385d5be868441de11c180363824192ff7 \\\n    --hash=sha256:6fccceb54c9574590abcddc8caf6cefa57c13b5b8b4260ab3ff88ad8f3c252b3 \\\n    --hash=sha256:7ac7913b028b8aa780ffae85922894a69e33d1c0bf270ea1774f382fe8bf95e7 \\\n    --hash=sha256:82ab903afa60b2da1da2e6f0c8c65e7c8868c60a869464c41971da929b3e82bc \\\n    --hash=sha256:8579bda9c3f78cb3b3ed8b9425213c53a25fa7e994b7ac01f2440b395babf660 \\\n    --hash=sha256:93f46e6ce42e5409f4d09ce1b0c7f80dd7e4373bcec635b6348b63e3c886eac8 \\\n    --hash=sha256:9c7a52e20cdd760738da38564ba1fed7942b623c0317489af1a598a8dedf088b \\\n    --hash=sha256:cb3bc0264b6ab30b43c4179ee6156bc18b4861e78bb329dd8d16537b7bbf827a \\\n    --hash=sha256:ccc01e4760d655aab7601c1ba7aa4ddd8b46f494ac46ec9c268df6f33ccddf4c \\\n    --hash=sha256:dacf591ac0c272a111181afad4b788a27fe70d213cfddd631d151cbc34f8ca2c \\\n    --hash=sha256:e9aadb442360a7e76f0c5c9d105f79a83d6df0e01e431bd1d5757e2c5871a1f3 \\\n    --hash=sha256:ef04360eda372ee5cd60aebe9be91258639c86ae2ea24093fb9182118008d009 \\\n    --hash=sha256:fa27b3a0dbad807b966b8db2d78da734cb812ca4787f7fbb143764800ce2fa9c\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n```\n\n----------------------------------------\n\nTITLE: Running Llama 13B Fine-tuning\nDESCRIPTION: Command to launch fine-tuning for Llama-2 13B model with optional test mode\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./run_llama_ft.sh --size=13b [--as-test]\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies: googleapis-common-protos Configuration\nDESCRIPTION: Definition of the googleapis-common-protos package dependency with version 1.61.0 and corresponding SHA256 hashes. The comment indicates this package is required via python/requirements_compiled_ray_test_py311_cu121.txt, google-api-core, and opentelemetry-exporter-otlp-proto-grpc.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\ngoogleapis-common-protos==1.61.0 \\\n    --hash=sha256:22f1915393bb3245343f6efe87f6fe868532efc12aa26b391b15132e1279f1c0 \\\n    --hash=sha256:8a64866a97f6304a7179873a465d6eee97b7a24ec6cfd78e0f575e96b821240b\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   google-api-core\n    #   opentelemetry-exporter-otlp-proto-grpc\n```\n\n----------------------------------------\n\nTITLE: Specifying Py-Spy Package Dependency with Python Version Condition\nDESCRIPTION: Defines the py-spy package dependency at version 0.4.0 with a condition that it's only used for Python versions less than 3.12. This profiling tool is listed in the main requirements.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\npy-spy==0.4.0 ; python_full_version < '3.12' \\\n    --hash=sha256:47cdda4c34d9b6cb01f3aaeceb2e88faf57da880207fe72ff6ff97e9bb6cc8a9 \\\n    --hash=sha256:77d8f637ade38367d944874776f45b703b7ac5938b1f7be8891f3a5876ddbb96 \\\n    --hash=sha256:806602ce7972782cc9c1e383f339bfc27bfb822d42485e6a3e0530ae5040e1f0 \\\n    --hash=sha256:87573e64dbfdfc89ba2e0f5e2f525aa84e0299c7eb6454b47ea335fde583a7a0 \\\n    --hash=sha256:8bf2f3702cef367a489faa45177b41a6c31b2a3e5bd78c978d44e29340152f5a \\\n    --hash=sha256:c5f06ffce4c9c98b7fc9f5e67e5e7db591173f1351837633f3f23d9378b1d18a \\\n    --hash=sha256:eee3d0bde85ca5cf4f01f012d461180ca76c24835a96f7b5c4ded64eb6a008ab \\\n    --hash=sha256:f2cf3f7130e7d780471faa5957441d3b4e0ec39a79b2c00f4c33d494f7728428\n```\n\n----------------------------------------\n\nTITLE: Package Requirement: lm-format-enforcer with Hashes\nDESCRIPTION: Specifies the lm-format-enforcer package version 0.10.11 with SHA-256 hashes for verification. This package is referenced as a dependency in the rayllm test requirements file and is used by vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nlm-format-enforcer==0.10.11 \\\n    --hash=sha256:563e0dbc930a6d50fb687951506c5de098c6e962601be0ce723f3b7d0b916a1b \\\n    --hash=sha256:8ab371924e166a1df68f243aca73a8a647bea5909f37edd6a53a694e7e7c3274\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Building and Opening Documentation Locally\nDESCRIPTION: Command to compile the documentation and open it in a web browser. This builds the entire documentation site.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/README.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake develop && open _build/html/index.html\n```\n\n----------------------------------------\n\nTITLE: Monotonic Package Dependency with Hash Verification\nDESCRIPTION: This snippet specifies the monotonic package with version 1.6 and SHA256 hash verification codes. It indicates that this dependency is required by the gsutil package through a compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_12\n\nLANGUAGE: pip\nCODE:\n```\nmonotonic==1.6 \\\n    --hash=sha256:3a55207bcfed53ddd5c5bae174524062935efed17792e9de2ad0205ce9ad63f7 \\\n    --hash=sha256:68687e19a14f11f26d140dd5c86f3dba4bf5df58003000ed467e0e2a69bca96c\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gsutil\n```\n\n----------------------------------------\n\nTITLE: Specifying Packaging Package with Hash Verification in Bash\nDESCRIPTION: Defines the packaging package version 23.0 with SHA256 hash verification for secure package installation. Used by multiple other packages in the requirements.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n```\n\n----------------------------------------\n\nTITLE: Specifying Parso Package with Hash Verification in Bash\nDESCRIPTION: Defines the parso package version 0.8.3 with SHA256 hash verification for secure package installation. Required by jedi.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\nparso==0.8.3 \\\n    --hash=sha256:8c07be290bb59f03588915921e29e8a50002acaf2cdc5fa0e0114f91709fafa0 \\\n    --hash=sha256:c001d4636cd3aecdaf33cbb40aebb59b094be2a74c556778ef5576c175e19e75\n```\n\n----------------------------------------\n\nTITLE: Tailing CloudWatch Logs (Bash)\nDESCRIPTION: This Bash command uses the AWS CLI to tail logs from a specified CloudWatch log group, allowing real-time monitoring of Ray cluster logs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\naws logs tail $log_group_name --follow\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents Configuration\nDESCRIPTION: Restructured Text configuration for hidden table of contents including getting started guides, user guides, examples and references.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/index.md#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{toctree}\n:hidden:\n\ngetting-started\nUser Guides <user-guides/index>\nExamples <examples/index>\nreferences/index\n```\n\n----------------------------------------\n\nTITLE: Creating RST Documentation Toctree\nDESCRIPTION: ReStructuredText toctree directive listing advanced Ray documentation topics. Sets maximum depth to -1 for unlimited nesting of sections.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/advanced-topics.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: -1\n\n    tips-for-first-time\n    starting-ray\n    ray-generator\n    namespaces\n    cross-language\n    using-ray-with-jupyter\n    ray-dag\n    miscellaneous\n    runtime_env_auth\n    user-spawn-processes\n```\n\n----------------------------------------\n\nTITLE: Listing PyTZ Package with Hash Values\nDESCRIPTION: Definition for the PyTZ package dependency with version 2022.7.1 and corresponding SHA256 hash values. The comment indicates this is required by pandas and typepy packages through the compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_30\n\nLANGUAGE: text\nCODE:\n```\npytz==2022.7.1 \\\n    --hash=sha256:01a0681c4b9684a28304615eba55d1ab31ae00bf68ec157ec3708a8182dbbcd0 \\\n    --hash=sha256:78f4f37d8198e0627c5f1143240bb0206b8691d8d7ac6d78fee88b78733f8c4a\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   pandas\n    #   typepy\n```\n\n----------------------------------------\n\nTITLE: Specifying TBLib Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the tblib package dependency with version 3.0.0 and SHA-256 hashes for verification. Comments indicate this package is required by distributed and requirements_byod_3.9.in and is constrained by requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_44\n\nLANGUAGE: pip\nCODE:\n```\ntblib==3.0.0 \\\n    --hash=sha256:80a6c77e59b55e83911e1e607c649836a69c103963c5f28a46cbeef44acf8129 \\\n    --hash=sha256:93622790a0a29e04f0346458face1e144dc4d32f493714c6c3dff82a4adb77e6\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n    #   distributed\n```\n\n----------------------------------------\n\nTITLE: Experiment Management and Data Sync\nDESCRIPTION: Commands for listing experiments, synchronizing files between local and cluster environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nray exec CLUSTER.YAML 'tune ls ~/ray_results'\n\nray rsync-up CLUSTER.YAML\n\nray rsync-down CLUSTER.YAML '~/ray_results' ~/cluster_results\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Hash Values for Ray Project\nDESCRIPTION: A detailed listing of Python package dependencies with SHA256 hash values for secure package verification. The file shows version pinning and includes comments indicating which component requires each dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:3e0153a805a98f5ada7e09826255ba99fb4f7524bb81bf6b47fb702666484ae1 \\\n--hash=sha256:410478a0c562d1a5bcc2f7ea448359fcb050ed48b3c6f6f4f18c313a9bdb1826 \\\n--hash=sha256:442acde1e068288a4ba7acfe05f5f343e19fac87bfc96d89eb886b0363e977ec \\\n--hash=sha256:48f6a4533887e189dae092f1cf981f2e3885175f7a0f33c91fb5b7b682b6bab6 \\\n--hash=sha256:4f57dab5fe3407b6c0c1cc907ac98e8a189f9e418f3b6e54d65a718aaafe3950 \\\n--hash=sha256:4f9c515e7914626b2a2e1e311794b4c35720a0be87af52b79ff8e1429fc25f19 \\\n--hash=sha256:55fdc093b5a3cb41d420884cdaf37a1e74c3c37a31f46e66286d9145d2063bd0 \\\n--hash=sha256:5667ed53d68d91920defdf4035d1cdaa3c3121dc0b113255124bcfada1cfa1b8 \\\n--hash=sha256:590344787a90ae57d62511dd7c736ed56b428f04cd8c161fcc5e7232c130c69a \\\n--hash=sha256:5a7d70357e7cee13f470c7883a063aae5fe209a493c57d86eb7f5a6f910fae09 \\\n--hash=sha256:5c3894db91f5a489fc8fa6a9991820f368f0b3cbdb9cd8849547ccfab3392d86 \\\n--hash=sha256:5c849d495bf5154cd8da18a9eb15db127d4dba2968d88831aff6f0331ea9bd4c \\\n--hash=sha256:64536573d0a2cb6e625cf309984e2d873979709f2cf22839bf2d61790b448ad5 \\\n--hash=sha256:693945278a31f2086d9bf3df0fe8254bbeaef1fe71e1351c3bd730aa7d31c41b \\\n--hash=sha256:6db4667b187a6742b33afbbaf05a7bc551ffcf1ced0000a571aedbb4aa42fc7b \\\n--hash=sha256:6eb73fa5426ea69ee0e012fb59cdc76a15b1283d6e32e4f8dc4482ec67d1194d \\\n--hash=sha256:722e1124aec435320ae01ee3ac7bec11a5d47f25d0ed6328f2273d287bc3abb0 \\\n--hash=sha256:7268252af60904bf52c26173cbadc3a071cece75f873705419c8681f24d3edea \\\n--hash=sha256:74fb4bee6880b529a0c6560885fce4dc95936920f9f20f53d99a213f7bf66776 \\\n--hash=sha256:780d3a35680ced9ce682fbcf4cb9c2bad3136eeff760ab33707b71db84664e3a \\\n--hash=sha256:82e8211d69a4f4bc360ea22cd6555f8e61a1bd211d1d5d39d3d228b48c83a897 \\\n--hash=sha256:89aa2c2eeb20957be2d950b85974b30a01a762f3308cd02bb15e1ad632e22dc7 \\\n--hash=sha256:8aefbba5f69d42246543407ed2461db31006b0f76c4e32dfd6f42215a2c41d09 \\\n--hash=sha256:96ec70beabbd3b10e8bfe52616a13561e58fe84c0101dd031dc78f250d5128b9 \\\n--hash=sha256:9750cc7fe1ae3b1611bb8cfc3f9ec11d532244235d75901fb6b8e42ce9229dfe \\\n--hash=sha256:9acbb16f06fe7f52f441bb6f413ebae6c37baa6ef9edd49cdd567216da8600cd \\\n--hash=sha256:9d3e0c25a2350080e9319724dede4f31f43a6c9779be48021a7f4ebde8b2d742 \\\n--hash=sha256:a06339f38e9ed3a64e4c4e43aec7f59084033647f908e4259d279a52d3757d09 \\\n--hash=sha256:a0cb6f11204443f27a1628b0e460f37fb30f624be6051d490fa7d7e26d4af3d0 \\\n--hash=sha256:a7496bfe1da7fb1a4e1cc23bb67c58fab69311cc7d32b5a99c2007b4b2a0e932 \\\n--hash=sha256:a828c57f00f729620a442881cc60e57cfcec6842ba38e1b19fd3e47ac0ff8dc1 \\\n--hash=sha256:a9b2de4cf0cdd5bd2dee4c4f63a653c61d2408055ab77b151c1957f221cabf2a \\\n--hash=sha256:b46c8ae3a8f1f41a0d2ef350c0b6e65822d80772fe46b653ab6b6274f61d4a49 \\\n--hash=sha256:b7e3ed87d4138356775346e6845cccbe66cd9e207f3cd11d2f0b9fd13681359d \\\n--hash=sha256:b7f2f9f912dca3934c1baec2e4585a674ef16fe00218d833856408c48d5beee7 \\\n--hash=sha256:ba60bb19387e13597fb059f32cd4d59445d7b18b69a745b8f8e5db0346f33480 \\\n--hash=sha256:beee944ae828747fd7cb216a70f120767fc9f4f00bacae8543c14a6831673f89 \\\n--hash=sha256:bfa4a17e17ce9abf47a74ae02f32d014c5e9404b6d9ac7f729e01562bbee601e \\\n--hash=sha256:c037a86e8513059a2613aaba4d817bb90b9d9b6b69aace3ce9c877e8c8ed402b \\\n--hash=sha256:c302220494f5c1ebeb0912ea782bcd5e2f8308037b3c7553fad0e48ebad6ad82 \\\n--hash=sha256:c6321c9efe29975232da3bd0af0ad216800a47e93d763ce64f291917a381b8eb \\\n--hash=sha256:c757a9dd70d72b076d6f68efdbb9bc943665ae954dad2801b874c8c69e185068 \\\n--hash=sha256:c99169d4ff810155ca50b4da3b075cbde79752443117d89429595c2e8e37fed8 \\\n--hash=sha256:c9c92be9fd329ac801cc420e08452b70e7aeab94ea4233a4804f0915c14eba9b \\\n--hash=sha256:cc7b01b3754ea68a62bd77ce6020afaffb44a590c2289089289363472d13aedb \\\n--hash=sha256:db9e724bebd621d9beca794f2a4ff1d26eed5965b004a97f1f1685a173b869c2 \\\n--hash=sha256:dca69045298ce5c11fd539682cff879cc1e664c245d1c64da929813e54241d11 \\\n--hash=sha256:dd9b1baec094d91bf36ec729445f7769d0d0cf6b64d04d86e45baf89e2b9059b \\\n--hash=sha256:e02a0e11cf6597299b9f3bbd3f93d79217cb90cfd1411aec33848b13f5c656cc \\\n--hash=sha256:e6a20a581f9ce92d389a8c7d7c3dd47c81fd5d6e655c8dddf341e14aa48659d0 \\\n--hash=sha256:e7004be74cbb7d9f34553a5ce5fb08be14fb33bc86f332fb71cbe5216362a497 \\\n--hash=sha256:e774d53b1a477a67838a904131c4b0eef6b3d8a651f8b138b04f748fccfefe17 \\\n--hash=sha256:edb678da49d9f72c9f6c609fbe41a5dfb9a9282f9e6a2253d5a91e0fc382d7c0 \\\n--hash=sha256:f146e0911cb2f1da549fc58fc7bcd2b836a44b79ef871980d605ec392ff6b0d2 \\\n--hash=sha256:f56e2333dda1fe0f909e7cc59f021eba0d2307bc6f012a1ccf2beca6ba362439 \\\n--hash=sha256:f9a3ea26252bd92f570600098783d1371354d89d5f6b7dfd87359d669f2109b5 \\\n--hash=sha256:f9aa1878d1083b276b0196f2dfbe00c9b7e752475ed3b682025ff20c1c1f51ac \\\n--hash=sha256:fb3c2db03683b5767dedb5769b8a40ebb47d6f7f45b1b3e3b4b51ec8ad9d9825 \\\n--hash=sha256:fbeb989b5cc29e8daf7f976b421c220f1b8c731cbf22b9130d8815418ea45887 \\\n--hash=sha256:fde5bd59ab5357e3853313127f4d3565fc7dad314a74d7b5d43c22c6a5ed2ced \\\n--hash=sha256:fe1a06da377e3a1062ae5fe0926e12b84eceb8a50b350ddca72dc85015873f74\n# via\n#   -c /tmp/ray-deps/requirements_compiled.txt\n#   aiohttp\n#   aiosignal\n```\n\n----------------------------------------\n\nTITLE: Documenting Ray Timeline Command in reStructuredText\nDESCRIPTION: This snippet uses the click directive to generate documentation for the 'ray timeline' command from the ray.scripts.scripts module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/cli.rst#2025-04-12_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. click:: ray.scripts.scripts:timeline\n   :prog: ray timeline\n   :show-nested:\n```\n\n----------------------------------------\n\nTITLE: Specifying Packaging Dependency with Version Constraint\nDESCRIPTION: Dependency specification for the Packaging library which provides utilities for working with Python package metadata. Used for handling package versions and requirements in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_22\n\nLANGUAGE: pip\nCODE:\n```\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n```\n\n----------------------------------------\n\nTITLE: Specifying pandas 1.5.3 Dependency with Hash Verification\nDESCRIPTION: Defines the pandas package at version 1.5.3 with SHA256 hash verification values for security. This is imported from requirements_compiled_rayllm_test_py311_cu121.txt and requirements.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    --hash=sha256:41179ce559943d83a9b4bbacb736b04c928b095b5f25dd2b7389eda08f46f373 \\\n    --hash=sha256:478ff646ca42b20376e4ed3fa2e8d7341e8a63105586efe54fa2508ee087f328 \\\n    --hash=sha256:50869a35cbb0f2e0cd5ec04b191e7b12ed688874bd05dd777c19b28cbea90996 \\\n    --hash=sha256:565fa34a5434d38e9d250af3c12ff931abaf88050551d9fbcdfafca50d62babf \\\n    --hash=sha256:5f2b952406a1588ad4cad5b3f55f520e82e902388a6d5a4a91baa8d38d23c7f6 \\\n    --hash=sha256:5fbcb19d6fceb9e946b3e23258757c7b225ba450990d9ed63ccceeb8cae609f7 \\\n    --hash=sha256:6973549c01ca91ec96199e940495219c887ea815b2083722821f1d7abfa2b4dc \\\n    --hash=sha256:74a3fd7e5a7ec052f183273dc7b0acd3a863edf7520f5d3a1765c04ffdb3b0b1 \\\n    --hash=sha256:7a0a56cef15fd1586726dace5616db75ebcfec9179a3a55e78f72c5639fa2a23 \\\n    --hash=sha256:7cec0bee9f294e5de5bbfc14d0573f65526071029d036b753ee6507d2a21480a \\\n    --hash=sha256:87bd9c03da1ac870a6d2c8902a0e1fd4267ca00f13bc494c9e5a9020920e1d51 \\\n    --hash=sha256:972d8a45395f2a2d26733eb8d0f629b2f90bebe8e8eddbb8829b180c09639572 \\\n    --hash=sha256:9842b6f4b8479e41968eced654487258ed81df7d1c9b7b870ceea24ed9459b31 \\\n    --hash=sha256:9f69c4029613de47816b1bb30ff5ac778686688751a5e9c99ad8c7031f6508e5 \\\n    --hash=sha256:a50d9a4336a9621cab7b8eb3fb11adb82de58f9b91d84c2cd526576b881a0c5a \\\n    --hash=sha256:bc4c368f42b551bf72fac35c5128963a171b40dce866fb066540eeaf46faa003 \\\n    --hash=sha256:c39a8da13cede5adcd3be1182883aea1c925476f4e84b2807a46e2775306305d \\\n    --hash=sha256:c3ac844a0fe00bfaeb2c9b51ab1424e5c8744f89860b138434a363b1f620f354 \\\n    --hash=sha256:c4c00e0b0597c8e4f59e8d461f797e5d70b4d025880516a8261b2817c47759ee \\\n    --hash=sha256:c74a62747864ed568f5a82a49a23a8d7fe171d0c69038b38cedf0976831296fa \\\n    --hash=sha256:dd05f7783b3274aa206a1af06f0ceed3f9b412cf665b7247eacd83be41cf7bf0 \\\n    --hash=sha256:dfd681c5dc216037e0b0a2c821f5ed99ba9f03ebcf119c7dac0e9a7b960b9ec9 \\\n    --hash=sha256:e474390e60ed609cec869b0da796ad94f420bb057d86784191eefc62b65819ae \\\n    --hash=sha256:f76d097d12c82a535fda9dfe5e8dd4127952b45fea9b0276cb30cca5ea313fbc\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with Hash Verification\nDESCRIPTION: Detailed package dependency specifications for Ray, including exact versions and SHA-256 hashes for security verification. This snippet includes aiohappyeyeballs, aiohttp, aiohttp-cors, aiorwlock, aiosignal, airportsdata and other dependencies with origin comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   aiohttp\naiohttp==3.11.16 \\\n    --hash=sha256:004511d3413737700835e949433536a2fe95a7d0297edd911a1e9705c5b5ea43 \\\n    --hash=sha256:0902e887b0e1d50424112f200eb9ae3dfed6c0d0a19fc60f633ae5a57c809656 \\\n    --hash=sha256:09b00dd520d88eac9d1768439a59ab3d145065c91a8fab97f900d1b5f802895e \\\n    --hash=sha256:0a2f451849e6b39e5c226803dcacfa9c7133e9825dcefd2f4e837a2ec5a3bb98 \\\n    --hash=sha256:0a950c2eb8ff17361abd8c85987fd6076d9f47d040ebffce67dce4993285e973 \\\n    --hash=sha256:0ad1fb47da60ae1ddfb316f0ff16d1f3b8e844d1a1e154641928ea0583d486ed \\\n    --hash=sha256:13ceac2c5cdcc3f64b9015710221ddf81c900c5febc505dbd8f810e770011540 \\\n    --hash=sha256:14461157d8426bcb40bd94deb0450a6fa16f05129f7da546090cebf8f3123b0f \\\n    --hash=sha256:16f8a2c9538c14a557b4d309ed4d0a7c60f0253e8ed7b6c9a2859a7582f8b1b8 \\\n    --hash=sha256:17ae4664031aadfbcb34fd40ffd90976671fa0c0286e6c4113989f78bebab37a \\\n    --hash=sha256:1ce63ae04719513dd2651202352a2beb9f67f55cb8490c40f056cea3c5c355ce \\\n    --hash=sha256:23a15727fbfccab973343b6d1b7181bfb0b4aa7ae280f36fd2f90f5476805682 \\\n    --hash=sha256:2540ddc83cc724b13d1838026f6a5ad178510953302a49e6d647f6e1de82bc34 \\\n    --hash=sha256:37dcee4906454ae377be5937ab2a66a9a88377b11dd7c072df7a7c142b63c37c \\\n    --hash=sha256:38bea84ee4fe24ebcc8edeb7b54bf20f06fd53ce4d2cc8b74344c5b9620597fd \\\n    --hash=sha256:3ab3367bb7f61ad18793fea2ef71f2d181c528c87948638366bf1de26e239183 \\\n    --hash=sha256:3ad1d59fd7114e6a08c4814983bb498f391c699f3c78712770077518cae63ff7 \\\n    --hash=sha256:3b4e6db8dc4879015b9955778cfb9881897339c8fab7b3676f8433f849425913 \\\n    --hash=sha256:3e061b09f6fa42997cf627307f220315e313ece74907d35776ec4373ed718b86 \\\n    --hash=sha256:42864e70a248f5f6a49fdaf417d9bc62d6e4d8ee9695b24c5916cb4bb666c802 \\\n    --hash=sha256:493910ceb2764f792db4dc6e8e4b375dae1b08f72e18e8f10f18b34ca17d0979 \\\n    --hash=sha256:4d0c970c0d602b1017e2067ff3b7dac41c98fef4f7472ec2ea26fd8a4e8c2149 \\\n    --hash=sha256:54eb3aead72a5c19fad07219acd882c1643a1027fbcdefac9b502c267242f955 \\\n    --hash=sha256:56a3443aca82abda0e07be2e1ecb76a050714faf2be84256dae291182ba59049 \\\n    --hash=sha256:576f5ca28d1b3276026f7df3ec841ae460e0fc3aac2a47cbf72eabcfc0f102e1 \\\n    --hash=sha256:58ede86453a6cf2d6ce40ef0ca15481677a66950e73b0a788917916f7e35a0bb \\\n    --hash=sha256:61c721764e41af907c9d16b6daa05a458f066015abd35923051be8705108ed17 \\\n    --hash=sha256:634d96869be6c4dc232fc503e03e40c42d32cfaa51712aee181e922e61d74814 \\\n    --hash=sha256:696ef00e8a1f0cec5e30640e64eca75d8e777933d1438f4facc9c0cdf288a810 \\\n    --hash=sha256:69a2cbd61788d26f8f1e626e188044834f37f6ae3f937bd9f08b65fc9d7e514e \\\n    --hash=sha256:6a792ce34b999fbe04a7a71a90c74f10c57ae4c51f65461a411faa70e154154e \\\n    --hash=sha256:6ac13b71761e49d5f9e4d05d33683bbafef753e876e8e5a7ef26e937dd766713 \\\n    --hash=sha256:6fdec0213244c39973674ca2a7f5435bf74369e7d4e104d6c7473c81c9bcc8c4 \\\n    --hash=sha256:72b1b03fb4655c1960403c131740755ec19c5898c82abd3961c364c2afd59fe7 \\\n    --hash=sha256:745f1ed5e2c687baefc3c5e7b4304e91bf3e2f32834d07baaee243e349624b24 \\\n    --hash=sha256:776c8e959a01e5e8321f1dec77964cb6101020a69d5a94cd3d34db6d555e01f7 \\\n    --hash=sha256:780df0d837276276226a1ff803f8d0fa5f8996c479aeef52eb040179f3156cbd \\\n    --hash=sha256:78e6e23b954644737e385befa0deb20233e2dfddf95dd11e9db752bdd2a294d3 \\\n    --hash=sha256:7951decace76a9271a1ef181b04aa77d3cc309a02a51d73826039003210bdc86 \\\n    --hash=sha256:7ba92a2d9ace559a0a14b03d87f47e021e4fa7681dc6970ebbc7b447c7d4b7cd \\\n    --hash=sha256:7f6428fee52d2bcf96a8aa7b62095b190ee341ab0e6b1bcf50c615d7966fd45b \\\n    --hash=sha256:87944bd16b7fe6160607f6a17808abd25f17f61ae1e26c47a491b970fb66d8cb \\\n    --hash=sha256:87a6e922b2b2401e0b0cf6b976b97f11ec7f136bfed445e16384fbf6fd5e8602 \\\n    --hash=sha256:8cb0688a8d81c63d716e867d59a9ccc389e97ac7037ebef904c2b89334407180 \\\n    --hash=sha256:8df6612df74409080575dca38a5237282865408016e65636a76a2eb9348c2567 \\\n    --hash=sha256:911a6e91d08bb2c72938bc17f0a2d97864c531536b7832abee6429d5296e5b27 \\\n    --hash=sha256:92b7ee222e2b903e0a4b329a9943d432b3767f2d5029dbe4ca59fb75223bbe2e \\\n    --hash=sha256:938f756c2b9374bbcc262a37eea521d8a0e6458162f2a9c26329cc87fdf06534 \\\n    --hash=sha256:9756d9b9d4547e091f99d554fbba0d2a920aab98caa82a8fb3d3d9bee3c9ae85 \\\n    --hash=sha256:98b88a2bf26965f2015a771381624dd4b0839034b70d406dc74fd8be4cc053e3 \\\n    --hash=sha256:9b751a6306f330801665ae69270a8a3993654a85569b3469662efaad6cf5cc50 \\\n    --hash=sha256:a2a450bcce4931b295fc0848f384834c3f9b00edfc2150baafb4488c27953de6 \\\n    --hash=sha256:a3814760a1a700f3cfd2f977249f1032301d0a12c92aba74605cfa6ce9f78489 \\\n    --hash=sha256:a5abcbba9f4b463a45c8ca8b7720891200658f6f46894f79517e6cd11f3405ca \\\n    --hash=sha256:a6db7458ab89c7d80bc1f4e930cc9df6edee2200127cfa6f6e080cf619eddfbd \\\n    --hash=sha256:ad497f38a0d6c329cb621774788583ee12321863cd4bd9feee1effd60f2ad133 \\\n    --hash=sha256:ad9509ffb2396483ceacb1eee9134724443ee45b92141105a4645857244aecc8 \\\n    --hash=sha256:bbcba75fe879ad6fd2e0d6a8d937f34a571f116a0e4db37df8079e738ea95c71 \\\n    --hash=sha256:c10d85e81d0b9ef87970ecbdbfaeec14a361a7fa947118817fcea8e45335fa46 \\\n    --hash=sha256:c15b2271c44da77ee9d822552201180779e5e942f3a71fb74e026bf6172ff287 \\\n    --hash=sha256:ca37057625693d097543bd88076ceebeb248291df9d6ca8481349efc0b05dcd0 \\\n    --hash=sha256:cc3a145479a76ad0ed646434d09216d33d08eef0d8c9a11f5ae5cdc37caa3540 \\\n    --hash=sha256:ccf10f16ab498d20e28bc2b5c1306e9c1512f2840f7b6a67000a517a4b37d5ee \\\n    --hash=sha256:cd464ba806e27ee24a91362ba3621bfc39dbbb8b79f2e1340201615197370f7c \\\n    --hash=sha256:d007aa39a52d62373bd23428ba4a2546eed0e7643d7bf2e41ddcefd54519842c \\\n    --hash=sha256:d0666afbe984f6933fe72cd1f1c3560d8c55880a0bdd728ad774006eb4241ecd \\\n    --hash=sha256:d07502cc14ecd64f52b2a74ebbc106893d9a9717120057ea9ea1fd6568a747e7 \\\n    --hash=sha256:d489d9778522fbd0f8d6a5c6e48e3514f11be81cb0a5954bdda06f7e1594b321 \\\n    --hash=sha256:df7db76400bf46ec6a0a73192b14c8295bdb9812053f4fe53f4e789f3ea66bbb \\\n    --hash=sha256:e3538bc9fe1b902bef51372462e3d7c96fce2b566642512138a480b7adc9d508 \\\n    --hash=sha256:e87fd812899aa78252866ae03a048e77bd11b80fb4878ce27c23cade239b42b2 \\\n    --hash=sha256:ecdb8173e6c7aa09eee342ac62e193e6904923bd232e76b4157ac0bfa670609f \\\n    --hash=sha256:f244b8e541f414664889e2c87cac11a07b918cb4b540c36f7ada7bfa76571ea2 \\\n    --hash=sha256:f4065145bf69de124accdd17ea5f4dc770da0a6a6e440c53f6e0a8c27b3e635c \\\n    --hash=sha256:f420bfe862fb357a6d76f2065447ef6f484bc489292ac91e29bc65d2d7a2c84d \\\n    --hash=sha256:f6ddd90d9fb4b501c97a4458f1c1720e42432c26cb76d28177c5b5ad4e332601 \\\n    --hash=sha256:fa73e8c2656a3653ae6c307b3f4e878a21f87859a9afab228280ddccd7369d71 \\\n    --hash=sha256:fadbb8f1d4140825069db3fedbbb843290fd5f5bc0a5dbd7eaf81d91bf1b003b \\\n    --hash=sha256:fb3d0cc5cdb926090748ea60172fa8a213cec728bd6c54eae18b96040fcd6227 \\\n    --hash=sha256:fb46bb0f24813e6cede6cc07b1961d4b04f331f7112a23b5e21f567da4ee50aa \\\n    --hash=sha256:fd36c119c5d6551bce374fcb5c19269638f8d09862445f85a5a48596fd59f4bb\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   aiohttp-cors\n    #   vllm\naiohttp-cors==0.7.0 \\\n    --hash=sha256:0451ba59fdf6909d0e2cd21e4c0a43752bc0703d33fc78ae94d9d9321710193e \\\n    --hash=sha256:4d39c6d7100fd9764ed1caf8cebf0eb01bf5e3f24e2e073fda6234bc48b19f5d\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\naiorwlock==1.3.0 \\\n    --hash=sha256:45baf8e4fa9a23e0bb325fbd67da80de1fd7ae1d4f59a6381754c60cec7b289b \\\n    --hash=sha256:83f12d87df4b9728a0b8fda1756585ab0d652b107bab59c6084e1b1ad692ab45\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\naiosignal==1.3.1 \\\n    --hash=sha256:54cd96e15e1649b75d6c87526a6ff0b6c1b0dd3459f43d9ca11d48c339b68cfc \\\n    --hash=sha256:f8376fb07dd1e86a584e4fcdec80b36b7f81aac666ebc724e2c090300dd83b17\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   aiohttp\n    #   ray\nairportsdata==20241001 \\\n    --hash=sha256:67d71cf2c5378cc17ff66b62b1e11aa2444043949c894543ac8fd8dafce192fd \\\n    --hash=sha256:fa0bd143b4f4be3557cb892fa0612ef210fd91a92bd720b4d8221de576a4fa00\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   outlines\nannotated-types==0.6.0 \\\n    --hash=sha256:0641064de18ba7a25dee8f96403ebc39113d0cb953a01429249d5c7564666a43 \\\n    --hash=sha256:563339e807e53ffd9c267e99fc6d9ea23eb8443c08f112651963e24e22f84a5d\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   pydantic\nanyio==3.7.1 \\\n    --hash=sha256:44a3c9aba0f5defa43261a8b3efb97891f2bd7d804e0e1f56419befa1adfc780 \\\n    --hash=sha256:91dee416e570e92c64041bd18b900d1d6fa78dff7048769ce5ac5ddad004fbb5\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   httpx\n    #   openai\n    #   starlette\n    #   watchfiles\nastor==0.8.1 \\\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and SHA256 hashes. It includes entries for mdurl, memray, and msgpack packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_12\n\nLANGUAGE: Text\nCODE:\n```\nmdurl==0.1.2 \\\n    --hash=sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8 \\\n    --hash=sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   markdown-it-py\nmemray==1.10.0 ; sys_platform != 'win32' \\\n    --hash=sha256:0a21745fb516b7a6efcd40aa7487c59e9313fcfc782d0193fcfcf00b48426874 \\\n    --hash=sha256:22f2a47871c172a0539bd72737bb6b294fc10c510464066b825d90fcd3bb4916 \\\n    --hash=sha256:23e8c402625cfb32d0e9edb5ec0945f3e5e54bc6b0c5699f6284302082b80bd4\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\nmsgpack==1.0.7 \\\n    --hash=sha256:04ad6069c86e531682f9e1e71b71c1c3937d6014a7c3e9edd2aa81ad58842862 \\\n    --hash=sha256:0bfdd914e55e0d2c9e1526de210f6fe8ffe9705f2b1dfcc4aecc92a4cb4b533d \\\n    --hash=sha256:1dc93e8e4653bdb5910aed79f11e165c85732067614f180f70534f056da97db3\n```\n\n----------------------------------------\n\nTITLE: Package Definition for msgspec in Python\nDESCRIPTION: Defines the msgspec package at version 0.19.0 with its cryptographic hash values for verification. The hash values ensure package integrity during installation. The comment indicates this package is required by vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\nmsgspec==0.19.0 \\\n    --hash=sha256:00e87ecfa9795ee5214861eab8326b0e75475c2e68a384002aa135ea2a27d909 \\\n    --hash=sha256:047cfa8675eb3bad68722cfe95c60e7afabf84d1bd8938979dd2b92e9e4a9551 \\\n    --hash=sha256:0553bbc77662e5708fe66aa75e7bd3e4b0f209709c48b299afd791d711a93c36 \\\n    --hash=sha256:067f0de1c33cfa0b6a8206562efdf6be5985b988b53dd244a8e06f993f27c8c0 \\\n    --hash=sha256:0684573a821be3c749912acf5848cce78af4298345cb2d7a8b8948a0a5a27cfe \\\n    --hash=sha256:0f5c043ace7962ef188746e83b99faaa9e3e699ab857ca3f367b309c8e2c6b12 \\\n    --hash=sha256:15c1e86fff77184c20a2932cd9742bf33fe23125fa3fcf332df9ad2f7d483044 \\\n    --hash=sha256:19746b50be214a54239aab822964f2ac81e38b0055cca94808359d779338c10e \\\n    --hash=sha256:2719647625320b60e2d8af06b35f5b12d4f4d281db30a15a1df22adb2295f633 \\\n    --hash=sha256:317050bc0f7739cb30d257ff09152ca309bf5a369854bbf1e57dffc310c1f20f \\\n    --hash=sha256:3b5541b2b3294e5ffabe31a09d604e23a88533ace36ac288fa32a420aa38d229 \\\n    --hash=sha256:3be5c02e1fee57b54130316a08fe40cca53af92999a302a6054cd451700ea7db \\\n    --hash=sha256:3c4ec642689da44618f68c90855a10edbc6ac3ff7c1d94395446c65a776e712a \\\n    --hash=sha256:43bbb237feab761b815ed9df43b266114203f53596f9b6e6f00ebd79d178cdf2 \\\n    --hash=sha256:45c8fb410670b3b7eb884d44a75589377c341ec1392b778311acdbfa55187716 \\\n    --hash=sha256:4cfc033c02c3e0aec52b71710d7f84cb3ca5eb407ab2ad23d75631153fdb1f12 \\\n    --hash=sha256:5f0f65f29b45e2816d8bded36e6b837a4bf5fb60ec4bc3c625fa2c6da4124537 \\\n    --hash=sha256:604037e7cd475345848116e89c553aa9a233259733ab51986ac924ab1b976f8e \\\n    --hash=sha256:60ef4bdb0ec8e4ad62e5a1f95230c08efb1f64f32e6e8dd2ced685bcc73858b5 \\\n    --hash=sha256:695b832d0091edd86eeb535cd39e45f3919f48d997685f7ac31acb15e0a2ed90 \\\n    --hash=sha256:6c7adf191e4bd3be0e9231c3b6dc20cf1199ada2af523885efc2ed218eafd011 \\\n    --hash=sha256:70eaef4934b87193a27d802534dc466778ad8d536e296ae2f9334e182ac27b6c \\\n    --hash=sha256:757b501fa57e24896cf40a831442b19a864f56d253679f34f260dcb002524a6c \\\n    --hash=sha256:82b2c42c1b9ebc89e822e7e13bbe9d17ede0c23c187469fdd9505afd5a481314 \\\n    --hash=sha256:a5bc1472223a643f5ffb5bf46ccdede7f9795078194f14edd69e3aab7020d327 \\\n    --hash=sha256:aa77046904db764b0462036bc63ef71f02b75b8f72e9c9dd4c447d6da1ed8f8e \\\n    --hash=sha256:ac7f7c377c122b649f7545810c6cd1b47586e3aa3059126ce3516ac7ccc6a6a9 \\\n    --hash=sha256:ca06aa08e39bf57e39a258e1996474f84d0dd8130d486c00bec26d797b8c5446 \\\n    --hash=sha256:d8dd848ee7ca7c8153462557655570156c2be94e79acec3561cf379581343259 \\\n    --hash=sha256:d911c442571605e17658ca2b416fd8579c5050ac9adc5e00c2cb3126c97f73bc \\\n    --hash=sha256:e695dad6897896e9384cf5e2687d9ae9feaef50e802f93602d35458e20d1fb19 \\\n    --hash=sha256:e78f46ff39a427e10b4a61614a2777ad69559cc8d603a7c05681f5a595ea98f7 \\\n    --hash=sha256:f04cad4385e20be7c7176bb8ae3dca54a08e9756cfc97bcdb4f18560c3042063 \\\n    --hash=sha256:f12d30dd6266557aaaf0aa0f9580a9a8fbeadfa83699c487713e355ec5f0bd86 \\\n    --hash=sha256:f98bd8962ad549c27d63845b50af3f53ec468b6318400c9f1adfe8b092d7b62f \\\n    --hash=sha256:fe2c4bf29bf4e89790b3117470dea2c20b59932772483082c468b990d45fb947\n    # via vllm\n```\n\n----------------------------------------\n\nTITLE: Listing SHA256 Hashes for Python Package Dependency\nDESCRIPTION: This snippet contains a list of SHA256 hashes for a Python package dependency. Each hash is preceded by the '--hash=sha256:' flag, which is commonly used in pip requirements files to ensure package integrity. The hashes are likely for different versions or builds of the same package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_24\n\nLANGUAGE: Text\nCODE:\n```\n--hash=sha256:c69697d3adff5aa4f874b19c0e4ed65180ceed6318ec856ebc423aa5850d84f7 \\\n--hash=sha256:c7d79f7d9aabd6011004e33b22bc13056a3e3fb54794d138af57f5ee9d9032cb \\\n--hash=sha256:ccaa3a4b521b780a7e771cc336a2dba389a0861592bbce09a476190bb0c8b4b3 \\\n--hash=sha256:ccd17349166b1bee6e529b4add61727d3f55edb7babbe4069b5764c9587a8cc6 \\\n--hash=sha256:ce1af883b94304f493698b00d0f006d56aea98aeb49d75ec7d98cd4a777e9285 \\\n--hash=sha256:d0e883008013c0e4aef84dcfe2a0b172c4d23c2669412cf5b3371003941f72bb \\\n--hash=sha256:d980e0325b6eddc81331d3f4551e2a333999fb176fd153e075c6d1c2530aa8a8 \\\n--hash=sha256:e17c9361d46a4d5addf777c6dd5eab0715a7684c2f11b88c67ac37edfba6c482 \\\n--hash=sha256:e2c08cc9b16f4f4bc522771d96734c7901e7ebef70c6c5c35dd0f10845270bcd \\\n--hash=sha256:e35ef8683211db69ffe129a25d5634319a677570ab6b2eba4afa860f54eeaf75 \\\n--hash=sha256:e3b9fd71836999aad54084906f8663dffcd2a7fb5cdafd6c37713b2e72be1760 \\\n--hash=sha256:ef9f7768395923c3039055c14334ba4d926f3baf7b776c923c93d80195624782 \\\n--hash=sha256:f52a265001d830bc425f82ca9eabda94a64a4d753b07d623a9f2863fde532b53 \\\n--hash=sha256:f91c4803173928a25e1a55b943c81f55b8872f0018be83e3ad4938adffb77dd2 \\\n--hash=sha256:fbd6748e8ab9b41171bb95c6142faf068f5ef1511935a0aa07025438dd9a9bc1 \\\n--hash=sha256:fe57328fbc1bfd0bd0514470ac692630f3901c0ee39052ae47acd1d90a436719 \\\n--hash=sha256:fea09ca13323376a2fdfb353a5fa2e59f90cd18d7ca4eaa1fd31f0a8b4f91e62\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies with Hash Verification\nDESCRIPTION: This snippet shows a portion of a pip requirements file that lists Python package dependencies with their specific versions and SHA256 hash values for security and reproducibility. Comments indicate which packages depend on these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n    --hash=sha256:ca80b121bbec76d7794fcb45e65a7eca660a76cc1a104ed439cdbd7df5f0b060 \\\n    --hash=sha256:cdf66977a976d6a3cfb006afdf825d1482f84f7b81179db33941f2fc9673bb1d \\\n    --hash=sha256:d4faf846ed132fd7ebfbbf4fde588a62d21faa0faa06e6f468b7faa6f436b661 \\\n    --hash=sha256:d7f87c2c02e03d99b95cfa6f7a776409083a9e4d468912e18c7680437b29222c \\\n    --hash=sha256:dd23df885318391856415e20acfd51a985cba6919f0be78ed89f5db9ff3a31cb \\\n    --hash=sha256:f5de3c676e57177b38857f6e3cdfbe8f38d1cd754b63200c0615eaa31f514b4f \\\n    --hash=sha256:f5e8e8d60e18d5f7fd49983f0c4696deeddaf6e608fbab33397671e2fcc6cc91 \\\n    --hash=sha256:f7cac622e11b4253ac4536a654fe221249065d9a69feb6cdcd4d9af3503602e0 \\\n    --hash=sha256:f8a04cf0c5b7139bc6368b461257d4a757ea2fe89b3773e494d235b7dd51119f \\\n    --hash=sha256:f8bb35ce57a63c9a6896c71a285818a3922d8ca05d150fd1fe49a7f57287b836 \\\n    --hash=sha256:fbfdce91239fe306772faab57597186710d5699213f4df099d1612da7320d682\n    # via\n    #   geventhttpclient\n    #   locust\ngeventhttpclient==2.3.1 \\\n    --hash=sha256:00675ba682fb7d19d659c14686fa8a52a65e3f301b56c2a4ee6333b380dd9467 \\\n    --hash=sha256:05a1bbdd43ae36bcc10b3dbfa0806aefc5033a91efecfddfe56159446a46ea71 \\\n    --hash=sha256:06e59d3397e63c65ecc7a7561a5289f0cf2e2c2252e29632741e792f57f5d124 \\\n    --hash=sha256:0d0972096a63b1ddaa73fa3dab2c7a136e3ab8bf7999a2f85a5dee851fa77cdd \\\n    --hash=sha256:2399e3d4e2fae8bbd91756189da6e9d84adf8f3eaace5eef0667874a705a29f8 \\\n    --hash=sha256:25d255383d3d6a6fbd643bb51ae1a7e4f6f7b0dbd5f3225b537d0bd0432eaf39 \\\n    --hash=sha256:265d9f31b4ac8f688eebef0bd4c814ffb37a16f769ad0c8c8b8c24a84db8eab5 \\\n    --hash=sha256:2de436a9d61dae877e4e811fb3e2594e2a1df1b18f4280878f318aef48a562b9 \\\n    --hash=sha256:321b73c73d73b85cfeff36b9b5ee04174ec8406fb3dadc129558a26ccb879360 \\\n    --hash=sha256:34107b506e2c40ec7784efa282469bf86888cacddced463dceeb58c201834897 \\\n    --hash=sha256:4436eef515b3e0c1d4a453ae32e047290e780a623c1eddb11026ae9d5fb03d42 \\\n    --hash=sha256:4890713433ca19b081f70b5f7ad258a0979ec3354f9538b50b3ad7d0a86f88de \\\n    --hash=sha256:4a374aad77c01539e786d0c7829bec2eba034ccd45733c1bf9811ad18d2a8ecd \\\n    --hash=sha256:4deaebc121036f7ea95430c2d0f80ab085b15280e6ab677a6360b70e57020e7f \\\n    --hash=sha256:4f843f81ee44ba4c553a1b3f73115e0ad8f00044023c24db29f5b1df3da08465 \\\n    --hash=sha256:50b54f67ba2087f4d9d2172065c5c5de0f0c7f865ac350116e5452de4be31444 \\\n    --hash=sha256:52c45d9f3dd9627844c12e9ca347258c7be585bed54046336220e25ea6eac155 \\\n    --hash=sha256:5d1cf7d8a4f8e15cc8fd7d88ac4cdb058d6274203a42587e594cc9f0850ac862 \\\n    --hash=sha256:5d51330a40ac9762879d0e296c279c1beae8cfa6484bb196ac829242c416b709 \\\n    --hash=sha256:5deb41c2f51247b4e568c14964f59d7b8e537eff51900564c88af3200004e678 \\\n    --hash=sha256:66c1e97460608304f400485ac099736fff3566d3d8db2038533d466f8cf5de5a \\\n    --hash=sha256:6b032a5cdb1721921f4cd36aad620af318263b462962cfb23d648cdb93aab232 \\\n    --hash=sha256:6ca50dd9761971d3557b897108933b34fb4a11533d52f0f2753840c740a2861a \\\n    --hash=sha256:76c367d175810facfe56281e516c9a5a4a191eff76641faaa30aa33882ed4b2f \\\n    --hash=sha256:77c1a2c6e3854bf87cd5588b95174640c8a881716bd07fa0d131d082270a6795 \\\n    --hash=sha256:7924e0883bc2b177cfe27aa65af6bb9dd57f3e26905c7675a2d1f3ef69df7cca \\\n    --hash=sha256:829d03c2a140edbe74ad1fb4f850384f585f3e06fc47cfe647d065412b93926f \\\n    --hash=sha256:83e22178b9480b0a95edf0053d4f30b717d0b696b3c262beabe6964d9c5224b1 \\\n    --hash=sha256:855ab1e145575769b180b57accb0573a77cd6a7392f40a6ef7bc9a4926ebd77b \\\n    --hash=sha256:8b599359779c2278018786c35d70664d441a7cd0d6baef2b2cd0d1685cf478ed \\\n    --hash=sha256:8ee6e741849c29e3129b1ec3828ac3a5e5dcb043402f852ea92c52334fb8cabf \\\n    --hash=sha256:97b072a282233384c1302a7dee88ad8bfedc916f06b1bc1da54f84980f1406a9 \\\n    --hash=sha256:994c543f156db7bce3bae15491a0e041eeb3f1cf467e0d1db0c161a900a90bec \\\n    --hash=sha256:9ddeb431836c2ef7fd33c505a06180dc907b474e0e8537a43ff12e12c9bf0307 \\\n    --hash=sha256:a364b30bec7a0a00dbe256e2b6807e4dc866bead7ac84aaa51ca5e2c3d15c258 \\\n    --hash=sha256:a58376d0d461fe0322ff2ad362553b437daee1eeb92b4c0e3b1ffef9e77defbe \\\n    --hash=sha256:ad0b507e354d2f398186dcb12fe526d0594e7c9387b514fb843f7a14fdf1729a \\\n    --hash=sha256:b40ddac8517c456818942c7812f555f84702105c82783238c9fcb8dc12675185 \\\n    --hash=sha256:b4beff505306aa9da5cdfe2f206b403ec7c8d06a22d6b7248365772858c4ee8c \\\n    --hash=sha256:b8ca7dcbe94cb563341087b00b6fbd0fdd70b2acc1b5d963f9ebbfbc1e5e2893 \\\n    --hash=sha256:bc9f2162d4e8cb86bb5322d99bfd552088a3eacd540a841298f06bb8bc1f1f03 \\\n    --hash=sha256:c071db313866c3d0510feb6c0f40ec086ccf7e4a845701b6316c82c06e8b9b29 \\\n    --hash=sha256:c31431e38df45b3c79bf3c9427c796adb8263d622bc6fa25e2f6ba916c2aad93 \\\n    --hash=sha256:c4624843c03a5337282a42247d987c2531193e57255ee307b36eeb4f243a0c21 \\\n    --hash=sha256:c6f1a56a66a90c4beae2f009b5e9d42db9a58ced165aa35441ace04d69cb7b37 \\\n    --hash=sha256:c9f1ef4ec048563cc621a47ff01a4f10048ff8b676d7a4d75e5433ed8e703e56 \\\n    --hash=sha256:cc34031905b2b31a80d88cd33d7e42b81812950e5304860ab6a65ee2803e2046 \\\n    --hash=sha256:ce2c7d18bac7ffdacc4a86cd490bea6136a7d1e1170f8624f2e3bbe3b189d5b8 \\\n    --hash=sha256:ce649d4e25c2d56023471df0bf1e8e2ab67dfe4ff12ce3e8fe7e6fae30cd672a \\\n    --hash=sha256:d3e33e87d0d5b9f5782c4e6d3cb7e3592fea41af52713137d04776df7646d71b \\\n    --hash=sha256:d614573621ba827c417786057e1e20e9f96c4f6b3878c55b1b7b54e1026693bc \\\n    --hash=sha256:da22ab7bf5af4ba3d07cffee6de448b42696e53e7ac1fe97ed289037733bf1c2 \\\n    --hash=sha256:ddcc3f0fdffd9a3801e1005b73026202cffed8199863fdef9315bea9a860a032 \\\n    --hash=sha256:e1c90abcc2735cd8dd2d2572a13da32f6625392dc04862decb5c6476a3ddee22 \\\n    --hash=sha256:ea77b67c186df90473416f4403839728f70ef6cf1689cec97b4f6bbde392a8a8 \\\n    --hash=sha256:f087af2ac439495b5388841d6f3c4de8d2573ca9870593d78f7b554aa5cfa7f5 \\\n    --hash=sha256:f0ae055b9ce1704f2ce72c0847df28f4e14dbb3eea79256cda6c909d82688ea3 \\\n    --hash=sha256:f10c62994f9052f23948c19de930b2d1f063240462c8bd7077c2b3290e61f4fa \\\n    --hash=sha256:f36f0c6ef88a27e60af8369d9c2189fe372c6f2943182a7568e0f2ad33bb69f1 \\\n    --hash=sha256:f440cc704f8a9869848a109b2c401805c17c070539b2014e7b884ecfc8591e33 \\\n    --hash=sha256:f82c454595a88a5e510ae0985711ef398386998b6f37d90fc30e9ff1a2001280 \\\n    --hash=sha256:fb0a9673074541ccda09a2423fa16f4528819ceb1ba19d252213f6aca7d4b44a \\\n    --hash=sha256:fe912c6456faab196b952adcd63e9353a0d5c8deb31c8d733d38f4f0ab22e359\n    # via locust\ngitdb==4.0.11 \\\n    --hash=sha256:81a3407ddd2ee8df444cbacea00e2d038e40150acfa3001696fe0dcf1d3adfa4 \\\n    --hash=sha256:bf5421126136d6d0af55bc1e7c1af1c397a34f5b7bd79e776cd3e89785c2b04b\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gitpython\ngitpython==3.1.40 \\\n    --hash=sha256:22b126e9ffb671fdd0c129796343a02bf67bf2994b35449ffc9321aa755e18a4 \\\n    --hash=sha256:cf14627d5a8049ffbf49915732e5eddbe8134c3bdb9d476e6182b676fc573f8a\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   wandb\ngoogle-api-core==1.34.0 \\\n    --hash=sha256:6fb380f49d19ee1d09a9722d0379042b7edb06c0112e4796c7a395078a043e71 \\\n    --hash=sha256:7421474c39d396a74dfa317dddbc69188f2336835f526087c7648f91105e32ff\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   google-cloud-core\n    #   google-cloud-storage\ngoogle-apitools==0.5.32 \\\n    --hash=sha256:b78f74116558e0476e19501b5b4b2ac7c93261a69c5449c861ea95cbc853c688 \\\n    --hash=sha256:c3763e52289f61e21c41d5531e20fbda9cc8484a088b8686fd460770db8bad13\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gsutil\ngoogle-auth[aiohttp]==2.23.4 \\\n    --hash=sha256:79905d6b1652187def79d491d6e23d0cbb3a21d3c7ba0dbaa9c8a01906b13ff3 \\\n    --hash=sha256:d4bbc92fe4b8bfd2f3e8d88e5ba7085935da208ee38a134fc280e7ce682a05f2\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gcsfs\n    #   google-api-core\n    #   google-auth-oauthlib\n    #   google-cloud-core\n    #   google-cloud-storage\n    #   gsutil\ngoogle-auth-oauthlib==1.0.0 \\\n    --hash=sha256:95880ca704928c300f48194d1770cf5b1462835b6e49db61445a520f793fd5fb \\\n    --hash=sha256:e375064964820b47221a7e1b7ee1fd77051b6323c3f9e3e19785f78ab67ecfc5\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gcsfs\ngoogle-cloud-core==2.4.1 \\\n    --hash=sha256:9b7749272a812bde58fff28868d0c5e2f585b82f37e09a1f6ed2d4d10f134073 \\\n    --hash=sha256:a9e6a4422b9ac5c29f79a0ede9485473338e2ce78d91f2370c01e730eab22e61\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   google-cloud-storage\ngoogle-cloud-storage==2.14.0 \\\n    --hash=sha256:2d23fcf59b55e7b45336729c148bb1c464468c69d5efbaee30f7201dd90eb97e \\\n    --hash=sha256:8641243bbf2a2042c16a6399551fbb13f062cbc9a2de38d6c0bb5426962e9dbd\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gcsfs\ngoogle-crc32c==1.5.0 \\\n    --hash=sha256:024894d9d3cfbc5943f8f230e23950cd4906b2fe004c72e29b209420a1e6b05a \\\n    --hash=sha256:02c65b9817512edc6a4ae7c7e987fea799d2e0ee40c53ec573a692bee24de876 \\\n    --hash=sha256:02ebb8bf46c13e36998aeaad1de9b48f4caf545e91d14041270d9dca767b780c \\\n    --hash=sha256:07eb3c611ce363c51a933bf6bd7f8e3878a51d124acfc89452a75120bc436289 \\\n    --hash=sha256:1034d91442ead5a95b5aaef90dbfaca8633b0247d1e41621d1e9f9db88c36298 \\\n    --hash=sha256:116a7c3c616dd14a3de8c64a965828b197e5f2d121fedd2f8c5585c547e87b02 \\\n    --hash=sha256:19e0a019d2c4dcc5e598cd4a4bc7b008546b0358bd322537c74ad47a5386884f \\\n    --hash=sha256:1c7abdac90433b09bad6c43a43af253e688c9cfc1c86d332aed13f9a7c7f65e2 \\\n    --hash=sha256:1e986b206dae4476f41bcec1faa057851f3889503a70e1bdb2378d406223994a \\\n    --hash=sha256:272d3892a1e1a2dbc39cc5cde96834c236d5327e2122d3aaa19f6614531bb6eb \\\n    --hash=sha256:278d2ed7c16cfc075c91378c4f47924c0625f5fc84b2d50d921b18b7975bd210 \\\n    --hash=sha256:2ad40e31093a4af319dadf503b2467ccdc8f67c72e4bcba97f8c10cb078207b5 \\\n    --hash=sha256:2e920d506ec85eb4ba50cd4228c2bec05642894d4c73c59b3a2fe20346bd00ee \\\n```\n\n----------------------------------------\n\nTITLE: Defining sphinxcontrib-applehelp dependency with version pinning and hash verification\nDESCRIPTION: Specifies sphinxcontrib-applehelp version 2.0.0 with SHA-256 hashes for verification. Comments indicate it's required by sphinx.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_42\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinxcontrib-applehelp==2.0.0 \\\n    --hash=sha256:2f29ef331735ce958efa4734873f084941970894c6090408b079c61b2e1c06d1 \\\n    --hash=sha256:4cd3f0ec4ac5dd9c17ec65e9ab272c9b867ea77425228e68ecf08d6b28ddbdb5\n    # via sphinx\n```\n\n----------------------------------------\n\nTITLE: Instantiating TensorflowTrainer for Distributed Training with Ray Train (Python)\nDESCRIPTION: Instantiates a `TensorflowTrainer` with 4 workers and uses it to run the distributed TensorFlow training function. Placeholders `__tf_trainer_begin__` and `__tf_trainer_end__` and the `:dedent: 0` parameter denote the code block.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"```{literalinclude} /../../python/ray/train/examples/tf/tensorflow_quick_start.py\n:language: python\n:start-after: __tf_trainer_begin__\n:end-before: __tf_trainer_end__\n:dedent: 0\n```\"\n```\n\n----------------------------------------\n\nTITLE: Terminating Detached Actor in Python\nDESCRIPTION: Python script to terminate a previously created detached Ray actor. Used for cleanup and demonstrating scale-down behavior.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport sys\n\nray.init(namespace=\"default_namespace\")\ndetached_actor = ray.get_actor(sys.argv[1])\nray.kill(detached_actor)\n```\n\n----------------------------------------\n\nTITLE: Installing a RayService with Unbalanced Ray Serve Replica Configuration\nDESCRIPTION: Downloads and applies a RayService YAML configuration with specific settings that will cause one worker pod to be unready due to the absence of a Ray Serve replica.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncurl -O https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-service.no-ray-serve-replica.yaml\nkubectl apply -f ray-service.no-ray-serve-replica.yaml\n```\n\n----------------------------------------\n\nTITLE: RLlib Config Reference Links in RST\nDESCRIPTION: A list of reference links to different configuration sections in RLlib documentation, organized by functional areas including training, environment, learners, callbacks, and various other settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_14\n\nLANGUAGE: rst\nCODE:\n```\n- :ref:`Config settings for training behavior (including algo-specific settings) <rllib-config-training>`\n- :ref:`Config settings for EnvRunners <rllib-config-env-runners>`\n- :ref:`Config settings for Learners <rllib-config-learners>`\n- :ref:`Config settings for adding callbacks <rllib-config-callbacks>`\n- :ref:`Config settings for multi-agent setups <rllib-config-multi_agent>`\n- :ref:`Config settings for offline RL <rllib-config-offline_data>`\n- :ref:`Config settings for evaluating policies <rllib-config-evaluation>`\n- :ref:`Config settings for the DL framework <rllib-config-framework>`\n- :ref:`Config settings for reporting and logging behavior <rllib-config-reporting>`\n- :ref:`Config settings for checkpointing <rllib-config-checkpointing>`\n- :ref:`Config settings for debugging <rllib-config-debugging>`\n- :ref:`Experimental config settings <rllib-config-experimental>`\n```\n\n----------------------------------------\n\nTITLE: Defining sentencepiece dependency with version pinning and hash verification\nDESCRIPTION: Specifies sentencepiece version 0.2.0 with numerous SHA-256 hashes for verification. Comments indicate it's required by mistral-common, vllm, and xgrammar.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_33\n\nLANGUAGE: plaintext\nCODE:\n```\nsentencepiece==0.2.0 \\\n    --hash=sha256:0461324897735512a32d222e3d886e24ad6a499761952b6bda2a9ee6e4313ea5 \\\n    --hash=sha256:0993dbc665f4113017892f1b87c3904a44d0640eda510abcacdfb07f74286d36 \\\n    --hash=sha256:0a91aaa3c769b52440df56fafda683b3aa48e3f2169cf7ee5b8c8454a7f3ae9b \\\n    --hash=sha256:0f67eae0dbe6f2d7d6ba50a354623d787c99965f068b81e145d53240198021b0 \\\n    --hash=sha256:1380ce6540a368de2ef6d7e6ba14ba8f3258df650d39ba7d833b79ee68a52040 \\\n    --hash=sha256:17982700c4f6dbb55fa3594f3d7e5dd1c8659a274af3738e33c987d2a27c9d5c \\\n    --hash=sha256:188779e1298a1c8b8253c7d3ad729cb0a9891e5cef5e5d07ce4592c54869e227 \\\n    --hash=sha256:1e0f9c4d0a6b0af59b613175f019916e28ade076e21242fd5be24340d8a2f64a \\\n    --hash=sha256:20813a68d4c221b1849c62c30e1281ea81687894d894b8d4a0f4677d9311e0f5 \\\n    --hash=sha256:22e37bac44dd6603388cb598c64ff7a76e41ca774646f21c23aadfbf5a2228ab \\\n    --hash=sha256:27f90c55a65013cbb8f4d7aab0599bf925cde4adc67ae43a0d323677b5a1c6cb \\\n    --hash=sha256:298f21cc1366eb60311aedba3169d30f885c363ddbf44214b0a587d2908141ad \\\n    --hash=sha256:2a3149e3066c2a75e0d68a43eb632d7ae728c7925b517f4c05c40f6f7280ce08 \\\n    --hash=sha256:2fde4b08cfe237be4484c6c7c2e2c75fb862cfeab6bd5449ce4caeafd97b767a \\\n    --hash=sha256:3212121805afc58d8b00ab4e7dd1f8f76c203ddb9dc94aa4079618a31cf5da0f \\\n    --hash=sha256:38aed822fb76435fa1f12185f10465a94ab9e51d5e8a9159e9a540ce926f0ffd \\\n    --hash=sha256:3f1ec95aa1e5dab11f37ac7eff190493fd87770f7a8b81ebc9dd768d1a3c8704 \\\n    --hash=sha256:4547683f330289ec4f093027bfeb87f9ef023b2eb6f879fdc4a8187c7e0ffb90 \\\n    --hash=sha256:4c378492056202d1c48a4979650981635fd97875a00eabb1f00c6a236b013b5e \\\n    --hash=sha256:536b934e244829e3fe6c4f198652cd82da48adb9aa145c9f00889542726dee3d \\\n    --hash=sha256:632f3594d3e7ac8b367bca204cb3fd05a01d5b21455acd097ea4c0e30e2f63d7 \\\n    --hash=sha256:6cf333625234f247ab357b0bd9836638405ea9082e1543d5b8408f014979dcbf \\\n    --hash=sha256:7140d9e5a74a0908493bb4a13f1f16a401297bd755ada4c707e842fbf6f0f5bf \\\n    --hash=sha256:787e480ca4c1d08c9985a7eb1eae4345c107729c99e9b5a9a00f2575fc7d4b4b \\\n    --hash=sha256:7a673a72aab81fef5ebe755c6e0cc60087d1f3a4700835d40537183c1703a45f \\\n    --hash=sha256:7b06b70af54daa4b4904cbb90b4eb6d35c9f3252fdc86c9c32d5afd4d30118d8 \\\n    --hash=sha256:7c867012c0e8bcd5bdad0f791609101cb5c66acb303ab3270218d6debc68a65e \\\n    --hash=sha256:7cd6175f7eaec7142d2bf6f6597ce7db4c9ac89acf93fcdb17410c3a8b781eeb \\\n    --hash=sha256:7fd6071249c74f779c5b27183295b9202f8dedb68034e716784364443879eaa6 \\\n    --hash=sha256:859ba1acde782609a0910a26a60e16c191a82bf39b5621107552c0cd79fad00f \\\n    --hash=sha256:89f65f69636b7e9c015b79dff9c9985a9bc7d19ded6f79ef9f1ec920fdd73ecf \\\n    --hash=sha256:926ef920ae2e8182db31d3f5d081ada57804e3e1d3a8c4ef8b117f9d9fb5a945 \\\n    --hash=sha256:98501e075f35dd1a1d5a20f65be26839fcb1938752ec61539af008a5aa6f510b \\\n    --hash=sha256:a1151d6a6dd4b43e552394aed0edfe9292820272f0194bd56c7c1660a0c06c3d \\\n    --hash=sha256:a52c19171daaf2e697dc6cbe67684e0fa341b1248966f6aebb541de654d15843 \\\n    --hash=sha256:b293734059ef656dcd65be62ff771507bea8fed0a711b6733976e1ed3add4553 \\\n    --hash=sha256:b99a308a2e5e569031ab164b74e6fab0b6f37dfb493c32f7816225f4d411a6dd \\\n    --hash=sha256:bcbbef6cc277f8f18f36959e305f10b1c620442d75addc79c21d7073ae581b50 \\\n    --hash=sha256:bed9cf85b296fa2b76fc2547b9cbb691a523864cebaee86304c43a7b4cb1b452 \\\n    --hash=sha256:c581258cf346b327c62c4f1cebd32691826306f6a41d8c4bec43b010dee08e75 \\\n    --hash=sha256:cdb701eec783d3ec86b7cd4c763adad8eaf6b46db37ee1c36e5e6c44b3fe1b5f \\\n    --hash=sha256:d0cb51f53b6aae3c36bafe41e86167c71af8370a039f542c43b0cce5ef24a68c \\\n    --hash=sha256:d1e5ca43013e8935f25457a4fca47e315780172c3e821b4b13a890668911c792 \\\n    --hash=sha256:d490142b0521ef22bc1085f061d922a2a6666175bb6b42e588ff95c0db6819b2 \\\n    --hash=sha256:d7b67e724bead13f18db6e1d10b6bbdc454af574d70efbb36f27d90387be1ca3 \\\n    --hash=sha256:d8cf876516548b5a1d6ac4745d8b554f5c07891d55da557925e5c13ff0b4e6ad \\\n    --hash=sha256:e3d1d2cc4882e8d6a1adf9d5927d7716f80617fc693385661caff21888972269 \\\n    --hash=sha256:e58b47f933aca74c6a60a79dcb21d5b9e47416256c795c2d58d55cec27f9551d \\\n    --hash=sha256:ea5f536e32ea8ec96086ee00d7a4a131ce583a1b18d130711707c10e69601cb2 \\\n    --hash=sha256:f295105c6bdbb05bd5e1b0cafbd78ff95036f5d3641e7949455a3f4e5e7c3109 \\\n    --hash=sha256:f4d158189eb2ecffea3a51edf6d25e110b3678ec47f1a40f2d541eafbd8f6250 \\\n    --hash=sha256:fb89f811e5efd18bab141afc3fea3de141c3f69f3fe9e898f710ae7fe3aab251 \\\n    --hash=sha256:ff88712338b01031910e8e61e7239aff3ce8869ee31a47df63cb38aadd591bea\n    # via\n    #   mistral-common\n    #   vllm\n    #   xgrammar\n```\n\n----------------------------------------\n\nTITLE: Running Nsight Profiling with NVTX for Ray Compiled Graph (Bash)\nDESCRIPTION: Command to enable Nsight profiling with NVTX annotations for a Ray Compiled Graph script. It sets the RAY_CGRAPH_ENABLE_NVTX_PROFILING environment variable to 1 before running the Python script.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/compiled-graph/profiling.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nRAY_CGRAPH_ENABLE_NVTX_PROFILING=1 python3 example.py\n```\n\n----------------------------------------\n\nTITLE: Java Thread Stack Trace for Netty/Vert.x HTTP Request Processing\nDESCRIPTION: A detailed thread stack trace showing the call chain from Java thread startup through Netty NIO event processing to JavaScript execution in a Vert.x HTTP server. This trace demonstrates how HTTP requests are processed from the network layer up through the application layer.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_87\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/ScriptRuntime:.setObjectElem_[j];org/mozilla/javascript/ScriptRuntime:.indexFromString_[j] 2\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Service Account for Ray Access\nDESCRIPTION: Command to create a service account named ray-user that will be used for submitting Ray jobs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create serviceaccount ray-user\n```\n\n----------------------------------------\n\nTITLE: Waiting for Placement Group Readiness in Java\nDESCRIPTION: This snippet demonstrates how to wait for a placement group to be ready in Java and how to retrieve all placement groups.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n// Wait for the placement group to be ready within the specified time(unit is seconds).\nboolean ready = pg.wait(60);\nAssert.assertTrue(ready);\n\n// You can look at placement group states using this API.\nList<PlacementGroup> allPlacementGroup = PlacementGroups.getAllPlacementGroups();\nfor (PlacementGroup group: allPlacementGroup) {\n  System.out.println(group);\n}\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Package dependency specifications with version pins and SHA256 hash verification. Includes pycurl 7.45.3, pydantic 2.9.2, and pydantic-core 2.23.4 with their corresponding hash values for security verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_17\n\nLANGUAGE: txt\nCODE:\n```\npycurl==7.45.3 \\\n    --hash=sha256:0c41a172d5e8a5cdd8328cc8134f47b2a57960ac677f7cda8520eaa9fbe7d990 \\\n    --hash=sha256:0f0e1251a608ffd75fc502f4014442e554c67d3d7a1b0a839c35efb6ad2f8bf8\n\npydantic==2.9.2 \\\n    --hash=sha256:d155cef71265d1e9807ed1c32b4c8deec042a44a50a4188b25ac67ecd81a9c0f \\\n    --hash=sha256:f048cec7b26778210e28a0459867920654d48e5e62db0958433636cde4254f12\n\npydantic-core==2.23.4 \\\n    --hash=sha256:0a7df63886be5e270da67e0966cf4afbae86069501d35c8c1b3b6c168f42cb36 \\\n    --hash=sha256:0cb3da3fd1b6a5d0279a01877713dbda118a2a4fc6f0d821a57da2e464793f05\n```\n\n----------------------------------------\n\nTITLE: Specifying Platform-Specific Dependencies with Hash Verification\nDESCRIPTION: This snippet shows how to specify platform-specific dependencies (pexpect for non-Windows systems) and other Python utility libraries with hash verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_30\n\nLANGUAGE: plaintext\nCODE:\n```\npexpect==4.8.0 ; sys_platform != 'win32' \\\n    --hash=sha256:0b48a55dcb3c05f3329815901ea4fc1537514d6ba867a152b581d69ae3710937 \\\n    --hash=sha256:fc65a43959d153d0114afe13997d439c22823a27cefceb5ff35c2178c6784c0c\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   ipython\npickleshare==0.7.5 \\\n    --hash=sha256:87683d47965c1da65cdacaf31c8441d12b8044cdec9aca500cd78fc2c683afca \\\n    --hash=sha256:9649af414d74d4df115d5d718f82acb59c9d418196b7b4290ed47a12ce62df56\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   ipython\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    --hash=sha256:0ea2a783a2bdf2a561808fe4a7a12e9aa3799b701ba305de596bc48b8bdfce9d \\\n    --hash=sha256:1530e8f3a4b965eb6a7785cf17a426c779333eb62c9a7d1bbcf3ffd5bf77a4aa \\\n    --hash=sha256:16563993329b79513f59142a6b02055e10514c1a8e86dca8b48a893e33cf91e3 \\\n    --hash=sha256:19aeb96d43902f0a783946a0a87dbdad5c84c936025b8419da0a0cd7724356b1 \\\n    --hash=sha256:1a1d1915db1a4fdb2754b9de292642a39a7fb28f1736699527bb649484fb966a \\\n    --hash=sha256:1b87bd9d81d179bd8ab871603bd80d8645729939f90b71e62914e816a76fc6bd \\\n```\n\n----------------------------------------\n\nTITLE: Pinned Platformdirs Dependency with Hash Verification\nDESCRIPTION: Specifies platformdirs version 3.11.0 with SHA256 hash verification for secure installation. Required by the virtualenv package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_22\n\nLANGUAGE: plaintext\nCODE:\n```\nplatformdirs==3.11.0 \\\n    --hash=sha256:cf8ee52a3afdb965072dcc652433e0c7e3e40cf5ea1477cd4b3b1d2eb75495b3 \\\n    --hash=sha256:e9d171d00af68be50e9202731309c4e658fd8bc76f55c11c7dd760d023bda68e\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installing the necessary Python packages for running the example including Ray Data, PyTorch and torchvision.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/01_batch_inference/start.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"ray[data]\" torch torchvision\n```\n\n----------------------------------------\n\nTITLE: Analyzing and Visualizing Tuning Results in Python\nDESCRIPTION: This section retrieves the best trial result from the tuning process, provides the path to stored checkpoints, and visualizes the learning curve of the model using Matplotlib. It assesses the performance of the model based on the mean accuracy metric across training iterations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport os\n\n# Get the best trial result\nbest_result = results_grid.get_best_result(metric=\"mean_accuracy\", mode=\"max\")\n\n# Print `path` where checkpoints are stored\nprint('Best result path:', best_result.path)\n\n# Print the best trial `config` reported at the last iteration\n# NOTE: This config is just what the trial ended up with at the last iteration.\n# See the next section for replaying the entire history of configs.\nprint(\"Best final iteration hyperparameter config:\\n\", best_result.config)\n\n# Plot the learning curve for the best trial\ndf = best_result.metrics_dataframe\n# Deduplicate, since PBT might introduce duplicate data\ndf = df.drop_duplicates(subset=\"training_iteration\", keep=\"last\")\ndf.plot(\"training_iteration\", \"mean_accuracy\")\nplt.xlabel(\"Training Iterations\")\nplt.ylabel(\"Test Accuracy\")\nplt.show()\n\n```\n\n----------------------------------------\n\nTITLE: Specifying google-crc32c Package Dependency with Hash Values\nDESCRIPTION: Definition of the google-crc32c package dependency at version 1.5.0 with multiple hash values for verification. This shows an extensive list of hash values for different platform-specific builds of the package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_14\n\nLANGUAGE: text\nCODE:\n```\ngoogle-crc32c==1.5.0 \\\n    --hash=sha256:024894d9d3cfbc5943f8f230e23950cd4906b2fe004c72e29b209420a1e6b05a \\\n    --hash=sha256:02c65b9817512edc6a4ae7c7e987fea799d2e0ee40c53ec573a692bee24de876 \\\n    --hash=sha256:02ebb8bf46c13e36998aeaad1de9b48f4caf545e91d14041270d9dca767b780c \\\n    --hash=sha256:07eb3c611ce363c51a933bf6bd7f8e3878a51d124acfc89452a75120bc436289 \\\n    --hash=sha256:1034d91442ead5a95b5aaef90dbfaca8633b0247d1e41621d1e9f9db88c36298 \\\n    --hash=sha256:116a7c3c616dd14a3de8c64a965828b197e5f2d121fedd2f8c5585c547e87b02 \\\n    --hash=sha256:19e0a019d2c4dcc5e598cd4a4bc7b008546b0358bd322537c74ad47a5386884f \\\n    --hash=sha256:1c7abdac90433b09bad6c43a43af253e688c9cfc1c86d332aed13f9a7c7f65e2 \\\n    --hash=sha256:1e986b206dae4476f41bcec1faa057851f3889503a70e1bdb2378d406223994a \\\n    --hash=sha256:272d3892a1e1a2dbc39cc5cde96834c236d5327e2122d3aaa19f6614531bb6eb \\\n    --hash=sha256:278d2ed7c16cfc075c91378c4f47924c0625f5fc84b2d50d921b18b7975bd210 \\\n    --hash=sha256:2ad40e31093a4af319dadf503b2467ccdc8f67c72e4bcba97f8c10cb078207b5 \\\n    --hash=sha256:2e920d506ec85eb4ba50cd4228c2bec05642894d4c73c59b3a2fe20346bd00ee \\\n    --hash=sha256:3359fc442a743e870f4588fcf5dcbc1bf929df1fad8fb9905cd94e5edb02e84c \\\n    --hash=sha256:37933ec6e693e51a5b07505bd05de57eee12f3e8c32b07da7e73669398e6630a \\\n    --hash=sha256:398af5e3ba9cf768787eef45c803ff9614cc3e22a5b2f7d7ae116df8b11e3314 \\\n    --hash=sha256:3b747a674c20a67343cb61d43fdd9207ce5da6a99f629c6e2541aa0e89215bcd \\\n    --hash=sha256:461665ff58895f508e2866824a47bdee72497b091c730071f2b7575d5762ab65 \\\n    --hash=sha256:4c6fdd4fccbec90cc8a01fc00773fcd5fa28db683c116ee3cb35cd5da9ef6c37 \\\n    --hash=sha256:5829b792bf5822fd0a6f6eb34c5f81dd074f01d570ed7f36aa101d6fc7a0a6e4 \\\n    --hash=sha256:596d1f98fc70232fcb6590c439f43b350cb762fb5d61ce7b0e9db4539654cc13 \\\n    --hash=sha256:5ae44e10a8e3407dbe138984f21e536583f2bba1be9491239f942c2464ac0894 \\\n    --hash=sha256:635f5d4dd18758a1fbd1049a8e8d2fee4ffed124462d837d1a02a0e009c3ab31 \\\n    --hash=sha256:64e52e2b3970bd891309c113b54cf0e4384762c934d5ae56e283f9a0afcd953e \\\n    --hash=sha256:66741ef4ee08ea0b2cc3c86916ab66b6aef03768525627fd6a1b34968b4e3709 \\\n    --hash=sha256:67b741654b851abafb7bc625b6d1cdd520a379074e64b6a128e3b688c3c04740 \\\n    --hash=sha256:6ac08d24c1f16bd2bf5eca8eaf8304812f44af5cfe5062006ec676e7e1d50afc \\\n    --hash=sha256:6f998db4e71b645350b9ac28a2167e6632c239963ca9da411523bb439c5c514d \\\n    --hash=sha256:72218785ce41b9cfd2fc1d6a017dc1ff7acfc4c17d01053265c41a2c0cc39b8c \\\n```\n\n----------------------------------------\n\nTITLE: Specifying Jsonschema and Related Dependencies for Ray Project\nDESCRIPTION: Defines the Jsonschema package dependency with version 4.23.0 and its specifications extension with version 2024.10.1, both with verification hashes. These dependencies are used by multiple components including the Mistral, Outlines, and Ray core frameworks.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_11\n\nLANGUAGE: pip\nCODE:\n```\njsonschema==4.23.0 \\\n    --hash=sha256:d71497fef26351a33265337fa77ffeb82423f3ea21283cd9467bb03999266bc4 \\\n    --hash=sha256:fbadb6f8b144a8f8cf9f0b89ba94501d143e50411a1278633f56a7acf7fd5566\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements/llm/llm-requirements.txt\n    #   -r python/requirements.txt\n    #   mistral-common\n    #   outlines\n    #   outlines-core\n    #   ray\njsonschema-specifications==2024.10.1 \\\n    --hash=sha256:0f38b83639958ce1152d02a7f062902c41c8fd20d558b0c34344292d417ae272 \\\n    --hash=sha256:a09a0680616357d9a0ecf05c12ad234479f549239d0f5b55f3deea67475da9bf\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   jsonschema\n```\n\n----------------------------------------\n\nTITLE: Including Announcements or Messages in Documentation\nDESCRIPTION: Markdown syntax for including pre-defined templates or announcements in documentation pages to ensure consistent messaging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/README.md#2025-04-12_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n.. include:: /_includes/<my-announcement>\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Verification\nDESCRIPTION: This snippet defines package dependencies with exact versions and SHA256 hash values for security. It includes google-cloud-storage, google-resumable-media, googleapis-common-protos, and grpcio packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_11\n\nLANGUAGE: Text\nCODE:\n```\ngoogle-resumable-media==2.6.0 \\\n    --hash=sha256:972852f6c65f933e15a4a210c2b96930763b47197cdf4aa5f5bea435efb626e7 \\\n    --hash=sha256:fc03d344381970f79eebb632a3c18bb1828593a2dc5572b5f90115ef7d11e81b\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   google-cloud-storage\ngooglapis-common-protos==1.61.0 \\\n    --hash=sha256:22f1915393bb3245343f6efe87f6fe868532efc12aa26b391b15132e1279f1c0 \\\n    --hash=sha256:8a64866a97f6304a7179873a465d6eee97b7a24ec6cfd78e0f575e96b821240b\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   google-api-core\n    #   opentelemetry-exporter-otlp-proto-grpc\ngrpcio==1.66.2 \\\n    --hash=sha256:02697eb4a5cbe5a9639f57323b4c37bcb3ab2d48cec5da3dc2f13334d72790dd \\\n    --hash=sha256:03b0b307ba26fae695e067b94cbb014e27390f8bc5ac7a3a39b7723fed085604 \\\n    --hash=sha256:05bc2ceadc2529ab0b227b1310d249d95d9001cd106aa4d31e8871ad3c428d73\n```\n\n----------------------------------------\n\nTITLE: GCP TPU Configuration (Autoscaler)\nDESCRIPTION: This YAML configuration demonstrates how to configure Ray autoscaling with TPUs (Tensor Processing Units) on Google Cloud Platform (GCP).  It includes necessary settings and resource specifications for using TPU VMs with Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/gcp/tpu.yaml\n            :language: yaml\n\n```\n\n----------------------------------------\n\nTITLE: Package Dependency Hash Verification for multidict and networkx\nDESCRIPTION: This snippet shows the exact hash values used to verify the integrity of the multidict and networkx packages when installing dependencies for the Ray project. Each hash ensures the downloaded package matches the expected content, preventing supply chain attacks.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nmultidict==6.0.5 \\\n    --hash=sha256:01265f5e40f5a17f8241d52656ed27192be03bfa8764d88e8220141d1e4b3556 \\\n    --hash=sha256:0275e35209c27a3f7951e1ce7aaf93ce0d163b28948444bec61dd7badc6d3f8c \\\n    --hash=sha256:04bde7a7b3de05732a4eb39c94574db1ec99abb56162d6c520ad26f83267de29 \\\n    --hash=sha256:04da1bb8c8dbadf2a18a452639771951c662c5ad03aefe4884775454be322c9b \\\n    --hash=sha256:09a892e4a9fb47331da06948690ae38eaa2426de97b4ccbfafbdcbe5c8f37ff8 \\\n    --hash=sha256:0d63c74e3d7ab26de115c49bffc92cc77ed23395303d496eae515d4204a625e7 \\\n    --hash=sha256:107c0cdefe028703fb5dafe640a409cb146d44a6ae201e55b35a4af8e95457dd \\\n    --hash=sha256:141b43360bfd3bdd75f15ed811850763555a251e38b2405967f8e25fb43f7d40 \\\n    --hash=sha256:14c2976aa9038c2629efa2c148022ed5eb4cb939e15ec7aace7ca932f48f9ba6 \\\n    --hash=sha256:19fe01cea168585ba0f678cad6f58133db2aa14eccaf22f88e4a6dccadfad8b3 \\\n    --hash=sha256:1d147090048129ce3c453f0292e7697d333db95e52616b3793922945804a433c \\\n    --hash=sha256:1d9ea7a7e779d7a3561aade7d596649fbecfa5c08a7674b11b423783217933f9 \\\n    --hash=sha256:215ed703caf15f578dca76ee6f6b21b7603791ae090fbf1ef9d865571039ade5 \\\n    --hash=sha256:21fd81c4ebdb4f214161be351eb5bcf385426bf023041da2fd9e60681f3cebae \\\n    --hash=sha256:220dd781e3f7af2c2c1053da9fa96d9cf3072ca58f057f4c5adaaa1cab8fc442 \\\n    --hash=sha256:228b644ae063c10e7f324ab1ab6b548bdf6f8b47f3ec234fef1093bc2735e5f9 \\\n    --hash=sha256:29bfeb0dff5cb5fdab2023a7a9947b3b4af63e9c47cae2a10ad58394b517fddc \\\n    --hash=sha256:2f4848aa3baa109e6ab81fe2006c77ed4d3cd1e0ac2c1fbddb7b1277c168788c \\\n    --hash=sha256:2faa5ae9376faba05f630d7e5e6be05be22913782b927b19d12b8145968a85ea \\\n    --hash=sha256:2ffc42c922dbfddb4a4c3b438eb056828719f07608af27d163191cb3e3aa6cc5 \\\n    --hash=sha256:37b15024f864916b4951adb95d3a80c9431299080341ab9544ed148091b53f50 \\\n    --hash=sha256:3cc2ad10255f903656017363cd59436f2111443a76f996584d1077e43ee51182 \\\n    --hash=sha256:3d25f19500588cbc47dc19081d78131c32637c25804df8414463ec908631e453 \\\n    --hash=sha256:403c0911cd5d5791605808b942c88a8155c2592e05332d2bf78f18697a5fa15e \\\n    --hash=sha256:411bf8515f3be9813d06004cac41ccf7d1cd46dfe233705933dd163b60e37600 \\\n    --hash=sha256:425bf820055005bfc8aa9a0b99ccb52cc2f4070153e34b701acc98d201693733 \\\n    --hash=sha256:435a0984199d81ca178b9ae2c26ec3d49692d20ee29bc4c11a2a8d4514c67eda \\\n    --hash=sha256:4a6a4f196f08c58c59e0b8ef8ec441d12aee4125a7d4f4fef000ccb22f8d7241 \\\n    --hash=sha256:4cc0ef8b962ac7a5e62b9e826bd0cd5040e7d401bc45a6835910ed699037a461 \\\n    --hash=sha256:51d035609b86722963404f711db441cf7134f1889107fb171a970c9701f92e1e \\\n    --hash=sha256:53689bb4e102200a4fafa9de9c7c3c212ab40a7ab2c8e474491914d2305f187e \\\n    --hash=sha256:55205d03e8a598cfc688c71ca8ea5f66447164efff8869517f175ea632c7cb7b \\\n    --hash=sha256:5c0631926c4f58e9a5ccce555ad7747d9a9f8b10619621f22f9635f069f6233e \\\n    --hash=sha256:5cb241881eefd96b46f89b1a056187ea8e9ba14ab88ba632e68d7a2ecb7aadf7 \\\n    --hash=sha256:60d698e8179a42ec85172d12f50b1668254628425a6bd611aba022257cac1386 \\\n    --hash=sha256:612d1156111ae11d14afaf3a0669ebf6c170dbb735e510a7438ffe2369a847fd \\\n    --hash=sha256:6214c5a5571802c33f80e6c84713b2c79e024995b9c5897f794b43e714daeec9 \\\n    --hash=sha256:6939c95381e003f54cd4c5516740faba40cf5ad3eeff460c3ad1d3e0ea2549bf \\\n    --hash=sha256:69db76c09796b313331bb7048229e3bee7928eb62bab5e071e9f7fcc4879caee \\\n    --hash=sha256:6bf7a982604375a8d49b6cc1b781c1747f243d91b81035a9b43a2126c04766f5 \\\n    --hash=sha256:766c8f7511df26d9f11cd3a8be623e59cca73d44643abab3f8c8c07620524e4a \\\n    --hash=sha256:76c0de87358b192de7ea9649beb392f107dcad9ad27276324c24c91774ca5271 \\\n    --hash=sha256:76f067f5121dcecf0d63a67f29080b26c43c71a98b10c701b0677e4a065fbd54 \\\n    --hash=sha256:7901c05ead4b3fb75113fb1dd33eb1253c6d3ee37ce93305acd9d38e0b5f21a4 \\\n    --hash=sha256:79660376075cfd4b2c80f295528aa6beb2058fd289f4c9252f986751a4cd0496 \\\n    --hash=sha256:79a6d2ba910adb2cbafc95dad936f8b9386e77c84c35bc0add315b856d7c3abb \\\n    --hash=sha256:7afcdd1fc07befad18ec4523a782cde4e93e0a2bf71239894b8d61ee578c1319 \\\n    --hash=sha256:7be7047bd08accdb7487737631d25735c9a04327911de89ff1b26b81745bd4e3 \\\n    --hash=sha256:7c6390cf87ff6234643428991b7359b5f59cc15155695deb4eda5c777d2b880f \\\n    --hash=sha256:7df704ca8cf4a073334e0427ae2345323613e4df18cc224f647f251e5e75a527 \\\n    --hash=sha256:85f67aed7bb647f93e7520633d8f51d3cbc6ab96957c71272b286b2f30dc70ed \\\n    --hash=sha256:896ebdcf62683551312c30e20614305f53125750803b614e9e6ce74a96232604 \\\n    --hash=sha256:92d16a3e275e38293623ebf639c471d3e03bb20b8ebb845237e0d3664914caef \\\n    --hash=sha256:99f60d34c048c5c2fabc766108c103612344c46e35d4ed9ae0673d33c8fb26e8 \\\n    --hash=sha256:9fe7b0653ba3d9d65cbe7698cca585bf0f8c83dbbcc710db9c90f478e175f2d5 \\\n    --hash=sha256:a3145cb08d8625b2d3fee1b2d596a8766352979c9bffe5d7833e0503d0f0b5e5 \\\n    --hash=sha256:aeaf541ddbad8311a87dd695ed9642401131ea39ad7bc8cf3ef3967fd093b626 \\\n    --hash=sha256:b55358304d7a73d7bdf5de62494aaf70bd33015831ffd98bc498b433dfe5b10c \\\n    --hash=sha256:b82cc8ace10ab5bd93235dfaab2021c70637005e1ac787031f4d1da63d493c1d \\\n    --hash=sha256:c0868d64af83169e4d4152ec612637a543f7a336e4a307b119e98042e852ad9c \\\n    --hash=sha256:c1c1496e73051918fcd4f58ff2e0f2f3066d1c76a0c6aeffd9b45d53243702cc \\\n    --hash=sha256:c9bf56195c6bbd293340ea82eafd0071cb3d450c703d2c93afb89f93b8386ccc \\\n    --hash=sha256:cbebcd5bcaf1eaf302617c114aa67569dd3f090dd0ce8ba9e35e9985b41ac35b \\\n    --hash=sha256:cd6c8fca38178e12c00418de737aef1261576bd1b6e8c6134d3e729a4e858b38 \\\n    --hash=sha256:ceb3b7e6a0135e092de86110c5a74e46bda4bd4fbfeeb3a3bcec79c0f861e450 \\\n    --hash=sha256:cf590b134eb70629e350691ecca88eac3e3b8b3c86992042fb82e3cb1830d5e1 \\\n    --hash=sha256:d3eb1ceec286eba8220c26f3b0096cf189aea7057b6e7b7a2e60ed36b373b77f \\\n    --hash=sha256:d65f25da8e248202bd47445cec78e0025c0fe7582b23ec69c3b27a640dd7a8e3 \\\n    --hash=sha256:d6f6d4f185481c9669b9447bf9d9cf3b95a0e9df9d169bbc17e363b7d5487755 \\\n    --hash=sha256:d84a5c3a5f7ce6db1f999fb9438f686bc2e09d38143f2d93d8406ed2dd6b9226 \\\n    --hash=sha256:d946b0a9eb8aaa590df1fe082cee553ceab173e6cb5b03239716338629c50c7a \\\n    --hash=sha256:dce1c6912ab9ff5f179eaf6efe7365c1f425ed690b03341911bf4939ef2f3046 \\\n    --hash=sha256:de170c7b4fe6859beb8926e84f7d7d6c693dfe8e27372ce3b76f01c46e489fcf \\\n    --hash=sha256:e02021f87a5b6932fa6ce916ca004c4d441509d33bbdbeca70d05dff5e9d2479 \\\n    --hash=sha256:e030047e85cbcedbfc073f71836d62dd5dadfbe7531cae27789ff66bc551bd5e \\\n    --hash=sha256:e0e79d91e71b9867c73323a3444724d496c037e578a0e1755ae159ba14f4f3d1 \\\n    --hash=sha256:e4428b29611e989719874670fd152b6625500ad6c686d464e99f5aaeeaca175a \\\n    --hash=sha256:e4972624066095e52b569e02b5ca97dbd7a7ddd4294bf4e7247d52635630dd83 \\\n    --hash=sha256:e7be68734bd8c9a513f2b0cfd508802d6609da068f40dc57d4e3494cefc92929 \\\n    --hash=sha256:e8e94e6912639a02ce173341ff62cc1201232ab86b8a8fcc05572741a5dc7d93 \\\n    --hash=sha256:ea1456df2a27c73ce51120fa2f519f1bea2f4a03a917f4a43c8707cf4cbbae1a \\\n    --hash=sha256:ebd8d160f91a764652d3e51ce0d2956b38efe37c9231cd82cfc0bed2e40b581c \\\n    --hash=sha256:eca2e9d0cc5a889850e9bbd68e98314ada174ff6ccd1129500103df7a94a7a44 \\\n    --hash=sha256:edd08e6f2f1a390bf137080507e44ccc086353c8e98c657e666c017718561b89 \\\n    --hash=sha256:f285e862d2f153a70586579c15c44656f888806ed0e5b56b64489afe4a2dbfba \\\n    --hash=sha256:f2a1dee728b52b33eebff5072817176c172050d44d67befd681609b4746e1c2e \\\n    --hash=sha256:f7e301075edaf50500f0b341543c41194d8df3ae5caf4702f2095f3ca73dd8da \\\n    --hash=sha256:fb616be3538599e797a2017cccca78e354c767165e8858ab5116813146041a24 \\\n    --hash=sha256:fce28b3c8a81b6b36dfac9feb1de115bab619b3c13905b419ec71d03a3fc1423 \\\n    --hash=sha256:fe5d7785250541f7f5019ab9cba2c71169dc7d74d0f45253f8313f436458a4ef\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   aiohttp\n    #   yarl\nnetworkx==3.2.1 \\\n    --hash=sha256:9f1bb5cf3409bf324e0a722c20bdb4c20ee39bf1c30ce8ae499c8502b0b5e0c6 \\\n    --hash=sha256:f18c69adc97877c42332c170849c96cefa91881c99a7cb3e95b7c659ebdc1ec2\n    # via\n```\n\n----------------------------------------\n\nTITLE: Defining sniffio dependency with version pinning and hash verification\nDESCRIPTION: Specifies sniffio version 1.3.1 with SHA-256 hashes for verification. Comments indicate it's required by compiled ray test requirements, anyio, and openai.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_38\n\nLANGUAGE: plaintext\nCODE:\n```\nsniffio==1.3.1 \\\n    --hash=sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2 \\\n    --hash=sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   anyio\n    #   openai\n```\n\n----------------------------------------\n\nTITLE: Specifying Pandocfilters Package with Hash Verification\nDESCRIPTION: Defines the pandocfilters package version 1.5.0 with SHA256 hash verification and notes that it's required by nbconvert.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_24\n\nLANGUAGE: text\nCODE:\n```\npandocfilters==1.5.0 \\\n    --hash=sha256:0b679503337d233b4339a817bfc8c50064e2eff681314376a47cb582305a7a38 \\\n    --hash=sha256:33aae3f25fd1a026079f5d27bdd52496f0e0803b3469282162bafdcbdf6ef14f\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   nbconvert\n```\n\n----------------------------------------\n\nTITLE: Specifying partial-json-parser 0.2.1.1.post5 Dependency with Hash Verification\nDESCRIPTION: Defines the partial-json-parser package at version 0.2.1.1.post5 with SHA256 hash verification values. This is imported from requirements_compiled_rayllm_test_py311_cu121.txt and used by vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_24\n\nLANGUAGE: plaintext\nCODE:\n```\npartial-json-parser==0.2.1.1.post5 \\\n    --hash=sha256:627715aaa3cb3fb60a65b0d62223243acaa6c70846520a90326fef3a2f0b61ca \\\n    --hash=sha256:992710ac67e90b367921d52727698928040f7713ba7ecb33b96371ea7aec82ca\n```\n\n----------------------------------------\n\nTITLE: Defining CFFI Package with Platform-Specific Condition\nDESCRIPTION: This snippet specifies the CFFI package with a version constraint and a platform-specific condition. It includes hash values for verification and indicates that it's not required for PyPy implementations.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_6\n\nLANGUAGE: Text\nCODE:\n```\ncffi==1.16.0 ; platform_python_implementation != 'PyPy' \\\n    --hash=sha256:0c9ef6ff37e974b73c25eecc13952c55bceed9112be2d9d938ded8e856138bcc \\\n    --hash=sha256:131fd094d1065b19540c3d72594260f118b231090295d8c34e19a7bbcf2e860a \\\n    --hash=sha256:1b8ebc27c014c59692bb2664c7d13ce7a6e9a629be20e54e7271fa696ff2b417 \\\n    --hash=sha256:2c56b361916f390cd758a57f2e16233eb4f64bcbeee88a4881ea90fca14dc6ab \\\n    --hash=sha256:2d92b25dbf6cae33f65005baf472d2c245c050b1ce709cc4588cdcdd5495b520 \\\n    --hash=sha256:31d13b0f99e0836b7ff893d37af07366ebc90b678b6664c955b54561fc36ef36 \\\n    --hash=sha256:32c68ef735dbe5857c810328cb2481e24722a59a2003018885514d4c09af9743 \\\n    --hash=sha256:3686dffb02459559c74dd3d81748269ffb0eb027c39a6fc99502de37d501faa8 \\\n    --hash=sha256:582215a0e9adbe0e379761260553ba11c58943e4bbe9c36430c4ca6ac74b15ed \\\n    --hash=sha256:5b50bf3f55561dac5438f8e70bfcdfd74543fd60df5fa5f62d94e5867deca684 \\\n    --hash=sha256:5bf44d66cdf9e893637896c7faa22298baebcd18d1ddb6d2626a6e39793a1d56 \\\n    --hash=sha256:6602bc8dc6f3a9e02b6c22c4fc1e47aa50f8f8e6d3f78a5e16ac33ef5fefa324 \\\n    --hash=sha256:673739cb539f8cdaa07d92d02efa93c9ccf87e345b9a0b556e3ecc666718468d \\\n```\n\n----------------------------------------\n\nTITLE: Specifying SciPy Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the SciPy package dependency with version 1.11.4 and multiple SHA-256 hashes for verification. Comments show this package is required by requirements_byod_3.9.in and several other packages like lightgbm, scikit-learn, and xgboost.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_37\n\nLANGUAGE: pip\nCODE:\n```\nscipy==1.11.4 \\\n    --hash=sha256:00150c5eae7b610c32589dda259eacc7c4f1665aedf25d921907f4d08a951b1c \\\n    --hash=sha256:028eccd22e654b3ea01ee63705681ee79933652b2d8f873e7949898dda6d11b6 \\\n    --hash=sha256:1b7c3dca977f30a739e0409fb001056484661cb2541a01aba0bb0029f7b68db8 \\\n    --hash=sha256:2c6ff6ef9cc27f9b3db93a6f8b38f97387e6e0591600369a297a50a8e96e835d \\\n    --hash=sha256:36750b7733d960d7994888f0d148d31ea3017ac15eef664194b4ef68d36a4a97 \\\n    --hash=sha256:530f9ad26440e85766509dbf78edcfe13ffd0ab7fec2560ee5c36ff74d6269ff \\\n    --hash=sha256:5e347b14fe01003d3b78e196e84bd3f48ffe4c8a7b8a1afbcb8f5505cb710993 \\\n    --hash=sha256:6550466fbeec7453d7465e74d4f4b19f905642c89a7525571ee91dd7adabb5a3 \\\n    --hash=sha256:6df1468153a31cf55ed5ed39647279beb9cfb5d3f84369453b49e4b8502394fd \\\n    --hash=sha256:6e619aba2df228a9b34718efb023966da781e89dd3d21637b27f2e54db0410d7 \\\n    --hash=sha256:8fce70f39076a5aa62e92e69a7f62349f9574d8405c0a5de6ed3ef72de07f446 \\\n    --hash=sha256:90a2b78e7f5733b9de748f589f09225013685f9b218275257f8a8168ededaeaa \\\n    --hash=sha256:91af76a68eeae0064887a48e25c4e616fa519fa0d38602eda7e0f97d65d57937 \\\n    --hash=sha256:933baf588daa8dc9a92c20a0be32f56d43faf3d1a60ab11b3f08c356430f6e56 \\\n    --hash=sha256:acf8ed278cc03f5aff035e69cb511741e0418681d25fbbb86ca65429c4f4d9cd \\\n    --hash=sha256:ad669df80528aeca5f557712102538f4f37e503f0c5b9541655016dd0932ca79 \\\n    --hash=sha256:b030c6674b9230d37c5c60ab456e2cf12f6784596d15ce8da9365e70896effc4 \\\n    --hash=sha256:b9999c008ccf00e8fbcce1236f85ade5c569d13144f77a1946bef8863e8f6eb4 \\\n    --hash=sha256:bc9a714581f561af0848e6b69947fda0614915f072dfd14142ed1bfe1b806710 \\\n    --hash=sha256:ce7fff2e23ab2cc81ff452a9444c215c28e6305f396b2ba88343a567feec9660 \\\n    --hash=sha256:cf00bd2b1b0211888d4dc75656c0412213a8b25e80d73898083f402b50f47e41 \\\n    --hash=sha256:d10e45a6c50211fe256da61a11c34927c68f277e03138777bdebedd933712fea \\\n    --hash=sha256:ee410e6de8f88fd5cf6eadd73c135020bfbbbdfcd0f6162c36a7638a1ea8cc65 \\\n    --hash=sha256:f313b39a7e94f296025e3cffc2c567618174c0b1dde173960cf23808f9fae4be \\\n    --hash=sha256:f3cd9e7b3c2c1ec26364856f9fbe78695fe631150f94cd1c22228456404cf1ec\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n    #   lightgbm\n    #   scikit-learn\n    #   xgboost\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Decoder Buffer Processing\nDESCRIPTION: Java stack trace focusing on Netty's HTTP decoder buffer processing. The trace shows how Netty processes byte buffers during HTTP message decoding, with emphasis on buffer iteration and character sequence appending.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_99\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/handler/codec/http/HttpObjectDecoder:.decode_[j];io/netty/buffer/AbstractByteBuf:.forEachByteAsc0_[j];io/netty/util/internal/AppendableCharSequence:.append_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Package Hash Definitions\nDESCRIPTION: SHA256 hash values for package verification, including extensive list of hashes for the regex v2024.11.6 and referencing v0.36.2 packages. The hashes are used to verify package integrity during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_43\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:b63731993cdddcc8e087c64e9cf003f909262b359110070183d7f3025d1c56b5 \\\n--hash=sha256:b6907da3017ef55139cf0e417c5123a84c7332520e73a6902ff1f79046cd3b94\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Comments\nDESCRIPTION: This snippet shows how to specify Python package dependencies with version numbers, hash values, and comments indicating the source or purpose of the dependency. It includes packages like mistral-common, mistune, mpmath, and msgpack.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_19\n\nLANGUAGE: Text\nCODE:\n```\nmistral-common==1.5.4 \\\n    --hash=sha256:0af4124ab09d1409761e91ec61681476882d46f9418eea8908d39c01222e0f6b \\\n    --hash=sha256:acef3367a4386d5dd3d9e23330348bbebe90a5cbd2fc5587d8a8d13d9893e537\n    # via vllm\nmistune==0.8.4 \\\n    --hash=sha256:59a3429db53c50b5c6bcc8a07f8848cb00d7dc8bdb431a4ab41920d201d4756e \\\n    --hash=sha256:88a1051873018da288eee8538d476dffe1262495144b33ecb586c4ab266bb8d4\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   nbconvert\nmpmath==1.3.0 \\\n    --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n    --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n    # via sympy\nmsgpack==1.0.7 \\\n    --hash=sha256:04ad6069c86e531682f9e1e71b71c1c3937d6014a7c3e9edd2aa81ad58842862 \\\n    --hash=sha256:0bfdd914e55e0d2c9e1526de210f6fe8ffe9705f2b1dfcc4aecc92a4cb4b533d \\\n    --hash=sha256:1dc93e8e4653bdb5910aed79f11e165c85732067614f180f70534f056da97db3 \\\n    --hash=sha256:1e2d69948e4132813b8d1131f29f9101bc2c915f26089a6d632001a5c1349672 \\\n    --hash=sha256:235a31ec7db685f5c82233bddf9858748b89b8119bf4538d514536c485c15fe0 \\\n    --hash=sha256:27dcd6f46a21c18fa5e5deed92a43d4554e3df8d8ca5a47bf0615d6a5f39dbc9 \\\n    --hash=sha256:28efb066cde83c479dfe5a48141a53bc7e5f13f785b92ddde336c716663039ee \\\n    --hash=sha256:3476fae43db72bd11f29a5147ae2f3cb22e2f1a91d575ef130d2bf49afd21c46 \\\n    --hash=sha256:36e17c4592231a7dbd2ed09027823ab295d2791b3b1efb2aee874b10548b7524 \\\n    --hash=sha256:384d779f0d6f1b110eae74cb0659d9aa6ff35aaf547b3955abf2ab4c901c4819 \\\n    --hash=sha256:38949d30b11ae5f95c3c91917ee7a6b239f5ec276f271f28638dec9156f82cfc \\\n    --hash=sha256:3967e4ad1aa9da62fd53e346ed17d7b2e922cba5ab93bdd46febcac39be636fc \\\n    --hash=sha256:3e7bf4442b310ff154b7bb9d81eb2c016b7d597e364f97d72b1acc3817a0fdc1 \\\n    --hash=sha256:3f0c8c6dfa6605ab8ff0611995ee30d4f9fcff89966cf562733b4008a3d60d82 \\\n    --hash=sha256:484ae3240666ad34cfa31eea7b8c6cd2f1fdaae21d73ce2974211df099a95d81 \\\n    --hash=sha256:4a7b4f35de6a304b5533c238bee86b670b75b03d31b7797929caa7a624b5dda6 \\\n    --hash=sha256:4cb14ce54d9b857be9591ac364cb08dc2d6a5c4318c1182cb1d02274029d590d \\\n    --hash=sha256:4e71bc4416de195d6e9b4ee93ad3f2f6b2ce11d042b4d7a7ee00bbe0358bd0c2 \\\n    --hash=sha256:52700dc63a4676669b341ba33520f4d6e43d3ca58d422e22ba66d1736b0a6e4c \\\n    --hash=sha256:572efc93db7a4d27e404501975ca6d2d9775705c2d922390d878fcf768d92c87 \\\n    --hash=sha256:576eb384292b139821c41995523654ad82d1916da6a60cff129c715a6223ea84 \\\n    --hash=sha256:5b0bf0effb196ed76b7ad883848143427a73c355ae8e569fa538365064188b8e \\\n    --hash=sha256:5b6ccc0c85916998d788b295765ea0e9cb9aac7e4a8ed71d12e7d8ac31c23c95 \\\n\n```\n\n----------------------------------------\n\nTITLE: Submitting Ray Job with Custom Runtime Environment (Bash)\nDESCRIPTION: This bash command submits a Ray job with a custom runtime environment. It uses the '--runtime-env-json' option to specify a particular version of the 'requests' module to be installed in the job's environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/quickstart.rst#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ ray job submit --runtime-env-json='{\"pip\": [\"requests==2.26.0\"]}' -- python script.py\n```\n\n----------------------------------------\n\nTITLE: Installing pyasn1-modules with Pinned Version and Hashes\nDESCRIPTION: Specifies pyasn1-modules package with version 0.3.0 and SHA256 hashes for verification. Comments indicate this is required by google-auth and oauth2client packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_25\n\nLANGUAGE: pip\nCODE:\n```\npyasn1-modules==0.3.0 \\\n    --hash=sha256:5bd01446b736eb9d31512a30d46c1ac3395d676c6f3cafa4c03eb54b9925631c \\\n    --hash=sha256:d3ccd6ed470d9ffbc716be08bd90efbd44d0734bc9303818f7336070984a162d\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   google-auth\n    #   oauth2client\n```\n\n----------------------------------------\n\nTITLE: Ray Performance Benchmark Results\nDESCRIPTION: Comprehensive benchmark results showing operations per second with standard deviations for Ray's core components including Plasma Store operations, task execution, and actor communication patterns.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.3/microbenchmark.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsingle client get calls (Plasma Store) per second 9609.58 +- 1094.38\nsingle client put calls (Plasma Store) per second 5244.46 +- 96.12\nsingle client put gigabytes per second 13.5 +- 3.12\nmulti client put calls (Plasma Store) per second 11147.13 +- 25.58\nmulti client put gigabytes per second 19.08 +- 7.08\nsingle client tasks sync per second 1271.4 +- 14.02\nsingle client tasks async per second 12516.66 +- 392.73\nmulti client tasks async per second 36893.52 +- 1313.69\n1:1 actor calls sync per second 1928.61 +- 43.71\n1:1 actor calls async per second 7219.07 +- 147.61\n1:1 actor calls concurrent per second 6267.15 +- 67.2\n1:n actor calls async per second 9926.58 +- 143.94\nn:n actor calls async per second 34545.18 +- 355.83\nn:n actor calls with arg async per second 12897.18 +- 203.78\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and SHA256 hashes. It includes multiple hash entries for each package to ensure integrity and security.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\ncharset-normalizer==3.3.2 \\\n    --hash=sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027 \\\n    --hash=sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087 \\\n    --hash=sha256:0a55554a2fa0d408816b3b5cedf0045f4b8e1a6065aec45849de2d6f3f8e9786 \\\n    --hash=sha256:0b2b64d2bb6d3fb9112bafa732def486049e63de9618b5843bcdd081d8144cd8 \\\n    --hash=sha256:10955842570876604d404661fbccbc9c7e684caf432c09c715ec38fbae45ae09 \\\n    --hash=sha256:122c7fa62b130ed55f8f285bfd56d5f4b4a5b503609d181f9ad85e55c89f4185 \\\n    --hash=sha256:1ceae2f17a9c33cb48e3263960dc5fc8005351ee19db217e9b1bb15d28c02574 \\\n    --hash=sha256:1d3193f4a680c64b4b6a9115943538edb896edc190f0b222e73761716519268e \\\n    --hash=sha256:1f79682fbe303db92bc2b1136016a38a42e835d932bab5b3b1bfcfbf0640e519 \\\n    --hash=sha256:2127566c664442652f024c837091890cb1942c30937add288223dc895793f898 \\\n    --hash=sha256:22afcb9f253dac0696b5a4be4a1c0f8762f8239e21b99680099abd9b2b1b2269 \\\n    --hash=sha256:25baf083bf6f6b341f4121c2f3c548875ee6f5339300e08be3f2b2ba1721cdd3 \\\n    --hash=sha256:2e81c7b9c8979ce92ed306c249d46894776a909505d8f5a4ba55b14206e3222f \\\n    --hash=sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6 \\\n    --hash=sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8 \\\n    --hash=sha256:37e55c8e51c236f95b033f6fb391d7d7970ba5fe7ff453dad675e88cf303377a \\\n    --hash=sha256:3d47fa203a7bd9c5b6cee4736ee84ca03b8ef23193c0d1ca99b5089f72645c73 \\\n    --hash=sha256:3e4d1f6587322d2788836a99c69062fbb091331ec940e02d12d179c1d53e25fc \\\n    --hash=sha256:42cb296636fcc8b0644486d15c12376cb9fa75443e00fb25de0b8602e64c1714 \\\n    --hash=sha256:45485e01ff4d3630ec0d9617310448a8702f70e9c01906b0d0118bdf9d124cf2 \\\n    --hash=sha256:4a78b2b446bd7c934f5dcedc588903fb2f5eec172f3d29e52a9096a43722adfc \\\n    --hash=sha256:4ab2fe47fae9e0f9dee8c04187ce5d09f48eabe611be8259444906793ab7cbce \\\n    --hash=sha256:4d0d1650369165a14e14e1e47b372cfcb31d6ab44e6e33cb2d4e57265290044d \\\n    --hash=sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e \\\n    --hash=sha256:55086ee1064215781fff39a1af09518bc9255b50d6333f2e4c74ca09fac6a8f6 \\\n    --hash=sha256:572c3763a264ba47b3cf708a44ce965d98555f618ca42c926a9c1616d8f34269 \\\n    --hash=sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96 \\\n    --hash=sha256:5b4c145409bef602a690e7cfad0a15a55c13320ff7a3ad7ca59c13bb8ba4d45d \\\n    --hash=sha256:6463effa3186ea09411d50efc7d85360b38d5f09b870c48e4600f63af490e56a \\\n    --hash=sha256:65f6f63034100ead094b8744b3b97965785388f308a64cf8d7c34f2f2e5be0c4 \\\n    --hash=sha256:663946639d296df6a2bb2aa51b60a2454ca1cb29835324c640dafb5ff2131a77 \\\n    --hash=sha256:6897af51655e3691ff853668779c7bad41579facacf5fd7253b0133308cf000d \\\n    --hash=sha256:68d1f8a9e9e37c1223b656399be5d6b448dea850bed7d0f87a8311f1ff3dabb0 \\\n    --hash=sha256:6ac7ffc7ad6d040517be39eb591cac5ff87416c2537df6ba3cba3bae290c0fed \\\n    --hash=sha256:6b3251890fff30ee142c44144871185dbe13b11bab478a88887a639655be1068 \\\n    --hash=sha256:6c4caeef8fa63d06bd437cd4bdcf3ffefe6738fb1b25951440d80dc7df8c03ac \\\n    --hash=sha256:6ef1d82a3af9d3eecdba2321dc1b3c238245d890843e040e41e470ffa64c3e25 \\\n    --hash=sha256:753f10e867343b4511128c6ed8c82f7bec3bd026875576dfd88483c5c73b2fd8 \\\n    --hash=sha256:7cd13a2e3ddeed6913a65e66e94b51d80a041145a026c27e6bb76c31a853c6ab \\\n    --hash=sha256:7ed9e526742851e8d5cc9e6cf41427dfc6068d4f5a3bb03659444b4cabf6bc26 \\\n    --hash=sha256:7f04c839ed0b6b98b1a7501a002144b76c18fb1c1850c8b98d458ac269e26ed2 \\\n    --hash=sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db \\\n    --hash=sha256:80402cd6ee291dcb72644d6eac93785fe2c8b9cb30893c1af5b8fdd753b9d40f \\\n    --hash=sha256:8465322196c8b4d7ab6d1e049e4c5cb460d0394da4a27d23cc242fbf0034b6b5 \\\n    --hash=sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99 \\\n    --hash=sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c \\\n    --hash=sha256:8bdb58ff7ba23002a4c5808d608e4e6c687175724f54a5dade5fa8c67b604e4d \\\n    --hash=sha256:8c622a5fe39a48f78944a87d4fb8a53ee07344641b0562c540d840748571b811 \\\n    --hash=sha256:8d756e44e94489e49571086ef83b2bb8ce311e730092d2c34ca8f7d925cb20aa \\\n    --hash=sha256:8f4a014bc36d3c57402e2977dada34f9c12300af536839dc38c0beab8878f38a \\\n    --hash=sha256:9063e24fdb1e498ab71cb7419e24622516c4a04476b17a2dab57e8baa30d6e03 \\\n    --hash=sha256:90d558489962fd4918143277a773316e56c72da56ec7aa3dc3dbbe20fdfed15b \\\n    --hash=sha256:923c0c831b7cfcb071580d3f46c4baf50f174be571576556269530f4bbd79d04 \\\n    --hash=sha256:95f2a5796329323b8f0512e09dbb7a1860c46a39da62ecb2324f116fa8fdc85c \\\n    --hash=sha256:96b02a3dc4381e5494fad39be677abcb5e6634bf7b4fa83a6dd3112607547001 \\\n    --hash=sha256:9f96df6923e21816da7e0ad3fd47dd8f94b2a5ce594e00677c0013018b813458 \\\n    --hash=sha256:a10af20b82360ab00827f916a6058451b723b4e65030c5a18577c8b2de5b3389 \\\n    --hash=sha256:a50aebfa173e157099939b17f18600f72f84eed3049e743b68ad15bd69b6bf99 \\\n    --hash=sha256:a981a536974bbc7a512cf44ed14938cf01030a99e9b3a06dd59578882f06f985 \\\n    --hash=sha256:a9a8e9031d613fd2009c182b69c7b2c1ef8239a0efb1df3f7c8da66d5dd3d537 \\\n    --hash=sha256:ae5f4161f18c61806f411a13b0310bea87f987c7d2ecdbdaad0e94eb2e404238 \\\n    --hash=sha256:aed38f6e4fb3f5d6bf81bfa990a07806be9d83cf7bacef998ab1a9bd660a581f \\\n    --hash=sha256:b01b88d45a6fcb69667cd6d2f7a9aeb4bf53760d7fc536bf679ec94fe9f3ff3d \\\n    --hash=sha256:b261ccdec7821281dade748d088bb6e9b69e6d15b30652b74cbbac25e280b796 \\\n    --hash=sha256:b2b0a0c0517616b6869869f8c581d4eb2dd83a4d79e0ebcb7d373ef9956aeb0a \\\n    --hash=sha256:b4a23f61ce87adf89be746c8a8974fe1c823c891d8f86eb218bb957c924bb143 \\\n    --hash=sha256:bd8f7df7d12c2db9fab40bdd87a7c09b1530128315d047a086fa3ae3435cb3a8 \\\n    --hash=sha256:beb58fe5cdb101e3a055192ac291b7a21e3b7ef4f67fa1d74e331a7f2124341c \\\n    --hash=sha256:c002b4ffc0be611f0d9da932eb0f704fe2602a9a949d1f738e4c34c75b0863d5 \\\n    --hash=sha256:c083af607d2515612056a31f0a8d9e0fcb5876b7bfc0abad3ecd275bc4ebc2d5 \\\n    --hash=sha256:c180f51afb394e165eafe4ac2936a14bee3eb10debc9d9e4db8958fe36afe711 \\\n    --hash=sha256:c235ebd9baae02f1b77bcea61bce332cb4331dc3617d254df3323aa01ab47bd4 \\\n    --hash=sha256:cd70574b12bb8a4d2aaa0094515df2463cb429d8536cfb6c7ce983246983e5a6 \\\n    --hash=sha256:d0eccceffcb53201b5bfebb52600a5fb483a20b61da9dbc885f8b103cbe7598c \\\n    --hash=sha256:d965bba47ddeec8cd560687584e88cf699fd28f192ceb452d1d7ee807c5597b7 \\\n    --hash=sha256:db364eca23f876da6f9e16c9da0df51aa4f104a972735574842618b8c6d999d4 \\\n    --hash=sha256:ddbb2551d7e0102e7252db79ba445cdab71b26640817ab1e3e3648dad515003b\n```\n\n----------------------------------------\n\nTITLE: Defining Task-to-Column Mapping for GLUE Tasks\nDESCRIPTION: This code defines a dictionary mapping GLUE tasks to their corresponding input column names in the dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntask_to_keys = {\n    \"cola\": (\"sentence\", None),\n    \"mnli\": (\"premise\", \"hypothesis\"),\n    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\n    \"qnli\": (\"question\", \"sentence\"),\n    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),\n    \"sst2\": (\"sentence\", None),\n    \"stsb\": (\"sentence1\", \"sentence2\"),\n    \"wnli\": (\"sentence1\", \"sentence2\"),\n}\n```\n\n----------------------------------------\n\nTITLE: Defining pyarrow Dependency for Ray Project\nDESCRIPTION: This snippet specifies the pyarrow package dependency with version 14.0.2 and comprehensive hash verification. PyArrow is a Python library for Apache Arrow which is directly required by Ray for efficient in-memory data serialization and interchange.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_32\n\nLANGUAGE: pip\nCODE:\n```\npyarrow==14.0.2 \\\n    --hash=sha256:059bd8f12a70519e46cd64e1ba40e97eae55e0cbe1695edd95384653d7626b23 \\\n    --hash=sha256:06ff1264fe4448e8d02073f5ce45a9f934c0f3db0a04460d0b01ff28befc3696 \\\n    --hash=sha256:1e6987c5274fb87d66bb36816afb6f65707546b3c45c44c28e3c4133c010a881 \\\n    --hash=sha256:209bac546942b0d8edc8debda248364f7f668e4aad4741bae58e67d40e5fcf75 \\\n    --hash=sha256:20e003a23a13da963f43e2b432483fdd8c38dc8882cd145f09f21792e1cf22a1 \\\n    --hash=sha256:22a768987a16bb46220cef490c56c671993fbee8fd0475febac0b3e16b00a10e \\\n    --hash=sha256:2cc61593c8e66194c7cdfae594503e91b926a228fba40b5cf25cc593563bcd07 \\\n    --hash=sha256:2dbba05e98f247f17e64303eb876f4a80fcd32f73c7e9ad975a83834d81f3fda \\\n    --hash=sha256:32356bfb58b36059773f49e4e214996888eeea3a08893e7dbde44753799b2a02 \\\n    --hash=sha256:36cef6ba12b499d864d1def3e990f97949e0b79400d08b7cf74504ffbd3eb025 \\\n    --hash=sha256:37c233ddbce0c67a76c0985612fef27c0c92aef9413cf5aa56952f359fcb7379 \\\n    --hash=sha256:3c0fa3bfdb0305ffe09810f9d3e2e50a2787e3a07063001dcd7adae0cee3601a \\\n    --hash=sha256:3f16111f9ab27e60b391c5f6d197510e3ad6654e73857b4e394861fc79c37200 \\\n    --hash=sha256:52809ee69d4dbf2241c0e4366d949ba035cbcf48409bf404f071f624ed313a2b \\\n    --hash=sha256:5c1da70d668af5620b8ba0a23f229030a4cd6c5f24a616a146f30d2386fec422 \\\n    --hash=sha256:63ac901baec9369d6aae1cbe6cca11178fb018a8d45068aaf5bb54f94804a866 \\\n    --hash=sha256:64df2bf1ef2ef14cee531e2dfe03dd924017650ffaa6f9513d7a1bb291e59c15 \\\n    --hash=sha256:66e986dc859712acb0bd45601229021f3ffcdfc49044b64c6d071aaf4fa49e98 \\\n    --hash=sha256:6dd4f4b472ccf4042f1eab77e6c8bce574543f54d2135c7e396f413046397d5a \\\n    --hash=sha256:75ee0efe7a87a687ae303d63037d08a48ef9ea0127064df18267252cfe2e9541 \\\n    --hash=sha256:76fc257559404ea5f1306ea9a3ff0541bf996ff3f7b9209fc517b5e83811fa8e \\\n    --hash=sha256:78ea56f62fb7c0ae8ecb9afdd7893e3a7dbeb0b04106f5c08dbb23f9c0157591 \\\n    --hash=sha256:87482af32e5a0c0cce2d12eb3c039dd1d853bd905b04f3f953f147c7a196915b \\\n    --hash=sha256:87e879323f256cb04267bb365add7208f302df942eb943c93a9dfeb8f44840b1 \\\n    --hash=sha256:a01d0052d2a294a5f56cc1862933014e696aa08cc7b620e8c0cce5a5d362e976 \\\n    --hash=sha256:a25eb2421a58e861f6ca91f43339d215476f4fe159eca603c55950c14f378cc5 \\\n    --hash=sha256:a51fee3a7db4d37f8cda3ea96f32530620d43b0489d169b285d774da48ca9785 \\\n    --hash=sha256:a898d134d00b1eca04998e9d286e19653f9d0fcb99587310cd10270907452a6b \\\n    --hash=sha256:b0c4a18e00f3a32398a7f31da47fefcd7a927545b396e1f15d0c85c2f2c778cd \\\n    --hash=sha256:ba9fe808596c5dbd08b3aeffe901e5f81095baaa28e7d5118e01354c64f22807 \\\n    --hash=sha256:c65bf4fd06584f058420238bc47a316e80dda01ec0dfb3044594128a6c2db794 \\\n    --hash=sha256:c87824a5ac52be210d32906c715f4ed7053d0180c1060ae3ff9b7e560f53f944 \\\n    --hash=sha256:e354fba8490de258be7687f341bc04aba181fc8aa1f71e4584f9890d9cb2dec2 \\\n    --hash=sha256:e4b123ad0f6add92de898214d404e488167b87b5dd86e9a434126bc2b7a5578d \\\n    --hash=sha256:f7d029f20ef56673a9730766023459ece397a05001f4e4d13805111d7c2108c0 \\\n    --hash=sha256:fc0de7575e841f1595ac07e5bc631084fd06ca8b03c0f2ecece733d23cd5102a\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Sources for Ray Project\nDESCRIPTION: This section defines the package sources for the Ray project, including the main PyPI index, an additional index for PyTorch wheels, and a find-links URL for specific torch packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cpu\n--find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html\n```\n\n----------------------------------------\n\nTITLE: Specifying PyYAML Package Version with Hashes\nDESCRIPTION: Defines the PyYAML package dependency with version 6.0.1 and multiple SHA256 hash verification values. PyYAML is required by multiple components including anyscale, jupyter-cache, jupytext, and sphinx-related packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_8\n\nLANGUAGE: txt\nCODE:\n```\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \\\n    --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \\\n    --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \\\n    --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \\\n    --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \\\n    --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \\\n    --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \\\n    --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \\\n    --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \\\n    --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \\\n    --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \\\n    --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \\\n    --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \\\n    --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \\\n    --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \\\n    --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \\\n    --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \\\n    --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \\\n    --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \\\n    --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \\\n    --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \\\n    --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \\\n    --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \\\n    --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \\\n    --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \\\n    --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \\\n    --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \\\n    --hash=sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef \\\n    --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \\\n    --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \\\n    --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \\\n    --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \\\n    --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \\\n    --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \\\n    --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \\\n    --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \\\n    --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \\\n    --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \\\n    --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \\\n    --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \\\n    --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \\\n    --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \\\n    --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \\\n    --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \\\n    --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \\\n    --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \\\n    --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \\\n    --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \\\n    --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f\n    # via\n    #   -r release/requirements_buildkite.in\n    #   anyscale\n    #   jupyter-cache\n    #   jupytext\n    #   myst-nb\n    #   myst-parser\n    #   sphinx-jsonschema\n    #   sphinxcontrib-redoc\n```\n\n----------------------------------------\n\nTITLE: Specifying ML Tracking Dependencies for Ray Project\nDESCRIPTION: This snippet defines the required versions of ML tracking integration libraries for the Ray project. It includes Comet ML, MLflow, and Weights & Biases (wandb) for experiment tracking and management.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/core-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# ML tracking integrations\ncomet-ml==3.44.1\nmlflow==2.9.2\nwandb==0.17.0\n```\n\n----------------------------------------\n\nTITLE: Defining Ray Cluster Configuration Structure in YAML\nDESCRIPTION: This YAML structure defines the top-level configuration options for a Ray cluster, including cluster name, worker settings, provider configuration, and node setup commands.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncluster_name: str\nmax_workers: int\nupscaling_speed: float\nidle_timeout_minutes: int\ndocker:\n    docker\nprovider:\n    provider\nauth:\n    auth\navailable_node_types:\n    node_types\nhead_node_type: str\nfile_mounts:\n    file_mounts\ncluster_synced_files:\n    - str\nrsync_exclude:\n    - str\nrsync_filter:\n    - str\ninitialization_commands:\n    - str\nsetup_commands:\n    - str\nhead_setup_commands:\n    - str\nworker_setup_commands:\n    - str\nhead_start_ray_commands:\n    - str\nworker_start_ray_commands:\n    - str\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Ray Development (Bash)\nDESCRIPTION: Commands to create and activate a conda environment named 'myenv' for Ray development using Python 3.9.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -c conda-forge python=3.9 -n myenv\nconda activate myenv\n```\n\n----------------------------------------\n\nTITLE: Specifying Pandas Package with Hash Verification in Bash\nDESCRIPTION: Defines the pandas package version 1.5.3 with multiple SHA256 hash verifications for secure package installation. This is a core data manipulation library required by the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    --hash=sha256:41179ce559943d83a9b4bbacb736b04c928b095b5f25dd2b7389eda08f46f373 \\\n    --hash=sha256:478ff646ca42b20376e4ed3fa2e8d7341e8a63105586efe54fa2508ee087f328 \\\n    --hash=sha256:50869a35cbb0f2e0cd5ec04b191e7b12ed688874bd05dd777c19b28cbea90996 \\\n    --hash=sha256:565fa34a5434d38e9d250af3c12ff931abaf88050551d9fbcdfafca50d62babf \\\n    --hash=sha256:5f2b952406a1588ad4cad5b3f55f520e82e902388a6d5a4a91baa8d38d23c7f6 \\\n    --hash=sha256:5fbcb19d6fceb9e946b3e23258757c7b225ba450990d9ed63ccceeb8cae609f7 \\\n    --hash=sha256:6973549c01ca91ec96199e940495219c887ea815b2083722821f1d7abfa2b4dc \\\n    --hash=sha256:74a3fd7e5a7ec052f183273dc7b0acd3a863edf7520f5d3a1765c04ffdb3b0b1 \\\n    --hash=sha256:7a0a56cef15fd1586726dace5616db75ebcfec9179a3a55e78f72c5639fa2a23 \\\n    --hash=sha256:7cec0bee9f294e5de5bbfc14d0573f65526071029d036b753ee6507d2a21480a \\\n    --hash=sha256:87bd9c03da1ac870a6d2c8902a0e1fd4267ca00f13bc494c9e5a9020920e1d51 \\\n    --hash=sha256:972d8a45395f2a2d26733eb8d0f629b2f90bebe8e8eddbb8829b180c09639572 \\\n    --hash=sha256:9842b6f4b8479e41968eced654487258ed81df7d1c9b7b870ceea24ed9459b31 \\\n    --hash=sha256:9f69c4029613de47816b1bb30ff5ac778686688751a5e9c99ad8c7031f6508e5 \\\n    --hash=sha256:a50d9a4336a9621cab7b8eb3fb11adb82de58f9b91d84c2cd526576b881a0c5a \\\n    --hash=sha256:bc4c368f42b551bf72fac35c5128963a171b40dce866fb066540eeaf46faa003 \\\n    --hash=sha256:c39a8da13cede5adcd3be1182883aea1c925476f4e84b2807a46e2775306305d \\\n    --hash=sha256:c3ac844a0fe00bfaeb2c9b51ab1424e5c8744f89860b138434a363b1f620f354 \\\n    --hash=sha256:c4c00e0b0597c8e4f59e8d461f797e5d70b4d025880516a8261b2817c47759ee \\\n    --hash=sha256:c74a62747864ed568f5a82a49a23a8d7fe171d0c69038b38cedf0976831296fa \\\n    --hash=sha256:dd05f7783b3274aa206a1af06f0ceed3f9b412cf665b7247eacd83be41cf7bf0 \\\n    --hash=sha256:dfd681c5dc216037e0b0a2c821f5ed99ba9f03ebcf119c7dac0e9a7b960b9ec9 \\\n    --hash=sha256:e474390e60ed609cec869b0da796ad94f420bb057d86784191eefc62b65819ae \\\n    --hash=sha256:f76d097d12c82a535fda9dfe5e8dd4127952b45fea9b0276cb30cca5ea313fbc\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry Proto Package with Hash Verification in Bash\nDESCRIPTION: Defines the OpenTelemetry proto package version 1.1.0 with SHA256 hash verification for secure package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-proto==1.1.0 \\\n    --hash=sha256:237ef4fdd7f752b2fe740352643f8ef82733bd8e0db8b46ed808125ac7c7f112 \\\n    --hash=sha256:ff1ad9a3c572075883c2af0053cefdfaba005d71eade783c4524d34660d53b60\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus Web UI via Port Forwarding\nDESCRIPTION: This command sets up port forwarding to access the Prometheus web interface on localhost:9090. It forwards the port from the Prometheus server pod in the prometheus-system namespace.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nkubectl port-forward prometheus-prometheus-kube-prometheus-prometheus-0 -n prometheus-system 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Specifying CloudWatch Agent AMI in Cluster Config (YAML)\nDESCRIPTION: This YAML snippet shows how to specify the CloudWatch Agent pre-installed AMI for head and worker nodes in the Ray cluster configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\navailable_node_types:\n    ray.head.default:\n        node_config:\n        InstanceType: c5a.large\n        ImageId: ami-0d88d9cbe28fac870\n    ray.worker.default:\n        node_config:\n        InstanceType: c5a.large\n        ImageId: ami-0d88d9cbe28fac870\n```\n\n----------------------------------------\n\nTITLE: Example RLlink PONG Response\nDESCRIPTION: Shows the structure of a PONG response sent by the server after receiving a PING message. It follows the same format with an 8-byte header and JSON body.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/external-envs.rst#2025-04-12_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n00000016{\"type\": \"PONG\"}\n```\n\n----------------------------------------\n\nTITLE: Monitoring and Observability Dependencies\nDESCRIPTION: OpenCensus and OpenTelemetry libraries for distributed tracing and metrics collection in Ray. These packages enable monitoring of Ray clusters and applications, helping with performance analysis and debugging of distributed workloads.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_21\n\nLANGUAGE: pip\nCODE:\n```\nopencensus==0.11.3 \\\n    --hash=sha256:9c33d572059f0f0e874fc34c697a39a4193aa9cf3203f7e777df42e9edeea56a \\\n    --hash=sha256:af7a98bd51e63968144d772f346d696ed498a32dbdc4be267cd6011c4ce05da8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\nopencensus-context==0.1.3 \\\n    --hash=sha256:073bb0590007af276853009fac7e4bab1d523c3f03baf4cb4511ca38967c6039 \\\n    --hash=sha256:a03108c3c10d8c80bb5ddf5c8a1f033161fa61972a9917f9b9b3a18517f0088c\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   opencensus\nopentelemetry-api==1.1.0 \\\n    --hash=sha256:38555cd773df903a2f7440778d6f8b48a86fd388604b171969bdbde4b746a558\n```\n\n----------------------------------------\n\nTITLE: Listing PyTorch-Lightning Package with Hash Values\nDESCRIPTION: Definition for the PyTorch-Lightning package dependency with version 1.8.6 and corresponding SHA256 hash values. The comment indicates this is required through both compiled requirements and ML BYOD requirements for Python 3.9.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_29\n\nLANGUAGE: text\nCODE:\n```\npytorch-lightning==1.8.6 \\\n    --hash=sha256:8b6b4126b85c56a9dd08a03f7096ce749bcb452a9a50f6201a7165dbd92d866d \\\n    --hash=sha256:c4af783579a1528e07f40dd9bd0128c162bbbcf74fe1ce4292fec63fa7e76ada\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_ml_byod_3.9.in\n```\n\n----------------------------------------\n\nTITLE: Installing BayesOpt and Ray Tune\nDESCRIPTION: This snippet installs the necessary libraries for running the example, including bayesian-optimization and ray[tune]. The `-q` flag is used for quiet installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\n!pip install -q bayesian-optimization==1.2.0 \"ray[tune]\"\n\n```\n\n----------------------------------------\n\nTITLE: Writing to a file in working directory\nDESCRIPTION: This snippet demonstrates writing data to a file within a specified working directory. It creates a 'hello.txt' file and writes 'Hello World!' to it.  This working directory will then be used as the runtime environment for the Ray job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"/tmp/runtime_env_working_dir/hello.txt\", \"w\") as hello_file:\n    hello_file.write(\"Hello World!\")\n```\n\n----------------------------------------\n\nTITLE: Specifying pandas Package Requirement\nDESCRIPTION: Defines the required version and hash values for the pandas package, with a specific Python version constraint. This ensures a specific, verified version is installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_19\n\nLANGUAGE: Text\nCODE:\n```\npandas==1.5.3 ; python_version < \"3.12\" \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406\n```\n\n----------------------------------------\n\nTITLE: Specifying openskill Package Requirement\nDESCRIPTION: Defines the required version and hash values for the openskill package. This ensures a specific, verified version is installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_17\n\nLANGUAGE: Text\nCODE:\n```\nopenskill==6.0.0 \\\n    --hash=sha256:eee2d0b3c1648663a480cf4680654dfd12bdc749a96d611b1904e191f2632f62 \\\n    --hash=sha256:f89b18930c2befd580407e7cf80a480bc69c3b25d2841346be6d875c8c4bc92e\n```\n\n----------------------------------------\n\nTITLE: Checking Deployment Status\nDESCRIPTION: Console command to verify the status of deployments after configuration update\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/inplace-updates.md#2025-04-12_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ serve status\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and SHA256 hashes for security verification. It includes comments indicating the packages that require these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_9\n\nLANGUAGE: Text\nCODE:\n```\njinja2==3.1.2 \\\n    --hash=sha256:31351a702a408a9e7595a8fc6150fc3f43bb6bf7e319770cbc0db9df9437e852 \\\n    --hash=sha256:6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   fastapi\n    #   memray\n    #   outlines\n    #   torch\n```\n\n----------------------------------------\n\nTITLE: PyTorch Benchmark Results Table\nDESCRIPTION: Markdown tables showing throughput comparison between non-compiled and compiled code across different batch sizes, backends, and modes\nSOURCE: https://github.com/ray-project/ray/blob/master/rllib/benchmarks/torch_compile/README.md#2025-04-12_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| batch_size | no compile throughput | compiled throughput | Speedup          |\n|------------|----------------------|---------------------|-------------------|\n| 1          | 578.1523201          | 531.928471          | -0.07995098789    |\n| 4          | 331.9012918          | 311.6321031         | -0.06106992993    |\n| 16         | 164.9463843          | 160.25641           | -0.0284333259     |\n```\n\n----------------------------------------\n\nTITLE: Specifying Jsonref Dependency for Ray Project\nDESCRIPTION: Defines the Jsonref package dependency with version 1.1.0 and verification hashes. This dependency is referenced in the LLM requirements file for the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_10\n\nLANGUAGE: pip\nCODE:\n```\njsonref==1.1.0 \\\n    --hash=sha256:32fe8e1d85af0fdefbebce950af85590b22b60f9e95443176adbde4e1ecea552 \\\n    --hash=sha256:590dc7773df6c21cbf948b5dac07a72a251db28b0238ceecce0a2abfa8ec30a9\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements/llm/llm-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Jinja2 Dependency for Ray Project\nDESCRIPTION: Defines the Jinja2 package dependency with version 3.1.2 and verification hashes. This dependency is used by FastAPI, Memray, Outlines, and Torch components within the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_8\n\nLANGUAGE: pip\nCODE:\n```\njinja2==3.1.2 \\\n    --hash=sha256:31351a702a408a9e7595a8fc6150fc3f43bb6bf7e319770cbc0db9df9437e852 \\\n    --hash=sha256:6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   fastapi\n    #   memray\n    #   outlines\n    #   torch\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with SHA256 Hashes for Ray Project\nDESCRIPTION: This code snippet contains a section of Python dependencies with their corresponding SHA256 hash values. Each dependency is listed with its version and multiple hash codes to ensure package authenticity. Comments at the end indicate which requirements files include these packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:068b63f23b17df8569b7fdca5517edef76171cf3897eb68beb01341131fbd2ad \\\n--hash=sha256:0c250a29735d4f15321007fb02865f0e6b6a41a6b88f1f523ca1596ab5f50bd5 \\\n--hash=sha256:1979bc0aeb89b33b588c51c54ab0161791149f2461ea7c7c946d95d5f93b56ae \\\n--hash=sha256:1a4471094e146b6790f61b98616ab8e44f72661879cc63fa1049d13ef711e71e \\\n--hash=sha256:1b280e6507ea8a4fa0c0a7150b4e526a8d113989e28eaaef946cc77ffd7efc0a \\\n--hash=sha256:1d0ce09d36d53bbbe566fe296965b23b961764c0bcf3ce2fa45f463745c04701 \\\n--hash=sha256:20b51fa3f588ff2fe658663db52a41a4f7aa6c04f6201449c6c7c476bd255c0d \\\n--hash=sha256:23b2d7679b73fe0e5a4560b672a39f98dfc6f60df63823b0a9970525325b95f6 \\\n--hash=sha256:23b701e65c7b36e4bf15546a89279bd4d8675faabc287d06bbcfac7d3c33e1e6 \\\n--hash=sha256:2471c201b70d58a0f0c1f91261542a03d9a5e088ed3dc6c160d614c01649c106 \\\n--hash=sha256:27657df69e8801be6c3638054e202a135c7f299267f1a55ed3a598934f6c0d75 \\\n--hash=sha256:29acab3f66f0f24674b7dc4736477bcd4bc3ad4b896f5f45379a67bce8b96868 \\\n--hash=sha256:32453c1de775c889eb4e22f1197fe3bdfe457d16476ea407472b9442e6295f7a \\\n--hash=sha256:3a670dc61eb0d0eb7080890c13de3066790f9049b47b0de04007090807c776b0 \\\n--hash=sha256:3e0153a805a98f5ada7e09826255ba99fb4f7524bb81bf6b47fb702666484ae1 \\\n--hash=sha256:410478a0c562d1a5bcc2f7ea448359fcb050ed48b3c6f6f4f18c313a9bdb1826 \\\n--hash=sha256:442acde1e068288a4ba7acfe05f5f343e19fac87bfc96d89eb886b0363e977ec \\\n--hash=sha256:48f6a4533887e189dae092f1cf981f2e3885175f7a0f33c91fb5b7b682b6bab6 \\\n--hash=sha256:4f57dab5fe3407b6c0c1cc907ac98e8a189f9e418f3b6e54d65a718aaafe3950 \\\n--hash=sha256:4f9c515e7914626b2a2e1e311794b4c35720a0be87af52b79ff8e1429fc25f19 \\\n--hash=sha256:55fdc093b5a3cb41d420884cdaf37a1e74c3c37a31f46e66286d9145d2063bd0 \\\n--hash=sha256:5667ed53d68d91920defdf4035d1cdaa3c3121dc0b113255124bcfada1cfa1b8 \\\n--hash=sha256:590344787a90ae57d62511dd7c736ed56b428f04cd8c161fcc5e7232c130c69a \\\n--hash=sha256:5a7d70357e7cee13f470c7883a063aae5fe209a493c57d86eb7f5a6f910fae09 \\\n--hash=sha256:5c3894db91f5a489fc8fa6a9991820f368f0b3cbdb9cd8849547ccfab3392d86 \\\n--hash=sha256:5c849d495bf5154cd8da18a9eb15db127d4dba2968d88831aff6f0331ea9bd4c \\\n--hash=sha256:64536573d0a2cb6e625cf309984e2d873979709f2cf22839bf2d61790b448ad5 \\\n--hash=sha256:693945278a31f2086d9bf3df0fe8254bbeaef1fe71e1351c3bd730aa7d31c41b \\\n--hash=sha256:6db4667b187a6742b33afbbaf05a7bc551ffcf1ced0000a571aedbb4aa42fc7b \\\n--hash=sha256:6eb73fa5426ea69ee0e012fb59cdc76a15b1283d6e32e4f8dc4482ec67d1194d \\\n--hash=sha256:722e1124aec435320ae01ee3ac7bec11a5d47f25d0ed6328f2273d287bc3abb0 \\\n--hash=sha256:7268252af60904bf52c26173cbadc3a071cece75f873705419c8681f24d3edea \\\n--hash=sha256:74fb4bee6880b529a0c6560885fce4dc95936920f9f20f53d99a213f7bf66776 \\\n--hash=sha256:780d3a35680ced9ce682fbcf4cb9c2bad3136eeff760ab33707b71db84664e3a \\\n--hash=sha256:82e8211d69a4f4bc360ea22cd6555f8e61a1bd211d1d5d39d3d228b48c83a897 \\\n--hash=sha256:89aa2c2eeb20957be2d950b85974b30a01a762f3308cd02bb15e1ad632e22dc7 \\\n--hash=sha256:8aefbba5f69d42246543407ed2461db31006b0f76c4e32dfd6f42215a2c41d09 \\\n--hash=sha256:96ec70beabbd3b10e8bfe52616a13561e58fe84c0101dd031dc78f250d5128b9 \\\n--hash=sha256:9750cc7fe1ae3b1611bb8cfc3f9ec11d532244235d75901fb6b8e42ce9229dfe \\\n--hash=sha256:9acbb16f06fe7f52f441bb6f413ebae6c37baa6ef9edd49cdd567216da8600cd \\\n--hash=sha256:9d3e0c25a2350080e9319724dede4f31f43a6c9779be48021a7f4ebde8b2d742 \\\n--hash=sha256:a06339f38e9ed3a64e4c4e43aec7f59084033647f908e4259d279a52d3757d09 \\\n--hash=sha256:a0cb6f11204443f27a1628b0e460f37fb30f624be6051d490fa7d7e26d4af3d0 \\\n--hash=sha256:a7496bfe1da7fb1a4e1cc23bb67c58fab69311cc7d32b5a99c2007b4b2a0e932 \\\n--hash=sha256:a828c57f00f729620a442881cc60e57cfcec6842ba38e1b19fd3e47ac0ff8dc1 \\\n--hash=sha256:a9b2de4cf0cdd5bd2dee4c4f63a653c61d2408055ab77b151c1957f221cabf2a \\\n--hash=sha256:b46c8ae3a8f1f41a0d2ef350c0b6e65822d80772fe46b653ab6b6274f61d4a49 \\\n--hash=sha256:b7e3ed87d4138356775346e6845cccbe66cd9e207f3cd11d2f0b9fd13681359d \\\n--hash=sha256:b7f2f9f912dca3934c1baec2e4585a674ef16fe00218d833856408c48d5beee7 \\\n--hash=sha256:ba60bb19387e13597fb059f32cd4d59445d7b18b69a745b8f8e5db0346f33480 \\\n--hash=sha256:beee944ae828747fd7cb216a70f120767fc9f4f00bacae8543c14a6831673f89 \\\n--hash=sha256:bfa4a17e17ce9abf47a74ae02f32d014c5e9404b6d9ac7f729e01562bbee601e \\\n--hash=sha256:c037a86e8513059a2613aaba4d817bb90b9d9b6b69aace3ce9c877e8c8ed402b \\\n--hash=sha256:c302220494f5c1ebeb0912ea782bcd5e2f8308037b3c7553fad0e48ebad6ad82 \\\n--hash=sha256:c6321c9efe29975232da3bd0af0ad216800a47e93d763ce64f291917a381b8eb \\\n--hash=sha256:c757a9dd70d72b076d6f68efdbb9bc943665ae954dad2801b874c8c69e185068 \\\n--hash=sha256:c99169d4ff810155ca50b4da3b075cbde79752443117d89429595c2e8e37fed8 \\\n--hash=sha256:c9c92be9fd329ac801cc420e08452b70e7aeab94ea4233a4804f0915c14eba9b \\\n--hash=sha256:cc7b01b3754ea68a62bd77ce6020afaffb44a590c2289089289363472d13aedb \\\n--hash=sha256:db9e724bebd621d9beca794f2a4ff1d26eed5965b004a97f1f1685a173b869c2 \\\n--hash=sha256:dca69045298ce5c11fd539682cff879cc1e664c245d1c64da929813e54241d11 \\\n--hash=sha256:dd9b1baec094d91bf36ec729445f7769d0d0cf6b64d04d86e45baf89e2b9059b \\\n--hash=sha256:e02a0e11cf6597299b9f3bbd3f93d79217cb90cfd1411aec33848b13f5c656cc \\\n--hash=sha256:e6a20a581f9ce92d389a8c7d7c3dd47c81fd5d6e655c8dddf341e14aa48659d0 \\\n--hash=sha256:e7004be74cbb7d9f34553a5ce5fb08be14fb33bc86f332fb71cbe5216362a497 \\\n--hash=sha256:e774d53b1a477a67838a904131c4b0eef6b3d8a651f8b138b04f748fccfefe17 \\\n--hash=sha256:edb678da49d9f72c9f6c609fbe41a5dfb9a9282f9e6a2253d5a91e0fc382d7c0 \\\n--hash=sha256:f146e0911cb2f1da549fc58fc7bcd2b836a44b79ef871980d605ec392ff6b0d2 \\\n--hash=sha256:f56e2333dda1fe0f909e7cc59f021eba0d2307bc6f012a1ccf2beca6ba362439 \\\n--hash=sha256:f9a3ea26252bd92f570600098783d1371354d89d5f6b7dfd87359d669f2109b5 \\\n--hash=sha256:f9aa1878d1083b276b0196f2dfbe00c9b7e752475ed3b682025ff20c1c1f51ac \\\n--hash=sha256:fb3c2db03683b5767dedb5769b8a40ebb47d6f7f45b1b3e3b4b51ec8ad9d9825 \\\n--hash=sha256:fbeb989b5cc29e8daf7f976b421c220f1b8c731cbf22b9130d8815418ea45887 \\\n--hash=sha256:fde5bd59ab5357e3853313127f4d3565fc7dad314a74d7b5d43c22c6a5ed2ced \\\n--hash=sha256:fe1a06da377e3a1062ae5fe0926e12b84eceb8a50b350ddca72dc85015873f74\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   aiohttp\n#   aiosignal\n```\n\n----------------------------------------\n\nTITLE: Compiled Package Dependencies with Hashes for Ray Project\nDESCRIPTION: Detailed listing of Python package dependencies with exact versions, cryptographic hashes for verification, and comments indicating the origin of each dependency. This ensures reproducible environments and secure package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\naiofiles==22.1.0 \\\n    --hash=sha256:1142fa8e80dbae46bb6339573ad4c8c0841358f79c6eb50a493dceca14621bad \\\n    --hash=sha256:9107f1ca0b2a5553987a94a3c9959fe5b491fdf731389aa5b7b1bd0733e32de6\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ypy-websocket\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   aiohttp\n```\n\n----------------------------------------\n\nTITLE: Multi-Model Configuration Application Builder\nDESCRIPTION: Demonstrates configuring multiple deployments in a single application builder using passed arguments for model weights and configurations\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/app-builder-guide.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef app_builder(args):\n    model1 = Deployment.bind(Model, args[\"model1_weights\"])\n    model2 = Deployment.bind(Model, args[\"model2_weights\"])\n    return model1, model2\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with ArrayList Operations\nDESCRIPTION: Stack trace showing HTTP request processing through Netty's NIO event loop, including ArrayList operations for message handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_95\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];org/vertx/java/core/http/impl/VertxHttpHandler:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/handler/codec/MessageToMessageEncoder:.write_[j];io/netty/handler/codec/http/HttpObjectEncoder:.encode_[j];java/util/ArrayList:.add_[j]\n```\n\n----------------------------------------\n\nTITLE: Specifying PyCurl Package Dependency with Hash Verification (Partial)\nDESCRIPTION: Defines the pycurl package dependency at version 7.45.3 with SHA256 hash verification (partially shown in the snippet). This package provides Python bindings for libcurl.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_27\n\nLANGUAGE: plaintext\nCODE:\n```\npycurl==7.45.3 \\\n    --hash=sha256:0c41a172d5e8a5cdd8328cc8134f47b2a57960ac677f7cda8520eaa9fbe7d990\n```\n\n----------------------------------------\n\nTITLE: Specifying nvidia-nccl-cu12 Package Requirement\nDESCRIPTION: Defines the required version and hash values for the nvidia-nccl-cu12 package. This ensures a specific, verified version is installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_13\n\nLANGUAGE: Text\nCODE:\n```\nnvidia-nccl-cu12==2.20.5 \\\n    --hash=sha256:057f6bf9685f75215d0c53bf3ac4a10b3e6578351de307abad9e18a99182af56 \\\n    --hash=sha256:1fc150d5c3250b170b29410ba682384b14581db722b2531b0d8d33c595f33d01\n```\n\n----------------------------------------\n\nTITLE: Specifying jmespath Package Version and Hashes\nDESCRIPTION: Defines the version and hash values for the jmespath package. It specifies version 1.0.1 and includes two hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_13\n\nLANGUAGE: Text\nCODE:\n```\njmespath==1.0.1 \\\n    --hash=sha256:02e2e4cc71b5bcab88332eebf907519190dd9e6e82107fa7f83b1003a6252980 \\\n    --hash=sha256:90261b206d6defd58fdd5e85f478bf633a2901798906be2ad389150c5c60edbe\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   boto3\n    #   botocore\n```\n\n----------------------------------------\n\nTITLE: Setting Trial Restore Retry Count\nDESCRIPTION: TUNE_RESTORE_RETRY_NUM sets the number of retry attempts before a trial's restore is deemed unsuccessful and restarted from scratch. Defaults to 0.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_RESTORE_RETRY_NUM=0\n```\n\n----------------------------------------\n\nTITLE: Watching Multiple Namespaces Configuration\nDESCRIPTION: Commands to set up and verify RBAC configuration for watching multiple specified namespaces using Role and RoleBinding.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/helm-chart-rbac.md#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Create a Kubernetes cluster using Kind.\nkind create cluster --image=kindest/node:v1.26.0\n\n# Create namespaces.\nkubectl create ns n1\nkubectl create ns n2\n\n# Install a KubeRay operator.\n# Set `singleNamespaceInstall` and `watchNamespace` in the `values.yaml` file.\n# (path: helm-chart/kuberay-operator)\nhelm install kuberay-operator .\n\n# Check ClusterRole\nkubectl get clusterrole | grep kuberay\n# (nothing found)\n\n# Check Role.\nkubectl get role --all-namespaces | grep kuberay\n#default       kuberay-operator-leader-election                 2023-10-15T05:34:27Z\n#n1            kuberay-operator                                 2023-10-15T05:34:27Z\n#n2            kuberay-operator                                 2023-10-15T05:34:27Z\n\n# Install RayCluster in the `default`, `n1`, and `n2` namespaces.\nhelm install raycluster kuberay/ray-cluster --version 1.3.0\nhelm install raycluster kuberay/ray-cluster --version 1.3.0 -n n1\nhelm install raycluster kuberay/ray-cluster --version 1.3.0 -n n2\n\n# KubeRay creates a RayCluster only in the `n1` and `n2` namespaces.\nkubectl get raycluster -A\n# NAMESPACE   NAME                 DESIRED WORKERS   AVAILABLE WORKERS   STATUS   AGE\n# default     raycluster-kuberay                                                  74s\n# n1          raycluster-kuberay   1                 1                   ready    70s\n# n2          raycluster-kuberay   1                 1                   ready    67s\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Cluster with KinD\nDESCRIPTION: Command to create a Kubernetes cluster using KinD (Kubernetes in Docker) for testing purposes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkind create cluster\n```\n\n----------------------------------------\n\nTITLE: Downloading RayJob Sample YAML for Kueue\nDESCRIPTION: Command to download a sample RayJob YAML from the KubeRay repository for use with Kueue.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-job.kueue-toy-sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Logger in Ray Tune\nDESCRIPTION: Demonstrates how to add a custom logger in Ray Tune, enabling the definition of specific logging behaviors and outputs during reinforcement learning training sessions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Custom logger\n# This script shows how to implement a custom logger within Ray Tune.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry-Exporter-OTLP-Proto-GRPC Dependency with Hash Verification\nDESCRIPTION: Defines the opentelemetry-exporter-otlp-proto-grpc package dependency with version 1.1.0 and includes SHA256 hash validations. This is a dependency of opentelemetry-exporter-otlp.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-exporter-otlp-proto-grpc==1.1.0 \\\n    --hash=sha256:281e9bbce73b08c1c93781cf7f4282396f74895987fdc051bea335f7dd086199 \\\n    --hash=sha256:5a4a86becf4f9fdf2910a5b869fc40ec9978044f93045fdce240fecb6c64681a\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up the Kubernetes Cluster\nDESCRIPTION: Removes the Kind cluster and all associated resources after completing the high availability testing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-high-availability.md#2025-04-12_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nkind delete cluster\n```\n\n----------------------------------------\n\nTITLE: Defining External Links in reStructuredText\nDESCRIPTION: This snippet demonstrates how to define external links in reStructuredText format. It includes links to various Ray community resources such as the discussion board, GitHub issues, StackOverflow, and social media platforms.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/involvement.rst#2025-04-12_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _`Discussion Board`: https://discuss.ray.io/\n.. _`GitHub Issues`: https://github.com/ray-project/ray/issues\n.. _`StackOverflow`: https://stackoverflow.com/questions/tagged/ray\n.. _`Pull Requests`: https://github.com/ray-project/ray/pulls\n.. _`Twitter`: https://twitter.com/raydistributed\n.. _`Meetup Group`: https://www.meetup.com/Bay-Area-Ray-Meetup/\n.. _`on GitHub`: https://github.com/ray-project/ray\n```\n\n----------------------------------------\n\nTITLE: Specifying pyarrow Package Version and Hashes\nDESCRIPTION: This snippet defines the version and hash values for the pyarrow package. It specifies version 14.0.2 and includes multiple hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_39\n\nLANGUAGE: Text\nCODE:\n```\npyarrow==14.0.2 \\\n    --hash=sha256:059bd8f12a70519e46cd64e1ba40e97eae55e0cbe1695edd95384653d7626b23 \\\n    --hash=sha256:06ff1264fe4448e8d02073f5ce45a9f934c0f3db0a04460d0b01ff28befc3696 \\\n    --hash=sha256:1e6987c5274fb87d66bb36816afb6f65707546b3c45c44c28e3c4133c010a881 \\\n    # ... (additional hash values omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Specifying PyASN1-Modules Package Dependency with Hash Verification\nDESCRIPTION: Defines the pyasn1-modules package dependency at version 0.3.0 with SHA256 hash verification. This package is required by google-auth and oauth2client components.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_26\n\nLANGUAGE: plaintext\nCODE:\n```\npyasn1-modules==0.3.0 \\\n    --hash=sha256:5bd01446b736eb9d31512a30d46c1ac3395d676c6f3cafa4c03eb54b9925631c \\\n    --hash=sha256:d3ccd6ed470d9ffbc716be08bd90efbd44d0734bc9303818f7336070984a162d\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for PyTorch Lightning and Ray\nDESCRIPTION: This code snippet installs the necessary dependencies for the project, including numpy, datasets, transformers, and pytorch_lightning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_cola_advanced.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install numpy datasets \"transformers>=4.19.1\" \"pytorch_lightning>=1.6.5\"\n```\n\n----------------------------------------\n\nTITLE: Get the best tuning result\nDESCRIPTION: Retrieves the best result from the hyperparameter tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nbest_result = tune_results.get_best_result()\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Hash Verification for Ray Project\nDESCRIPTION: This snippet shows how Python dependencies are specified with exact version numbers and SHA256 hash verification. Each package includes version constraints, hash values, and comments indicating which part of the project requires the dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_6\n\nLANGUAGE: pip\nCODE:\n```\n    --hash=sha256:a2913c5375154b6ef2e91c10b5720ea6e21007412f6437504ffea2109b5a33d7 \\\n    --hash=sha256:a30596bae9403a342c978fb47d9b0ee277699fa53bbafad14706af51fe543d16 \\\n    --hash=sha256:b03c2ae5d2f0fc05f9a2c0c997e1bc18c8229f392234e8a0194f202169ccd278 \\\n    --hash=sha256:b6cd2203306b63e41acdf39aa93b86fb566049aeb6dc489b70e34bcd07adca74 \\\n    --hash=sha256:b7ffe927ee6531c78f81aa17e684e2ff617daeba7f189f911065b2ea2d526dec \\\n    --hash=sha256:b8cac287fafc4ad485b8a9b67d0ee80c66bf3574f655d3b97ef2e1082360faf1 \\\n    --hash=sha256:ba334e6e4b1d92442b75ddacc615c5476d4ad55cc29b15d590cc6b86efa487e2 \\\n    --hash=sha256:ba3e4a42397c25b7ff88cdec6e2a16c2be18720f317506ee25210f6d31925f9c \\\n    --hash=sha256:c41fb5e6a5fe9ebcd58ca3abfeb51dffb5d83d6775405305bfa8715b76521922 \\\n    --hash=sha256:cd2030f6650c089aeb304cf093f3244d34745ce0cfcc39f20c6fbfe030102e2a \\\n    --hash=sha256:cd65d75953847815962c84a4654a84850b2bb4aed3f26fadcc1c13892e1e29f6 \\\n    --hash=sha256:e4985a790f921508f36f81831817cbc03b102d643b5fcb81cd33df3fa291a1a1 \\\n    --hash=sha256:e807b3188f9eb0eaa7bbb579b462c5ace579f1cedb28107ce8b48a9f7ad3679e \\\n    --hash=sha256:f12764b8fffc7a123f641d7d049d382b73f96a34117e0b637b80643169cec8ac \\\n    --hash=sha256:f8837fe1d6ac4a8052a9a8ddab256bc006242696f03368a4009be7ee3075cdb7\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   pyopenssl\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Dependencies with Version Constraints and Hashes\nDESCRIPTION: This snippet demonstrates the pattern used to specify individual package dependencies with their exact version numbers and cryptographic hashes. Each package entry includes the package name, version constraint, multiple SHA-256 hashes, and comment indicating which requirement files referenced this dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\ncrcmod==1.7 \\\n    --hash=sha256:dc7051a0db5f2bd48665a990d3ec1cc305a466a77358ca4492826f41f283601e\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gsutil\ncryptography==42.0.5 \\\n    --hash=sha256:0270572b8bd2c833c3981724b8ee9747b3ec96f699a9665470018594301439ee \\\n    --hash=sha256:111a0d8553afcf8eb02a4fea6ca4f59d48ddb34497aa8706a6cf536f1a5ec576 \\\n    --hash=sha256:16a48c23a62a2f4a285699dba2e4ff2d1cff3115b9df052cdd976a18856d8e3d \\\n    --hash=sha256:1b95b98b0d2af784078fa69f637135e3c317091b615cd0905f8b8a087e86fa30 \\\n    --hash=sha256:1f71c10d1e88467126f0efd484bd44bca5e14c664ec2ede64c32f20875c0d413 \\\n    --hash=sha256:2424ff4c4ac7f6b8177b53c17ed5d8fa74ae5955656867f5a8affaca36a27abb \\\n    --hash=sha256:2bce03af1ce5a5567ab89bd90d11e7bbdff56b8af3acbbec1faded8f44cb06da \\\n    --hash=sha256:329906dcc7b20ff3cad13c069a78124ed8247adcac44b10bea1130e36caae0b4 \\\n    --hash=sha256:37dd623507659e08be98eec89323469e8c7b4c1407c85112634ae3dbdb926fdd \\\n    --hash=sha256:3eaafe47ec0d0ffcc9349e1708be2aaea4c6dd4978d76bf6eb0cb2c13636c6fc \\\n    --hash=sha256:5e6275c09d2badf57aea3afa80d975444f4be8d3bc58f7f80d2a484c6f9485c8 \\\n    --hash=sha256:6fe07eec95dfd477eb9530aef5bead34fec819b3aaf6c5bd6d20565da607bfe1 \\\n    --hash=sha256:7367d7b2eca6513681127ebad53b2582911d1736dc2ffc19f2c3ae49997496bc \\\n    --hash=sha256:7cde5f38e614f55e28d831754e8a3bacf9ace5d1566235e39d91b35502d6936e \\\n    --hash=sha256:9481ffe3cf013b71b2428b905c4f7a9a4f76ec03065b05ff499bb5682a8d9ad8 \\\n    --hash=sha256:98d8dc6d012b82287f2c3d26ce1d2dd130ec200c8679b6213b3c73c08b2b7940 \\\n    --hash=sha256:a011a644f6d7d03736214d38832e030d8268bcff4a41f728e6030325fea3e400 \\\n    --hash=sha256:a2913c5375154b6ef2e91c10b5720ea6e21007412f6437504ffea2109b5a33d7 \\\n    --hash=sha256:a30596bae9403a342c978fb47d9b0ee277699fa53bbafad14706af51fe543d16 \\\n    --hash=sha256:b03c2ae5d2f0fc05f9a2c0c997e1bc18c8229f392234e8a0194f202169ccd278 \\\n    --hash=sha256:b6cd2203306b63e41acdf39aa93b86fb566049aeb6dc489b70e34bcd07adca74 \\\n    --hash=sha256:b7ffe927ee6531c78f81aa17e684e2ff617daeba7f189f911065b2ea2d526dec \\\n    --hash=sha256:b8cac287fafc4ad485b8a9b67d0ee80c66bf3574f655d3b97ef2e1082360faf1 \\\n    --hash=sha256:ba334e6e4b1d92442b75ddacc615c5476d4ad55cc29b15d590cc6b86efa487e2 \\\n    --hash=sha256:ba3e4a42397c25b7ff88cdec6e2a16c2be18720f317506ee25210f6d31925f9c \\\n    --hash=sha256:c41fb5e6a5fe9ebcd58ca3abfeb51dffb5d83d6775405305bfa8715b76521922 \\\n    --hash=sha256:cd2030f6650c089aeb304cf093f3244d34745ce0cfcc39f20c6fbfe030102e2a \\\n    --hash=sha256:cd65d75953847815962c84a4654a84850b2bb4aed3f26fadcc1c13892e1e29f6 \\\n    --hash=sha256:e4985a790f921508f36f81831817cbc03b102d643b5fcb81cd33df3fa291a1a1 \\\n    --hash=sha256:e807b3188f9eb0eaa7bbb579b462c5ace579f1cedb28107ce8b48a9f7ad3679e \\\n    --hash=sha256:f12764b8fffc7a123f641d7d049d382b73f96a34117e0b637b80643169cec8ac \\\n    --hash=sha256:f8837fe1d6ac4a8052a9a8ddab256bc006242696f03368a4009be7ee3075cdb7\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   pyopenssl\ncython==0.29.37 \\\n    --hash=sha256:0301d4739c6894e012f1d410052082fdda9e63888c815d9e23e0f7f82fff7d79 \\\n    --hash=sha256:0544f7a3e4437b89b356baa15387494c18214e03f2ffaddada5a2c71c3dfd24b \\\n    --hash=sha256:0a0a6d5972bb3b8c7363cf19a42a988bb0c0bb5ebd9c736c84eca85113ccfdbe \\\n    --hash=sha256:12192ab269e7185720f2d2f8894587bf1da4276db1b9b869e4622a093f18cae6 \\\n    --hash=sha256:177481b0a7e003e5c49e2bf0dda1d6fe610c239f17642a5da9f18c2ad0c5f6b6 \\\n    --hash=sha256:2618af0b8df26d32ee4e8858d4ad8167546596762620aeade84954ae37194a0e \\\n    --hash=sha256:29415d8eb2fdc1ea518ca4810c50a2d062b387d4c9fbcfb3352346e93db22c6d \\\n    --hash=sha256:2ad634dc77a6a74022881826099eccac19c9b79153942cc82e754ffac2bec116 \\\n    --hash=sha256:2de3e729d25f041036e81e2f15683dd129f977dfb5b06267e30e8d7acec43225 \\\n    --hash=sha256:3f87bef1808d255cf13be378c7ad27ae7c6db6df7732217d32428d1daf4109be \\\n    --hash=sha256:4658499a41255431f6bbdca7e634e9c8d3a4c190bf24b4aa1646dac751d3da4d \\\n    --hash=sha256:562f8f911dbd6f1a1b9be8f6cba097125700355688f613994ccd4406f220557a \\\n    --hash=sha256:6c672089fba6a8f6690b8d7924a58c04477771401ad101d53171a13405ee12cb \\\n    --hash=sha256:6cddb567dadb3aa3e280a8a35e5126030915ea744c2812206e9c194b8881475d \\\n    --hash=sha256:79ecfc48694e156402c05561e0adb0e25a6e9d35ac0b41693733a08219d38c58 \\\n    --hash=sha256:852cd4378cbc9ade02f53709107ff9fdad55019a3a636e8a27663ba6cfce10b6 \\\n    --hash=sha256:8bf38373773f967cfd793997a6fb96cf972d41a9fce987ace5767349d6f15572 \\\n    --hash=sha256:8c39c2f5a0fe29bb01de9b1fb449bf65bed6f192317c677f181732791c63fe28 \\\n    --hash=sha256:9450e0766ab65947f8a2a36f9e59079fc879c3807ec936c61725a48c97741a52 \\\n    --hash=sha256:95f1d6a83ef2729e67b3fa7318c829ce5b07ac64c084cd6af11c228e0364662c \\\n    --hash=sha256:9a455347e20ddfad0c5dfee32a3e855ee96811269e5fd86be622ddc4cb326404 \\\n    --hash=sha256:9e68bafeeb97d5a403fb1f7700bd4a55a1f8989824c323ae02ae8a4fcd88f6a1 \\\n    --hash=sha256:a6164a05440dcd9daa760c6488bc91bdac1380c7b4b3aca38cf307ba66042d54 \\\n    --hash=sha256:ac910a28a2fd3d280faf3077b6fe63b97a4b93994ff05647581846f0e4b2f8d1 \\\n    --hash=sha256:af03854571738307a5f30cc6b724081d72db12f907699e7fdfc04c12c839158e \\\n    --hash=sha256:af8e7b4397620e2d18259a11f3bfa026eff9846657e397d02616962dd5dd035a \\\n    --hash=sha256:b048354fd380278f2fa096e7526973beb6e0491a9d44d7e4e29df52612d25776 \\\n    --hash=sha256:b225d5e2091c224d4ab328165fef224ba3919b3ed44bd9b3241416f523b4d51a \\\n    --hash=sha256:b6c48f1032b379135a5b4a31976d6c468e02490688acf9254c6c8ed27bd4cbd4 \\\n    --hash=sha256:b82584836e9e7c0d6effee976595e5cd7fa88dbef3e96e900187983c1d4637d1 \\\n    --hash=sha256:bbce388431a2608a81c8ab13cb14c50611473843ca766031b8b24bb1723faf79 \\\n    --hash=sha256:c33508ede9172a6f6f99d5a6dadc7fee23c840423b411ef8b5a403c04e530297 \\\n    --hash=sha256:cc1b9ce2b73b9ee8c305e06173b35c7c202d4b82d084a0cd73dcedfd6d310aec \\\n    --hash=sha256:d94caf90ae9cb56116ca6d54cdcbccd3c4df6b0cb7233922b2233ee7fe81d05b \\\n    --hash=sha256:e14cd44c830e53cf9d7269c87a6bcc638bb065ec07e24990e338162c7001d3c3 \\\n    --hash=sha256:e841a8b4f9ceefb2916e32dac4f28a895cd519e8ece71505144da1ee355c548a \\\n    --hash=sha256:e8af5975ecfae254d8c0051204fca995dda8f93cf9f0bbf7571e3cda2b0cef4d \\\n    --hash=sha256:ea6d208be1906c5df25b674777d5905c6d8e9ef0b201b830849e0729ba08caba \\\n    --hash=sha256:f2d621fe4cb50007446742134a890500b34e3f50abaf7993baaca02634af7e15 \\\n    --hash=sha256:f813d4a6dd94adee5d4ff266191d1d95bf6d4164a4facc535422c021b2504cfb \\\n    --hash=sha256:fa5b6a0f69bf1823c9fd038fa77a2568b78fda2de045a95b48a71dee4d0d578f \\\n    --hash=sha256:fe0eaf6b1e9ee97c5ee7bfc943f00e36cf59d929db16886cb018352bff8208da\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\ndask[complete]==2022.10.2 ; python_version < \"3.12\" \\\n    --hash=sha256:42cb43f601709575fa46ce09e74bea83fdd464187024f56954e09d9b428ceaab \\\n    --hash=sha256:928003a97b890a14c8a09a01f15320d261053bda530a8bf191d84f33db4a63b8\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n    #   distributed\ndecorator==5.1.1 \\\n    --hash=sha256:637996211036b6385ef91435e4fae22989472f9d571faba8927ba8253acbc330 \\\n    --hash=sha256:b8c3f85900b9dc423225913c5aace94729fe1fa9763b38939a95226f02d37186\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gcsfs\ndill==0.3.7 \\\n    --hash=sha256:76b122c08ef4ce2eedcd4d1abd8e641114bfc6c2867f49f3c41facf65bf19f5e \\\n    --hash=sha256:cc1c8b182eb3013e24bd475ff2e9295af86c1a38eb1aff128dac8962a9ce3c03\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   petastorm\ndiskcache==5.6.3 \\\n```\n\n----------------------------------------\n\nTITLE: Initializing Airflow DAG with Default Arguments\nDESCRIPTION: Sets up the basic DAG configuration with default arguments including owner and scheduling parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/workflow/examples/comparisons/airflow/etl_airflow.py.txt#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndefault_args = {\n    'owner': 'airflow',\n}\n```\n\n----------------------------------------\n\nTITLE: Excluded Package Comment\nDESCRIPTION: This snippet contains a comment indicating that the 'ray' package was intentionally excluded from the output or requirements file. This comment provides context that certain packages are being filtered from the dependency list.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_43\n\nLANGUAGE: plain text\nCODE:\n```\n# The following packages were excluded from the output:\n# ray\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Tune with setup_wandb Function\nDESCRIPTION: This function sets up a Tune experiment using the train_function_wandb that utilizes the setup_wandb utility.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-wandb.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef tune_with_setup():\n    \"\"\"Example for using the setup_wandb utility with the function API\"\"\"\n    tuner = tune.Tuner(\n        train_function_wandb,\n        tune_config=tune.TuneConfig(\n            metric=\"loss\",\n            mode=\"min\",\n        ),\n        param_space={\n            \"mean\": tune.grid_search([1, 2, 3, 4, 5]),\n            \"sd\": tune.uniform(0.2, 0.8),\n        },\n    )\n    tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining smart-open dependency with version pinning and hash verification\nDESCRIPTION: Specifies smart-open version 6.2.0 with SHA-256 hashes. Comments indicate it's required by compiled ray test requirements and multiple requirements files.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_36\n\nLANGUAGE: plaintext\nCODE:\n```\nsmart-open==6.2.0 \\\n    --hash=sha256:088bf00f9327c71e549bc2f86567d3320df5d89667f009ce1c16568976068ef7 \\\n    --hash=sha256:1b4df5c8365218f3852c507451920ccad606c80b0acb4e67508e50ba9b5d2632\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Packaging Library with Hash Verification\nDESCRIPTION: This code shows the specification for the Packaging library with hash verification. The comments show that this package is a dependency for numerous other packages in the Ray project ecosystem.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_27\n\nLANGUAGE: plaintext\nCODE:\n```\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   huggingface-hub\n    #   ipykernel\n    #   jupyter-server\n    #   jupyterlab\n    #   jupyterlab-server\n    #   jupytext\n    #   lazy-loader\n    #   lm-format-enforcer\n    #   nbconvert\n    #   pytest\n    #   ray\n    #   scikit-image\n    #   sphinx\n    #   tensorboardx\n    #   transformers\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Cluster with KinD\nDESCRIPTION: Command to create a Kubernetes cluster using KinD (Kubernetes in Docker) for testing the KubeRay and YuniKorn integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkind create cluster\n```\n\n----------------------------------------\n\nTITLE: Creating Version Check Admonition in reStructuredText\nDESCRIPTION: This snippet creates an admonition block in reStructuredText to remind readers to check for the latest version of the contributor guide. It includes a hyperlink to the most recent version of the guide.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_includes/_latest_contribution_doc.rst#2025-04-12_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. admonition:: Check your version!\n\n    Things can change quickly, and so does this contributor guide.\n    To make sure you've got the most cutting edge version of this guide,\n    go check out the\n    `latest version <https://docs.ray.io/en/master/ray-contribute/getting-involved.html>`__.\n```\n\n----------------------------------------\n\nTITLE: MultiDict Package Dependency with Hash Verification\nDESCRIPTION: This snippet specifies the multidict package with version 6.0.5 and corresponding SHA256 hash verification codes. The partial listing shows the beginning of the package definition with multiple hash options for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_14\n\nLANGUAGE: pip\nCODE:\n```\nmultidict==6.0.5 \\\n    --hash=sha256:01265f5e40f5a17f8241d52656ed27192be03bfa8764d88e8220141d1e4b3556 \\\n    --hash=sha256:0275e35209c27a3f7951e1ce7aaf93ce0d163b28948444bec61dd7badc6d3f8c \\\n    --hash=sha256:04bde7a7b3de05732a4eb39c94574db1ec99abb56162d6c520ad26f83267de29 \\\n    --hash=sha256:04da1bb8c8dbadf2a18a452639771951c662c5ad03aefe4884775454be322c9b \\\n    --hash=sha256:09a892e4a9fb47331da06948690ae38eaa2426de97b4ccbfafbdcbe5c8f37ff8 \\\n    --hash=sha256:0d63c74e3d7ab26de115c49bffc92cc77ed23395303d496eae515d4204a625e7 \\\n    --hash=sha256:107c0cdefe028703fb5dafe640a409cb146d44a6ae201e55b35a4af8e95457dd \\\n    --hash=sha256:141b43360bfd3bdd75f15ed811850763555a251e38b2405967f8e25fb43f7d40 \\\n    --hash=sha256:14c2976aa9038c2629efa2c148022ed5eb4cb939e15ec7aace7ca932f48f9ba6 \\\n    --hash=sha256:19fe01cea168585ba0f678cad6f58133db2aa14eccaf22f88e4a6dccadfad8b3 \\\n    --hash=sha256:1d147090048129ce3c453f0292e7697d333db95e52616b3793922945804a433c \\\n    --hash=sha256:1d9ea7a7e779d7a3561aade7d596649fbecfa5c08a7674b11b423783217933f9 \\\n    --hash=sha256:215ed703caf15f578dca76ee6f6b21b7603791ae090fbf1ef9d865571039ade5 \\\n    --hash=sha256:21fd81c4ebdb4f214161be351eb5bcf385426bf023041da2fd9e60681f3cebae \\\n    --hash=sha256:220dd781e3f7af2c2c1053da9fa96d9cf3072ca58f057f4c5adaaa1cab8fc442 \\\n    --hash=sha256:228b644ae063c10e7f324ab1ab6b548bdf6f8b47f3ec234fef1093bc2735e5f9 \\\n    --hash=sha256:29bfeb0dff5cb5fdab2023a7a9947b3b4af63e9c47cae2a10ad58394b517fddc \\\n    --hash=sha256:2f4848aa3baa109e6ab81fe2006c77ed4d3cd1e0ac2c1fbddb7b1277c168788c \\\n    --hash=sha256:2faa5ae9376faba05f630d7e5e6be05be22913782b927b19d12b8145968a85ea \\\n    --hash=sha256:2ffc42c922dbfddb4a4c3b438eb056828719f07608af27d163191cb3e3aa6cc5 \\\n    --hash=sha256:37b15024f864916b4951adb95d3a80c9431299080341ab9544ed148091b53f50 \\\n    --hash=sha256:3cc2ad10255f903656017363cd59436f2111443a76f996584d1077e43ee51182 \\\n    --hash=sha256:3d25f19500588cbc47dc19081d78131c32637c25804df8414463ec908631e453 \\\n    --hash=sha256:403c0911cd5d5791605808b942c88a8155c2592e05332d2bf78f18697a5fa15e \\\n    --hash=sha256:411bf8515f3be9813d06004cac41ccf7d1cd46dfe233705933dd163b60e37600 \\\n    --hash=sha256:425bf820055005bfc8aa9a0b99ccb52cc2f4070153e34b701acc98d201693733 \\\n    --hash=sha256:435a0984199d81ca178b9ae2c26ec3d49692d20ee29bc4c11a2a8d4514c67eda \\\n    --hash=sha256:4a6a4f196f08c58c59e0b8ef8ec441d12aee4125a7d4f4fef000ccb22f8d7241 \\\n    --hash=sha256:4cc0ef8b962ac7a5e62b9e826bd0cd5040e7d401bc45a6835910ed699037a461 \\\n    --hash=sha256:51d035609b86722963404f711db441cf7134f1889107fb171a970c9701f92e1e \\\n    --hash=sha256:53689bb4e102200a4fafa9de9c7c3c212ab40a7ab2c8e474491914d2305f187e \\\n    --hash=sha256:55205d03e8a598cfc688c71ca8ea5f66447164efff8869517f175ea632c7cb7b \\\n    --hash=sha256:5c0631926c4f58e9a5ccce555ad7747d9a9f8b10619621f22f9635f069f6233e \\\n    --hash=sha256:5cb241881eefd96b46f89b1a056187ea8e9ba14ab88ba632e68d7a2ecb7aadf7 \\\n    --hash=sha256:60d698e8179a42ec85172d12f50b1668254628425a6bd611aba022257cac1386 \\\n    --hash=sha256:612d1156111ae11d14afaf3a0669ebf6c170dbb735e510a7438ffe2369a847fd \\\n    --hash=sha256:6214c5a5571802c33f80e6c84713b2c79e024995b9c5897f794b43e714daeec9 \\\n\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version Constraints\nDESCRIPTION: Lists required Python packages with pinned version numbers. Includes testing frameworks (pytest-asyncio, pytest-timeout), load testing tool (locust), and JSON processing library (orjson).\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_llm_byod_3.11.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npytest-asyncio==0.21.2\npytest-timeout==2.1.0\nlocust==2.33.0\norjson==3.10.15\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet demonstrates how to specify Python package dependencies with their versions and SHA256 hash values for verification. It includes packages like annotated-types, anyio, and argcomplete with their respective hashes.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nannotated-types==0.6.0 \\\n    --hash=sha256:0641064de18ba7a25dee8f96403ebc39113d0cb953a01429249d5c7564666a43 \\\n    --hash=sha256:563339e807e53ffd9c267e99fc6d9ea23eb8443c08f112651963e24e22f84a5d\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   pydantic\nanyio==3.7.1 \\\n    --hash=sha256:44a3c9aba0f5defa43261a8b3efb97891f2bd7d804e0e1f56419befa1adfc780 \\\n    --hash=sha256:91dee416e570e92c64041bd18b900d1d6fa78dff7048769ce5ac5ddad004fbb5\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   starlette\nargcomplete==3.3.0 \\\n    --hash=sha256:c168c3723482c031df3c207d4ba8fa702717ccb9fc0bfe4117166c1f537b4a54 \\\n    --hash=sha256:fd03ff4a5b9e6580569d34b273f741e85cd9e072f3feeeee3eba4891c70eda62\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gsutil\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution Stack Trace with JavaScript Name Resolution\nDESCRIPTION: This stack trace shows the execution path leading to JavaScript name resolution within the Vert.x application. It demonstrates how the Mozilla JavaScript engine handles variable and function name lookups in the context of a Vert.x application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_85\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/ScriptRuntime:.name_[j];org/mozilla/javascript/ScriptRuntime:.nameOrFunction_[j];vtable chunks_[j]\n```\n\n----------------------------------------\n\nTITLE: Specifying prometheus-client Package Dependency\nDESCRIPTION: This snippet specifies the prometheus-client package version 0.19.0 with SHA-256 hashes for verification. It lists multiple Ray project components that depend on this package including jupyter-server, nbclassic, notebook, prometheus-fastapi-instrumentator, and vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_34\n\nLANGUAGE: plaintext\nCODE:\n```\nprometheus-client==0.19.0 \\\n    --hash=sha256:4585b0d1223148c27a225b10dbec5ae9bc4c81a99a3fa80774fa6209935324e1 \\\n    --hash=sha256:c88b1e6ecf6b41cd8fb5731c7ae919bf66df6ec6fafa555cd6c0e16ca169ae92\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   jupyter-server\n    #   nbclassic\n    #   notebook\n    #   prometheus-fastapi-instrumentator\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Specifying Requests Package Dependency with Hash Verification\nDESCRIPTION: A requirements entry for the requests package (version 2.31.0) with SHA256 hashes for verification. The entry includes a comprehensive list of packages that depend on this library.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_34\n\nLANGUAGE: plaintext\nCODE:\n```\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   google-api-core\n    #   huggingface-hub\n    #   mistral-common\n    #   outlines\n    #   ray\n    #   tiktoken\n    #   transformers\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Exploring Recorded Expert Data Stored as Parquet Files in Python\nDESCRIPTION: This snippet demonstrates how to explore the directory containing recorded expert episodes saved in Parquet format, following the data recording process. It provides a command-line statement to list the contents of the storage path.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n$ ls -la /tmp/docs_rllib_offline_recording/cartpole-v1\n\ndrwxr-xr-x. 22 user user 440 21. Nov 17:23 .\ndrwxr-xr-x.  3 user user  60 21. Nov 17:23 ..\ndrwxr-xr-x.  2 user user 540 21. Nov 17:23 run-000001-00004\n```\n\n----------------------------------------\n\nTITLE: Pinned Prometheus Client Dependency with Hash Verification\nDESCRIPTION: Specifies prometheus-client version 0.19.0 with SHA256 hash verification for secure installation. Required by prometheus-fastapi-instrumentator and vllm packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\nprometheus-client==0.19.0 \\\n    --hash=sha256:4585b0d1223148c27a225b10dbec5ae9bc4c81a99a3fa80774fa6209935324e1 \\\n    --hash=sha256:c88b1e6ecf6b41cd8fb5731c7ae919bf66df6ec6fafa555cd6c0e16ca169ae92\n```\n\n----------------------------------------\n\nTITLE: Verifying Package Hashes for Python Dependencies\nDESCRIPTION: This snippet shows the format used to specify package versions and their corresponding SHA256 hashes for verification. It includes multiple hash options for each package to support different distributions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_39\n\nLANGUAGE: Text\nCODE:\n```\nplatformdirs==3.11.0 \\\n    --hash=sha256:cf8ee52a3afdb965072dcc652433e0c7e3e40cf5ea1477cd4b3b1d2eb75495b3 \\\n    --hash=sha256:e9d171d00af68be50e9202731309c4e658fd8bc76f55c11c7dd760d023bda68e\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   jupyter-core\n    #   virtualenv\n```\n\n----------------------------------------\n\nTITLE: Copying Subject Images for Fine-tuning in Bash\nDESCRIPTION: This bash script copies the subject images to be used for fine-tuning into the appropriate directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/dreambooth_finetuning.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp -r \"$INSTANCE_DIR\" \"$IMAGES_OWN_DIR\"\n```\n\n----------------------------------------\n\nTITLE: Listing dependency hash values for Python packages in Ray project\nDESCRIPTION: A section of a requirements file containing SHA256 hash values for package dependencies. This format is used by pip for pinning exact versions of packages with hash verification for security and reproducibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_13\n\nLANGUAGE: plain\nCODE:\n```\n--hash=sha256:857d6565f9aa3464764c2cb6a2e3c2e75e1970e877c188f4aeae45954a314e0c \\\n--hash=sha256:8671622256a0859f5089cbe0ce4693c2af407bc053dcc99aadff7f5310b4aa02 \\\n--hash=sha256:88f7c383071981c74ec1998ba9b437659e4fd02a3c4a4d3efc16774eb108d0ec \\\n--hash=sha256:8aecb5a7f6f7f8fe9cac0bcadd39efaca8bbf8d1bf242e9f175cbe4c925116c3 \\\n--hash=sha256:91bbf398ac8bb7d65a5a52127407c05f75a18d7015a270fdd94bbcb04e65d573 \\\n--hash=sha256:936e8880cc00f839aa4173f94466a8406a96ddce814651075f95837316369899 \\\n--hash=sha256:953dd5481bd6252bd480d6ec431f61d7d87fdcbbb71b0d2bdcfc6ae00bb6fb10 \\\n--hash=sha256:95ae6c5a196e2f239150aa4a479967351df7f44800c93e5a975ec726fef005e2 \\\n--hash=sha256:9a2b5915c333e4364367140443b59f09feae42184459b913f0f41b9fed55794a \\\n--hash=sha256:9ae6c3363261021144121427b1552b29e7b59de9d6a75bf51e03bc072efb3c37 \\\n--hash=sha256:9b556596c49fa1232b0fff4b0e69b9d4083a502e60e404b44341e2f8fb7187f5 \\\n--hash=sha256:9c131447768ed7bc05a02553d939e7f0e807e533441901dd504e217b76307745 \\\n--hash=sha256:9d9d5726474cbbef279fd709008f91a49c4f758bec9c062dfbba88eab00e3ff9 \\\n--hash=sha256:a1bdcbebd4e13446a14de4dd1825f1e778e099f17f79718b4aeaf2403624b0f7 \\\n--hash=sha256:a602ed9bd2c7d85bd58592c28e101bd9ff9c718fbde06545a70945ffd5d11868 \\\n--hash=sha256:a8edae5253efa75c2fc79a90068fe540b197d1c7ab5803b800fccfe240eed33c \\\n--hash=sha256:a905affe76f1802edcac554e3ccf68188bea16546071d7583fb1b693f9cf756b \\\n--hash=sha256:a9e7c6d89c77bb2770c9491d988f26a4b161d05c8ca58f63fb1f1b6b9a74be45 \\\n--hash=sha256:aa9b5abd07f71b081a33115d9758ef6077924082055005808f68feccb27616bd \\\n--hash=sha256:aaa5c173a26960fe67daa69aa93d6d6a1cd714a6eb13802d4e4bd1d24a530644 \\\n--hash=sha256:ac7674d1638df129d9cb4503d20ffc3922bd463c865ef3cb412f2c926108e9a4 \\\n--hash=sha256:b1541e50b78e15fa06a2670157a1962ef06591d4c998b998047fff5e3236880e \\\n--hash=sha256:b1980dbcaad634fe78e710c8587383e6e3f61dbe146bcbfd13a9c8ab2d7b1192 \\\n--hash=sha256:bafa65e3acae612a7799ada439bd202403414ebe23f52e5b17f6ffc2eb98c2be \\\n--hash=sha256:bb5bd6212eb0edfd1e8f254585290ea1dadc3687dd8fd5e2fd9a87c31915cdab \\\n--hash=sha256:bbdd69e20fe2943b51e2841fc1e6a3c1de460d630f65bde12452d8c97209464d \\\n--hash=sha256:bc354b1393dce46026ab13075f77b30e40b61b1a53e852e99d3cc5dd1af4bc85 \\\n--hash=sha256:bcee502c649fa6351b44bb014b98c09cb00982a475a1912a9881ca28ab4f9cd9 \\\n--hash=sha256:bdd9abccd0927673cffe601d2c6cdad1c9321bf3437a2f507d6b037ef91ea307 \\\n--hash=sha256:c42ae7e010d7d6bc51875d768110c10e8a59494855c3d4c348b068f5fb81fdcd \\\n--hash=sha256:c71b5b860c5215fdbaa56f715bc218e45a98477f816b46cfde4a84d25b13274e \\\n--hash=sha256:c7721a3ef41591341388bb2265395ce522aba52f969d33dacd822da8f018aff8 \\\n--hash=sha256:ca8e44b5ba3edb682ea4e6185b49661fc22b230cf811b9c13963c9f982d1d964 \\\n--hash=sha256:cb53669442895763e61df5c995f0e8361b61662f26c1b04ee82899c2789c8f69 \\\n--hash=sha256:cc02c06e9e320869d7d1bd323df6dd4281e78ac2e7f8526835d3d48c69060683 \\\n--hash=sha256:d3caa09e613ece43ac292fbed513a4bce170681a447d25ffcbc1b647d45a39c5 \\\n--hash=sha256:d82411dbf4d3127b6cde7da0f9373e37ad3a43e89ef374965465928f01c2b979 \\\n--hash=sha256:dbcb2dc07308453db428a95a4d03259bd8caea97d7f0776842299f2d00c72fc8 \\\n--hash=sha256:dd4fda67f5faaef4f9ee5383435048ee3e11ad996901225ad7615bc92245bc8e \\\n--hash=sha256:ddd92e18b783aeb86ad2132d84a4b795fc5ec612e3545c1b687e7747e66e2b53 \\\n--hash=sha256:de362ac8bc962408ad8fae28f3967ce1a262b5d63ab8cefb42662566737f1dc7 \\\n--hash=sha256:e214025e23db238805a600f1f37bf9f9a15413c7bf5f9d6ae194f84980c78722 \\\n--hash=sha256:e8f9f93a23634cfafbad6e46ad7d09e0f4a25a2400e4a64b1b7b7c0fbaa06d9d \\\n--hash=sha256:e96a1788f24d03e8d61679f9881a883ecdf9c445a38f9ae3f3f193ab6c591c66 \\\n--hash=sha256:ec53a09aee61d45e7dbe7e91252ff0491b6b5fee3d85b2d45b173d8ab453efc1 \\\n--hash=sha256:f10250bb190fb0742e3e1958dd5c100524c2cc5096c67c8da51233f7448dc137 \\\n--hash=sha256:f1faee2a831fe249e1bae9cbc68d3cd8a30f7e37851deee4d7962b17c410dd56 \\\n--hash=sha256:f610d980e3fccf4394ab3806de6065682982f3d27c12d4ce3ee46a8183d64a6a \\\n--hash=sha256:f6c35b2f87c004270fa2e703b872fcc984d714d430b305145c39d53074e1ffe0 \\\n--hash=sha256:f836f39678cb47c9541f04d8ed4545719dc31ad850bf1832d6b4171e30d65d23 \\\n--hash=sha256:f99768232f036b4776ce419d3244a04fe83784bce871b16d2c2e984c7fcea847 \\\n--hash=sha256:fd814847901df6e8de13ce69b84c31fc9b3fb591224d6762d0b256d510cbf382 \\\n--hash=sha256:fdb325b7fba1e2c40b9b1db407f85642e32404131c08480dd652110fc908561b\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu124.txt\n#   nbconvert\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Requirements file specifying exact versions and SHA256 hashes for Python package dependencies, ensuring package integrity and reproducible builds.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_8\n\nLANGUAGE: txt\nCODE:\n```\n--hash=sha256:cae0274952c079886567f3f4f685bcaf5708f0a23a5f5216fdab71f81a6c0273 \\\n--hash=sha256:cd67cf24a553339d5062eff51013780a00d6f97a39ca062781d06b3a73b15462 \\\n--hash=sha256:d3515f198eaa2f0ed49f8819d5732d70698c3fa37384146079b3799b97667a94 \\\n# via\n#   -c release/ray_release/byod/requirements_compiled.txt\n#   google-cloud-storage\n#   google-resumable-media\n```\n\n----------------------------------------\n\nTITLE: Specifying python-dotenv Package Version with Hashes\nDESCRIPTION: Defines the python-dotenv package dependency with version 1.0.1 and SHA256 hash verification values. This package is used via pydantic-settings for managing environment variables.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_7\n\nLANGUAGE: txt\nCODE:\n```\npython-dotenv==1.0.1 \\\n    --hash=sha256:e324ee90a023d808f1959c46bcbc04446a10ced277783dc6ee09987c37ec10ca \\\n    --hash=sha256:f7b63ef50f1b690dddf550d03497b66d609393b40b564ed0d674909a68ebf16a\n    # via pydantic-settings\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: A requirements.txt style file containing package dependencies with pinned versions and SHA256 hash verification. Includes dependencies for testing, documentation, and Google Cloud integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:c96b93b7f0a746f9e77d325bcfb87422a3d8bd4f03136ae8a85b37f1898d5fc0\n# via -r release/requirements_buildkite.in\ndocutils==0.20.1 \\\n    --hash=sha256:96f387a2c5562db4476f09f13bbab2192e764cac08ebbf3a34a95d9b1e4a59d6 \\\n    --hash=sha256:f08a4e276c3a1583a86dce3e34aba3fe04d02bba2dd51ed16106244e8a923e3b\n```\n\n----------------------------------------\n\nTITLE: Testing Generated Regex Pattern for XML Content Extraction in Python\nDESCRIPTION: Verifies a generated code snippet that uses regular expressions to find content between XML tags. The code creates a sample XML string with book information and applies the regex pattern to extract text between angle brackets.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\nline = \"\"\"\n<bookstore>\n  <book category=\"fiction\">\n    <title>The Great Gatsby</title>\n    <author>F. Scott Fitzgerald</author>\n    <year>1925</year>\n  </book>\n  <book category=\"non-fiction\">\n    <title>Sapiens: A Brief History of Humankind</title>\n    <author>Yuval Noah Harari</author>\n    <year>2011</year>\n  </book>\n</bookstore>\n\"\"\"\nre.findall(\">.*<\", line)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how package dependencies are specified with their versions and SHA256 hashes. It includes packages like google-resumable-media, googleapis-common-protos, grpcio, and gymnasium.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_10\n\nLANGUAGE: Text\nCODE:\n```\ngoogle-resumable-media==2.6.0 \\\n    --hash=sha256:972852f6c65f933e15a4a210c2b96930763b47197cdf4aa5f5bea435efb626e7 \\\n    --hash=sha256:fc03d344381970f79eebb632a3c18bb1828593a2dc5572b5f90115ef7d11e81b\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-cloud-storage\ngooglapis-common-protos==1.61.0 \\\n    --hash=sha256:22f1915393bb3245343f6efe87f6fe868532efc12aa26b391b15132e1279f1c0 \\\n    --hash=sha256:8a64866a97f6304a7179873a465d6eee97b7a24ec6cfd78e0f575e96b821240b\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-api-core\n    #   opentelemetry-exporter-otlp-proto-grpc\ngrpcio==1.66.2 \\\n    --hash=sha256:02697eb4a5cbe5a9639f57323b4c37bcb3ab2d48cec5da3dc2f13334d72790dd \\\n    --hash=sha256:03b0b307ba26fae695e067b94cbb014e27390f8bc5ac7a3a39b7723fed085604\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   grpcio-tools\n    #   opentelemetry-exporter-otlp-proto-grpc\ngymnasium==1.0.0 \\\n    --hash=sha256:9d2b66f30c1b34fe3c2ce7fae65ecf365d0e9982d2b3d860235e773328a3b403 \\\n    --hash=sha256:b6f40e1e24c5bd419361e1a5b86a9117d2499baecc3a660d44dfff4c465393ad\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for charset-normalizer\nDESCRIPTION: This snippet shows the dependency specification for the charset-normalizer package, including its version and multiple file hashes for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\ncharset-normalizer==3.3.2 \\\n    --hash=sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027 \\\n    --hash=sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087 \\\n    --hash=sha256:0a55554a2fa0d408816b3b5cedf0045f4b8e1a6065aec45849de2d6f3f8e9786 \\\n    --hash=sha256:0b2b64d2bb6d3fb9112bafa732def486049e63de9618b5843bcdd081d8144cd8 \\\n    --hash=sha256:10955842570876604d404661fbccbc9c7e684caf432c09c715ec38fbae45ae09 \\\n    --hash=sha256:122c7fa62b130ed55f8f285bfd56d5f4b4a5b503609d181f9ad85e55c89f4185 \\\n    --hash=sha256:1ceae2f17a9c33cb48e3263960dc5fc8005351ee19db217e9b1bb15d28c02574 \\\n    --hash=sha256:1d3193f4a680c64b4b6a9115943538edb896edc190f0b222e73761716519268e \\\n    --hash=sha256:1f79682fbe303db92bc2b1136016a38a42e835d932bab5b3b1bfcfbf0640e519 \\\n    --hash=sha256:2127566c664442652f024c837091890cb1942c30937add288223dc895793f898 \\\n    --hash=sha256:22afcb9f253dac0696b5a4be4a1c0f8762f8239e21b99680099abd9b2b1b2269 \\\n    --hash=sha256:25baf083bf6f6b341f4121c2f3c548875ee6f5339300e08be3f2b2ba1721cdd3 \\\n    --hash=sha256:2e81c7b9c8979ce92ed306c249d46894776a909505d8f5a4ba55b14206e3222f \\\n    --hash=sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6 \\\n    --hash=sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8 \\\n    --hash=sha256:37e55c8e51c236f95b033f6fb391d7d7970ba5fe7ff453dad675e88cf303377a \\\n    --hash=sha256:3d47fa203a7bd9c5b6cee4736ee84ca03b8ef23193c0d1ca99b5089f72645c73 \\\n    --hash=sha256:3e4d1f6587322d2788836a99c69062fbb091331ec940e02d12d179c1d53e25fc \\\n    --hash=sha256:42cb296636fcc8b0644486d15c12376cb9fa75443e00fb25de0b8602e64c1714 \\\n    --hash=sha256:45485e01ff4d3630ec0d9617310448a8702f70e9c01906b0d0118bdf9d124cf2 \\\n    --hash=sha256:4a78b2b446bd7c934f5dcedc588903fb2f5eec172f3d29e52a9096a43722adfc \\\n    --hash=sha256:4ab2fe47fae9e0f9dee8c04187ce5d09f48eabe611be8259444906793ab7cbce \\\n    --hash=sha256:4d0d1650369165a14e14e1e47b372cfcb31d6ab44e6e33cb2d4e57265290044d \\\n    --hash=sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e \\\n    --hash=sha256:55086ee1064215781fff39a1af09518bc9255b50d6333f2e4c74ca09fac6a8f6 \\\n    --hash=sha256:572c3763a264ba47b3cf708a44ce965d98555f618ca42c926a9c1616d8f34269 \\\n    --hash=sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96 \\\n    --hash=sha256:5b4c145409bef602a690e7cfad0a15a55c13320ff7a3ad7ca59c13bb8ba4d45d \\\n    --hash=sha256:6463effa3186ea09411d50efc7d85360b38d5f09b870c48e4600f63af490e56a \\\n    --hash=sha256:65f6f63034100ead094b8744b3b97965785388f308a64cf8d7c34f2f2e5be0c4 \\\n    --hash=sha256:663946639d296df6a2bb2aa51b60a2454ca1cb29835324c640dafb5ff2131a77 \\\n    --hash=sha256:6897af51655e3691ff853668779c7bad41579facacf5fd7253b0133308cf000d \\\n    --hash=sha256:68d1f8a9e9e37c1223b656399be5d6b448dea850bed7d0f87a8311f1ff3dabb0 \\\n    --hash=sha256:6ac7ffc7ad6d040517be39eb591cac5ff87416c2537df6ba3cba3bae290c0fed \\\n    --hash=sha256:6b3251890fff30ee142c44144871185dbe13b11bab478a88887a639655be1068 \\\n    --hash=sha256:6c4caeef8fa63d06bd437cd4bdcf3ffefe6738fb1b25951440d80dc7df8c03ac \\\n    --hash=sha256:6ef1d82a3af9d3eecdba2321dc1b3c238245d890843e040e41e470ffa64c3e25 \\\n    --hash=sha256:753f10e867343b4511128c6ed8c82f7bec3bd026875576dfd88483c5c73b2fd8 \\\n    --hash=sha256:7cd13a2e3ddeed6913a65e66e94b51d80a041145a026c27e6bb76c31a853c6ab \\\n    --hash=sha256:7ed9e526742851e8d5cc9e6cf41427dfc6068d4f5a3bb03659444b4cabf6bc26 \\\n    --hash=sha256:7f04c839ed0b6b98b1a7501a002144b76c18fb1c1850c8b98d458ac269e26ed2 \\\n    --hash=sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db \\\n    --hash=sha256:80402cd6ee291dcb72644d6eac93785fe2c8b9cb30893c1af5b8fdd753b9d40f \\\n    --hash=sha256:8465322196c8b4d7ab6d1e049e4c5cb460d0394da4a27d23cc242fbf0034b6b5 \\\n    --hash=sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99 \\\n    --hash=sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c \\\n    --hash=sha256:8bdb58ff7ba23002a4c5808d608e4e6c687175724f54a5dade5fa8c67b604e4d \\\n    --hash=sha256:8c622a5fe39a48f78944a87d4fb8a53ee07344641b0562c540d840748571b811 \\\n    --hash=sha256:8d756e44e94489e49571086ef83b2bb8ce311e730092d2c34ca8f7d925cb20aa \\\n    --hash=sha256:8f4a014bc36d3c57402e2977dada34f9c12300af536839dc38c0beab8878f38a \\\n    --hash=sha256:9063e24fdb1e498ab71cb7419e24622516c4a04476b17a2dab57e8baa30d6e03 \\\n    --hash=sha256:90d558489962fd4918143277a773316e56c72da56ec7aa3dc3dbbe20fdfed15b \\\n    --hash=sha256:923c0c831b7cfcb071580d3f46c4baf50f174be571576556269530f4bbd79d04 \\\n    --hash=sha256:95f2a5796329323b8f0512e09dbb7a1860c46a39da62ecb2324f116fa8fdc85c \\\n    --hash=sha256:96b02a3dc4381e5494fad39be677abcb5e6634bf7b4fa83a6dd3112607547001 \\\n```\n\n----------------------------------------\n\nTITLE: Specifying Rich Package Dependency with Hash Verification\nDESCRIPTION: A requirements entry for the rich package (version 13.3.2) with SHA256 hashes for verification. The entry includes references to parent packages that depend on this library.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_35\n\nLANGUAGE: plaintext\nCODE:\n```\nrich==13.3.2 \\\n    --hash=sha256:91954fe80cfb7985727a467ca98a7618e5dd15178cc2da10f553b36a93859001 \\\n    --hash=sha256:a104f37270bf677148d8acb07d33be1569eeee87e2d1beb286a4e9113caf6f2f\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   memray\n    #   typer\n```\n\n----------------------------------------\n\nTITLE: Running Sequential Data Processing\nDESCRIPTION: Function to measure and display runtime of sequential data processing\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef print_runtime(input_data, start_time):\n    print(f'Runtime: {time.time() - start_time:.2f} seconds, data:')\n    print(*input_data, sep=\"\\n\")\n\n\nstart = time.time()\ndata = [retrieve(item) for item in range(8)]\nprint_runtime(data, start)\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: A pip requirements file listing package dependencies with their exact versions and SHA256 hash values for verification. Includes packages like jupyter-server, jupyterlab, and various scientific computing libraries with specified version constraints.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_12\n\nLANGUAGE: text\nCODE:\n```\njupyter-server==1.24.0 \\\n    --hash=sha256:23368e8e214baf82b313d4c5a0d828ca73015e1a192ce3829bd74e62fab8d046 \\\n    --hash=sha256:c88ddbe862966ea1aea8c3ccb89a5903abd8fbcfe5cd14090ef549d403332c37\n```\n\n----------------------------------------\n\nTITLE: Package Requirements Installation Command\nDESCRIPTION: UV pip compile command for generating requirements with specific constraints and package hashes\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_1\n\nLANGUAGE: pip\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cu124 --find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c python/requirements_compiled_ray_test_py311_cu124.txt python/requirements.txt python/requirements/cloud-requirements.txt python/requirements/base-test-requirements.txt python/requirements/llm/llm-requirements.txt python/requirements/llm/llm-test-requirements.txt -o python/requirements_compiled_rayllm_test_py311_cu124.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Frozen List Package Dependencies with Multiple Hash Verifications\nDESCRIPTION: A detailed specification of the frozenlist package with an extensive list of SHA256 hash options for verification. This allows secure installation from various sources while maintaining integrity verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nfrozenlist==1.4.1 \\\n    --hash=sha256:04ced3e6a46b4cfffe20f9ae482818e34eba9b5fb0ce4056e4cc9b6e212d09b7 \\\n    --hash=sha256:0633c8d5337cb5c77acbccc6357ac49a1770b8c487e5b3505c57b949b4b82e98 \\\n    --hash=sha256:068b63f23b17df8569b7fdca5517edef76171cf3897eb68beb01341131fbd2ad \\\n    --hash=sha256:0c250a29735d4f15321007fb02865f0e6b6a41a6b88f1f523ca1596ab5f50bd5 \\\n    --hash=sha256:1979bc0aeb89b33b588c51c54ab0161791149f2461ea7c7c946d95d5f93b56ae \\\n    --hash=sha256:1a4471094e146b6790f61b98616ab8e44f72661879cc63fa1049d13ef711e71e \\\n    --hash=sha256:1b280e6507ea8a4fa0c0a7150b4e526a8d113989e28eaaef946cc77ffd7efc0a \\\n    --hash=sha256:1d0ce09d36d53bbbe566fe296965b23b961764c0bcf3ce2fa45f463745c04701 \\\n    --hash=sha256:20b51fa3f588ff2fe658663db52a41a4f7aa6c04f6201449c6c7c476bd255c0d \\\n    --hash=sha256:23b2d7679b73fe0e5a4560b672a39f98dfc6f60df63823b0a9970525325b95f6 \\\n    --hash=sha256:23b701e65c7b36e4bf15546a89279bd4d8675faabc287d06bbcfac7d3c33e1e6 \\\n    --hash=sha256:2471c201b70d58a0f0c1f91261542a03d9a5e088ed3dc6c160d614c01649c106 \\\n    --hash=sha256:27657df69e8801be6c3638054e202a135c7f299267f1a55ed3a598934f6c0d75 \\\n    --hash=sha256:29acab3f66f0f24674b7dc4736477bcd4bc3ad4b896f5f45379a67bce8b96868 \\\n    --hash=sha256:32453c1de775c889eb4e22f1197fe3bdfe457d16476ea407472b9442e6295f7a \\\n    --hash=sha256:3a670dc61eb0d0eb7080890c13de3066790f9049b47b0de04007090807c776b0 \\\n    --hash=sha256:3e0153a805a98f5ada7e09826255ba99fb4f7524bb81bf6b47fb702666484ae1 \\\n    --hash=sha256:410478a0c562d1a5bcc2f7ea448359fcb050ed48b3c6f6f4f18c313a9bdb1826 \\\n    --hash=sha256:442acde1e068288a4ba7acfe05f5f343e19fac87bfc96d89eb886b0363e977ec \\\n    --hash=sha256:48f6a4533887e189dae092f1cf981f2e3885175f7a0f33c91fb5b7b682b6bab6 \\\n    --hash=sha256:4f57dab5fe3407b6c0c1cc907ac98e8a189f9e418f3b6e54d65a718aaafe3950 \\\n    --hash=sha256:4f9c515e7914626b2a2e1e311794b4c35720a0be87af52b79ff8e1429fc25f19 \\\n    --hash=sha256:55fdc093b5a3cb41d420884cdaf37a1e74c3c37a31f46e66286d9145d2063bd0 \\\n    --hash=sha256:5667ed53d68d91920defdf4035d1cdaa3c3121dc0b113255124bcfada1cfa1b8 \\\n    --hash=sha256:590344787a90ae57d62511dd7c736ed56b428f04cd8c161fcc5e7232c130c69a \\\n    --hash=sha256:5a7d70357e7cee13f470c7883a063aae5fe209a493c57d86eb7f5a6f910fae09 \\\n    --hash=sha256:5c3894db91f5a489fc8fa6a9991820f368f0b3cbdb9cd8849547ccfab3392d86 \\\n    --hash=sha256:5c849d495bf5154cd8da18a9eb15db127d4dba2968d88831aff6f0331ea9bd4c \\\n    --hash=sha256:64536573d0a2cb6e625cf309984e2d873979709f2cf22839bf2d61790b448ad5 \\\n    --hash=sha256:693945278a31f2086d9bf3df0fe8254bbeaef1fe71e1351c3bd730aa7d31c41b \\\n    --hash=sha256:6db4667b187a6742b33afbbaf05a7bc551ffcf1ced0000a571aedbb4aa42fc7b \\\n    --hash=sha256:6eb73fa5426ea69ee0e012fb59cdc76a15b1283d6e32e4f8dc4482ec67d1194d \\\n    --hash=sha256:722e1124aec435320ae01ee3ac7bec11a5d47f25d0ed6328f2273d287bc3abb0 \\\n    --hash=sha256:7268252af60904bf52c26173cbadc3a071cece75f873705419c8681f24d3edea \\\n    --hash=sha256:74fb4bee6880b529a0c6560885fce4dc95936920f9f20f53d99a213f7bf66776 \\\n    --hash=sha256:780d3a35680ced9ce682fbcf4cb9c2bad3136eeff760ab33707b71db84664e3a \\\n    --hash=sha256:82e8211d69a4f4bc360ea22cd6555f8e61a1bd211d1d5d39d3d228b48c83a897 \\\n    --hash=sha256:89aa2c2eeb20957be2d950b85974b30a01a762f3308cd02bb15e1ad632e22dc7 \\\n    --hash=sha256:8aefbba5f69d42246543407ed2461db31006b0f76c4e32dfd6f42215a2c41d09 \\\n    --hash=sha256:96ec70beabbd3b10e8bfe52616a13561e58fe84c0101dd031dc78f250d5128b9 \\\n    --hash=sha256:9750cc7fe1ae3b1611bb8cfc3f9ec11d532244235d75901fb6b8e42ce9229dfe \\\n    --hash=sha256:9acbb16f06fe7f52f441bb6f413ebae6c37baa6ef9edd49cdd567216da8600cd \\\n    --hash=sha256:9d3e0c25a2350080e9319724dede4f31f43a6c9779be48021a7f4ebde8b2d742 \\\n    --hash=sha256:a06339f38e9ed3a64e4c4e43aec7f59084033647f908e4259d279a52d3757d09 \\\n    --hash=sha256:a0cb6f11204443f27a1628b0e460f37fb30f624be6051d490fa7d7e26d4af3d0 \\\n    --hash=sha256:a7496bfe1da7fb1a4e1cc23bb67c58fab69311cc7d32b5a99c2007b4b2a0e932 \\\n    --hash=sha256:a828c57f00f729620a442881cc60e57cfcec6842ba38e1b19fd3e47ac0ff8dc1 \\\n    --hash=sha256:a9b2de4cf0cdd5bd2dee4c4f63a653c61d2408055ab77b151c1957f221cabf2a \\\n    --hash=sha256:b46c8ae3a8f1f41a0d2ef350c0b6e65822d80772fe46b653ab6b6274f61d4a49 \\\n    --hash=sha256:b7e3ed87d4138356775346e6845cccbe66cd9e207f3cd11d2f0b9fd13681359d \\\n    --hash=sha256:b7f2f9f912dca3934c1baec2e4585a674ef16fe00218d833856408c48d5beee7 \\\n    --hash=sha256:ba60bb19387e13597fb059f32cd4d59445d7b18b69a745b8f8e5db0346f33480 \\\n    --hash=sha256:beee944ae828747fd7cb216a70f120767fc9f4f00bacae8543c14a6831673f89 \\\n    --hash=sha256:bfa4a17e17ce9abf47a74ae02f32d014c5e9404b6d9ac7f729e01562bbee601e \\\n    --hash=sha256:c037a86e8513059a2613aaba4d817bb90b9d9b6b69aace3ce9c877e8c8ed402b \\\n    --hash=sha256:c302220494f5c1ebeb0912ea782bcd5e2f8308037b3c7553fad0e48ebad6ad82 \\\n    --hash=sha256:c6321c9efe29975232da3bd0af0ad216800a47e93d763ce64f291917a381b8eb \\\n    --hash=sha256:c757a9dd70d72b076d6f68efdbb9bc943665ae954dad2801b874c8c69e185068 \\\n    --hash=sha256:c99169d4ff810155ca50b4da3b075cbde79752443117d89429595c2e8e37fed8 \\\n    --hash=sha256:c9c92be9fd329ac801cc420e08452b70e7aeab94ea4233a4804f0915c14eba9b \\\n    --hash=sha256:cc7b01b3754ea68a62bd77ce6020afaffb44a590c2289089289363472d13aedb \\\n    --hash=sha256:db9e724bebd621d9beca794f2a4ff1d26eed5965b004a97f1f1685a173b869c2 \\\n    --hash=sha256:dca69045298ce5c11fd539682cff879cc1e664c245d1c64da929813e54241d11 \\\n    --hash=sha256:dd9b1baec094d91bf36ec729445f7769d0d0cf6b64d04d86e45baf89e2b9059b \\\n    --hash=sha256:e02a0e11cf6597299b9f3bbd3f93d79217cb90cfd1411aec33848b13f5c656cc \\\n    --hash=sha256:e6a20a581f9ce92d389a8c7d7c3dd47c81fd5d6e655c8dddf341e14aa48659d0 \\\n    --hash=sha256:e7004be74cbb7d9f34553a5ce5fb08be14fb33bc86f332fb71cbe5216362a497 \\\n    --hash=sha256:e774d53b1a477a67838a904131c4b0eef6b3d8a651f8b138b04f748fccfefe17 \\\n    --hash=sha256:edb678da49d9f72c9f6c609fbe41a5dfb9a9282f9e6a2253d5a91e0fc382d7c0 \\\n    --hash=sha256:f146e0911cb2f1da549fc58fc7bcd2b836a44b79ef871980d605ec392ff6b0d2 \\\n    --hash=sha256:f56e2333dda1fe0f909e7cc59f021eba0d2307bc6f012a1ccf2beca6ba362439 \\\n    --hash=sha256:f9a3ea26252bd92f570600098783d1371354d89d5f6b7dfd87359d669f2109b5 \\\n    --hash=sha256:f9aa1878d1083b276b0196f2dfbe00c9b7e752475ed3b682025ff20c1c1f51ac \\\n    --hash=sha256:fb3c2db03683b5767dedb5769b8a40ebb47d6f7f45b1b3e3b4b51ec8ad9d9825 \\\n    --hash=sha256:fbeb989b5cc29e8daf7f976b421c220f1b8c731cbf22b9130d8815418ea45887 \\\n    --hash=sha256:fde5bd59ab5357e3853313127f4d3565fc7dad314a74d7b5d43c22c6a5ed2ced \\\n    --hash=sha256:fe1a06da377e3a1062ae5fe0926e12b84eceb8a50b350ddca72dc85015873f74\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   aiohttp\n    #   aiosignal\n    #   ray\n```\n\n----------------------------------------\n\nTITLE: Package Version and Hash Specification\nDESCRIPTION: This snippet shows how a specific package version (y-py 0.6.2) is specified along with multiple hash values for different distributions or builds of that version.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_42\n\nLANGUAGE: text\nCODE:\n```\ny-py==0.6.2 \\\n    --hash=sha256:015f7f6c1ce8a83d57955d1dc7ddd57cb633ae00576741a4fc9a0f72ed70007d \\\n    --hash=sha256:032365dfe932bfab8e80937ad6093b4c22e67d63ad880096b5fa8768f8d829ba \\\n    --hash=sha256:0649a41cd3c98e290c16592c082dbe42c7ffec747b596172eebcafb7fd8767b0\n```\n\n----------------------------------------\n\nTITLE: S3 Access Configuration\nDESCRIPTION: YAML configuration for enabling S3 access for worker nodes\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\navailable_node_types:\n  ray.worker.default:\n    node_config:\n      ...\n      IamInstanceProfile:\n        Arn: arn:aws:iam::YOUR_AWS_ACCOUNT:YOUR_INSTANCE_PROFILE\n```\n\n----------------------------------------\n\nTITLE: Defining LLM API Structure in reStructuredText\nDESCRIPTION: This code snippet defines the structure of the LLM API documentation using reStructuredText. It includes sections for the LLM processor builder, Processor class, and processor configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/llm.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _llm-ref:\n\nLarge Language Model (LLM) API\n==============================\n\n.. currentmodule:: ray.data.llm\n\nLLM processor builder\n---------------------\n\n.. autosummary::\n    :nosignatures:\n    :toctree: doc/\n\n    ~build_llm_processor\n\nProcessor\n---------\n\n.. autosummary::\n    :nosignatures:\n    :toctree: doc/\n\n    ~Processor\n\nProcessor configs\n-----------------\n\n.. autosummary::\n    :nosignatures:\n    :template: autosummary/class_without_autosummary_noinheritance.rst\n    :toctree: doc/\n\n    ~ProcessorConfig\n    ~HttpRequestProcessorConfig\n    ~vLLMEngineProcessorConfig\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with VertxHandler ReadComplete Handling\nDESCRIPTION: This stack trace shows Netty's event processing flow ending at the VertxHandler's channelReadComplete method. The trace illustrates how read operations in the NIO event loop propagate through the channel pipeline to Vert.x framework handlers.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_41\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Hash Verification\nDESCRIPTION: A requirements.txt style file that specifies package versions with hash verification for the Ray project. Each package entry includes the exact version, SHA256 hashes for verification, and comments indicating which other packages depend on it.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_32\n\nLANGUAGE: plaintext\nCODE:\n```\nreferencing==0.36.2 \\\n    --hash=sha256:df2e89862cd09deabbdba16944cc3f10feb6b3e6f18e902f7cc25609a34775aa \\\n    --hash=sha256:e8699adbbf8b5c7de96d8ffa0eb5c158b3beafce084968e2ea8bb08c6794dcd0\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   jsonschema\n    #   jsonschema-specifications\n    #   outlines\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Package dependency specifications with pinned versions and SHA-256 hash verification for security. Each package includes source comments indicating which requirement file requires it.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_5\n\nLANGUAGE: pip-requirements\nCODE:\n```\nentrypoints==0.4 \\\n    --hash=sha256:b706eddaa9218a19ebcd67b56818f05bb27589b1ca9e8d797b74affad4ccacd4 \\\n    --hash=sha256:f174b5ff827504fd3cd97cc3f8649f3693f51538c7e4bdf3ef002c8429d42f9f\nfastapi==0.115.0 \\\n    --hash=sha256:17ea427674467486e997206a5ab25760f6b09e069f099b96f5b55a32fb6f1631 \\\n    --hash=sha256:f93b4ca3529a8ebc6fc3fcf710e5efa8de3df9b41570958abf1d97d843138004\nfastjsonschema==2.19.0 \\\n    --hash=sha256:b9fd1a2dd6971dbc7fee280a95bd199ae0dd9ce22beb91cc75e9c1c528a5170e \\\n    --hash=sha256:e25df6647e1bc4a26070b700897b07b542ec898dd4f1f6ea013e7f6a88417225\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Requirements with Hashes\nDESCRIPTION: This snippet shows how package requirements are specified with version numbers and hash values. It includes multiple hash values for each package to ensure integrity across different builds or platforms.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_4\n\nLANGUAGE: Text\nCODE:\n```\nclick==8.1.7 \\\n    --hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28 \\\n    --hash=sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   typer\n    #   uvicorn\n```\n\n----------------------------------------\n\nTITLE: Specifying platformdirs Package Dependency\nDESCRIPTION: This snippet specifies the platformdirs package version 3.11.0 with SHA-256 hashes for verification. It also indicates that this dependency is required by jupyter-core and virtualenv components in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_32\n\nLANGUAGE: plaintext\nCODE:\n```\nplatformdirs==3.11.0 \\\n    --hash=sha256:cf8ee52a3afdb965072dcc652433e0c7e3e40cf5ea1477cd4b3b1d2eb75495b3 \\\n    --hash=sha256:e9d171d00af68be50e9202731309c4e658fd8bc76f55c11c7dd760d023bda68e\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   jupyter-core\n    #   virtualenv\n```\n\n----------------------------------------\n\nTITLE: Specifying Outlines-core Package with Hash Verification in Bash\nDESCRIPTION: Defines the Outlines-core package version 0.1.26 with multiple SHA256 hash verifications for secure package installation. Required for the outlines package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\noutlines-core==0.1.26 \\\n    --hash=sha256:00f409f72c11f6ffadb57066950dd384d5388015028c1a1a615c9a64988dae3e \\\n    --hash=sha256:11ff56af56cb54c563b7f25d86cd9ee77f3fed825f1d4dccd9449bb1e4e89538 \\\n    --hash=sha256:15a3684fa29564da2db03934cf0097bef3e871f70d3af0ef2b52fdb886da2e09 \\\n    --hash=sha256:19f462f6b00935708677ad27cb4df55e0e17f6ffe713ab750f5f2683b090f95d \\\n    --hash=sha256:1e0ea28a76da31d25b6f53242bf13e1b59a0241badf82353c88f55e1cf81b128 \\\n    --hash=sha256:2f8641aab4a6bd84516907492ce82099503129da01b3c29c1dc9ad50320bae77 \\\n    --hash=sha256:3f59aeccea21ed6ff3cf52102fd163f26d279821c20e5127ddd18d4ea4d0c8d2 \\\n    --hash=sha256:481c4301341e77cc8f1832d616784adb4d461b4fec65878e7c0d2cba7163a189 \\\n    --hash=sha256:64e01c0cfa9ba371634d7c3f6ea1862397cef98e4509fe98e3f57faa721a72d6 \\\n    --hash=sha256:6a962a7452e7ac170fa04d405342cadae2d28fafa5b1830cef7aa610257ed32f \\\n    --hash=sha256:7b7849cf40028319ebb9d8ba0fe4c590ef5888eebe524a81b3af30aaa06ea21c \\\n    --hash=sha256:8cc8c87d89bd267356f8149c9066cbb98970425ec162997fbf195c3f1feb7009 \\\n    --hash=sha256:9525321b48700dcaaabf60bcdc951e45f9357ba3fb3e1bfc81b662d7d4170e7c \\\n    --hash=sha256:9b36bff12779e58883747116893a17b3551bbd10865878b951b03a44d112229a \\\n    --hash=sha256:9d792a43ed9d8a4e1b38f4d83fe99db442d57aad4404c2edf98b710892eda47e \\\n    --hash=sha256:a3c4196148e47f455f1ace78e329d5b97e531cbc406456d681592952adae7e17 \\\n    --hash=sha256:a84b7cd2fb6268bf990dd3d479ffb4fa0bace6f571cb85b15b6cdb44b84f5b69 \\\n    --hash=sha256:a8932044a3d9329be53a226118850638f85b4d7842f9b863d0a123f23de220cd \\\n    --hash=sha256:ad8564ecd7b64bcb840596c5049ff1c1a96346de494302ffcc0f2b188c15675e \\\n    --hash=sha256:b6787b07b7c673fc3087d2b537719ecac8e03b10a47d032dd1926985c32885b0 \\\n    --hash=sha256:bba56604efdbc5932c7a8a88c2b8b0d0c740ab883b0012fb5464a9736796802b \\\n    --hash=sha256:e86a1bb46adc5cbf6dfd7a7fe4105e0e2a4c6e041732a053126b41c521a1f223 \\\n    --hash=sha256:f19765c151abfc970996368080aeea6d2a19e927817fe4e2af6726e639be3de4 \\\n    --hash=sha256:f38d290a7f6e5e12cbfcaee03269dfc0dbda49b360024b4279d1aba251fdc346 \\\n    --hash=sha256:f54633bca50055d42ea4d94ae06dcbe52d3d76a9b621b75723b1177d0d952953\n```\n\n----------------------------------------\n\nTITLE: Defining send2trash dependency with version pinning and hash verification\nDESCRIPTION: Specifies send2trash version 1.8.3 with SHA-256 hashes for verification. Comments indicate it's required by compiled ray test requirements, jupyter-server, nbclassic, and notebook.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_32\n\nLANGUAGE: plaintext\nCODE:\n```\nsend2trash==1.8.3 \\\n    --hash=sha256:0c31227e0bd08961c7665474a3d1ef7193929fedda4233843689baa056be46c9 \\\n    --hash=sha256:b18e7a3966d99871aefeb00cfbcfdced55ce4871194810fc71f4aa484b953abf\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jupyter-server\n    #   nbclassic\n    #   notebook\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Hashes in Requirements File\nDESCRIPTION: This snippet shows the format used to specify SHA256 hashes for package versions in a Python requirements file. Each line contains a hash value for a specific package version.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_41\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:81b19725065dcb43df02b37e03278c011a09e49757287dca60c5aecdd5a0b8ed \\\n--hash=sha256:833b58d5d0b7e5b9832869f039203389ac7cbf01765639c7309fd50ef619e0b1 \\\n--hash=sha256:88bd7b6bd70a5b6803c1abf6bca012f7ed963e58c68d76ee20b9d751c74a3248 \\\n--hash=sha256:8ad85f7f4e20964db4daadcab70b47ab05c7c1cf2a7c1e51087bfaa83831854c \\\n--hash=sha256:8c0ce1e99116d5ab21355d8ebe53d9460366704ea38ae4d9f6933188f327b456\n```\n\n----------------------------------------\n\nTITLE: Listing Pygments Package with Hash Values\nDESCRIPTION: Definition for the Pygments package dependency with version 2.18.0 and corresponding SHA256 hash values. The comment indicates this is required by IPython and Rich packages through the compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_21\n\nLANGUAGE: text\nCODE:\n```\npygments==2.18.0 \\\n    --hash=sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199 \\\n    --hash=sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   ipython\n    #   rich\n```\n\n----------------------------------------\n\nTITLE: Specifying json5 Package Version and Hashes\nDESCRIPTION: Defines the version and hash values for the json5 package. It specifies version 0.9.14 and includes two hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_14\n\nLANGUAGE: Text\nCODE:\n```\njson5==0.9.14 \\\n    --hash=sha256:740c7f1b9e584a468dbb2939d8d458db3427f2c93ae2139d05f47e453eae964f \\\n    --hash=sha256:9ed66c3a6ca3510a976a9ef9b8c0787de24802724ab1860bc0153c7fdd589b02\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jupyterlab-server\n```\n\n----------------------------------------\n\nTITLE: Initializing the Search with Points to Evaluate in Python\nDESCRIPTION: This snippet creates the search algorithm with the initial parameters to evaluate, which may initiate the optimization process more effectively.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsearcher = OptunaSearch(points_to_evaluate=initial_params)\nalgo = ConcurrencyLimiter(searcher, max_concurrent=4)\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment for Ray Development (Bash)\nDESCRIPTION: Commands to create and activate a Python virtual environment named 'myenv' for Ray development using venv.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv myenv\nsource myenv/bin/activate\npython -m pip install --upgrade pip wheel\n```\n\n----------------------------------------\n\nTITLE: Accessing Object Summary using Ray CLI\nDESCRIPTION: Command to display a summary of objects in a Ray cluster. Requires Ray installation with 'default' extras and dashboard component.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/reference/cli.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nray summary objects\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto-scaling PreLearner Pool\nDESCRIPTION: Example of enabling auto-scaling for the Post-Processing PreLearner actor pool by specifying a range for concurrency.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .offline_data(\n        map_batches_kwargs={\n            \"concurrency\": (4, 8),\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Hash Values\nDESCRIPTION: Package requirements list with specific versions and SHA256 hash values for package verification. Includes packages like locket, locust, markdown and their dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:ef01e46939293a0a36729350a7384dc17a36880176d0f17d82b738bbe77e4528\nlocket==1.0.0 \\\n    --hash=sha256:5c0d4c052a8bbbf750e056a8e65ccd309086f4f0f18a2eac306a8dfa4112a632 \\\n    --hash=sha256:b6c819a722f7b6bd955b80781788e4a66a55628b858d347536b7e81325a3a5e3\nlocust==2.18.0 \\\n    --hash=sha256:55036b2601ad7a2725885ceafb28f90390128a9a5dc631809da462f53b37cd56 \\\n    --hash=sha256:f8d668c2c33518c705664bc869791d58fc98ba8f1aadbf2335be36e4e681feae\nmarkdown==3.5.1 \\\n    --hash=sha256:5874b47d4ee3f0b14d764324d2c94c03ea66bee56f2d929da9f2508d65e722dc \\\n    --hash=sha256:b65d7beb248dc22f2e8a31fb706d93798093c308dc1aba295aedeb9d41a813bd\n```\n\n----------------------------------------\n\nTITLE: Creating RayJob with Automatic Shutdown\nDESCRIPTION: kubectl command to apply a RayJob configuration with shutdownAfterJobFinishes set to true.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-job.shutdown.yaml\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution in Netty NIO Event Loop with Network I/O\nDESCRIPTION: This stack trace shows the execution flow of a Java thread through Netty's NIO event loop, including channel read and write operations, and ending with a system call for writing data. It also includes kernel-level memory allocation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_31\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];sk_stream_alloc_skb_[k];__alloc_skb_[k];__kmalloc_node_track_caller_[k]\n```\n\n----------------------------------------\n\nTITLE: Documenting Ray Debug Command in reStructuredText\nDESCRIPTION: This snippet uses the click directive to generate documentation for the 'ray debug' command from the ray.scripts.scripts module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/cli.rst#2025-04-12_snippet_4\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. click:: ray.scripts.scripts:debug\n   :prog: ray debug\n   :show-nested:\n```\n\n----------------------------------------\n\nTITLE: Deleting Ray Cluster in Kubernetes\nDESCRIPTION: Command to delete a Ray cluster to free up resources in the queue\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete raycluster test-cluster-0\n```\n\n----------------------------------------\n\nTITLE: Removing a Placement Group - Java\nDESCRIPTION: This Java example shows how to remove a placement group using Ray. It verifies if the group is indeed removed using assertions in the Java Ray APIs. It requires an existing placement group to operate on.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/placement-group.rst#2025-04-12_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nPlacementGroups.removePlacementGroup(placementGroup.getId());\n\nPlacementGroup removedPlacementGroup = PlacementGroups.getPlacementGroup(placementGroup.getId());\nAssert.assertEquals(removedPlacementGroup.getState(), PlacementGroupState.REMOVED);\n```\n\n----------------------------------------\n\nTITLE: Ray Remote Task Testing\nDESCRIPTION: Example of testing remote task execution in Ray with different batch sizes and iterations, demonstrating parallel processing capabilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/testing-tips.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(address=cluster.address)\n\n@ray.remote\ndef f(x):\n    return x\n\nfor _ in range(1):\n    ray.get([f.remote(1) for _ in range(1000)])\n\nfor _ in range(10):\n    ray.get([f.remote(1) for _ in range(100)])\n\nfor _ in range(100):\n    ray.get([f.remote(1) for _ in range(10)])\n\nfor _ in range(1000):\n    ray.get([f.remote(1) for _ in range(1)])\n```\n\n----------------------------------------\n\nTITLE: Configuring markupsafe package dependency with hash verification\nDESCRIPTION: Specification for the markupsafe package (version 2.1.3) with SHA256 hash values for verification. This package is a dependency for the Ray project's Python requirements.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_16\n\nLANGUAGE: plain\nCODE:\n```\nmarkupsafe==2.1.3 \\\n    --hash=sha256:05fb21170423db021895e1ea1e1f3ab3adb85d1c2333cbc2310f2a26bc77272e \\\n    --hash=sha256:0a4e4a1aff6c7ac4cd55792abf96c915634c2b97e3cc1c7129578aa68ebd754e \\\n    --hash=sha256:10bbfe99883db80bdbaff2dcf681dfc6533a614f700da1287707e8a5d78a8431 \\\n    --hash=sha256:134da1eca9ec0ae528110ccc9e48041e0828d79f24121a1a146161103c76e686 \\\n    --hash=sha256:14ff806850827afd6b07a5f32bd917fb7f45b046ba40c57abdb636674a8b559c \\\n    --hash=sha256:1577735524cdad32f9f694208aa75e422adba74f1baee7551620e43a3141f559 \\\n    --hash=sha256:1b40069d487e7edb2676d3fbdb2b0829ffa2cd63a2ec26c4938b2d34391b4ecc \\\n    --hash=sha256:1b8dd8c3fd14349433c79fa8abeb573a55fc0fdd769133baac1f5e07abf54aeb \\\n    --hash=sha256:1f67c7038d560d92149c060157d623c542173016c4babc0c1913cca0564b9939 \\\n    --hash=sha256:282c2cb35b5b673bbcadb33a585408104df04f14b2d9b01d4c345a3b92861c2c \\\n    --hash=sha256:2c1b19b3aaacc6e57b7e25710ff571c24d6c3613a45e905b1fde04d691b98ee0 \\\n    --hash=sha256:2ef12179d3a291be237280175b542c07a36e7f60718296278d8593d21ca937d4 \\\n    --hash=sha256:338ae27d6b8745585f87218a3f23f1512dbf52c26c28e322dbe54bcede54ccb9 \\\n    --hash=sha256:3c0fae6c3be832a0a0473ac912810b2877c8cb9d76ca48de1ed31e1c68386575 \\\n    --hash=sha256:3fd4abcb888d15a94f32b75d8fd18ee162ca0c064f35b11134be77050296d6ba \\\n    --hash=sha256:42de32b22b6b804f42c5d98be4f7e5e977ecdd9ee9b660fda1a3edf03b11792d \\\n    --hash=sha256:47d4f1c5f80fc62fdd7777d0d40a2e9dda0a05883ab11374334f6c4de38adffd \\\n    --hash=sha256:504b320cd4b7eff6f968eddf81127112db685e81f7e36e75f9f84f0df46041c3 \\\n    --hash=sha256:525808b8019e36eb524b8c68acdd63a37e75714eac50e988180b169d64480a00 \\\n    --hash=sha256:56d9f2ecac662ca1611d183feb03a3fa4406469dafe241673d521dd5ae92a155 \\\n    --hash=sha256:5bbe06f8eeafd38e5d0a4894ffec89378b6c6a625ff57e3028921f8ff59318ac \\\n    --hash=sha256:65c1a9bcdadc6c28eecee2c119465aebff8f7a584dd719facdd9e825ec61ab52 \\\n\n```\n\n----------------------------------------\n\nTITLE: Defining shellingham dependency with version pinning and hash verification\nDESCRIPTION: Specifies shellingham version 1.5.4 with SHA-256 hashes for verification. Comments indicate it's required by compiled ray test requirements and typer.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_34\n\nLANGUAGE: plaintext\nCODE:\n```\nshellingham==1.5.4 \\\n    --hash=sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686 \\\n    --hash=sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   typer\n```\n\n----------------------------------------\n\nTITLE: Logging Complex Data with MetricsLogger in Python\nDESCRIPTION: Shows how to log complex data like images using the MetricsLogger by setting 'reduce=None' and 'clear_on_reduce=True' to manage memory efficiently.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\n\n# Log three consecutive render frames from the env.\n# Make sure to set ``clear_on_reduce=True`` to avoid memory leaks.\nenv.reset()\nlogger.log_value(\"some_images\", value=env.render(), reduce=None, clear_on_reduce=True)\nenv.step(0)\nlogger.log_value(\"some_images\", value=env.render())\nenv.step(1)\nlogger.log_value(\"some_images\", value=env.render())\n```\n\n----------------------------------------\n\nTITLE: Displaying Best Hyperparameters from Updated Tuner - Python\nDESCRIPTION: This snippet prints out the best hyperparameters discovered from the updated tuning experiment using the ConfigSpace.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Defining Google Cloud Dependencies with Cryptographic Hashes in requirements.txt\nDESCRIPTION: This snippet defines various Google Cloud related packages including google-api-core, google-auth, google-cloud-core, google-cloud-storage, and google-crc32c with their specific versions and SHA256 hashes. It also shows the dependency relationships between these packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\ngoogle-api-core==1.34.0 \\\n    --hash=sha256:6fb380f49d19ee1d09a9722d0379042b7edb06c0112e4796c7a395078a043e71 \\\n    --hash=sha256:7421474c39d396a74dfa317dddbc69188f2336835f526087c7648f91105e32ff\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-cloud-core\n    #   google-cloud-storage\n    #   opencensus\ngoogle-auth==2.23.4 \\\n    --hash=sha256:79905d6b1652187def79d491d6e23d0cbb3a21d3c7ba0dbaa9c8a01906b13ff3 \\\n    --hash=sha256:d4bbc92fe4b8bfd2f3e8d88e5ba7085935da208ee38a134fc280e7ce682a05f2\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   google-api-core\n    #   google-cloud-core\n    #   google-cloud-storage\ngoogle-cloud-core==2.4.1 \\\n    --hash=sha256:9b7749272a812bde58fff28868d0c5e2f585b82f37e09a1f6ed2d4d10f134073 \\\n    --hash=sha256:a9e6a4422b9ac5c29f79a0ede9485473338e2ce78d91f2370c01e730eab22e61\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-cloud-storage\ngoogle-cloud-storage==2.14.0 \\\n    --hash=sha256:2d23fcf59b55e7b45336729c148bb1c464468c69d5efbaee30f7201dd90eb97e \\\n    --hash=sha256:8641243bbf2a2042c16a6399551fbb13f062cbc9a2de38d6c0bb5426962e9dbd\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\ngoogle-crc32c==1.5.0 \\\n    --hash=sha256:024894d9d3cfbc5943f8f230e23950cd4906b2fe004c72e29b209420a1e6b05a \\\n    --hash=sha256:02c65b9817512edc6a4ae7c7e987fea799d2e0ee40c53ec573a692bee24de876 \\\n    --hash=sha256:02ebb8bf46c13e36998aeaad1de9b48f4caf545e91d14041270d9dca767b780c \\\n    --hash=sha256:07eb3c611ce363c51a933bf6bd7f8e3878a51d124acfc89452a75120bc436289 \\\n    --hash=sha256:1034d91442ead5a95b5aaef90dbfaca8633b0247d1e41621d1e9f9db88c36298 \\\n    --hash=sha256:116a7c3c616dd14a3de8c64a965828b197e5f2d121fedd2f8c5585c547e87b02 \\\n    --hash=sha256:19e0a019d2c4dcc5e598cd4a4bc7b008546b0358bd322537c74ad47a5386884f \\\n    --hash=sha256:1c7abdac90433b09bad6c43a43af253e688c9cfc1c86d332aed13f9a7c7f65e2 \\\n    --hash=sha256:1e986b206dae4476f41bcec1faa057851f3889503a70e1bdb2378d406223994a \\\n    --hash=sha256:272d3892a1e1a2dbc39cc5cde96834c236d5327e2122d3aaa19f6614531bb6eb \\\n    --hash=sha256:278d2ed7c16cfc075c91378c4f47924c0625f5fc84b2d50d921b18b7975bd210 \\\n    --hash=sha256:2ad40e31093a4af319dadf503b2467ccdc8f67c72e4bcba97f8c10cb078207b5 \\\n    --hash=sha256:2e920d506ec85eb4ba50cd4228c2bec05642894d4c73c59b3a2fe20346bd00ee \\\n    --hash=sha256:3359fc442a743e870f4588fcf5dcbc1bf929df1fad8fb9905cd94e5edb02e84c \\\n    --hash=sha256:37933ec6e693e51a5b07505bd05de57eee12f3e8c32b07da7e73669398e6630a \\\n    --hash=sha256:398af5e3ba9cf768787eef45c803ff9614cc3e22a5b2f7d7ae116df8b11e3314 \\\n    --hash=sha256:3b747a674c20a67343cb61d43fdd9207ce5da6a99f629c6e2541aa0e89215bcd \\\n    --hash=sha256:461665ff58895f508e2866824a47bdee72497b091c730071f2b7575d5762ab65 \\\n    --hash=sha256:4c6fdd4fccbec90cc8a01fc00773fcd5fa28db683c116ee3cb35cd5da9ef6c37 \\\n    --hash=sha256:5829b792bf5822fd0a6f6eb34c5f81dd074f01d570ed7f36aa101d6fc7a0a6e4 \\\n    --hash=sha256:596d1f98fc70232fcb6590c439f43b350cb762fb5d61ce7b0e9db4539654cc13 \\\n    --hash=sha256:5ae44e10a8e3407dbe138984f21e536583f2bba1be9491239f942c2464ac0894 \\\n    --hash=sha256:635f5d4dd18758a1fbd1049a8e8d2fee4ffed124462d837d1a02a0e009c3ab31 \\\n    --hash=sha256:64e52e2b3970bd891309c113b54cf0e4384762c934d5ae56e283f9a0afcd953e \\\n    --hash=sha256:66741ef4ee08ea0b2cc3c86916ab66b6aef03768525627fd6a1b34968b4e3709 \\\n    --hash=sha256:67b741654b851abafb7bc625b6d1cdd520a379074e64b6a128e3b688c3c04740 \\\n    --hash=sha256:6ac08d24c1f16bd2bf5eca8eaf8304812f44af5cfe5062006ec676e7e1d50afc \\\n    --hash=sha256:6f998db4e71b645350b9ac28a2167e6632c239963ca9da411523bb439c5c514d \\\n    --hash=sha256:72218785ce41b9cfd2fc1d6a017dc1ff7acfc4c17d01053265c41a2c0cc39b8c\n```\n\n----------------------------------------\n\nTITLE: Requests HTTP Library Dependency with Hash Verification\nDESCRIPTION: Specifies the requests package (version 2.31.0) with hash signatures. The comments indicate it's used by multiple packages including huggingface-hub, mistral-common, ray, and transformers.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_30\n\nLANGUAGE: txt\nCODE:\n```\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   google-api-core\n    #   huggingface-hub\n    #   mistral-common\n    #   outlines\n    #   ray\n    #   tiktoken\n    #   transformers\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Headers and Labels\nDESCRIPTION: Defines RST labels and section headers for API stability documentation including cross-reference labels for PublicAPI, DeveloperAPI, and Deprecated API definitions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/stability.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _api-stability:\n\nAPI Stability\n=============\n\n.. _api-stability-alpha:\n\nAlpha\n~~~~~\n\n.. _api-stability-beta:\n\nBeta\n~~~~\n\n.. _api-stability-stable:\n\nStable\n~~~~~~\n\n.. _public-api-def:\n\n.. autofunction:: ray.util.annotations.PublicAPI\n\n.. _developer-api-def:\n\n.. autofunction:: ray.util.annotations.DeveloperAPI\n\n.. _deprecated-api-def:\n\n.. autofunction:: ray.util.annotations.Deprecated\n```\n\n----------------------------------------\n\nTITLE: Ray Project Benchmark Timing Results\nDESCRIPTION: Displays benchmark timing statistics including total execution time, average iteration time, and min/max iteration values in seconds.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.2.0/stress_tests/test_dead_actors.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nFinished in: 133.60612034797668s\nAverage iteration time: 1.3360581374168397s\nMax iteration time: 5.137001276016235s\nMin iteration time: 0.15551400184631348s\n```\n\n----------------------------------------\n\nTITLE: Redis AOF Configuration Options\nDESCRIPTION: Additional Redis AOF (Append-Only File) configuration options showing sync frequency and rewrite settings with default values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-persistent-ft.md#2025-04-12_snippet_2\n\nLANGUAGE: redis\nCODE:\n```\n# Sync the log to disk every second.\n# Alternatives are \"no\" and \"always\" (every write).\nappendfsync everysec\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n```\n\n----------------------------------------\n\nTITLE: Upgrading KubeRay from v1.2.X to v1.3.0\nDESCRIPTION: Commands to upgrade KubeRay installation from version 1.2.X to 1.3.0, including CRD updates, operator upgrade, and verification steps using Helm and kubectl.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/upgrade-guide.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Upgrade the CRD to v1.3.0.\n# Note: This example uses kubectl because Helm doesn't support lifecycle management of CRDs.\n# See the Helm documentation for more details: https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#some-caveats-and-explanations\n$ kubectl replace -k \"github.com/ray-project/kuberay/ray-operator/config/crd?ref=v1.3.0\"\n\n# Upgrade kuberay-operator to v1.3.0. This step doesn't upgrade the CRDs.\n$ helm upgrade kuberay-operator kuberay/kuberay-operator --version v1.3.0\n\n# Install a RayCluster using the v1.3.0 helm chart to verify the success of the upgrade.\n$ helm install raycluster kuberay/ray-cluster --version 1.3.0\n```\n\n----------------------------------------\n\nTITLE: Kernel-level Memory Allocation in Network I/O Context\nDESCRIPTION: This snippet shows kernel-level operations related to memory allocation during network I/O processing. It includes functions for physical address resolution and slab allocation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_32\n\nLANGUAGE: kernel\nCODE:\n```\n__phys_addr_[k]\n```\n\nLANGUAGE: kernel\nCODE:\n```\nget_slab_[k]\n```\n\nLANGUAGE: kernel\nCODE:\n```\nkmem_cache_alloc_node_[k]\n```\n\nLANGUAGE: kernel\nCODE:\n```\nksize_[k]\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with Hash Verification\nDESCRIPTION: Defines specific versions of Python packages with SHA256 hash verification for secure package installation. Includes dependencies like cachetools 5.3.2, certifi 2025.1.31, cffi 1.16.0, and charset-normalizer 3.3.2.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\ncachetools==5.3.2 \\\n    --hash=sha256:086ee420196f7b2ab9ca2db2520aca326318b68fe5ba8bc4d49cca91add450f2 \\\n    --hash=sha256:861f35a13a451f94e301ce2bec7cac63e881232ccce7ed67fab9b5df4d3beaa1\ncertifi==2025.1.31 \\\n    --hash=sha256:3d5da6925056f6f18f119200434a4780a94263f10d1c21d032a6f6b2baa20651 \\\n    --hash=sha256:ca78db4565a652026a4db2bcdf68f2fb589ea80d0be70e03929ed730746b84fe\ncffi==1.16.0 ; platform_python_implementation != 'PyPy' \\\n    --hash=sha256:0c9ef6ff37e974b73c25eecc13952c55bceed9112be2d9d938ded8e856138bcc\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward Methods for RLModule in Python\nDESCRIPTION: Examples of implementing forward methods for RLModule, showing how to return actions or action distribution inputs for inference and exploration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.core import Columns, TorchRLModule\n\nclass MyTorchPolicy(TorchRLModule):\n    ...\n\n    def _forward_inference(self, batch):\n        ...\n        return {\n            Columns.ACTIONS: ...  # RLlib uses these actions as-is\n        }\n\n    def _forward_exploration(self, batch):\n        ...\n        return {\n            Columns.ACTIONS: ...  # RLlib uses these actions as-is (no sampling step!)\n            Columns.ACTION_DIST_INPUTS: ...  # If provided, RLlib uses these dist inputs to compute probs and logp.\n        }\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.core import Columns, TorchRLModule\n\nclass MyTorchPolicy(TorchRLModule):\n    ...\n\n    def _forward_inference(self, batch):\n        ...\n        return {\n            # RLlib:\n            # - Generates distribution from ACTION_DIST_INPUTS parameters.\n            # - Converts distribution to a deterministic equivalent.\n            # - Samples from the deterministic distribution.\n            Columns.ACTION_DIST_INPUTS: ...\n        }\n\n    def _forward_exploration(self, batch):\n        ...\n        return {\n            # RLlib:\n            # - Generates distribution from ACTION_DIST_INPUTS parameters.\n            # - Samples from the stochastic distribution.\n            # - Computes action probs and logs automatically using the sampled\n            #   actions and the distribution.\n            Columns.ACTION_DIST_INPUTS: ...\n        }\n```\n\n----------------------------------------\n\nTITLE: Creating hidden toctree for KubeRay troubleshooting documentation in Markdown\nDESCRIPTION: This snippet defines a hidden table of contents (toctree) in Markdown for organizing KubeRay troubleshooting documentation. It references two troubleshooting guide pages: general troubleshooting and RayService-specific troubleshooting.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n\ntroubleshooting/troubleshooting\ntroubleshooting/rayservice-troubleshooting\n```\n```\n\n----------------------------------------\n\nTITLE: Specifying packaging Package Requirement\nDESCRIPTION: Defines the required version and hash values for the packaging package. This ensures a specific, verified version is installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_18\n\nLANGUAGE: Text\nCODE:\n```\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution Stack Trace with Prototype Retrieval\nDESCRIPTION: This stack trace shows a similar execution path to the previous one, but ends with retrieving the prototype of a JavaScript object. It demonstrates the interaction between Java and JavaScript runtimes in a Vert.x application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_81\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptableObject:.getPrototype_[j]\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies with Pinned Versions and Hash Verification\nDESCRIPTION: This code lists Python package dependencies with exact versions, SHA256 hash verification, and dependency information in comments. It includes common ML/DL packages like torch, transformers, and tokenizers with specific version constraints for package integrity and security.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_35\n\nLANGUAGE: pip\nCODE:\n```\nstarlette==0.37.2 \\\n    --hash=sha256:6fe59f29268538e5d0d182f2791a479a0c64638e6935d1c6989e63fb2699c6ee \\\n    --hash=sha256:9af890290133b79fc3db55474ade20f6220a364a0402e0b556e7cd5e1e093823\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   fastapi\n    #   prometheus-fastapi-instrumentator\nsympy==1.13.1 \\\n    --hash=sha256:9cebf7e04ff162015ce31c9c6c9144daa34a93bd082f54fd8f12deca4f47515f \\\n    --hash=sha256:db36cdc64bf61b9b24578b6f7bab1ecdd2452cf008f34faa33776680c26d66f8\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\ntensorboardx==2.6.2.2 \\\n    --hash=sha256:160025acbf759ede23fd3526ae9d9bfbfd8b68eb16c38a010ebe326dc6395db8 \\\n    --hash=sha256:c6476d7cd0d529b0b72f4acadb1269f9ed8b22f441e87a84f2a3b940bb87b666\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\ntifffile==2024.7.21 \\\n    --hash=sha256:7f335b5d6ca49401fe0f1d87deb206f5dae47297e47b1ed52a676d05d6d26798 \\\n    --hash=sha256:818b577d49350421fb511f389f937984f9feaa2cd8177fa00823001920bf3483\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   scikit-image\ntiktoken==0.9.0 \\\n    --hash=sha256:03935988a91d6d3216e2ec7c645afbb3d870b37bcb67ada1943ec48678e7ee33 \\\n    --hash=sha256:11a20e67fdf58b0e2dea7b8654a288e481bb4fc0289d3ad21291f8d0849915fb \\\n    --hash=sha256:15a2752dea63d93b0332fb0ddb05dd909371ededa145fe6a3242f46724fa7990 \\\n    --hash=sha256:26113fec3bd7a352e4b33dbaf1bd8948de2507e30bd95a44e2b1156647bc01b4 \\\n    --hash=sha256:26242ca9dc8b58e875ff4ca078b9a94d2f0813e6a535dcd2205df5d49d927cc7 \\\n    --hash=sha256:27d457f096f87685195eea0165a1807fae87b97b2161fe8c9b1df5bd74ca6f63 \\\n    --hash=sha256:2b0e8e05a26eda1249e824156d537015480af7ae222ccb798e5234ae0285dbdb \\\n    --hash=sha256:2cf8ded49cddf825390e36dd1ad35cd49589e8161fdcb52aa25f0583e90a3e01 \\\n    --hash=sha256:3ebcec91babf21297022882344c3f7d9eed855931466c3311b1ad6b64befb3df \\\n    --hash=sha256:45556bc41241e5294063508caf901bf92ba52d8ef9222023f83d2483a3055348 \\\n    --hash=sha256:586c16358138b96ea804c034b8acf3f5d3f0258bd2bc3b0227af4af5d622e382 \\\n    --hash=sha256:5a62d7a25225bafed786a524c1b9f0910a1128f4232615bf3f8257a73aaa3b16 \\\n    --hash=sha256:5ea0edb6f83dc56d794723286215918c1cde03712cbbafa0348b33448faf5b95 \\\n    --hash=sha256:75f6d5db5bc2c6274b674ceab1615c1778e6416b14705827d19b40e6355f03e0 \\\n    --hash=sha256:8b3d80aad8d2c6b9238fc1a5524542087c52b860b10cbf952429ffb714bc1136 \\\n    --hash=sha256:92a5fb085a6a3b7350b8fc838baf493317ca0e17bd95e8642f95fc69ecfed1de \\\n    --hash=sha256:95e811743b5dfa74f4b227927ed86cbc57cad4df859cb3b643be797914e41794 \\\n    --hash=sha256:99376e1370d59bcf6935c933cb9ba64adc29033b7e73f5f7569f3aad86552b22 \\\n    --hash=sha256:a6600660f2f72369acb13a57fb3e212434ed38b045fd8cc6cdd74947b4b5d210 \\\n    --hash=sha256:b2a21133be05dc116b1d0372af051cd2c6aa1d2188250c9b553f9fa49301b336 \\\n    --hash=sha256:badb947c32739fb6ddde173e14885fb3de4d32ab9d8c591cbd013c22b4c31dd2 \\\n    --hash=sha256:c6386ca815e7d96ef5b4ac61e0048cd32ca5a92d5781255e13b31381d28667dc \\\n    --hash=sha256:cc156cb314119a8bb9748257a2eaebd5cc0753b6cb491d26694ed42fc7cb3139 \\\n    --hash=sha256:cd69372e8c9dd761f0ab873112aba55a0e3e506332dd9f7522ca466e817b1b7a \\\n    --hash=sha256:d02a5ca6a938e0490e1ff957bc48c8b078c88cb83977be1625b1fd8aac792c5d \\\n    --hash=sha256:d9c59ccc528c6c5dd51820b3474402f69d9a9e1d656226848ad68a8d5b2e5108 \\\n    --hash=sha256:e15b16f61e6f4625a57a36496d28dd182a8a60ec20a534c5343ba3cafa156ac7 \\\n    --hash=sha256:e5fd49e7799579240f03913447c0cdfa1129625ebd5ac440787afc4345990427 \\\n    --hash=sha256:e88f121c1c22b726649ce67c089b90ddda8b9662545a8aeb03cfef15967ddd03 \\\n    --hash=sha256:f0968d5beeafbca2a72c595e8385a1a1f8af58feaebb02b227229b69ca5357fd \\\n    --hash=sha256:f32cc56168eac4851109e9b5d327637f15fd662aa30dd79f964b7c39fbadd26e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   mistral-common\n    #   vllm\n    #   xgrammar\ntokenizers==0.21.0 \\\n    --hash=sha256:089d56db6782a73a27fd8abf3ba21779f5b85d4a9f35e3b493c7bbcbbf0d539b \\\n    --hash=sha256:3c4c93eae637e7d2aaae3d376f06085164e1660f89304c0ab2b1d08a406636b2 \\\n    --hash=sha256:400832c0904f77ce87c40f1a8a27493071282f785724ae62144324f171377273 \\\n    --hash=sha256:4145505a973116f91bc3ac45988a92e618a6f83eb458f49ea0790df94ee243ff \\\n    --hash=sha256:6b177fb54c4702ef611de0c069d9169f0004233890e0c4c5bd5508ae05abf193 \\\n    --hash=sha256:6b43779a269f4629bebb114e19c3fca0223296ae9fea8bb9a7a6c6fb0657ff8e \\\n    --hash=sha256:87841da5a25a3a5f70c102de371db120f41873b854ba65e52bccd57df5a3780c \\\n    --hash=sha256:9aeb255802be90acfd363626753fda0064a8df06031012fe7d52fd9a905eb00e \\\n    --hash=sha256:c87ca3dc48b9b1222d984b6b7490355a6fdb411a2d810f6f05977258400ddb74 \\\n    --hash=sha256:d8b09dbeb7a8d73ee204a70f94fc06ea0f17dcf0844f16102b9f414f0b7463ba \\\n    --hash=sha256:e84ca973b3a96894d1707e189c14a774b701596d579ffc7e69debfc036a61a04 \\\n    --hash=sha256:eb1702c2f27d25d9dd5b389cc1f2f51813e99f8ca30d9e25348db6585a97e24a \\\n    --hash=sha256:eb7202d231b273c34ec67767378cd04c767e967fda12d4a9e36208a34e2f137e \\\n    --hash=sha256:ee0894bf311b75b0c03079f33859ae4b2334d675d4e93f5a4132e1eae2834fe4 \\\n    --hash=sha256:f53ea537c925422a2e0e92a24cce96f6bc5046bbef24a1652a5edc8ba975f62e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   transformers\n    #   vllm\ntorch==2.6.0+cu124 \\\n    --hash=sha256:0f3bc53c988ce9568cd876a2a5316761e84a8704135ec8068f5f81b4417979cb \\\n    --hash=sha256:3313061c1fec4c7310cf47944e84513dcd27b6173b72a349bb7ca68d0ee6e9c0 \\\n    --hash=sha256:35cba404c0d742406cdcba1609085874bc60facdfbc50e910c47a92405fef44c \\\n    --hash=sha256:519330eef09534acad8110b6f423d2fe58c1d8e9ada999ed077a637a0021f908 \\\n    --hash=sha256:6a1fb2714e9323f11edb6e8abf7aad5f79e45ad25c081cde87681a18d99c29eb \\\n    --hash=sha256:7cc45c5b39d74875cfafe908b7f55c544147cc16b01e795feb2fe766583efe78 \\\n    --hash=sha256:7f2ba7f7c0459320a521696f6b5bccc187f59890b23c9dfb6c49b0b87c6bfc97 \\\n    --hash=sha256:a393b506844035c0dac2f30ea8478c343b8e95a429f06f3b3cadfc7f53adb597 \\\n    --hash=sha256:c2eb62b99161d87be486c88fd82441274cc892bce8c48dbc28c055cb147732ce \\\n    --hash=sha256:d4c3e9a8d31a7c0fcbb9da17c31a1917e1fac26c566a4cfbd8c9568ad7cade79 \\\n    --hash=sha256:e661267cd0242462ab100bdd67f651988aa9f67eb31609d6909afcac891df612\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   compressed-tensors\n    #   outlines\n    #   torchaudio\n    #   torchvision\n    #   vllm\n    #   xformers\n    #   xgrammar\ntorchaudio==2.6.0+cu124 \\\n    --hash=sha256:004ff6bcee0ac78747253c09db67d281add4308a9b87a7bf1769da5914998639 \\\n    --hash=sha256:1184cdaa3ae35135d9183c3e8a89d839e414ea2a14bbcaab0c8833369abb5af6 \\\n    --hash=sha256:1bc23963f447c910a0060b130b04b407d2ea218b2a553e674c829d5f17eb8c8e \\\n    --hash=sha256:231eddbfd8bafd06b2c9f55cd6f33e61f58b25b19f2d51382a95e8f12887689f \\\n    --hash=sha256:2b9cdda37156abe395e470ce16d9626d71b73f73eab6fc184f476f843ba12cc1 \\\n    --hash=sha256:359220c7db655ccdf1d5f1c5c034b30741eb49f9ac20ae27b9272b4f837eec1d \\\n    --hash=sha256:3e5ffa69606171c74f3e2b969785ead50b782ca657e746aaee1ee7cc88dcfc08 \\\n    --hash=sha256:6b54f97fff96b4ba3da44b6b3f50727c25122d1479107b119d1275944ec83ea1 \\\n    --hash=sha256:a25e146ce66ea9a6aed39008cc2001891bdf75253af479a4c32096678b2073b3 \\\n    --hash=sha256:b8c15d7e0e81a23630a2de552ebacfe6643990dc890f83f426e43ff62efe8651\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\ntorchvision==0.21.0+cu124 \\\n    --hash=sha256:000a013584ad2304ab30496318145f284ac364622addb5ee3a5abd2769ba146f \\\n    --hash=sha256:0c6aefb70ab2b312065240c804e459ac7b0e449867afd469b38d2fd47f9391a7 \\\n    --hash=sha256:137376805aca5ba57bd2c7a3ecb8569df961dbe82b128aac9b3b0a7125ef9385 \\\n    --hash=sha256:3d3e74018eaa7837c73e3764dad3b7792b7544401c25a42977e9744303731bd3 \\\n    --hash=sha256:4b70acf3b4b96a0ceb1374116626c9bef9e8be016b57b1284e482260ca1896d6 \\\n    --hash=sha256:579b6a7fffc34a860c57a7131221ef125831f5961431f8da15760ab1ef752d44 \\\n    --hash=sha256:6afb21a22f5497e08ea4dbd4544472330d8249bf09dafd239302552cad6906b2 \\\n    --hash=sha256:8fcf55321b206de70ff8e01c884fa42e57a60b1cb749341b96e0f22c8a7c9ec7 \\\n    --hash=sha256:ec63c2ee792757492da40590e34b14f2fceda29050558c215f0c1f3b08149c0f \\\n    --hash=sha256:efb53ea0af7bf09b7b53e2a18b9be6d245f7d46a90b51d5cf97f37e9b929a991\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\ntqdm==4.64.1 \\\n    --hash=sha256:5f4f682a004951c1b450bc753c710e9280c5746ce6ffedee253ddbcbf54cf1e4 \\\n    --hash=sha256:6fee160d6ffcd1b1c68c65f14c829c22832bc401726335ce92c52d395944a6a1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   gguf\n    #   huggingface-hub\n    #   openai\n    #   outlines\n    #   transformers\n    #   vllm\ntransformers==4.49.0 \\\n    --hash=sha256:6b4fded1c5fee04d384b1014495b4235a2b53c87503d7d592423c06128cbbe03 \\\n    --hash=sha256:7e40e640b5b8dc3f48743f5f5adbdce3660c82baafbd3afdfc04143cdbd2089e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   compressed-tensors\n    #   vllm\n    #   xgrammar\ntriton==3.2.0 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:0fc1217eed33c7695272f981f5a8874ce3cb0195bbb2bfed16d58edd0aefef04 \\\n    --hash=sha256:142dd3a9ac2fc3433768eeb4a4cd120655e2f658f4bf42726d2ea7f3748abffa \\\n    --hash=sha256:30ceed0eff2c4a73b14eb63e052992f44bbdf175f3fad21e1ac8097a772de7ee \\\n    --hash=sha256:468a01c9aa6e18fe2bba49c5e5002c1fd5f61b1af891c0594eaf446fe1aaae10 \\\n    --hash=sha256:8009a1fb093ee8546495e96731336a33fb8856a38e45bb4ab6affd6dbc3ba220 \\\n    --hash=sha256:8d9b215efc1c26fa7eefb9a157915c92d52e000d2bf83e5f69704047e63f125c \\\n    --hash=sha256:b3e54983cd51875855da7c68ec05c05cf8bb08df361b1d5b69e05e40b0c9bd62 \\\n    --hash=sha256:d528960c898f74596d5a8af1d70a7f0899c05a0781205eab51407b67f1644652 \\\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and SHA256 hashes for security. It includes entries for pyyaml, referencing, requests, rich, and rpds-py packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_19\n\nLANGUAGE: Text\nCODE:\n```\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    # ... (additional hashes omitted for brevity)\n\nreferencing==0.36.2 \\\n    --hash=sha256:df2e89862cd09deabbdba16944cc3f10feb6b3e6f18e902f7cc25609a34775aa \\\n    --hash=sha256:e8699adbbf8b5c7de96d8ffa0eb5c158b3beafce084968e2ea8bb08c6794dcd0\n\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n\nrich==13.3.2 \\\n    --hash=sha256:91954fe80cfb7985727a467ca98a7618e5dd15178cc2da10f553b36a93859001 \\\n    --hash=sha256:a104f37270bf677148d8acb07d33be1569eeee87e2d1beb286a4e9113caf6f2f\n\nrpds-py==0.22.3 \\\n    --hash=sha256:009de23c9c9ee54bf11303a966edf4d9087cd43a6003672e6aa7def643d06518 \\\n    --hash=sha256:02fbb9c288ae08bcb34fb41d516d5eeb0455ac35b5512d03181d755d80810059 \\\n    # ... (additional hashes omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Creating GKE Cluster with CPU Nodes\nDESCRIPTION: This command creates a Kubernetes cluster named 'kuberay-gpu-cluster' with 1 CPU node in the us-west1-b zone, using the e2-standard-4 machine type. It enables autoscaling with a minimum of 0 and maximum of 1 node.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container clusters create kuberay-gpu-cluster \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-west1-b --machine-type e2-standard-4\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple RayJobs for Queue Testing\nDESCRIPTION: Commands to create multiple RayJob custom resources to demonstrate Kueue's job queueing functionality. This setup helps in observing how Kueue manages multiple jobs with limited resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f ray-job.pytorch-distributed-training.yaml\nkubectl create -f ray-job.pytorch-distributed-training.yaml\nkubectl create -f ray-job.pytorch-distributed-training.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining six dependency with version pinning and hash verification\nDESCRIPTION: Specifies six version 1.16.0 with SHA-256 hashes. Comments indicate it's required by multiple packages including asttokens, bleach, halo, oauth2client, and others.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_35\n\nLANGUAGE: plaintext\nCODE:\n```\nsix==1.16.0 \\\n    --hash=sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926 \\\n    --hash=sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   asttokens\n    #   bleach\n    #   halo\n    #   oauth2client\n    #   python-dateutil\n    #   rfc3339-validator\n```\n\n----------------------------------------\n\nTITLE: Configuring pip-compile for Ray Project Dependencies\nDESCRIPTION: Command to generate a compiled requirements file for the Ray project using pip-compile. It specifies various options including index URLs, find-links, and safety settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cpu --find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c python/requirements_compiled_rayllm_test_py311_cpu.txt python/requirements.txt python/requirements/llm/llm-requirements.txt -o python/requirements_compiled_rayllm_py311_cpu.txt\n```\n\n----------------------------------------\n\nTITLE: Checking Volcano Queue Status\nDESCRIPTION: Command to check the status of the Volcano queue to see running jobs and resource allocation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get queue kuberay-test-queue -o yaml\n\n# apiVersion: scheduling.volcano.sh/v1beta1\n# kind: Queue\n# metadata:\n#   creationTimestamp: \"2022-12-01T04:43:21Z\"\n#   generation: 1\n#   name: kuberay-test-queue\n#   resourceVersion: \"4427348\"\n#   uid: a6c4f9df-d58c-4da8-8a58-e01c93eca45a\n# spec:\n#   capability:\n#     cpu: 4\n#     memory: 6Gi\n#   reclaimable: true\n#   weight: 1\n# status:\n#   reservation: {}\n#   running: 1\n#   state: Open\n```\n\n----------------------------------------\n\nTITLE: Specifying Pandas Package with Hash Verification\nDESCRIPTION: Defines the pandas package version 1.5.3 with multiple SHA256 hash verifications and notes that it's required by python/requirements.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_23\n\nLANGUAGE: text\nCODE:\n```\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    --hash=sha256:41179ce559943d83a9b4bbacb736b04c928b095b5f25dd2b7389eda08f46f373 \\\n    --hash=sha256:478ff646ca42b20376e4ed3fa2e8d7341e8a63105586efe54fa2508ee087f328 \\\n    --hash=sha256:50869a35cbb0f2e0cd5ec04b191e7b12ed688874bd05dd777c19b28cbea90996 \\\n    --hash=sha256:565fa34a5434d38e9d250af3c12ff931abaf88050551d9fbcdfafca50d62babf \\\n    --hash=sha256:5f2b952406a1588ad4cad5b3f55f520e82e902388a6d5a4a91baa8d38d23c7f6 \\\n    --hash=sha256:5fbcb19d6fceb9e946b3e23258757c7b225ba450990d9ed63ccceeb8cae609f7 \\\n    --hash=sha256:6973549c01ca91ec96199e940495219c887ea815b2083722821f1d7abfa2b4dc \\\n    --hash=sha256:74a3fd7e5a7ec052f183273dc7b0acd3a863edf7520f5d3a1765c04ffdb3b0b1 \\\n    --hash=sha256:7a0a56cef15fd1586726dace5616db75ebcfec9179a3a55e78f72c5639fa2a23 \\\n    --hash=sha256:7cec0bee9f294e5de5bbfc14d0573f65526071029d036b753ee6507d2a21480a \\\n    --hash=sha256:87bd9c03da1ac870a6d2c8902a0e1fd4267ca00f13bc494c9e5a9020920e1d51 \\\n    --hash=sha256:972d8a45395f2a2d26733eb8d0f629b2f90bebe8e8eddbb8829b180c09639572 \\\n    --hash=sha256:9842b6f4b8479e41968eced654487258ed81df7d1c9b7b870ceea24ed9459b31 \\\n    --hash=sha256:9f69c4029613de47816b1bb30ff5ac778686688751a5e9c99ad8c7031f6508e5 \\\n    --hash=sha256:a50d9a4336a9621cab7b8eb3fb11adb82de58f9b91d84c2cd526576b881a0c5a \\\n    --hash=sha256:bc4c368f42b551bf72fac35c5128963a171b40dce866fb066540eeaf46faa003 \\\n    --hash=sha256:c39a8da13cede5adcd3be1182883aea1c925476f4e84b2807a46e2775306305d \\\n    --hash=sha256:c3ac844a0fe00bfaeb2c9b51ab1424e5c8744f89860b138434a363b1f620f354 \\\n    --hash=sha256:c4c00e0b0597c8e4f59e8d461f797e5d70b4d025880516a8261b2817c47759ee \\\n    --hash=sha256:c74a62747864ed568f5a82a49a23a8d7fe171d0c69038b38cedf0976831296fa \\\n    --hash=sha256:dd05f7783b3274aa206a1af06f0ceed3f9b412cf665b7247eacd83be41cf7bf0 \\\n    --hash=sha256:dfd681c5dc216037e0b0a2c821f5ed99ba9f03ebcf119c7dac0e9a7b960b9ec9 \\\n    --hash=sha256:e474390e60ed609cec869b0da796ad94f420bb057d86784191eefc62b65819ae \\\n    --hash=sha256:f76d097d12c82a535fda9dfe5e8dd4127952b45fea9b0276cb30cca5ea313fbc\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Ray Training Status Table\nDESCRIPTION: ASCII table showing training results for 4 IMPALA trials on different Atari games. Displays trial names, status, environment, iterations, total time, timesteps, and rewards achieved.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.6/stress_tests/application_stress_test.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nMemory usage on this node: 19.9/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/640 CPUs, 0/8 GPUs, 0.0/1906.49 GiB heap, 0.0/566.21 GiB objects\nResult logdir: /home/ubuntu/ray_results/atari-impala\nNumber of trials: 4 (4 TERMINATED)\n+------------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------+\n| Trial name                                     | status     | loc   | env                         |   iter |   total time (s) |       ts |   reward |\n|------------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_a4e31_00000      | TERMINATED |       | BreakoutNoFrameskip-v4      |    680 |          6847.72 | 30047500 |   358.37 |\n| IMPALA_BeamRiderNoFrameskip-v4_a4e31_00001     | TERMINATED |       | BeamRiderNoFrameskip-v4     |    694 |          6987.42 | 30001500 |  2657.04 |\n| IMPALA_QbertNoFrameskip-v4_a4e31_00002         | TERMINATED |       | QbertNoFrameskip-v4         |    676 |          6821.02 | 30016500 |  3606.75 |\n| IMPALA_SpaceInvadersNoFrameskip-v4_a4e31_00003 | TERMINATED |       | SpaceInvadersNoFrameskip-v4 |    684 |          6889.83 | 30025000 |   653.4  |\n+------------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------+\n```\n\n----------------------------------------\n\nTITLE: Installing pydantic with Pinned Version and Hashes\nDESCRIPTION: Specifies pydantic package with version 2.9.2 and SHA256 hashes for verification. Comments indicate this is required by fastapi and is directly specified in requirements_byod_3.9.in.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_27\n\nLANGUAGE: pip\nCODE:\n```\npydantic==2.9.2 \\\n    --hash=sha256:d155cef71265d1e9807ed1c32b4c8deec042a44a50a4188b25ac67ecd81a9c0f \\\n    --hash=sha256:f048cec7b26778210e28a0459867920654d48e5e62db0958433636cde4254f12\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n    #   fastapi\n```\n\n----------------------------------------\n\nTITLE: Specifying PyZMQ Package Version with Hashes\nDESCRIPTION: Defines the PyZMQ package dependency with version 26.0.3 and multiple SHA256 hash verification values. PyZMQ is a Python binding for ZeroMQ, which is likely used for messaging and communication in the Ray distributed framework.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_9\n\nLANGUAGE: txt\nCODE:\n```\npyzmq==26.0.3 \\\n    --hash=sha256:01fbfbeb8249a68d257f601deb50c70c929dc2dfe683b754659569e502fbd3aa \\\n    --hash=sha256:0270b49b6847f0d106d64b5086e9ad5dc8a902413b5dbbb15d12b60f9c1747a4 \\\n    --hash=sha256:03c0ae165e700364b266876d712acb1ac02693acd920afa67da2ebb91a0b3c09 \\\n    --hash=sha256:068ca17214038ae986d68f4a7021f97e187ed278ab6dccb79f837d765a54d753 \\\n    --hash=sha256:082a2988364b60bb5de809373098361cf1dbb239623e39e46cb18bc035ed9c0c \\\n    --hash=sha256:0aaf982e68a7ac284377d051c742610220fd06d330dcd4c4dbb4cdd77c22a537 \\\n    --hash=sha256:0c0991f5a96a8e620f7691e61178cd8f457b49e17b7d9cfa2067e2a0a89fc1d5 \\\n    --hash=sha256:115f8359402fa527cf47708d6f8a0f8234f0e9ca0cab7c18c9c189c194dbf620 \\\n    --hash=sha256:15c59e780be8f30a60816a9adab900c12a58d79c1ac742b4a8df044ab2a6d920 \\\n    --hash=sha256:1b7d0e124948daa4d9686d421ef5087c0516bc6179fdcf8828b8444f8e461a77 \\\n    --hash=sha256:1c8eb19abe87029c18f226d42b8a2c9efdd139d08f8bf6e085dd9075446db450 \\\n    --hash=sha256:204e0f176fd1d067671157d049466869b3ae1fc51e354708b0dc41cf94e23a3a \\\n    --hash=sha256:2136f64fbb86451dbbf70223635a468272dd20075f988a102bf8a3f194a411dc \\\n    --hash=sha256:2b291d1230845871c00c8462c50565a9cd6026fe1228e77ca934470bb7d70ea0 \\\n    --hash=sha256:2c18645ef6294d99b256806e34653e86236eb266278c8ec8112622b61db255de \\\n    --hash=sha256:2cc4e280098c1b192c42a849de8de2c8e0f3a84086a76ec5b07bfee29bda7d18 \\\n    --hash=sha256:2ed8357f4c6e0daa4f3baf31832df8a33334e0fe5b020a61bc8b345a3db7a606 \\\n    --hash=sha256:3191d312c73e3cfd0f0afdf51df8405aafeb0bad71e7ed8f68b24b63c4f36500 \\\n    --hash=sha256:3401613148d93ef0fd9aabdbddb212de3db7a4475367f49f590c837355343972 \\\n    --hash=sha256:34106f68e20e6ff253c9f596ea50397dbd8699828d55e8fa18bd4323d8d966e6 \\\n    --hash=sha256:3516119f4f9b8671083a70b6afaa0a070f5683e431ab3dc26e9215620d7ca1ad \\\n    --hash=sha256:38ece17ec5f20d7d9b442e5174ae9f020365d01ba7c112205a4d59cf19dc38ee \\\n    --hash=sha256:3b4032a96410bdc760061b14ed6a33613ffb7f702181ba999df5d16fb96ba16a \\\n    --hash=sha256:3bf8b000a4e2967e6dfdd8656cd0757d18c7e5ce3d16339e550bd462f4857e59 \\\n    --hash=sha256:3e3070e680f79887d60feeda051a58d0ac36622e1759f305a41059eff62c6da7 \\\n    --hash=sha256:4496b1282c70c442809fc1b151977c3d967bfb33e4e17cedbf226d97de18f709 \\\n    --hash=sha256:44dd6fc3034f1eaa72ece33588867df9e006a7303725a12d64c3dff92330f625 \\\n    --hash=sha256:4adfbb5451196842a88fda3612e2c0414134874bffb1c2ce83ab4242ec9e027d \\\n    --hash=sha256:4b7c0c0b3244bb2275abe255d4a30c050d541c6cb18b870975553f1fb6f37527 \\\n    --hash=sha256:4c82a6d952a1d555bf4be42b6532927d2a5686dd3c3e280e5f63225ab47ac1f5 \\\n    --hash=sha256:5344b896e79800af86ad643408ca9aa303a017f6ebff8cee5a3163c1e9aec987 \\\n    --hash=sha256:5bde86a2ed3ce587fa2b207424ce15b9a83a9fa14422dcc1c5356a13aed3df9d \\\n    --hash=sha256:5bf6c237f8c681dfb91b17f8435b2735951f0d1fad10cc5dfd96db110243370b \\\n    --hash=sha256:5dbb9c997932473a27afa93954bb77a9f9b786b4ccf718d903f35da3232317de \\\n    --hash=sha256:69ea9d6d9baa25a4dc9cef5e2b77b8537827b122214f210dd925132e34ae9b12 \\\n    --hash=sha256:6b3146f9ae6af82c47a5282ac8803523d381b3b21caeae0327ed2f7ecb718798 \\\n    --hash=sha256:6bcb34f869d431799c3ee7d516554797f7760cb2198ecaa89c3f176f72d062be \\\n    --hash=sha256:6ca08b840fe95d1c2bd9ab92dac5685f949fc6f9ae820ec16193e5ddf603c3b2 \\\n    --hash=sha256:6ca7a9a06b52d0e38ccf6bca1aeff7be178917893f3883f37b75589d42c4ac20 \\\n    --hash=sha256:703c60b9910488d3d0954ca585c34f541e506a091a41930e663a098d3b794c67 \\\n    --hash=sha256:715bdf952b9533ba13dfcf1f431a8f49e63cecc31d91d007bc1deb914f47d0e4 \\\n    --hash=sha256:72b67f966b57dbd18dcc7efbc1c7fc9f5f983e572db1877081f075004614fcdd \\\n    --hash=sha256:74423631b6be371edfbf7eabb02ab995c2563fee60a80a30829176842e71722a \\\n    --hash=sha256:77a85dca4c2430ac04dc2a2185c2deb3858a34fe7f403d0a946fa56970cf60a1 \\\n    --hash=sha256:7821d44fe07335bea256b9f1f41474a642ca55fa671dfd9f00af8d68a920c2d4 \\\n    --hash=sha256:788f15721c64109cf720791714dc14afd0f449d63f3a5487724f024345067381 \\\n    --hash=sha256:7ca684ee649b55fd8f378127ac8462fb6c85f251c2fb027eb3c887e8ee347bcd \\\n    --hash=sha256:7daa3e1369355766dea11f1d8ef829905c3b9da886ea3152788dc25ee6079e02 \\\n    --hash=sha256:7e6bc96ebe49604df3ec2c6389cc3876cabe475e6bfc84ced1bf4e630662cb35 \\\n    --hash=sha256:80b12f25d805a919d53efc0a5ad7c0c0326f13b4eae981a5d7b7cc343318ebb7 \\\n    --hash=sha256:871587bdadd1075b112e697173e946a07d722459d20716ceb3d1bd6c64bd08ce \\\n    --hash=sha256:88b88282e55fa39dd556d7fc04160bcf39dea015f78e0cecec8ff4f06c1fc2b5 \\\n    --hash=sha256:8d7a498671ca87e32b54cb47c82a92b40130a26c5197d392720a1bce1b3c77cf \\\n    --hash=sha256:926838a535c2c1ea21c903f909a9a54e675c2126728c21381a94ddf37c3cbddf \\\n    --hash=sha256:971e8990c5cc4ddcff26e149398fc7b0f6a042306e82500f5e8db3b10ce69f84 \\\n    --hash=sha256:9b273ecfbc590a1b98f014ae41e5cf723932f3b53ba9367cfb676f838038b32c \\\n    --hash=sha256:a42db008d58530efa3b881eeee4991146de0b790e095f7ae43ba5cc612decbc5 \\\n    --hash=sha256:a72a84570f84c374b4c287183debc776dc319d3e8ce6b6a0041ce2e400de3f32 \\\n\n```\n\n----------------------------------------\n\nTITLE: Installing Mars Package with Pip\nDESCRIPTION: Command to install Mars via pip. Version 0.8.3 or higher is required for Ray integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/mars-on-ray.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pymars>=0.8.3\n```\n\n----------------------------------------\n\nTITLE: Identifying Ray Head Pod\nDESCRIPTION: Retrieves the name of the Ray head pod using a Kubernetes selector.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl get pods --selector=app=ray-cluster-head\n```\n\n----------------------------------------\n\nTITLE: Package Hash Declarations\nDESCRIPTION: SHA-256 hash declarations for Python package dependencies. Each line specifies a package version and its corresponding hash for verification during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:ae561d74bcfacfe96970e3ec847cdeeda7fe2cb3ad38ff44ad370de75cef5615 \\\n--hash=sha256:bd151867c7007c1af27c901d3fd9dd178e41468775b782e083d0d125228a915f \\\n--hash=sha256:cc0a1859332167c29a30ccf65c12bc56c3e4524244ac36df04c126a6d7711959 \\\n--hash=sha256:d3c4fbc944bc2c0529da3efe0c5accab20df6c99aef7adfd17e3d0fecd10a80a \\\n--hash=sha256:eb381bc5a1b8f17477700447a6cc676f22e91cc54a96f45dabe803f7fb0aec4d \\\n--hash=sha256:eb59626839181af5918b7f82d179dd50501383c3d4d0d3ebf30f4d8be81485f6\n```\n\n----------------------------------------\n\nTITLE: Defining pip package requirements and dependencies for Ray project\nDESCRIPTION: A comprehensive pip requirements file that lists all dependencies for the Ray project with version constraints and dependency chains. The file includes additional PyTorch indexes and packages for ML, data processing, cloud integration, testing, and development utilities.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled.txt#2025-04-12_snippet_0\n\nLANGUAGE: pip-requirements\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cpu\n--find-links https://data.pyg.org/whl/torch-2.3.0+cpu.html\n\nabsl-py==1.4.0\n    # via\n    #   array-record\n    #   dm-control\n    #   dm-env\n    #   etils\n    #   labmaze\n    #   ml-collections\n    #   mujoco\n    #   open-spiel\n    #   tensorboard\n    #   tensorflow\n    #   tensorflow-datasets\n    #   tensorflow-metadata\n    #   tensorflow-probability\nacccelerate==0.28.0\n    # via -r python/requirements/ml/core-requirements.txt\nadagio==0.2.4\n    # via\n    #   fugue\n    #   qpd\nadal==1.2.7\n    # via msrestazure\naim==3.23.0 ; python_version < \"3.12\"\n    # via -r python/requirements/ml/tune-test-requirements.txt\naim-ui==3.23.0\n    # via aim\naimrecords==0.0.7\n    # via aim\naimrocks==0.5.2\n    # via aim\naioboto3==11.2.0\n    # via -r python/requirements/ml/data-requirements.txt\naiobotocore==2.5.0\n    # via\n    #   aioboto3\n    #   s3fs\naiofiles==22.1.0\n    # via\n    #   aim\n    #   gradio\n    #   ypy-websocket\naiohappyeyeballs==2.6.1\n    # via aiohttp\naiohttp==3.11.16\n    # via\n    #   -r python/requirements.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements/test-requirements.txt\n    #   aiobotocore\n    #   aiohttp-cors\n    #   datasets\n    #   delta-sharing\n    #   fsspec\n    #   google-auth\n    #   pytest-aiohttp\n    #   s3fs\n    #   torch-geometric\naiohttp-cors==0.7.0\n    # via -r python/requirements.txt\naioitertools==0.11.0\n    # via aiobotocore\naiorwlock==1.3.0\n    # via -r python/requirements.txt\naiosignal==1.3.1\n    # via\n    #   aiohttp\n    #   ray\naiosqlite==0.19.0\n    # via ypy-websocket\nalabaster==0.7.13\n    # via sphinx\nale-py==0.10.1\n    # via -r python/requirements/ml/rllib-test-requirements.txt\nalembic==1.12.1\n    # via\n    #   aim\n    #   mlflow\n    #   optuna\naltair==5.1.2\n    # via gradio\nannotated-types==0.6.0\n    # via pydantic\nantlr4-python3-runtime==4.11.1\n    # via\n    #   fugue-sql-antlr\n    #   qpd\nanyio==3.7.1\n    # via\n    #   httpx\n    #   jupyter-server\n    #   starlette\n    #   watchfiles\nappdirs==1.4.4\n    # via fs\napplicationinsights==0.11.10\n    # via azure-cli-telemetry\nargcomplete==3.3.0\n    # via\n    #   azure-cli-core\n    #   gsutil\n    #   knack\n    #   yq\nargon2-cffi==23.1.0\n    # via\n    #   jupyter-server\n    #   nbclassic\n    #   notebook\nargon2-cffi-bindings==21.2.0\n    # via argon2-cffi\nargparse==1.4.0\n    # via mosaicml\narray-record==0.5.1 ; python_version < \"3.12\" and sys_platform != \"darwin\" and platform_system != \"Windows\"\n    # via\n    #   -r python/requirements/ml/dl-cpu-requirements.txt\n    #   tensorflow-datasets\narrow==1.3.0\n    # via isoduration\nasn1crypto==1.5.1\n    # via snowflake-connector-python\nasttokens==2.4.1\n    # via stack-data\nastunparse==1.6.3\n    # via tensorflow\nasync-exit-stack==1.0.1\n    # via -r python/requirements/test-requirements.txt\nasync-generator==1.10\n    # via -r python/requirements/test-requirements.txt\nasync-timeout==4.0.3\n    # via redis\nattrs==25.1.0\n    # via\n    #   -r python/requirements/test-requirements.txt\n    #   aiohttp\n    #   glom\n    #   jschema-to-python\n    #   jsonschema\n    #   jupyter-cache\n    #   open-spiel\n    #   referencing\n    #   sarif-om\n    #   semgrep\naws-sam-translator==1.81.0\n    # via cfn-lint\naws-xray-sdk==2.12.1\n    # via moto\nax-platform==0.3.2\n    # via -r python/requirements/ml/tune-requirements.txt\nazure-cli-core==2.62.0\n    # via -r python/requirements/test-requirements.txt\nazure-cli-telemetry==1.1.0\n    # via azure-cli-core\nazure-common==1.1.28\n    # via\n    #   azure-mgmt-compute\n    #   azure-mgmt-network\n    #   azure-mgmt-resource\nazure-core==1.29.5\n    # via\n    #   azure-identity\n    #   azure-mgmt-core\n    #   msrest\nazure-identity==1.17.1\n    # via -r python/requirements/test-requirements.txt\nazure-mgmt-compute==31.0.0\n    # via -r python/requirements/test-requirements.txt\nazure-mgmt-core==1.4.0\n    # via\n    #   azure-cli-core\n    #   azure-mgmt-compute\n    #   azure-mgmt-network\n    #   azure-mgmt-resource\nazure-mgmt-network==25.4.0\n    # via -r python/requirements/test-requirements.txt\nazure-mgmt-resource==23.1.1\n    # via -r python/requirements/test-requirements.txt\nbabel==2.13.1\n    # via\n    #   jupyterlab-server\n    #   sphinx\nbackcall==0.2.0\n    # via ipython\nbackoff==1.10.0\n    # via\n    #   -r python/requirements/test-requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   segment-analytics-python\nbase58==2.0.1\n    # via aimrecords\nbash-kernel==0.10.0\n    # via -r python/requirements/test-requirements.txt\nbayesian-optimization==1.4.3\n    # via\n    #   -r python/requirements/ml/tune-requirements.txt\n    #   nevergrad\nbcrypt==4.0.1\n    # via paramiko\nbeautifulsoup4==4.11.1\n    # via\n    #   -r python/requirements/test-requirements.txt\n    #   nbconvert\nblack==22.10.0\n    # via -r python/requirements/lint-requirements.txt\nbleach==6.1.0\n    # via nbconvert\nbokeh==2.4.3\n    # via dask\nboltons==21.0.0\n    # via\n    #   face\n    #   glom\n    #   semgrep\nboto==2.49.0\n    # via gcs-oauth2-boto-plugin\nboto3==1.26.76\n    # via\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements/test-requirements.txt\n    #   aim\n    #   aiobotocore\n    #   aws-sam-translator\n    #   moto\n    #   smart-open\nbotocore==1.29.76\n    # via\n    #   -r python/requirements/cloud-requirements.txt\n    #   aiobotocore\n    #   aws-xray-sdk\n    #   boto3\n    #   moto\n    #   s3transfer\nbotorch==0.8.5\n    # via ax-platform\nbraceexpand==0.1.7\n    # via webdataset\nbracex==2.4\n    # via wcmatch\ncachetools==5.3.2\n    # via\n    #   aim\n    #   google-auth\ncertifi==2025.1.31\n    # via\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements/security-requirements.txt\n    #   clickhouse-connect\n    #   httpcore\n    #   httpx\n    #   kubernetes\n    #   msrest\n    #   requests\n    #   sentry-sdk\n    #   snowflake-connector-python\ncffi==1.16.0\n    # via\n    #   argon2-cffi-bindings\n    #   cryptography\n    #   pymunk\n    #   pynacl\n    #   snowflake-connector-python\n    #   soundfile\ncfgv==3.4.0\n    # via pre-commit\ncfn-lint==0.83.3\n    # via moto\ncharset-normalizer==3.3.2\n    # via\n    #   requests\n    #   snowflake-connector-python\nclang-format==12.0.1\n    # via -r python/requirements/lint-requirements.txt\nclick==8.1.7\n    # via\n    #   -r python/requirements.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   aim\n    #   black\n    #   click-option-group\n    #   dask\n    #   databricks-cli\n    #   distributed\n    #   flask\n    #   jupyter-cache\n    #   mlflow\n    #   pyiceberg\n    #   ray\n    #   semgrep\n    #   tensorflow-datasets\n    #   typer\n    #   uvicorn\n    #   wandb\nclick-option-group==0.5.6\n    # via semgrep\nclickhouse-connect==0.8.10\n    # via -r python/requirements/ml/data-test-requirements.txt\ncloudpickle==2.2.0\n    # via\n    #   -r python/requirements/test-requirements.txt\n    #   dask\n    #   distributed\n    #   gymnasium\n    #   hyperopt\n    #   mlagents-envs\n    #   mlflow\n    #   pymars\n    #   statsforecast\n    #   tensorflow-probability\ncma==3.2.2\n    # via nevergrad\ncmdstanpy==1.2.0\n    # via prophet\ncolorama==0.4.6\n    # via\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements/test-requirements.txt\n    #   bayesian-optimization\n    #   halo\n    #   log-symbols\n    #   semgrep\ncoloredlogs==15.0.1\n    # via onnxruntime\ncolorful==0.5.5\n    # via -r python/requirements.txt\ncolorlog==6.7.0\n    # via optuna\ncomet-ml==3.44.1\n    # via -r python/requirements/ml/core-requirements.txt\ncomm==0.2.0\n    # via\n    #   ipykernel\n    #   ipywidgets\nconfigobj==5.0.8\n    # via everett\nconfigspace==0.7.1 ; python_version < \"3.12\"\n    # via\n    #   -r python/requirements/ml/tune-requirements.txt\n    #   hpbandster\ncontextlib2==21.6.0\n    # via\n    #   ml-collections\n    #   pytest-shutil\ncontourpy==1.1.1\n    # via matplotlib\ncoverage==7.6.12\n    # via nbval\ncramjam==2.8.3\n    # via python-snappy\ncrc32c==2.3\n    # via -r python/requirements/ml/data-requirements.txt\ncrcmod==1.7\n    # via gsutil\ncryptography==42.0.5\n    # via\n    #   -r python/requirements/test-requirements.txt\n    #   adal\n    #   aim\n    #   azure-cli-core\n    #   azure-identity\n    #   moto\n    #   msal\n    #   paramiko\n    #   pyjwt\n    #   pyopenssl\n    #   python-jose\n    #   snowflake-connector-python\n    #   sshpubkeys\n    #   trustme\ncupy-cuda12x==13.1.0 ; sys_platform != \"darwin\"\n    # via\n    #   -r python/requirements.txt\n    #   -r python/requirements/ml/dl-cpu-requirements.txt\ncycler==0.12.1\n    # via matplotlib\ncython==0.29.37\n    # via\n    #   -r python/requirements/test-requirements.txt\n    #   gpy\ndask==2022.10.2 ; python_version < \"3.12\"\n    # via\n    #   -r python/requirements/ml/data-requirements.txt\n    #   distributed\ndatabricks-cli==0.18.0\n    # via mlflow\ndatasets==2.19.1\n    # via\n    #   -r python/requirements/ml/data-test-requirements.txt\n    #   -r python/requirements/ml/train-requirements.txt\n    #   evaluate\n    #   mosaicml\ndebugpy==1.8.0\n    # via ipykernel\ndecorator==5.1.1\n    # via\n    #   ipython\n    #   moviepy\n    #   paramz\n    #   tensorflow-probability\ndecord==0.6.0\n    # via -r python/requirements/ml/data-test-requirements.txt\ndeepspeed==0.12.3\n    # via -r python/requirements/ml/train-requirements.txt\ndefusedxml==0.7.1\n    # via\n    #   nbconvert\n    #   pymars\n    #   semgrep\ndelta-sharing==1.0.5\n    # via -r python/requirements/ml/data-test-requirements.txt\ndeltalake==0.18.2\n    # via -r python/requirements/ml/data-test-requirements.txt\ndill==0.3.7\n    # via\n    #   datasets\n    #   evaluate\n    #   multiprocess\ndistlib==0.3.7\n    # via virtualenv\ndistributed==2022.10.2 ; python_version < \"3.12\"\n    # via\n    #   -r python/requirements/ml/data-requirements.txt\n    #   dask\ndistro==1.9.0\n    # via azure-cli-core\ndm-control==1.0.12 ; python_version < \"3.12\"\n    # via -r python/requirements/ml/rllib-test-requirements.txt\ndm-env==1.6\n    # via dm-control\ndm-tree==0.1.8\n    # via\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Ray-based Time Series Forecasting\nDESCRIPTION: This snippet imports necessary libraries and modules for implementing the AutoML system, including Ray, pandas, numpy, StatsForecast, and scikit-learn components.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/automl_for_time_series.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Union, Callable, Dict, Type, Tuple\nimport time\nimport ray\nimport itertools\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import ETS, AutoARIMA, _TS\nfrom pyarrow import parquet as pq\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Collection of Python package specifications with their SHA256 hashes for package verification and version pinning. Includes comments indicating dependency relationships and source files.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_50\n\nLANGUAGE: plaintext\nCODE:\n```\nrsa==4.7.2 \\\n    --hash=sha256:78f9a9bf4e7be0c5ded4583326e7461e3a3c5aae24073648b4bdfa797d78c9d2 \\\n    --hash=sha256:9d689e6ca1b3038bc82bf8d23e944b6b6037bc02301a574935b2dd946e0353b9\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   google-auth\n    #   oauth2client\n```\n\n----------------------------------------\n\nTITLE: Ray Training Status Table Output\nDESCRIPTION: ASCII table showing detailed results from 16 training trials including trial names, status, scores, iterations, and runtimes. All trials completed successfully (TERMINATED status) with consistent iteration counts of 1440.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/tune_tests/scalability_tests/test_long_running_large_checkpoints.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n+--------------------------------+------------+-------+-----------+--------+------------------+---------+\n| Trial name                     | status     | loc   |     score |   iter |   total time (s) |   score |\n|--------------------------------+------------+-------+-----------+--------+------------------+---------|\n| function_trainable_6fcc8_00000 | TERMINATED |       | 0.579711  |   1440 |          88874.5 | 1439.58 |\n| function_trainable_6fcc8_00001 | TERMINATED |       | 0.0229354 |   1440 |          88099.5 | 1439.02 |\n| function_trainable_6fcc8_00002 | TERMINATED |       | 0.73754   |   1440 |          88080.4 | 1439.74 |\n| function_trainable_6fcc8_00003 | TERMINATED |       | 0.250762  |   1440 |          88978.9 | 1439.25 |\n| function_trainable_6fcc8_00004 | TERMINATED |       | 0.552658  |   1440 |          88755   | 1439.55 |\n| function_trainable_6fcc8_00005 | TERMINATED |       | 0.700542  |   1440 |          88247.4 | 1439.7  |\n| function_trainable_6fcc8_00006 | TERMINATED |       | 0.593315  |   1440 |          88936.5 | 1439.59 |\n| function_trainable_6fcc8_00007 | TERMINATED |       | 0.078047  |   1440 |          88642.1 | 1439.08 |\n| function_trainable_6fcc8_00008 | TERMINATED |       | 0.196008  |   1440 |          88830.8 | 1439.2  |\n| function_trainable_6fcc8_00009 | TERMINATED |       | 0.114978  |   1440 |          88515.2 | 1439.11 |\n| function_trainable_6fcc8_00010 | TERMINATED |       | 0.136566  |   1440 |          88532.2 | 1439.14 |\n| function_trainable_6fcc8_00011 | TERMINATED |       | 0.570846  |   1440 |          88125.4 | 1439.57 |\n| function_trainable_6fcc8_00012 | TERMINATED |       | 0.0276995 |   1440 |          88653.2 | 1439.03 |\n| function_trainable_6fcc8_00013 | TERMINATED |       | 0.283965  |   1440 |          88259.4 | 1439.28 |\n| function_trainable_6fcc8_00014 | TERMINATED |       | 0.198422  |   1440 |          89021.9 | 1439.2  |\n| function_trainable_6fcc8_00015 | TERMINATED |       | 0.0232406 |   1440 |          88861.6 | 1439.02 |\n+--------------------------------+------------+-------+-----------+--------+------------------+---------+\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Package dependency specifications with exact versions and SHA256 hashes for verification. Each entry includes the package name, version, and hash verification codes along with comments indicating the source requirement files.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nentrypoints==0.4 \\\n    --hash=sha256:b706eddaa9218a19ebcd67b56818f05bb27589b1ca9e8d797b74affad4ccacd4 \\\n    --hash=sha256:f174b5ff827504fd3cd97cc3f8649f3693f51538c7e4bdf3ef002c8429d42f9f\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jupyter-client\n    #   nbconvert\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry SDK Package with Hash Verification in Bash\nDESCRIPTION: Defines the OpenTelemetry SDK package version 1.1.0 with SHA256 hash verification for secure package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-sdk==1.1.0 \\\n    --hash=sha256:ba29274aab656572e97e0339afaad6f2bded4102324b1475ab7412079498df6e \\\n    --hash=sha256:da7dfa6188e8a39f34b99495260e6a1d398c86a9de064c7f0805db6f16733d94\n```\n\n----------------------------------------\n\nTITLE: Avoiding Overwriting Search Algorithm State\nDESCRIPTION: Demonstrates how to prevent overwriting search algorithm state by using unique experiment names. This pattern ensures that different tuning runs store their states in separate directories, allowing for proper restoration later.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/suggestion.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsearch_alg = HyperOptSearch()\ntuner_1 = tune.Tuner(\n    train_fn,\n    tune_config=tune.TuneConfig(\n        num_samples=5,\n        search_alg=search_alg,\n    ),\n    run_config=tune.RunConfig(\n        name=\"my-experiment-1\",\n        storage_path=\"~/my_results\",\n    )\n)\nresults = tuner_1.fit()\n\nsearch_alg2 = HyperOptSearch()\nsearch_alg2.restore_from_dir(\n  os.path.join(\"~/my_results\", \"my-experiment-1\")\n)\n```\n\n----------------------------------------\n\nTITLE: HTTP Requests on Remote Ray Cluster via curl\nDESCRIPTION: Uses curl to send HTTP PUT requests to a deployed Ray Serve application on a remote cluster. Requires the correct IP address for communication.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dev-workflow.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n\"curl -X PUT http://<head-node-ip-address>:8000/?name=Ray\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependency with Hash Values\nDESCRIPTION: This snippet defines the 'multidict' package dependency with its version and multiple SHA256 hash values for verification. It's part of a requirements file used for package management in Python projects.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_17\n\nLANGUAGE: Text\nCODE:\n```\nmultidict==6.0.5 \\\n    --hash=sha256:01265f5e40f5a17f8241d52656ed27192be03bfa8764d88e8220141d1e4b3556 \\\n    --hash=sha256:0275e35209c27a3f7951e1ce7aaf93ce0d163b28948444bec61dd7badc6d3f8c \\\n    --hash=sha256:04bde7a7b3de05732a4eb39c94574db1ec99abb56162d6c520ad26f83267de29 \\\n    --hash=sha256:04da1bb8c8dbadf2a18a452639771951c662c5ad03aefe4884775454be322c9b \\\n    --hash=sha256:09a892e4a9fb47331da06948690ae38eaa2426de97b4ccbfafbdcbe5c8f37ff8 \\\n    --hash=sha256:0d63c74e3d7ab26de115c49bffc92cc77ed23395303d496eae515d4204a625e7 \\\n    --hash=sha256:107c0cdefe028703fb5dafe640a409cb146d44a6ae201e55b35a4af8e95457dd \\\n    --hash=sha256:141b43360bfd3bdd75f15ed811850763555a251e38b2405967f8e25fb43f7d40 \\\n    --hash=sha256:14c2976aa9038c2629efa2c148022ed5eb4cb939e15ec7aace7ca932f48f9ba6 \\\n    --hash=sha256:19fe01cea168585ba0f678cad6f58133db2aa14eccaf22f88e4a6dccadfad8b3 \\\n    --hash=sha256:1d147090048129ce3c453f0292e7697d333db95e52616b3793922945804a433c \\\n    --hash=sha256:1d9ea7a7e779d7a3561aade7d596649fbecfa5c08a7674b11b423783217933f9 \\\n    --hash=sha256:215ed703caf15f578dca76ee6f6b21b7603791ae090fbf1ef9d865571039ade5 \\\n    --hash=sha256:21fd81c4ebdb4f214161be351eb5bcf385426bf023041da2fd9e60681f3cebae \\\n    --hash=sha256:220dd781e3f7af2c2c1053da9fa96d9cf3072ca58f057f4c5adaaa1cab8fc442 \\\n    --hash=sha256:228b644ae063c10e7f324ab1ab6b548bdf6f8b47f3ec234fef1093bc2735e5f9 \\\n    --hash=sha256:29bfeb0dff5cb5fdab2023a7a9947b3b4af63e9c47cae2a10ad58394b517fddc \\\n    --hash=sha256:2f4848aa3baa109e6ab81fe2006c77ed4d3cd1e0ac2c1fbddb7b1277c168788c \\\n    --hash=sha256:2faa5ae9376faba05f630d7e5e6be05be22913782b927b19d12b8145968a85ea \\\n    --hash=sha256:2ffc42c922dbfddb4a4c3b438eb056828719f07608af27d163191cb3e3aa6cc5 \\\n    --hash=sha256:37b15024f864916b4951adb95d3a80c9431299080341ab9544ed148091b53f50 \\\n    --hash=sha256:3cc2ad10255f903656017363cd59436f2111443a76f996584d1077e43ee51182 \\\n    --hash=sha256:3d25f19500588cbc47dc19081d78131c32637c25804df8414463ec908631e453 \\\n    --hash=sha256:403c0911cd5d5791605808b942c88a8155c2592e05332d2bf78f18697a5fa15e \\\n    --hash=sha256:411bf8515f3be9813d06004cac41ccf7d1cd46dfe233705933dd163b60e37600 \\\n    --hash=sha256:425bf820055005bfc8aa9a0b99ccb52cc2f4070153e34b701acc98d201693733 \\\n    --hash=sha256:435a0984199d81ca178b9ae2c26ec3d49692d20ee29bc4c11a2a8d4514c67eda \\\n    --hash=sha256:4a6a4f196f08c58c59e0b8ef8ec441d12aee4125a7d4f4fef000ccb22f8d7241 \\\n    --hash=sha256:4cc0ef8b962ac7a5e62b9e826bd0cd5040e7d401bc45a6835910ed699037a461 \\\n    --hash=sha256:51d035609b86722963404f711db441cf7134f1889107fb171a970c9701f92e1e \\\n    --hash=sha256:53689bb4e102200a4fafa9de9c7c3c212ab40a7ab2c8e474491914d2305f187e \\\n    --hash=sha256:55205d03e8a598cfc688c71ca8ea5f66447164efff8869517f175ea632c7cb7b \\\n    --hash=sha256:5c0631926c4f58e9a5ccce555ad7747d9a9f8b10619621f22f9635f069f6233e \\\n    --hash=sha256:5cb241881eefd96b46f89b1a056187ea8e9ba14ab88ba632e68d7a2ecb7aadf7 \\\n    --hash=sha256:60d698e8179a42ec85172d12f50b1668254628425a6bd611aba022257cac1386 \\\n    --hash=sha256:612d1156111ae11d14afaf3a0669ebf6c170dbb735e510a7438ffe2369a847fd \\\n    --hash=sha256:6214c5a5571802c33f80e6c84713b2c79e024995b9c5897f794b43e714daeec9 \\\n    --hash=sha256:6939c95381e003f54cd4c5516740faba40cf5ad3eeff460c3ad1d3e0ea2549bf \\\n    --hash=sha256:69db76c09796b313331bb7048229e3bee7928eb62bab5e071e9f7fcc4879caee \\\n    --hash=sha256:6bf7a982604375a8d49b6cc1b781c1747f243d91b81035a9b43a2126c04766f5 \\\n    --hash=sha256:766c8f7511df26d9f11cd3a8be623e59cca73d44643abab3f8c8c07620524e4a \\\n    --hash=sha256:76c0de87358b192de7ea9649beb392f107dcad9ad27276324c24c91774ca5271 \\\n    --hash=sha256:76f067f5121dcecf0d63a67f29080b26c43c71a98b10c701b0677e4a065fbd54 \\\n    --hash=sha256:7901c05ead4b3fb75113fb1dd33eb1253c6d3ee37ce93305acd9d38e0b5f21a4 \\\n    --hash=sha256:79660376075cfd4b2c80f295528aa6beb2058fd289f4c9252f986751a4cd0496 \\\n    --hash=sha256:79a6d2ba910adb2cbafc95dad936f8b9386e77c84c35bc0add315b856d7c3abb \\\n    --hash=sha256:7afcdd1fc07befad18ec4523a782cde4e93e0a2bf71239894b8d61ee578c1319 \\\n    --hash=sha256:7be7047bd08accdb7487737631d25735c9a04327911de89ff1b26b81745bd4e3 \\\n    --hash=sha256:7c6390cf87ff6234643428991b7359b5f59cc15155695deb4eda5c777d2b880f \\\n    --hash=sha256:7df704ca8cf4a073334e0427ae2345323613e4df18cc224f647f251e5e75a527 \\\n    --hash=sha256:85f67aed7bb647f93e7520633d8f51d3cbc6ab96957c71272b286b2f30dc70ed \\\n    --hash=sha256:896ebdcf62683551312c30e20614305f53125750803b614e9e6ce74a96232604 \\\n    --hash=sha256:92d16a3e275e38293623ebf639c471d3e03bb20b8ebb845237e0d3664914caef \\\n    --hash=sha256:99f60d34c048c5c2fabc766108c103612344c46e35d4ed9ae0673d33c8fb26e8 \\\n    --hash=sha256:9fe7b0653ba3d9d65cbe7698cca585bf0f8c83dbbcc710db9c90f478e175f2d5 \\\n    --hash=sha256:a3145cb08d8625b2d3fee1b2d596a8766352979c9bffe5d7833e0503d0f0b5e5 \\\n    --hash=sha256:aeaf541ddbad8311a87dd695ed9642401131ea39ad7bc8cf3ef3967fd093b626 \\\n    --hash=sha256:b55358304d7a73d7bdf5de62494aaf70bd33015831ffd98bc498b433dfe5b10c \\\n    --hash=sha256:b82cc8ace10ab5bd93235dfaab2021c70637005e1ac787031f4d1da63d493c1d \\\n    --hash=sha256:c0868d64af83169e4d4152ec612637a543f7a336e4a307b119e98042e852ad9c \\\n    --hash=sha256:c1c1496e73051918fcd4f58ff2e0f2f3066d1c76a0c6aeffd9b45d53243702cc \\\n    --hash=sha256:c9bf56195c6bbd293340ea82eafd0071cb3d450c703d2c93afb89f93b8386ccc \\\n```\n\n----------------------------------------\n\nTITLE: Multiple RayDaskCallbacks in Python\nDESCRIPTION: This snippet demonstrates using multiple callback instances together. This allows combining different callback functionalities to achieve more complex monitoring and control over task execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/dask-on-ray.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Example showing how to pass multiple callbacks.\"\"\"\n\nfrom ray.util.dask import RayDaskCallback\n\n\ndef my_presubmit(dsk, state):\n    print(\"Presubmit one\")\n\n\ndef my_postsubmit(dsk, state, id):\n    print(\"Postsubmit one\")\n\n\ncallback_one = RayDaskCallback(\n    _ray_presubmit=my_presubmit, _ray_postsubmit=my_postsubmit\n)\n\n\ndef my_pretask(dsk, state, task):\n    print(\"Pretask two\")\n\n\ndef my_posttask(dsk, state, task, result):\n    print(\"Posttask two\")\n\n\ncallback_two = RayDaskCallback(\n    _ray_pretask=my_pretask, _ray_posttask=my_posttask\n)\n\ncallbacks = [callback_one, callback_two]\n\n```\n\n----------------------------------------\n\nTITLE: Listing Dependency Hashes for Ray Project\nDESCRIPTION: This snippet contains a long list of SHA256 hashes for various dependencies used in the Ray project. Each line represents a specific version of a package and its corresponding hash. This is typically used for ensuring package integrity and reproducibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_6\n\nLANGUAGE: Text\nCODE:\n```\n--hash=sha256:084659fac3c83fd674596612aeff6041a18402f1e1bc19ca39e417d554468482 \\\n--hash=sha256:10d4204d8ca33146e761c79f83cc861df20e7ae9f6487ca290a97702daf56006 \\\n--hash=sha256:11b71d67b4725e7e2a9f6e9c0ac1239bbc0c48cce3dc59f98635efc57d6dac83 \\\n--hash=sha256:150906b40ff188a3260cbee25380e7494ee85048584998c1e66df0c7a11c17a6 \\\n--hash=sha256:175873691124f3d0da55aeea1d90660a6ea7a3cfea137c38afa0a5ffabe37b88 \\\n--hash=sha256:177f55a886d74f1808763976ac4efd29b7ed15c69f4d838bbd74d9d09cf6fa86 \\\n--hash=sha256:19c0fa39fa154e7e0b7f82f88ef85faa2a4c23cc65aae2f5aea625e3c13c735a \\\n--hash=sha256:1eedfeb6089ed3fad42e81a67755846ad4dcc14d73698c120a82e4ccf0f1f9f6 \\\n--hash=sha256:225b67a1f6d602de0ce7f6c1c3ae89a4aa25d3de9be857999e9124f15dab486a \\\n--hash=sha256:242b8feb3c493ab78be289c034a1f659e8826e2233786e36f2893a950a719bb6 \\\n--hash=sha256:254ec27fdb5b1ee60684f91683be95e5133c994cc54e86a0b0963afa25c8f8a6 \\\n--hash=sha256:25e9185e2d06c16ee438ed39bf62935ec436474a6ac4f9358524220f1b236e43 \\\n--hash=sha256:26ab812fa0c845df815e506be30337e2df27e88399b985d0bb4e3ecfe72df31c \\\n--hash=sha256:26ca695eeee5f9f1aeeb211ffc12f10bcb6f71e2989988fda61dabd65db878d4 \\\n--hash=sha256:26dc97754b57d2fd00ac2b24dfa341abffc380b823211994c4efac7f13b9e90e \\\n--hash=sha256:270755f15174fb983890c49881e93f8f1b80f0b5e3a3cc1394a255706cabd203 \\\n--hash=sha256:2aafc5a503855ea5885559eae883978c9b6d8c8993d67766ee73d82e841300dd \\\n--hash=sha256:2d036c7187b9422ae5b262badb87a20a49eb6c5238b2004e96d4da1231badef1 \\\n--hash=sha256:33499e85e739a4b60c9dac710c20a08dc73cb3240c9a0e22325e671b27b70d24 \\\n--hash=sha256:37eee5b638f0e0dcd18d21f59b679686bbd18917b87db0193ae36f9c23c355fc \\\n--hash=sha256:38cf1c40a921d05c5edc61a785c0ddb4bed67827069f535d794ce6bcded919fc \\\n--hash=sha256:3acae97ffd19bf091c72df4d726d552c473f3576409b2a7ca36b2f535ffff4a3 \\\n--hash=sha256:3c5ebac750d9d5f2706654c638c041635c385596caf68f81342011ddfa1e5598 \\\n--hash=sha256:3d482efec8b7dc6bfaedc0f166b2ce349df0011f5d2f1f25537ced4cfc34fd98 \\\n--hash=sha256:407653af5617f0757261ae249d3fba09504d7a71ab36ac057c938572d1bc9331 \\\n--hash=sha256:40a783fb7ee353c50bd3853e626f15677ea527ae556429453685ae32280c19c2 \\\n--hash=sha256:41e81317dd6a0127cabce83c0c9c3fbecceae981c8391e6f1dec88a77c8a569a \\\n--hash=sha256:41f4c96227a67a013e7de5ff8f20fb496ce573893b7f4f2707d065907bffdbd6 \\\n--hash=sha256:469f29f9093c9d834432034d33f5fe45699e664f12a13bf38c04967ce233d688 \\\n--hash=sha256:4745f4ac52cc6686390c40eaa01d48b18997cb130833154801a442323cc78f91 \\\n--hash=sha256:4868f6bd7c9d98904b748a2653031fc9c2f85b6237009d475b1008bfaeb0a5aa \\\n--hash=sha256:4aa223cd1e36b642092c326d694d8bf59b71ddddc94cdb752bbbb1c5c91d833b \\\n--hash=sha256:4dd484681c15e6b9a977c785a345d3e378d72678fd5f1f3c0509608da24f2ac0 \\\n--hash=sha256:4f2790949cf385d985a31984907fecb3896999329103df4e4983a4a41e13e840 \\\n--hash=sha256:512ecfbefef6dac7bc5eaaf46177b2de58cdf7acac8793fe033b24ece0b9566c \\\n--hash=sha256:516d9227919612425c8ef1c9b869bbbee249bc91912c8aaffb66116c0b447ebd \\\n--hash=sha256:53e431da3fc53360db73eedf6f7124d1076e1b4ee4276b36fb25514544ceb4a3 \\\n--hash=sha256:595ba5be69b35777474fa07f80fc260ea71255656191adb22a8c53aba4479231 \\\n--hash=sha256:5b5ff4911aea936a47d9376fd3ab17e970cc543d1b68921886e7f64bd28308d1 \\\n--hash=sha256:5d41e6daee2813ecceea8eda38062d69e280b39df793f5a942fa515b8ed67953 \\\n--hash=sha256:5e999ba8dd90e93d57410c5e67ebb67ffcaadcea0ad973240fdfd3a135506250 \\\n--hash=sha256:5f239eb799a2081495ea659d8d4a43a8f42cd1fe9ff2e7e436295c38a10c286a \\\n--hash=sha256:635fee4e041ab9c479e31edda27fcf966ea9614fff1317e280d99eb3e5ab6fe2 \\\n--hash=sha256:65db0f2eefcaad1a3950f498aabb4875c8890438bc80b19362cf633b87a8ab20 \\\n--hash=sha256:6b507132dcfc0dea440cce23ee2182c0ce7aba7054576efc65634f080dbe9434 \\\n--hash=sha256:6b9d9bb600328a1ce523ab4f454859e9d439150abb0906c5a1983c146580ebab \\\n--hash=sha256:70c8daf4faca8da5a6d655f9af86faf6ec2e1768f4b8b9d0226c02f3d6209703 \\\n--hash=sha256:77bf3ac639c1ff567ae3b47f8d4cc3dc20f9966a2a6dd2311dcc055d3d04fb8a \\\n--hash=sha256:784c1214cb6dd1e3b15dd8b91b9a53852aed16671cc3fbe4786f4f1db07089e2 \\\n--hash=sha256:7eb6a0587eded33aeefea9f916899d42b1799b7b14b8f8ff2753c0ac1741edac \\\n--hash=sha256:7ed1b0132f24beeec5a78b67d9388656d03e6a7c837394f99257e2d55b461611 \\\n--hash=sha256:8ad4aeb3e9a97286573c03df758fc7627aecdd02f1da04516a86dc159bf70121 \\\n--hash=sha256:964faa8a861d2664f0c7ab0c181af0bea66098b1919439815ca8803ef136fc4e \\\n--hash=sha256:9dc1b507c12eb0481d071f3c1808f0529ad41dc415d0ca11f7ebfc666e66a18b \\\n--hash=sha256:9ebfef07dbe1d93efb94b4700f2d278494e9162565a54f124c404a5656d7ff09 \\\n--hash=sha256:a45f84b09ac9c3d35dfcf6a27fd0634d30d183205230a0ebe8373a0e8cfa0906 \\\n--hash=sha256:a4f55095ad087474999ee28d3398bae183a66be4823f753cd7d67dd0153427c9 \\\n--hash=sha256:a6d511cc297ff0883bc3708b465ff82d7560193169a8b93260f74ecb0a5e08a7 \\\n--hash=sha256:a8ad4c766d3f33ba8fd692f9aa297c9058970530a32c728a2c4bfd2616d3358b \\\n--hash=sha256:aa2f457b4af386254372dfa78a2eda2563680d982422641a85f271c859df1987 \\\n--hash=sha256:b03f7941783b4c4a26051846dea594628b38f6940a2fdc0df00b221aed39314c \\\n--hash=sha256:b0dae11d8f5ded51699c74d9548dcc5938e0804cc8298ec0aa0da95c21fff57b \\\n--hash=sha256:b91ced227c41aa29c672814f50dbb05ec93536abf8f43cd14ec9521ea09afe4e \\\n--hash=sha256:bc633a9fe1eb87e250b5c57d389cf28998e4292336926b0b6cdaee353f89a237 \\\n--hash=sha256:bebb4d6715c814597f85297c332297c6ce81e29436125ca59d1159b07f423eb1 \\\n--hash=sha256:c336a6d235522a62fef872c6295a42ecb0c4e1d0f1a3e500fe949415761b8a19 \\\n--hash=sha256:c6514f963b023aeee506678a1cf821fe31159b925c4b76fe2afa94cc70b3222b \\\n--hash=sha256:c693e916709c2465b02ca0ad7b387c4f8423d1db7b4649c551f27a529181c5ad \\\n--hash=sha256:c81131869240e3e568916ef4c307f8b99583efaa60a8112ef27a366eefba8ef0 \\\n--hash=sha256:d02a72df14dfdbaf228424573a07af10637bd490f0901cee872c4f434a735b94 \\\n--hash=sha256:d2a8fa9d6d6f891f3deec72f5cc668e6f66b188ab14bb1ab52422fe8e644f312 \\\n--hash=sha256:d2b27e6af28f07e2f195552b37d7d66b150adbaa39a6d327766ffd695799780f \\\n--hash=sha256:d2fe69c5434391727efa54b47a1e7986bb0186e72a41b203df8f5b0a19a4f669 \\\n--hash=sha256:d3f3ed29cd9f978c604708511a1f9c2fdcb6c38b9aae36a51905b8811ee5cbf1 \\\n--hash=sha256:d573faf8eb7e6b1cbbcb4f5b247c60ca8be39fe2c674495df0eb4318303137fe \\\n--hash=sha256:e0bbdd76ce9aa5d4209d65f2b27fc6e5ef1312ae6c5333c26db3f5ade53a1e99 \\\n--hash=sha256:e7c4ea22b6739b162c9ecaaa41d718dfad48a244909fe7ef4b54c0b530effc5a \\\n--hash=sha256:e93e1a4b4b33daed65d781a57a522ff153dcf748dee70b40c7258c5861e1768a \\\n--hash=sha256:e97fdf088d4b31ff4ba35db26d9cc472ac7ef4a2ff2badeabf8d727b3377fc52 \\\n--hash=sha256:e9fa4c9bf273ca41f940bceb86922a7667cd5bf90e95dbb157cbb8441008482c \\\n--hash=sha256:eaad4ff2de1c3823fddf82f41121bdf453d922e9a238642b1dedb33c4e4f98ad \\\n--hash=sha256:f1f62b2413c3a0e846c3b838b2ecd6c7a19ec6793b2a522745b0869e37ab5bc1 \\\n--hash=sha256:f6d6cff3538391e8486a431569b77921adfcdef14eb18fbf19b7c0a5294d4e6a \\\n--hash=sha256:f9aa05d09ecf4c75157197f27cdc9cfaeb7c5f15021c6373932bf3e124af029f \\\n--hash=sha256:fa2fddcb7107e0d1808086ca306dcade7df60a13a6c347a7acf1ec139aa6789a \\\n--hash=sha256:faa6b09ee09433b87992fb5a2859efd1c264ddc37280d2dd5db502126d0e7f27\n```\n\n----------------------------------------\n\nTITLE: Specifying Parso Library Dependency\nDESCRIPTION: This code defines the Parso library dependency with hash verification. It's used by the Jedi autocompletion library, which likely supports development tools in the Ray project, and is referenced from the requirements_compiled.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nparso==0.8.3 \\\n    --hash=sha256:8c07be290bb59f03588915921e29e8a50002acaf2cdc5fa0e0114f91709fafa0 \\\n    --hash=sha256:c001d4636cd3aecdaf33cbb40aebb59b094be2a74c556778ef5576c175e19e75\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jedi\n```\n\n----------------------------------------\n\nTITLE: Comparing Pandas DataFrame and Ray Data APIs\nDESCRIPTION: This code snippet presents a table comparing common Pandas DataFrame operations with their Ray Data equivalents. It covers operations like showing data, getting schema, counting rows, and various data transformations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/from_other_data_libs.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf.head()                 # Pandas\nds.show()                 # Ray Data\n\ndf.dtypes                 # Pandas\nds.schema()               # Ray Data\n\nlen(df) or df.shape[0]    # Pandas\nds.count()                # Ray Data\n\ndf.truncate()             # Pandas\nds.limit()                # Ray Data\n\ndf.iterrows()             # Pandas\nds.iter_rows()            # Ray Data\n\ndf.drop()                 # Pandas\nds.drop_columns()         # Ray Data\n\ndf.transform()            # Pandas\nds.map_batches() or ds.map() # Ray Data\n\ndf.groupby()              # Pandas\nds.groupby()              # Ray Data\n\ndf.groupby().apply()      # Pandas\nds.groupby().map_groups() # Ray Data\n\ndf.sample()               # Pandas\nds.random_sample()        # Ray Data\n\ndf.sort_values()          # Pandas\nds.sort()                 # Ray Data\n\ndf.append()               # Pandas\nds.union()                # Ray Data\n\ndf.aggregate()            # Pandas\nds.aggregate()            # Ray Data\n\ndf.min()                  # Pandas\nds.min()                  # Ray Data\n\ndf.max()                  # Pandas\nds.max()                  # Ray Data\n\ndf.sum()                  # Pandas\nds.sum()                  # Ray Data\n\ndf.mean()                 # Pandas\nds.mean()                 # Ray Data\n\ndf.std()                  # Pandas\nds.std()                  # Ray Data\n```\n\n----------------------------------------\n\nTITLE: Listing Regex Python Package with Comprehensive SHA256 Hashes\nDESCRIPTION: This snippet shows the regex package version 2024.11.6 with its extensive list of SHA256 hash values for verification. This format ensures package integrity by providing multiple hash checksums for different distributions of the same package version.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_45\n\nLANGUAGE: plaintext\nCODE:\n```\nregex==2024.11.6 \\\n    --hash=sha256:02a02d2bb04fec86ad61f3ea7f49c015a0681bf76abb9857f945d26159d2968c \\\n    --hash=sha256:02e28184be537f0e75c1f9b2f8847dc51e08e6e171c6bde130b2687e0c33cf60 \\\n    --hash=sha256:040df6fe1a5504eb0f04f048e6d09cd7c7110fef851d7c567a6b6e09942feb7d \\\n    --hash=sha256:068376da5a7e4da51968ce4c122a7cd31afaaec4fccc7856c92f63876e57b51d \\\n    --hash=sha256:06eb1be98df10e81ebaded73fcd51989dcf534e3c753466e4b60c4697a003b67 \\\n    --hash=sha256:072623554418a9911446278f16ecb398fb3b540147a7828c06e2011fa531e773 \\\n    --hash=sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0 \\\n    --hash=sha256:08986dce1339bc932923e7d1232ce9881499a0e02925f7402fb7c982515419ef \\\n    --hash=sha256:0a86e7eeca091c09e021db8eb72d54751e527fa47b8d5787caf96d9831bd02ad \\\n    --hash=sha256:0c32f75920cf99fe6b6c539c399a4a128452eaf1af27f39bce8909c9a3fd8cbe \\\n    --hash=sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3 \\\n    --hash=sha256:1062b39a0a2b75a9c694f7a08e7183a80c63c0d62b301418ffd9c35f55aaa114 \\\n    --hash=sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4 \\\n    --hash=sha256:149f5008d286636e48cd0b1dd65018548944e495b0265b45e1bffecce1ef7f39 \\\n    --hash=sha256:164d8b7b3b4bcb2068b97428060b2a53be050085ef94eca7f240e7947f1b080e \\\n    --hash=sha256:167ed4852351d8a750da48712c3930b031f6efdaa0f22fa1933716bfcd6bf4a3 \\\n    --hash=sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7 \\\n    --hash=sha256:202eb32e89f60fc147a41e55cb086db2a3f8cb82f9a9a88440dcfc5d37faae8d \\\n    --hash=sha256:220902c3c5cc6af55d4fe19ead504de80eb91f786dc102fbd74894b1551f095e \\\n    --hash=sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a \\\n    --hash=sha256:2c89a8cc122b25ce6945f0423dc1352cb9593c68abd19223eebbd4e56612c5b7 \\\n    --hash=sha256:2d548dafee61f06ebdb584080621f3e0c23fff312f0de1afc776e2a2ba99a74f \\\n    --hash=sha256:2e34b51b650b23ed3354b5a07aab37034d9f923db2a40519139af34f485f77d0 \\\n    --hash=sha256:32f9a4c643baad4efa81d549c2aadefaeba12249b2adc5af541759237eee1c54 \\\n    --hash=sha256:3a51ccc315653ba012774efca4f23d1d2a8a8f278a6072e29c7147eee7da446b \\\n    --hash=sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c \\\n    --hash=sha256:40291b1b89ca6ad8d3f2b82782cc33807f1406cf68c8d440861da6304d8ffbbd \\\n    --hash=sha256:41758407fc32d5c3c5de163888068cfee69cb4c2be844e7ac517a52770f9af57 \\\n    --hash=sha256:4181b814e56078e9b00427ca358ec44333765f5ca1b45597ec7446d3a1ef6e34 \\\n    --hash=sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d \\\n    --hash=sha256:50153825ee016b91549962f970d6a4442fa106832e14c918acd1c8e479916c4f \\\n    --hash=sha256:5056b185ca113c88e18223183aa1a50e66507769c9640a6ff75859619d73957b \\\n    --hash=sha256:5071b2093e793357c9d8b2929dfc13ac5f0a6c650559503bb81189d0a3814519 \\\n    --hash=sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4 \\\n    --hash=sha256:52fb28f528778f184f870b7cf8f225f5eef0a8f6e3778529bdd40c7b3920796a \\\n    --hash=sha256:5478c6962ad548b54a591778e93cd7c456a7a29f8eca9c49e4f9a806dcc5d638 \\\n    --hash=sha256:5670bce7b200273eee1840ef307bfa07cda90b38ae56e9a6ebcc9f50da9c469b \\\n    --hash=sha256:5704e174f8ccab2026bd2f1ab6c510345ae8eac818b613d7d73e785f1310f839 \\\n    --hash=sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07 \\\n    --hash=sha256:5e7e351589da0850c125f1600a4c4ba3c722efefe16b297de54300f08d734fbf \\\n    --hash=sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff \\\n    --hash=sha256:658f90550f38270639e83ce492f27d2c8d2cd63805c65a13a14d36ca126753f0 \\\n    --hash=sha256:684d7a212682996d21ca12ef3c17353c021fe9de6049e19ac8481ec35574a70f \\\n    --hash=sha256:69ab78f848845569401469da20df3e081e6b5a11cb086de3eed1d48f5ed57c95 \\\n    --hash=sha256:6f44ec28b1f858c98d3036ad5d7d0bfc568bdd7a74f9c24e25f41ef1ebfd81a4 \\\n    --hash=sha256:70b7fa6606c2881c1db9479b0eaa11ed5dfa11c8d60a474ff0e095099f39d98e \\\n    --hash=sha256:764e71f22ab3b305e7f4c21f1a97e1526a25ebdd22513e251cf376760213da13 \\\n    --hash=sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519 \\\n    --hash=sha256:805e6b60c54bf766b251e94526ebad60b7de0c70f70a4e6210ee2891acb70bf2 \\\n    --hash=sha256:8447d2d39b5abe381419319f942de20b7ecd60ce86f16a23b0698f22e1b70008 \\\n    --hash=sha256:86fddba590aad9208e2fa8b43b4c098bb0ec74f15718bb6a704e3c63e2cef3e9 \\\n    --hash=sha256:89d75e7293d2b3e674db7d4d9b1bee7f8f3d1609428e293771d1a962617150cc \\\n    --hash=sha256:93c0b12d3d3bc25af4ebbf38f9ee780a487e8bf6954c115b9f015822d3bb8e48 \\\n    --hash=sha256:94d87b689cdd831934fa3ce16cc15cd65748e6d689f5d2b8f4f4df2065c9fa20 \\\n    --hash=sha256:9714398225f299aa85267fd222f7142fcb5c769e73d7733344efc46f2ef5cf89 \\\n    --hash=sha256:982e6d21414e78e1f51cf595d7f321dcd14de1f2881c5dc6a6e23bbbbd68435e \\\n    --hash=sha256:997d6a487ff00807ba810e0f8332c18b4eb8d29463cfb7c820dc4b6e7562d0cf \\\n    --hash=sha256:a03e02f48cd1abbd9f3b7e3586d97c8f7a9721c436f51a5245b3b9483044480b \\\n    --hash=sha256:a36fdf2af13c2b14738f6e973aba563623cb77d753bbbd8d414d18bfaa3105dd \\\n    --hash=sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84 \\\n    --hash=sha256:a7c2155f790e2fb448faed6dd241386719802296ec588a8b9051c1f5c481bc29 \\\n    --hash=sha256:a93c194e2df18f7d264092dc8539b8ffb86b45b899ab976aa15d48214138e81b \\\n    --hash=sha256:abfa5080c374a76a251ba60683242bc17eeb2c9818d0d30117b4486be10c59d3 \\\n    --hash=sha256:ac10f2c4184420d881a3475fb2c6f4d95d53a8d50209a2500723d831036f7c45 \\\n    --hash=sha256:ad182d02e40de7459b73155deb8996bbd8e96852267879396fb274e8700190e3 \\\n    --hash=sha256:b2837718570f95dd41675328e111345f9b7095d821bac435aac173ac80b19983 \\\n    --hash=sha256:b489578720afb782f6ccf2840920f3a32e31ba28a4b162e13900c3e6bd3f930e \\\n    --hash=sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7 \\\n    --hash=sha256:b85c2530be953a890eaffde05485238f07029600e8f098cdf1848d414a8b45e4 \\\n    --hash=sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e \\\n    --hash=sha256:ba9b72e5643641b7d41fa1f6d5abda2c9a263ae835b917348fc3c928182ad467 \\\n    --hash=sha256:bb26437975da7dc36b7efad18aa9dd4ea569d2357ae6b783bf1118dabd9ea577 \\\n    --hash=sha256:bb8f74f2f10dbf13a0be8de623ba4f9491faf58c24064f32b65679b021ed0001 \\\n    --hash=sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0 \\\n    --hash=sha256:bec9931dfb61ddd8ef2ebc05646293812cb6b16b60cf7c9511a832b6f1854b55 \\\n    --hash=sha256:c36f9b6f5f8649bb251a5f3f66564438977b7ef8386a52460ae77e6070d309d9 \\\n    --hash=sha256:cdf58d0e516ee426a48f7b2c03a332a4114420716d55769ff7108c37a09951bf \\\n    --hash=sha256:d1cee317bfc014c2419a76bcc87f071405e3966da434e03e13beb45f8aced1a6 \\\n    --hash=sha256:d22326fcdef5e08c154280b71163ced384b428343ae16a5ab2b3354aed12436e \\\n    --hash=sha256:d3660c82f209655a06b587d55e723f0b813d3a7db2e32e5e7dc64ac2a9e86fde \\\n    --hash=sha256:da8f5fc57d1933de22a9e23eec290a0d8a5927a5370d24bda9a6abe50683fe62 \\\n    --hash=sha256:df951c5f4a1b1910f1a99ff42c473ff60f8225baa1cdd3539fe2819d9543e9df \\\n    --hash=sha256:e5364a4502efca094731680e80009632ad6624084aff9a23ce8c8c6820de3e51 \\\n    --hash=sha256:ea1bfda7162605f6e8178223576856b3d791109f15ea99a9f95c16a7636fb5 \\\n    --hash=sha256:f02f93b92358ee3f78660e43b4b0091229260c5d5c408d17d60bf26b6c900e86\n```\n\n----------------------------------------\n\nTITLE: Robots.txt Configuration\nDESCRIPTION: This robots.txt configuration allows crawling of the 'latest' and 'master' versions of the Ray documentation. It uses wildcards and explicit paths to specify allowed URLs while disallowing access to other parts of the site.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/robots.txt#2025-04-12_snippet_0\n\nLANGUAGE: robots.txt\nCODE:\n```\n\"# Algolia-Crawler-Verif: C37A60AFB138256B\nUser-agent: *\nDisallow: /\nAllow: /*/latest/\nAllow: /en/latest/   # Fallback for bots that don't understand wildcards\nAllow: /*/master/\nAllow: /en/master/   # Fallback for bots that don't understand wildcards\nSitemap: https://docs.ray.io/en/latest/sitemap.xml\"\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Detailed package requirements specification with exact versions and SHA256 hash verification. Includes dependencies like crc32c, crcmod, cryptography and others with their corresponding verified hashes.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\ncrc32c==2.3 \\\n    --hash=sha256:0369e637d13db5c06e45a34b069ff2ba292ac881e8a44a8658ccf3edaa9c392f \\\n    --hash=sha256:0c1f3e28b8aec8a0f7727337fafa31f0ace38e59e054c51fecb923535c6dc6e6\n```\n\n----------------------------------------\n\nTITLE: Defining OpenTelemetry Semantic Conventions Dependency\nDESCRIPTION: This code specifies the OpenTelemetry semantic conventions package with hash verification. It's used by the OpenTelemetry SDK component and is referenced from the requirements_compiled.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-semantic-conventions==0.20b0 \\\n    --hash=sha256:ecae7367203e5204c70518e6d24b438480d6a6f1e5c8ee9dc2145f176ff4452e \\\n    --hash=sha256:fac014ac2098b1a05fe58af77cbe74c825ff869d6d53d316c393cc77f507ec15\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Package Hash Values - Initial Dependencies\nDESCRIPTION: SHA-256 hash values for package verification, likely related to vllm dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:15c1e86fff77184c20a2932cd9742bf33fe23125fa3fcf332df9ad2f7d483044 \\\n--hash=sha256:19746b50be214a54239aab822964f2ac81e38b0055cca94808359d779338c10e \\\n--hash=sha256:2719647625320b60e2d8af06b35f5b12d4f4d281db30a15a1df22adb2295f633\n```\n\n----------------------------------------\n\nTITLE: Configuring Serve Controller Request Processing Time\nDESCRIPTION: Illustrates the configuration of the Serve Controller's request processing interval to manage overload by setting the environment variable `RAY_SERVE_CONTROL_LOOP_INTERVAL_S`. This adjustment helps give the Controller more time to process requests, especially under high CPU conditions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/performance.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport RAY_SERVE_CONTROL_LOOP_INTERVAL_S=0.5\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: This snippet shows how Python package dependencies are specified with exact versions and hash values for verification. It includes packages like gsutil, gymnasium, h5py, and others.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_9\n\nLANGUAGE: Text\nCODE:\n```\ngsutil==5.27 \\\n    --hash=sha256:681a2d844acdf05fac989da6dd406944ae11cb27a4cf3c9edef74d2585ab5f05\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\ngymnasium[atari]==1.0.0 \\\n    --hash=sha256:9d2b66f30c1b34fe3c2ce7fae65ecf365d0e9982d2b3d860235e773328a3b403 \\\n    --hash=sha256:b6f40e1e24c5bd419361e1a5b86a9117d2499baecc3a660d44dfff4c465393ad\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\nh5py==3.10.0 \\\n    --hash=sha256:012ab448590e3c4f5a8dd0f3533255bc57f80629bf7c5054cf4c87b30085063c \\\n    --hash=sha256:212bb997a91e6a895ce5e2f365ba764debeaef5d2dca5c6fb7098d66607adf99 \\\n    --hash=sha256:2381e98af081b6df7f6db300cd88f88e740649d77736e4b53db522d8874bf2dc\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Verification\nDESCRIPTION: This snippet shows how to specify Python package dependencies in a requirements file, including version numbers and SHA256 hash values for verification. It demonstrates the format for multiple packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\ndocutils==0.19 \\\n    --hash=sha256:33995a6753c30b7f577febfc2c50411fec6aac7f7ffeb7c4cfe5991072dcf9e6 \\\n    --hash=sha256:5e1de4d849fee02c63b040a4a3fd567f4ab104defd8a5511fbbc24a8a017efbc\n    # via sphinx\neinops==0.8.1 \\\n    --hash=sha256:919387eb55330f5757c6bea9165c5ff5cfe63a642682ea788a6d472576d81737 \\\n    --hash=sha256:de5d960a7a761225532e0f1959e5315ebeafc0cd43394732f103ca44b9837e84\n    # via vllm\nemail-validator==2.2.0 \\\n    --hash=sha256:561977c2d73ce3611850a06fa56b414621e0c8faa9d66f2611407d87465da631 \\\n    --hash=sha256:cb690f344c617a714f22e66ae771445a1ceb46821152df8e165c5f9a364582b7\n    # via fastapi\n```\n\n----------------------------------------\n\nTITLE: SHA256 Hash Verification for Package Dependencies\nDESCRIPTION: Collection of SHA256 hash values used to verify the integrity of Python package dependencies. This includes hashes for multidict version 6.0.5 and networkx version 3.2.1 packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:a40821a89dc373d6427e2b44b572efc36a2778d3f543299e2f24eb1a5de65415 \\\n--hash=sha256:b291f0ee7961a597cbbcc77709374087fa2a9afe7bdb6a40dbbd9b127e79afee \\\n--hash=sha256:b573a43ef7c368ba4ea06050a957c2a7550f729c31f11dd616d2ac4aba99888d\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with Hash Verification\nDESCRIPTION: Package dependency specifications that include version pins and SHA256 hash verification to ensure package integrity. Each package entry includes its source requirements files and dependency relationships through comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:fb70487c95786e345af5e854ffec8cb8cc781bcc5df7930c4fbb7feaa72e1cdf \\\n--hash=sha256:fe96281713168a3270878255983d2cb1a97e034325c8c2c25169a69289d3ecfa \\\n--hash=sha256:ff1f7882e56c40b0d33c4922c15dfa30612f05fb785074a012f7cda74d1c3679\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Documentation for Classes using Jinja2 Template\nDESCRIPTION: This template creates Sphinx documentation for a class, including its methods and attributes. It uses autosummary to generate summaries and avoids issues with inherited instance attributes. The template includes sections for methods and attributes, and filters out undocumented class members.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/autosummary/class.rst#2025-04-12_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{#\n  It's a known bug (https://github.com/sphinx-doc/sphinx/issues/9884)\n  that autosummary will generate warning for inherited instance attributes.\n  Those warnings will fail our build.\n  For now, we don't autosummary classes with inherited instance attributes.\n  To opt out, use `:template: autosummary/class_without_autosummary.rst`\n#}\n\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n   :show-inheritance:\n\n   {% block methods %}\n   {% if methods %}\n   .. rubric:: {{ _('Methods') }}\n\n   .. autosummary::\n      :nosignatures:\n      :toctree:\n\n   {% for item in methods %}\n      {{ item | filter_out_undoc_class_members(name, module) }}\n   {%- endfor %}\n\n   {% endif %}\n   {% endblock %}\n\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: {{ _('Attributes') }}\n\n   .. autosummary::\n      :nosignatures:\n      :toctree:\n\n   {% for item in attributes %}\n      ~{{ name }}.{{ item }}\n   {%- endfor %}\n\n   {% endif %}\n   {% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Running KubeRay Benchmark Experiment\nDESCRIPTION: Executes a Python script to run the KubeRay memory benchmark experiment and logs the output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# You can modify `memory_benchmark_utils` to run the experiment you want to run.\n# (path: benchmark/memory_benchmark/scripts)\npython3 memory_benchmark_utils.py | tee benchmark_log\n```\n\n----------------------------------------\n\nTITLE: Creating a GKE Kubernetes Cluster with gcloud\nDESCRIPTION: Command to create a Google Kubernetes Engine (GKE) cluster with 2 nodes in the us-west1-b zone using e2-standard-4 machine type.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngcloud container clusters create kuberay-cluster \\\n    --num-nodes=2 --zone=us-west1-b --machine-type e2-standard-4\n```\n\n----------------------------------------\n\nTITLE: Starting Anyscale Session and Running Workload for Ray Distributed Tests\nDESCRIPTION: These commands start an Anyscale session with a specified Ray wheel and run a test workload. The <RAY_WHEEL_LINK> should be replaced with the actual link to the Ray wheel, and <WORKLOAD_NAME> with the name of the workload to run.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/long_running_distributed_tests/README.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ anyscale start --ray-wheel=<RAY_WHEEL_LINK>\n$ anyscale run test_workload --workload=<WORKLOAD_NAME>\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Package\nDESCRIPTION: Command to install Ray with default dependencies using pip package manager.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"ray[default]\"\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with SHA-256 Hashes\nDESCRIPTION: This code snippet contains package dependencies with their corresponding SHA-256 hash values for verification. It starts with a partial list of hashes for an unnamed package and then lists multidict version 6.0.5 with its complete set of hash values. The file includes a comment indicating it's referenced by a requirements file for rayllm test environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_11\n\nLANGUAGE: plain\nCODE:\n```\n    --hash=sha256:19746b50be214a54239aab822964f2ac81e38b0055cca94808359d779338c10e \\\n    --hash=sha256:2719647625320b60e2d8af06b35f5b12d4f4d281db30a15a1df22adb2295f633 \\\n    --hash=sha256:317050bc0f7739cb30d257ff09152ca309bf5a369854bbf1e57dffc310c1f20f \\\n    --hash=sha256:3b5541b2b3294e5ffabe31a09d604e23a88533ace36ac288fa32a420aa38d229 \\\n    --hash=sha256:3be5c02e1fee57b54130316a08fe40cca53af92999a302a6054cd451700ea7db \\\n    --hash=sha256:3c4ec642689da44618f68c90855a10edbc6ac3ff7c1d94395446c65a776e712a \\\n    --hash=sha256:43bbb237feab761b815ed9df43b266114203f53596f9b6e6f00ebd79d178cdf2 \\\n    --hash=sha256:45c8fb410670b3b7eb884d44a75589377c341ec1392b778311acdbfa55187716 \\\n    --hash=sha256:4cfc033c02c3e0aec52b71710d7f84cb3ca5eb407ab2ad23d75631153fdb1f12 \\\n    --hash=sha256:5f0f65f29b45e2816d8bded36e6b837a4bf5fb60ec4bc3c625fa2c6da4124537 \\\n    --hash=sha256:604037e7cd475345848116e89c553aa9a233259733ab51986ac924ab1b976f8e \\\n    --hash=sha256:60ef4bdb0ec8e4ad62e5a1f95230c08efb1f64f32e6e8dd2ced685bcc73858b5 \\\n    --hash=sha256:695b832d0091edd86eeb535cd39e45f3919f48d997685f7ac31acb15e0a2ed90 \\\n    --hash=sha256:6c7adf191e4bd3be0e9231c3b6dc20cf1199ada2af523885efc2ed218eafd011 \\\n    --hash=sha256:70eaef4934b87193a27d802534dc466778ad8d536e296ae2f9334e182ac27b6c \\\n    --hash=sha256:757b501fa57e24896cf40a831442b19a864f56d253679f34f260dcb002524a6c \\\n    --hash=sha256:82b2c42c1b9ebc89e822e7e13bbe9d17ede0c23c187469fdd9505afd5a481314 \\\n    --hash=sha256:a5bc1472223a643f5ffb5bf46ccdede7f9795078194f14edd69e3aab7020d327 \\\n    --hash=sha256:aa77046904db764b0462036bc63ef71f02b75b8f72e9c9dd4c447d6da1ed8f8e \\\n    --hash=sha256:ac7f7c377c122b649f7545810c6cd1b47586e3aa3059126ce3516ac7ccc6a6a9 \\\n    --hash=sha256:ca06aa08e39bf57e39a258e1996474f84d0dd8130d486c00bec26d797b8c5446 \\\n    --hash=sha256:d8dd848ee7ca7c8153462557655570156c2be94e79acec3561cf379581343259 \\\n    --hash=sha256:d911c442571605e17658ca2b416fd8579c5050ac9adc5e00c2cb3126c97f73bc \\\n    --hash=sha256:e695dad6897896e9384cf5e2687d9ae9feaef50e802f93602d35458e20d1fb19 \\\n    --hash=sha256:e78f46ff39a427e10b4a61614a2777ad69559cc8d603a7c05681f5a595ea98f7 \\\n    --hash=sha256:f04cad4385e20be7c7176bb8ae3dca54a08e9756cfc97bcdb4f18560c3042063 \\\n    --hash=sha256:f12d30dd6266557aaaf0aa0f9580a9a8fbeadfa83699c487713e355ec5f0bd86 \\\n    --hash=sha256:f98bd8962ad549c27d63845b50af3f53ec468b6318400c9f1adfe8b092d7b62f \\\n    --hash=sha256:fe2c4bf29bf4e89790b3117470dea2c20b59932772483082c468b990d45fb947\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   vllm\nmultidict==6.0.5 \\\n    --hash=sha256:01265f5e40f5a17f8241d52656ed27192be03bfa8764d88e8220141d1e4b3556 \\\n    --hash=sha256:0275e35209c27a3f7951e1ce7aaf93ce0d163b28948444bec61dd7badc6d3f8c \\\n    --hash=sha256:04bde7a7b3de05732a4eb39c94574db1ec99abb56162d6c520ad26f83267de29 \\\n    --hash=sha256:04da1bb8c8dbadf2a18a452639771951c662c5ad03aefe4884775454be322c9b \\\n    --hash=sha256:09a892e4a9fb47331da06948690ae38eaa2426de97b4ccbfafbdcbe5c8f37ff8 \\\n    --hash=sha256:0d63c74e3d7ab26de115c49bffc92cc77ed23395303d496eae515d4204a625e7 \\\n    --hash=sha256:107c0cdefe028703fb5dafe640a409cb146d44a6ae201e55b35a4af8e95457dd \\\n    --hash=sha256:141b43360bfd3bdd75f15ed811850763555a251e38b2405967f8e25fb43f7d40 \\\n    --hash=sha256:14c2976aa9038c2629efa2c148022ed5eb4cb939e15ec7aace7ca932f48f9ba6 \\\n    --hash=sha256:19fe01cea168585ba0f678cad6f58133db2aa14eccaf22f88e4a6dccadfad8b3 \\\n    --hash=sha256:1d147090048129ce3c453f0292e7697d333db95e52616b3793922945804a433c \\\n    --hash=sha256:1d9ea7a7e779d7a3561aade7d596649fbecfa5c08a7674b11b423783217933f9 \\\n    --hash=sha256:215ed703caf15f578dca76ee6f6b21b7603791ae090fbf1ef9d865571039ade5 \\\n    --hash=sha256:21fd81c4ebdb4f214161be351eb5bcf385426bf023041da2fd9e60681f3cebae \\\n    --hash=sha256:220dd781e3f7af2c2c1053da9fa96d9cf3072ca58f057f4c5adaaa1cab8fc442 \\\n    --hash=sha256:228b644ae063c10e7f324ab1ab6b548bdf6f8b47f3ec234fef1093bc2735e5f9 \\\n    --hash=sha256:29bfeb0dff5cb5fdab2023a7a9947b3b4af63e9c47cae2a10ad58394b517fddc \\\n    --hash=sha256:2f4848aa3baa109e6ab81fe2006c77ed4d3cd1e0ac2c1fbddb7b1277c168788c \\\n    --hash=sha256:2faa5ae9376faba05f630d7e5e6be05be22913782b927b19d12b8145968a85ea \\\n    --hash=sha256:2ffc42c922dbfddb4a4c3b438eb056828719f07608af27d163191cb3e3aa6cc5 \\\n    --hash=sha256:37b15024f864916b4951adb95d3a80c9431299080341ab9544ed148091b53f50 \\\n    --hash=sha256:3cc2ad10255f903656017363cd59436f2111443a76f996584d1077e43ee51182 \\\n    --hash=sha256:3d25f19500588cbc47dc19081d78131c32637c25804df8414463ec908631e453 \\\n    --hash=sha256:403c0911cd5d5791605808b942c88a8155c2592e05332d2bf78f18697a5fa15e \\\n    --hash=sha256:411bf8515f3be9813d06004cac41ccf7d1cd46dfe233705933dd163b60e37600 \\\n    --hash=sha256:425bf820055005bfc8aa9a0b99ccb52cc2f4070153e34b701acc98d201693733 \\\n    --hash=sha256:435a0984199d81ca178b9ae2c26ec3d49692d20ee29bc4c11a2a8d4514c67eda \\\n    --hash=sha256:4a6a4f196f08c58c59e0b8ef8ec441d12aee4125a7d4f4fef000ccb22f8d7241 \\\n    --hash=sha256:4cc0ef8b962ac7a5e62b9e826bd0cd5040e7d401bc45a6835910ed699037a461 \\\n    --hash=sha256:51d035609b86722963404f711db441cf7134f1889107fb171a970c9701f92e1e \\\n    --hash=sha256:53689bb4e102200a4fafa9de9c7c3c212ab40a7ab2c8e474491914d2305f187e \\\n    --hash=sha256:55205d03e8a598cfc688c71ca8ea5f66447164efff8869517f175ea632c7cb7b \\\n    --hash=sha256:5c0631926c4f58e9a5ccce555ad7747d9a9f8b10619621f22f9635f069f6233e \\\n    --hash=sha256:5cb241881eefd96b46f89b1a056187ea8e9ba14ab88ba632e68d7a2ecb7aadf7 \\\n    --hash=sha256:60d698e8179a42ec85172d12f50b1668254628425a6bd611aba022257cac1386 \\\n    --hash=sha256:612d1156111ae11d14afaf3a0669ebf6c170dbb735e510a7438ffe2369a847fd \\\n    --hash=sha256:6214c5a5571802c33f80e6c84713b2c79e024995b9c5897f794b43e714daeec9 \\\n    --hash=sha256:6939c95381e003f54cd4c5516740faba40cf5ad3eeff460c3ad1d3e0ea2549bf \\\n    --hash=sha256:69db76c09796b313331bb7048229e3bee7928eb62bab5e071e9f7fcc4879caee \\\n    --hash=sha256:6bf7a982604375a8d49b6cc1b781c1747f243d91b81035a9b43a2126c04766f5 \\\n    --hash=sha256:766c8f7511df26d9f11cd3a8be623e59cca73d44643abab3f8c8c07620524e4a \\\n    --hash=sha256:76c0de87358b192de7ea9649beb392f107dcad9ad27276324c24c91774ca5271 \\\n    --hash=sha256:76f067f5121dcecf0d63a67f29080b26c43c71a98b10c701b0677e4a065fbd54 \\\n    --hash=sha256:7901c05ead4b3fb75113fb1dd33eb1253c6d3ee37ce93305acd9d38e0b5f21a4 \\\n    --hash=sha256:79660376075cfd4b2c80f295528aa6beb2058fd289f4c9252f986751a4cd0496 \\\n    --hash=sha256:79a6d2ba910adb2cbafc95dad936f8b9386e77c84c35bc0add315b856d7c3abb \\\n    --hash=sha256:7afcdd1fc07befad18ec4523a782cde4e93e0a2bf71239894b8d61ee578c1319 \\\n    --hash=sha256:7be7047bd08accdb7487737631d25735c9a04327911de89ff1b26b81745bd4e3 \\\n    --hash=sha256:7c6390cf87ff6234643428991b7359b5f59cc15155695deb4eda5c777d2b880f \\\n    --hash=sha256:7df704ca8cf4a073334e0427ae2345323613e4df18cc224f647f251e5e75a527 \\\n    --hash=sha256:85f67aed7bb647f93e7520633d8f51d3cbc6ab96957c71272b286b2f30dc70ed \\\n    --hash=sha256:896ebdcf62683551312c30e20614305f53125750803b614e9e6ce74a96232604 \\\n    --hash=sha256:92d16a3e275e38293623ebf639c471d3e03bb20b8ebb845237e0d3664914caef \\\n    --hash=sha256:99f60d34c048c5c2fabc766108c103612344c46e35d4ed9ae0673d33c8fb26e8 \\\n    --hash=sha256:9fe7b0653ba3d9d65cbe7698cca585bf0f8c83dbbcc710db9c90f478e175f2d5 \\\n    --hash=sha256:a3145cb08d8625b2d3fee1b2d596a8766352979c9bffe5d7833e0503d0f0b5e5 \\\n    --hash=sha256:aeaf541ddbad8311a87dd695ed9642401131ea39ad7bc8cf3ef3967fd093b626 \\\n    --hash=sha256:b55358304d7a73d7bdf5de62494aaf70bd33015831ffd98bc498b433dfe5b10c \\\n    --hash=sha256:b82cc8ace10ab5bd93235dfaab2021c70637005e1ac787031f4d1da63d493c1d \\\n    --hash=sha256:c0868d64af83169e4d4152ec612637a543f7a336e4a307b119e98042e852ad9c \\\n    --hash=sha256:c1c1496e73051918fcd4f58ff2e0f2f3066d1c76a0c6aeffd9b45d53243702cc \\\n    --hash=sha256:c9bf56195c6bbd293340ea82eafd0071cb3d450c703d2c93afb89f93b8386ccc \\\n    --hash=sha256:cbebcd5bcaf1eaf302617c114aa67569dd3f090dd0ce8ba9e35e9985b41ac35b \\\n    --hash=sha256:cd6c8fca38178e12c00418de737aef1261576bd1b6e8c6134d3e729a4e858b38 \\\n    --hash=sha256:ceb3b7e6a0135e092de86110c5a74e46bda4bd4fbfeeb3a3bcec79c0f861e450 \\\n    --hash=sha256:cf590b134eb70629e350691ecca88eac3e3b8b3c86992042fb82e3cb1830d5e1 \\\n    --hash=sha256:d3eb1ceec286eba8220c26f3b0096cf189aea7057b6e7b7a2e60ed36b373b77f \\\n    --hash=sha256:d65f25da8e248202bd47445cec78e0025c0fe7582b23ec69c3b27a640dd7a8e3 \\\n    --hash=sha256:d6f6d4f185481c9669b9447bf9d9cf3b95a0e9df9d169bbc17e363b7d5487755 \\\n    --hash=sha256:d84a5c3a5f7ce6db1f999fb9438f686bc2e09d38143f2d93d8406ed2dd6b9226 \\\n    --hash=sha256:d946b0a9eb8aaa590df1fe082cee553ceab173e6cb5b03239716338629c50c7a \\\n    --hash=sha256:dce1c6912ab9ff5f179eaf6efe7365c1f425ed690b03341911bf4939ef2f3046 \\\n    --hash=sha256:de170c7b4fe6859beb8926e84f7d7d6c693dfe8e27372ce3b76f01c46e489fcf \\\n    --hash=sha256:e02021f87a5b6932fa6ce916ca004c4d441509d33bbdbeca70d05dff5e9d2479 \\\n    --hash=sha256:e030047e85cbcedbfc073f71836d62dd5dadfbe7531cae27789ff66bc551bd5e \\\n    --hash=sha256:e0e79d91e71b9867c73323a3444724d496c037e578a0e1755ae159ba14f4f3d1 \\\n    --hash=sha256:e4428b29611e989719874670fd152b6625500ad6c686d464e99f5aaeeaca175a \\\n    --hash=sha256:e4972624066095e52b569e02b5ca97dbd7a7ddd4294bf4e7247d52635630dd83 \\\n    --hash=sha256:e7be68734bd8c9a513f2b0cfd508802d6609da068f40dc57d4e3494cefc92929 \\\n    --hash=sha256:e8e94e6912639a02ce173341ff62cc1201232ab86b8a8fcc05572741a5dc7d93 \\\n    --hash=sha256:ea1456df2a27c73ce51120fa2f519f1bea2f4a03a917f4a43c8707cf4cbbae1a \\\n    --hash=sha256:ebd8d160f91a764652d3e51ce0d2956b38efe37c9231cd82cfc0bed2e40b581c \\\n    --hash=sha256:eca2e9d0cc5a889850e9bbd68e98314ada174ff6ccd1129500103df7a94a7a44 \\\n    --hash=sha256:edd08e6f2f1a390bf137080507e44ccc086353c8e98c657e666c017718561b89 \\\n    --hash=sha256:f285e862d2f153a70586579c15c44656f888806ed0e5b56b64489afe4a2dbfba \\\n    --hash=sha256:f2a1dee728b52b33eebff5072817176c172050d44d67befd681609b4746e1c2e \\\n    --hash=sha256:f7e301075edaf50500f0b341543c41194d8df3ae5caf4702f2095f3ca73dd8da \\\n```\n\n----------------------------------------\n\nTITLE: Including TensorFlow Regression Example Code\nDESCRIPTION: This snippet is responsible for including the TensorFlow regression example code from an external Python file into the documentation. It allows users to view and utilize the example as they explore the capabilities of Ray with TensorFlow.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/tf/tensorflow_regression_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. literalinclude:: /../../python/ray/train/examples/tf/tensorflow_regression_example.py\n```\n\n----------------------------------------\n\nTITLE: Installing Anyscale and Initializing Project\nDESCRIPTION: Commands to install the Anyscale CLI and initialize the project. This is required before running any long-running tests.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/long_running_tests/README.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install anyscale\n$ anyscale init\n```\n\n----------------------------------------\n\nTITLE: Listing PyTableWriter Package with Hash Values\nDESCRIPTION: Definition for the PyTableWriter package dependency with version 1.2.0 and corresponding SHA256 hash values. The comment indicates this is required by the lm-eval package.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_26\n\nLANGUAGE: text\nCODE:\n```\npytablewriter==1.2.0 \\\n    --hash=sha256:0204a4bb684a22140d640f2599f09e137bcdc18b3dd49426f4a555016e246b46 \\\n    --hash=sha256:4a30e2bb4bf5bc1069b1d2b2bc41947577c4517ab0875b23a5b194d296f543d8\n    # via lm-eval\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenCensus-Context Dependency with Hash Verification\nDESCRIPTION: Defines the opencensus-context package dependency with version 0.1.3 and includes SHA256 hash validations. This is a dependency of the opencensus package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nopencensus-context==0.1.3 \\\n    --hash=sha256:073bb0590007af276853009fac7e4bab1d523c3f03baf4cb4511ca38967c6039 \\\n    --hash=sha256:a03108c3c10d8c80bb5ddf5c8a1f033161fa61972a9917f9b9b3a18517f0088c\n```\n\n----------------------------------------\n\nTITLE: Running Tune Unit Tests in Python\nDESCRIPTION: Command to run all Tune unit tests using pytest after setting up the development environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npytest ray/python/ray/tune/tests/\n```\n\n----------------------------------------\n\nTITLE: Citing Ray Tune with BibTeX in LaTeX\nDESCRIPTION: BibTeX citation for the Ray Tune research paper titled 'Tune: A Research Platform for Distributed Model Selection and Training'. This citation format is intended for academic papers referencing Ray Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/index.rst#2025-04-12_snippet_3\n\nLANGUAGE: tex\nCODE:\n```\n@article{liaw2018tune,\n    title={Tune: A Research Platform for Distributed Model Selection and Training},\n    author={Liaw, Richard and Liang, Eric and Nishihara, Robert\n            and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},\n    journal={arXiv preprint arXiv:1807.05118},\n    year={2018}\n}\n```\n\n----------------------------------------\n\nTITLE: Java Stack Trace with ScriptableObject Slot Creation in Rhino\nDESCRIPTION: This stack trace extends to show slot creation in Mozilla Rhino's ScriptableObject during JavaScript execution in Vert.x. It demonstrates how Rhino manages property storage when new properties are assigned during HTTP request processing in JavaScript handlers.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_65\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.createFunctionActivation_[j];org/mozilla/javascript/IdScriptableObject:.put_[j];org/mozilla/javascript/ScriptableObject:.getSlot_[j];org/mozilla/javascript/ScriptableObject:.createSlot_[j] 2\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Ray Project\nDESCRIPTION: Additional Python package dependencies with their versions and hashes for the Ray project. These include packages like jmespath, json5, jsonpatch, and various JSON-related libraries with their source references.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_13\n\nLANGUAGE: text\nCODE:\n```\njmespath==1.0.1 \\\n    --hash=sha256:02e2e4cc71b5bcab88332eebf907519190dd9e6e82107fa7f83b1003a6252980 \\\n    --hash=sha256:90261b206d6defd58fdd5e85f478bf633a2901798906be2ad389150c5c60edbe\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   boto3\n    #   botocore\njson5==0.9.14 \\\n    --hash=sha256:740c7f1b9e584a468dbb2939d8d458db3427f2c93ae2139d05f47e453eae964f \\\n    --hash=sha256:9ed66c3a6ca3510a976a9ef9b8c0787de24802724ab1860bc0153c7fdd589b02\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   jupyterlab-server\njsonpatch==1.32 \\\n    --hash=sha256:26ac385719ac9f54df8a2f0827bb8253aa3ea8ab7b3368457bcdb8c14595a397 \\\n    --hash=sha256:b6ddfe6c3db30d81a96aaeceb6baf916094ffa23d7dd5fa2c13e13f8b6e600c2\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\njsonpointer==2.4 \\\n    --hash=sha256:15d51bba20eea3165644553647711d150376234112651b4f1811022aecad7d7a \\\n    --hash=sha256:585cee82b70211fa9e6043b7bb89db6e1aa49524340dde8ad6b63206ea689d88\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   jsonpatch\n    #   jsonschema\njsonref==1.1.0 \\\n    --hash=sha256:32fe8e1d85af0fdefbebce950af85590b22b60f9e95443176adbde4e1ecea552 \\\n    --hash=sha256:590dc7773df6c21cbf948b5dac07a72a251db28b0238ceecce0a2abfa8ec30a9\n    # via -r python/requirements/llm/llm-requirements.txt\njsonschema==4.23.0 \\\n    --hash=sha256:d71497fef26351a33265337fa77ffeb82423f3ea21283cd9467bb03999266bc4 \\\n    --hash=sha256:fbadb6f8b144a8f8cf9f0b89ba94501d143e50411a1278633f56a7acf7fd5566\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements/llm/llm-requirements.txt\n    #   -r python/requirements.txt\n    #   jupyter-events\n    #   jupyterlab-server\n    #   mistral-common\n    #   nbformat\n    #   outlines\n    #   outlines-core\n    #   ray\njsonschema-specifications==2024.10.1 \\\n    --hash=sha256:0f38b83639958ce1152d02a7f062902c41c8fd20d558b0c34344292d417ae272 \\\n    --hash=sha256:a09a0680616357d9a0ecf05c12ad234479f549239d0f5b55f3deea67475da9bf\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   jsonschema\njupyter-client==7.3.4 \\\n    --hash=sha256:17d74b0d0a7b24f1c8c527b24fcf4607c56bee542ffe8e3418e50b21e514b621 \\\n    --hash=sha256:aa9a6c32054b290374f95f73bb0cae91455c58dfb84f65c8591912b8f65e6d56\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   ipykernel\n    #   jupyter-server\n    #   nbclassic\n    #   nbclient\n    #   notebook\njupyter-core==5.5.0 \\\n    --hash=sha256:880b86053bf298a8724994f95e99b99130659022a4f7f45f563084b6223861d3 \\\n    --hash=sha256:e11e02cd8ae0a9de5c6c44abf5727df9f2581055afe00b22183f621ba3585805\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   ipykernel\n    #   jupyter-client\n    #   jupyter-server\n    #   jupyterlab\n    #   nbclassic\n    #   nbconvert\n    #   nbformat\n    #   notebook\njupyter-events==0.6.3 \\\n    --hash=sha256:57a2749f87ba387cd1bfd9b22a0875b889237dbf2edc2121ebb22bde47036c17 \\\n```\n\n----------------------------------------\n\nTITLE: Example Output from Llama2-70b Model Generation\nDESCRIPTION: This is a sample output generated by the Llama2-70b model after deployment. It shows the model's ability to continue a given story prompt.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/intel-gaudi-inference.md#2025-04-12_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nOnce upon a time, in a far-off land, there was a magical kingdom called \"Happily Ever Laughter.\" It was a place where laughter was the key to unlocking all the joys of life, and where everyone lived in perfect harmony.\n\nIn this kingdom, there was a beautiful princess named Lily. She was kind, gentle, and had a heart full of laughter. Every day, she would wake up with a big smile on her face, ready to face whatever adventures the day might bring.\n\nOne day, a wicked sorcerer cast a spell on the kingdom\nOnce upon a time, in a far-off land, there was a magical kingdom called \"Happily Ever Laughter.\" It was a place where laughter was the key to unlocking all the joys of life, and where everyone lived in perfect harmony.\n\nIn this kingdom, there was a beautiful princess named Lily. She was kind, gentle, and had a heart full of laughter. Every day, she would wake up with a big smile on her face, ready to face whatever adventures the day might bring.\n\nOne day, a wicked sorcerer cast a spell on the kingdom\n```\n\n----------------------------------------\n\nTITLE: Citing RLlib - First BibTeX Entry\nDESCRIPTION: This BibTeX entry is for citing the \"RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem\" paper, presented at NeurIPS 2021. It includes the title, authors, booktitle, year, and URL for referencing the paper in academic publications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_10\n\nLANGUAGE: BibTeX\nCODE:\n```\n@inproceedings{liang2021rllib,\n    title={{\\RLlib} Flow: Distributed Reinforcement Learning is a Dataflow Problem},\n    author={\n        Wu, Zhanghao and\n        Liang, Eric and\n        Luo, Michael and\n        Mika, Sven and\n        Gonzalez, Joseph E. and\n        Stoica, Ion\n    },\n    booktitle={Conference on Neural Information Processing Systems ({NeurIPS})},\n    year={2021},\n    url={https://proceedings.neurips.cc/paper/2021/file/2bce32ed409f5ebcee2a7b417ad9beed-Paper.pdf}\n}\n```\n\n----------------------------------------\n\nTITLE: Netty Channel Read/Write Stack Trace with TCP Push\nDESCRIPTION: Stack trace showing the execution path from Netty's NIO event loop through channel read completion, flush operations, and TCP packet transmission ending with push pending frames\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_6\n\nLANGUAGE: stacktrace\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];__tcp_push_pending_frames_[k]\n```\n\n----------------------------------------\n\nTITLE: Package Index Sources Configuration\nDESCRIPTION: Configuration for package index sources used in Ray's requirements file, including PyPI and specialized PyTorch CUDA 12.4 wheel sources. These define where packages will be downloaded from during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cu124\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html\n```\n\n----------------------------------------\n\nTITLE: Creating High Priority RayJob\nDESCRIPTION: Shell command to create a new RayJob using the configured YAML file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\n$ kubectl create -f ray-job.pytorch-distributed-training.yaml\nrayjob.ray.io/prod-pytorch-text-classifier-gkp9b created\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Prefetching in RLlib's Offline RL API\nDESCRIPTION: This snippet demonstrates how to configure batch prefetching using the 'iter_batches_kwargs' parameter in RLlib's Offline RL API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .offline_data(\n        iter_batches_kwargs={\n            \"prefetch_batches\": 2,\n        }\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: JVM Parallel Garbage Collection with String Table Processing\nDESCRIPTION: Thread stack trace showing JVM parallel garbage collection with focus on string table management. This trace captures the special handling of Java string objects during garbage collection to support string interning.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_115\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;VMThread::run;VMThread::loop;VMThread::evaluate_operation;VM_Operation::evaluate;VM_ParallelGCFailedAllocation::doit;ParallelScavengeHeap::failed_mem_allocate;PSScavenge::invoke;PSScavenge::invoke_no_policy;StringTable::unlink_or_oops_do\n```\n\n----------------------------------------\n\nTITLE: Listing Pynvml Package with Hash Values\nDESCRIPTION: Definition for the Pynvml package dependency with version 11.5.0 and corresponding SHA256 hash values. The comment indicates this is required by the DeepSpeed package through the compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_22\n\nLANGUAGE: text\nCODE:\n```\npynvml==11.5.0 \\\n    --hash=sha256:5cce014ac01b098d08f06178f86c37be409b80b2e903a5a03ce15eed60f55e25 \\\n    --hash=sha256:d027b21b95b1088b9fc278117f9f61b7c67f8e33a787e9f83f735f0f71ac32d0\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   deepspeed\n```\n\n----------------------------------------\n\nTITLE: Installing pycparser with Pinned Version and Hashes\nDESCRIPTION: Specifies pycparser package with version 2.21 and SHA256 hashes for verification. Comments indicate this is required by the cffi package.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_26\n\nLANGUAGE: pip\nCODE:\n```\npycparser==2.21 \\\n    --hash=sha256:8ee45429555515e1f6b185e78100aea234072576aa43ab53aefcae078162fca9 \\\n    --hash=sha256:e644fdec12f7872f86c58ff790da456218b10f863970249516d60a5eaca77206\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   cffi\n```\n\n----------------------------------------\n\nTITLE: Tracing Java Call Stack in Netty/Vert.x HTTP Request-Response Flow\nDESCRIPTION: These stack traces show the complete execution path of HTTP requests through a Vert.x server, from thread initialization to NIO event processing, JavaScript request handling, and response writing. The traces reveal how Netty processes selected keys, handles channel reads, executes JavaScript callbacks, and writes HTTP responses.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_94\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];org/vertx/java/core/http/impl/VertxHttpHandler:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/handler/codec/MessageToMessageEncoder:.write_[j] 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];org/vertx/java/core/http/impl/VertxHttpHandler:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/handler/codec/MessageToMessageEncoder:.write_[j];io/netty/buffer/AbstractByteBuf:.writeBytes_[j] 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];org/vertx/java/core/http/impl/VertxHttpHandler:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/handler/codec/MessageToMessageEncoder:.write_[j];io/netty/handler/codec/http/HttpObjectEncoder:.encode_[j];io/netty/buffer/AbstractByteBuf:.writeBytes_[j] 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];org/vertx/java/core/http/impl/VertxHttpHandler:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/handler/codec/MessageToMessageEncoder:.write_[j];io/netty/handler/codec/http/HttpObjectEncoder:.encode_[j];io/netty/buffer/AbstractByteBufAllocator:.directBuffer_[j] 2\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];org/vertx/java/core/http/impl/VertxHttpHandler:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/handler/codec/MessageToMessageEncoder:.write_[j];io/netty/handler/codec/http/HttpObjectEncoder:.encode_[j];io/netty/buffer/AbstractByteBufAllocator:.directBuffer_[j];io/netty/util/concurrent/FastThreadLocal:.get_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with SHA256 Hashes\nDESCRIPTION: A requirements.txt style file containing pinned package versions with their corresponding SHA256 hashes for package verification. Includes dependencies like gcs-oauth2-boto-plugin, gcsfs, gevent, and geventhttpclient with specified versions.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:b7adcdd5adbebf1adf17378da5ba3f543684dbec47b1cda1f3997e573cd542c4\n# via\n#   -c release/ray_release/byod/requirements_compiled.txt\n#   tensorflow\ngcs-oauth2-boto-plugin==3.0 \\\n--hash=sha256:f4120b08b7f8d32904674c98f07d4caf4083a58343c0c0fa0016e0f0254dfe31\n# via\n#   -c release/ray_release/byod/requirements_compiled.txt\n#   gsutil\ngcsfs==2023.5.0 \\\n--hash=sha256:02a815e1cf28197ab4f57335e89dc5df8744a065c7c956d42692b50a9e8f1625 \\\n--hash=sha256:4f2ebc41814de3f566f85dec208704cf19823b9d04a55fd12b3142aef9046525\n```\n\n----------------------------------------\n\nTITLE: NVIDIA CUDA Libraries Requirements for Ray Project\nDESCRIPTION: Package requirements specifying NVIDIA CUDA libraries for Linux x86_64 platforms. Each entry includes version constraints, SHA256 hashes for verification, and dependency information.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_16\n\nLANGUAGE: requirements.txt\nCODE:\n```\n--hash=sha256:165764f44ef8c61fcdfdfdbe769d687e06374059fbb388b6c89ecb0e28793a6f \\\n    --hash=sha256:6278562929433d68365a07a4a1546c237ba2849852c0d4b2262a486e805b977a\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\nnvidia-cufft-cu12==11.2.1.3 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:5dad8008fc7f92f5ddfa2101430917ce2ffacd86824914c82e28990ad7f00399 \\\n    --hash=sha256:d802f4954291101186078ccbe22fc285a902136f974d369540fd4a5333d1440b \\\n    --hash=sha256:f083fc24912aa410be21fa16d157fed2055dab1cc4b6934a0e03cba69eb242b9\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\nnvidia-curand-cu12==10.3.5.147 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:1f173f09e3e3c76ab084aba0de819c49e56614feae5c12f69883f4ae9bb5fad9 \\\n    --hash=sha256:a88f583d4e0bb643c49743469964103aa59f7f708d862c3ddb0fc07f851e3b8b \\\n    --hash=sha256:f307cc191f96efe9e8f05a87096abc20d08845a841889ef78cb06924437f6771\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\nnvidia-cusolver-cu12==11.6.1.9 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:19e33fa442bcfd085b3086c4ebf7e8debc07cfe01e11513cc6d332fd918ac260 \\\n    --hash=sha256:d338f155f174f90724bbde3758b7ac375a70ce8e706d70b018dd3375545fc84e \\\n    --hash=sha256:e77314c9d7b694fcebc84f58989f3aa4fb4cb442f12ca1a9bde50f5e8f6d1b9c\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\nnvidia-cusparse-cu12==12.3.1.170 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:9bc90fb087bc7b4c15641521f31c0371e9a612fc2ba12c338d3ae032e6b6797f \\\n    --hash=sha256:9d32f62896231ebe0480efd8a7f702e143c98cfaa0e8a76df3386c1ba2b54df3 \\\n    --hash=sha256:ea4f11a2904e2a8dc4b1833cc1b5181cde564edd0d5cd33e3c168eff2d1863f1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   nvidia-cusolver-cu12\n    #   torch\nnvidia-cusparselt-cu12==0.6.2 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:0057c91d230703924c0422feabe4ce768841f9b4b44d28586b6f6d2eb86fbe70 \\\n    --hash=sha256:067a7f6d03ea0d4841c85f0c6f1991c5dda98211f6302cb83a4ab234ee95bef8 \\\n    --hash=sha256:df2c24502fd76ebafe7457dbc4716b2fec071aabaed4fb7691a201cde03704d9\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\nnvidia-nccl-cu12==2.21.5 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:8579076d30a8c24988834445f8d633c697d42397e92ffc3f63fa26766d25e0a0\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\nnvidia-nvjitlink-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:06b3b9b25bf3f8af351d664978ca26a16d2c5127dbd53c0497e28d1fb9611d57 \\\n    --hash=sha256:4abe7fef64914ccfa909bc2ba39739670ecc9e820c83ccc7a6ed414122599b83 \\\n    --hash=sha256:fd9020c501d27d135f983c6d3e244b197a7ccad769e34df53a42e276b0e25fa1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   nvidia-cusolver-cu12\n    #   nvidia-cusparse-cu12\n    #   torch\nnvidia-nvtx-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:641dccaaa1139f3ffb0d3164b4b84f9d253397e38246a4f2f36728b48566d485 \\\n    --hash=sha256:781e950d9b9f60d8241ccea575b32f5105a5baf4c2351cab5256a24869f12a1a \\\n    --hash=sha256:7959ad635db13edf4fc65c06a6e9f9e55fc2f92596db928d169c0bb031e88ef3\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   torch\n```\n\n----------------------------------------\n\nTITLE: Calculating Total Train Batch Size in Python\nDESCRIPTION: Demonstrates how to calculate the total effective train batch size by setting batch size per learner and the number of learners.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/algorithm-config.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconfig.training(train_batch_size_per_learner=256)\nconfig.learners(num_learners=2)\nprint(config.total_train_batch_size)  # expect: 512 = 256 * 2\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Verification\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and SHA256 hash values for security verification. It includes options for Python version constraints and references to other requirement files.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nmodin==0.22.2 ; python_version < \"3.12\" \\\n    --hash=sha256:532fe0bfb2dcf06c0ad2d467721ef489fd58bb3ef7150bcf4a7ddd1069be1e4d \\\n    --hash=sha256:fa897dc59d5b9a8496be044185689fdd337b9f26cc81c4144b217a2a94d029bc\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_ml_byod_3.9.in\nmonotonic==1.6 \\\n    --hash=sha256:3a55207bcfed53ddd5c5bae174524062935efed17792e9de2ad0205ce9ad63f7 \\\n    --hash=sha256:68687e19a14f11f26d140dd5c86f3dba4bf5df58003000ed467e0e2a69bca96c\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gsutil\n```\n\n----------------------------------------\n\nTITLE: Setting Databricks Token Environment Variable\nDESCRIPTION: This command sets the `DATABRICKS_TOKEN` environment variable, which is required for authenticating with a Databricks SQL warehouse. Replace `...` with the actual token value.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_29\n\nLANGUAGE: console\nCODE:\n```\nexport DATABRICKS_TOKEN=...\n```\n\n----------------------------------------\n\nTITLE: Creating Regularization Images with Ray Data in Bash\nDESCRIPTION: This bash script uses Ray Data to generate regularization images in parallel for the DreamBooth fine-tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/dreambooth_finetuning.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython dreambooth/generate_class_images.py \\\n    --pretrained_model_name_or_path=\"$PRETRAINED_MODEL\" \\\n    --cache_dir=\"$CACHE_DIR\" \\\n    --prompt=\"$CLASS_PROMPT\" \\\n    --class_data_dir=\"$CLASS_DIR\" \\\n    --num_inference_steps=50 \\\n    --num_class_images=200 \\\n    --ray_num_workers=4\n```\n\n----------------------------------------\n\nTITLE: Listing SHA256 Hashes for Python Package Dependencies\nDESCRIPTION: This snippet contains SHA256 hash values for various Python package dependencies. It includes hashes for multiple versions of each package to ensure integrity during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_7\n\nLANGUAGE: Text\nCODE:\n```\n--hash=sha256:59ec7b7c7e1a61061850d53aaf8e93db63dce0c936db1fda2658b70e4a1be709 \\\n--hash=sha256:59edc41b24031bc25108e210c0def6f6c2191210492a972d585a06ff246bb79b \\\n--hash=sha256:5a580c91d686376f0f7c295357595c5a026e6cbc3d77b7c36e290201e7c11ecb \\\n--hash=sha256:5b94529f9b2591b7af5f3e0e730a4e0a41ea174af35a4fd067775f9bdfeee01a \\\n--hash=sha256:5c7b3b3a728dc6faf3fc372ef24f21d1e3cee2ac3e9596691d746e5a536de920 \\\n--hash=sha256:5c90ae8c8d32e472be041e76f9d2f2dbff4d0b0be8bd4041770eddb18cf49a4e \\\n--hash=sha256:5e7139af55d1688f8b960ee9ad5adafc4ac17c1c473fe07133ac092310d76544 \\\n--hash=sha256:5ff5cf3571589b6d13bfbfd6bcd7a3f659e42f96b5fd1c4830c4cf21d4f5ef45 \\\n--hash=sha256:620ced262a86244e2be10a676b646f29c34537d0d9cc8eb26c08f53d98013390 \\\n--hash=sha256:6512cb89e334e4700febbffaaa52761b65b4f5a3cf33f960213d5656cea36a77 \\\n--hash=sha256:6c08e1312a9cf1074d17b17728d3dfce2a5125b2d791527f33ffbe805200a355 \\\n--hash=sha256:6c3bd3cde54cafb87d74d8db50b909705c62b17c2099b8f2e25b461882e544ff \\\n--hash=sha256:6ef7afcd2d281494c0a9101d5c571970708ad911d028137cd558f02b851c08b4 \\\n--hash=sha256:7269d9e5f1084a653d575c7ec012ff57f0c042258bf5db0954bf551c158466e7 \\\n--hash=sha256:72d40b33e834371fd330fb1472ca19d9b8327acb79a5821d4008391db8e29f20 \\\n--hash=sha256:74d1b44c6cfc897df648cc9fdaa09bc3e7679926e6f96df05775d4fb3946571c \\\n--hash=sha256:74db36e14a7d1ce0986fa104f7d5637aea5c82ca6326ed0ec5694280942d1162 \\\n--hash=sha256:763773d53f07244148ccac5b084da5adb90bfaee39c197554f01b286cf869228 \\\n--hash=sha256:76c6a5964640638cdeaa0c359382e5703e9293030fe730018ca06bc2010c4437 \\\n--hash=sha256:76d9289ed3f7501012e05abb8358bbb129149dbd173f1f57a1bf1c22d19ab7cc \\\n--hash=sha256:7931d8f1f67c4be9ba1dd9c451fb0eeca1a25b89e4d3f89e828fe12a519b782a \\\n--hash=sha256:7b8b454bac16428b22560d0a1cf0a09875339cab69df61d7805bf48919415901 \\\n--hash=sha256:7e5bab140c309cb3a6ce373a9e71eb7e4873c70c2dda01df6820474f9889d6d4 \\\n--hash=sha256:83d78376d0d4fd884e2c114d0621624b73d2aba4e2788182d286309ebdeed770 \\\n--hash=sha256:852542f9481f4a62dbb5dd99e8ab7aedfeb8fb6342349a181d4036877410f525 \\\n--hash=sha256:85267bd1aa8880a9c88a8cb71e18d3d64d2751a790e6ca6c27b8ccc724bcd5ad \\\n--hash=sha256:88a2df29d4724b9237fc0c6eaf2a1adae0cdc0b3e9f4d8e7dc54b16812d2d81a \\\n--hash=sha256:88b9f257ca61b838b6f8094a62418421f87ac2a1069f7e896c36a7d86b5d4c29 \\\n--hash=sha256:8ab3919a9997ab7ef2fbbed0cc99bb28d3c13e6d4b1ad36e97e482558a91be90 \\\n--hash=sha256:92dea1ffe3714fa8eb6a314d2b3c773208d865a0e0d35e713ec54eea08a66250 \\\n--hash=sha256:9407b6a5f0d675e8a827ad8742e1d6b49d9c1a1da5d952a67d50ef5f4170b18d \\\n--hash=sha256:9408acf3270c4b6baad483865191e3e582b638b1654a007c62e3efe96f09a9a3 \\\n--hash=sha256:955e8513d07a283056b1396e9a57ceddbd272d9252c14f154d450d227606eb54 \\\n--hash=sha256:9db8ea4c388fdb0f780fe91346fd438657ea602d58348753d9fb265ce1bca67f \\\n--hash=sha256:9eaa8b117dc8337728e834b9c6e2611f10c79e38f65157c4c38e9400286f5cb1 \\\n--hash=sha256:a51a263952b1429e429ff236d2f5a21c5125437861baeed77f5e1cc2d2c7c6da \\\n--hash=sha256:a6aa6315319a052b4ee378aa171959c898a6183f15c1e541821c5c59beaa0238 \\\n--hash=sha256:aa12042de0171fad672b6c59df69106d20d5596e4f87b5e8f76df757a7c399aa \\\n--hash=sha256:aaf7be1207676ac608a50cd08f102f6742dbfc70e8d60c4db1c6897f62f71523 \\\n--hash=sha256:b0157420efcb803e71d1b28e2c287518b8808b7cf1ab8af36718fd0a2c453eb0 \\\n--hash=sha256:b3f7e75f3015df442238cca659f8baa5f42ce2a8582727981cbfa15fee0ee205 \\\n--hash=sha256:b9098e0049e88c6a24ff64545cdfc50807818ba6c1b739cae221bbbcbc58aad3 \\\n--hash=sha256:ba55dce0a9b8ff59495ddd050a0225d58bd0983d09f87cfe2b6aec4f2c1234e4 \\\n--hash=sha256:bb86433b1cfe686da83ce32a9d3a8dd308e85c76b60896d58f082136f10bffac \\\n--hash=sha256:bbea0db94288e29afcc4c28afbf3a7ccaf2d7e027489c449cf7e8f83c6346eb9 \\\n--hash=sha256:bbf1d63eef84b2e8c89011b7f2235b1e0bf7dacc11cac9431fc6468e99ac77fb \\\n--hash=sha256:c7940c1dc63eb37a67721b10d703247552416f719c4188c54e04334321351ced \\\n--hash=sha256:c9bf3325c47b11b2e51bca0824ea217c7cd84491d8ac4eefd1e409705ef092bd \\\n--hash=sha256:cdc8a402aaee9a798b50d8b827d7ecf75edc5fb35ea0f91f213ff927c15f4ff0 \\\n--hash=sha256:ceec1a6bc6cab1d6ff5d06592a91a692f90ec7505d6463a88a52cc0eb58545da \\\n--hash=sha256:cfe6ab8da05c01ba6fbea630377b5da2cd9bcbc6338510116b01c1bc939a2c18 \\\n--hash=sha256:d099e745a512f7e3bbe7249ca835f4d357c586d78d79ae8f1dcd4d8adeb9bda9 \\\n--hash=sha256:d0ef46024e6a3d79c01ff13801cb19d0cad7fd859b15037aec74315540acc276 \\\n--hash=sha256:d2e5a98f0ec99beb3c10e13b387f8db39106d53993f498b295f0c914328b1333 \\\n--hash=sha256:da4cfb373035def307905d05041c1d06d8936452fe89d464743ae7fb8371078b \\\n--hash=sha256:da802a19d6e15dffe4b0c24b38b3af68e6e1a68e6e1d8f30148c83864f3881db \\\n--hash=sha256:dced8146011d2bc2e883f9bd68618b8247387f4bbec46d7392b3c3b032640126 \\\n--hash=sha256:dfdd7c0b105af050eb3d64997809dc21da247cf44e63dc73ff0fd20b96be55a9 \\\n--hash=sha256:e368f200bbc2e4f905b8e71eb38b3c04333bddaa6a2464a6355487b02bb7fb09 \\\n--hash=sha256:e391b1f0a8a5a10ab3b9bb6afcfd74f2175f24f8975fb87ecae700d1503cdee0 \\\n--hash=sha256:e57e563a57fb22a142da34f38acc2fc1a5c864bc29ca1517a88abc963e60d6ec \\\n--hash=sha256:e5d706eba36b4c4d5bc6c6377bb6568098765e990cfc21ee16d13963fab7b3e7 \\\n--hash=sha256:ec20916e7b4cbfb1f12380e46486ec4bcbaa91a9c448b97023fde0d5bbf9e4ff \\\n--hash=sha256:f1d072c2eb0ad60d4c183f3fb44ac6f73fb7a8f16a2694a91f988275cbf352f9 \\\n--hash=sha256:f846c260f483d1fd217fe5ed7c173fb109efa6b1fc8381c8b7552c5781756192 \\\n--hash=sha256:f91de7223d4c7b793867797bacd1ee53bfe7359bd70d27b7b58a04efbb9436c8 \\\n--hash=sha256:faae4860798c31530dd184046a900e652c95513796ef51a12bc086710c2eec4d \\\n--hash=sha256:fc579bf0f502e54926519451b920e875f433aceb4624a3646b3252b5caa9e0b6 \\\n--hash=sha256:fcc700eadbbccbf6bc1bcb9dbe0786b4b1cb91ca0dcda336eef5c2beed37b797 \\\n--hash=sha256:fd32ea360bcbb92d28933fc05ed09bffcb1704ba3fc7942e81db0fd4f81a7892 \\\n--hash=sha256:fdb7adb641a0d13bdcd4ef48e062363d8a9ad4a182ac7647ec88f695e719ae9f\n```\n\n----------------------------------------\n\nTITLE: Ray Cluster Throughput Test Log Output\nDESCRIPTION: Log output showing the execution time for a Ray cluster throughput test. The test measures total run time including tuning loop time and verifies it against a 120-second threshold.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/tune_tests/scalability_tests/test_result_throughput_cluster.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n2021-04-06 00:51:28,163 INFO tune.py:549 -- Total run time: 115.61 seconds (114.16 seconds for the tuning loop).\nThe result throughput cluster test took 116.29 seconds, which is below the budget of 120.00 seconds. Test successful.\n\n--- PASSED: RESULT THROUGHPUT CLUSTER ::: 116.29 <= 120.00 ---\n```\n\n----------------------------------------\n\nTITLE: Installing rpds-py Package with Hash Verification\nDESCRIPTION: This snippet specifies the installation of rpds-py package version 0.22.3 with numerous SHA256 hash verifications for various platform builds. The package provides immutable data structures.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_49\n\nLANGUAGE: pip\nCODE:\n```\nrpds-py==0.22.3 \\\n    --hash=sha256:009de23c9c9ee54bf11303a966edf4d9087cd43a6003672e6aa7def643d06518 \\\n    --hash=sha256:02fbb9c288ae08bcb34fb41d516d5eeb0455ac35b5512d03181d755d80810059 \\\n    --hash=sha256:0a0461200769ab3b9ab7e513f6013b7a97fdeee41c29b9db343f3c5a8e2b9e61 \\\n    --hash=sha256:0b09865a9abc0ddff4e50b5ef65467cd94176bf1e0004184eb915cbc10fc05c5 \\\n    --hash=sha256:0b8db6b5b2d4491ad5b6bdc2bc7c017eec108acbf4e6785f42a9eb0ba234f4c9 \\\n    --hash=sha256:0c150c7a61ed4a4f4955a96626574e9baf1adf772c2fb61ef6a5027e52803543 \\\n    --hash=sha256:0f3cec041684de9a4684b1572fe28c7267410e02450f4561700ca5a3bc6695a2 \\\n    --hash=sha256:1352ae4f7c717ae8cba93421a63373e582d19d55d2ee2cbb184344c82d2ae55a \\\n    --hash=sha256:177c7c0fce2855833819c98e43c262007f42ce86651ffbb84f37883308cb0e7d \\\n    --hash=sha256:1978d0021e943aae58b9b0b196fb4895a25cc53d3956b8e35e0b7682eefb6d56 \\\n    --hash=sha256:1a60bce91f81ddaac922a40bbb571a12c1070cb20ebd6d49c48e0b101d87300d \\\n    --hash=sha256:1aef18820ef3e4587ebe8b3bc9ba6e55892a6d7b93bac6d29d9f631a3b4befbd \\\n    --hash=sha256:1e9663daaf7a63ceccbbb8e3808fe90415b0757e2abddbfc2e06c857bf8c5e2b \\\n    --hash=sha256:20070c65396f7373f5df4005862fa162db5d25d56150bddd0b3e8214e8ef45b4 \\\n    --hash=sha256:214b7a953d73b5e87f0ebece4a32a5bd83c60a3ecc9d4ec8f1dca968a2d91e99 \\\n    --hash=sha256:22bebe05a9ffc70ebfa127efbc429bc26ec9e9b4ee4d15a740033efda515cf3d \\\n    --hash=sha256:24e8abb5878e250f2eb0d7859a8e561846f98910326d06c0d51381fed59357bd \\\n    --hash=sha256:26fd7cac7dd51011a245f29a2cc6489c4608b5a8ce8d75661bb4a1066c52dfbe \\\n    --hash=sha256:27b1d3b3915a99208fee9ab092b8184c420f2905b7d7feb4aeb5e4a9c509b8a1 \\\n    --hash=sha256:27e98004595899949bd7a7b34e91fa7c44d7a97c40fcaf1d874168bb652ec67e \\\n    --hash=sha256:2b8f60e1b739a74bab7e01fcbe3dddd4657ec685caa04681df9d562ef15b625f \\\n    --hash=sha256:2de29005e11637e7a2361fa151f780ff8eb2543a0da1413bb951e9f14b699ef3 \\\n    --hash=sha256:2e8b55d8517a2fda8d95cb45d62a5a8bbf9dd0ad39c5b25c8833efea07b880ca \\\n    --hash=sha256:2fa4331c200c2521512595253f5bb70858b90f750d39b8cbfd67465f8d1b596d \\\n    --hash=sha256:3445e07bf2e8ecfeef6ef67ac83de670358abf2996916039b16a218e3d95e97e \\\n    --hash=sha256:3453e8d41fe5f17d1f8e9c383a7473cd46a63661628ec58e07777c2fff7196dc \\\n    --hash=sha256:378753b4a4de2a7b34063d6f95ae81bfa7b15f2c1a04a9518e8644e81807ebea \\\n    --hash=sha256:3af6e48651c4e0d2d166dc1b033b7042ea3f871504b6805ba5f4fe31581d8d38 \\\n    --hash=sha256:3dfcbc95bd7992b16f3f7ba05af8a64ca694331bd24f9157b49dadeeb287493b \\\n    --hash=sha256:3f21f0495edea7fdbaaa87e633a8689cd285f8f4af5c869f27bc8074638ad69c \\\n    --hash=sha256:4041711832360a9b75cfb11b25a6a97c8fb49c07b8bd43d0d02b45d0b499a4ff \\\n    --hash=sha256:44d61b4b7d0c2c9ac019c314e52d7cbda0ae31078aabd0f22e583af3e0d79723 \\\n    --hash=sha256:4617e1915a539a0d9a9567795023de41a87106522ff83fbfaf1f6baf8e85437e \\\n    --hash=sha256:4b232061ca880db21fa14defe219840ad9b74b6158adb52ddf0e87bead9e8493 \\\n    --hash=sha256:5246b14ca64a8675e0a7161f7af68fe3e910e6b90542b4bfb5439ba752191df6 \\\n    --hash=sha256:5725dd9cc02068996d4438d397e255dcb1df776b7ceea3b9cb972bdb11260a83 \\\n    --hash=sha256:583f6a1993ca3369e0f80ba99d796d8e6b1a3a2a442dd4e1a79e652116413091 \\\n    --hash=sha256:59259dc58e57b10e7e18ce02c311804c10c5a793e6568f8af4dead03264584d1 \\\n    --hash=sha256:593eba61ba0c3baae5bc9be2f5232430453fb4432048de28399ca7376de9c627 \\\n    --hash=sha256:59f4a79c19232a5774aee369a0c296712ad0e77f24e62cad53160312b1c1eaa1 \\\n    --hash=sha256:5f0e260eaf54380380ac3808aa4ebe2d8ca28b9087cf411649f96bad6900c728 \\\n    --hash=sha256:62d9cfcf4948683a18a9aff0ab7e1474d407b7bab2ca03116109f8464698ab16 \\\n    --hash=sha256:64607d4cbf1b7e3c3c8a14948b99345eda0e161b852e122c6bb71aab6d1d798c \\\n    --hash=sha256:655ca44a831ecb238d124e0402d98f6212ac527a0ba6c55ca26f616604e60a45 \\\n    --hash=sha256:666ecce376999bf619756a24ce15bb14c5bfaf04bf00abc7e663ce17c3f34fe7 \\\n    --hash=sha256:68049202f67380ff9aa52f12e92b1c30115f32e6895cd7198fa2a7961621fc5a \\\n    --hash=sha256:69803198097467ee7282750acb507fba35ca22cc3b85f16cf45fb01cb9097730 \\\n    --hash=sha256:6c7b99ca52c2c1752b544e310101b98a659b720b21db00e65edca34483259967 \\\n    --hash=sha256:6dd9412824c4ce1aca56c47b0991e65bebb7ac3f4edccfd3f156150c96a7bf25 \\\n    --hash=sha256:70eb60b3ae9245ddea20f8a4190bd79c705a22f8028aaf8bbdebe4716c3fab24 \\\n    --hash=sha256:70fb28128acbfd264eda9bf47015537ba3fe86e40d046eb2963d75024be4d055 \\\n    --hash=sha256:7b2513ba235829860b13faa931f3b6846548021846ac808455301c23a101689d \\\n    --hash=sha256:7ef9d9da710be50ff6809fed8f1963fecdfecc8b86656cadfca3bc24289414b0 \\\n    --hash=sha256:81e69b0a0e2537f26d73b4e43ad7bc8c8efb39621639b4434b76a3de50c6966e \\\n    --hash=sha256:8633e471c6207a039eff6aa116e35f69f3156b3989ea3e2d755f7bc41754a4a7 \\\n    --hash=sha256:8bd7c8cfc0b8247c8799080fbff54e0b9619e17cdfeb0478ba7295d43f635d7c \\\n    --hash=sha256:9253fc214112405f0afa7db88739294295f0e08466987f1d70e29930262b4c8f \\\n    --hash=sha256:99b37292234e61325e7a5bb9689e55e48c3f5f603af88b1642666277a81f1fbd \\\n    --hash=sha256:9bd7228827ec7bb817089e2eb301d907c0d9827a9e558f22f762bb690b131652 \\\n    --hash=sha256:9beeb01d8c190d7581a4d59522cd3d4b6887040dcfc744af99aa59fef3e041a8 \\\n    --hash=sha256:a63cbdd98acef6570c62b92a1e43266f9e8b21e699c363c0fef13bd530799c11 \\\n    --hash=sha256:a76e42402542b1fae59798fab64432b2d015ab9d0c8c47ba7addddbaf7952333 \\\n    --hash=sha256:ac0a03221cdb5058ce0167ecc92a8c89e8d0decdc9e99a2ec23380793c4dcb96 \\\n    --hash=sha256:b0b4136a252cadfa1adb705bb81524eee47d9f6aab4f2ee4fa1e9d3cd4581f64 \\\n    --hash=sha256:b25bc607423935079e05619d7de556c91fb6adeae9d5f80868dde3468657994b \\\n    --hash=sha256:b3d504047aba448d70cf6fa22e06cb09f7cbd761939fdd47604f5e007675c24e \\\n    --hash=sha256:bb47271f60660803ad11f4c61b42242b8c1312a31c98c578f79ef9387bbde21c \\\n    --hash=sha256:bbb232860e3d03d544bc03ac57855cd82ddf19c7a07651a7c0fdb95e9efea8b9 \\\n    --hash=sha256:bc27863442d388870c1809a87507727b799c8460573cfbb6dc0eeaef5a11b5ec \\\n    --hash=sha256:bc51abd01f08117283c5ebf64844a35144a0843ff7b2983e0648e4d3d9f10dbb \\\n    --hash=sha256:be2eb3f2495ba669d2a985f9b426c1797b7d48d6963899276d22f23e33d47e37 \\\n    --hash=sha256:bf9db5488121b596dbfc6718c76092fda77b703c1f7533a226a5a9f65248f8ad \\\n    --hash=sha256:c58e2339def52ef6b71b8f36d13c3688ea23fa093353f3a4fee2556e62086ec9 \\\n    --hash=sha256:cfbc454a2880389dbb9b5b398e50d439e2e58669160f27b60e5eca11f68ae17c \\\n    --hash=sha256:cff63a0272fcd259dcc3be1657b07c929c466b067ceb1c20060e8d10af56f5bf \\\n    --hash=sha256:d115bffdd417c6d806ea9069237a4ae02f513b778e3789a359bc5856e0404cc4 \\\n    --hash=sha256:d20cfb4e099748ea39e6f7b16c91ab057989712d31761d3300d43134e26e165f \\\n    --hash=sha256:d48424e39c2611ee1b84ad0f44fb3b2b53d473e65de061e3f460fc0be5f1939d \\\n    --hash=sha256:e0fa2d4ec53dc51cf7d3bb22e0aa0143966119f42a0c3e4998293a3dd2856b09 \\\n    --hash=sha256:e32fee8ab45d3c2db6da19a5323bc3362237c8b653c70194414b892fd06a080d \\\n    --hash=sha256:e35ba67d65d49080e8e5a1dd40101fccdd9798adb9b050ff670b7d74fa41c566 \\\n    --hash=sha256:e3fb866d9932a3d7d0c82da76d816996d1667c44891bd861a0f97ba27e84fc74 \\\n    --hash=sha256:e61b02c3f7a1e0b75e20c3978f7135fd13cb6cf551bf4a6d29b999a88830a338 \\\n    --hash=sha256:e67ba3c290821343c192f7eae1d8fd5999ca2dc99994114643e2f2d3e6138b15 \\\n    --hash=sha256:e79dd39f1e8c3504be0607e5fc6e86bb60fe3584bec8b782578c3b0fde8d932c \\\n    --hash=sha256:e89391e6d60251560f0a8f4bd32137b077a80d9b7dbe6d5cab1cd80d2746f648 \\\n    --hash=sha256:ea7433ce7e4bfc3a85654aeb6747babe3f66eaf9a1d0c1e7a4435bbdf27fea84 \\\n    --hash=sha256:eaf16ae9ae519a0e237a0f528fd9f0197b9bb70f40263ee57ae53c2b8d48aeb3 \\\n    --hash=sha256:eb0c341fa71df5a4595f9501df4ac5abfb5a09580081dffbd1ddd4654e6e9123 \\n\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost Training Benchmark on Single Node\nDESCRIPTION: This command runs an XGBoost training benchmark using Ray Train's XGBoostTrainer on a single node with 10GB of data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/benchmarks.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython train_batch_inference_benchmark.py \"xgboost\" --size=10GB\n```\n\n----------------------------------------\n\nTITLE: y-py Package Dependency with SHA256 Hashes\nDESCRIPTION: This snippet shows the y-py package dependency with version 0.6.2 and its SHA256 hash values. The package is used by jupyter-ydoc and ypy-websocket as indicated in the via comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_35\n\nLANGUAGE: text\nCODE:\n```\ny-py==0.6.2 \\\n    --hash=sha256:015f7f6c1ce8a83d57955d1dc7ddd57cb633ae00576741a4fc9a0f72ed70007d \\\n    --hash=sha256:032365dfe932bfab8e80937ad6093b4c22e67d63ad880096b5fa8768f8d829ba \\\n    --hash=sha256:0649a41cd3c98e290c16592c082dbe42c7ffec747b596172eebcafb7fd8767b0 \\\n    --hash=sha256:0787e85645bb4986c27e271715bc5ce21bba428a17964e5ec527368ed64669bc \\\n    --hash=sha256:0cd6213c3cf2b9eee6f2c9867f198c39124c557f4b3b77d04a73f30fd1277a59 \\\n    --hash=sha256:0f2d881f0f8bf5674f8fe4774a438c545501e40fa27320c73be4f22463af4b05 \\\n    --hash=sha256:17bce637a89f6e75f0013be68becac3e38dc082e7aefaf38935e89215f0aa64a \\\n    --hash=sha256:17edd21eef863d230ea00004ebc6d582cc91d325e7132deb93f0a90eb368c855 \\\n    --hash=sha256:1d5b544e79ace93fdbd0b36ed329c86e346898153ac7ba2ec62bc9b4c6b745c9 \\\n    --hash=sha256:1f798165158b76365a463a4f8aa2e3c2a12eb89b1fc092e7020e93713f2ad4dc \\\n    --hash=sha256:266ec46ab9f9cb40fbb5e649f55c329fc4620fa0b1a8117bdeefe91595e182dc \\\n    --hash=sha256:26cb1307c3ca9e21a3e307ab2c2099677e071ae9c26ec10ddffb3faceddd76b3 \\\n    --hash=sha256:2a497ebe617bec6a420fc47378856caae40ab0652e756f3ed40c5f1fe2a12220 \\\n    --hash=sha256:2b4fac4ea2ce27b86d173ae45765ced7f159120687d4410bb6d0846cbdb170a3 \\\n    --hash=sha256:2cf817a72ffec4295def5c5be615dd8f1e954cdf449d72ebac579ff427951328 \\\n    --hash=sha256:2d2b054a1a5f4004967532a4b82c6d1a45421ef2a5b41d35b6a8d41c7142aabe \\\n    --hash=sha256:316e5e1c40259d482883d1926fd33fa558dc87b2bd2ca53ce237a6fe8a34e473 \\\n    --hash=sha256:35fcb9def6ce137540fdc0e91b08729677548b9c393c0151a6359fd199da3bd7 \\\n    --hash=sha256:376c5cc0c177f03267340f36aec23e5eaf19520d41428d87605ca2ca3235d845 \\\n    --hash=sha256:3ba99d0bdbd9cabd65f914cd07b4fb2e939ce199b54ae5ace1639ce1edf8e0a2 \\\n    --hash=sha256:3c011303eb2b360695d2bd4bd7ca85f42373ae89fcea48e7fa5b8dc6fc254a98 \\\n    --hash=sha256:4757a82a50406a0b3a333aa0122019a331bd6f16e49fed67dca423f928b3fd4d \\\n    --hash=sha256:47fcc19158150dc4a6ae9a970c5bc12f40b0298a2b7d0c573a510a7b6bead3f3 \\\n    --hash=sha256:4c28d977f516d4928f6bc0cd44561f6d0fdd661d76bac7cdc4b73e3c209441d9 \\\n    --hash=sha256:5415083f7f10eac25e1c434c87f07cb9bfa58909a6cad6649166fdad21119fc5 \\\n    --hash=sha256:613f83713714972886e81d71685403098a83ffdacf616f12344b52bc73705107 \\\n    --hash=sha256:69cfbcbe0a05f43e780e6a198080ba28034bf2bb4804d7d28f71a0379bfd1b19 \\\n    --hash=sha256:6c2f2831c5733b404d2f2da4bfd02bb4612ae18d0822e14ae79b0b92436b816d \\\n    --hash=sha256:7227f232f2daf130ba786f6834548f2cfcfa45b7ec4f0d449e72560ac298186c \\\n    --hash=sha256:72875641a907523d37f4619eb4b303611d17e0a76f2ffc423b62dd1ca67eef41 \\\n    --hash=sha256:7c7302619fc962e53093ba4a94559281491c045c925e5c4defec5dac358e0568 \\\n    --hash=sha256:7cbefd4f1060f05768227ddf83be126397b1d430b026c64e0eb25d3cf50c5734 \\\n    --hash=sha256:80a827e173372682959a57e6b8cc4f6468b1a4495b4bc7a775ef6ca05ae3e8e8 \\\n    --hash=sha256:82f2e5b31678065e7a7fa089ed974af5a4f076673cf4f414219bdadfc3246a21 \\\n    --hash=sha256:82f5ca62bedbf35aaf5a75d1f53b4457a1d9b6ff033497ca346e2a0cedf13d14 \\\n    --hash=sha256:8448da4092265142662bbd3fc46cb8b0796b1e259189c020bc8f738899abd0b5 \\\n    --hash=sha256:863e175ce5585f9ff3eba2aa16626928387e2a576157f02c8eb247a218ecdeae \\\n    --hash=sha256:86422c6090f34906c062fd3e4fdfdccf3934f2922021e979573ae315050b4288 \\\n    --hash=sha256:898fede446ca1926b8406bdd711617c2aebba8227ee8ec1f0c2f8568047116f7 \\\n    --hash=sha256:8f5c14d25611b263b876e9ada1701415a13c3e9f02ea397224fbe4ca9703992b \\\n    --hash=sha256:8f6071328aad06fdcc0a4acc2dc4839396d645f5916de07584af807eb7c08407 \\\n    --hash=sha256:932abb560fe739416b50716a72ba6c6c20b219edded4389d1fc93266f3505d4b \\\n    --hash=sha256:9b7cafbe946b4cafc1e5709957e6dd5c6259d241d48ed75713ded42a5e8a4663 \\\n    --hash=sha256:9b8822a5c0fd9a8cffcabfcc0cd7326bad537ee614fc3654e413a03137b6da1a \\\n    --hash=sha256:a21148b8ea09a631b752d975f9410ee2a31c0e16796fdc113422a6d244be10e5 \\\n    --hash=sha256:a3932f53418b408fa03bd002e6dc573a74075c2c092926dde80657c39aa2e054 \\\n    --hash=sha256:a70aee572da3994238c974694767365f237fc5949a550bee78a650fe16f83184 \\\n    --hash=sha256:ae80d505aee7b3172cdcc2620ca6e2f85586337371138bb2b71aa377d2c31e9a \\\n    --hash=sha256:b2686d7d8ca31531458a48e08b0344a8eec6c402405446ce7d838e2a7e43355a \\\n    --hash=sha256:bae1b1ad8d2b8cf938a60313f8f7461de609621c5dcae491b6e54975f76f83c5 \\\n    --hash=sha256:bd302c6d46a3be57664571a5f0d4224646804be9890a01d73a0b294f2d3bbff1 \\\n    --hash=sha256:beea5ad9bd9e56aa77a6583b6f4e347d66f1fe7b1a2cb196fff53b7634f9dc84 \\\n    --hash=sha256:bf6020560584671e76375b7a0539e0d5388fc70fa183c99dc769895f7ef90233 \\\n    --hash=sha256:c011997f62d0c3b40a617e61b7faaaf6078e4eeff2e95ce4c45838db537816eb \\\n    --hash=sha256:c08311db17647a47d4898fc6f8d9c1f0e58b927752c894877ff0c38b3db0d6e1 \\\n    --hash=sha256:c26bada6cd109095139237a46f50fc4308f861f0d304bc9e70acbc6c4503d158 \\\n    --hash=sha256:c31240e30d5636ded02a54b7280aa129344fe8e964fd63885e85d9a8a83db206 \\\n    --hash=sha256:ce0ae49879d10610cf3c40f4f376bb3cc425b18d939966ac63a2a9c73eb6f32a \\\n    --hash=sha256:ce15a842c2a0bf46180ae136743b561fa276300dd7fa61fe76daf00ec7dc0c2d \\\n    --hash=sha256:ce7c20b9395696d3b5425dccf2706d374e61ccf8f3656bff9423093a6df488f5 \\\n    --hash=sha256:cfc8381df1f0f873da8969729974f90111cfb61a725ef0a2e0e6215408fe1217 \\\n    --hash=sha256:d1dca48687f41efd862355e58b0aa31150586219324901dbea2989a506e291d4 \\\n    --hash=sha256:d3bbe2f925cc587545c8d01587b4523177408edd252a32ce6d61b97113fe234d \\\n    --hash=sha256:d917f5bc27b85611ceee4eb85f0e4088b0a03b4eed22c472409933a94ee953cf \\\n    --hash=sha256:dab84c52f64e10adc79011a08673eb80286c159b14e8fb455524bf2994f0cb38 \\\n    --hash=sha256:de9cfafe97c75cd3ea052a24cd4aabf9fb0cfc3c0f9f810f00121cdf123db9e4 \\\n    --hash=sha256:df35ea436592eb7e30e59c5403ec08ec3a5e7759e270cf226df73c47b3e739f5 \\\n    --hash=sha256:e13cba03c7af8c8a846c4495875a09d64362cc4caeed495ada5390644411bbe7 \\\n    --hash=sha256:e1935d12e503780b859d343161a80df65205d23cad7b4f6c3df6e50321e188a3 \\\n    --hash=sha256:e42258f66ad9f16d9b62e9c9642742982acb1f30b90f5061522048c1cb99814f \\\n    --hash=sha256:e794e44fa260300b8850246c6371d94014753c73528f97f6ccb42f5e7ce698ae \\\n    --hash=sha256:e8638355ae2f996356f7f281e03a3e3ce31f1259510f9d551465356532e0302c \\\n    --hash=sha256:e92878cc05e844c8da937204bc34c2e6caf66709ce5936802fbfb35f04132892 \\\n    --hash=sha256:ff32548e45e45bf3280ac1d28b3148337a5c6714c28db23aeb0693e33eba257e\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jupyter-ydoc\n    #   ypy-websocket\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with SHA256 Hashes\nDESCRIPTION: This code snippet shows a portion of a requirements file that lists Python packages with their versions and SHA256 hashes. Each package includes its version, hash values, and comments indicating which packages depend on it. This format ensures secure and reproducible package installations.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:da70d4d51c8b306bb7a031d5cff6cc25ad253affe89b70352af5f1cb68e74b53 \\\n    --hash=sha256:daf3cb43b7cf2ba96d614252ce1684c1bccee6b2183a01328c98d36fcd7d5cb0 \\\n    --hash=sha256:dca1e2f3ca00b84a396bc1bce13dd21f680f035314d2379c4160c98153b2059b \\\n    --hash=sha256:dd4f49ae60e10adbc94b45c0b5e6a179acc1736cf7a90160b404076ee283cf83 \\\n    --hash=sha256:e1f145462f1fa6e4a4ae3c0f782e580ce44d57c8f2c7aae1b6fa88c0b2efdb41 \\\n    --hash=sha256:e3391d1e16e2a5a1507d83e4a8b100f4ee626e8eca43cf2cadb543de69827c4c \\\n    --hash=sha256:fcd2469d6a2cf298f198f0487e0a5b1a47a42ca0fa4dfd1b6862c999f018ebbf \\\n    --hash=sha256:fd096eb7ffef17c456cfa587523c5f92321ae02427ff955bebe9e3c63bc9f0da \\\n    --hash=sha256:fe754d231288e1e64323cfad462fcee8f0288654c10bdf4f603a39ed923bef33\n    # via sqlalchemy\nh11==0.14.0 \\\n    --hash=sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d \\\n    --hash=sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761\n    # via uvicorn\nhttplib2==0.22.0 \\\n    --hash=sha256:14ae0a53c1ba8f3d37e9e27cf37eabb0fb9980f435ba405d546948b009dd64dc \\\n    --hash=sha256:d7a10bc5ef5ab08322488bde8c726eeee5c8618723fdb399597ec58f3d82df81\n    # via oauth2client\nhumanize==4.9.0 \\\n    --hash=sha256:582a265c931c683a7e9b8ed9559089dea7edcf6cc95be39a3cbc2c5d5ac2bcfa \\\n    --hash=sha256:ce284a76d5b1377fd8836733b983bfb0b76f1aa1c090de2566fcf008d7f6ab16\n    # via anyscale\nidna==3.7 \\\n    --hash=sha256:028ff3aadf0609c1fd278d8ea3089299412a7a8b9bd005dd08b9f8285bcb5cfc \\\n    --hash=sha256:82fee1fc78add43492d3a1898bfa6d8a904cc97d8427f683ed8e798d07761aa0\n    # via\n    #   anyio\n    #   requests\n    #   yarl\nimagesize==1.4.1 \\\n    --hash=sha256:0d8d18d08f840c19d0ee7ca1fd82490fdc3729b7ac93f49870406ddde8ef8d8b \\\n    --hash=sha256:69150444affb9cb0d5cc5a92b3676f0b2fb7cd9ae39e947a5e11a36b4497cd4a\n    # via sphinx\nimportlib-metadata==7.1.0 \\\n    --hash=sha256:30962b96c0c223483ed6cc7280e7f0199feb01a0e40cfae4d4450fc6fab1f570 \\\n    --hash=sha256:b78938b926ee8d5f020fc4772d487045805a55ddbad2ecf21c6d60938dc7fcd2\n    # via\n    #   jupyter-cache\n    #   jupyter-client\n    #   keyring\n    #   myst-nb\n    #   sphinx\n    #   twine\niniconfig==2.0.0 \\\n    --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n    --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n    # via pytest\nipykernel==6.29.5 \\\n    --hash=sha256:afdb66ba5aa354b09b91379bac28ae4afebbb30e8b39510c9690afb7a10421b5 \\\n    --hash=sha256:f093a22c4a40f8828f8e330a9c297cb93dcab13bd9678ded6de8e5cf81c56215\n    # via myst-nb\nipython==8.18.1 \\\n    --hash=sha256:ca6f079bb33457c66e233e4580ebfc4128855b4cf6370dddd73842a9563e8a27 \\\n    --hash=sha256:e8267419d72d81955ec1177f8a29aaa90ac80ad647499201119e2f05e99aa397\n    # via\n    #   ipykernel\n    #   myst-nb\njaraco-classes==3.4.0 \\\n    --hash=sha256:47a024b51d0239c0dd8c8540c6c7f484be3b8fcf0b2d85c13825780d3b3f3acd \\\n    --hash=sha256:f662826b6bed8cace05e7ff873ce0f9283b5c924470fe664fff1c2f00f581790\n    # via keyring\njaraco-context==5.3.0 \\\n    --hash=sha256:3e16388f7da43d384a1a7cd3452e72e14732ac9fe459678773a3608a812bf266 \\\n    --hash=sha256:c2f67165ce1f9be20f32f650f25d8edfc1646a8aeee48ae06fb35f90763576d2\n    # via keyring\njaraco-functools==4.0.1 \\\n    --hash=sha256:3b24ccb921d6b593bdceb56ce14799204f473976e2a9d4b15b04d0f2c2326664 \\\n    --hash=sha256:d33fa765374c0611b52f8b3a795f8900869aa88c84769d4d1746cd68fb28c3e8\n    # via keyring\njedi==0.19.1 \\\n    --hash=sha256:cf0496f3651bc65d7174ac1b7d043eff454892c708a87d1b683e57b569927ffd \\\n    --hash=sha256:e983c654fe5c02867aef4cdfce5a2fbb4a50adc0af145f70504238f18ef5e7e0\n    # via ipython\njeepney==0.8.0 \\\n    --hash=sha256:5efe48d255973902f6badc3ce55e2aa6c5c3b3bc642059ef3a91247bcfcc5806 \\\n    --hash=sha256:c0a454ad016ca575060802ee4d590dd912e35c122fa04e70306de3d076cce755\n    # via\n    #   keyring\n    #   secretstorage\njinja2==3.1.3 \\\n    --hash=sha256:7d6d50dd97d52cbc355597bd845fabfbac3f551e1f99619e39a35ce8c370b5fa \\\n    --hash=sha256:ac8bd6544d4bb2c9792bf3a159e80bba8fda7f07e81bc3aed565432d5925ba90\n    # via\n    #   -r release/requirements_buildkite.in\n    #   myst-parser\n    #   sphinx\n    #   sphinxcontrib-redoc\njmespath==1.0.1 \\\n    --hash=sha256:02e2e4cc71b5bcab88332eebf907519190dd9e6e82107fa7f83b1003a6252980 \\\n    --hash=sha256:90261b206d6defd58fdd5e85f478bf633a2901798906be2ad389150c5c60edbe\n    # via\n    #   boto3\n    #   botocore\njsonpatch==1.33 \\\n    --hash=sha256:0ae28c0cd062bbd8b8ecc26d7d164fbbea9652a1a3693f3b956c1eae5145dade \\\n    --hash=sha256:9fcd4009c41e6d12348b4a0ff2563ba56a2923a7dfee731d004e212e1ee5030c\n    # via anyscale\njsonpointer==2.4 \\\n    --hash=sha256:15d51bba20eea3165644553647711d150376234112651b4f1811022aecad7d7a \\\n    --hash=sha256:585cee82b70211fa9e6043b7bb89db6e1aa49524340dde8ad6b63206ea689d88\n    # via\n    #   jsonpatch\n    #   sphinx-jsonschema\njsonschema==4.23.0 \\\n    --hash=sha256:d71497fef26351a33265337fa77ffeb82423f3ea21283cd9467bb03999266bc4 \\\n    --hash=sha256:fbadb6f8b144a8f8cf9f0b89ba94501d143e50411a1278633f56a7acf7fd5566\n    # via\n    #   -r release/requirements_buildkite.in\n    #   anyscale\n    #   nbformat\n    #   sphinxcontrib-redoc\njsonschema-specifications==2023.12.1 \\\n    --hash=sha256:48a76787b3e70f5ed53f1160d2b81f586e4ca6d1548c5de7085d1682674764cc \\\n    --hash=sha256:87e4fdf3a94858b8a2ba2778d9ba57d8a9cafca7c7489c46ba0d30a8bc6a9c3c\n    # via jsonschema\njupyter-cache==0.6.1 \\\n    --hash=sha256:26f83901143edf4af2f3ff5a91e2d2ad298e46e2cee03c8071d37a23a63ccbfc \\\n    --hash=sha256:2fce7d4975805c77f75bdfc1bc2e82bc538b8e5b1af27f2f5e06d55b9f996a82\n    # via myst-nb\njupyter-client==8.6.2 \\\n    --hash=sha256:2bda14d55ee5ba58552a8c53ae43d215ad9868853489213f37da060ced54d8df \\\n    --hash=sha256:50cbc5c66fd1b8f65ecb66bc490ab73217993632809b6e505687de18e9dea39f\n    # via\n    #   ipykernel\n    #   nbclient\njupyter-core==5.7.2 \\\n    --hash=sha256:4f7315d2f6b4bcf2e3e7cb6e46772eba760ae459cd1f59d29eb57b0a01bd7409 \\\n    --hash=sha256:aa5f8d32bbf6b431ac830496da7392035d6f61b4f54872f15c4bd2a9c3f536d9\n    # via\n    #   ipykernel\n    #   jupyter-client\n    #   nbclient\n    #   nbformat\njupytext==1.15.2 \\\n    --hash=sha256:c9976e24d834e991906c1de55af4b6d512d764f6372aabae45fc1ea72b589173 \\\n    --hash=sha256:ef2a1a3eb8f63d84a3b3772014bdfbe238e4e12a30c4309b8c89e0a54adeb7d1\n    # via -r release/requirements-doc.txt\nkeyring==25.2.0 \\\n    --hash=sha256:19f17d40335444aab84b19a0d16a77ec0758a9c384e3446ae2ed8bd6d53b67a5 \\\n    --hash=sha256:7045f367268ce42dba44745050164b431e46f6e92f99ef2937dfadaef368d8cf\n    # via twine\nlog-symbols==0.0.14 \\\n    --hash=sha256:4952106ff8b605ab7d5081dd2c7e6ca7374584eff7086f499c06edd1ce56dcca \\\n    --hash=sha256:cf0bbc6fe1a8e53f0d174a716bc625c4f87043cc21eb55dd8a740cfe22680556\n    # via anyscale\nmarkdown-it-py==3.0.0 \\\n    --hash=sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1 \\\n    --hash=sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb\n    # via\n    #   jupytext\n    #   mdit-py-plugins\n    #   myst-parser\n    #   rich\nmarkupsafe==2.1.5 \\\n    --hash=sha256:00e046b6dd71aa03a41079792f8473dc494d564611a8f89bbbd7cb93295ebdcf \\\n    --hash=sha256:075202fa5b72c86ad32dc7d0b56024ebdbcf2048c0ba09f1cde31bfdd57bcfff \\\n    --hash=sha256:0e397ac966fdf721b2c528cf028494e86172b4feba51d65f81ffd65c63798f3f \\\n    --hash=sha256:17b950fccb810b3293638215058e432159d2b71005c74371d784862b7e4683f3 \\\n    --hash=sha256:1f3fbcb7ef1f16e48246f704ab79d79da8a46891e2da03f8783a5b6fa41a9532 \\\n    --hash=sha256:2174c595a0d73a3080ca3257b40096db99799265e1c27cc5a610743acd86d62f \\\n    --hash=sha256:2b7c57a4dfc4f16f7142221afe5ba4e093e09e728ca65c51f5620c9aaeb9a617 \\\n    --hash=sha256:2d2d793e36e230fd32babe143b04cec8a8b3eb8a3122d2aceb4a371e6b09b8df \\\n    --hash=sha256:30b600cf0a7ac9234b2638fbc0fb6158ba5bdcdf46aeb631ead21248b9affbc4 \\\n    --hash=sha256:397081c1a0bfb5124355710fe79478cdbeb39626492b15d399526ae53422b906 \\\n    --hash=sha256:3a57fdd7ce31c7ff06cdfbf31dafa96cc533c21e443d57f5b1ecc6cdc668ec7f \\\n    --hash=sha256:3c6b973f22eb18a789b1460b4b91bf04ae3f0c4234a0a6aa6b0a92f6f7b951d4 \\\n    --hash=sha256:3e53af139f8579a6d5f7b76549125f0d94d7e630761a2111bc431fd820e163b8 \\\n    --hash=sha256:4096e9de5c6fdf43fb4f04c26fb114f61ef0bf2e5604b6ee3019d51b69e8c371 \\\n    --hash=sha256:4275d846e41ecefa46e2015117a9f491e57a71ddd59bbead77e904dc02b1bed2 \\\n    --hash=sha256:4c31f53cdae6ecfa91a77820e8b151dba54ab528ba65dfd235c80b086d68a465 \\\n    --hash=sha256:4f11aa001c540f62c6166c7726f71f7573b52c68c31f014c25cc7901deea0b52 \\\n    --hash=sha256:5049256f536511ee3f7e1b3f87d1d1209d327e818e6ae1365e8653d7e3abb6a6 \\\n    --hash=sha256:58c98fee265677f63a4385256a6d7683ab1832f3ddd1e66fe948d5880c21a169 \\\n    --hash=sha256:598e3276b64aff0e7b3451b72e94fa3c238d452e7ddcd893c3ab324717456bad \\\n    --hash=sha256:5b7b716f97b52c5a14bffdf688f971b2d5ef4029127f1ad7a513973cfd818df2 \\\n    --hash=sha256:5dedb4db619ba5a2787a94d877bc8ffc0566f92a01c0ef214865e54ecc9ee5e0 \\\n    --hash=sha256:619bc166c4f2de5caa5a633b8b7326fbe98e0ccbfacabd87268a2b15ff73a029 \\\n    --hash=sha256:629ddd2ca402ae6dbedfceeba9c46d5f7b2a61d9749597d4307f943ef198fc1f \\\n    --hash=sha256:656f7526c69fac7f600bd1f400991cc282b417d17539a1b228617081106feb4a \\\n    --hash=sha256:6ec585f69cec0aa07d945b20805be741395e28ac1627333b1c5b0105962ffced \\\n    --hash=sha256:72b6be590cc35924b02c78ef34b467da4ba07e4e0f0454a2c5907f473fc50ce5 \\\n    --hash=sha256:7502934a33b54030eaf1194c21c692a534196063db72176b0c4028e140f8f32c \\\n    --hash=sha256:7a68b554d356a91cce1236aa7682dc01df0edba8d043fd1ce607c49dd3c1edcf \\\n    --hash=sha256:7b2e5a267c855eea6b4283940daa6e88a285f5f2a67f2220203786dfa59b37e9 \\\n    --hash=sha256:823b65d8706e32ad2df51ed89496147a42a2a6e01c13cfb6ffb8b1e92bc910bb \\\n    --hash=sha256:8590b4ae07a35970728874632fed7bd57b26b0102df2d2b233b6d9d82f6c62ad \\\n    --hash=sha256:8dd717634f5a044f860435c1d8c16a270ddf0ef8588d4887037c5028b859b0c3 \\\n    --hash=sha256:8dec4936e9c3100156f8a2dc89c4b88d5c435175ff03413b443469c7c8c5f4d1 \\\n    --hash=sha256:97cafb1f3cbcd3fd2b6fbfb99ae11cdb14deea0736fc2b0952ee177f2b813a46 \\\n\n```\n\n----------------------------------------\n\nTITLE: Package Requirements List\nDESCRIPTION: Detailed package specifications with version constraints and cryptographic hashes for Python dependencies\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_2\n\nLANGUAGE: pip\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cu124\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html\n\naiofiles==22.1.0 \\\n    --hash=sha256:1142fa8e80dbae46bb6339573ad4c8c0841358f79c6eb50a493dceca14621bad \\\n    --hash=sha256:9107f1ca0b2a5553987a94a3c9959fe5b491fdf731389aa5b7b1bd0733e32de6\n```\n\n----------------------------------------\n\nTITLE: ASCII Table of RL Training Results\nDESCRIPTION: Formatted ASCII table showing training results for different RL algorithms including trial names, status, iterations, total time, timesteps and reward values for the BreakoutNoFrameskip-v4 environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.0/rllib_regression.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n+----------------------------------------+------------+--------------------+--------+------------------+-------------+----------+\n| Trial name                             | status     | loc                |   iter |   total time (s) |   timesteps |   reward |\n|----------------------------------------+------------+--------------------+--------+------------------+-------------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_e25c5ce8 | TERMINATED |                    |    310 |         3604.3   |     9648000 |   399.75 |\n| IMPALA_BreakoutNoFrameskip-v4_e25cdfec | TERMINATED |                    |    310 |         3611.22  |     9552000 |   403.49 |\n| IMPALA_BreakoutNoFrameskip-v4_e25d65de | TERMINATED |                    |    310 |         3604.05  |     9604500 |   390.47 |\n| IMPALA_BreakoutNoFrameskip-v4_e25ddf0a | TERMINATED |                    |    310 |         3604.18  |     9447000 |   365.57 |\n| PPO_BreakoutNoFrameskip-v4_e25e4ddc    | TERMINATED |                    |    604 |         3605.24  |     3020000 |    42.59 |\n| PPO_BreakoutNoFrameskip-v4_e25eb8da    | TERMINATED |                    |    613 |         3603.35  |     3065000 |    50.58 |\n| PPO_BreakoutNoFrameskip-v4_e25f4520    | TERMINATED |                    |    608 |         3600.01  |     3040000 |    51.2  |\n| PPO_BreakoutNoFrameskip-v4_e25fd44a    | TERMINATED |                    |    613 |         3604.97  |     3065000 |    26.19 |\n| APEX_BreakoutNoFrameskip-v4_e2603fa2   | TERMINATED |                    |    115 |         3622.22  |     5169280 |    48.56 |\n| APEX_BreakoutNoFrameskip-v4_e260d462   | TERMINATED |                    |    115 |         3616.47  |     5201280 |    45.01 |\n| APEX_BreakoutNoFrameskip-v4_e2615a90   | TERMINATED |                    |    115 |         3615.15  |     5134080 |    25.02 |\n| APEX_BreakoutNoFrameskip-v4_e261cbec   | TERMINATED |                    |    115 |         3625.55  |     5189280 |    47.01 |\n| A2C_BreakoutNoFrameskip-v4_e26260a2    | TERMINATED |                    |    351 |         3602.43  |     3478000 |   145.84 |\n| A2C_BreakoutNoFrameskip-v4_e26331f8    | TERMINATED |                    |    351 |         3601.93  |     3577000 |   178.73 |\n| A2C_BreakoutNoFrameskip-v4_e2639e68    | TERMINATED |                    |    351 |         3607.28  |     3521500 |   185.45 |\n| A2C_BreakoutNoFrameskip-v4_e2642630    | TERMINATED |                    |    351 |         3606.35  |     3460000 |   122.32 |\n| DQN_BreakoutNoFrameskip-v4_99c588f2    | TERMINATED |                    |     32 |         3643.26  |      320000 |    19.24 |\n| DQN_BreakoutNoFrameskip-v4_99c62de8    | TERMINATED |                    |     31 |         3619.43  |      310000 |    19.97 |\n| DQN_BreakoutNoFrameskip-v4_99c6c01e    | TERMINATED |                    |     33 |         3680.95  |      330000 |    16.91 |\n| DQN_BreakoutNoFrameskip-v4_99c73f44    | TERMINATED |                    |     31 |         3625.12  |      310000 |    16.23 |\n+----------------------------------------+------------+--------------------+--------+------------------+-------------+----------+\n```\n\n----------------------------------------\n\nTITLE: Specifying Numpy Dependency with Hash Verification\nDESCRIPTION: Defines the numpy package dependency with version 1.26.4 and includes multiple SHA256 hash validations for security. The comment indicates this is used by various other packages like cupy-cuda12x, gymnasium, imageio, and others.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy==1.26.4 \\\n    --hash=sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b \\\n    --hash=sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818 \\\n    --hash=sha256:1af303d6b2210eb850fcf03064d364652b7120803a0b872f5211f5234b399f20 \\\n    --hash=sha256:1dda2e7b4ec9dd512f84935c5f126c8bd8b9f2fc001e9f54af255e8c5f16b0e0 \\\n    --hash=sha256:2a02aba9ed12e4ac4eb3ea9421c420301a0c6460d9830d74a9df87efa4912010 \\\n    --hash=sha256:2e4ee3380d6de9c9ec04745830fd9e2eccb3e6cf790d39d7b98ffd19b0dd754a \\\n    --hash=sha256:3373d5d70a5fe74a2c1bb6d2cfd9609ecf686d47a2d7b1d37a8f3b6bf6003aea \\\n    --hash=sha256:47711010ad8555514b434df65f7d7b076bb8261df1ca9bb78f53d3b2db02e95c \\\n    --hash=sha256:4c66707fabe114439db9068ee468c26bbdf909cac0fb58686a42a24de1760c71 \\\n    --hash=sha256:50193e430acfc1346175fcbdaa28ffec49947a06918b7b92130744e81e640110 \\\n    --hash=sha256:52b8b60467cd7dd1e9ed082188b4e6bb35aa5cdd01777621a1658910745b90be \\\n    --hash=sha256:60dedbb91afcbfdc9bc0b1f3f402804070deed7392c23eb7a7f07fa857868e8a \\\n    --hash=sha256:62b8e4b1e28009ef2846b4c7852046736bab361f7aeadeb6a5b89ebec3c7055a \\\n    --hash=sha256:666dbfb6ec68962c033a450943ded891bed2d54e6755e35e5835d63f4f6931d5 \\\n    --hash=sha256:675d61ffbfa78604709862923189bad94014bef562cc35cf61d3a07bba02a7ed \\\n    --hash=sha256:679b0076f67ecc0138fd2ede3a8fd196dddc2ad3254069bcb9faf9a79b1cebcd \\\n    --hash=sha256:7349ab0fa0c429c82442a27a9673fc802ffdb7c7775fad780226cb234965e53c \\\n    --hash=sha256:7ab55401287bfec946ced39700c053796e7cc0e3acbef09993a9ad2adba6ca6e \\\n    --hash=sha256:7e50d0a0cc3189f9cb0aeb3a6a6af18c16f59f004b866cd2be1c14b36134a4a0 \\\n    --hash=sha256:95a7476c59002f2f6c590b9b7b998306fba6a5aa646b1e22ddfeaf8f78c3a29c \\\n    --hash=sha256:96ff0b2ad353d8f990b63294c8986f1ec3cb19d749234014f4e7eb0112ceba5a \\\n    --hash=sha256:9fad7dcb1aac3c7f0584a5a8133e3a43eeb2fe127f47e3632d43d677c66c102b \\\n    --hash=sha256:9ff0f4f29c51e2803569d7a51c2304de5554655a60c5d776e35b4a41413830d0 \\\n    --hash=sha256:a354325ee03388678242a4d7ebcd08b5c727033fcff3b2f536aea978e15ee9e6 \\\n    --hash=sha256:a4abb4f9001ad2858e7ac189089c42178fcce737e4169dc61321660f1a96c7d2 \\\n    --hash=sha256:ab47dbe5cc8210f55aa58e4805fe224dac469cde56b9f731a4c098b91917159a \\\n    --hash=sha256:afedb719a9dcfc7eaf2287b839d8198e06dcd4cb5d276a3df279231138e83d30 \\\n    --hash=sha256:b3ce300f3644fb06443ee2222c2201dd3a89ea6040541412b8fa189341847218 \\\n    --hash=sha256:b97fe8060236edf3662adfc2c633f56a08ae30560c56310562cb4f95500022d5 \\\n    --hash=sha256:bfe25acf8b437eb2a8b2d49d443800a5f18508cd811fea3181723922a8a82b07 \\\n    --hash=sha256:cd25bcecc4974d09257ffcd1f098ee778f7834c3ad767fe5db785be9a4aa9cb2 \\\n    --hash=sha256:d209d8969599b27ad20994c8e41936ee0964e6da07478d6c35016bc386b66ad4 \\\n    --hash=sha256:d5241e0a80d808d70546c697135da2c613f30e28251ff8307eb72ba696945764 \\\n    --hash=sha256:edd8b5fe47dab091176d21bb6de568acdd906d1887a4584a15a9a96a1dca06ef \\\n    --hash=sha256:f870204a840a60da0b12273ef34f7051e98c3b5961b61b0c2c1be6dfd64fbcd3 \\\n    --hash=sha256:ffa75af20b44f8dba823498024771d5ac50620e6915abac414251bd971b4529f\n```\n\n----------------------------------------\n\nTITLE: Tracing Netty NIO Event Loop with xen_clocksource_get_cycles in Java and Kernel\nDESCRIPTION: A stack trace showing the complete flow from Java thread creation through Netty's NIO event loop processing to kernel-level network operations, ending with Xen's clock cycle retrieval.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_27\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ktime_get_real_[k];xen_clocksource_get_cycles_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Deploying Frozen VMs from OVF Template in vSphere for Ray\nDESCRIPTION: YAML configuration for deploying frozen VMs from an OVF template in vSphere for use with Ray. This setup specifies the VM name prefix, library item (template), resource pool, and datastore.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nfrozen_vm:\n    name: frozen-vm-prefix\n    library_item: frozen-vm-template\n    resource_pool: frozen-vm-resource-pool\n    datastore: vsanDatastore\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements List\nDESCRIPTION: Comprehensive list of Python package dependencies with version specifications and dependency annotations. Includes packages for machine learning (TensorFlow, PyTorch), data processing (Pandas, NumPy), cloud services (AWS, GCP), and development tools.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled.txt#2025-04-12_snippet_1\n\nLANGUAGE: pip\nCODE:\n```\ndnspython==2.4.2\ndocker==6.1.3\ndocker-pycreds==0.4.0\ndocstring-parser==0.15\ndocutils==0.19\ndulwich==0.21.6\necdsa==0.18.0\nentrypoints==0.4\net-xmlfile==1.1.0\netils==1.5.2 ; python_version < \"3.12\"\nevaluate==0.4.3\neverett==3.1.0\nexecnet==2.1.1\nexecuting==2.0.1\nface==22.0.0\nfairscale==0.4.6\nfarama-notifications==0.0.4\nfastapi==0.115.0\nfastavro==1.9.4\nfasteners==0.19\nfastjsonschema==2.19.0\nfastrlock==0.8.2\nfeather-format==0.4.1\nffmpy==0.3.1\nfilelock==3.17.0\nfiletype==1.2.0\nflask==2.1.3\nflask-cors==4.0.0\nflatbuffers==23.5.26\nfonttools==4.45.1\nfqdn==1.5.1\nfreezegun==1.1.0\nfrozenlist==1.4.1\nfs==2.4.16\nfsspec==2023.5.0\nfugue==0.8.7\nfugue-sql-antlr==0.2.0\nfuture==1.0.0\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Package dependency specifications with pinned versions and SHA256 hashes for security verification. Includes dependencies for Jupyter, AWS, and other Python packages with their specific versions and file hashes.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nargon2-cffi==23.1.0 \\\n    --hash=sha256:879c3e79a2729ce768ebb7d36d4609e3a78a4ca2ec3a9f12286ca057e3d0db08 \\\n    --hash=sha256:c670642b78ba29641818ab2e68bd4e6a78ba53b7eff7b4c3815ae16abf91c7ea\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray ID Bit Layouts\nDESCRIPTION: Visual representation of the bit layouts for JobID, ActorID, TaskID, and ObjectID in Ray. It shows the composition and size of each ID type using ASCII art.\nSOURCE: https://github.com/ray-project/ray/blob/master/src/ray/design_docs/id_specification.md#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n high bits                                                                           low bits\n<------------------------------------------------------------------------------------------\n\n                                                                                 4B\n                                                                        +-----------------+\n                                                                        |   unique bytes  |  JobID     4B\n                                                                        +-----------------+\n\n                                                               12B                4B\n                                                      +-----------------+-----------------+\n                                                      |   unique bytes  |      JobID      |  ActorID   16B\n                                                      +-----------------+-----------------+\n\n                                   8B                                   16B\n                  +-----------------------------------+-----------------------------------+\n                  |           unique bytes            |              ActorID              |  TaskID   24B\n                  +-----------------------------------+-----------------------------------+\n\n                                   24B                                          4B\n+-----------------------------------------------------------------------+-----------------+\n|                                 TaskID                                |   index bytes   |  ObjectID 28B\n+-----------------------------------------------------------------------+-----------------+\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with SHA256 Hashes\nDESCRIPTION: Pinned versions of Python package dependencies with their corresponding SHA256 hashes for secure installation verification. Each package includes its version and multiple hash values for different distributions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\ncharset-normalizer==3.3.2 \\\n    --hash=sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027 \\\n    --hash=sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087\n\nclick==8.1.7 \\\n    --hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28 \\\n    --hash=sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de\n\ncloudpickle==2.2.0 \\\n    --hash=sha256:3f4219469c55453cfe4737e564b67c2a149109dabf7f242478948b895f61106f \\\n    --hash=sha256:7428798d5926d8fcbfd092d18d01a2a03daf8237d8fcdc8095d256b8490796f0\n\ncryptography==42.0.5 \\\n    --hash=sha256:0270572b8bd2c833c3981724b8ee9747b3ec96f699a9665470018594301439ee \\\n    --hash=sha256:111a0d8553afcf8eb02a4fea6ca4f59d48ddb34497aa8706a6cf536f1a5ec576\n```\n\n----------------------------------------\n\nTITLE: Checking AWS Environment and Exiting SSH Session\nDESCRIPTION: This snippet shows a terminal session where AWS environment settings are checked, the server's IP address is displayed, and the SSH session is terminated with the exit command.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_attach.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nChecking AWS environment settings\nFetched IP: .+\nubuntu@ip-.+:~$ exit\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies Hash List\nDESCRIPTION: List of Python package dependencies including cupy-cuda12x, debugpy, decorator, defusedxml, distlib, and dm-tree along with their SHA256 hash values for package verification. Each package entry includes version constraints and multiple hash values.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:1f71c10d1e88467126f0efd484bd44bca5e14c664ec2ede64c32f20875c0d413 \\\n--hash=sha256:2424ff4c4ac7f6b8177b53c17ed5d8fa74ae5955656867f5a8affaca36a27abb \\\n# Additional hashes omitted for brevity...\n```\n\n----------------------------------------\n\nTITLE: Specifying openai-whisper Package Requirement\nDESCRIPTION: Defines the required version and hash value for the openai-whisper package. This ensures a specific, verified version is installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_16\n\nLANGUAGE: Text\nCODE:\n```\nopenai-whisper==20231117 \\\n    --hash=sha256:7af424181436f1800cc0b7d75cf40ede34e9ddf1ba4983a910832fcf4aade4a4\n```\n\n----------------------------------------\n\nTITLE: Defining Security Package Version Constraints\nDESCRIPTION: Specifies minimum version requirements for idna and certifi packages. These constraints are used for security purposes when compiling the constraint file, but the packages may not be actual project dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/security-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nidna>=3.7\ncertifi>=2025.1.31\n```\n\n----------------------------------------\n\nTITLE: Specifying TensorBoardX Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the tensorboardx package dependency with version 2.6.2.2 and SHA-256 hashes for verification. Comments indicate this package is required by requirements_byod_3.9.in and is constrained by requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_47\n\nLANGUAGE: pip\nCODE:\n```\ntensorboardx==2.6.2.2 \\\n    --hash=sha256:160025acbf759ede23fd3526ae9d9bfbfd8b68eb16c38a010ebe326dc6395db8 \\\n    --hash=sha256:c6476d7cd0d529b0b72f4acadb1269f9ed8b22f441e87a84f2a3b940bb87b666\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n```\n\n----------------------------------------\n\nTITLE: Specifying Pandas Library with Multiple Hash Verifications\nDESCRIPTION: This snippet shows how to pin a specific Pandas version with extensive hash verification. The comments indicate this is a dependency specified in the main Ray requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\npandas==1.5.3 \\\n    --hash=sha256:14e45300521902689a81f3f41386dc86f19b8ba8dd5ac5a3c7010ef8d2932813 \\\n    --hash=sha256:26d9c71772c7afb9d5046e6e9cf42d83dd147b5cf5bcb9d97252077118543792 \\\n    --hash=sha256:3749077d86e3a2f0ed51367f30bf5b82e131cc0f14260c4d3e499186fccc4406 \\\n    --hash=sha256:41179ce559943d83a9b4bbacb736b04c928b095b5f25dd2b7389eda08f46f373 \\\n    --hash=sha256:478ff646ca42b20376e4ed3fa2e8d7341e8a63105586efe54fa2508ee087f328 \\\n    --hash=sha256:50869a35cbb0f2e0cd5ec04b191e7b12ed688874bd05dd777c19b28cbea90996 \\\n    --hash=sha256:565fa34a5434d38e9d250af3c12ff931abaf88050551d9fbcdfafca50d62babf \\\n    --hash=sha256:5f2b952406a1588ad4cad5b3f55f520e82e902388a6d5a4a91baa8d38d23c7f6 \\\n    --hash=sha256:5fbcb19d6fceb9e946b3e23258757c7b225ba450990d9ed63ccceeb8cae609f7 \\\n    --hash=sha256:6973549c01ca91ec96199e940495219c887ea815b2083722821f1d7abfa2b4dc \\\n    --hash=sha256:74a3fd7e5a7ec052f183273dc7b0acd3a863edf7520f5d3a1765c04ffdb3b0b1 \\\n    --hash=sha256:7a0a56cef15fd1586726dace5616db75ebcfec9179a3a55e78f72c5639fa2a23 \\\n    --hash=sha256:7cec0bee9f294e5de5bbfc14d0573f65526071029d036b753ee6507d2a21480a \\\n    --hash=sha256:87bd9c03da1ac870a6d2c8902a0e1fd4267ca00f13bc494c9e5a9020920e1d51 \\\n    --hash=sha256:972d8a45395f2a2d26733eb8d0f629b2f90bebe8e8eddbb8829b180c09639572 \\\n    --hash=sha256:9842b6f4b8479e41968eced654487258ed81df7d1c9b7b870ceea24ed9459b31 \\\n    --hash=sha256:9f69c4029613de47816b1bb30ff5ac778686688751a5e9c99ad8c7031f6508e5 \\\n    --hash=sha256:a50d9a4336a9621cab7b8eb3fb11adb82de58f9b91d84c2cd526576b881a0c5a \\\n    --hash=sha256:bc4c368f42b551bf72fac35c5128963a171b40dce866fb066540eeaf46faa003 \\\n    --hash=sha256:c39a8da13cede5adcd3be1182883aea1c925476f4e84b2807a46e2775306305d \\\n    --hash=sha256:c3ac844a0fe00bfaeb2c9b51ab1424e5c8744f89860b138434a363b1f620f354 \\\n    --hash=sha256:c4c00e0b0597c8e4f59e8d461f797e5d70b4d025880516a8261b2817c47759ee \\\n    --hash=sha256:c74a62747864ed568f5a82a49a23a8d7fe171d0c69038b38cedf0976831296fa \\\n    --hash=sha256:dd05f7783b3274aa206a1af06f0ceed3f9b412cf665b7247eacd83be41cf7bf0 \\\n    --hash=sha256:dfd681c5dc216037e0b0a2c821f5ed99ba9f03ebcf119c7dac0e9a7b960b9ec9 \\\n    --hash=sha256:e474390e60ed609cec869b0da796ad94f420bb057d86784191eefc62b65819ae \\\n    --hash=sha256:f76d097d12c82a535fda9dfe5e8dd4127952b45fea9b0276cb30cca5ea313fbc\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying pydantic Dependency\nDESCRIPTION: Defines the pydantic package dependency with version 2.9.2 and two hash values. This ensures a specific, verified version of pydantic is used in the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_31\n\nLANGUAGE: Text\nCODE:\n```\npydantic==2.9.2 \\\n    --hash=sha256:d155cef71265d1e9807ed1c32b4c8deec042a44a50a4188b25ac67ecd81a9c0f \\\n    --hash=sha256:f048cec7b26778210e28a0459867920654d48e5e62db0958433636cde4254f12\n```\n\n----------------------------------------\n\nTITLE: Incorrect Argument Placement in Ray Job Submission\nDESCRIPTION: Illustrates incorrect placement of arguments in a Ray job submission command. The working directory argument is placed after the entrypoint command, which is not supported.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/cli.rst#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nray job submit -- python script.py --working-dir=\".\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Results of RLlib Experiments using Python\nDESCRIPTION: This snippet prints the final hyperparameters and performance metrics of the best-performing trial from a population-based training (PBT) session. It utilizes the pprint module for formatted output, fetching the best trial's configuration and metrics from the results object. This snippet does not require additional dependencies other than those already imported within the context of using Ray Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_ppo_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pprint\n\nbest_result = results.get_best_result()\n\nprint(\"Best performing trial's final set of hyperparameters:\\n\")\npprint.pprint(\n    {k: v for k, v in best_result.config.items() if k in hyperparam_mutations}\n)\n\nprint(\"\\nBest performing trial's final reported metrics:\\n\")\n\nmetrics_to_print = [\n    \"episode_reward_mean\",\n    \"episode_reward_max\",\n    \"episode_reward_min\",\n    \"episode_len_mean\",\n]\npprint.pprint({k: v for k, v in best_result.metrics.items() if k in metrics_to_print})\n\n```\n\n----------------------------------------\n\nTITLE: Installing Latest Ray Wheels for Python Development (Bash)\nDESCRIPTION: Command to install the latest Ray wheels for Python 3.9 development without building from source.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp39-cp39-manylinux2014_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Checking Python README Format\nDESCRIPTION: This shell script checks the Python README format using setup.py to ensure restructured text compliance and strict metadata validation. It requires navigating to the 'python' directory before execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncd python\npython setup.py check --restructuredtext --strict --metadata\n```\n\n----------------------------------------\n\nTITLE: AWS Environment Configuration Check\nDESCRIPTION: Shows the AWS configuration details including IAM profile, EC2 key pair, VPC subnets, security groups, and AMI settings for the cluster deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_up.txt#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nAWS config\n  IAM Profile: .+ [default]\n  EC2 Key pair (all available node types): .+ [default]\n  VPC Subnets (all available node types): subnet-.+ [default]\n  EC2 Security groups (all available node types): sg-.+ [default]\n  EC2 AMI (all available node types): ami-.+ [dlami]\n```\n\n----------------------------------------\n\nTITLE: Modifying Main Function for Ray Compatibility\nDESCRIPTION: Changes to the main function to accept configuration parameters from Ray instead of parsing command-line arguments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/sd.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef main(config):\n    args = config[\"args\"]\n```\n\n----------------------------------------\n\nTITLE: Displaying Best Hyperparameters - Python\nDESCRIPTION: This snippet prints out the best hyperparameters discovered during the tuning process for minimizing the mean loss of the objective function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Java Stack Trace - Request Processing Path\nDESCRIPTION: Complete stack trace showing the execution path from Java thread creation through Netty NIO event processing to Vert.x HTTP request handling and JavaScript runtime operations. The trace demonstrates the layered architecture of the server implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_59\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.getPropFunctionAndThis_[j]\n```\n\n----------------------------------------\n\nTITLE: Converting ObjectRefs to concurrent.futures.Futures in Ray\nDESCRIPTION: Demonstrates how to wrap Ray ObjectRefs into concurrent.futures.Future objects for compatibility with existing concurrent.futures APIs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/async_api.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport concurrent\n\nrefs = [some_task.remote() for _ in range(4)]\nfuts = [ref.future() for ref in refs]\nfor fut in concurrent.futures.as_completed(futs):\n    assert fut.done()\n    print(fut.result())\n```\n\n----------------------------------------\n\nTITLE: Merging LoRA Weights\nDESCRIPTION: Command to merge LoRA fine-tuned weights with original model weights\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/04_finetuning_llms_with_deepspeed/README.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython merge_lora_weights.py --model-name=7b --checkpoint=<path to your checkpoint> --output-path=<desired output path>\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry Exporter OTLP Proto gRPC Package with Hash Verification in Bash\nDESCRIPTION: Defines the OpenTelemetry exporter OTLP Proto gRPC package version 1.1.0 with SHA256 hash verification for secure package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-exporter-otlp-proto-grpc==1.1.0 \\\n    --hash=sha256:281e9bbce73b08c1c93781cf7f4282396f74895987fdc051bea335f7dd086199 \\\n    --hash=sha256:5a4a86becf4f9fdf2910a5b869fc40ec9978044f93045fdce240fecb6c64681a\n```\n\n----------------------------------------\n\nTITLE: Specifying Pickleshare Package with Hash Verification\nDESCRIPTION: Defines the pickleshare package version 0.7.5 with SHA256 hash verification and notes that it's required by ipython.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_28\n\nLANGUAGE: text\nCODE:\n```\npickleshare==0.7.5 \\\n    --hash=sha256:87683d47965c1da65cdacaf31c8441d12b8044cdec9aca500cd78fc2c683afca \\\n    --hash=sha256:9649af414d74d4df115d5d718f82acb59c9d418196b7b4290ed47a12ce62df56\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ipython\n```\n\n----------------------------------------\n\nTITLE: Java Netty NIO Event Processing with Nested JavaScript Generated File Execution\nDESCRIPTION: Stack trace showing the execution path with nested calls to the same generated JavaScript file. This trace illustrates how JavaScript modules are executed during the HTTP request processing flow.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_53\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Package Dependency Hash Listings for Ray Project\nDESCRIPTION: A comprehensive list of Python package dependencies with their versions and SHA256 hash values. This ensures package integrity during installation and pinpoints exact versions for reproducible environments. Each entry includes comments indicating which parent packages require these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_20\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:2e203fdf807ac7e12ab59ca2bfcabb38c7cf0b33c41efeb00f8e5da1d86af480 \\\n--hash=sha256:33e3d65a85a2a4a0dc3b092b938a4062b1a05f3a9abde65ea93b233bca0e03f2 \\\n--hash=sha256:374a5e5049eda9e0a44c696c7ade3ff355f06b1fe0bb945ea3cac2bc336478a2 \\\n--hash=sha256:37b0fe330e4a58d3c58b24d91d1eb102aeec675a3db4c292ec3928ecd892a9a6 \\\n--hash=sha256:3d5639516376dce1940ea36edf408c554475369f5da2abd45d44621cb616f769 \\\n--hash=sha256:42c6dcb030aefb668a2b7009c85b27f90e51e6a3b4d5c9bc4c57631292015b0d \\\n--hash=sha256:4a7cd62e831afe623fbb7aabbb4fe583212115b3ef38a9f6b71869ba644624a2 \\\n--hash=sha256:4ba762ed58e8d68657fc1281e9bb72e1c3e79cc5d464be146e260c541ec12d84 \\\n--hash=sha256:4fc714bdbfb534f94034efaa6eadd74e5b93c8fa6315565a222f7b6f42ca1166 \\\n--hash=sha256:4ffa2ebd4c8530079140dd2d7f794a9d9a73cbb8e9d59ffe24c63436efa8f271 \\\n--hash=sha256:5a1504ad17ba4210df3a045132a7baeeba5a200e930f57512ee02909fc5c4cb5 \\\n--hash=sha256:5c364564d17da23db1106787675fc7af45f2f7b58b4173bfdd105564e132e6fb \\\n--hash=sha256:5e11661ce0fd30a6790e8bcdf263b9ec5988e95e63cf901972107efc49218b13 \\\n--hash=sha256:5f54b118ce5de9ac21c363d9b3caa6c800341e8c47a508787e5868c6b79c9323 \\\n--hash=sha256:5f5ff8d839f4566a474a969508fe1c5e59c31c80d9e140566f9a37bba7b8d556 \\\n--hash=sha256:61817945f2fe7d166e75fbfb28004034b48e44878177fc54d81688e7b85a3665 \\\n--hash=sha256:624e278a7d29b6445e4e813af92af37820fafb6dcc55c012c834f9e26f9aaaef \\\n--hash=sha256:63e46b3169866bd62849936de036f901a9356e36376079b05efa83caeaa02ceb \\\n--hash=sha256:6531b7ca5f951d663c339002e91aaebda765ec7d61b7d1e3991051906ddde119 \\\n--hash=sha256:68665f4c17edcceecc112dfed5dbe6f92261fb9d6054b47d01bf6371a6196126 \\\n--hash=sha256:696dd8d674d6ce621ab9d45b205df149399e4bb9aa34102c970b721554828510 \\\n--hash=sha256:6f783e0ec4803c787bcea93e13e9932edab72068f68ecffdf86a99fd5918878b \\\n--hash=sha256:723314c1d51722ab28bfcd5240d858512ffd3116449c557a1336cbe3919beb87 \\\n--hash=sha256:74b9127ffea03643e998e0c5ad9bd3811d3dac8c676e47db17b0ee7c3c3bf35f \\\n--hash=sha256:7530e201d10d7d14abce4fb54cfe5b94a0aefc87da539d0346a484ead376c3cc \\\n--hash=sha256:77733e3892bb0a7fa797826361ce8a9184d25c8dffaec60b7ffe928153680ba8 \\\n--hash=sha256:78ddaaa81421a29574a682b3179d4cf9e6d405a09b99d93ddcf7e5239c742e21 \\\n--hash=sha256:7c9129eb40958b3d4500fa2467e6a83356b3b61bfff1b414c7361d9220f9ae8f \\\n--hash=sha256:7d32706badfe136888bdea71c0def994644e09fff0bfe47441deaed8e96fdbc6 \\\n--hash=sha256:81965a16b675b35e1d09dd14df53f190f9129c0202356ed44ab2728b1c905658 \\\n--hash=sha256:8394d940e5d400d04cad4f75c0598665cbb81aecefaca82ca85bd28264af7f9b \\\n--hash=sha256:86d2f57d3e1379a9525c5ab067b27dbb8a0642fb5d454e17a9ac434f9ce523e3 \\\n--hash=sha256:883a91b5dd7d26492ff2f04f40fbb652de40fcc0afe07e8129e8ae779c2110eb \\\n--hash=sha256:88ad334a15b32a791ea935af224b9de1bf99bcd62fabf745d5f3442199d86d59 \\\n--hash=sha256:9261d3ce84fa1d38ed649c3638feefeae23d32ba9182963e465d58d62203bd24 \\\n--hash=sha256:97df63000f4fea395b2824da80e169731088656d1818a11b95f3b173747b6cd9 \\\n--hash=sha256:98d134c954828488b153d88ba1f34e14259284f256180ce659e8d83e9c05eaa3 \\\n--hash=sha256:996a38a83508c54c78a5f41456b0103c30508fed9abcad0a59b876d7398f25fd \\\n--hash=sha256:9a5bce9d23aac8f0cf0836ecfc033896aa8443b501c58d0602dbfd5bd5b37753 \\\n--hash=sha256:9a6b5099eeec78827553827f4c6b8615978bb4b6a88e5d9b93eddf8bb6790f55 \\\n--hash=sha256:9d18368b137c6295db49ce7218b1a9ba15c5bc254c96d7c9f9e924a9bc7825ad \\\n--hash=sha256:a4fa4fc04dff799089689f4fd502ce7d59de529fc2f40a2c8836886c03e0175a \\\n--hash=sha256:a5c7ba8ffb6d6f8f2ab08743be203654bb1aaa8c9dcb09f82ddd34eadb695605 \\\n--hash=sha256:aea443fffa9fbe3af1a9ba721a87f926fe548d32cab71d188a6ede77d0ff244e \\\n--hash=sha256:b10bd51f823d891193d4717448fab065733958bdb6a6b351967bd349d48d5c9b \\\n--hash=sha256:ba1a0996f6c2773bd83e63f18914c1de3c9dd26d55f4ac302a7efe93fb8e7433 \\\n--hash=sha256:bb2802e667b7051a1bebbfe93684841cc9351004e2badbd6411bf357ab8d5ac8 \\\n--hash=sha256:cfdd16ab5e59fc31b5e906d1a3f666571abc367598e3e02c83403acabc092e07 \\\n--hash=sha256:d06b0c8da4f16d1d1e352134427cb194a0a6e19ad5db9161bf32b2113409e728 \\\n--hash=sha256:d0776dea117cf5272382634bd2a5c1b6eb16767c223c6a5317cd3e2a757c61a0 \\\n--hash=sha256:d18ca8148bebe1b0a382a27a8ee60350091a6ddaf475fa05ef50dc35b5df6327 \\\n--hash=sha256:d4488a93b071c04dc20f5cecc3631fc78b9789dd72483ba15d423b5b3689b555 \\\n--hash=sha256:d5f7a395a8cf1621939692dba2a6b6a830efa6b3cee787d82c7de1ad2930de64 \\\n--hash=sha256:d7a80d21d613eec45e3d41eb22f8f94ddc758a6c4720842dc74c0581f54993d6 \\\n--hash=sha256:d97683ddee4723ae8c95d1eddac7c192e8c552da0c73a925a89fa8649bf13eea \\\n--hash=sha256:dcedcd19a557e182628afa1d553c3895a9f825b936415d0dbd3cd0bbcfd29b4b \\\n--hash=sha256:de6d1d1b9e5101508cb37ab0d972357cac5235f5c6533d1071964c47139257df \\\n--hash=sha256:df49e7a0861a8c36d089c1ed57d308623d60416dab2647a4a17fe050ba85de0e \\\n--hash=sha256:df933278128ea1cd77772673c73954e53a1c95a4fdf41eef97c2b779271bd0bd \\\n--hash=sha256:e08277a400de01bc72436a0ccd02bdf596631411f592ad985dcee21445bd0068 \\\n--hash=sha256:e38e63e6f3d1cec5a27e0afe90a085af8b6806ee208b33030e65b6516353f1a3 \\\n--hash=sha256:e55541f756f9b3ee346b840103f32779c695a19826a4c442b7954550a0972040 \\\n--hash=sha256:ec4e55f79b1c4ffb2eecd8a0cfba9955a2588497d96851f4c8f99aa4a1d39b12 \\\n--hash=sha256:ed1a53de42fbe34853ba90513cea21673481cd81ed1be739f7f2efb931b24916 \\\n--hash=sha256:ed541d70698978a20eb63d8c5d72f2cc6d7079d9d90f6b50bad07826f1320f5f \\\n--hash=sha256:f09e2ff1f17c2b51f2bc76d1cc33da96298f0a036a137f5440ab3ec5360b624f \\\n--hash=sha256:f220b0eea5965dec25480b6333c788fb72ce5f9129e8759ef876a1d805d00801 \\\n--hash=sha256:f3e0da4ebaef65158d4dfd7d3678aad692f7666877df0002b8a522cdf088f231 \\\n--hash=sha256:f455ee30a9d61d3e1a15abd5068827773d6e4dc513e795f380cdd59932c782d5 \\\n--hash=sha256:f5ef8f42bec47f21d07668a043f077d507e5bf4e668d5c6dfe6aaba89de1a5b8 \\\n--hash=sha256:f69a8e0b033b747bb3e36a44e7732f0c99f7edd5cea723d45bc0d6e95377ffee \\\n--hash=sha256:ff02b6d461a6de369f07ec15e465a88895f3223eb75073ffea56b84d9331f607\n# via\n#   -c release/ray_release/byod/requirements_compiled.txt\n#   pydantic\n```\n\n----------------------------------------\n\nTITLE: Analyzing Netty/Vert.x HTTP Request Processing with ScriptableObject.getSlot\nDESCRIPTION: Call stack trace showing the Java execution path from thread start through Netty's NIO event processing to Vert.x HTTP handlers and Mozilla JavaScript runtime. This specific trace ends with ScriptRuntime.setObjectProp and ScriptableObject.getSlot for property lookup.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_72\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/ScriptableObject:.getSlot_[j] 1\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Requirements for Ray Project\nDESCRIPTION: Package requirements for OpenTelemetry libraries used in the Ray project for observability and monitoring. Each entry includes version constraints, SHA256 hashes, and dependency information.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_18\n\nLANGUAGE: requirements.txt\nCODE:\n```\nopentelemetry-api==1.1.0 \\\n    --hash=sha256:38555cd773df903a2f7440778d6f8b48a86fd388604b171969bdbde4b746a558 \\\n    --hash=sha256:704a3b2a7511d2c9065013d362a8371bc452ae6c0521941de680af2a5ca94884\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   opentelemetry-sdk\nopentelemetry-exporter-otlp==1.1.0 \\\n    --hash=sha256:2a2135f87cdad417408d34fc6131879d5cee1d7af7546b4a1f67fd178b262f4e \\\n    --hash=sha256:61ee0a6e9a12dd7191aedca34a8a3e7cc4e8e92504a71adf390b6d2bcc36d0d4\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\nopentelemetry-exporter-otlp-proto-grpc==1.1.0 \\\n    --hash=sha256:281e9bbce73b08c1c93781cf7f4282396f74895987fdc051bea335f7dd086199 \\\n    --hash=sha256:5a4a86becf4f9fdf2910a5b869fc40ec9978044f93045fdce240fecb6c64681a\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   opentelemetry-exporter-otlp\nopentelemetry-proto==1.1.0 \\\n    --hash=sha256:237ef4fdd7f752b2fe740352643f8ef82733bd8e0db8b46ed808125ac7c7f112 \\\n    --hash=sha256:ff1ad9a3c572075883c2af0053cefdfaba005d71eade783c4524d34660d53b60\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\nopentelemetry-sdk==1.1.0 \\\n    --hash=sha256:ba29274aab656572e97e0339afaad6f2bded4102324b1475ab7412079498df6e \\\n    --hash=sha256:da7dfa6188e8a39f34b99495260e6a1d398c86a9de064c7f0805db6f16733d94\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\nopentelemetry-semantic-conventions==0.20b0 \\\n    --hash=sha256:ecae7367203e5204c70518e6d24b438480d6a6f1e5c8ee9dc2145f176ff4452e \\\n    --hash=sha256:fac014ac2098b1a05fe58af77cbe74c825ff869d6d53d316c393cc77f507ec15\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack with TCP Data Receive Flow\nDESCRIPTION: Stack trace showing data reception path in Netty with kernel-level TCP processing. The trace extends through the TCP/IP stack and shows how received data flows from network device through kernel TCP implementation to the Java application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_16\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];tcp_v4_do_rcv_[k];tcp_event_data_recv_[k] 1\n```\n\n----------------------------------------\n\nTITLE: JVM Parallel Garbage Collection with Object Liveness Check\nDESCRIPTION: Thread stack trace showing JVM parallel garbage collection with focus on checking if objects are still alive. This trace captures the mark phase of garbage collection where live objects are identified.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_114\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;VMThread::run;VMThread::loop;VMThread::evaluate_operation;VM_Operation::evaluate;VM_ParallelGCFailedAllocation::doit;ParallelScavengeHeap::failed_mem_allocate;PSScavenge::invoke;PSScavenge::invoke_no_policy;PSIsAliveClosure::do_object_b\n```\n\n----------------------------------------\n\nTITLE: Creating RayJob Instances for Gang Scheduling Demo\nDESCRIPTION: Commands to create two RayJob instances with the same priority to demonstrate gang scheduling behavior with Kueue.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f ray-job.kueue-toy-sample.yaml\nkubectl create -f ray-job.kueue-toy-sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Header Splitting\nDESCRIPTION: Java stack trace showing Netty's HTTP header splitting process. This trace follows the HTTP decoder's header processing to the point of splitting header name-value pairs, with a frequency count of 5 indicating high occurrence.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_105\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/handler/codec/http/HttpObjectDecoder:.decode_[j];io/netty/handler/codec/http/HttpObjectDecoder:.readHeaders_[j];io/netty/handler/codec/http/HttpObjectDecoder:.splitHeader_[j] 5\n```\n\n----------------------------------------\n\nTITLE: Specifying Six Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the six package dependency with version 1.16.0 and SHA-256 hashes for verification. Comments show this package is required by multiple dependencies including astunparse, tensorflow, google-pasta, and more.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_40\n\nLANGUAGE: pip\nCODE:\n```\nsix==1.16.0 \\\n    --hash=sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926 \\\n    --hash=sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   astunparse\n    #   gcs-oauth2-boto-plugin\n    #   geventhttpclient\n    #   google-apitools\n    #   google-pasta\n    #   gsutil\n    #   oauth2client\n    #   petastorm\n    #   python-dateutil\n    #   pyu2f\n    #   tensorboard\n    #   tensorflow\n    #   trueskill\n```\n\n----------------------------------------\n\nTITLE: Netty TCP File System Notification Stack Trace\nDESCRIPTION: Call stack trace showing Netty's data path from NIO event processing to filesystem notification during write operations. This trace documents how network I/O in Netty triggers filesystem notification events in the kernel.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_36\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];fsnotify_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Setting PreLearner Pool Size\nDESCRIPTION: Configuration for setting the actor pool size in the Post-Processing PreLearner layer using concurrency parameter.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .offline_data(\n        map_batches_kwargs={\n            \"concurrency\": 4,\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Key Configuration for RayCluster YAML with GKE Workload Identity\nDESCRIPTION: The essential configuration for a RayCluster YAML manifest to use Workload Identity in GKE, specifying the Kubernetes service account and enabling GKE metadata server.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n      spec:\n        serviceAccountName: my-ksa\n        nodeSelector:\n          iam.gke.io/gke-metadata-server-enabled: \"true\"\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies with SHA256 Hashes\nDESCRIPTION: This snippet shows the format of package dependencies in a requirements file with SHA256 hash values for verification. It lists the 'referencing' package version 0.36.2 with its hash values and comments indicating which packages depend on it.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_44\n\nLANGUAGE: plaintext\nCODE:\n```\nreferencing==0.36.2 \\\n    --hash=sha256:df2e89862cd09deabbdba16944cc3f10feb6b3e6f18e902f7cc25609a34775aa \\\n    --hash=sha256:e8699adbbf8b5c7de96d8ffa0eb5c158b3beafce084968e2ea8bb08c6794dcd0\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   jsonschema\n    #   jsonschema-specifications\n    #   outlines\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements List\nDESCRIPTION: Lists required Python packages and their specific versions needed for the Ray project. Includes ML frameworks like PyTorch, Hugging Face Transformers, and Diffusers, along with utility libraries for visualization and API development.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/testing/docker/03_serving_stable_diffusion/requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate==0.20.3\ndiffusers==0.17.1\nfastapi==0.97.0\nipywidgets\nmatplotlib==3.7.1\nnumpy==1.24.3\ntorch==2.0.1\ntransformers==4.30.1\n```\n\n----------------------------------------\n\nTITLE: Long-Running Ray Cluster on Spark Setup\nDESCRIPTION: Example demonstrating how to create a persistent Ray cluster on Spark that can be accessed by remote processes. The cluster runs until the Spark application is terminated.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/spark.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\nimport time\nfrom ray.util.spark import setup_ray_cluster, MAX_NUM_WORKER_NODES\n\nif __name__ == \"__main__\":\n    spark = SparkSession \\\n        .builder \\\n        .appName(\"long running ray cluster on spark\") \\\n        .config(\"spark.task.cpus\", \"4\") \\\n        .getOrCreate()\n\n    cluster_address = setup_ray_cluster(\n        max_worker_nodes=MAX_NUM_WORKER_NODES\n    )\n    print(\"Ray cluster is set up, you can connect to this ray cluster \"\n          f\"via address ray://{cluster_address}\")\n\n    while True:\n        time.sleep(10)\n```\n\n----------------------------------------\n\nTITLE: Netty TCP Connection with Established Options Stack Trace\nDESCRIPTION: Call stack trace showing Netty NIO event loop processing through channel handlers to TCP socket write operations with focus on established TCP options. The trace documents the flow from Java through JVM to kernel TCP connection handling for established connections.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_34\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];tcp_send_mss_[k];tcp_current_mss_[k];tcp_established_options_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Defining OpenTelemetry Dependencies\nDESCRIPTION: Specifies the OpenTelemetry packages required by the Ray project with exact version pins and hash verification. Includes the API, SDK, exporters, and related components with dependency annotations.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-api==1.1.0 \\\n    --hash=sha256:38555cd773df903a2f7440778d6f8b48a86fd388604b171969bdbde4b746a558 \\\n    --hash=sha256:704a3b2a7511d2c9065013d362a8371bc452ae6c0521941de680af2a5ca94884\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\n    #   opentelemetry-sdk\nopentelemetry-exporter-otlp==1.1.0 \\\n    --hash=sha256:2a2135f87cdad417408d34fc6131879d5cee1d7af7546b4a1f67fd178b262f4e \\\n    --hash=sha256:61ee0a6e9a12dd7191aedca34a8a3e7cc4e8e92504a71adf390b6d2bcc36d0d4\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\nopentelemetry-exporter-otlp-proto-grpc==1.1.0 \\\n    --hash=sha256:281e9bbce73b08c1c93781cf7f4282396f74895987fdc051bea335f7dd086199 \\\n    --hash=sha256:5a4a86becf4f9fdf2910a5b869fc40ec9978044f93045fdce240fecb6c64681a\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   opentelemetry-exporter-otlp\nopentelemetry-proto==1.1.0 \\\n    --hash=sha256:237ef4fdd7f752b2fe740352643f8ef82733bd8e0db8b46ed808125ac7c7f112 \\\n    --hash=sha256:ff1ad9a3c572075883c2af0053cefdfaba005d71eade783c4524d34660d53b60\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\nopentelemetry-sdk==1.1.0 \\\n    --hash=sha256:ba29274aab656572e97e0339afaad6f2bded4102324b1475ab7412079498df6e \\\n    --hash=sha256:da7dfa6188e8a39f34b99495260e6a1d398c86a9de064c7f0805db6f16733d94\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\nopentelemetry-semantic-conventions==0.20b0 \\\n    --hash=sha256:ecae7367203e5204c70518e6d24b438480d6a6f1e5c8ee9dc2145f176ff4452e \\\n    --hash=sha256:fac014ac2098b1a05fe58af77cbe74c825ff869d6d53d316c393cc77f507ec15\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Annotating Kubernetes Service Account with IAM Service Account\nDESCRIPTION: Adds an annotation to the Kubernetes service account to specify which IAM service account it should use for GCP API access.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl annotate serviceaccount my-ksa \\\n    --namespace default \\\n    iam.gke.io/gcp-service-account=my-iam-sa@my-project-id.iam.gserviceaccount.com\n```\n\n----------------------------------------\n\nTITLE: HTTP Query Example - Console\nDESCRIPTION: Example of querying the deployed model using curl command\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/batch.md#2025-04-12_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ curl \"http://localhost:8000/?text=Once+upon+a+time\"\n```\n\n----------------------------------------\n\nTITLE: Specifying numexpr Package Requirement\nDESCRIPTION: Defines the required version and hash values for the numexpr package. This ensures a specific, verified version is installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_11\n\nLANGUAGE: Text\nCODE:\n```\nnumexpr==2.8.4 \\\n    --hash=sha256:059546e8f6283ccdb47c683101a890844f667fa6d56258d48ae2ecf1b3875957 \\\n    --hash=sha256:17ac9cfe6d0078c5fc06ba1c1bbd20b8783f28c6f475bbabd3cad53683075cab \\\n    --hash=sha256:3f039321d1c17962c33079987b675fb251b273dbec0f51aac0934e932446ccc3\n```\n\n----------------------------------------\n\nTITLE: Specifying Remote S3 Working Directory\nDESCRIPTION: Example of specifying an AWS S3 URI as a working directory in a runtime environment dictionary\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nruntime_env = {..., \"working_dir\": \"s3://example_bucket/example.zip\", ...}\n```\n\n----------------------------------------\n\nTITLE: Specifying Protobuf Dependency for Ray Project\nDESCRIPTION: This snippet defines the protobuf package dependency with version 3.20.3 and includes multiple hash verifications. The package is required by several components including Google API Core, gRPC tools, OpenTelemetry Proto, and TensorBoardX.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_27\n\nLANGUAGE: pip\nCODE:\n```\nprotobuf==3.20.3 \\\n    --hash=sha256:03038ac1cfbc41aa21f6afcbcd357281d7521b4157926f30ebecc8d4ea59dcb7 \\\n    --hash=sha256:28545383d61f55b57cf4df63eebd9827754fd2dc25f80c5253f9184235db242c \\\n    --hash=sha256:2e3427429c9cffebf259491be0af70189607f365c2f41c7c3764af6f337105f2 \\\n    --hash=sha256:398a9e0c3eaceb34ec1aee71894ca3299605fa8e761544934378bbc6c97de23b \\\n    --hash=sha256:44246bab5dd4b7fbd3c0c80b6f16686808fab0e4aca819ade6e8d294a29c7050 \\\n    --hash=sha256:447d43819997825d4e71bf5769d869b968ce96848b6479397e29fc24c4a5dfe9 \\\n    --hash=sha256:67a3598f0a2dcbc58d02dd1928544e7d88f764b47d4a286202913f0b2801c2e7 \\\n    --hash=sha256:74480f79a023f90dc6e18febbf7b8bac7508420f2006fabd512013c0c238f454 \\\n    --hash=sha256:819559cafa1a373b7096a482b504ae8a857c89593cf3a25af743ac9ecbd23480 \\\n    --hash=sha256:899dc660cd599d7352d6f10d83c95df430a38b410c1b66b407a6b29265d66469 \\\n    --hash=sha256:8c0c984a1b8fef4086329ff8dd19ac77576b384079247c770f29cc8ce3afa06c \\\n    --hash=sha256:9aae4406ea63d825636cc11ffb34ad3379335803216ee3a856787bcf5ccc751e \\\n    --hash=sha256:a7ca6d488aa8ff7f329d4c545b2dbad8ac31464f1d8b1c87ad1346717731e4db \\\n    --hash=sha256:b6cc7ba72a8850621bfec987cb72623e703b7fe2b9127a161ce61e61558ad905 \\\n    --hash=sha256:bf01b5720be110540be4286e791db73f84a2b721072a3711efff6c324cdf074b \\\n    --hash=sha256:c02ce36ec760252242a33967d51c289fd0e1c0e6e5cc9397e2279177716add86 \\\n    --hash=sha256:d9e4432ff660d67d775c66ac42a67cf2453c27cb4d738fc22cb53b5d84c135d4 \\\n    --hash=sha256:daa564862dd0d39c00f8086f88700fdbe8bc717e993a21e90711acfed02f2402 \\\n    --hash=sha256:de78575669dddf6099a8a0f46a27e82a1783c557ccc38ee620ed8cc96d3be7d7 \\\n    --hash=sha256:e64857f395505ebf3d2569935506ae0dfc4a15cb80dc25261176c784662cdcc4 \\\n    --hash=sha256:f4bd856d702e5b0d96a00ec6b307b0f51c1982c2bf9c0052cf9019e9a544ba99 \\\n    --hash=sha256:f4c42102bc82a51108e449cbb32b19b180022941c727bac0cfd50170341f16ee\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\n    #   google-api-core\n    #   googleapis-common-protos\n    #   grpcio-tools\n    #   opentelemetry-proto\n    #   tensorboardx\n```\n\n----------------------------------------\n\nTITLE: Defining pyyaml Package Requirements with Hash Verification\nDESCRIPTION: Specifies pyyaml package version 6.0.1 with SHA256 hash verification. Used by multiple dependencies including huggingface-hub, jupyter-events, ray and others.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_41\n\nLANGUAGE: text\nCODE:\n```\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Ray Dashboard Access\nDESCRIPTION: Command to set up port forwarding to access the Ray dashboard from a browser, which requires authorization headers to be added.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward svc/ray-cluster-with-auth-head-svc 8265:8265 &\n```\n\n----------------------------------------\n\nTITLE: Updating Ray Cluster Image to Include ML Dependencies\nDESCRIPTION: Commands to modify the Ray cluster to use the ray-ml image that includes TensorFlow and other machine learning dependencies required by the MobileNet classifier.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# Uninstall RayCluster\nhelm uninstall raycluster\n\n# Install the RayCluster CR with the Ray image `rayproject/ray-ml:${RAY_VERSION}`\nhelm install raycluster kuberay/ray-cluster --version 1.3.0 --set image.repository=rayproject/ray-ml\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Processing with JavaScript Runtime\nDESCRIPTION: Stack trace showing the flow from Netty NIO event processing through Vert.x handlers to Mozilla JavaScript runtime execution. Demonstrates the handling of channel reads and event propagation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_67\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.createFunctionActivation_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j]\n```\n\n----------------------------------------\n\nTITLE: Ray Project Documentation in RST\nDESCRIPTION: RST formatted documentation that outlines contribution guidelines, communication channels, and PR review processes for the Ray project. Includes links to resources and detailed steps for both internal and external contributors.\nSOURCE: https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nContributing to Ray\n===================\n\nReporting bugs and asking questions\n-----------------------------------\n\nYou can post questions or issues or feedback through the following channels:\n\n1. `Discourse forum`_: For discussions about development and questions about usage.\n2. `GitHub Issues`_: For bug reports and feature requests.\n3. `StackOverflow`_\n\nTo contribute a patch:\n----------------------\n\nWe welcome contributions! See `Getting Involved`_. To set up your development environment, see\nthe `Setting up your development environment`_ section.\n\n\n.. _`Discourse forum`: https://discuss.ray.io/\n.. _`GitHub Issues`: https://github.com/ray-project/ray/issues\n.. _`StackOverflow`: https://stackoverflow.com/questions/tagged/ray\n.. _`Getting Involved`: https://docs.ray.io/en/latest/ray-contribute/getting-involved.html\n.. _`Setting up your development environment`: https://docs.ray.io/en/latest/ray-contribute/getting-involved.html#setting-up-your-development-environment\n\nPR Review Process\n-----------------\n\nFor contributors who are in the ray-project organization:\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n- When you first create a PR, add a reviewer to the `assignee` section.\n- Assignees will review your PR and add `@author-action-required` label if further actions are required.\n- Address their comments and remove `@author-action-required` label from the PR.\n- Repeat this process until assignees approve your PR.\n- Once the PR is approved, the author is in charge of ensuring the PR passes the build. Add `test-ok` label if the build succeeds.\n- Committers will merge the PR once the build is passing.\n\nFor contributors who are not in the ray-project organization:\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n- Your PRs will have assignees shortly. Assignees or PRs will be actively engaging with contributors to merge the PR.\n- Please actively ping assignees after you address your comments!\n```\n\n----------------------------------------\n\nTITLE: Accessing PyTorch Lightning Checkpoints in Ray Train\nDESCRIPTION: This example demonstrates how to retrieve and load PyTorch Lightning checkpoint files from a Ray Train checkpoint.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pytorch_lightning as pl\n\ncheckpoint = result.checkpoint\nwith checkpoint.as_directory() as ckpt_dir:\n    ckpt_path = os.path.join(ckpt_dir, \"checkpoint.ckpt\")\n    model = YourLightningModule.load_from_checkpoint(ckpt_path)\n```\n\n----------------------------------------\n\nTITLE: OAuth and OpenAI Integration Dependencies\nDESCRIPTION: OAuth2 client and OpenAI API integration libraries required for cloud authentication and AI capabilities in Ray. The OAuth2 client supports Google Cloud integration while the OpenAI package enables using OpenAI models through Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_20\n\nLANGUAGE: pip\nCODE:\n```\noauth2client==4.1.3 \\\n    --hash=sha256:b8a81cc5d60e2d364f0b1b98f958dbd472887acaf1a5b05e21c28c31a2d6d3ac \\\n    --hash=sha256:d486741e451287f69568a4d26d70d9acd73a2bbfa275746c535b4209891cccc6\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/cloud-requirements.txt\nopenai==1.63.2 \\\n    --hash=sha256:1f38b27b5a40814c2b7d8759ec78110df58c4a614c25f182809ca52b080ff4d4 \\\n    --hash=sha256:aeabeec984a7d2957b4928ceaa339e2ead19c61cfcf35ae62b7c363368d26360\n    # via vllm\n```\n\n----------------------------------------\n\nTITLE: Defining an Evaluation Function in Python\nDESCRIPTION: This snippet defines a simple evaluation function that simulates a long-running machine learning experiment by including a sleep time. It takes hyperparameters as input and returns a computed score based on the inputs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(step, width, height, activation):\n    time.sleep(0.1)\n    activation_boost = 10 if activation==\"relu\" else 0\n    return (0.1 + width * step / 100) ** (-1) + height * 0.1 + activation_boost\n```\n\n----------------------------------------\n\nTITLE: Constraining PyTorch Dependencies for Ray ML\nDESCRIPTION: This configuration specifies torch and torchvision as required packages while constraining their versions using an external requirements file. The constraint ensures version consistency between driver and Ray ML cluster environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ml_user_tests/horovod/driver_requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../../../python/requirements/ml/dl-cpu-requirements.txt\n\ntorch\ntorchvision\n```\n\n----------------------------------------\n\nTITLE: Tracing Netty NIO Event Loop with pvclock_clocksource_read in Java and Kernel\nDESCRIPTION: A stack trace showing the complete flow from Java thread creation through Netty's NIO event loop processing to kernel-level network operations. The trace ends with clock operations in the kernel, specifically at pvclock_clocksource_read.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_25\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ktime_get_real_[k];getnstimeofday_[k];xen_clocksource_get_cycles_[k];pvclock_clocksource_read_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Deploying RayJob for Distributed Training\nDESCRIPTION: This command deploys the modified RayJob configuration to the Kubernetes cluster, initiating the distributed PyTorch training job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f ray-job.pytorch-image-classifier.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring YuniKorn Queue Resources in ConfigMap\nDESCRIPTION: YAML configuration for creating a queue with specified CPU and memory resources in YuniKorn. This defines a queue named 'root.test' with 4 CPUs and 6Gi RAM capacity.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  # Metadata for the ConfigMap, skip for brevity.\ndata:\n  queues.yaml: |\n    partitions:\n      - name: default\n        queues:\n          - name: root\n            queues:\n              - name: test\n                submitacl: \"*\"\n                parent: false\n                resources:\n                  guaranteed:\n                    memory: 6G\n                    vcore: 4\n                  max:\n                    memory: 6G\n                    vcore: 4\n```\n\n----------------------------------------\n\nTITLE: Manual RayCluster Finalizer Removal\nDESCRIPTION: Command to manually remove the RayCluster finalizer when automatic cleanup fails.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nkubectl patch rayclusters.ray.io raycluster-external-redis --type json --patch='[ { \"op\": \"remove\", \"path\": \"/metadata/finalizers\" } ]'\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray\nDESCRIPTION: Initializes Ray by shutting down any existing Ray instance and starting a new one.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nif ray.is_initialized():\n    ray.shutdown()\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Storing Ray Benchmark Results in JSON\nDESCRIPTION: This JSON object stores performance metrics for various Ray operations, including get/put calls, actor interactions, and task execution. It includes data for both single and multi-client scenarios, as well as synchronous and asynchronous operations. The metrics are represented as arrays with two values: likely the mean and standard deviation of the measurements.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/microbenchmark.txt#2025-04-12_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"single_client_get_calls\": [\n    34647.91400708946,\n    311.7390971967917\n  ],\n  \"single_client_put_calls\": [\n    58969.83872190603,\n    869.618205663433\n  ],\n  \"multi_client_put_calls\": [\n    199832.5755298421,\n    2482.9205035774476\n  ],\n  \"single_client_get_calls_Plasma_Store\": [\n    7082.757370159696,\n    146.62873820799672\n  ],\n  \"single_client_put_calls_Plasma_Store\": [\n    6321.65654587901,\n    11.077913617295936\n  ],\n  \"multi_client_put_calls_Plasma_Store\": [\n    9186.218655830648,\n    112.23231532820908\n  ],\n  \"single_client_put_gigabytes\": [\n    20.299125005168346,\n    5.063681202623047\n  ],\n  \"single_client_tasks_and_get_batch\": [\n    13.14018865978927,\n    0.3152301478634011\n  ],\n  \"multi_client_put_gigabytes\": [\n    36.56441662881655,\n    1.843382220404724\n  ],\n  \"single_client_get_object_containing_10k_refs\": [\n    10.351906653488715,\n    0.23442465466734483\n  ],\n  \"single_client_tasks_sync\": [\n    1257.4155346823063,\n    16.879731074181798\n  ],\n  \"single_client_tasks_async\": [\n    13436.707639489237,\n    467.0229967004351\n  ],\n  \"multi_client_tasks_async\": [\n    37893.82918345513,\n    2501.210898297811\n  ],\n  \"1_1_actor_calls_sync\": [\n    2018.517206134362,\n    4.133444448098185\n  ],\n  \"1_1_actor_calls_async\": [\n    5107.498479502846,\n    155.05763494606228\n  ],\n  \"1_1_actor_calls_concurrent\": [\n    4974.868578485068,\n    46.89895438701842\n  ],\n  \"1_n_actor_calls_async\": [\n    13035.656413458306,\n    263.67959962428176\n  ],\n  \"n_n_actor_calls_async\": [\n    42424.91241384691,\n    909.2063842725172\n  ],\n  \"n_n_actor_calls_with_arg_async\": [\n    2910.8727809194884,\n    142.55651461439174\n  ],\n  \"1_1_async_actor_calls_sync\": [\n    1434.0111494545497,\n    15.145616176257736\n  ],\n  \"1_1_async_actor_calls_async\": [\n    3227.631490168903,\n    74.52309737428871\n  ],\n  \"1_1_async_actor_calls_with_args_async\": [\n    2417.18007329992,\n    42.010241468147406\n  ],\n  \"1_n_async_actor_calls_async\": [\n    13212.476889889944,\n    280.91562344862103\n  ],\n  \"n_n_async_actor_calls_async\": [\n    32212.030653578477,\n    4172.2556150359205\n  ],\n  \"client__get_calls\": [\n    1518.5267029642152,\n    18.33838666361156\n  ],\n  \"client__put_calls\": [\n    869.7170835067376,\n    8.603084105450836\n  ],\n  \"client__put_gigabytes\": [\n    0.11768745420143228,\n    0.002542373184018965\n  ],\n  \"client__tasks_and_put_batch\": [\n    58861.12144186892,\n    546.7701167395176\n  ],\n  \"client__1_1_actor_calls_sync\": [\n    472.8343418119895,\n    6.16968890867776\n  ],\n  \"client__1_1_actor_calls_async\": [\n    742.6478263697102,\n    2.886810073788351\n  ],\n  \"client__1_1_actor_calls_concurrent\": [\n    729.3572241473628,\n    19.903703549912592\n  ],\n  \"client__tasks_and_get_batch\": [\n    0.6990944804839968,\n    0.00738047968242822\n  ],\n  \"_runtime\": 558.9188287258148,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_AHVUzrAzUMiLZ4p9EEAbL68s\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Netty TCP Stack Trace with SRCU Read Lock\nDESCRIPTION: Call stack trace showing Netty's NIO event processing path with focus on the kernel's SRCU (Sleepable Read-Copy Update) read locking mechanism during filesystem notification. This trace illustrates synchronization mechanisms used during network I/O operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_37\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];fsnotify_[k];__srcu_read_lock_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Java NIO Event Loop Processing and Network I/O Stack Trace\nDESCRIPTION: This stack trace shows the execution path of a Java NIO event loop, including Netty framework processing, channel operations, and system-level write calls. It demonstrates the flow from high-level Java code down to native system calls for network I/O.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_5\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];  3\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];Java_sun_nio_ch_FileDispatcherImpl_write0 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;sys_write_[k] 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];fget_light_[k] 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];__srcu_read_lock_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Specifying ptyprocess Dependency with OS Conditional\nDESCRIPTION: This snippet defines the ptyprocess package dependency with version 0.7.0, conditionally applied for non-Windows platforms. The package is required by pexpect and terminado for handling pseudo-terminal processes in Unix-like environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_29\n\nLANGUAGE: pip\nCODE:\n```\nptyprocess==0.7.0 ; os_name != 'nt' or sys_platform != 'win32' \\\n    --hash=sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35 \\\n    --hash=sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   pexpect\n    #   terminado\n```\n\n----------------------------------------\n\nTITLE: Including HyperBand Function Example in RST Documentation\nDESCRIPTION: This RST directive includes the hyperband_function_example.py file from the Ray Tune examples directory. The directive makes the example code accessible within the documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/hyperband_function_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. literalinclude:: /../../python/ray/tune/examples/hyperband_function_example.py\n```\n\n----------------------------------------\n\nTITLE: Tracing Netty NIO Event Loop with socket buffer allocation in Java and Kernel\nDESCRIPTION: A stack trace showing the complete flow from Java thread creation through Netty's NIO event loop processing to kernel-level network operations, ending with socket buffer allocation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_30\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];sk_stream_alloc_skb_[k];__alloc_skb_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Channel Write Operation Stack Trace\nDESCRIPTION: Stack trace showing complete flow of a Netty NIO channel write operation from the event loop through to kernel system calls. Demonstrates the layers involved in processing network I/O from Java through to the Linux kernel.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_38\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];rw_verify_area_[k]\n```\n\n----------------------------------------\n\nTITLE: Java Netty NIO Event Processing with JavaScript IdScriptable Property Access\nDESCRIPTION: Stack trace showing the execution path to JavaScript IdScriptable property access. This trace shows how the JavaScript engine accesses properties on IdScriptableObject instances during HTTP request handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_58\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.name_[j];org/mozilla/javascript/IdScriptableObject:.get_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Deploying Ray Serve LLM Configuration\nDESCRIPTION: Bash command to deploy a Ray Serve LLM configuration using a YAML file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/llm/serving-llms.rst#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nserve run config.yaml\n```\n\n----------------------------------------\n\nTITLE: Specifying Pillow Package with Hash Verification in Bash\nDESCRIPTION: Defines the Pillow package version 10.3.0 with multiple SHA256 hash verifications for secure package installation. This is an image processing library.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    --hash=sha256:0ea2a783a2bdf2a561808fe4a7a12e9aa3799b701ba305de596bc48b8bdfce9d \\\n    --hash=sha256:1530e8f3a4b965eb6a7785cf17a426c779333eb62c9a7d1bbcf3ffd5bf77a4aa \\\n    --hash=sha256:16563993329b79513f59142a6b02055e10514c1a8e86dca8b48a893e33cf91e3 \\\n    --hash=sha256:19aeb96d43902f0a783946a0a87dbdad5c84c936025b8419da0a0cd7724356b1 \\\n    --hash=sha256:1a1d1915db1a4fdb2754b9de292642a39a7fb28f1736699527bb649484fb966a \\\n    --hash=sha256:1b87bd9d81d179bd8ab871603bd80d8645729939f90b71e62914e816a76fc6bd\n```\n\n----------------------------------------\n\nTITLE: Testing Process Signal Handling\nDESCRIPTION: Demonstrates the implementation of process signal handling using os.kill() to send a SIGUSR1 signal to the current process.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport os, signal\n\nos.kill(os.getpid(), signal.SIGUSR1)  # Terminate the current process~\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and SHA256 hashes for security. It includes examples for packages like bleach, boto3, and botocore.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nbleach==6.1.0 \\\n    --hash=sha256:0a31f1837963c41d46bbf1331b8778e1308ea0791db03cc4e7357b97cf42a8fe \\\n    --hash=sha256:3225f354cfc436b9789c66c4ee030194bee0568fbf9cbdad3bc8b5c26c5f12b6\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   nbconvert\nboto3==1.26.76 \\\n    --hash=sha256:30c7d967ed1c6b5a05643e42cae9d4d36c3f1cb6782637ddc7007a104cfd9027 \\\n    --hash=sha256:b4c2969b7677762914394b8273cc1905dfe5b71f250741c1a575487ae357e729\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements/cloud-requirements.txt\nbotocore==1.29.76 \\\n    --hash=sha256:70735b00cd529f152992231ca6757e458e5ec25db43767b3526e9a35b2f143b7 \\\n    --hash=sha256:c2f67b6b3f8acf2968eafca06526f07b9fb0d27bac4c68a635d51abb675134a7\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   boto3\n    #   s3transfer\n```\n\n----------------------------------------\n\nTITLE: Package Requirement: lz4 with Hashes\nDESCRIPTION: Specifies the lz4 package version 4.3.3 with multiple SHA-256 hashes for verification. This package is referenced in both the main requirements.txt file and the compiled rayllm test requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nlz4==4.3.3 \\\n    --hash=sha256:01fe674ef2889dbb9899d8a67361e0c4a2c833af5aeb37dd505727cf5d2a131e \\\n    --hash=sha256:054b4631a355606e99a42396f5db4d22046a3397ffc3269a348ec41eaebd69d2 \\\n    --hash=sha256:0a136e44a16fc98b1abc404fbabf7f1fada2bdab6a7e970974fb81cf55b636d0 \\\n    --hash=sha256:0e9c410b11a31dbdc94c05ac3c480cb4b222460faf9231f12538d0074e56c563 \\\n    --hash=sha256:222a7e35137d7539c9c33bb53fcbb26510c5748779364014235afc62b0ec797f \\\n    --hash=sha256:24b3206de56b7a537eda3a8123c644a2b7bf111f0af53bc14bed90ce5562d1aa \\\n    --hash=sha256:2b901c7784caac9a1ded4555258207d9e9697e746cc8532129f150ffe1f6ba0d \\\n    --hash=sha256:2f7b1839f795315e480fb87d9bc60b186a98e3e5d17203c6e757611ef7dcef61 \\\n    --hash=sha256:30e8c20b8857adef7be045c65f47ab1e2c4fabba86a9fa9a997d7674a31ea6b6 \\\n    --hash=sha256:31ea4be9d0059c00b2572d700bf2c1bc82f241f2c3282034a759c9a4d6ca4dc2 \\\n    --hash=sha256:337cb94488a1b060ef1685187d6ad4ba8bc61d26d631d7ba909ee984ea736be1 \\\n    --hash=sha256:33c9a6fd20767ccaf70649982f8f3eeb0884035c150c0b818ea660152cf3c809 \\\n    --hash=sha256:363ab65bf31338eb364062a15f302fc0fab0a49426051429866d71c793c23394 \\\n    --hash=sha256:43cf03059c0f941b772c8aeb42a0813d68d7081c009542301637e5782f8a33e2 \\\n    --hash=sha256:56f4fe9c6327adb97406f27a66420b22ce02d71a5c365c48d6b656b4aaeb7775 \\\n    --hash=sha256:5d35533bf2cee56f38ced91f766cd0038b6abf46f438a80d50c52750088be93f \\\n    --hash=sha256:6756212507405f270b66b3ff7f564618de0606395c0fe10a7ae2ffcbbe0b1fba \\\n    --hash=sha256:6cdc60e21ec70266947a48839b437d46025076eb4b12c76bd47f8e5eb8a75dcc \\\n    --hash=sha256:abc197e4aca8b63f5ae200af03eb95fb4b5055a8f990079b5bdf042f568469dd \\\n    --hash=sha256:b14d948e6dce389f9a7afc666d60dd1e35fa2138a8ec5306d30cd2e30d36b40c \\\n    --hash=sha256:b47839b53956e2737229d70714f1d75f33e8ac26e52c267f0197b3189ca6de24 \\\n    --hash=sha256:b6d9ec061b9eca86e4dcc003d93334b95d53909afd5a32c6e4f222157b50c071 \\\n    --hash=sha256:b891880c187e96339474af2a3b2bfb11a8e4732ff5034be919aa9029484cd201 \\\n    --hash=sha256:bca8fccc15e3add173da91be8f34121578dc777711ffd98d399be35487c934bf \\\n    --hash=sha256:c81703b12475da73a5d66618856d04b1307e43428a7e59d98cfe5a5d608a74c6 \\\n    --hash=sha256:d2507ee9c99dbddd191c86f0e0c8b724c76d26b0602db9ea23232304382e1f21 \\\n    --hash=sha256:e36cd7b9d4d920d3bfc2369840da506fa68258f7bb176b8743189793c055e43d \\\n    --hash=sha256:e7d84b479ddf39fe3ea05387f10b779155fc0990125f4fb35d636114e1c63a2e \\\n    --hash=sha256:eac9af361e0d98335a02ff12fb56caeb7ea1196cf1a49dbf6f17828a131da807 \\\n    --hash=sha256:edfd858985c23523f4e5a7526ca6ee65ff930207a7ec8a8f57a01eae506aaee7 \\\n    --hash=sha256:ee9ff50557a942d187ec85462bb0960207e7ec5b19b3b48949263993771c6205 \\\n    --hash=sha256:f0e822cd7644995d9ba248cb4b67859701748a93e2ab7fc9bc18c599a52e4604 \\\n    --hash=sha256:f180904f33bdd1e92967923a43c22899e303906d19b2cf8bb547db6653ea6e7d \\\n    --hash=sha256:f1d18718f9d78182c6b60f568c9a9cec8a7204d7cb6fad4e511a2ef279e4cb05 \\\n    --hash=sha256:f4c7bf687303ca47d69f9f0133274958fd672efaa33fb5bcde467862d6c621f0 \\\n    --hash=sha256:f76176492ff082657ada0d0f10c794b6da5800249ef1692b35cf49b1e93e8ef7\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of Authenticated Remote URI in Python\nDESCRIPTION: Example of an insecure way to include credentials in a remote URI for a private GitHub repository, which should be avoided.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/runtime_env_auth.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nruntime_env = {\"working_dir\": (\n        \"https://username:personal_access_token@github.com/\"\n        \"username/repo/archive/refs/heads/master.zip\"\n    )\n}\n```\n\n----------------------------------------\n\nTITLE: Running Clang-Tidy for C++ Linting\nDESCRIPTION: This shell script checks C++ code using clang-tidy for linting, requiring clang and clang-tidy version 12 to be installed. It ensures the code follows specified C++ guidelines.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n./ci/lint/check-git-clang-tidy-output.sh\n```\n\n----------------------------------------\n\nTITLE: Enabling Full Tracebacks in Ray AIR\nDESCRIPTION: RAY_AIR_FULL_TRACEBACKS when set to 1 enables printing of full tracebacks for training functions, including internal code paths. Defaults to 0 (disabled).\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nRAY_AIR_FULL_TRACEBACKS=1\n```\n\n----------------------------------------\n\nTITLE: Warning Admonition for Ingress Security\nDESCRIPTION: A warning message emphasizing the importance of exposing ingresses only to authorized users due to the Ray Dashboard's read and write access to the Ray Cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{admonition} Warning\n:class: warning\n**Only expose Ingresses to authorized users.** The Ray Dashboard provides read and write access to the Ray Cluster. Anyone with access to this Ingress can execute arbitrary code on the Ray Cluster.\n```\n```\n\n----------------------------------------\n\nTITLE: Checking Pending PodGroup Status\nDESCRIPTION: Command to check the status of the second RayCluster's PodGroup showing that it's pending due to insufficient resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get podgroup ray-test-cluster-1-pg -o yaml\n\n# apiVersion: scheduling.volcano.sh/v1beta1\n# kind: PodGroup\n# metadata:\n#   creationTimestamp: \"2022-12-01T04:48:18Z\"\n#   generation: 2\n#   name: ray-test-cluster-1-pg\n#   namespace: test\n#   ownerReferences:\n#   - apiVersion: ray.io/v1alpha1\n#     blockOwnerDeletion: true\n#     controller: true\n#     kind: RayCluster\n#     name: test-cluster-1\n#     uid: b3cf83dc-ef3a-4bb1-9c42-7d2a39c53358\n#   resourceVersion: \"4427976\"\n#   uid: 9087dd08-8f48-4592-a62e-21e9345b0872\n# spec:\n#   minMember: 3\n#   minResources:\n#     cpu: \"3\"\n#     memory: 4Gi\n#   queue: kuberay-test-queue\n# status:\n#   conditions:\n#   - lastTransitionTime: \"2022-12-01T04:48:19Z\"\n#     message: '3/3 tasks in gang unschedulable: pod group is not ready, 3 Pending,\n#       3 minAvailable; Pending: 3 Undetermined'\n#     reason: NotEnoughResources\n#     status: \"True\"\n#     transitionID: 3956b64f-fc52-4779-831e-d379648eecfc\n#     type: Unschedulable\n#   phase: Pending\n```\n\n----------------------------------------\n\nTITLE: Disabling New Console Output in Ray AIR\nDESCRIPTION: RAY_AIR_NEW_OUTPUT when set to 0 disables the experimental new console output feature in Ray AIR.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nRAY_AIR_NEW_OUTPUT=0\n```\n\n----------------------------------------\n\nTITLE: Specifying pydantic-core Dependency\nDESCRIPTION: Defines the pydantic-core package dependency with version 2.23.4 and multiple hash values. This ensures a specific, verified version of pydantic-core is used in the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_32\n\nLANGUAGE: Text\nCODE:\n```\npydantic-core==2.23.4 \\\n    --hash=sha256:0a7df63886be5e270da67e0966cf4afbae86069501d35c8c1b3b6c168f42cb36 \\\n    --hash=sha256:0cb3da3fd1b6a5d0279a01877713dbda118a2a4fc6f0d821a57da2e464793f05 \\\n    --hash=sha256:0dbd8dbed2085ed23b5c04afa29d8fd2771674223135dc9bc937f3c09284d071\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray Project Experiment Results Table\nDESCRIPTION: This snippet shows a formatted table of experiment results for various reinforcement learning algorithms on the Breakout game. It includes details such as trial name, status, iterations, total time, timesteps, and reward for each completed trial.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.6/rllib_regression.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n+-------------------------------------------+------------+-------+--------+------------------+---------+----------+\n| Trial name                                | status     | loc   |   iter |   total time (s) |      ts |   reward |\n|-------------------------------------------+------------+-------+--------+------------------+---------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_926d2_00000 | TERMINATED |       |    357 |          3601.43 | 7523500 |   315.74 |\n| IMPALA_BreakoutNoFrameskip-v4_926d2_00001 | TERMINATED |       |    356 |          3603.43 | 7455000 |   340.84 |\n| IMPALA_BreakoutNoFrameskip-v4_926d2_00002 | TERMINATED |       |    357 |          3601.72 | 7622250 |   306.88 |\n| IMPALA_BreakoutNoFrameskip-v4_926d2_00003 | TERMINATED |       |    357 |          3601.02 | 7561500 |   331.75 |\n| PPO_BreakoutNoFrameskip-v4_926d2_00004    | TERMINATED |       |    692 |          3600.28 | 3460000 |    28.44 |\n| PPO_BreakoutNoFrameskip-v4_926d2_00005    | TERMINATED |       |    946 |          3600.06 | 4730000 |    53.61 |\n| PPO_BreakoutNoFrameskip-v4_926d2_00006    | TERMINATED |       |    943 |          3602.39 | 4715000 |    37.03 |\n| PPO_BreakoutNoFrameskip-v4_926d2_00007    | TERMINATED |       |    944 |          3602.4  | 4720000 |    36.35 |\n| PPO_BreakoutNoFrameskip-v4_926d2_00008    | TERMINATED |       |     27 |          3600.14 |  135000 |     2.02 |\n| PPO_BreakoutNoFrameskip-v4_926d2_00009    | TERMINATED |       |     27 |          3723.31 |  135000 |     5.37 |\n| PPO_BreakoutNoFrameskip-v4_926d2_00010    | TERMINATED |       |     21 |          3704.86 |  105000 |     1.5  |\n| PPO_BreakoutNoFrameskip-v4_926d2_00011    | TERMINATED |       |     22 |          3630.76 |  110000 |     1.98 |\n| APEX_BreakoutNoFrameskip-v4_926d2_00012   | TERMINATED |       |    106 |          3607.17 | 5372960 |    74.44 |\n| APEX_BreakoutNoFrameskip-v4_926d2_00013   | TERMINATED |       |    107 |          3603.72 | 8074080 |    87.35 |\n| APEX_BreakoutNoFrameskip-v4_926d2_00014   | TERMINATED |       |     90 |          3633.64 | 6787680 |    59.02 |\n| APEX_BreakoutNoFrameskip-v4_926d2_00015   | TERMINATED |       |     90 |          3616.73 | 6784160 |    61.9  |\n| A2C_BreakoutNoFrameskip-v4_926d2_00016    | TERMINATED |       |    354 |          3603.13 | 3347000 |    74.53 |\n| A2C_BreakoutNoFrameskip-v4_926d2_00017    | TERMINATED |       |    353 |          3601.95 | 3374000 |   174.69 |\n| A2C_BreakoutNoFrameskip-v4_926d2_00018    | TERMINATED |       |    353 |          3601.02 | 3377000 |   149.69 |\n| A2C_BreakoutNoFrameskip-v4_926d2_00019    | TERMINATED |       |    353 |          3605.72 | 3732000 |   203.06 |\n| DQN_BreakoutNoFrameskip-v4_926d2_00020    | TERMINATED |       |     27 |          3695.93 |  280000 |    14.71 |\n| DQN_BreakoutNoFrameskip-v4_926d2_00021    | TERMINATED |       |     26 |          3613.54 |  270000 |    16.37 |\n| DQN_BreakoutNoFrameskip-v4_926d2_00022    | TERMINATED |       |     27 |          3661.86 |  280000 |    15.43 |\n| DQN_BreakoutNoFrameskip-v4_926d2_00023    | TERMINATED |       |     27 |          3680.85 |  280000 |    15.62 |\n+-------------------------------------------+------------+-------+--------+------------------+---------+----------+\n```\n\n----------------------------------------\n\nTITLE: Package Hash Verification List\nDESCRIPTION: List of package requirements and their corresponding SHA256 hash values for dependency verification. Each package includes multiple hash values to ensure integrity during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_37\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:5094d9206c64181d0f6e76ebd8fb2f8fe274950a63890ee9e0ebfd58bf9d787b \\\n--hash=sha256:54d6921f07555713b9300bee9c50fb46e57e2e639027089b1d795ecd9f7fa910 \\\n--hash=sha256:578e281c393af575879990861823ef19d66e2b1d0098414855dd367e234f5b3c \\\n# Additional hashes truncated for brevity\n```\n\n----------------------------------------\n\nTITLE: Specifying S3Transfer Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the S3Transfer package dependency with version 0.6.2 and SHA-256 hashes for verification. Comments indicate that this package is required by boto3 and is constrained by release/ray_release/byod/requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_35\n\nLANGUAGE: pip\nCODE:\n```\ns3transfer==0.6.2 \\\n    --hash=sha256:b014be3a8a2aab98cfe1abc7229cc5a9a0cf05eb9c1f2b86b230fd8df3f78084 \\\n    --hash=sha256:cab66d3380cca3e70939ef2255d01cd8aece4a4907a9528740f668c4b0611861\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   boto3\n```\n\n----------------------------------------\n\nTITLE: Tracing Netty NIO Event Loop with xen_clocksource_read in Java and Kernel\nDESCRIPTION: A stack trace showing the complete flow from Java thread creation through Netty's NIO event loop processing to kernel-level network operations. This trace includes Xen's clocksource read implementation for timing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_26\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ktime_get_real_[k];getnstimeofday_[k];xen_clocksource_get_cycles_[k];xen_clocksource_read_[k];pvclock_clocksource_read_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Network Processing Stack Trace\nDESCRIPTION: Stack trace showing complete flow from Netty NIO event loop through kernel networking, including TCP processing and IP routing. Demonstrates channel read/write operations, buffer handling, and kernel network stack traversal.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_24\n\nLANGUAGE: stack-trace\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];tcp_v4_do_rcv_[k];tcp_rcv_established_[k];tcp_ack_[k];tcp_clean_rtx_queue_[k];tcp_valid_rtt_meas_[k];tcp_rtt_estimator_[k]\n```\n\n----------------------------------------\n\nTITLE: Showing Sequential Hierarchical Action Timeline\nDESCRIPTION: A text diagram illustrating a sequential hierarchical action timeline where one step of the top-level policy corresponds to four low-level actions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/hierarchical-envs.rst#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ntop-level: action_0 -------------------------------------> action_1 ->\nlow-level: action_0 -> action_1 -> action_2 -> action_3 -> action_4 ->\n```\n\n----------------------------------------\n\nTITLE: APPO Configuration\nDESCRIPTION: This section refers to the APPOConfig class in RLlib, highlighting its training-related members. It provides a way to configure the APPO algorithm's training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-algorithms.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: ray.rllib.algorithms.appo.appo.APPOConfig\n   :members: training\n```\n\n----------------------------------------\n\nTITLE: Expected Output of Installation Validation\nDESCRIPTION: Shows the expected output after successful KubeRay operator installation, displaying the pod status as running.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/kuberay-operator-installation.md#2025-04-12_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nNAME                                READY   STATUS    RESTARTS   AGE\nkuberay-operator-6bc45dd644-gwtqv   1/1     Running   0          24s\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution Stack Trace with Built-in Prototype Retrieval\nDESCRIPTION: This stack trace shows the execution path leading to the retrieval of a built-in prototype in the JavaScript runtime. It demonstrates how the Vert.x application interacts with core JavaScript functionalities.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_83\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/TopLevel:.getBuiltinPrototype_[j]\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Performance Test Results\nDESCRIPTION: JSON configuration containing test execution metrics including pages per second (17.07), total pages processed (1000), execution time (58.59s), and associated resource URLs for the Anyscale platform session and Ray wheel build.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/benchmarks/many_pgs.txt#2025-04-12_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"pgs_per_second\": 17.06879130613137,\n  \"num_pgs\": 1000,\n  \"time\": 58.586456537246704,\n  \"success\": \"1\",\n  \"_runtime\": 69.5553240776062,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_gr3X2VEThCAQrtiHrJRd8yxW\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom OfflinePreLearner in RLlib\nDESCRIPTION: Shows how to integrate a custom OfflinePreLearner class using AlgorithmConfig. The example demonstrates specifying a TextOfflinePreLearner with custom vocabulary parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .offline_data(\n        # Provide your custom `OfflinePreLearner`.\n        prelearner_class=TextOfflinePreLearner,\n        # Provide special keyword arguments your `OfflinePreLearner` needs.\n        prelearner_kwargs={\n            \"vocabulary\": vocabulary,\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Requirements file entries listing Python packages with exact version pins and SHA256 hash verification. Each entry includes the package name, version, hash values for verification, and comments indicating which parent packages require these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_33\n\nLANGUAGE: pip\nCODE:\n```\nqpd==0.4.4 \\\n    --hash=sha256:e0ed05b88e321ea9935874377bda11339c90f1469f34344e9b41d16b8088e136 \\\n    --hash=sha256:fc02b53d990f505353ec495682fbc107dfc06c59e66d2206b5d2db2b5700b629\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   fugue\n```\n\nLANGUAGE: pip\nCODE:\n```\nreferencing==0.36.2 \\\n    --hash=sha256:df2e89862cd09deabbdba16944cc3f10feb6b3e6f18e902f7cc25609a34775aa \\\n    --hash=sha256:e8699adbbf8b5c7de96d8ffa0eb5c158b3beafce084968e2ea8bb08c6794dcd0\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   jsonschema\n    #   jsonschema-specifications\n```\n\nLANGUAGE: pip\nCODE:\n```\nregex==2024.5.15 \\\n    --hash=sha256:0721931ad5fe0dda45d07f9820b90b2148ccdd8e45bb9e9b42a146cb4f695649 \\\n    --hash=sha256:10002e86e6068d9e1c91eae8295ef690f02f913c57db120b58fdd35a6bb1af35 \\\n    --hash=sha256:10e4ce0dca9ae7a66e6089bb29355d4432caed736acae36fef0fdd7879f0b0cb \\\n    --hash=sha256:119af6e56dce35e8dfb5222573b50c89e5508d94d55713c75126b753f834de68 \\\n    --hash=sha256:1337b7dbef9b2f71121cdbf1e97e40de33ff114801263b275aafd75303bd62b5 \\\n    --hash=sha256:13cdaf31bed30a1e1c2453ef6015aa0983e1366fad2667657dbcac7b02f67133 \\\n    --hash=sha256:1595f2d10dff3d805e054ebdc41c124753631b6a471b976963c7b28543cf13b0 \\\n    --hash=sha256:16093f563098448ff6b1fa68170e4acbef94e6b6a4e25e10eae8598bb1694b5d \\\n    --hash=sha256:1878b8301ed011704aea4c806a3cadbd76f84dece1ec09cc9e4dc934cfa5d4da \\\n    --hash=sha256:19068a6a79cf99a19ccefa44610491e9ca02c2be3305c7760d3831d38a467a6f \\\n    --hash=sha256:19dfb1c504781a136a80ecd1fff9f16dddf5bb43cec6871778c8a907a085bb3d \\\n    --hash=sha256:1b5269484f6126eee5e687785e83c6b60aad7663dafe842b34691157e5083e53 \\\n    --hash=sha256:1c1c174d6ec38d6c8a7504087358ce9213d4332f6293a94fbf5249992ba54efa \\\n    --hash=sha256:2431b9e263af1953c55abbd3e2efca67ca80a3de8a0437cb58e2421f8184717a \\\n    --hash=sha256:287eb7f54fc81546346207c533ad3c2c51a8d61075127d7f6d79aaf96cdee890 \\\n    --hash=sha256:2b4c884767504c0e2401babe8b5b7aea9148680d2e157fa28f01529d1f7fcf67 \\\n    --hash=sha256:35cb514e137cb3488bce23352af3e12fb0dbedd1ee6e60da053c69fb1b29cc6c \\\n    --hash=sha256:391d7f7f1e409d192dba8bcd42d3e4cf9e598f3979cdaed6ab11288da88cb9f2 \\\n    --hash=sha256:3ad070b823ca5890cab606c940522d05d3d22395d432f4aaaf9d5b1653e47ced \\\n    --hash=sha256:3cd7874d57f13bf70078f1ff02b8b0aa48d5b9ed25fc48547516c6aba36f5741 \\\n    --hash=sha256:3e507ff1e74373c4d3038195fdd2af30d297b4f0950eeda6f515ae3d84a1770f \\\n    --hash=sha256:455705d34b4154a80ead722f4f185b04c4237e8e8e33f265cd0798d0e44825fa \\\n    --hash=sha256:4a605586358893b483976cffc1723fb0f83e526e8f14c6e6614e75919d9862cf \\\n    --hash=sha256:4babf07ad476aaf7830d77000874d7611704a7fcf68c9c2ad151f5d94ae4bfc4 \\\n    --hash=sha256:4eee78a04e6c67e8391edd4dad3279828dd66ac4b79570ec998e2155d2e59fd5 \\\n    --hash=sha256:5397de3219a8b08ae9540c48f602996aa6b0b65d5a61683e233af8605c42b0f2 \\\n    --hash=sha256:5b5467acbfc153847d5adb21e21e29847bcb5870e65c94c9206d20eb4e99a384 \\\n    --hash=sha256:5eaa7ddaf517aa095fa8da0b5015c44d03da83f5bd49c87961e3c997daed0de7 \\\n    --hash=sha256:632b01153e5248c134007209b5c6348a544ce96c46005d8456de1d552455b014 \\\n    --hash=sha256:64c65783e96e563103d641760664125e91bd85d8e49566ee560ded4da0d3e704 \\\n    --hash=sha256:64f18a9a3513a99c4bef0e3efd4c4a5b11228b48aa80743be822b71e132ae4f5 \\\n    --hash=sha256:673b5a6da4557b975c6c90198588181029c60793835ce02f497ea817ff647cb2 \\\n    --hash=sha256:68811ab14087b2f6e0fc0c2bae9ad689ea3584cad6917fc57be6a48bbd012c49 \\\n    --hash=sha256:6e8d717bca3a6e2064fc3a08df5cbe366369f4b052dcd21b7416e6d71620dca1 \\\n    --hash=sha256:71a455a3c584a88f654b64feccc1e25876066c4f5ef26cd6dd711308aa538694 \\\n    --hash=sha256:72d7a99cd6b8f958e85fc6ca5b37c4303294954eac1376535b03c2a43eb72629 \\\n    --hash=sha256:7b59138b219ffa8979013be7bc85bb60c6f7b7575df3d56dc1e403a438c7a3f6 \\\n    --hash=sha256:7dbe2467273b875ea2de38ded4eba86cbcbc9a1a6d0aa11dcf7bd2e67859c435 \\\n    --hash=sha256:833616ddc75ad595dee848ad984d067f2f31be645d603e4d158bba656bbf516c \\\n    --hash=sha256:87e2a9c29e672fc65523fb47a90d429b70ef72b901b4e4b1bd42387caf0d6835 \\\n    --hash=sha256:8fe45aa3f4aa57faabbc9cb46a93363edd6197cbc43523daea044e9ff2fea83e \\\n    --hash=sha256:9e717956dcfd656f5055cc70996ee2cc82ac5149517fc8e1b60261b907740201 \\\n    --hash=sha256:9efa1a32ad3a3ea112224897cdaeb6aa00381627f567179c0314f7b65d354c62 \\\n    --hash=sha256:9ff11639a8d98969c863d4617595eb5425fd12f7c5ef6621a4b74b71ed8726d5 \\\n    --hash=sha256:a094801d379ab20c2135529948cb84d417a2169b9bdceda2a36f5f10977ebc16 \\\n    --hash=sha256:a0981022dccabca811e8171f913de05720590c915b033b7e601f35ce4ea7019f \\\n    --hash=sha256:a0bd000c6e266927cb7a1bc39d55be95c4b4f65c5be53e659537537e019232b1 \\\n    --hash=sha256:a32b96f15c8ab2e7d27655969a23895eb799de3665fa94349f3b2fbfd547236f \\\n    --hash=sha256:a81e3cfbae20378d75185171587cbf756015ccb14840702944f014e0d93ea09f \\\n    --hash=sha256:ac394ff680fc46b97487941f5e6ae49a9f30ea41c6c6804832063f14b2a5a145 \\\n    --hash=sha256:ada150c5adfa8fbcbf321c30c751dc67d2f12f15bd183ffe4ec7cde351d945b3 \\\n    --hash=sha256:b2b6f1b3bb6f640c1a92be3bbfbcb18657b125b99ecf141fb3310b5282c7d4ed \\\n    --hash=sha256:b802512f3e1f480f41ab5f2cfc0e2f761f08a1f41092d6718868082fc0d27143 \\\n    --hash=sha256:ba68168daedb2c0bab7fd7e00ced5ba90aebf91024dea3c88ad5063c2a562cca \\\n    --hash=sha256:bfc4f82cabe54f1e7f206fd3d30fda143f84a63fe7d64a81558d6e5f2e5aaba9 \\\n    --hash=sha256:c0c18345010870e58238790a6779a1219b4d97bd2e77e1140e8ee5d14df071aa \\\n    --hash=sha256:c3bea0ba8b73b71b37ac833a7f3fd53825924165da6a924aec78c13032f20850 \\\n    --hash=sha256:c486b4106066d502495b3025a0a7251bf37ea9540433940a23419461ab9f2a80 \\\n    --hash=sha256:c49e15eac7c149f3670b3e27f1f28a2c1ddeccd3a2812cba953e01be2ab9b5fe \\\n    --hash=sha256:c6a2b494a76983df8e3d3feea9b9ffdd558b247e60b92f877f93a1ff43d26656 \\\n    --hash=sha256:cab12877a9bdafde5500206d1020a584355a97884dfd388af3699e9137bf7388 \\\n    --hash=sha256:cac27dcaa821ca271855a32188aa61d12decb6fe45ffe3e722401fe61e323cd1 \\\n    --hash=sha256:cdd09d47c0b2efee9378679f8510ee6955d329424c659ab3c5e3a6edea696294 \\\n    --hash=sha256:cf2430df4148b08fb4324b848672514b1385ae3807651f3567871f130a728cc3 \\\n    --hash=sha256:d0a3d8d6acf0c78a1fff0e210d224b821081330b8524e3e2bc5a68ef6ab5803d \\\n    --hash=sha256:d0c0c0003c10f54a591d220997dd27d953cd9ccc1a7294b40a4be5312be8797b \\\n    --hash=sha256:d1f059a4d795e646e1c37665b9d06062c62d0e8cc3c511fe01315973a6542e40 \\\n    --hash=sha256:d347a741ea871c2e278fde6c48f85136c96b8659b632fb57a7d1ce1872547600 \\\n    --hash=sha256:d3ee02d9e5f482cc8309134a91eeaacbdd2261ba111b0fef3748eeb4913e6a2c \\\n    --hash=sha256:d99ceffa25ac45d150e30bd9ed14ec6039f2aad0ffa6bb87a5936f5782fc1569 \\\n    --hash=sha256:e38a7d4e8f633a33b4c7350fbd8bad3b70bf81439ac67ac38916c4a86b465456 \\\n    --hash=sha256:e4682f5ba31f475d58884045c1a97a860a007d44938c4c0895f41d64481edbc9 \\\n    --hash=sha256:e5bb9425fe881d578aeca0b2b4b3d314ec88738706f66f219c194d67179337cb \\\n    --hash=sha256:e64198f6b856d48192bf921421fdd8ad8eb35e179086e99e99f711957ffedd6e \\\n    --hash=sha256:e6662686aeb633ad65be2a42b4cb00178b3fbf7b91878f9446075c404ada552f \\\n    --hash=sha256:ec54d5afa89c19c6dd8541a133be51ee1017a38b412b1321ccb8d6ddbeb4cf7d \\\n    --hash=sha256:f5b1dff3ad008dccf18e652283f5e5339d70bf8ba7c98bf848ac33db10f7bc7a \\\n    --hash=sha256:f8ec0c2fea1e886a19c3bee0cd19d862b3aa75dcdfb42ebe8ed30708df64687a \\\n    --hash=sha256:f9ebd0a36102fcad2f03696e8af4ae682793a5d30b46c647eaf280d6cfb32796\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   diffusers\n    #   nltk\n    #   sacrebleu\n    #   tiktoken\n    #   transformers\n```\n\nLANGUAGE: pip\nCODE:\n```\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   datasets\n    #   diffusers\n    #   evaluate\n    #   fsspec\n    #   gcsfs\n    #   google-api-core\n    #   google-auth\n    #   google-cloud-storage\n    #   huggingface-hub\n    #   locust\n    #   requests-oauthlib\n    #   tiktoken\n    #   torchtext\n    #   transformers\n    #   wandb\n```\n\nLANGUAGE: pip\nCODE:\n```\nrequests-oauthlib==2.0.0 \\\n    --hash=sha256:7dd8a5c40426b779b0868c404bdef9768deccf22749cde15852df527e6269b36 \\\n    --hash=sha256:b3dffaebd884d8cd778494369603a9e7b58d29111bf6b41bdc2dcd87203af4e9\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   google-auth-oauthlib\n```\n\nLANGUAGE: pip\nCODE:\n```\nretry-decorator==1.1.1 \\\n    --hash=sha256:e1e8ad02e518fe11073f2ea7d80b6b8be19daa27a60a1838aff7c731ddcf2ebe\n    # via\n```\n\n----------------------------------------\n\nTITLE: Defining sphinx dependency with version pinning and hash verification\nDESCRIPTION: Specifies sphinx version 6.2.1 with SHA-256 hashes for verification. Comments indicate it's required by python/requirements/llm/llm-test-requirements.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_41\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx==6.2.1 \\\n    --hash=sha256:6d56a34697bb749ffa0152feafc4b19836c755d90a7c59b72bc7dfd371b9cc6b \\\n    --hash=sha256:97787ff1fa3256a3eef9eda523a63dbf299f7b47e053cfcf684a1c2a8380c912\n    # via -r python/requirements/llm/llm-test-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying pillow 10.3.0 Dependency with Hash Verification\nDESCRIPTION: Defines the pillow package at version 10.3.0 with SHA256 hash verification values. This is imported from requirements_compiled_rayllm_test_py311_cu121.txt and used by multiple packages including imageio, mistral-common, scikit-image, torchvision, and vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_25\n\nLANGUAGE: plaintext\nCODE:\n```\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    --hash=sha256:0ea2a783a2bdf2a561808fe4a7a12e9aa3799b701ba305de596bc48b8bdfce9d \\\n    --hash=sha256:1530e8f3a4b965eb6a7785cf17a426c779333eb62c9a7d1bbcf3ffd5bf77a4aa \\\n    --hash=sha256:16563993329b79513f59142a6b02055e10514c1a8e86dca8b48a893e33cf91e3 \\\n    --hash=sha256:19aeb96d43902f0a783946a0a87dbdad5c84c936025b8419da0a0cd7724356b1 \\\n    --hash=sha256:1a1d1915db1a4fdb2754b9de292642a39a7fb28f1736699527bb649484fb966a \\\n    --hash=sha256:1b87bd9d81d179bd8ab871603bd80d8645729939f90b71e62914e816a76fc6bd \\\n    --hash=sha256:1dfc94946bc60ea375cc39cff0b8da6c7e5f8fcdc1d946beb8da5c216156ddd8 \\\n    --hash=sha256:2034f6759a722da3a3dbd91a81148cf884e91d1b747992ca288ab88c1de15999 \\\n    --hash=sha256:261ddb7ca91fcf71757979534fb4c128448b5b4c55cb6152d280312062f69599 \\\n    --hash=sha256:2ed854e716a89b1afcedea551cd85f2eb2a807613752ab997b9974aaa0d56936 \\\n    --hash=sha256:3102045a10945173d38336f6e71a8dc71bcaeed55c3123ad4af82c52807b9375 \\\n    --hash=sha256:339894035d0ede518b16073bdc2feef4c991ee991a29774b33e515f1d308e08d \\\n    --hash=sha256:412444afb8c4c7a6cc11a47dade32982439925537e483be7c0ae0cf96c4f6a0b \\\n    --hash=sha256:4203efca580f0dd6f882ca211f923168548f7ba334c189e9eab1178ab840bf60 \\\n    --hash=sha256:45ebc7b45406febf07fef35d856f0293a92e7417ae7933207e90bf9090b70572 \\\n    --hash=sha256:4b5ec25d8b17217d635f8935dbc1b9aa5907962fae29dff220f2659487891cd3 \\\n    --hash=sha256:4c8e73e99da7db1b4cad7f8d682cf6abad7844da39834c288fbfa394a47bbced \\\n    --hash=sha256:4e6f7d1c414191c1199f8996d3f2282b9ebea0945693fb67392c75a3a320941f \\\n    --hash=sha256:4eaa22f0d22b1a7e93ff0a596d57fdede2e550aecffb5a1ef1106aaece48e96b \\\n    --hash=sha256:50b8eae8f7334ec826d6eeffaeeb00e36b5e24aa0b9df322c247539714c6df19 \\\n    --hash=sha256:50fd3f6b26e3441ae07b7c979309638b72abc1a25da31a81a7fbd9495713ef4f \\\n    --hash=sha256:51243f1ed5161b9945011a7360e997729776f6e5d7005ba0c6879267d4c5139d \\\n    --hash=sha256:5d512aafa1d32efa014fa041d38868fda85028e3f930a96f85d49c7d8ddc0383 \\\n    --hash=sha256:5f77cf66e96ae734717d341c145c5949c63180842a545c47a0ce7ae52ca83795 \\\n    --hash=sha256:6b02471b72526ab8a18c39cb7967b72d194ec53c1fd0a70b050565a0f366d355 \\\n    --hash=sha256:6fb1b30043271ec92dc65f6d9f0b7a830c210b8a96423074b15c7bc999975f57 \\\n    --hash=sha256:7161ec49ef0800947dc5570f86568a7bb36fa97dd09e9827dc02b718c5643f09 \\\n    --hash=sha256:72d622d262e463dfb7595202d229f5f3ab4b852289a1cd09650362db23b9eb0b \\\n    --hash=sha256:74d28c17412d9caa1066f7a31df8403ec23d5268ba46cd0ad2c50fb82ae40462 \\\n    --hash=sha256:78618cdbccaa74d3f88d0ad6cb8ac3007f1a6fa5c6f19af64b55ca170bfa1edf \\\n    --hash=sha256:793b4e24db2e8742ca6423d3fde8396db336698c55cd34b660663ee9e45ed37f \\\n    --hash=sha256:798232c92e7665fe82ac085f9d8e8ca98826f8e27859d9a96b41d519ecd2e49a \\\n    --hash=sha256:81d09caa7b27ef4e61cb7d8fbf1714f5aec1c6b6c5270ee53504981e6e9121ad \\\n    --hash=sha256:8ab74c06ffdab957d7670c2a5a6e1a70181cd10b727cd788c4dd9005b6a8acd9 \\\n    --hash=sha256:8eb0908e954d093b02a543dc963984d6e99ad2b5e36503d8a0aaf040505f747d \\\n    --hash=sha256:90b9e29824800e90c84e4022dd5cc16eb2d9605ee13f05d47641eb183cd73d45 \\\n    --hash=sha256:9797a6c8fe16f25749b371c02e2ade0efb51155e767a971c61734b1bf6293994 \\\n    --hash=sha256:9d2455fbf44c914840c793e89aa82d0e1763a14253a000743719ae5946814b2d \\\n    --hash=sha256:9d3bea1c75f8c53ee4d505c3e67d8c158ad4df0d83170605b50b64025917f338 \\\n    --hash=sha256:9e2ec1e921fd07c7cda7962bad283acc2f2a9ccc1b971ee4b216b75fad6f0463 \\\n    --hash=sha256:9e91179a242bbc99be65e139e30690e081fe6cb91a8e77faf4c409653de39451 \\\n    --hash=sha256:a0eaa93d054751ee9964afa21c06247779b90440ca41d184aeb5d410f20ff591 \\\n    --hash=sha256:a2c405445c79c3f5a124573a051062300936b0281fee57637e706453e452746c \\\n    --hash=sha256:aa7e402ce11f0885305bfb6afb3434b3cd8f53b563ac065452d9d5654c7b86fd \\\n    --hash=sha256:aff76a55a8aa8364d25400a210a65ff59d0168e0b4285ba6bf2bd83cf675ba32 \\\n    --hash=sha256:b09b86b27a064c9624d0a6c54da01c1beaf5b6cadfa609cf63789b1d08a797b9 \\\n    --hash=sha256:b14f16f94cbc61215115b9b1236f9c18403c15dd3c52cf629072afa9d54c1cbf \\\n    --hash=sha256:b50811d664d392f02f7761621303eba9d1b056fb1868c8cdf4231279645c25f5 \\\n    --hash=sha256:b7bc2176354defba3edc2b9a777744462da2f8e921fbaf61e52acb95bafa9828 \\\n    --hash=sha256:c78e1b00a87ce43bb37642c0812315b411e856a905d58d597750eb79802aaaa3 \\\n    --hash=sha256:c83341b89884e2b2e55886e8fbbf37c3fa5efd6c8907124aeb72f285ae5696e5 \\\n    --hash=sha256:ca2870d5d10d8726a27396d3ca4cf7976cec0f3cb706debe88e3a5bd4610f7d2 \\\n    --hash=sha256:ccce24b7ad89adb5a1e34a6ba96ac2530046763912806ad4c247356a8f33a67b \\\n    --hash=sha256:cd5e14fbf22a87321b24c88669aad3a51ec052eb145315b3da3b7e3cc105b9a2 \\\n    --hash=sha256:ce49c67f4ea0609933d01c0731b34b8695a7a748d6c8d186f95e7d085d2fe475 \\\n    --hash=sha256:d33891be6df59d93df4d846640f0e46f1a807339f09e79a8040bc887bdcd7ed3 \\\n    --hash=sha256:d3b2348a78bc939b4fed6552abfd2e7988e0f81443ef3911a4b8498ca084f6eb \\\n    --hash=sha256:d886f5d353333b4771d21267c7ecc75b710f1a73d72d03ca06df49b09015a9ef \\\n    --hash=sha256:d93480005693d247f8346bc8ee28c72a2191bdf1f6b5db469c096c0c867ac015 \\\n    --hash=sha256:dc1a390a82755a8c26c9964d457d4c9cbec5405896cba94cf51f36ea0d855002 \\\n    --hash=sha256:dd78700f5788ae180b5ee8902c6aea5a5726bac7c364b202b4b3e3ba2d293170 \\\n    --hash=sha256:e46f38133e5a060d46bd630faa4d9fa0202377495df1f068a8299fd78c84de84 \\\n    --hash=sha256:e4b878386c4bf293578b48fc570b84ecfe477d3b77ba39a6e87150af77f40c57 \\\n    --hash=sha256:f0d0591a0aeaefdaf9a5e545e7485f89910c977087e7de2b6c388aec32011e9f \\\n    --hash=sha256:fdcbb4068117dfd9ce0138d068ac512843c52295ed996ae6dd1faf537b6dbc27 \\\n    --hash=sha256:ff61bfd9253c3915e6d41c651d5f962da23eda633cf02262990094a18a55371a\n```\n\n----------------------------------------\n\nTITLE: Connecting to the GKE Cluster\nDESCRIPTION: Downloads Google Cloud credentials and configures the Kubernetes CLI to authenticate with the GKE cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-tpu-cluster.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container clusters get-credentials $CLUSTER_NAME --zone $ZONE\n```\n\n----------------------------------------\n\nTITLE: Tracing Netty NIO Event Loop with skb_dst_set_noref in Java and Kernel\nDESCRIPTION: A stack trace showing the complete flow from Java thread creation through Netty's NIO event loop processing to kernel-level network operations, ending with socket buffer destination setup.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_28\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];skb_dst_set_noref_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how Python package dependencies are specified with exact versions and hash values. It includes packages like pygments, pyopenssl, pyparsing, pytest, and others, along with their respective hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_38\n\nLANGUAGE: Text\nCODE:\n```\npygments==2.18.0 \\\n    --hash=sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199 \\\n    --hash=sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ipython\n    #   nbconvert\n    #   rich\npyopenssl==24.2.1 \\\n    --hash=sha256:4247f0dbe3748d560dcbb2ff3ea01af0f9a1a001ef5f7c4c647956ed8cbf0e95 \\\n    --hash=sha256:967d5719b12b243588573f39b0c677637145c7a1ffedcd495a487e58177fbb8d\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\npyparsing==3.1.1 \\\n    --hash=sha256:32c7c0b711493c72ff18a981d24f28aaf9c1fb7ed5e9667c9e84e3db623bdbfb \\\n    --hash=sha256:ede28a1a32462f5a9705e07aea48001a08f7cf81a021585011deba701581a0db\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   httplib2\npytest==7.4.4 \\\n    --hash=sha256:2cf0005922c6ace4a3e2ec8b4080eb0d9753fdc93107415332f50ce9e7994280 \\\n    --hash=sha256:b090cdf5ed60bf4c45261be03239c2c1c22df034fbffe691abe93cd80cea01d8\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/base-test-requirements.txt\n    #   pytest-aiohttp\n    #   pytest-asyncio\n```\n\n----------------------------------------\n\nTITLE: Example Output of Load Shedding Test\nDESCRIPTION: This bash code shows example output from running the load shedding test.  The output demonstrates the service starting up, warnings from the proxy indicating dropped requests due to backpressure, and successful responses from the deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/best-practices.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n2024-02-28 11:12:22,287 INFO worker.py:1744 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265\n(ProxyActor pid=21011) INFO 2024-02-28 11:12:24,088 proxy 127.0.0.1 proxy.py:1140 - Proxy actor 15b7c620e64c8c69fb45559001000000 starting on node ebc04d744a722577f3a049da12c9f83d9ba6a4d100e888e5fcfa19d9.\n(ProxyActor pid=21011) INFO 2024-02-28 11:12:24,089 proxy 127.0.0.1 proxy.py:1357 - Starting HTTP server on node: ebc04d744a722577f3a049da12c9f83d9ba6a4d100e888e5fcfa19d9 listening on port 8000\n(ProxyActor pid=21011) INFO:     Started server process [21011]\n(ServeController pid=21008) INFO 2024-02-28 11:12:24,199 controller 21008 deployment_state.py:1614 - Deploying new version of deployment SlowDeployment in application 'default'. Setting initial target number of replicas to 1.\n(ServeController pid=21008) INFO 2024-02-28 11:12:24,300 controller 21008 deployment_state.py:1924 - Adding 1 replica to deployment SlowDeployment in application 'default'.\n(ProxyActor pid=21011) WARNING 2024-02-28 11:12:27,141 proxy 127.0.0.1 544437ef-f53a-4991-bb37-0cda0b05cb6a / router.py:96 - Request dropped due to backpressure (num_queued_requests=2, max_queued_requests=2).\n(ProxyActor pid=21011) WARNING 2024-02-28 11:12:27,142 proxy 127.0.0.1 44dcebdc-5c07-4a92-b948-7843443d19cc / router.py:96 - Request dropped due to backpressure (num_queued_requests=2, max_queued_requests=2).\n(ProxyActor pid=21011) WARNING 2024-02-28 11:12:27,143 proxy 127.0.0.1 83b444ae-e9d6-4ac6-84b7-f127c48f6ba7 / router.py:96 - Request dropped due to backpressure (num_queued_requests=2, max_queued_requests=2).\n(ProxyActor pid=21011) WARNING 2024-02-28 11:12:27,144 proxy 127.0.0.1 f92b47c2-6bff-4a0d-8e5b-126d948748ea / router.py:96 - Request dropped due to backpressure (num_queued_requests=2, max_queued_requests=2).\n(ProxyActor pid=21011) WARNING 2024-02-28 11:12:27,145 proxy 127.0.0.1 cde44bcc-f3e7-4652-b487-f3f2077752aa / router.py:96 - Request dropped due to backpressure (num_queued_requests=2, max_queued_requests=2).\n(ServeReplica:default:SlowDeployment pid=21013) INFO 2024-02-28 11:12:28,168 default_SlowDeployment 8ey9y40a e3b77013-7dc8-437b-bd52-b4839d215212 / replica.py:373 - __CALL__ OK 2007.7ms\n(ServeReplica:default:SlowDeployment pid=21013) INFO 2024-02-28 11:12:30,175 default_SlowDeployment 8ey9y40a 601e7b0d-1cd3-426d-9318-43c2c4a57a53 / replica.py:373 - __CALL__ OK 4013.5ms\n(ServeReplica:default:SlowDeployment pid=21013) INFO 2024-02-28 11:12:32,183 default_SlowDeployment 8ey9y40a 0655fa12-0b44-4196-8fc5-23d31ae6fcb9 / replica.py:373 - __CALL__ OK 3987.9ms\n(ServeReplica:default:SlowDeployment pid=21013) INFO 2024-02-28 11:12:34,188 default_SlowDeployment 8ey9y40a c49dee09-8de1-4e7a-8c2f-8ce3f6d8ef34 / replica.py:373 - __CALL__ OK 3960.8ms\nRequest finished with status code 200.\nRequest finished with status code 200.\nRequest finished with status code 200.\nRequest finished with status code 200.\n\n```\n\n----------------------------------------\n\nTITLE: Native Thread and Socket Channel Operations\nDESCRIPTION: This snippet shows operations related to native threads and socket channel writing, including accessing the current native thread and performing a write operation on a socket channel.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_4\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/NativeThread:.current_[j]\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j]\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Distributed Workers in Ray\nDESCRIPTION: This snippet initializes worker processes in a distributed Ray setup, specifically for a setup where the world size is 2 but consists of 4 GPUs. It begins by setting the number of workers, then launches these workers as remote processes. Each worker calls the setup function with parameters specifying the total number of workers and their specific index. After all worker setups are complete, the 'allreduce_call' and 'p2p_call' are invoked remotely to execute distributed operations. The usage of ray.get waits for the remote functions to complete and retrieve results.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/ray-collective.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Note that the world size is 2 but there are 4 GPUs.\nnum_workers = 2\nworkers = []\ninit_rets = []\nfor i in range(num_workers):\n    w = Worker.remote()\n    workers.append(w)\n    init_rets.append(w.setup.remote(num_workers, i))\na = ray.get(init_rets)\nresults = ray.get([w.allreduce_call.remote() for w in workers])\nresults = ray.get([w.p2p_call.remote() for w in workers])\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Version Pins and Hashes\nDESCRIPTION: Detailed package requirements file that specifies exact versions and SHA256 hashes for each Python package dependency. Includes packages like accessible-pygments, aioboto3, aiohttp, and other dependencies with their respective version constraints and security hashes.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\naccessible-pygments==0.0.5 \\\n    --hash=sha256:40918d3e6a2b619ad424cb91e556bd3bd8865443d9f22f1dcdf79e33c8046872 \\\n    --hash=sha256:88ae3211e68a1d0b011504b2ffc1691feafce124b845bd072ab6f9f66f34d4b7\n    # via pydata-sphinx-theme\naioboto3==12.4.0 \\\n    --hash=sha256:0fa03ac7a8c2c187358dd27cdf84da05e91bc1a3bd85519cad13521343a3d767 \\\n    --hash=sha256:a8d5a60852482cc7a472f3544e5ad7d2f5a911054ffa066357140dc6690da94b\n    # via -r release/requirements_buildkite.in\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with NPM\nDESCRIPTION: Installs all project dependencies based on the versions specified in package-lock.json file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/README.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm ci\n```\n\n----------------------------------------\n\nTITLE: Running Ray Shuffle on Multi-Node Cluster\nDESCRIPTION: Executes Ray's experimental shuffle operation across 4 nodes, with 8 CPUs per node, 200 partitions, 500MB partition size, and 5GB object store memory per node.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/data_processing_tests/streaming_shuffle.txt#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m ray.experimental.shuffle --num-cpus=8 --num-partitions=200 --partition-size=500e6 --object-store-memory=5e9 --num-nodes=4\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Call Trace with TCP Write Path\nDESCRIPTION: Call stack trace showing the complete path from Java Netty NIO event loop down to Linux kernel's TCP/IP stack for network write operations. The trace illustrates how data flows from a Java application through Netty's channel handler pipeline to the kernel's networking subsystem.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_10\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];dev_hard_start_xmit_[k];loopback_xmit_[k];netif_rx_[k];netif_rx.part.82_[k];xen_restore_fl_direct_end_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Implementing Extract Task in Airflow TaskFlow API\nDESCRIPTION: Defines an extract task that reads order data from a hardcoded JSON string and converts it to a Python dictionary.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/workflow/examples/comparisons/airflow/etl_airflow.py.txt#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@task()\ndef extract():\n    \"\"\"\n    #### Extract task\n    A simple Extract task to get data ready for the rest of the data\n    pipeline. In this case, getting data is simulated by reading from a\n    hardcoded JSON string.\n    \"\"\"\n    data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n\n    order_data_dict = json.loads(data_string)\n    return order_data_dict\n```\n\n----------------------------------------\n\nTITLE: Java Netty NIO Event Processing with JavaScript Execution Stack Trace\nDESCRIPTION: This stack trace shows the call chain from Java thread initialization through Netty's NIO event processing to Mozilla Rhino JavaScript execution. It demonstrates how HTTP requests flow through the Vertx framework and are processed by JavaScript handlers.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_50\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/WrapFactory:.wrapAsJavaObject_[j];java/util/HashMap:.get_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution Stack Trace with Parent Scope Retrieval\nDESCRIPTION: This stack trace follows a similar path to the previous ones but ends with retrieving the parent scope of a JavaScript object. It illustrates the scope management in the JavaScript runtime within a Vert.x application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_82\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptableObject:.getParentScope_[j]\n```\n\n----------------------------------------\n\nTITLE: Ray Project Performance Stage Results\nDESCRIPTION: Detailed performance metrics showing execution times across 5 stages including iteration statistics, actor creation time and scheduling spread.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.5.0/stress_tests/many_tasks.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nStage 0 results:\n        Total time: 1.707322597503662\nStage 1 results:\n        Total time: 11.126858711242676\n        Average iteration time: 1.1126749038696289\n        Max iteration time: 1.3153302669525146\n        Min iteration time: 1.068936824798584\nStage 2 results:\n        Total time: 1.1159861087799072\n        Average iteration time: 0.22299699783325194\n        Max iteration time: 0.22810912132263184\n        Min iteration time: 0.21525931358337402\nStage 3 results:\n        Actor creation time: 0.01723313331604004\n        Total time: 5.573189735412598\nStage 4 results:\n        Scheduling spread: 0.9455271408000044.\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Package requirements file listing Python dependencies with exact versions and SHA256 hash values for security verification. Used to ensure reproducible builds and package integrity.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_34\n\nLANGUAGE: txt\nCODE:\n```\nscipy==1.11.4 \\\n    --hash=sha256:00150c5eae7b610c32589dda259eacc7c4f1665aedf25d921907f4d08a951b1c \\\n    --hash=sha256:028eccd22e654b3ea01ee63705681ee79933652b2d8f873e7949898dda6d11b6\n\nsemidbm==0.5.1 \\\n    --hash=sha256:0dd74b5e9276eb5af186ace8b74165acec0c887e746bdae60340be91b99cffaf \\\n    --hash=sha256:add3e644dd6afcce83d1752b34ff80fa4e2b37b4ce6bce3289ad19d6f0bcd6ae\n\nsentencepiece==0.1.96 \\\n    --hash=sha256:1dac8c2ad02b5ebc1179c0a14cbc7d7c6f4fd73d4dd51820626402d0aefc974e \\\n    --hash=sha256:203443a7bd4295b6a3695787235abe0e77d4c369d7156a6b9a397c540a38bd27\n\nsentry-sdk==2.10.0 \\\n    --hash=sha256:545fcc6e36c335faa6d6cda84669b6e17025f31efbf3b2211ec14efe008b75d1 \\\n    --hash=sha256:87b3d413c87d8e7f816cc9334bff255a83d8b577db2b22042651c30c19c09190\n```\n\n----------------------------------------\n\nTITLE: Analyzing Netty/Vert.x HTTP Request Processing with ScriptableObject.createSlot\nDESCRIPTION: Call stack trace showing the Java execution path from thread start through Netty's NIO event processing to Vert.x HTTP handlers and Mozilla JavaScript runtime. This specific trace shows property operations ending with ScriptableObject.createSlot, which is used to create new slots in JavaScript objects.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_71\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/IdScriptableObject:.put_[j];org/mozilla/javascript/ScriptableObject:.getSlot_[j];org/mozilla/javascript/ScriptableObject:.createSlot_[j] 3\n```\n\n----------------------------------------\n\nTITLE: Analyzing Java Thread Stack Trace in Netty/Vert.x HTTP Server Processing\nDESCRIPTION: This stack trace shows the execution path of an HTTP request being processed through Netty's NIO event loop to a Vert.x HTTP server with JavaScript handler. It demonstrates the call chain from thread creation through network I/O handling to Mozilla Rhino JavaScript execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_60\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Java Stack Trace with NativeJavaObject Property Access in Vert.x\nDESCRIPTION: This stack trace shows property access on a NativeJavaObject, which is Mozilla Rhino's way of exposing Java objects to JavaScript. The trace demonstrates how JavaScript code in Vert.x accesses Java object properties during HTTP request processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_63\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaObject:.get_[j] 2\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Channel Write Path with TX Queue Selection\nDESCRIPTION: Stack trace showing Netty NIO write operation path including TX queue selection in the kernel networking stack\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_9\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_pick_tx_[k]\n```\n\n----------------------------------------\n\nTITLE: Java Netty NIO Event Processing with JavaScript Name or Function Resolution\nDESCRIPTION: Stack trace showing the execution path to JavaScript name or function resolution. This trace illustrates how the JavaScript engine resolves function names during HTTP request processing in Vertx.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_57\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.nameOrFunction_[j];org/mozilla/javascript/ScriptableObject$Slot:.getValue_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Displaying Distributed Worker Process Information in Ray\nDESCRIPTION: This snippet shows the output of Ray's distributed worker process setup, including node IDs, IP addresses, and rank information for each worker in the distributed training job.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama_pretrain.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n(TorchTrainer pid=36054) Started distributed worker processes: \n(TorchTrainer pid=36054) - (node_id=409da2dba1dc3e5b8e58a2b766a4a19d90e7879c28c2fb13644148b8, ip=100.83.111.228, pid=36561) world_rank=0, local_rank=0, node_rank=0\n(TorchTrainer pid=36054) - (node_id=409da2dba1dc3e5b8e58a2b766a4a19d90e7879c28c2fb13644148b8, ip=100.83.111.228, pid=36562) world_rank=1, local_rank=1, node_rank=0\n(TorchTrainer pid=36054) - (node_id=409da2dba1dc3e5b8e58a2b766a4a19d90e7879c28c2fb13644148b8, ip=100.83.111.228, pid=36563) world_rank=2, local_rank=2, node_rank=0\n(TorchTrainer pid=36054) - (node_id=409da2dba1dc3e5b8e58a2b766a4a19d90e7879c28c2fb13644148b8, ip=100.83.111.228, pid=36564) world_rank=3, local_rank=3, node_rank=0\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Cluster Status\nDESCRIPTION: Shell commands to verify the deployment status by checking pods and configmaps in the default namespace.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-gcs-ft.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get pods\nkubectl get configmaps\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies with Hash Values\nDESCRIPTION: This snippet shows a portion of a requirements file listing Python package dependencies. Each entry includes the package name, version, and SHA256 hash value for verification. The file uses the pip hash checking mode format.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_31\n\nLANGUAGE: Text\nCODE:\n```\npygments==2.18.0 \\\n    --hash=sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199 \\\n    --hash=sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   rich\npyopenssl==24.2.1 \\\n    --hash=sha256:4247f0dbe3748d560dcbb2ff3ea01af0f9a1a001ef5f7c4c647956ed8cbf0e95 \\\n    --hash=sha256:967d5719b12b243588573f39b0c677637145c7a1ffedcd495a487e58177fbb8d\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\npython-dateutil==2.8.2 \\\n    --hash=sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86 \\\n    --hash=sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   pandas\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Utility Libraries with Hash Verification\nDESCRIPTION: This snippet shows how to specify various Python utility libraries like pandocfilters, parso, partial-json-parser, and pathspec with hash verification. Each includes comments showing which packages depend on them.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_29\n\nLANGUAGE: plaintext\nCODE:\n```\npandocfilters==1.5.0 \\\n    --hash=sha256:0b679503337d233b4339a817bfc8c50064e2eff681314376a47cb582305a7a38 \\\n    --hash=sha256:33aae3f25fd1a026079f5d27bdd52496f0e0803b3469282162bafdcbdf6ef14f\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   nbconvert\nparso==0.8.3 \\\n    --hash=sha256:8c07be290bb59f03588915921e29e8a50002acaf2cdc5fa0e0114f91709fafa0 \\\n    --hash=sha256:c001d4636cd3aecdaf33cbb40aebb59b094be2a74c556778ef5576c175e19e75\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   jedi\npartial-json-parser==0.2.1.1.post5 \\\n    --hash=sha256:627715aaa3cb3fb60a65b0d62223243acaa6c70846520a90326fef3a2f0b61ca \\\n    --hash=sha256:992710ac67e90b367921d52727698928040f7713ba7ecb33b96371ea7aec82ca\n    # via vllm\npathspec==0.11.2 \\\n    --hash=sha256:1d6ed233af05e679efb96b1851550ea95bbb64b7c490b0f5aa52996c11e92a20 \\\n    --hash=sha256:e0d8d0ac2f12da61956eb2306b69f9469b42f4deb0f3cb6ed47b9cce9996ced3\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Loki Using Helm\nDESCRIPTION: Commands to add Grafana Helm repository and install Loki in single replica mode using Helm chart.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-operator-logs.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n\n# Install Loki with single replica mode\nhelm install loki grafana/loki --version 6.21.0 -f https://raw.githubusercontent.com/grafana/loki/refs/heads/main/production/helm/loki/single-binary-values.yaml\n```\n\n----------------------------------------\n\nTITLE: Launching Ray on Slurm with Python Helper Script\nDESCRIPTION: Example usage of a Python helper script to generate and launch Slurm scripts for Ray jobs. Allows specifying experiment name, command, number of nodes, and other options.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/slurm.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython slurm-launch.py --exp-name test --command \"python your_file.py\" --num-nodes 3\n```\n\nLANGUAGE: bash\nCODE:\n```\npython slurm-launch.py --exp-name test --command \"python your_file.py\" --num-nodes 3 --node NODE_NAMES\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray in Python\nDESCRIPTION: This snippet initializes the Ray runtime environment. The logging is configured to be off to streamline output during execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nray.init(configure_logging=False)  # initialize Ray\n```\n\n----------------------------------------\n\nTITLE: System Error Message for Failed Ray Workers\nDESCRIPTION: Alternative error message showing when a Ray worker has failed with an unrecoverable error. This typically occurs when a worker process has been terminated unexpectedly.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-memory.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nWorker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned.\n```\n\n----------------------------------------\n\nTITLE: Netty Channel Processing Flow with TCP Socket Write Operations\nDESCRIPTION: Detailed call stack trace showing the execution path from Netty's NIO event loop through channel handlers to system TCP socket operations. The trace follows the complete flow from Java application code through JVM internals to kernel networking functions, ending with MTU calculations.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_33\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];tcp_send_mss_[k];tcp_current_mss_[k];ipv4_mtu_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Running TensorFlow Training Parity Benchmark on GPU\nDESCRIPTION: This command runs a benchmark comparing Ray Train's TensorflowTrainer to native TensorFlow Distributed on GPU, using 16 workers across multiple nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/benchmarks.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython workloads/tensorflow_benchmark.py run --num-runs 3 --num-epochs 200 --num-workers 16 --cpus-per-worker 4 --batch-size 64 --use-gpu\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop with Hypervisor Stack Trace\nDESCRIPTION: Complete stack trace showing Netty network operations extending through the hypervisor layer. This trace shows the full data path from Java application code through system calls, kernel TCP/IP stack, and includes virtualization operations involving the hypervisor.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_17\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];tcp_v4_do_rcv_[k];tcp_rcv_established_[k];sock_def_readable_[k];__wake_up_sync_key_[k];check_events_[k];hypercall_page_[k] 19\n```\n\n----------------------------------------\n\nTITLE: Creating Gradio App Builder Function\nDESCRIPTION: Function that constructs the Gradio interface with text summarization functionality using Hugging Face's pipeline\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/gradio-integration.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_app():\n    pipe = pipeline(\"summarization\", model=\"t5-small\")\n    def summarize(text):\n        result = pipe(text, min_length=5, max_length=100)\n        return result[0][\"summary_text\"]\n    io = gr.Interface(\n        fn=summarize,\n        inputs=gr.Textbox(lines=3, placeholder=\"Text to summarize\"),\n        outputs=\"text\",\n        title=\"Text Summarization with Gradio + Ray Serve\",\n    )\n    return io\n```\n\n----------------------------------------\n\nTITLE: Installing requests Package with Hash Verification\nDESCRIPTION: This snippet specifies the installation of requests package version 2.31.0 with SHA256 hash verification. It also lists the packages that depend on requests in the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_45\n\nLANGUAGE: pip\nCODE:\n```\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   google-api-core\n    #   google-cloud-storage\n    #   huggingface-hub\n    #   jupyterlab-server\n    #   mistral-common\n    #   outlines\n    #   ray\n    #   sphinx\n    #   tiktoken\n    #   transformers\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with Hash Verification for Ray Project\nDESCRIPTION: This snippet defines Python package dependencies with exact versions and SHA256 hash verification codes to ensure security and reproducibility. The file includes comments showing which parent packages require these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_11\n\nLANGUAGE: pip\nCODE:\n```\nml-dtypes==0.3.2 \\\n    --hash=sha256:2c34f2ba9660b21fe1034b608308a01be82bbef2a92fb8199f24dc6bad0d5226 \\\n    --hash=sha256:3a17ef2322e60858d93584e9c52a5be7dd6236b056b7fa1ec57f1bb6ba043e33 \\\n    --hash=sha256:533059bc5f1764fac071ef54598db358c167c51a718f68f5bb55e3dee79d2967 \\\n    --hash=sha256:6604877d567a29bfe7cc02969ae0f2425260e5335505cf5e7fefc3e5465f5655 \\\n    --hash=sha256:6b35c4e8ca957c877ac35c79ffa77724ecc3702a1e4b18b08306c03feae597bb \\\n    --hash=sha256:763697ab8a88d47443997a7cdf3aac7340049aed45f7521f6b0ec8a0594821fe \\\n    --hash=sha256:7a4c3fcbf86fa52d0204f07cfd23947ef05b4ad743a1a988e163caa34a201e5e \\\n    --hash=sha256:7afde548890a92b41c0fed3a6c525f1200a5727205f73dc21181a2726571bb53 \\\n    --hash=sha256:7ba8e1fafc7fff3e643f453bffa7d082df1678a73286ce8187d3e825e776eb94 \\\n    --hash=sha256:91f8783fd1f2c23fd3b9ee5ad66b785dafa58ba3cdb050c4458021fa4d1eb226 \\\n    --hash=sha256:93b78f53431c93953f7850bb1b925a17f0ab5d97527e38a7e865b5b4bc5cfc18 \\\n    --hash=sha256:961134ea44c7b8ca63eda902a44b58cd8bd670e21d62e255c81fba0a8e70d9b7 \\\n    --hash=sha256:b89b194e9501a92d289c1ffd411380baf5daafb9818109a4f49b0a1b6dce4462 \\\n    --hash=sha256:c7b3fb3d4f6b39bcd4f6c4b98f406291f0d681a895490ee29a0f95bab850d53c \\\n    --hash=sha256:d1a746fe5fb9cd974a91070174258f0be129c592b93f9ce7df6cc336416c3fbd \\\n    --hash=sha256:e8505946df1665db01332d885c2020b4cb9e84a8b1241eb4ba69d59591f65855 \\\n    --hash=sha256:f47619d978ab1ae7dfdc4052ea97c636c6263e1f19bd1be0e42c346b98d15ff4\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   tensorflow\n```\n\n----------------------------------------\n\nTITLE: Ray Performance Metrics Configuration\nDESCRIPTION: JSON configuration containing placement group creation/removal timings, runtime metrics, session URL, and build details for Ray v1.7.0\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/stress_tests/placement_group.txt#2025-04-12_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": 1,\n  \"avg_pg_create_time_ms\": 0.9874122837809874,\n  \"avg_pg_remove_time_ms\": 4.4027920900909265,\n  \"_runtime\": 458.8596382141113,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_7uQL743cWCzdDT3ZYTpRDETi\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Packaging Package with Hash Verification\nDESCRIPTION: Defines the packaging package version 23.0 with SHA256 hash verification and notes that it's required by multiple packages including ipykernel, jupyter-server, and others.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_22\n\nLANGUAGE: text\nCODE:\n```\npackaging==23.0 \\\n    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \\\n    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   ipykernel\n    #   jupyter-server\n    #   jupyterlab\n    #   jupyterlab-server\n    #   lazy-loader\n    #   nbconvert\n    #   pytest\n    #   scikit-image\n    #   tensorboardx\n```\n\n----------------------------------------\n\nTITLE: Setting Result Buffer Length in Ray Tune\nDESCRIPTION: TUNE_RESULT_BUFFER_LENGTH sets the buffer size for trainable results before passing them to the driver. Setting it to 1 disables result buffering.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_RESULT_BUFFER_LENGTH=1\n```\n\n----------------------------------------\n\nTITLE: Analyzing Netty/Vert.x HTTP Request Processing Call Stack with ScriptableObject.put\nDESCRIPTION: Call stack trace showing the Java execution path from thread start through Netty's NIO event processing to Vert.x HTTP handlers and Mozilla JavaScript runtime. This specific trace ends with ScriptRuntime.setObjectProp and IdScriptableObject.put.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_70\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/IdScriptableObject:.put_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Kueue Resources for Priority Scheduling\nDESCRIPTION: YAML configuration for Kueue resources including ResourceFlavor, ClusterQueue, LocalQueue, and WorkloadPriorityClass. This setup defines the resource quotas and priority classes for job scheduling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# kueue-resources.yaml\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"default-flavor\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue\"\nspec:\n  preemption:\n    withinClusterQueue: LowerPriority\n  namespaceSelector: {} # Match all namespaces.\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"nvidia.com/gpu\"]\n    flavors:\n    - name: \"default-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 2\n      - name: \"memory\"\n        nominalQuota: 8G\n      - name: \"nvidia.com/gpu\" # ClusterQueue only has quota for a single GPU.\n        nominalQuota: 1\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: \"default\"\n  name: \"user-queue\"\nspec:\n  clusterQueue: \"cluster-queue\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: WorkloadPriorityClass\nmetadata:\n  name: prod-priority\nvalue: 1000\ndescription: \"Priority class for prod jobs\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: WorkloadPriorityClass\nmetadata:\n  name: dev-priority\nvalue: 100\ndescription: \"Priority class for development jobs\"\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation Format\nDESCRIPTION: RST markup defining the structure and content of the Ray ML infrastructure documentation, including section headers, images, and external links.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-air/getting-started.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _ray-for-ml-infra:\\n\\nRay for ML Infrastructure\\n=========================\\n\\n.. tip::\\n\\n    We'd love to hear from you if you are using Ray to build a ML platform! Fill out `this short form <https://forms.gle/wCCdbaQDtgErYycT6>`__ to get involved.\\n\\nRay and its AI libraries provide unified compute runtime for teams looking to simplify their ML platform.\\nRay's libraries such as Ray Train, Ray Data, and Ray Serve can be used to compose end-to-end ML workflows, providing features and APIs for\\ndata preprocessing as part of training, and transitioning from training to serving.\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Header Hash Calculation\nDESCRIPTION: Java stack trace showing Netty's hash calculation for HTTP headers. This trace follows the HTTP header processing flow to the specific header hash computation used for efficient header lookup and storage.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_104\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/handler/codec/http/HttpObjectDecoder:.decode_[j];io/netty/handler/codec/http/HttpObjectDecoder:.readHeaders_[j];io/netty/handler/codec/http/HttpHeaders:.hash_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Ray Tune Training Log Output\nDESCRIPTION: Log output showing the execution time of a durable trainable test that completed successfully within the 500 second time budget. Shows both total run time and tuning loop duration.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/tune_tests/scalability_tests/test_durable_trainable.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n2021-04-06 03:27:21,932 INFO tune.py:549 -- Total run time: 393.52 seconds (391.87 seconds for the tuning loop).\nThe durable trainable test took 393.67 seconds, which is below the budget of 500.00 seconds. Test successful.\n\n--- PASSED: DURABLE TRAINABLE ::: 393.67 <= 500.00 ---\n```\n\n----------------------------------------\n\nTITLE: Adjusting Parameters for Smoke Test\nDESCRIPTION: Reduces the number of epochs and samples when running in smoke test mode to allow for faster testing and debugging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nif SMOKE_TEST:\n    num_epochs = 3\n    num_samples = 3\n```\n\n----------------------------------------\n\nTITLE: Citing Tune in Academic Research (BibTeX)\nDESCRIPTION: BibTeX entry for citing the Tune framework in academic research papers. Includes author information, paper title, and publication details.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tune/README.rst#2025-04-12_snippet_0\n\nLANGUAGE: tex\nCODE:\n```\n@article{liaw2018tune,\n    title={Tune: A Research Platform for Distributed Model Selection and Training},\n    author={Liaw, Richard and Liang, Eric and Nishihara, Robert and\n            Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},\n    journal={arXiv preprint arXiv:1807.05118},\n    year={2018}\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for ResNet Training on HPU\nDESCRIPTION: Python imports for necessary libraries including PyTorch, Ray, and Habana frameworks for HPU support in ResNet training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import Dict\nfrom tempfile import TemporaryDirectory\n\nimport torch\nfrom filelock import FileLock\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nfrom tqdm import tqdm\n\nimport ray\nimport ray.train as train\nfrom ray.train import ScalingConfig, Checkpoint\nfrom ray.train.torch import TorchTrainer\nfrom ray.train.torch import TorchConfig\nfrom ray.runtime_env import RuntimeEnv\n\nimport habana_frameworks.torch.core as htcore\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: Comprehensive list of Python package dependencies with version constraints and dependency relationships shown in comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_compiled.txt#2025-04-12_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nabsl-py==1.4.0\n    # via\n    #   array-record\n    #   dm-control\n    #   dm-env\n    #   etils\n    #   labmaze\n    #   ml-collections\n    #   mujoco\n    #   open-spiel\n    #   tensorboard\n    #   tensorflow\n    #   tensorflow-datasets\n    #   tensorflow-metadata\n    #   tensorflow-probability\nacccelerate==0.28.0\n    # via -r python/requirements/ml/core-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Project Dependencies\nDESCRIPTION: Defines required Python packages and their versions for the Ray project. Includes packages for environment rendering (pyglet, imageio-ffmpeg), ONNX-related functionality with platform-specific conditions, and msgpack serialization libraries.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/rllib-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# For auto-generating an env-rendering Window.\npyglet==1.5.15\nimageio-ffmpeg==0.4.5\nonnx==1.15.0; sys_platform != 'darwin' or platform_machine != 'arm64'\nonnxruntime==1.18.0; sys_platform != 'darwin' or platform_machine != 'arm64'\ntf2onnx==1.15.1; sys_platform != 'darwin' or platform_machine != 'arm64'\nrich==13.3.2\n# Msgpack checkpoint stuff.\nmsgpack\nmsgpack-numpy\normsgpack\n```\n\n----------------------------------------\n\nTITLE: Package Hash Verification Configuration\nDESCRIPTION: Comprehensive list of SHA-256 hashes for verifying package integrity and security during installation, along with version pinning for key dependencies like fsspec, gitdb, gitpython, and various Google Cloud packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:3e0153a805a98f5ada7e09826255ba99fb4f7524bb81bf6b47fb702666484ae1 \\\n--hash=sha256:410478a0c562d1a5bcc2f7ea448359fcb050ed48b3c6f6f4f18c313a9bdb1826 \\\n--hash=sha256:442acde1e068288a4ba7acfe05f5f343e19fac87bfc96d89eb886b0363e977ec \\\n# Additional hashes omitted for brevity...\n```\n\n----------------------------------------\n\nTITLE: Defining Platform-Specific Pexpect Dependency\nDESCRIPTION: This code specifies the Pexpect library dependency with hash verification, conditionally included only on non-Windows platforms. It's used by IPython for interactive shell capabilities and is referenced from the requirements_compiled.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\npexpect==4.8.0 ; sys_platform != 'win32' \\\n    --hash=sha256:0b48a55dcb3c05f3329815901ea4fc1537514d6ba867a152b581d69ae3710937 \\\n    --hash=sha256:fc65a43959d153d0114afe13997d439c22823a27cefceb5ff35c2178c6784c0c\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ipython\n```\n\n----------------------------------------\n\nTITLE: Deploying a Static Ray Cluster on Kubernetes\nDESCRIPTION: Applies a Kubernetes configuration to deploy a static Ray cluster with fault tolerance enabled using an external Redis.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml\n```\n\n----------------------------------------\n\nTITLE: Ray Dashboard Connection Output\nDESCRIPTION: Sample output when Ray initializes, showing the dashboard connection information. The output displays the URL to access the dashboard interface.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/getting-started.rst#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nINFO worker.py:1487 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265.\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Processing with UTF-8 Encoding\nDESCRIPTION: Java stack trace showing HTTP request processing with UTF-8 encoding operations. This trace extends through the Netty event loop, channel handlers, and Mozilla JavaScript components, ending with UTF-8 encoder initialization and array copy operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_97\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];sun/nio/cs/UTF_8$Encoder:.<init>_[j];jbyte_disjoint_arraycopy_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies with pip\nDESCRIPTION: Installs Ray Serve, requests, and transformers packages needed for the application\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/aws-neuron-core-inference.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install \"ray[serve]\" requests transformers\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Header Buffer Processing\nDESCRIPTION: Java stack trace showing Netty's buffer processing during HTTP header reading. This trace extends the header reading process to include byte buffer iteration, with a frequency count of 2 indicating higher occurrence.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_103\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/handler/codec/http/HttpObjectDecoder:.decode_[j];io/netty/handler/codec/http/HttpObjectDecoder:.readHeaders_[j];io/netty/buffer/AbstractByteBuf:.forEachByteAsc0_[j] 2\n```\n\n----------------------------------------\n\nTITLE: Starting Development Server\nDESCRIPTION: Runs the React application in development mode on localhost:3000 with hot-reload functionality and lint error reporting.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/README.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Running Ray Shuffle on Single Node\nDESCRIPTION: Executes Ray's experimental shuffle operation with 32 CPUs, 200 partitions, 500MB partition size, and 20GB object store memory on a single node.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/data_processing_tests/streaming_shuffle.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m ray.experimental.shuffle --num-cpus=32 --num-partitions=200 --partition-size=500e6 --object-store-memory=20e9\n```\n\n----------------------------------------\n\nTITLE: Checking Cluster Resources\nDESCRIPTION: Executes a Python command in the head pod to print cluster resources\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nkubectl exec -it $HEAD_POD -- python -c \"import pprint; import ray; ray.init(); pprint.pprint(ray.cluster_resources(), sort_dicts=True)\"\n```\n\n----------------------------------------\n\nTITLE: Defining hyperparameter space with Nevergrad\nDESCRIPTION: Defines the hyperparameter space using Nevergrad's format. This is an alternative to using Tune's search space API.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"space = ng.p.Dict(\\n    width=ng.p.Scalar(lower=0, upper=20),\\n    height=ng.p.Scalar(lower=-100, upper=100),\\n    activation=ng.p.Choice(choices=[\\\"relu\\\", \\\"tanh\\\"])\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Runtime Concurrency Group Configuration in Java\nDESCRIPTION: Shows how to dynamically assign methods to different concurrency groups at runtime in Java.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/actors/concurrency_group_api.rst#2025-04-12_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n// Executed in the \"io\" group (as defined in the actor creation).\nmyActor.task(ConcurrentActor::f2).remote();\n\n// Executed in the \"compute\" group.\nmyActor.task(ConcurrentActor::f2).setConcurrencyGroup(\"compute\").remote();\n```\n\n----------------------------------------\n\nTITLE: Connecting to an Existing Mars on Ray Runtime\nDESCRIPTION: Code to connect to an already initialized Mars on Ray runtime using its web interface URL.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/mars-on-ray.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mars\nmars.new_ray_session('http://<web_ip>:<ui_port>')\n# perform computation\n```\n\n----------------------------------------\n\nTITLE: Vert.x JavaScript Processing Stack Trace\nDESCRIPTION: Thread stack trace showing JavaScript execution path in a Vert.x HTTP server. This trace reveals how the server processes JavaScript code through Mozilla Rhino when handling HTTP requests.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_88\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j] 2\n```\n\n----------------------------------------\n\nTITLE: Tracing Netty NIO Event Loop with spin lock operations in Java and Kernel\nDESCRIPTION: A stack trace showing the complete flow from Java thread creation through Netty's NIO event loop processing to kernel-level network operations, highlighting spin lock operations for synchronization.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_29\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];lock_sock_nested_[k];_raw_spin_lock_bh_[k];local_bh_disable_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Ray Worker Process Warning Messages and Performance Metrics\nDESCRIPTION: System log showing worker process warnings on node 4bc8d5ab28cef16a5ad3d30c4502a260d1e1df3d588225a348183a5b and final placement group timing metrics. Includes warnings about Python worker process count and references to potential workarounds.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.6.0/stress_tests/placement_group.txt#2025-04-12_snippet_0\n\nLANGUAGE: log\nCODE:\n```\nINFO:__main__:remove_group iteration 622\n[...]\nINFO:__main__:remove_group iteration 665\n2021-08-25 00:24:46,958\tWARNING worker.py:1215 -- WARNING: 8 PYTHON worker processes have been started on node: 4bc8d5ab28cef16a5ad3d30c4502a260d1e1df3d588225a348183a5b with address: 172.31.50.22. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n2021-08-25 00:24:48,990\tWARNING worker.py:1215 -- WARNING: 10 PYTHON worker processes have been started on node: 4bc8d5ab28cef16a5ad3d30c4502a260d1e1df3d588225a348183a5b with address: 172.31.50.22. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n2021-08-25 00:24:50,791\tWARNING worker.py:1215 -- WARNING: 12 PYTHON worker processes have been started on node: 4bc8d5ab28cef16a5ad3d30c4502a260d1e1df3d588225a348183a5b with address: 172.31.50.22. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\nAvg placement group creating time: 0.6881522147153959 ms\nAvg placement group removing time: 4.041917145644407 ms\nPASSED.\n```\n\n----------------------------------------\n\nTITLE: Analyzing Netty/Vert.x HTTP Request Processing with vtable chunks\nDESCRIPTION: Call stack trace showing the Java execution path from thread start through Netty's NIO event processing to Vert.x HTTP handlers and Mozilla JavaScript runtime. This specific trace ends with ScriptRuntime.setObjectProp and vtable chunks access for virtual method dispatch.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_73\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];vtable chunks_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution Trace with Netty, Vert.x, and Mozilla Rhino\nDESCRIPTION: This trace shows the execution path of a Java thread handling an HTTP request through Netty's NIO event loop, Vert.x handlers, and Mozilla Rhino's JavaScript interpreter. It includes object creation, property access, and function activation in the JavaScript runtime.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_68\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/IdScriptableObject:.has_[j]\n```\n\n----------------------------------------\n\nTITLE: Recording speedscope package version and commit information\nDESCRIPTION: Shows the version of speedscope package (1.5.3), the timestamp when it was recorded (Thu Jan 16 00:10:56 PST 2020), and the corresponding git commit hash.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/release.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nspeedscope@1.5.3\nThu Jan 16 00:10:56 PST 2020\n707462e9cffec2bda49587c39d621ba89d1b51cb\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop with TCP Acknowledgment Processing\nDESCRIPTION: Stack trace showing Netty network operations with TCP acknowledgment processing. This trace shows the complete path from Java application through the network stack, including TCP's acknowledgment handling mechanism which is critical for reliable data transmission.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_18\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];tcp_v4_do_rcv_[k];tcp_rcv_established_[k];tcp_ack_[k] 3\n```\n\n----------------------------------------\n\nTITLE: Redeploying Updated Configuration\nDESCRIPTION: Console command to redeploy the updated Serve configuration without stopping the Ray cluster\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/inplace-updates.md#2025-04-12_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ serve deploy serve_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Continuing Ray Tune Training Example\nDESCRIPTION: Demonstrates how to create a new experiment that continues training from a checkpoint of a previous experiment, with modified hyperparameter search space and additional epochs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n__iter_experimentation_resume_start__\n```\n\n----------------------------------------\n\nTITLE: Defining Pathspec Library Dependency\nDESCRIPTION: This snippet specifies the Pathspec library dependency with hash verification. It's referenced from both the requirements_compiled.txt and cloud-requirements.txt files, likely used for path pattern matching functionality in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\npathspec==0.11.2 \\\n    --hash=sha256:1d6ed233af05e679efb96b1851550ea95bbb64b7c490b0f5aa52996c11e92a20 \\\n    --hash=sha256:e0d8d0ac2f12da61956eb2306b69f9469b42f4deb0f3cb6ed47b9cce9996ced3\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with Cryptographic Hashes for Ray Project\nDESCRIPTION: This snippet contains a fragment of a Python requirements file listing package dependencies with their versions and cryptographic hashes. The file includes packages like virtualenv, vllm, watchfiles, wcwidth, webcolors, webencodings, websocket-client, and websockets with their specific SHA256 hashes for security verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_45\n\nLANGUAGE: requirements.txt\nCODE:\n```\n--hash=sha256:5ee4d4ef48036ff6e5cfffb09dd192c7a5027153948d85b8da7ff705065bacc6 \\\n    --hash=sha256:67dd654b8ca23aed0a8e99010b4c34aca62f4b7fce88f39d452ed7622c94845c \\\n    --hash=sha256:787ae31ad8a2856fc4e7c095341cccc7209bd657d0e71ad0dc2ea83c4a6fa8af \\\n    --hash=sha256:86975dca1c773a2c9864f4c52c5a55631038e387b47eaf56210f873887b6c8dc \\\n    --hash=sha256:87c43e0f13022b998eb9b973b5e97200c8b90823454d4bc06ab33829e09fb9bb \\\n    --hash=sha256:88cb67cdbc0e483da00af0b2c3cdad4b7c61ceb1ee0f33fe00e09c81e3a6cb75 \\\n    --hash=sha256:8a375441696e2eda1c43c44ccb66e04d61ceeffcd76e4929e527b7fa401b90fb \\\n    --hash=sha256:a5c39f217ab3c663dc699c04cbd50c13813e31d917642d459fdcec07555cc553 \\\n    --hash=sha256:b9fb766bb57b7388745d8bcc53a359b116b8a04c83a2288069809d2b3466c37e \\\n    --hash=sha256:baa0e6291d91649c6ba4ed4b2f982f9fa165b5bbd50a9e203c416a2797bab3c6 \\\n    --hash=sha256:baa4dcdbd9ae0a372f2167a207cd98c9f9a1ea1188a8a526431eef2f8116cc8d \\\n    --hash=sha256:bc09f0ff191e61c2d592a752423c767b4ebb2986daa9ed62908e2b1b9a9ae206 \\\n    --hash=sha256:bd53ecc9a0f3d87ab847503c2e1552b690362e005ab54e8a48ba97da3924c0dc \\\n    --hash=sha256:bfd55dfcc2a512316e65f16e503e9e450cab148ef11df4e4e679b5e8253a5281 \\\n    --hash=sha256:c097078b8031190c934ed0ebfee8cc5f9ba9642e6eb88322b9958b649750f72b \\\n    --hash=sha256:c0f3fa6200b3108919f8bdabb9a7f87f20e7097ea3c543754cabc7d717d95cf8 \\\n    --hash=sha256:e678ad6fe52af2c58d2ae3c73dc85524ba8abe637f134bf3564ed07f555c5e79 \\\n    --hash=sha256:ec7e6b09a6fdded42403182ab6b832b71f4edaf7f37a9a0e371a01db5f0cb45f \\\n    --hash=sha256:f0ce1b49560b1d2d8a2977e3ba4afb2414fb46b86a1b64056bc4ab929efdafbe \\\n    --hash=sha256:f38b2e090258d051d68a5b14d1da7203a3c3677321cf32a95a6f4db4dd8b6f26 \\\n    --hash=sha256:f3df876acd7ec037a3d005b3ab85a7e4110422e4d9c1571d4fc89b0fc41b6816 \\\n    --hash=sha256:f7089d2dc73179ce5ac255bdf37c236a9f914b264825fdaacaded6990a7fb4c2\n    # via uvicorn\nvirtualenv==20.29.1 \\\n    --hash=sha256:4e4cb403c0b0da39e13b46b1b2476e505cb0046b25f242bee80f62bf990b2779 \\\n    --hash=sha256:b8b8970138d32fb606192cb97f6cd4bb644fa486be9308fb9b63f81091b5dc35\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\nvllm==0.8.2 \\\n    --hash=sha256:32442b686c5dad8e6ddcf5a8b0cf3f741359fed6a9e9e940009f1daf80ae15de \\\n    --hash=sha256:9b337b1c4072ccb94b1bf2b716593fadbe2dcb8d091f9bcbd6b5c6d37f9842ac\n    # via -r python/requirements/llm/llm-requirements.txt\nwatchfiles==0.19.0 \\\n    --hash=sha256:0089c6dc24d436b373c3c57657bf4f9a453b13767150d17284fc6162b2791911 \\\n    --hash=sha256:09ea3397aecbc81c19ed7f025e051a7387feefdb789cf768ff994c1228182fda \\\n    --hash=sha256:176a9a7641ec2c97b24455135d58012a5be5c6217fc4d5fef0b2b9f75dbf5154 \\\n    --hash=sha256:18b28f6ad871b82df9542ff958d0c86bb0d8310bb09eb8e87d97318a3b5273af \\\n    --hash=sha256:20b44221764955b1e703f012c74015306fb7e79a00c15370785f309b1ed9aa8d \\\n    --hash=sha256:3d7d267d27aceeeaa3de0dd161a0d64f0a282264d592e335fff7958cc0cbae7c \\\n    --hash=sha256:5471582658ea56fca122c0f0d0116a36807c63fefd6fdc92c71ca9a4491b6b48 \\\n    --hash=sha256:5569fc7f967429d4bc87e355cdfdcee6aabe4b620801e2cf5805ea245c06097c \\\n    --hash=sha256:68dce92b29575dda0f8d30c11742a8e2b9b8ec768ae414b54f7453f27bdf9545 \\\n    --hash=sha256:79c533ff593db861ae23436541f481ec896ee3da4e5db8962429b441bbaae16e \\\n    --hash=sha256:7f3920b1285a7d3ce898e303d84791b7bf40d57b7695ad549dc04e6a44c9f120 \\\n    --hash=sha256:91633e64712df3051ca454ca7d1b976baf842d7a3640b87622b323c55f3345e7 \\\n    --hash=sha256:945be0baa3e2440151eb3718fd8846751e8b51d8de7b884c90b17d271d34cae8 \\\n    --hash=sha256:9afd0d69429172c796164fd7fe8e821ade9be983f51c659a38da3faaaaac44dc \\\n    --hash=sha256:9c75eff897786ee262c9f17a48886f4e98e6cfd335e011c591c305e5d083c056 \\\n    --hash=sha256:b538014a87f94d92f98f34d3e6d2635478e6be6423a9ea53e4dd96210065e193 \\\n    --hash=sha256:b6577b8c6c8701ba8642ea9335a129836347894b666dd1ec2226830e263909d3 \\\n    --hash=sha256:c0376deac92377817e4fb8f347bf559b7d44ff556d9bc6f6208dd3f79f104aaf \\\n    --hash=sha256:cae3dde0b4b2078f31527acff6f486e23abed307ba4d3932466ba7cdd5ecec79 \\\n    --hash=sha256:cb5d45c4143c1dd60f98a16187fd123eda7248f84ef22244818c18d531a249d1 \\\n    --hash=sha256:d9b073073e048081e502b6c6b0b88714c026a1a4c890569238d04aca5f9ca74b \\\n    --hash=sha256:fac19dc9cbc34052394dbe81e149411a62e71999c0a19e1e09ce537867f95ae0\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   uvicorn\n    #   vllm\nwcwidth==0.2.13 \\\n    --hash=sha256:3da69048e4540d84af32131829ff948f1e022c1c6bdb8d6102117aac784f6859 \\\n    --hash=sha256:72ea0c06399eb286d978fdedb6923a9eb47e1c486ce63e9b4e64fc18303972b5\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   prompt-toolkit\nwebcolors==24.6.0 \\\n    --hash=sha256:1d160d1de46b3e81e58d0a280d0c78b467dc80f47294b91b1ad8029d2cedb55b \\\n    --hash=sha256:8cf5bc7e28defd1d48b9e83d5fc30741328305a8195c29a8e668fa45586568a1\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jsonschema\nwebencodings==0.5.1 \\\n    --hash=sha256:a0af1213f3c2226497a97e2b3aa01a7e4bee4f403f95be16fc9acd2947514a78 \\\n    --hash=sha256:b36a1c245f2d304965eb4e0a82848379241dc04b865afcc4aab16748587e1923\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   bleach\n    #   tinycss2\nwebsocket-client==1.8.0 \\\n    --hash=sha256:17b44cc997f5c498e809b22cdf2d9c7a9e71c02c8cc2b6c56e7c2d1239bfa526 \\\n    --hash=sha256:3239df9f44da632f96012472805d40a23281a991027ce11d2f45a6f24ac4c3da\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   jupyter-server\nwebsockets==15.0 \\\n    --hash=sha256:0e389efe46ccb25a1f93d08c7a74e8123a2517f7b7458f043bd7529d1a63ffeb \\\n    --hash=sha256:0f2205cdb444a42a7919690238fb5979a05439b9dbb73dd47c863d39640d85ab \\\n    --hash=sha256:10552fed076757a70ba2c18edcbc601c7637b30cdfe8c24b65171e824c7d6081 \\\n    --hash=sha256:110a847085246ab8d4d119632145224d6b49e406c64f1bbeed45c6f05097b680 \\\n    --hash=sha256:1206432cc6c644f6fc03374b264c5ff805d980311563202ed7fef91a38906276 \\\n    --hash=sha256:1657a9eecb29d7838e3b415458cc494e6d1b194f7ac73a34aa55c6fb6c72d1f3 \\\n    --hash=sha256:17f2854c6bd9ee008c4b270f7010fe2da6c16eac5724a175e75010aacd905b31 \\\n    --hash=sha256:190bc6ef8690cd88232a038d1b15714c258f79653abad62f7048249b09438af3 \\\n    --hash=sha256:1caf951110ca757b8ad9c4974f5cac7b8413004d2f29707e4d03a65d54cedf2b \\\n    --hash=sha256:24d5333a9b2343330f0f4eb88546e2c32a7f5c280f8dd7d3cc079beb0901781b \\\n    --hash=sha256:26ba70fed190708551c19a360f9d7eca8e8c0f615d19a574292b7229e0ae324c \\\n    --hash=sha256:2bd8ef197c87afe0a9009f7a28b5dc613bfc585d329f80b7af404e766aa9e8c7 \\\n    --hash=sha256:2ea4f210422b912ebe58ef0ad33088bc8e5c5ff9655a8822500690abc3b1232d \\\n    --hash=sha256:30cff3ef329682b6182c01c568f551481774c476722020b8f7d0daacbed07a17 \\\n    --hash=sha256:327adab7671f3726b0ba69be9e865bba23b37a605b585e65895c428f6e47e766 \\\n    --hash=sha256:32e02a2d83f4954aa8c17e03fe8ec6962432c39aca4be7e8ee346b05a3476904 \\\n    --hash=sha256:37d66646f929ae7c22c79bc73ec4074d6db45e6384500ee3e0d476daf55482a9 \\\n    --hash=sha256:3a302241fbe825a3e4fe07666a2ab513edfdc6d43ce24b79691b45115273b5e7 \\\n    --hash=sha256:3abd670ca7ce230d5a624fd3d55e055215d8d9b723adee0a348352f5d8d12ff4 \\\n    --hash=sha256:4095a1f2093002c2208becf6f9a178b336b7572512ee0a1179731acb7788e8ad \\\n    --hash=sha256:45535fead66e873f411c1d3cf0d3e175e66f4dd83c4f59d707d5b3e4c56541c4 \\\n    --hash=sha256:45d464622314973d78f364689d5dbb9144e559f93dca11b11af3f2480b5034e1 \\\n    --hash=sha256:4f7290295794b5dec470867c7baa4a14182b9732603fd0caf2a5bf1dc3ccabf3 \\\n    --hash=sha256:4ff380aabd7a74a42a760ee76c68826a8f417ceb6ea415bd574a035a111fd133 \\\n    --hash=sha256:51ffd53c53c4442415b613497a34ba0aa7b99ac07f1e4a62db5dcd640ae6c3c3 \\\n    --hash=sha256:5294fcb410ed0a45d5d1cdedc4e51a60aab5b2b3193999028ea94afc2f554b05 \\\n    --hash=sha256:56e3efe356416bc67a8e093607315951d76910f03d2b3ad49c4ade9207bf710d \\\n    --hash=sha256:5d3cc75ef3e17490042c47e0523aee1bcc4eacd2482796107fd59dd1100a44bc \\\n    --hash=sha256:5e6ee18a53dd5743e6155b8ff7e8e477c25b29b440f87f65be8165275c87fef0 \\\n    --hash=sha256:67a04754d121ea5ca39ddedc3f77071651fb5b0bc6b973c71c515415b44ed9c5 \\\n    --hash=sha256:7394c0b7d460569c9285fa089a429f58465db930012566c03046f9e3ab0ed181 \\\n    --hash=sha256:789c43bf4a10cd067c24c321238e800b8b2716c863ddb2294d2fed886fa5a689 \\\n    --hash=sha256:7ac67b542505186b3bbdaffbc303292e1ee9c8729e5d5df243c1f20f4bb9057e \\\n    --hash=sha256:8561c48b0090993e3b2a54db480cab1d23eb2c5735067213bb90f402806339f5 \\\n    --hash=sha256:86bfb52a9cfbcc09aba2b71388b0a20ea5c52b6517c0b2e316222435a8cdab72 \\\n    --hash=sha256:8711682a629bbcaf492f5e0af72d378e976ea1d127a2d47584fa1c2c080b436b \\\n    --hash=sha256:89da58e4005e153b03fe8b8794330e3f6a9774ee9e1c3bd5bc52eb098c3b0c4f \\\n    --hash=sha256:89f72524033abbfde880ad338fd3c2c16e31ae232323ebdfbc745cbb1b3dcc03 \\\n    --hash=sha256:8bf1ab71f9f23b0a1d52ec1682a3907e0c208c12fef9c3e99d2b80166b17905f \\\n    --hash=sha256:8d7bbbe2cd6ed80aceef2a14e9f1c1b61683194c216472ed5ff33b700e784e37 \\\n    --hash=sha256:94c4a9b01eede952442c088d415861b0cf2053cbd696b863f6d5022d4e4e2453 \\\n    --hash=sha256:98dcf978d4c6048965d1762abd534c9d53bae981a035bfe486690ba11f49bbbb \\\n    --hash=sha256:a4cc73a6ae0a6751b76e69cece9d0311f054da9b22df6a12f2c53111735657c8 \\\n    --hash=sha256:a9f8e33747b1332db11cf7fcf4a9512bef9748cb5eb4d3f7fbc8c30d75dc6ffc \\\n    --hash=sha256:ace960769d60037ca9625b4c578a6f28a14301bd2a1ff13bb00e824ac9f73e55 \\\n    --hash=sha256:ae721bcc8e69846af00b7a77a220614d9b2ec57d25017a6bbde3a99473e41ce8 \\\n    --hash=sha256:aea01f40995fa0945c020228ab919b8dfc93fc8a9f2d3d705ab5b793f32d9e99 \\\n    --hash=sha256:b499caef4bca9cbd0bd23cd3386f5113ee7378094a3cb613a2fa543260fe9506 \\\n    --hash=sha256:b89504227a5311610e4be16071465885a0a3d6b0e82e305ef46d9b064ce5fb72 \\\n    --hash=sha256:bd66b4865c8b853b8cca7379afb692fc7f52cf898786537dfb5e5e2d64f0a47f \\\n    --hash=sha256:bfcd3acc1a81f106abac6afd42327d2cf1e77ec905ae11dc1d9142a006a496b6 \\\n\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Monitor for Containerized Multi-Node Testing\nDESCRIPTION: Command to start the Docker monitor process, which manages the docker compose setup for containerized multi-node testing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/fake-autoscaler.rst#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$ python ./python/ray/autoscaler/_private/fake_multi_node/docker_monitor.py \\\n    ./python/ray/autoscaler/_private/fake_multi_node/example_docker.yaml\n```\n\n----------------------------------------\n\nTITLE: Identifying Out-of-Memory Worker Exit Errors in Ray\nDESCRIPTION: Error message showing when a Ray worker has been terminated by the Linux OOM killer. This message appears when calling ray.get on tasks or actors that were running on a worker killed unexpectedly by SIGKILL.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/debug-memory.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nWorker exit type: UNEXPECTED_SY STEM_EXIT Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n```\n\n----------------------------------------\n\nTITLE: Dependency Requirements List for Ray ML Project\nDESCRIPTION: Specifies the required Python packages and their versions for running machine learning workloads in the Ray project. Includes deep learning frameworks, model training utilities, and evaluation tools.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/testing/docker/04_finetuning_llms_with_deepspeed/requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndeepspeed==0.10.2\nfairscale\ntransformers>=4.36.2\ndataset\naccelerate\nevaluate\nwandb\npytorch-lightning\nprotobuf<3.21.0\ntorchmetrics\nsentencepiece\npeft==0.7.0\n```\n\n----------------------------------------\n\nTITLE: Reproducing Resource Insufficiency in Kubernetes Cluster\nDESCRIPTION: This snippet shows how to reproduce a situation where a Kubernetes cluster doesn't have enough resources to accommodate a Ray Serve application, leading to a loop of RayCluster restarts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Step 1: Get the number of CPUs available on the node\nkubectl get nodes -o custom-columns=NODE:.metadata.name,ALLOCATABLE_CPU:.status.allocatable.cpu\n\n# [Example output]\n# NODE                 ALLOCATABLE_CPU\n# kind-control-plane   8\n\n# Step 2: Install a KubeRay operator.\n\n# Step 3: Create a RayService with autoscaling enabled.\nkubectl apply -f ray-service.insufficient-resources.yaml\n\n# Step 4: The Kubernetes cluster will not have enough resources to accommodate the serve application.\nkubectl describe rayservices.ray.io rayservice-sample -n $YOUR_NAMESPACE\n\n# [Example output]\n# fruit_app_FruitMarket:\n#   Health Last Update Time:  2023-07-11T02:10:02Z\n#   Last Update Time:         2023-07-11T02:10:35Z\n#   Message:                  Deployment \"fruit_app_FruitMarket\" has 1 replicas that have taken more than 30s to be scheduled. This may be caused by waiting for the cluster to auto-scale, or waiting for a runtime environment to install. Resources required for each replica: {\"CPU\": 1.0}, resources available: {}.\n#   Status:                   UPDATING\n\n# Step 5: A new RayCluster will be created after `serviceUnhealthySecondThreshold` (300s here) seconds.\n# Check the logs of the KubeRay operator to find the reason for restarting the RayCluster.\nkubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n\n# [Example output]\n# 2023-07-11T02:14:58.109Z\tINFO\tcontrollers.RayService\tRestart RayCluster\t{\"appName\": \"fruit_app\", \"restart reason\": \"The status of the serve application fruit_app has not been RUNNING for more than 300.000000 seconds. Hence, KubeRay operator labels the RayCluster unhealthy and will prepare a new RayCluster.\"}\n# 2023-07-11T02:14:58.109Z\tINFO\tcontrollers.RayService\tRestart RayCluster\t{\"deploymentName\": \"fruit_app_FruitMarket\", \"appName\": \"fruit_app\", \"restart reason\": \"The status of the serve deployment fruit_app_FruitMarket or the serve application fruit_app has not been HEALTHY/RUNNING for more than 300.000000 seconds. Hence, KubeRay operator labels the RayCluster unhealthy and will prepare a new RayCluster. The message of the serve deployment is: Deployment \\\"fruit_app_FruitMarket\\\" has 1 replicas that have taken more than 30s to be scheduled. This may be caused by waiting for the cluster to auto-scale, or waiting for a runtime environment to install. Resources required for each replica: {\\\"CPU\\\": 1.0}, resources available: {}.\"}\n# .\n# .\n# .\n# 2023-07-11T02:14:58.122Z\tINFO\tcontrollers.RayService\tRestart RayCluster\t{\"ServiceName\": \"default/rayservice-sample\", \"AvailableWorkerReplicas\": 1, \"DesiredWorkerReplicas\": 5, \"restart reason\": \"The serve application is unhealthy, restarting the cluster. If the AvailableWorkerReplicas is not equal to DesiredWorkerReplicas, this may imply that the Autoscaler does not have enough resources to scale up the cluster. Hence, the serve application does not have enough resources to run. Please check https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayservice-troubleshooting.md for more details.\", \"RayCluster\": {\"apiVersion\": \"ray.io/v1alpha1\", \"kind\": \"RayCluster\", \"namespace\": \"default\", \"name\": \"rayservice-sample-raycluster-hvd9f\"}}\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Transfer Encoding Check\nDESCRIPTION: Java stack trace showing HTTP transfer encoding verification. This trace follows Netty's event processing through to HTTP header analysis, specifically checking if the transfer encoding is chunked.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_100\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/handler/codec/http/HttpObjectDecoder:.decode_[j];io/netty/handler/codec/http/HttpHeaders:.isTransferEncodingChunked_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray Project Status and Trial Results\nDESCRIPTION: This snippet shows the status of a Ray project, including memory usage, resource allocation, and a table of trial results for various Atari game environments using the IMPALA algorithm. It includes information such as trial name, status, environment, iterations, total time, timesteps, and reward.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.5/stress_tests/application_stress_test.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n== Status ==\nMemory usage on this node: 23.4/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/640 CPUs, 0/8 GPUs, 0.0/1905.86 GiB heap, 0.0/566.21 GiB objects\nResult logdir: /home/ubuntu/ray_results/atari-impala\nNumber of trials: 4 (4 TERMINATED)\n+------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------+\n| Trial name                               | status     | loc   | env                         |   iter |   total time (s) |       ts |   reward |\n|------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_00000      | TERMINATED |       | BreakoutNoFrameskip-v4      |    360 |          5974.47 | 30014500 |  488.4   |\n| IMPALA_BeamRiderNoFrameskip-v4_00001     | TERMINATED |       | BeamRiderNoFrameskip-v4     |    366 |          6026.01 | 30031500 |  424.84  |\n| IMPALA_QbertNoFrameskip-v4_00002         | TERMINATED |       | QbertNoFrameskip-v4         |    356 |          5897.19 | 30077500 | 5230.25  |\n| IMPALA_SpaceInvadersNoFrameskip-v4_00003 | TERMINATED |       | SpaceInvadersNoFrameskip-v4 |    361 |          5990.74 | 30103500 |  806.043 |\n+------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------+\n```\n\n----------------------------------------\n\nTITLE: Displaying Ray Performance Benchmarks\nDESCRIPTION: Comprehensive benchmark results showing performance metrics for Ray operations including Plasma Store operations, task execution, and actor calls. Includes measurements for synchronous and asynchronous operations in various client configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.5/microbenchmark.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsingle client get calls (Plasma Store) per second 8214.4 +- 892.64\nsingle client put calls (Plasma Store) per second 5356.26 +- 36.53\nsingle client put gigabytes per second 13.46 +- 2.94\nmulti client put calls (Plasma Store) per second 11351.56 +- 62.33\nmulti client put gigabytes per second 16.44 +- 8.49\nsingle client tasks sync per second 1311.81 +- 66.92\nsingle client tasks async per second 15531.87 +- 220.75\nmulti client tasks async per second 43140.09 +- 880.94\n1:1 actor calls sync per second 1916.18 +- 97.0\n1:1 actor calls async per second 5963.83 +- 115.07\n1:1 actor calls concurrent per second 5604.0 +- 222.31\n1:n actor calls async per second 10275.4 +- 167.94\nn:n actor calls async per second 34553.25 +- 196.74\nn:n actor calls with arg async per second 12522.36 +- 228.16\n1:1 async-actor calls sync per second 1096.88 +- 4.27\n1:1 async-actor calls async per second 3407.27 +- 74.07\n1:1 async-actor calls with args async per second 2406.66 +- 46.86\n1:n async-actor calls async per second 9109.89 +- 121.35\nn:n async-actor calls async per second 27338.51 +- 358.6\n```\n\n----------------------------------------\n\nTITLE: Disabling Dated Subdirectories in Ray Tune\nDESCRIPTION: Setting TUNE_DISABLE_DATED_SUBDIR to 1 prevents Ray Tune from automatically adding date strings to experiment directories when the name is not explicitly specified.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_DISABLE_DATED_SUBDIR=1\n```\n\n----------------------------------------\n\nTITLE: Analyzing Netty NIO Event Loop Stack Trace with TCP/IP Kernel Operations\nDESCRIPTION: A stack trace showing the complete path of network operations from Java Netty event loop through kernel TCP/IP stack. The trace demonstrates how data flows from Java application code through Netty's NIO components, into system calls, and through the Linux kernel's TCP/IP implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_15\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];__inet_lookup_established_[k] 3\n```\n\n----------------------------------------\n\nTITLE: Long-running Ray Script\nDESCRIPTION: Python script demonstrating a continuous Ray task execution with sleep intervals.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/quickstart.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# script.py\nimport ray\nimport time\n\n@ray.remote\ndef hello_world():\n    return \"hello world\"\n\nray.init()\nwhile True:\n    print(ray.get(hello_world.remote()))\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution with Netty NIO and Socket Operations\nDESCRIPTION: This stack trace shows the execution path of a Java thread using Netty's NIO implementation for socket operations. It includes the process of reading data from a socket channel and the underlying system calls.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_106\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/socket/nio/NioSocketChannel:.doReadBytes_[j];sun/nio/ch/SocketChannelImpl:.read_[j];sun/nio/ch/FileDispatcherImpl:.read0_[j];read\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependency Source and Constraints\nDESCRIPTION: This snippet provides additional context for the package dependencies. It indicates that the hashes are sourced from a specific requirements file for Ray tests on Python 3.11 with CUDA 12.1. It also mentions that these dependencies are related to the 'aiohttp' package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_27\n\nLANGUAGE: Text\nCODE:\n```\n# via\n#   -c python/requirements_compiled_ray_test_py311_cu121.txt\n#   aiohttp\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Resources\nDESCRIPTION: Terminates port forwarding and deletes the Kind cluster\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nkillall kubectl\nkind delete cluster\n```\n\n----------------------------------------\n\nTITLE: Specifying prompt-toolkit Package Dependency\nDESCRIPTION: This snippet specifies the prompt-toolkit package version 3.0.41 with SHA-256 hashes for verification. It indicates that this dependency is required by ipython in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_36\n\nLANGUAGE: plaintext\nCODE:\n```\nprompt-toolkit==3.0.41 \\\n    --hash=sha256:941367d97fc815548822aa26c2a269fdc4eb21e9ec05fc5d447cf09bad5d75f0 \\\n    --hash=sha256:f36fe301fafb7470e86aaf90f036eef600a3210be4decf461a5b1ca8403d3cb2\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   ipython\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with Hashes\nDESCRIPTION: Detailed package requirements with specific versions and SHA-256 hashes for security verification\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_1\n\nLANGUAGE: pip-requirements\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cpu\n--find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html\n\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   aiohttp\n```\n\n----------------------------------------\n\nTITLE: Updating ECR Image Configuration\nDESCRIPTION: Commands to set environment variables and update the Ray cluster manifest with AWS account and region information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/aws-trainium/llama3.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport AWS_ACCOUNT_ID=<enter_your_aws_account_id> # for ex: 111222333444\nexport REGION=<enter_your_aws_region> # for ex: us-east-2\nsed -i \"s/<AWS_ACCOUNT_ID>/$AWS_ACCOUNT_ID/g\" 1-llama3-finetune-trn1-create-raycluster.yaml\nsed -i \"s/<REGION>/$REGION/g\" 1-llama3-finetune-trn1-create-raycluster.yaml\n```\n\n----------------------------------------\n\nTITLE: JavaScript Object Property Setting in Netty/Vert.x Request Processing\nDESCRIPTION: This stack trace demonstrates the process of setting a property on a JavaScript object during the execution of server-side JavaScript code in response to an HTTP request in a Netty/Vert.x environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_78\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/IdScriptableObject:.put_[j]\n```\n\n----------------------------------------\n\nTITLE: Listing Additional Dependencies for Ray[all] in Python\nDESCRIPTION: This code block enumerates additional dependencies required for the full Ray package (ray[all]). It includes libraries for various functionalities such as data processing, machine learning, and API development.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements.txt#2025-04-12_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsmart_open\nlz4\nnumpy>=1.20\naiorwlock\nopentelemetry-exporter-otlp\nscipy\ncolorful\nrich\nopentelemetry-sdk\nfastapi\ngymnasium==1.0.0\nvirtualenv!=20.21.1,>=20.0.24\nopentelemetry-api\nopencensus\naiohttp_cors\ndm_tree\nuvicorn\nscikit-image>=0.21.0\nprometheus_client>=0.7.1\npandas\ntensorboardX\naiohttp>=3.7\nstarlette\ntyper\nfsspec\npandas>=1.3\npydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3  # Serve users can use pydantic<2\npy-spy>=0.2.0; python_version < '3.12'\npy-spy>=0.4.0; python_version >= '3.12'\nmemray; sys_platform != \"win32\" # memray is not supported on Windows\npyOpenSSL\n```\n\n----------------------------------------\n\nTITLE: Kubernetes YAML Configuration for Redis Setup - YAML\nDESCRIPTION: This YAML snippet provides an example configuration for deploying a single-node Redis instance on Kubernetes. It includes a ConfigMap for Redis configuration, a Service to expose Redis, and a Deployment for the Redis container. This setup is intended for development purposes only.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n\"\"\"Kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: redis-config\n  labels:\n    app: redis\ndata:\n  redis.conf: |-\n    port 6379\n    bind 0.0.0.0\n    protected-mode no\n    requirepass 5241590000000000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  type: ClusterIP\n  ports:\n    - name: redis\n      port: 6379\n  selector:\n    app: redis\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n        - name: redis\n          image: redis:5.0.8\n          command:\n            - \"sh\"\n            - \"-c\"\n            - \"redis-server /usr/local/etc/redis/redis.conf\"\n          ports:\n            - containerPort: 6379\n          volumeMounts:\n            - name: config\n              mountPath: /usr/local/etc/redis/redis.conf\n              subPath: redis.conf\n      volumes:\n        - name: config\n          configMap:\n            name: redis-config\n---\"\n```\n\n----------------------------------------\n\nTITLE: Ray Performance Metrics Output\nDESCRIPTION: Performance benchmark results showing throughput metrics for Ray's core operations including Plasma Store operations, task execution, and actor calls. Note requires OMP_NUM_THREADS=64 for optimal put operations performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.2/microbenchmark.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsingle client get calls (Plasma Store) per second 11743.14 +- 2062.85\nsingle client put calls (Plasma Store) per second 3133.08 +- 89.81\nsingle client put gigabytes per second 10.33 +- 7.96\nmulti client put calls (Plasma Store) per second 3590.16 +- 22.04\nmulti client put gigabytes per second 23.38 +- 0.63\nsingle client tasks sync per second 1263.59 +- 63.16\nsingle client tasks async per second 13959.14 +- 393.16\nmulti client tasks async per second 42285.81 +- 238.55\n1:1 actor calls sync per second 2159.21 +- 112.97\n1:1 actor calls async per second 7048.53 +- 63.8\n1:1 actor calls concurrent per second 6167.01 +- 75.67\n1:n actor calls async per second 12241.67 +- 62.13\nn:n actor calls async per second 41766.33 +- 672.14\nn:n actor calls with arg async per second 13134.22 +- 71.68\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Behavior in RLlib\nDESCRIPTION: Shows how to configure training-specific settings using the training method of AlgorithmConfig, including hyperparameters and learning strategies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/algorithm-config.rst#2025-04-12_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nconfig.training(gamma=0.99, lr=0.001)\n```\n\n----------------------------------------\n\nTITLE: Specifying RSA Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the RSA package dependency with version 4.7.2 and SHA-256 hashes for verification. It includes comments indicating that this package is required by gcs-oauth2-boto-plugin, google-auth, and oauth2client.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_33\n\nLANGUAGE: pip\nCODE:\n```\nrsa==4.7.2 \\\n    --hash=sha256:78f9a9bf4e7be0c5ded4583326e7461e3a3c5aae24073648b4bdfa797d78c9d2 \\\n    --hash=sha256:9d689e6ca1b3038bc82bf8d23e944b6b6037bc02301a574935b2dd946e0353b9\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gcs-oauth2-boto-plugin\n    #   google-auth\n    #   oauth2client\n```\n\n----------------------------------------\n\nTITLE: Java Process Writing with Hypervisor Interaction\nDESCRIPTION: Thread stack trace showing Java process performing write operations with hypervisor interactions. This suggests the Java process is running in a virtualized environment with hypercalls for certain operations.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_117\n\nLANGUAGE: java\nCODE:\n```\njava;write;check_events_[k];hypercall_page_[k]\n```\n\n----------------------------------------\n\nTITLE: Ray Project Performance Metrics JSON\nDESCRIPTION: JSON output showing detailed performance metrics for a Ray project execution across multiple stages. Includes timing data for different stages, iteration statistics, runtime information, and project-specific URLs for session and commit references.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/stress_tests/many_tasks.txt#2025-04-12_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": 1,\n  \"stage_0_time\": 5.256332874298096,\n  \"stage_1_time\": 174.50774693489075,\n  \"stage_1_avg_iteration_time\": 17.450765538215638,\n  \"stage_1_max_iteration_time\": 17.627604961395264,\n  \"stage_1_min_iteration_time\": 17.23277997970581,\n  \"stage_2_time\": 268.01243686676025,\n  \"stage_2_avg_iteration_time\": 53.60213441848755,\n  \"stage_2_max_iteration_time\": 59.097413063049316,\n  \"stage_2_min_iteration_time\": 48.71518564224243,\n  \"stage_3_creation_time\": 0.5777060985565186,\n  \"stage_3_time\": 2066.70570230484,\n  \"stage_4_spread\": 3.2197082901427945,\n  \"_runtime\": 5045.744384527206,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_b8v2V4Tr7vwee6tCDjTjdXLL\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with JavaScript Object Wrapping\nDESCRIPTION: This stack trace shows Netty's event processing flow with JavaScript object wrapping. The trace illustrates how HTTP server handlers in Vert.x use Mozilla's Rhino JavaScript engine to wrap Java objects for use in JavaScript during request processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_49\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/WrapFactory:.wrapAsJavaObject_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Full Documentation Build from Scratch\nDESCRIPTION: Command to perform a complete rebuild of the documentation without using any cache.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmake develop\n```\n\n----------------------------------------\n\nTITLE: Java Netty NIO Event Processing with RelinkedSlot Value Access\nDESCRIPTION: Stack trace showing the execution path to JavaScript RelinkedSlot getValue method. This trace demonstrates how optimized property access works in the JavaScript engine during HTTP request processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_55\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.getObjectProp_[j];org/mozilla/javascript/ScriptableObject$RelinkedSlot:.getValue_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Stopping Ray Tune Experiment on Trial Errors\nDESCRIPTION: Uses FailureConfig to stop the entire experiment immediately if any trial encounters a runtime error.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-stopping.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    train_function,\n    tune_config=tune.TuneConfig(\n        num_samples=5,\n        failure_config=ray.tune.FailureConfig(fail_fast=True)\n    ),\n    param_space={\"base_accuracy\": 0.3}\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Environment Variable Propagation in Python with Ray\nDESCRIPTION: Illustrates a scenario in which environment variables set on the Ray Driver are not propagated to Worker processes. The solution involves using Ray's runtime environment configuration to explicitly pass the required environment variables.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/general-debugging.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example where the environment variable set on the Driver is not propagated to Worker processes\nimport os\nimport ray\n\nray.init()\n\n@ray.remote\ndef func():\n    return os.environ.get(\"MY_ENV_VAR\", \"not set\")\n\nprint(ray.get(func.remote()))\n```\n\nLANGUAGE: python\nCODE:\n```\n# Fix by enabling Runtime Environments\ndriver_runtime_env = {\"env_vars\": {\"MY_ENV_VAR\": \"foo\"}}\nray.init(runtime_env=driver_runtime_env)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for DeepSpeed Integration\nDESCRIPTION: This requirements file specifies the exact version of DeepSpeed (0.12.3), the datasets package with no version constraint, and a minimum version of huggingface-hub (0.24.0 or newer) required for the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/train-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndeepspeed==0.12.3\ndatasets\nhuggingface-hub>=0.24.0\n```\n\n----------------------------------------\n\nTITLE: Running Large-Scale GPU Image Training Benchmark\nDESCRIPTION: This command runs a PyTorch training benchmark on 100GB of image data using 16 workers across multiple nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/benchmarks.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython pytorch_training_e2e.py --data-size-gb=100 --num-workers=16\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Request Processing Stack Trace\nDESCRIPTION: Java stack trace showing the flow of an HTTP request through Netty's NIO event loop, channel handlers, and Mozilla JavaScript components. The trace starts from thread initialization and proceeds through Netty's selection key processing to HTTP header handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_96\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];io/netty/handler/codec/http/DefaultHttpHeaders:.set_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Forcing Execution on Specified Node\nDESCRIPTION: Defines utility functions to schedule Ray tasks on specific nodes, based on node affinity or resource availability, ensuring efficient resource utilization during the model download step.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport subprocess\nimport ray.util.scheduling_strategies\n\ndef force_on_node(node_id: str, remote_func_or_actor_class):\n    scheduling_strategy = ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(\n        node_id=node_id, soft=False\n    )\n    options = {\"scheduling_strategy\": scheduling_strategy}\n    return remote_func_or_actor_class.options(**options)\n\ndef run_on_every_node(remote_func_or_actor_class, **remote_kwargs):\n    refs = []\n    for node in ray.nodes():\n        if node[\"Alive\"] and node[\"Resources\"].get(\"GPU\", None):\n            refs.append(\n                force_on_node(node[\"NodeID\"], remote_func_or_actor_class).remote(\n                    **remote_kwargs\n                )\n            )\n    return ray.get(refs)\n\n\n@ray.remote(num_gpus=1)\ndef download_model():\n    from transformers.utils.hub import TRANSFORMERS_CACHE\n\n    path = os.path.expanduser(\n        os.path.join(TRANSFORMERS_CACHE, \"models--EleutherAI--gpt-j-6B\")\n    )\n    subprocess.run([\"mkdir\", \"-p\", os.path.join(path, \"snapshots\", \"main\")])\n    subprocess.run([\"mkdir\", \"-p\", os.path.join(path, \"refs\")])\n    if os.path.exists(os.path.join(path, \"refs\", \"main\")):\n        return\n    subprocess.run(\n        [\n            \"aws\",\n            \"s3\",\n            \"sync\",\n            \"--no-sign-request\",\n            \"s3://large-dl-models-mirror/models--EleutherAI--gpt-j-6B/main/\",\n            os.path.join(path, \"snapshots\", \"main\"),\n        ]\n    )\n    with open(os.path.join(path, \"snapshots\", \"main\", \"hash\"), \"r\") as f:\n        f_hash = f.read().strip()\n    with open(os.path.join(path, \"refs\", \"main\"), \"w\") as f:\n        f.write(f_hash)\n    os.rename(\n        os.path.join(path, \"snapshots\", \"main\"), os.path.join(path, \"snapshots\", f_hash)\n    )\n\n\n_ = run_on_every_node(download_model)\n```\n\n----------------------------------------\n\nTITLE: Expected Warning Message for HCCL Backend\nDESCRIPTION: A warning message that appears when using the HCCL backend with PyTorch distributed. This warning is expected and will be resolved in SynapseAI version 1.14.0 or later.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/resnet.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n\n```\n\n----------------------------------------\n\nTITLE: Ray Status Output Display\nDESCRIPTION: Text-based status output showing system resource usage, scheduling status, and training results table for multiple Atari environments using IMPALA algorithm. Contains memory metrics, CPU/GPU allocation, and detailed trial results including completion status, environment names, iterations, timing and reward scores.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.3/stress_tests/application_stress_test.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n== Status ==\nMemory usage on this node: 23.2/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/640 CPUs, 0/8 GPUs, 0.0/1905.76 GiB heap, 0.0/566.21 GiB objects\nResult logdir: /home/ubuntu/ray_results/atari-impala\nNumber of trials: 4 (4 TERMINATED)\n+------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------+\n| Trial name                               | status     | loc   | env                         |   iter |   total time (s) |       ts |   reward |\n|------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_00000      | TERMINATED |       | BreakoutNoFrameskip-v4      |    378 |          6722.44 | 30011000 |   470.62 |\n| IMPALA_BeamRiderNoFrameskip-v4_00001     | TERMINATED |       | BeamRiderNoFrameskip-v4     |    362 |          6494.28 | 30026000 |  3006.44 |\n| IMPALA_QbertNoFrameskip-v4_00002         | TERMINATED |       | QbertNoFrameskip-v4         |    364 |          6512.25 | 30016500 | 10843.5  |\n| IMPALA_SpaceInvadersNoFrameskip-v4_00003 | TERMINATED |       | SpaceInvadersNoFrameskip-v4 |    380 |          6752.71 | 30014500 |   677.1  |\n+------------------------------------------+------------+-------+-----------------------------+--------+------------------+----------+----------+\n```\n\n----------------------------------------\n\nTITLE: Setting Autoscaler Environment Variable - Bash\nDESCRIPTION: Command to set the AUTOSCALER_MAX_NUM_FAILURES environment variable to infinity for long-running clusters to prevent unexpected autoscaler crashes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/large-cluster-best-practices.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AUTOSCALER_MAX_NUM_FAILURES=inf;\n```\n\n----------------------------------------\n\nTITLE: Platform-specific Package Dependency for xformers\nDESCRIPTION: Defines a platform-specific dependency for the xformers package, limited to x86_64 Linux systems, with SHA256 hash verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_38\n\nLANGUAGE: text\nCODE:\n```\nxformers==0.0.29.post2 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:bbf0e9505f6b2e2b7738eeb3c22e94c45e6297fbdae66626febb0dbfe28c5050\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Ray Network Processing with TCP RTT Estimation\nDESCRIPTION: This stack trace is similar to the previous ones but includes TCP round-trip time (RTT) estimation. It demonstrates how Ray's network processing incorporates TCP performance optimization techniques at the kernel level.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_23\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];tcp_v4_do_rcv_[k];tcp_rcv_established_[k];tcp_ack_[k];tcp_clean_rtx_queue_[k];tcp_rtt_estimator_[k]\n```\n\n----------------------------------------\n\nTITLE: Minimal AWS Configuration (Autoscaler)\nDESCRIPTION: This YAML configuration showcases a minimal Ray autoscaler setup for AWS, defining the essential parameters needed to launch and manage a Ray cluster on AWS. It includes node types and resource allocation, demonstrating a basic but functional configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/ray-cluster-configuration.rst#2025-04-12_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/aws/example-minimal.yaml\n            :language: yaml\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Dependencies for Ray Documentation\nDESCRIPTION: Comprehensive requirements file that specifies exact versions of Python packages needed for building Ray documentation. Includes Sphinx and its extensions, MyST parser for markdown, Jupyter conversion tools, and other utilities. External dependencies like ML libraries are meant to be mocked rather than installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements-doc.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Production requirements. This is what readthedocs.com picks up\n\n# Required to build the docs on 3.12 due to pkg_resources deprecation\nsetuptools>=70.0.0\n\n# Syntax highlighting\nPygments==2.16.1\n\n# Sphinx\nsphinx==7.3.7\nsphinx-click==5.1.0\nsphinx-copybutton==0.5.2\nsphinxemoji==0.2.0\nsphinx-jsonschema==1.19.1\nsphinx-sitemap==2.5.1\nsphinxcontrib-redoc==1.6.0\nsphinx-remove-toctrees==0.0.3\nsphinx_design==0.5.0\nsphinx-autobuild==2024.4.16\npydata-sphinx-theme==0.14.1\nautodoc_pydantic==2.2.0\nappnope\nsphinx-docsearch==0.0.7\n\npydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3\n\n# MyST\nmyst-parser==2.0.0 # Needed to parse markdown\nmyst-nb==1.0.0rc0 # Most recent version of myst-nb; pin when new release is made\n\n# Jupyter conversion\njupytext==1.15.2\n\n# Pin urllib to avoid downstream ssl incompatibility issues\nurllib3 < 1.27\n\n# External dependencies such as ML libraries should be mocked out, not added here.\n# See doc/source/conf.py for examples of how to mock out external dependencies.\nclick==8.1.7\nboto3==1.34.69\nrequests==2.32.3\n```\n\n----------------------------------------\n\nTITLE: Reporting Metrics to Ray Train During Training\nDESCRIPTION: Captures validation loss and reports it to Ray Train, which allows tracking of metrics during distributed training and enables features like early stopping.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/convert_existing_pytorch_code_to_ray_train.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n        test_loss = test(test_dataloader, model, loss_fn)\n        train.report(dict(loss=test_loss))\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution and Garbage Collection\nDESCRIPTION: This snippet shows the execution path of Java threads, including garbage collection tasks such as scavenging roots and promoting objects.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_0\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;GCTaskThread::run;ScavengeRootsTask::do_it;ClassLoaderDataGraph::oops_do;ClassLoaderData::oops_do;PSScavengeKlassClosure::do_klass\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;GCTaskThread::run;StealTask::do_it;PSPromotionManager::drain_stacks_depth;oopDesc* PSPromotionManager::copy_to_survivor_space<false>;InstanceKlass::oop_push_contents\n```\n\n----------------------------------------\n\nTITLE: Ejecting Configuration\nDESCRIPTION: Removes the single build dependency and copies all configuration files and transitive dependencies into the project for full control. This is a one-way operation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/README.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm run eject\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Head Pod to Avoid Ray Serve Replicas\nDESCRIPTION: YAML configuration for the Ray head Pod that sets CPU resources to 0, preventing Ray Serve replicas from running on the head Pod and forcing them to run on worker Pods.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.md#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nheadGroupSpec:\n  rayStartParams:\n    num-cpus: \"0\"\n  template: ...\n```\n\n----------------------------------------\n\nTITLE: Ray Training Results Table Output\nDESCRIPTION: ASCII table showing training metrics across 12 trials of different reinforcement learning algorithms. Includes trial names, status, iterations, runtime, timesteps, rewards and episode statistics.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.2.0/rllib_regression_torch.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n+-------------------------------------------+------------+-------+--------+------------------+---------+-----------+----------------------+----------------------+--------------------+\n| Trial name                                | status     | loc   |   iter |   total time (s) |      ts |    reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n|-------------------------------------------+------------+-------+--------+------------------+---------+-----------+----------------------+----------------------+--------------------|\n| A2C_BreakoutNoFrameskip-v4_a6f57_00000    | TERMINATED |       |    353 |          3603.76 | 3378500 |   1.93    |               15     |                0     |            821.58  |\n| A2C_BreakoutNoFrameskip-v4_a6f57_00001    | TERMINATED |       |    353 |          3608.48 | 3404500 |   1.15    |                6     |                0     |            701.51  |\n| APEX_BreakoutNoFrameskip-v4_a6f57_00002   | TERMINATED |       |    113 |          3615.57 | 5680160 |   1.6381  |                9     |                0     |            773.381 |\n| APEX_BreakoutNoFrameskip-v4_a6f57_00003   | TERMINATED |       |    114 |          3636.38 | 5764800 |   1.39655 |                6     |                0     |            735.914 |\n| DQN_BreakoutNoFrameskip-v4_a6f57_00004    | TERMINATED |       |     27 |          3684.72 |  280000 |   1.79    |               12     |                0     |            743.6   |\n| DQN_BreakoutNoFrameskip-v4_a6f57_00005    | TERMINATED |       |     27 |          3685.26 |  280000 |   1.14    |                5     |                0     |            699.19  |\n| IMPALA_BreakoutNoFrameskip-v4_a6f57_00006 | TERMINATED |       |    356 |          3606.67 | 7850250 |   1.7803  |               12     |                0     |            795.455 |\n| IMPALA_BreakoutNoFrameskip-v4_a6f57_00007 | TERMINATED |       |    355 |          3609.98 | 7903500 |   1.68217 |                8     |                0     |            796.659 |\n| PPO_BreakoutNoFrameskip-v4_a6f57_00008    | TERMINATED |       |   1401 |          3601.51 | 7005000 |   2.61    |               10     |                0     |            897.83  |\n| PPO_BreakoutNoFrameskip-v4_a6f57_00009    | TERMINATED |       |   1406 |          3600.35 | 7030000 |   1.47    |               11     |                0     |            647.8   |\n| SAC_HalfCheetahBulletEnv-v0_a6f57_00010   | TERMINATED |       |     37 |          3686.44 |   46000 | 641.43    |              723.144 |              504.62  |           1000     |\n| SAC_HalfCheetahBulletEnv-v0_a6f57_00011   | TERMINATED |       |     37 |          3645.16 |   46000 | 631.65    |              664.021 |              599.864 |           1000     |\n+-------------------------------------------+------------+-------+--------+------------------+---------+-----------+----------------------+----------------------+--------------------+\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Cluster with Kind\nDESCRIPTION: Command to create a Kubernetes cluster using Kind with a specific Kubernetes version.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed for Vicuna-13B Training\nDESCRIPTION: This snippet sets up the DeepSpeed configuration for training the Vicuna-13B model. It enables ZeRO stage-3 optimization, CPU offloading for the optimizer, and other performance-enhancing settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(MODEL_NAME)\nHIDDEN_SIZE = config.hidden_size\n\ndeepspeed_configs = {\n    \"zero_allow_untested_optimizer\": True,\n    \"bf16\": {\"enabled\": True},\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"reduce_bucket_size\": HIDDEN_SIZE * HIDDEN_SIZE,\n        \"stage3_prefetch_bucket_size\": 0.9 * HIDDEN_SIZE * HIDDEN_SIZE,\n        \"stage3_param_persistence_threshold\": 10 * HIDDEN_SIZE,\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Stack Trace Retrieval using GDB\nDESCRIPTION: Command to view current stack trace of any running Ray process using GDB, useful for debugging CPU utilization issues or infinite loops.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/profiling.rst#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo gdb -batch -ex \"thread apply all bt\" -p <pid>\n```\n\n----------------------------------------\n\nTITLE: Installing Istio with Custom Configuration\nDESCRIPTION: Commands to download and install Istio with specific configuration including 100% trace sampling, disabled sanitize_te, and TLS 1.3 enabled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/istio.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Download Istioctl and its manifests.\nexport ISTIO_VERSION=1.21.1\ncurl -L https://istio.io/downloadIstio | sh -\ncd istio-1.21.1\nexport PATH=$PWD/bin:$PATH\n\n# Install Istio with:\n#   1. 100% trace sampling for demo purposes.\n#   2. \"sanitize_te\" disabled for proper gRPC interception. This is required by Istio 1.21.0 (https://github.com/istio/istio/issues/49685).\n#   3. TLS 1.3 enabled.\nistioctl install -y -f - <<EOF\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\n  meshConfig:\n    defaultConfig:\n      tracing:\n        sampling: 100\n      runtimeValues:\n        envoy.reloadable_features.sanitize_te: \"false\"\n    meshMTLS:\n      minProtocolVersion: TLSV1_3\nEOF\n\n# Install Istio addons, including the Kiali and Jaeger dashboards.\nkubectl apply -f samples/addons\n# Enable the Istio sidecar auto injection.\nkubectl label namespace default istio-injection=enabled\n```\n\n----------------------------------------\n\nTITLE: Running Tune Experiment with Space\nDESCRIPTION: Runs the Tune experiment using the Nevergrad search algorithm and the Nevergrad-defined search space. Note the param_space only includes steps here, the other parameters are passed to the search alg.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"tuner = tune.Tuner(\\n    objective,\\n    tune_config=tune.TuneConfig(\\n#         metric=\\\"mean_loss\\\",\\n#         mode=\\\"min\\\",\n```\n\nLANGUAGE: python\nCODE:\n```\n        search_alg=algo,\\n        num_samples=num_samples,\\n    ),\\n    param_space={\\\"steps\\\": 100},\\n)\\nresults = tuner.fit()\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Even Number Function - Doctest Style\nDESCRIPTION: Example showing how to write a doctest-style code snippet that demonstrates a basic even number checking function with interactive Python-like syntax.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/writing-code-snippets.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> def is_even(x):\n...     return (x % 2) == 0\n>>> is_even(0)\nTrue\n>>> is_even(1)\nFalse\n```\n\n----------------------------------------\n\nTITLE: Launching Jaeger Dashboard for Distributed Tracing\nDESCRIPTION: This command starts the Jaeger dashboard, which is used for distributed tracing in the Istio service mesh. It allows for detailed analysis of request flows and performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/istio.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nistioctl dashboard jaeger\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Thread Callstack with Socket Reading\nDESCRIPTION: Thread stack trace showing the full call path from Java thread execution through Netty NIO event processing down to low-level socket reading operations in the kernel. Shows the complete data path from kernel socket buffer to Java application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_108\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/socket/nio/NioSocketChannel:.doReadBytes_[j];sun/nio/ch/SocketChannelImpl:.read_[j];sun/nio/ch/FileDispatcherImpl:.read0_[j];read;system_call_fastpath_[k];sys_read_[k];vfs_read_[k];do_sync_read_[k];sock_aio_read_[k];sock_aio_read.part.13_[k];do_sock_read.isra.12_[k];inet_recvmsg_[k];tcp_recvmsg_[k];skb_copy_datagram_iovec_[k];copy_user_enhanced_fast_string_[k]\n```\n\n----------------------------------------\n\nTITLE: Receiving Data on Multiple GPUs using Ray\nDESCRIPTION: This code snippet demonstrates how to receive data on multiple GPUs in a distributed computing setup using the Ray framework. The collective.recv_multigpu function is used to receive data, where 'self.recv2' is a placeholder for the data, and '0, 0' represent the source rank and tag. The GPUs are addressed by index in the cluster; however, the world size does not match the number of GPUs, as there are only 2 workers for 4 GPUs. The output, stored in 'self.recv2', is then returned.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/ray-collective.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncollective.recv_multigpu(self.recv2, 0, 0, \"8\")\nreturn self.recv2\n```\n\n----------------------------------------\n\nTITLE: Import necessary libraries\nDESCRIPTION: This code snippet imports the required libraries for the PBT example. It includes numpy for numerical operations, matplotlib for plotting, ray for distributed computing, and specific modules from ray.tune for hyperparameter tuning and scheduling. It also imports helper functions from a local file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport pickle\nimport tempfile\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import PopulationBasedTraining\nfrom ray.tune.tune_config import TuneConfig\nfrom ray.tune.tuner import Tuner\n\nfrom pbt_visualization_utils import (\n    get_init_theta,\n    plot_parameter_history,\n    plot_Q_history,\n    make_animation,\n)\n\"\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Whitespace Detection\nDESCRIPTION: Java stack trace showing Netty's HTTP whitespace detection during message parsing. This trace focuses on the HTTP decoder's process of finding whitespace characters in HTTP messages as part of request parsing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_101\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/handler/codec/http/HttpObjectDecoder:.decode_[j];io/netty/handler/codec/http/HttpObjectDecoder:.findWhitespace_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Ray Build on Ubuntu (Bash)\nDESCRIPTION: Commands to install necessary dependencies for building Ray from source on Ubuntu.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install -y build-essential curl clang-12 pkg-config psmisc unzip\n\n# Install Bazelisk.\nci/env/install-bazel.sh\n\n# Install node version manager and node 14\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash\nnvm install 14\nnvm use 14\n```\n\n----------------------------------------\n\nTITLE: Ray Autoscaler Status Output Format\nDESCRIPTION: This snippet displays the formatted output of the Ray autoscaler status command, showing node status categories and resource utilization. It indicates that there are no active or pending nodes, but 4 idle nodes, no recent failures, and shows CPU and other resources with no current resource demands.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_status_multinode.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n======== Autoscaler status: .+\nNode status\n---------------------------------------------------------------\nActive:\n \\(no active nodes\\)\nIdle:\n 1 node_.+\n 1 node_.+\n 1 node_.+\n 1 node_.+\nPending:\n \\(no pending nodes\\)\nRecent failures:\n \\(no failures\\)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/8.0 CPU\n 0.+\n 0.+\n\nDemands:\n \\(no resource demands\\)\n```\n\n----------------------------------------\n\nTITLE: Generating Compiled Requirements with uv pip compile for Ray LLM\nDESCRIPTION: Command to compile Python requirements for Ray, specifically for LLM components with CUDA 12.4 support. It uses uv pip compile to generate hashes, strip extras, and maintain unsafe packages with specific index sources.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cu124 --find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c python/requirements_compiled_rayllm_test_py311_cu124.txt python/requirements.txt python/requirements/llm/llm-requirements.txt -o python/requirements_compiled_rayllm_py311_cu124.txt\n```\n\n----------------------------------------\n\nTITLE: Netty Channel Write Operation Stack Trace\nDESCRIPTION: Thread stack trace showing HTTP response write operations in Netty. This trace demonstrates how response data is written back through the Netty channel pipeline in a Vert.x HTTP server.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_92\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];sun/reflect/DelegatingMethodAccessorImpl:.invoke_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j];io/netty/channel/AbstractChannelHandlerContext:.write_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Inspecting ClusterQueue Status\nDESCRIPTION: Commands to check the status of the ClusterQueue, including pending workloads and resource usage. These commands provide insights into how Kueue is managing the job queue and resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get clusterqueue\nkubectl get clusterqueue cluster-queue -o yaml\n```\n\n----------------------------------------\n\nTITLE: Installing MySQL Connector\nDESCRIPTION: This command demonstrates how to install the MySQL Connector/Python library using pip, which is required for connecting to MySQL databases from Python.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_23\n\nLANGUAGE: console\nCODE:\n```\npip install mysql-connector-python\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Cluster with Custom Resources in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a Ray cluster with custom resources using Python. It shows how to specify custom resources like GPUs and set the number of CPUs per ray worker.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/project_files/requirements_project/requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init(\n    num_cpus=8,\n    num_gpus=1,\n    resources={\"Custom1\": 1, \"Custom2\": 2},\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying pyasn1 Package Version and Hashes\nDESCRIPTION: This snippet defines the version and hash values for the pyasn1 package. It specifies version 0.5.1 and includes hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_40\n\nLANGUAGE: Text\nCODE:\n```\npyasn1==0.5.1 \\\n    --hash=sha256:4439847c58d40b1d0a573d07e3856e95333f1976294494c325775aeca506eb58 \\\n    --hash=sha256:6d391a96e59b23130a5cfa74d6fd7f388dbbe26cc8f1edf39fdddf08d9d6676c\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Learners in RLlib\nDESCRIPTION: Configuration for scaling the learning process by specifying multiple learners with GPU allocations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nconfig = (\n    AlgorithmConfig()\n    .learners(\n        num_learners=4,\n        num_gpus_per_learner=1,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Netty/Vert.x HTTP Request Processing Call Stack with IdScriptableObject.findInstanceIdInfo\nDESCRIPTION: Call stack trace showing the Java execution path from thread start through Netty's NIO event processing to Vert.x HTTP handlers and Mozilla JavaScript runtime. This specific trace ends with ScriptRuntime.setObjectProp and IdScriptableObject.findInstanceIdInfo.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_69\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/IdScriptableObject:.findInstanceIdInfo_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Including Custom Function Checkpointing Example in reStructuredText Documentation\nDESCRIPTION: A reStructuredText directive that includes the custom_func_checkpointing.py example file from the Ray Tune examples directory. This directive embeds the example code directly into the documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/custom_func_checkpointing.rst#2025-04-12_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. literalinclude:: /../../python/ray/tune/examples/custom_func_checkpointing.py\n```\n\n----------------------------------------\n\nTITLE: Java Stack Trace with Function Finding in Netty/Vert.x JavaScript Execution\nDESCRIPTION: This stack trace shows the process of finding JavaScript functions in Mozilla Rhino while processing HTTP requests in Vert.x. It reveals how JavaScript function resolution works when JS handlers are invoked in response to HTTP events from Netty.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_62\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.findFunction_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for PyTorch Lightning and Ray Tune\nDESCRIPTION: Basic imports needed for the PyTorch Lightning MNIST classifier example, including PyTorch, PyTorch Lightning, and various utility libraries for handling data and metrics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-lightning.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport tempfile\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nfrom filelock import FileLock\nfrom torchmetrics import Accuracy\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Processing with JavaScript Property Resolution\nDESCRIPTION: Java stack trace showing HTTP request handling with JavaScript property resolution. The trace follows Netty's event processing chain through Mozilla JavaScript runtime, focusing on name resolution within JavaScript objects.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_98\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.name_[j];org/mozilla/javascript/ScriptRuntime:.nameOrFunction_[j];org/mozilla/javascript/IdScriptableObject:.get_[j];org/mozilla/javascript/ScriptableObject$RelinkedSlot:.getValue_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Ray Task Summary Output\nDESCRIPTION: Example output showing task statistics including total actor tasks, scheduled actors, and task state counts grouped by function name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_29\n\nLANGUAGE: text\nCODE:\n```\n======== Tasks Summary: 2022-07-22 08:54:38.332537 ========\nStats:\n------------------------------------\ntotal_actor_scheduled: 2\ntotal_actor_tasks: 0\ntotal_tasks: 2\n\n\nTable (group by func_name):\n------------------------------------\nFUNC_OR_CLASS_NAME        STATE_COUNTS    TYPE\n0   task_running_300_seconds  RUNNING: 2      NORMAL_TASK\n1   Actor.__init__            FINISHED: 2     ACTOR_CREATION_TASK\n```\n\n----------------------------------------\n\nTITLE: Listing Kubernetes Nodes with GPU\nDESCRIPTION: Shows how to list Kubernetes nodes to verify the addition of a new GPU node to the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get nodes\nNAME                                                  STATUS   ROLES    AGE   VERSION\ngke-kuberay-gpu-cluster-default-pool-8d883840-fd6d    Ready    <none>   14m   v1.29.0-gke.1381000\ngke-kuberay-gpu-cluster-gpu-node-pool-b176212e-g3db   Ready    <none>   46s   v1.29.0-gke.1381000  # new node with GPUs\n```\n\n----------------------------------------\n\nTITLE: Sphinx Template for Python Class Documentation\nDESCRIPTION: This is a Sphinx documentation template that generates API documentation for Python classes. It displays the class name as a header, sets the current module context, and renders the class documentation with all members and inheritance information.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/autosummary/class_without_autosummary.rst#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n    :members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Redirecting Ray Logs to stderr in Python\nDESCRIPTION: Python code to set the RAY_LOG_TO_STDERR environment variable and initialize Ray. This redirects Ray logs to stderr instead of writing to log files.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"RAY_LOG_TO_STDERR\"] = \"1\"\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Disabling Ray Tune's Automatic Callback Loggers\nDESCRIPTION: Setting TUNE_DISABLE_AUTO_CALLBACK_LOGGERS to 1 prevents Ray Tune from automatically adding CSV and JSON logger callbacks if they haven't been explicitly provided.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_DISABLE_AUTO_CALLBACK_LOGGERS=1\n```\n\n----------------------------------------\n\nTITLE: Installing RayService Sample\nDESCRIPTION: Commands to download and apply a sample RayService configuration\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncurl -O https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-service.sample.yaml\nkubectl apply -f ray-service.sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Thread Callstack with TCP Window Selection\nDESCRIPTION: Thread stack trace showing Netty NIO operations with focus on TCP window selection in the kernel. This trace captures flow control mechanisms between Java networking code and kernel TCP implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_110\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/socket/nio/NioSocketChannel:.doReadBytes_[j];sun/nio/ch/SocketChannelImpl:.read_[j];sun/nio/ch/FileDispatcherImpl:.read0_[j];read;system_call_fastpath_[k];sys_read_[k];vfs_read_[k];do_sync_read_[k];sock_aio_read_[k];sock_aio_read.part.13_[k];do_sock_read.isra.12_[k];inet_recvmsg_[k];tcp_recvmsg_[k];tcp_cleanup_rbuf_[k];__tcp_select_window_[k]\n```\n\n----------------------------------------\n\nTITLE: Documenting Ray Status Command in reStructuredText\nDESCRIPTION: This snippet uses the click directive to generate documentation for the 'ray status' command from the ray.scripts.scripts module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/cli.rst#2025-04-12_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. click:: ray.scripts.scripts:status\n   :prog: ray status\n   :show-nested:\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Cluster with Kind for RayService Testing\nDESCRIPTION: Creates a local Kubernetes cluster using Kind with Kubernetes version 1.26.0 for testing KubeRay RayService.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n----------------------------------------\n\nTITLE: Disabling Usage Stats via Command Line\nDESCRIPTION: Command line options to disable Ray usage statistics collection when starting a Ray cluster\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/usage-stats.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nray start --head --disable-usage-stats\n```\n\n----------------------------------------\n\nTITLE: Java Call Stack for Netty/Vert.x HTTP Request Processing with JavaScript Handlers\nDESCRIPTION: Stack trace showing the call flow from thread initialization through Netty NIO event handling to Vert.x HTTP processing and JavaScript execution via Mozilla Rhino. The trace captures the complete path of HTTP request processing, including Netty's channel operations, Vert.x handling, and JavaScript execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_86\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/IdScriptableObject:.has_[j];org/mozilla/javascript/ScriptableObject:.getSlot_[j] 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.createFunctionActivation_[j] 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.createFunctionActivation_[j];org/mozilla/javascript/IdScriptableObject:.setAttributes_[j] 2\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.createFunctionActivation_[j];org/mozilla/javascript/TopLevel:.getBuiltinPrototype_[j] 2\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/IdScriptableObject:.put_[j];org/mozilla/javascript/ScriptableObject:.getSlot_[j];org/mozilla/javascript/ScriptableObject:.createSlot_[j] 1\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/ScriptRuntime:.indexFromString_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Resolving Unready Worker Pod by Adding Ray Serve Replicas\nDESCRIPTION: Commands to update the RayService configuration by increasing the number of Ray Serve replicas from 1 to 2, which resolves the unready worker Pod issue by ensuring all worker Pods have replicas.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.md#2025-04-12_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n# Step 7.1: Update the num_replicas of the app from 1 to 2.\n# [ray-service.no-ray-serve-replica.yaml]\n# deployments:\n#   - name: BaseService\n#     num_replicas: 2\n#     max_replicas_per_node: 1\n#     ray_actor_options:\n#       num_cpus: 0.1\n\n# Step 7.2: Apply the updated RayService config.\nkubectl apply -f ray-service.no-ray-serve-replica.yaml\n\n# Step 7.3: List all Ray Pods in the `default` namespace.\nkubectl get pods -l=ray.io/is-ray-node=yes\n```\n\n----------------------------------------\n\nTITLE: Checking RayCluster Status with YuniKorn Labels\nDESCRIPTION: Command to describe a RayCluster resource and view its YuniKorn-related labels, including gang scheduling configuration and queue assignment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl describe raycluster test-yunikorn-0\n\nName:         test-yunikorn-0\nNamespace:    default\nLabels:       ray.io/gang-scheduling-enabled=true\n              yunikorn.apache.org/app-id=test-yunikorn-0\n              yunikorn.apache.org/queue=root.test\nAnnotations:  <none>\nAPI Version:  ray.io/v1\nKind:         RayCluster\nMetadata:\n  Creation Timestamp:  2024-09-29T09:52:30Z\n  Generation:          1\n  Resource Version:    951\n  UID:                 cae1dbc9-5a67-4b43-b0d9-be595f21ab85\n# Other fields are skipped for brevity\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Call Trace with IP Packet Delivery Path\nDESCRIPTION: Call stack trace showing the Netty NIO event loop with the kernel IP packet delivery path. This trace demonstrates how network packets are processed through the IP protocol layer in the Linux kernel after being sent from the Java Netty application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_13\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio mTLS STRICT Mode\nDESCRIPTION: Commands to enable Istio mTLS in STRICT mode and configure KubeRay operator settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/istio.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Set ENABLE_INIT_CONTAINER_INJECTION=false on the KubeRay operator.\nhelm upgrade kuberay-operator kuberay/kuberay-operator --version 1.3.0 \\\n  --set env\\[0\\].name=ENABLE_INIT_CONTAINER_INJECTION \\\n  --set-string env\\[0\\].value=false\n\n# Apply mTLS STRICT mode on Istio.\nkubectl apply -f - <<EOF\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: \"default\"\n  namespace: \"default\"\nspec:\n  mtls:\n    mode: STRICT\nEOF\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Cluster with Kind\nDESCRIPTION: Creates a local Kubernetes cluster using Kind with specified node version\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n----------------------------------------\n\nTITLE: Querying Updated Ray Serve Application with cURL\nDESCRIPTION: This console command demonstrates how to query the updated Ray Serve application using cURL. It sends a POST request to the summarize_translate endpoint with a sample text, expecting a German translation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/kubernetes.md#2025-04-12_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ curl -X POST -H \"Content-Type: application/json\" localhost:8000/summarize_translate -d '\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief\"'\nEs war die beste Zeit, es war die schlimmste Zeit .\n```\n\n----------------------------------------\n\nTITLE: Calculating Test Error for Model Evaluation\nDESCRIPTION: Evaluates the model's performance on test data by calculating the mean absolute error between predicted and actual values. This provides a measure of the model's accuracy on unseen data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate restored model on test data.\nerror = sklearn.metrics.mean_absolute_error(test_y, pred_y)\nprint(f\"Test error: {error}\")\n```\n\n----------------------------------------\n\nTITLE: Example Output Format for Stable Diffusion Generation\nDESCRIPTION: Demonstrates the expected CLI output format when generating images with the Stable Diffusion model, showing prompt input and generation statistics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/README.md#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nEnter a prompt (or 'q' to quit):   twin peaks sf in basquiat painting style\n\nGenerating image(s)...\n\nGenerated 4 image(s) in 8.75 seconds to the directory: 58b298d9\n```\n\n----------------------------------------\n\nTITLE: Example of Handling EnvRunner Failures in RLlib - Python\nDESCRIPTION: The snippet provides configuration settings to tolerate occasional failures of environment runners during evaluation. This ensures that parallel sampling completes successfully despite individual runner failures.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-advanced-api.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Fault tolerance settings\n'ignore_env_runner_failures': True,\n'restart_failed_env_runners': True\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in Markdown\nDESCRIPTION: This code snippet defines a hidden table of contents in Markdown format, listing setup guides for different cloud platforms.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/k8s-cluster-setup.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n\naws-eks-gpu-cluster\ngcp-gke-gpu-cluster\ngcp-gke-tpu-cluster\nazure-aks-gpu-cluster\n```\n```\n\n----------------------------------------\n\nTITLE: Numcodecs Package Requirements\nDESCRIPTION: Version specification and hash verification for numcodecs package v0.12.1, used via zarr dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nnumcodecs==0.12.1 \\\n--hash=sha256:05d91a433733e7eef268d7e80ec226a0232da244289614a8f3826901aec1098e \\\n--hash=sha256:0e79bf9d1d37199ac00a60ff3adb64757523291d19d03116832e600cac391c51\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry-Proto Dependency with Hash Verification\nDESCRIPTION: Defines the opentelemetry-proto package dependency with version 1.1.0 and includes SHA256 hash validations. This is a dependency of opentelemetry-exporter-otlp-proto-grpc.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-proto==1.1.0 \\\n    --hash=sha256:237ef4fdd7f752b2fe740352643f8ef82733bd8e0db8b46ed808125ac7c7f112 \\\n    --hash=sha256:ff1ad9a3c572075883c2af0053cefdfaba005d71eade783c4524d34660d53b60\n```\n\n----------------------------------------\n\nTITLE: ExecutionOptions Module Import and Documentation Definition\nDESCRIPTION: RST documentation defining the current module context and autosummary directives for ExecutionOptions class documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/execution_options.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: ray.data\n\n.. autosummary::\n   :nosignatures:\n   :toctree: doc/\n   :template: autosummary/class_without_autosummary.rst\n\n   ExecutionOptions\n```\n\n----------------------------------------\n\nTITLE: Listing CUDA Version Identifiers\nDESCRIPTION: A list of CUDA version identifiers where each version is represented as 'cu' followed by the version number. These identifiers are commonly used for package compatibility or build configuration in machine learning frameworks.\nSOURCE: https://github.com/ray-project/ray/blob/master/docker/retag-lambda/cuda_versions.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncu128\ncu125\ncu124\ncu123\ncu121\ncu118\ncu117\n```\n\n----------------------------------------\n\nTITLE: Downloading Fine-tuned Vicuna-13B Checkpoints from S3 in Python\nDESCRIPTION: This snippet demonstrates how to download the fine-tuned Vicuna-13B model checkpoints from S3 to a local storage directory using the AWS CLI through a system call in Python.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.system(f\"aws s3 sync s3://{result.checkpoint.path} /mnt/local_storage\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Inception Score for GAN Evaluation\nDESCRIPTION: Defines an inception score implementation for evaluating GAN-generated images. It uses a pre-trained LeNet classification model on MNIST data which is stored in Ray's object store.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_guide.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n   :language: python\n   :start-after: __INCEPTION_SCORE_begin__\n   :end-before: __INCEPTION_SCORE_end__\n```\n\n----------------------------------------\n\nTITLE: Netty Channel Flush Operation\nDESCRIPTION: This snippet shows the process of flushing data in a Netty channel, including the invocation of various handlers in the pipeline and the actual write operation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_2\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j]\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Ray Cluster Config for CloudWatch (YAML)\nDESCRIPTION: This YAML configuration sets up a basic Ray cluster with CloudWatch integration. It specifies the AWS provider, region, CloudWatch agent and dashboard configs, and node types with pre-installed CloudWatch agent AMIs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nprovider:\n    type: aws\n    region: us-west-2\n    availability_zone: us-west-2a\n    # Start by defining a `cloudwatch` section to enable CloudWatch integration with your Ray cluster.\n    cloudwatch:\n        agent:\n            # Path to Unified CloudWatch Agent config file\n            config: \"cloudwatch/example-cloudwatch-agent-config.json\"\n        dashboard:\n            # CloudWatch Dashboard name\n            name: \"example-dashboard-name\"\n            # Path to the CloudWatch Dashboard config file\n            config: \"cloudwatch/example-cloudwatch-dashboard-config.json\"\n\nauth:\n    ssh_user: ubuntu\n\navailable_node_types:\n    ray.head.default:\n        node_config:\n        InstanceType: c5a.large\n        ImageId: ami-0d88d9cbe28fac870  # Unified CloudWatch agent pre-installed AMI, us-west-2\n        resources: {}\n    ray.worker.default:\n        node_config:\n            InstanceType: c5a.large\n            ImageId: ami-0d88d9cbe28fac870  # Unified CloudWatch agent pre-installed AMI, us-west-2\n            IamInstanceProfile:\n                Name: ray-autoscaler-cloudwatch-v1\n        resources: {}\n        min_workers: 0\n```\n\n----------------------------------------\n\nTITLE: Client Setup and Request Execution\nDESCRIPTION: Commands to download and execute the Stable Diffusion client script for making inference requests.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/stable-diffusion-rayservice.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncurl -LO https://raw.githubusercontent.com/ray-project/serve_config_examples/master/stable_diffusion/stable_diffusion_req.py\n\npython stable_diffusion_req.py\n```\n\n----------------------------------------\n\nTITLE: Fetching Node IP Addresses (Python)\nDESCRIPTION: Functions to retrieve external or internal IP addresses of cluster nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/vsphere/ARCHITECTURE.md#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nexternal_ip\ninternal_ip\n```\n\n----------------------------------------\n\nTITLE: Viewing Batch Inference Job Logs in Kubernetes\nDESCRIPTION: Sample output from the Ray batch inference job showing the execution progress with resource utilization statistics, model loading, and image classification results. The output confirms successful processing of images with the HuggingFace Vision Transformer model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-batch-inference-example.md#2025-04-12_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n[...]\nRunning: 62.0/64.0 CPU, 4.0/4.0 GPU, 955.57 MiB/12.83 GiB object_store_memory:   0%|          | 0/200 [00:05<?, ?it/s]\nRunning: 61.0/64.0 CPU, 4.0/4.0 GPU, 999.41 MiB/12.83 GiB object_store_memory:   0%|          | 0/200 [00:05<?, ?it/s]\nRunning: 61.0/64.0 CPU, 4.0/4.0 GPU, 999.41 MiB/12.83 GiB object_store_memory:   0%|          | 1/200 [00:05<17:04,  5.15s/it]\nRunning: 61.0/64.0 CPU, 4.0/4.0 GPU, 1008.68 MiB/12.83 GiB object_store_memory:   0%|          | 1/200 [00:05<17:04,  5.15s/it]\nRunning: 61.0/64.0 CPU, 4.0/4.0 GPU, 1008.68 MiB/12.83 GiB object_store_memory: 100%|| 1/1 [00:05<00:00,  5.15s/it]\n\n2023-08-22 15:48:33,905 WARNING actor_pool_map_operator.py:267 -- To ensure full parallelization across an actor pool of size 4, the specified batch size should be at most 5. Your configured batch size for this operator was 16.\n<PIL.Image.Image image mode=RGB size=500x375 at 0x7B37546CF7F0>\nLabel:  tench, Tinca tinca\n<PIL.Image.Image image mode=RGB size=500x375 at 0x7B37546AE430>\nLabel:  tench, Tinca tinca\n<PIL.Image.Image image mode=RGB size=500x375 at 0x7B37546CF430>\nLabel:  tench, Tinca tinca\n<PIL.Image.Image image mode=RGB size=500x375 at 0x7B37546AE430>\nLabel:  tench, Tinca tinca\n<PIL.Image.Image image mode=RGB size=500x375 at 0x7B37546CF7F0>\nLabel:  tench, Tinca tinca\n2023-08-22 15:48:36,522 SUCC cli.py:33 -- -----------------------------------\n2023-08-22 15:48:36,522 SUCC cli.py:34 -- Job 'rayjob-sample-ft8lh' succeeded\n2023-08-22 15:48:36,522 SUCC cli.py:35 -- -----------------------------------\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes ServiceAccount for GCS Access\nDESCRIPTION: This command creates a Kubernetes ServiceAccount that will grant the RayCluster access to mount the GCS bucket.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create serviceaccount pytorch-distributed-training\n```\n\n----------------------------------------\n\nTITLE: Setting Placement Group Reconciliation Interval\nDESCRIPTION: TUNE_PLACEMENT_GROUP_RECON_INTERVAL sets how often (in seconds) to reconcile placement groups, ensuring requested placement groups match pending/running trials.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_PLACEMENT_GROUP_RECON_INTERVAL=5\n```\n\n----------------------------------------\n\nTITLE: Specifying py-spy Dependency with Python Version Constraint\nDESCRIPTION: This snippet defines the py-spy package dependency with version 0.4.0, conditionally applied for Python versions below 3.12. py-spy is a sampling profiler for Python programs and is directly required by Ray for performance profiling.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_31\n\nLANGUAGE: pip\nCODE:\n```\npy-spy==0.4.0 ; python_full_version < '3.12' \\\n    --hash=sha256:47cdda4c34d9b6cb01f3aaeceb2e88faf57da880207fe72ff6ff97e9bb6cc8a9 \\\n    --hash=sha256:77d8f637ade38367d944874776f45b703b7ac5938b1f7be8891f3a5876ddbb96 \\\n    --hash=sha256:806602ce7972782cc9c1e383f339bfc27bfb822d42485e6a3e0530ae5040e1f0 \\\n    --hash=sha256:87573e64dbfdfc89ba2e0f5e2f525aa84e0299c7eb6454b47ea335fde583a7a0 \\\n    --hash=sha256:8bf2f3702cef367a489faa45177b41a6c31b2a3e5bd78c978d44e29340152f5a \\\n    --hash=sha256:c5f06ffce4c9c98b7fc9f5e67e5e7db591173f1351837633f3f23d9378b1d18a \\\n    --hash=sha256:eee3d0bde85ca5cf4f01f012d461180ca76c24835a96f7b5c4ded64eb6a008ab \\\n    --hash=sha256:f2cf3f7130e7d780471faa5957441d3b4e0ec39a79b2c00f4c33d494f7728428\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Sample Ray Python Script\nDESCRIPTION: A simple Ray Python script that initializes Ray, defines a remote function, and executes it.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kubectl-plugin.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init(address=\"auto\")\n\n@ray.remote\ndef f(x):\n    return x * x\n\nfutures = [f.remote(i) for i in range(4)]\nprint(ray.get(futures)) # [0, 1, 4, 9]\n```\n\n----------------------------------------\n\nTITLE: CloudWatch Alarm Actions Configuration (JSON)\nDESCRIPTION: This JSON snippet demonstrates the placeholder for specifying CloudWatch Alarm actions in the alarm configuration file. Users need to replace this with actual alarm actions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n\"AlarmActions\":[\n    \"TODO: Add alarm actions! See https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\"\n]\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Ray - Python\nDESCRIPTION: This snippet shuts down the Ray environment cleanly, ensuring no lingering processes remain after the completion of the experiments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Cluster Min Workers\nDESCRIPTION: Sets the minimum number of worker nodes to maintain in the cluster regardless of utilization, excluding the head node. Must be less than max_workers and greater than or equal to 0, defaults to 0.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/configuring-autoscaling.rst#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmin_workers: 0\n```\n\n----------------------------------------\n\nTITLE: Creating a GKE Cluster with Ray Operator\nDESCRIPTION: Creates a Standard GKE cluster with the Ray Operator addon enabled. Uses n1-standard-16 machine type and the specified cluster version and location.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-tpu-cluster.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container clusters create $CLUSTER_NAME \\\n    --addons=RayOperator \\\n    --machine-type=n1-standard-16 \\\n    --cluster-version=$CLUSTER_VERSION \\\n    --location=$ZONE\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Settings for Authentication with Ray Dashboard\nDESCRIPTION: Configuration settings for Grafana's ini file to enable embedding in Ray Dashboard when authentication is required. These settings allow embedding while maintaining security requirements for cookies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/configure-manage-dashboard.md#2025-04-12_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[security]\nallow_embedding = true\ncookie_secure = true\ncookie_samesite = none\n```\n\n----------------------------------------\n\nTITLE: Setting Up SSH Server in Ray Head Container\nDESCRIPTION: Bash commands to install and run SSH server in a Ray head node container for remote debugging access. It installs the OpenSSH server, creates the necessary directory, and starts the SSH daemon in debug mode.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/ray-distributed-debugger.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install openssh-server\nsudo mkdir -p /run/sshd\nsudo /usr/sbin/sshd -D\n```\n\n----------------------------------------\n\nTITLE: Setting Memory Limits with Ray Remote Function\nDESCRIPTION: Example showing how to cap memory usage for a Ray remote function to 500MiB using the memory parameter. If the function exceeds this limit, it will fail with an OOM error.\nSOURCE: https://github.com/ray-project/ray/blob/master/src/ray/common/cgroup/README.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote(memory=500 * 1024 * 1024)\ndef some_function(x):\n    pass\n\nobj = some_function.remote()\n```\n\n----------------------------------------\n\nTITLE: Submitting Ray Job\nDESCRIPTION: Submits a Ray job to print cluster resources using the Ray job submission SDK\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nray job submit --address http://localhost:8265 -- python -c \"import pprint; import ray; ray.init(); pprint.pprint(ray.cluster_resources(), sort_dicts=True)\"\n```\n\n----------------------------------------\n\nTITLE: Ray Progress Output - Actor Initialization and Broadcasting\nDESCRIPTION: Terminal progress bar output showing completion status of actor initialization (50/50 nodes at 22.15 iterations/sec) and object broadcasting (50/50 nodes at 10586.33 iterations/sec) across a Ray cluster. Includes metrics showing 50 total nodes and broadcast time of approximately 3.37 seconds for broadcasting 1GB of data to all nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.5.0/scalability/object_store.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nEnsure all actors have started.: 100%|| 50/50 [00:02<00:00, 22.15it/s]\nBroadcasting objects: 100%|| 50/50 [00:00<00:00, 10586.33it/s]\nnum_nodes: 50\nBroadcast time: 3368.40826964 (1073741824 B x 50 nodes)\n```\n\n----------------------------------------\n\nTITLE: Compiling Ray Project Requirements with uv pip\nDESCRIPTION: This command compiles the requirements for the Ray project using uv pip. It specifies various options including generating hashes, stripping extras, using unsafe packages, and specifying index URLs and find-links. The command compiles multiple requirement files into a single output file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cu121 --find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c python/requirements_compiled_ray_test_py311_cu121.txt python/requirements.txt python/requirements/cloud-requirements.txt python/requirements/base-test-requirements.txt python/requirements/llm/llm-requirements.txt python/requirements/llm/llm-test-requirements.txt -o python/requirements_compiled_rayllm_test_py311_cu121.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Ray Tune Logging with Default Settings\nDESCRIPTION: Shows how to set up default logging in Ray Tune to generate trial folders with autogenerated trainable and trial names. Results are saved to ~/ray_results by default.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-output.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# This logs to two different trial folders:\n# ~/ray_results/trainable_name/trial_name_1 and ~/ray_results/trainable_name/trial_name_2\n# trainable_name and trial_name are autogenerated.\ntuner = tune.Tuner(trainable, run_config=RunConfig(num_samples=2))\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Defining Pillow Dependency\nDESCRIPTION: Specifies the Pillow package (Python Imaging Library) with exact version 10.3.0 and SHA256 hash verification for secure installation. The beginning of this entry shows multiple hash options for different distribution formats.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    --hash=sha256:0ea2a783a2bdf2a561808fe4a7a12e9aa3799b701ba305de596bc48b8bdfce9d \\\n    --hash=sha256:1530e8f3a4b965eb6a7785cf17a426c779333eb62c9a7d1bbcf3ffd5bf77a4aa \\\n    --hash=sha256:16563993329b79513f59142a6b02055e10514c1a8e86dca8b48a893e33cf91e3 \\\n    --hash=sha256:19aeb96d43902f0a783946a0a87dbdad5c84c936025b8419da0a0cd7724356b1 \\\n    --hash=sha256:1a1d1915db1a4fdb2754b9de292642a39a7fb28f1736699527bb649484fb966a \\\n    --hash=sha256:1b87bd9d81d179bd8ab871603bd80d8645729939f90b71e62914e816a76fc6bd \\\n    --hash=sha256:1dfc94946bc60ea375cc39cff0b8da6c7e5f8fcdc1d946beb8da5c216156ddd8 \\\n    --hash=sha256:2034f6759a722da3a3dbd91a81148cf884e91d1b747992ca288ab88c1de15999 \\\n    --hash=sha256:261ddb7ca91fcf71757979534fb4c128448b5b4c55cb6152d280312062f69599 \\\n    --hash=sha256:2ed854e716a89b1afcedea551cd85f2eb2a807613752ab997b9974aaa0d56936 \\\n\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Ray Project\nDESCRIPTION: This snippet lists several Python packages with their specific versions and hash values. It includes dependencies like filelock, frozenlist, fsspec, gguf, and various Google API packages. The file uses pip's requirements format with pinned versions and integrity hashes.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_7\n\nLANGUAGE: Plain Text\nCODE:\n```\nfilelock==3.17.0 \\\n    --hash=sha256:533dc2f7ba78dc2f0f531fc6c4940addf7b70a481e269a5a3b93be94ffbe8338 \\\n    --hash=sha256:ee4e77401ef576ebb38cd7f13b9b28893194acc20a8e68e18730ba9c0e54660e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   huggingface-hub\n    #   ray\n    #   torch\n    #   transformers\n    #   virtualenv\n    #   vllm\nfrozenlist==1.4.1 \\\n    --hash=sha256:04ced3e6a46b4cfffe20f9ae482818e34eba9b5fb0ce4056e4cc9b6e212d09b7 \\\n    --hash=sha256:0633c8d5337cb5c77acbccc6357ac49a1770b8c487e5b3505c57b949b4b82e98 \\\n    # ... (truncated for brevity)\nfsspec==2023.5.0 \\\n    --hash=sha256:51a4ad01a5bb66fcc58036e288c0d53d3975a0df2a5dc59a93b59bade0391f2a \\\n    --hash=sha256:b3b56e00fb93ea321bc9e5d9cf6f8522a0198b20eb24e02774d329e9c6fb84ce\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   huggingface-hub\n    #   torch\ngguf==0.10.0 \\\n    --hash=sha256:52a30ef26328b419ffc47d9269fc580c238edf1c8a19b5ea143c323e04a038c1 \\\n    --hash=sha256:706089fba756a06913227841b4a6c8398360fa991569fd974e663a92b224e33f\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\ngoogle-api-core==1.34.0 \\\n    --hash=sha256:6fb380f49d19ee1d09a9722d0379042b7edb06c0112e4796c7a395078a043e71 \\\n    --hash=sha256:7421474c39d396a74dfa317dddbc69188f2336835f526087c7648f91105e32ff\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   opencensus\ngoogle-auth==2.23.4 \\\n    --hash=sha256:79905d6b1652187def79d491d6e23d0cbb3a21d3c7ba0dbaa9c8a01906b13ff3 \\\n    --hash=sha256:d4bbc92fe4b8bfd2f3e8d88e5ba7085935da208ee38a134fc280e7ce682a05f2\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   google-api-core\ngoogleapis-common-protos==1.61.0 \\\n    --hash=sha256:22f1915393bb3245343f6efe87f6fe868532efc12aa26b391b15132e1279f1c0 \\\n    --hash=sha256:8a64866a97f6304a7179873a465d6eee97b7a24ec6cfd78e0f575e96b821240b\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   google-api-core\n    #   opentelemetry-exporter-otlp-proto-grpc\ngrpcio==1.66.2 \\\n    --hash=sha256:02697eb4a5cbe5a9639f57323b4c37bcb3ab2d48cec5da3dc2f13334d72790dd \\\n    --hash=sha256:03b0b307ba26fae695e067b94cbb014e27390f8bc5ac7a3a39b7723fed085604 \\\n    # ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Ray Cgroup Directory Structure\nDESCRIPTION: ASCII representation of the cgroup v2 folder hierarchy created by Ray for resource management. Shows the tree structure starting from the node-level directory down to individual task directories.\nSOURCE: https://github.com/ray-project/ray/blob/master/src/ray/common/cgroup/README.md#2025-04-12_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n     /sys/fs/cgroup/ray_node_<node_id>\n        /                       \\\n.../internal           .../application\n                        /            \\\n                .../default  .../<task_id>_<attempt_id> (*N)\n```\n\n----------------------------------------\n\nTITLE: Ray Network Processing with Time Retrieval\nDESCRIPTION: This stack trace is similar to the previous one but includes additional kernel-level operations for retrieving the current time. It demonstrates how time-related operations are integrated into the network processing flow in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_21\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];tcp_v4_do_rcv_[k];tcp_rcv_established_[k];tcp_ack_[k];tcp_clean_rtx_queue_[k];ktime_get_real_[k];getnstimeofday_[k]\n```\n\n----------------------------------------\n\nTITLE: Downloading Data for a Given Task\nDESCRIPTION: This function `download_data` downloads the data for a specified task (currently only supports RTE). It retrieves the data from a URL, extracts it, and places it in the designated data directory. It checks if the data already exists to avoid redundant downloads.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_transformers.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef download_data(task_name, data_dir=\"./data\"):\n    # Download RTE training data\n    print(\"Downloading dataset.\")\n    import urllib\n    import zipfile\n\n    if task_name == \"rte\":\n        url = \"https://dl.fbaipublicfiles.com/glue/data/RTE.zip\"\n    else:\n        raise ValueError(\"Unknown task: {}\".format(task_name))\n    data_file = os.path.join(data_dir, \"{}.zip\".format(task_name))\n    if not os.path.exists(data_file):\n        urllib.request.urlretrieve(url, data_file)\n        with zipfile.ZipFile(data_file) as zip_ref:\n            zip_ref.extractall(data_dir)\n        print(\"Downloaded data for task {} to {}\".format(task_name, data_dir))\n    else:\n        print(\n            \"Data already exists. Using downloaded data for task {} from {}\".format(\n                task_name, data_dir\n            )\n        )\n```\n\n----------------------------------------\n\nTITLE: PyTorch Lightning Training Function with Checkpoint Support for PBT\nDESCRIPTION: Implements a training function that supports checkpoint loading and saving for Population Based Training with Ray Tune. The function sets up a PyTorch Lightning Trainer with appropriate callbacks and handles checkpoint restoration when provided by the PBT scheduler.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-vanilla-pytorch-lightning.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef train_mnist_tune_checkpoint(config, num_epochs=10, num_gpus=0, data_dir=\"~/data\"):\n    data_dir = os.path.expanduser(data_dir)\n    kwargs = {\n        \"max_epochs\": num_epochs,\n        # If fractional GPUs passed in, convert to int.\n        \"gpus\": math.ceil(num_gpus),\n        \"logger\": TensorBoardLogger(save_dir=os.getcwd(), name=\"\", version=\".\"),\n        \"enable_progress_bar\": False,\n        \"callbacks\": [\n            TuneReportCheckpointCallback(\n                metrics={\"loss\": \"ptl/val_loss\", \"mean_accuracy\": \"ptl/val_accuracy\"},\n                filename=\"checkpoint\",\n                on=\"validation_end\",\n            )\n        ],\n    }\n\n    checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            kwargs[\"resume_from_checkpoint\"] = os.path.join(checkpoint_dir, \"checkpoint\")\n\n    model = LightningMNISTClassifier(config=config, data_dir=data_dir)\n    trainer = pl.Trainer(**kwargs)\n\n    trainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Custom Callback for Curriculum Learning\nDESCRIPTION: This code defines a custom callback class, `MyCallbacks`, that overrides the `on_train_result` method to implement curriculum learning. Based on the mean episode return, the callback sets the environment's task (difficulty level) for each worker. This allows the agent to learn in increasingly challenging environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-advanced-api.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nfrom ray import tune\nfrom ray.rllib.callbacks.callbacks import RLlibCallback\n\nclass MyCallbacks(RLlibCallback):\n    def on_train_result(self, algorithm, result, **kwargs):\n        if result[\"env_runners\"][\"episode_return_mean\"] > 200:\n            task = 2\n        elif result[\"env_runners\"][\"episode_return_mean\"] > 100:\n            task = 1\n        else:\n            task = 0\n        algorithm.env_runner_group.foreach_worker(\n            lambda ev: ev.foreach_env(\n                lambda env: env.set_task(task)))\n\nray.init()\ntune.Tuner(\n    \"PPO\",\n    param_space={\n        \"env\": YourEnv,\n        \"callbacks\": MyCallbacks,\n    },\n).fit()\n```\n\n----------------------------------------\n\nTITLE: Defining Package Dependencies for Ray Project\nDESCRIPTION: Detailed specification of package dependencies including aiohappyeyeballs, aiohttp, aiohttp-cors, aiorwlock, aiosignal, airportsdata, annotated-types, anyio, and astor with their exact versions and hashes.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_3\n\nLANGUAGE: Pip\nCODE:\n```\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   aiohttp\naiohttp==3.11.16 \\\n    --hash=sha256:004511d3413737700835e949433536a2fe95a7d0297edd911a1e9705c5b5ea43 \\\n    --hash=sha256:0902e887b0e1d50424112f200eb9ae3dfed6c0d0a19fc60f633ae5a57c809656 \\\n    --hash=sha256:09b00dd520d88eac9d1768439a59ab3d145065c91a8fab97f900d1b5f802895e \\\n    --hash=sha256:0a2f451849e6b39e5c226803dcacfa9c7133e9825dcefd2f4e837a2ec5a3bb98 \\\n    --hash=sha256:0a950c2eb8ff17361abd8c85987fd6076d9f47d040ebffce67dce4993285e973 \\\n    --hash=sha256:0ad1fb47da60ae1ddfb316f0ff16d1f3b8e844d1a1e154641928ea0583d486ed \\\n    --hash=sha256:13ceac2c5cdcc3f64b9015710221ddf81c900c5febc505dbd8f810e770011540 \\\n    --hash=sha256:14461157d8426bcb40bd94deb0450a6fa16f05129f7da546090cebf8f3123b0f \\\n    --hash=sha256:16f8a2c9538c14a557b4d309ed4d0a7c60f0253e8ed7b6c9a2859a7582f8b1b8 \\\n    --hash=sha256:17ae4664031aadfbcb34fd40ffd90976671fa0c0286e6c4113989f78bebab37a \\\n    --hash=sha256:1ce63ae04719513dd2651202352a2beb9f67f55cb8490c40f056cea3c5c355ce \\\n    --hash=sha256:23a15727fbfccab973343b6d1b7181bfb0b4aa7ae280f36fd2f90f5476805682 \\\n    --hash=sha256:2540ddc83cc724b13d1838026f6a5ad178510953302a49e6d647f6e1de82bc34 \\\n    --hash=sha256:37dcee4906454ae377be5937ab2a66a9a88377b11dd7c072df7a7c142b63c37c \\\n    --hash=sha256:38bea84ee4fe24ebcc8edeb7b54bf20f06fd53ce4d2cc8b74344c5b9620597fd \\\n    --hash=sha256:3ab3367bb7f61ad18793fea2ef71f2d181c528c87948638366bf1de26e239183 \\\n    --hash=sha256:3ad1d59fd7114e6a08c4814983bb498f391c699f3c78712770077518cae63ff7 \\\n    --hash=sha256:3b4e6db8dc4879015b9955778cfb9881897339c8fab7b3676f8433f849425913 \\\n    --hash=sha256:3e061b09f6fa42997cf627307f220315e313ece74907d35776ec4373ed718b86 \\\n    --hash=sha256:42864e70a248f5f6a49fdaf417d9bc62d6e4d8ee9695b24c5916cb4bb666c802 \\\n    --hash=sha256:493910ceb2764f792db4dc6e8e4b375dae1b08f72e18e8f10f18b34ca17d0979 \\\n    --hash=sha256:4d0c970c0d602b1017e2067ff3b7dac41c98fef4f7472ec2ea26fd8a4e8c2149 \\\n    --hash=sha256:54eb3aead72a5c19fad07219acd882c1643a1027fbcdefac9b502c267242f955 \\\n    --hash=sha256:56a3443aca82abda0e07be2e1ecb76a050714faf2be84256dae291182ba59049 \\\n    --hash=sha256:576f5ca28d1b3276026f7df3ec841ae460e0fc3aac2a47cbf72eabcfc0f102e1 \\\n    --hash=sha256:58ede86453a6cf2d6ce40ef0ca15481677a66950e73b0a788917916f7e35a0bb \\\n    --hash=sha256:61c721764e41af907c9d16b6daa05a458f066015abd35923051be8705108ed17 \\\n    --hash=sha256:634d96869be6c4dc232fc503e03e40c42d32cfaa51712aee181e922e61d74814 \\\n    --hash=sha256:696ef00e8a1f0cec5e30640e64eca75d8e777933d1438f4facc9c0cdf288a810 \\\n    --hash=sha256:69a2cbd61788d26f8f1e626e188044834f37f6ae3f937bd9f08b65fc9d7e514e \\\n    --hash=sha256:6a792ce34b999fbe04a7a71a90c74f10c57ae4c51f65461a411faa70e154154e \\\n    --hash=sha256:6ac13b71761e49d5f9e4d05d33683bbafef753e876e8e5a7ef26e937dd766713 \\\n    --hash=sha256:6fdec0213244c39973674ca2a7f5435bf74369e7d4e104d6c7473c81c9bcc8c4 \\\n    --hash=sha256:72b1b03fb4655c1960403c131740755ec19c5898c82abd3961c364c2afd59fe7 \\\n    --hash=sha256:745f1ed5e2c687baefc3c5e7b4304e91bf3e2f32834d07baaee243e349624b24 \\\n    --hash=sha256:776c8e959a01e5e8321f1dec77964cb6101020a69d5a94cd3d34db6d555e01f7 \\\n    --hash=sha256:780df0d837276276226a1ff803f8d0fa5f8996c479aeef52eb040179f3156cbd \\\n    --hash=sha256:78e6e23b954644737e385befa0deb20233e2dfddf95dd11e9db752bdd2a294d3 \\\n    --hash=sha256:7951decace76a9271a1ef181b04aa77d3cc309a02a51d73826039003210bdc86 \\\n    --hash=sha256:7ba92a2d9ace559a0a14b03d87f47e021e4fa7681dc6970ebbc7b447c7d4b7cd \\\n    --hash=sha256:7f6428fee52d2bcf96a8aa7b62095b190ee341ab0e6b1bcf50c615d7966fd45b \\\n    --hash=sha256:87944bd16b7fe6160607f6a17808abd25f17f61ae1e26c47a491b970fb66d8cb \\\n    --hash=sha256:87a6e922b2b2401e0b0cf6b976b97f11ec7f136bfed445e16384fbf6fd5e8602 \\\n    --hash=sha256:8cb0688a8d81c63d716e867d59a9ccc389e97ac7037ebef904c2b89334407180 \\\n    --hash=sha256:8df6612df74409080575dca38a5237282865408016e65636a76a2eb9348c2567 \\\n    --hash=sha256:911a6e91d08bb2c72938bc17f0a2d97864c531536b7832abee6429d5296e5b27 \\\n    --hash=sha256:92b7ee222e2b903e0a4b329a9943d432b3767f2d5029dbe4ca59fb75223bbe2e \\\n    --hash=sha256:938f756c2b9374bbcc262a37eea521d8a0e6458162f2a9c26329cc87fdf06534 \\\n    --hash=sha256:9756d9b9d4547e091f99d554fbba0d2a920aab98caa82a8fb3d3d9bee3c9ae85 \\\n    --hash=sha256:98b88a2bf26965f2015a771381624dd4b0839034b70d406dc74fd8be4cc053e3 \\\n    --hash=sha256:9b751a6306f330801665ae69270a8a3993654a85569b3469662efaad6cf5cc50 \\\n    --hash=sha256:a2a450bcce4931b295fc0848f384834c3f9b00edfc2150baafb4488c27953de6 \\\n    --hash=sha256:a3814760a1a700f3cfd2f977249f1032301d0a12c92aba74605cfa6ce9f78489 \\\n    --hash=sha256:a5abcbba9f4b463a45c8ca8b7720891200658f6f46894f79517e6cd11f3405ca \\\n    --hash=sha256:a6db7458ab89c7d80bc1f4e930cc9df6edee2200127cfa6f6e080cf619eddfbd \\\n    --hash=sha256:ad497f38a0d6c329cb621774788583ee12321863cd4bd9feee1effd60f2ad133 \\\n    --hash=sha256:ad9509ffb2396483ceacb1eee9134724443ee45b92141105a4645857244aecc8 \\\n    --hash=sha256:bbcba75fe879ad6fd2e0d6a8d937f34a571f116a0e4db37df8079e738ea95c71 \\\n    --hash=sha256:c10d85e81d0b9ef87970ecbdbfaeec14a361a7fa947118817fcea8e45335fa46 \\\n    --hash=sha256:c15b2271c44da77ee9d822552201180779e5e942f3a71fb74e026bf6172ff287 \\\n    --hash=sha256:ca37057625693d097543bd88076ceebeb248291df9d6ca8481349efc0b05dcd0 \\\n    --hash=sha256:cc3a145479a76ad0ed646434d09216d33d08eef0d8c9a11f5ae5cdc37caa3540 \\\n    --hash=sha256:ccf10f16ab498d20e28bc2b5c1306e9c1512f2840f7b6a67000a517a4b37d5ee \\\n    --hash=sha256:cd464ba806e27ee24a91362ba3621bfc39dbbb8b79f2e1340201615197370f7c \\\n    --hash=sha256:d007aa39a52d62373bd23428ba4a2546eed0e7643d7bf2e41ddcefd54519842c \\\n    --hash=sha256:d0666afbe984f6933fe72cd1f1c3560d8c55880a0bdd728ad774006eb4241ecd \\\n    --hash=sha256:d07502cc14ecd64f52b2a74ebbc106893d9a9717120057ea9ea1fd6568a747e7 \\\n    --hash=sha256:d489d9778522fbd0f8d6a5c6e48e3514f11be81cb0a5954bdda06f7e1594b321 \\\n    --hash=sha256:df7db76400bf46ec6a0a73192b14c8295bdb9812053f4fe53f4e789f3ea66bbb \\\n    --hash=sha256:e3538bc9fe1b902bef51372462e3d7c96fce2b566642512138a480b7adc9d508 \\\n    --hash=sha256:e87fd812899aa78252866ae03a048e77bd11b80fb4878ce27c23cade239b42b2 \\\n    --hash=sha256:ecdb8173e6c7aa09eee342ac62e193e6904923bd232e76b4157ac0bfa670609f \\\n    --hash=sha256:f244b8e541f414664889e2c87cac11a07b918cb4b540c36f7ada7bfa76571ea2 \\\n    --hash=sha256:f4065145bf69de124accdd17ea5f4dc770da0a6a6e440c53f6e0a8c27b3e635c \\\n    --hash=sha256:f420bfe862fb357a6d76f2065447ef6f484bc489292ac91e29bc65d2d7a2c84d \\\n    --hash=sha256:f6ddd90d9fb4b501c97a4458f1c1720e42432c26cb76d28177c5b5ad4e332601 \\\n    --hash=sha256:fa73e8c2656a3653ae6c307b3f4e878a21f87859a9afab228280ddccd7369d71 \\\n    --hash=sha256:fadbb8f1d4140825069db3fedbbb843290fd5f5bc0a5dbd7eaf81d91bf1b003b \\\n    --hash=sha256:fb3d0cc5cdb926090748ea60172fa8a213cec728bd6c54eae18b96040fcd6227 \\\n    --hash=sha256:fb46bb0f24813e6cede6cc07b1961d4b04f331f7112a23b5e21f567da4ee50aa \\\n    --hash=sha256:fd36c119c5d6551bce374fcb5c19269638f8d09862445f85a5a48596fd59f4bb\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   aiohttp-cors\n    #   vllm\naiohttp-cors==0.7.0 \\\n    --hash=sha256:0451ba59fdf6909d0e2cd21e4c0a43752bc0703d33fc78ae94d9d9321710193e \\\n    --hash=sha256:4d39c6d7100fd9764ed1caf8cebf0eb01bf5e3f24e2e073fda6234bc48b19f5d\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   -r python/requirements.txt\naiorwlock==1.3.0 \\\n    --hash=sha256:45baf8e4fa9a23e0bb325fbd67da80de1fd7ae1d4f59a6381754c60cec7b289b \\\n    --hash=sha256:83f12d87df4b9728a0b8fda1756585ab0d652b107bab59c6084e1b1ad692ab45\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   -r python/requirements.txt\naiosignal==1.3.1 \\\n    --hash=sha256:54cd96e15e1649b75d6c87526a6ff0b6c1b0dd3459f43d9ca11d48c339b68cfc \\\n    --hash=sha256:f8376fb07dd1e86a584e4fcdec80b36b7f81aac666ebc724e2c090300dd83b17\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   aiohttp\n    #   ray\nairportsdata==20241001 \\\n    --hash=sha256:67d71cf2c5378cc17ff66b62b1e11aa2444043949c894543ac8fd8dafce192fd \\\n    --hash=sha256:fa0bd143b4f4be3557cb892fa0612ef210fd91a92bd720b4d8221de576a4fa00\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   outlines\nannotated-types==0.6.0 \\\n    --hash=sha256:0641064de18ba7a25dee8f96403ebc39113d0cb953a01429249d5c7564666a43 \\\n    --hash=sha256:563339e807e53ffd9c267e99fc6d9ea23eb8443c08f112651963e24e22f84a5d\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   pydantic\nanyio==3.7.1 \\\n    --hash=sha256:44a3c9aba0f5defa43261a8b3efb97891f2bd7d804e0e1f56419befa1adfc780 \\\n    --hash=sha256:91dee416e570e92c64041bd18b900d1d6fa78dff7048769ce5ac5ddad004fbb5\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   httpx\n    #   openai\n    #   starlette\n    #   watchfiles\nastor==0.8.1 \\\n```\n\n----------------------------------------\n\nTITLE: Generated Config Comment\nDESCRIPTION: Comment header indicating this file was automatically generated using the uv tool\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# This file was autogenerated by uv via the following command:\n```\n\n----------------------------------------\n\nTITLE: Adding Sonatype Repository for Ray Java Snapshots\nDESCRIPTION: Adds the Sonatype repository to the pom.xml file to access Ray Java snapshot versions. This is necessary for using the latest, unreleased versions of Ray Java.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_20\n\nLANGUAGE: xml\nCODE:\n```\n\"<!-- only needed for snapshot version of ray -->\\n<repositories>\\n  <repository>\\n    <id>sonatype</id>\\n    <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\\n    <releases>\\n      <enabled>false</enabled>\\n    </releases>\\n    <snapshots>\\n      <enabled>true</enabled>\\n    </snapshots>\\n  </repository>\\n</repositories>\\n\\n<dependencies>\"\n```\n\n----------------------------------------\n\nTITLE: Checking Pod Status in RayCluster with YuniKorn\nDESCRIPTION: Command to list Pods that are scheduled by YuniKorn. This shows the status of Pods from the RayCluster that was created with YuniKorn scheduling enabled.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/yunikorn.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl get pods\n\nNAME                                  READY   STATUS    RESTARTS   AGE\ntest-yunikorn-0-head-98fmp            1/1     Running   0          67s\ntest-yunikorn-0-worker-worker-42tgg   1/1     Running   0          67s\ntest-yunikorn-0-worker-worker-467mn   1/1     Running   0          67s\n```\n\n----------------------------------------\n\nTITLE: Verifying GPU Node Creation\nDESCRIPTION: This command checks the successful creation of the cluster with 4 GPUs by listing the nodes and their GPU allocations.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n```\n\n----------------------------------------\n\nTITLE: Checking RayService Deployment Status\nDESCRIPTION: Command to check the status of the deployed RayService and its output showing a healthy deployment. This verifies that the model has been successfully loaded and is ready to serve inference requests.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/vllm-rayservice.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n$ kubectl get rayservice llama-3-8b -o yaml\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with Hashes\nDESCRIPTION: Package dependency specifications with pinned versions and SHA256 hashes for security verification. Each package listing includes version number, hash values, and dependency relationships via comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_44\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:75266c25d394bb5d70f83a38b1b4d858c074a767c18f7ff87443bdf193c1b236 \\\n--hash=sha256:79cd153330c071cb9582351c1f3c3c55a1adbf85556bfc5d521b744c7280728f \\\n--hash=sha256:a38f6c413a83bc1089d4eecd0acd88e8190df6e0c4423ee45ba59cc0a8001324 \\\n--hash=sha256:dfb1ae1d7da1e869a6a6a315cc2b2652c43e3aabb5184da4d363d1b4bb2c86a4\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Node Tolerations in Kubernetes YAML\nDESCRIPTION: YAML configuration snippet showing the tolerations setup for worker pods to be scheduled on GPU nodes in a Kubernetes cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/text-summarizer-rayservice.md#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Please add the following taints to the GPU node.\ntolerations:\n    - key: \"ray.io/node-type\"\n      operator: \"Equal\"\n      value: \"worker\"\n      effect: \"NoSchedule\"\n```\n\n----------------------------------------\n\nTITLE: Sphinx AutoClass Documentation Template with Jinja2\nDESCRIPTION: Jinja2 template that generates Sphinx documentation for Python classes and their methods. The template supports API grouping and creates autosummary sections for methods. It uses custom filters like has_public_constructor, get_api_groups, and select_api_group to organize the documentation structure.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/autosummary/class_v2.rst#2025-04-12_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n.. currentmodule:: {{ module }}\n\n{% if name | has_public_constructor(module) %}\n{{ name }}\n{{ '-' * name | length }}\n\n.. autoclass:: {{ objname }}\n{% endif %}\n\n{% block methods %}\n{% if methods %}\n{% set api_groups = methods | get_api_groups(name, module) %}\n{% for api_group in api_groups %}\n\n{% if api_groups | length > 1 %}\n{{ api_group }}\n{{ '-' * api_group | length }}\n{% endif %}\n\n.. autosummary::\n   :nosignatures:\n   :toctree: doc\n\n   {% for method in methods | select_api_group(name, module, api_group) %}\n      {{ name }}.{{ method }}\n   {% endfor %}\n\n{% endfor %}\n{% endif %}\n{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Terminating Ray Cluster Nodes (Python)\nDESCRIPTION: Function called during 'ray down' command execution to delete all nodes in the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/vsphere/ARCHITECTURE.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nterminate_nodes\n```\n\n----------------------------------------\n\nTITLE: Using Viskit with Tune for Visualization\nDESCRIPTION: This snippet shows how to use Viskit to visualize Tune experimental results stored in CSV format. It involves cloning the rllab repository and running Viskit's frontend script to visualize the results from a specified experiment directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/logging.rst#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/rll/rllab.git\n$ python rllab/rllab/viskit/frontend.py ~/ray_results/my_experiment\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Kubernetes Resources\nDESCRIPTION: Commands to clean up all resources created during the quickstart, including the RayService, KubeRay operator, and curl Pod. This ensures proper removal of all components from the Kubernetes cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md#2025-04-12_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# Delete the RayService.\nkubectl delete -f https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-service.sample.yaml\n\n# Uninstall the KubeRay operator.\nhelm uninstall kuberay-operator\n\n# Delete the curl Pod.\nkubectl delete pod curl\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Collective Dependencies in Python\nDESCRIPTION: Installation commands for the required dependencies to use Ray collective communication with GLOO and NCCL backends.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/ray-collective.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install pygloo\npip install cupy-cudaxxx # replace xxx with the right cuda version in your environment\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Ray Dashboard for Kubernetes Service\nDESCRIPTION: This command sets up port forwarding to access the Ray Dashboard from the Ray head service on localhost:8265.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/tpu-serve-stable-diffusion.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nkubectl port-forward svc/stable-diffusion-tpu-head-svc 8265:8265 &\n```\n\n----------------------------------------\n\nTITLE: Configuring Storage Path for Training Artifacts\nDESCRIPTION: Sets up storage path configuration for saving training checkpoints and artifacts, either using S3 bucket or NFS path for multi-node training scenarios.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstorage_path = \"s3://your-bucket-here\"  # TODO: Set up cloud storage\n# storage_path=\"/mnt/path/to/nfs\"     # TODO: Alternatively, set up NFS\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os, re\n\nartifact_storage = os.environ.get(\"ANYSCALE_ARTIFACT_STORAGE\", \"artifact_storage\")\nuser_name = re.sub(r\"\\s+\", \"__\", os.environ.get(\"ANYSCALE_USERNAME\", \"user\"))\nstorage_path = f\"{artifact_storage}/{user_name}/gptj-deepspeed-finetune\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Parameters\nDESCRIPTION: This code sets up variables to control GPU usage and the number of workers for training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuse_gpu = True  # set this to False to run on CPUs\nnum_workers = 1  # set this to number of GPUs or CPUs you want to use\n```\n\n----------------------------------------\n\nTITLE: Connecting to Ray Cluster in Python\nDESCRIPTION: Code snippet showing how to connect to a running Ray cluster using Python. It demonstrates the basic ray.init() method to establish a connection to the Ray runtime.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_start_windows_osx.txt#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Run Tune with MLflow Integration\nDESCRIPTION: This snippet sets up a test run using either a temporary directory for MLflow tracking or a specified MLFLOW_TRACKING_URI, and then runs the `tune_with_callback` and `tune_with_setup` functions to demonstrate both MLflow integration methods with Ray Tune. It includes logic to print the resulting MLflow runs if `smoke_test` is False.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-mlflow.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsmoke_test = True\n\nif smoke_test:\n    mlflow_tracking_uri = os.path.join(tempfile.gettempdir(), \"mlruns\")\nelse:\n    mlflow_tracking_uri = \"<MLFLOW_TRACKING_URI>\"\n\ntune_with_callback(mlflow_tracking_uri, finish_fast=smoke_test)\nif not smoke_test:\n    df = mlflow.search_runs(\n        [mlflow.get_experiment_by_name(\"mlflow_callback_example\").experiment_id]\n    )\n    print(df)\n\ntune_with_setup(mlflow_tracking_uri, finish_fast=smoke_test)\nif not smoke_test:\n    df = mlflow.search_runs(\n        [mlflow.get_experiment_by_name(\"setup_mlflow_example\").experiment_id]\n    )\n    print(df)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Progress Bar for Ray Data Iterable Dataset\nDESCRIPTION: Implements a customized progress bar class (DollyV2ProgressBar) that extends TQDMProgressBar to properly track training progress with Ray Data Iterable Datasets. The code also initializes this progress bar with the calculated number of iterations per epoch.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/dolly_lightning_fsdp_finetuning.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create a customized progress bar for Ray Data Iterable Dataset\nclass DollyV2ProgressBar(TQDMProgressBar):\n    def __init__(self, num_iters_per_epoch, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_iters_per_epoch = num_iters_per_epoch\n    \n    def on_train_epoch_start(self, trainer, *_):\n        super().on_train_epoch_start(trainer, *_)\n        self.train_progress_bar.reset(self.num_iters_per_epoch)\n\nnum_iters_per_epoch = train_ds.count() // (num_workers * batch_size_per_worker)\nprog_bar = DollyV2ProgressBar(num_iters_per_epoch)\n```\n\n----------------------------------------\n\nTITLE: PyTorch CPU Package Source Configuration\nDESCRIPTION: Package source configuration for PyTorch CPU wheels and related dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_compiled.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cpu\n--find-links https://data.pyg.org/whl/torch-2.3.0+cpu.html\n```\n\n----------------------------------------\n\nTITLE: Defining scipiy dependency with version pinning and hash verification\nDESCRIPTION: Specifies scipy version 1.11.4 with SHA-256 hashes for verification. Comments indicate it's required by python/requirements.txt, scikit-image, and vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_31\n\nLANGUAGE: plaintext\nCODE:\n```\nscipy==1.11.4 \\\n    --hash=sha256:00150c5eae7b610c32589dda259eacc7c4f1665aedf25d921907f4d08a951b1c \\\n    --hash=sha256:028eccd22e654b3ea01ee63705681ee79933652b2d8f873e7949898dda6d11b6 \\\n    --hash=sha256:1b7c3dca977f30a739e0409fb001056484661cb2541a01aba0bb0029f7b68db8 \\\n    --hash=sha256:2c6ff6ef9cc27f9b3db93a6f8b38f97387e6e0591600369a297a50a8e96e835d \\\n    --hash=sha256:36750b7733d960d7994888f0d148d31ea3017ac15eef664194b4ef68d36a4a97 \\\n    --hash=sha256:530f9ad26440e85766509dbf78edcfe13ffd0ab7fec2560ee5c36ff74d6269ff \\\n    --hash=sha256:5e347b14fe01003d3b78e196e84bd3f48ffe4c8a7b8a1afbcb8f5505cb710993 \\\n    --hash=sha256:6550466fbeec7453d7465e74d4f4b19f905642c89a7525571ee91dd7adabb5a3 \\\n    --hash=sha256:6df1468153a31cf55ed5ed39647279beb9cfb5d3f84369453b49e4b8502394fd \\\n    --hash=sha256:6e619aba2df228a9b34718efb023966da781e89dd3d21637b27f2e54db0410d7 \\\n    --hash=sha256:8fce70f39076a5aa62e92e69a7f62349f9574d8405c0a5de6ed3ef72de07f446 \\\n    --hash=sha256:90a2b78e7f5733b9de748f589f09225013685f9b218275257f8a8168ededaeaa \\\n    --hash=sha256:91af76a68eeae0064887a48e25c4e616fa519fa0d38602eda7e0f97d65d57937 \\\n    --hash=sha256:933baf588daa8dc9a92c20a0be32f56d43faf3d1a60ab11b3f08c356430f6e56 \\\n    --hash=sha256:acf8ed278cc03f5aff035e69cb511741e0418681d25fbbb86ca65429c4f4d9cd \\\n    --hash=sha256:ad669df80528aeca5f557712102538f4f37e503f0c5b9541655016dd0932ca79 \\\n    --hash=sha256:b030c6674b9230d37c5c60ab456e2cf12f6784596d15ce8da9365e70896effc4 \\\n    --hash=sha256:b9999c008ccf00e8fbcce1236f85ade5c569d13144f77a1946bef8863e8f6eb4 \\\n    --hash=sha256:bc9a714581f561af0848e6b69947fda0614915f072dfd14142ed1bfe1b806710 \\\n    --hash=sha256:ce7fff2e23ab2cc81ff452a9444c215c28e6305f396b2ba88343a567feec9660 \\\n    --hash=sha256:cf00bd2b1b0211888d4dc75656c0412213a8b25e80d73898083f402b50f47e41 \\\n    --hash=sha256:d10e45a6c50211fe256da61a11c34927c68f277e03138777bdebedd933712fea \\\n    --hash=sha256:ee410e6de8f88fd5cf6eadd73c135020bfbbbdfcd0f6162c36a7638a1ea8cc65 \\\n    --hash=sha256:f313b39a7e94f296025e3cffc2c567618174c0b1dde173960cf23808f9fae4be \\\n    --hash=sha256:f3cd9e7b3c2c1ec26364856f9fbe78695fe631150f94cd1c22228456404cf1ec\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\n    #   scikit-image\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Compiling Ray Dependencies with uv pip\nDESCRIPTION: This command uses uv pip to compile dependencies from multiple requirement files, specifying package sources, compilation constraints, and output file. It includes special handling for Ray and some unsafe packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cu124 --find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c /tmp/ray-deps/requirements_compiled.txt python/requirements.txt python/requirements/cloud-requirements.txt python/requirements/base-test-requirements.txt -o python/requirements_compiled_ray_test_py311_cu124.txt\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Notebook Execution\nDESCRIPTION: Command to build documentation while executing embedded notebooks, with caching to improve build times on subsequent runs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/README.md#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nRUN_NOTEBOOKS=\"cache\" make develop\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Kubernetes Resources\nDESCRIPTION: Commands to clean up all the resources created during the tutorial, including deleting the RayService, uninstalling the KubeRay operator, and removing the curl Pod.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.md#2025-04-12_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n# Delete the RayService.\nkubectl delete -f ray-service.no-ray-serve-replica.yaml\n\n# Uninstall the KubeRay operator.\nhelm uninstall kuberay-operator\n\n# Delete the curl Pod.\nkubectl delete pod curl\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Thread Callstack with Access Verification\nDESCRIPTION: Thread stack trace showing Netty NIO operations with kernel memory access verification. This trace captures the security boundary checks performed by the kernel during I/O operations initiated from Java.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_111\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/socket/nio/NioSocketChannel:.doReadBytes_[j];sun/nio/ch/SocketChannelImpl:.read_[j];sun/nio/ch/FileDispatcherImpl:.read0_[j];read;system_call_fastpath_[k];sys_read_[k];vfs_read_[k];rw_verify_area_[k]\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Ray Tune and HyperOpt in Python\nDESCRIPTION: Imports necessary packages including Ray, Ray Tune, HyperOpt, and required classes for running experiments. These dependencies must be installed beforehand.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport ray\nfrom ray import tune\nfrom ray.tune.search import ConcurrencyLimiter\nfrom ray.tune.search.hyperopt import HyperOptSearch\nfrom hyperopt import hp\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Package dependency declarations with version constraints and SHA256 hash verification. Each entry specifies a package name, version, and one or more hash values for verification during installation. Additional metadata indicates source requirements files.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:e09031c87a1e51556fdcb46e5bd4f59dfb743061cf93c4d6831bf894f125eb57 \\\n--hash=sha256:e4dd52d80b8c83fdce44e12478ad2e85c64ea965e75d66dbeafb0a3e77308fcc \\\n--hash=sha256:f698de3fd0c4e6972b92290a45bd9b1536bffe8c6759c62471efaa8acb4c37bc \\\n--hash=sha256:fec21693218efe39aa7f8599346e90c705afa52c5b31ae019b2e57e8f6542bb2 \\\n--hash=sha256:ffcc3f7c66b5f5b7931a5aa68fc9cecc51e685ef90282f4a82f0f5e9b704ad11\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Ray Data LLM and vLLM\nDESCRIPTION: This bash command installs the necessary Python packages for running the Ray Data LLM example with vLLM and xgrammar.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/llm/examples/batch/vllm-with-structural-output.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -qU \"ray[data]\" \"vllm==0.7.2\" \"xgrammar==0.1.11\"\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Cluster with Kind\nDESCRIPTION: Creates a local Kubernetes cluster using Kind with Kubernetes version 1.26.0.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/kuberay-operator-installation.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n----------------------------------------\n\nTITLE: Installing KubeRay Operator with Volcano Support via Helm\nDESCRIPTION: Helm command to install the KubeRay Operator with batch scheduler support enabled for Volcano integration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Install the Helm chart with --enable-batch-scheduler flag set to true\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.3.0 --set batchScheduler.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Deleting Ray Cluster Network Policy\nDESCRIPTION: Removes the network policy associated with the static Ray cluster from Kubernetes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl delete -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster-networkpolicy.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Specific Package Dependencies for Ray\nDESCRIPTION: This section lists specific Python packages with their exact versions and hash values required for the Ray project. It includes information on the source of each dependency and any additional constraints.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\naiofiles==22.1.0 \\\n    --hash=sha256:1142fa8e80dbae46bb6339573ad4c8c0841358f79c6eb50a493dceca14621bad \\\n    --hash=sha256:9107f1ca0b2a5553987a94a3c9959fe5b491fdf731389aa5b7b1bd0733e32de6\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ypy-websocket\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   aiohttp\naiohttp==3.11.16 \\\n    --hash=sha256:004511d3413737700835e949433536a2fe95a7d0297edd911a1e9705c5b5ea43 \\\n    --hash=sha256:0902e887b0e1d50424112f200eb9ae3dfed6c0d0a19fc60f633ae5a57c809656 \\\n    --hash=sha256:09b00dd520d88eac9d1768439a59ab3d145065c91a8fab97f900d1b5f802895e \\\n    --hash=sha256:0a2f451849e6b39e5c226803dcacfa9c7133e9825dcefd2f4e837a2ec5a3bb98 \\\n    --hash=sha256:0a950c2eb8ff17361abd8c85987fd6076d9f47d040ebffce67dce4993285e973 \\\n    --hash=sha256:0ad1fb47da60ae1ddfb316f0ff16d1f3b8e844d1a1e154641928ea0583d486ed \\\n    --hash=sha256:13ceac2c5cdcc3f64b9015710221ddf81c900c5febc505dbd8f810e770011540 \\\n    --hash=sha256:14461157d8426bcb40bd94deb0450a6fa16f05129f7da546090cebf8f3123b0f \\\n    --hash=sha256:16f8a2c9538c14a557b4d309ed4d0a7c60f0253e8ed7b6c9a2859a7582f8b1b8 \\\n    --hash=sha256:17ae4664031aadfbcb34fd40ffd90976671fa0c0286e6c4113989f78bebab37a \\\n    --hash=sha256:1ce63ae04719513dd2651202352a2beb9f67f55cb8490c40f056cea3c5c355ce \\\n    --hash=sha256:23a15727fbfccab973343b6d1b7181bfb0b4aa7ae280f36fd2f90f5476805682 \\\n    --hash=sha256:2540ddc83cc724b13d1838026f6a5ad178510953302a49e6d647f6e1de82bc34 \\\n    --hash=sha256:37dcee4906454ae377be5937ab2a66a9a88377b11dd7c072df7a7c142b63c37c \\\n    --hash=sha256:38bea84ee4fe24ebcc8edeb7b54bf20f06fd53ce4d2cc8b74344c5b9620597fd \\\n    --hash=sha256:3ab3367bb7f61ad18793fea2ef71f2d181c528c87948638366bf1de26e239183 \\\n    --hash=sha256:3ad1d59fd7114e6a08c4814983bb498f391c699f3c78712770077518cae63ff7 \\\n    --hash=sha256:3b4e6db8dc4879015b9955778cfb9881897339c8fab7b3676f8433f849425913 \\\n    --hash=sha256:3e061b09f6fa42997cf627307f220315e313ece74907d35776ec4373ed718b86 \\\n    --hash=sha256:42864e70a248f5f6a49fdaf417d9bc62d6e4d8ee9695b24c5916cb4bb666c802 \\\n    --hash=sha256:493910ceb2764f792db4dc6e8e4b375dae1b08f72e18e8f10f18b34ca17d0979 \\\n    --hash=sha256:4d0c970c0d602b1017e2067ff3b7dac41c98fef4f7472ec2ea26fd8a4e8c2149 \\\n    --hash=sha256:54eb3aead72a5c19fad07219acd882c1643a1027fbcdefac9b502c267242f955 \\\n    --hash=sha256:56a3443aca82abda0e07be2e1ecb76a050714faf2be84256dae291182ba59049 \\\n    --hash=sha256:576f5ca28d1b3276026f7df3ec841ae460e0fc3aac2a47cbf72eabcfc0f102e1 \\\n    --hash=sha256:58ede86453a6cf2d6ce40ef0ca15481677a66950e73b0a788917916f7e35a0bb \\\n    --hash=sha256:61c721764e41af907c9d16b6daa05a458f066015abd35923051be8705108ed17 \\\n    --hash=sha256:634d96869be6c4dc232fc503e03e40c42d32cfaa51712aee181e922e61d74814 \\\n    --hash=sha256:696ef00e8a1f0cec5e30640e64eca75d8e777933d1438f4facc9c0cdf288a810 \\\n    --hash=sha256:69a2cbd61788d26f8f1e626e188044834f37f6ae3f937bd9f08b65fc9d7e514e \\\n    --hash=sha256:6a792ce34b999fbe04a7a71a90c74f10c57ae4c51f65461a411faa70e154154e \\\n    --hash=sha256:6ac13b71761e49d5f9e4d05d33683bbafef753e876e8e5a7ef26e937dd766713 \\\n    --hash=sha256:6fdec0213244c39973674ca2a7f5435bf74369e7d4e104d6c7473c81c9bcc8c4 \\\n    --hash=sha256:72b1b03fb4655c1960403c131740755ec19c5898c82abd3961c364c2afd59fe7 \\\n    --hash=sha256:745f1ed5e2c687baefc3c5e7b4304e91bf3e2f32834d07baaee243e349624b24 \\\n    --hash=sha256:776c8e959a01e5e8321f1dec77964cb6101020a69d5a94cd3d34db6d555e01f7 \\\n    --hash=sha256:780df0d837276276226a1ff803f8d0fa5f8996c479aeef52eb040179f3156cbd \\\n    --hash=sha256:78e6e23b954644737e385befa0deb20233e2dfddf95dd11e9db752bdd2a294d3 \\\n    --hash=sha256:7951decace76a9271a1ef181b04aa77d3cc309a02a51d73826039003210bdc86 \\\n    --hash=sha256:7ba92a2d9ace559a0a14b03d87f47e021e4fa7681dc6970ebbc7b447c7d4b7cd \\\n    --hash=sha256:7f6428fee52d2bcf96a8aa7b62095b190ee341ab0e6b1bcf50c615d7966fd45b \\\n    --hash=sha256:87944bd16b7fe6160607f6a17808abd25f17f61ae1e26c47a491b970fb66d8cb \\\n    --hash=sha256:87a6e922b2b2401e0b0cf6b976b97f11ec7f136bfed445e16384fbf6fd5e8602 \\\n    --hash=sha256:8cb0688a8d81c63d716e867d59a9ccc389e97ac7037ebef904c2b89334407180 \\\n    --hash=sha256:8df6612df74409080575dca38a5237282865408016e65636a76a2eb9348c2567 \\\n    --hash=sha256:911a6e91d08bb2c72938bc17f0a2d97864c531536b7832abee6429d5296e5b27 \\\n    --hash=sha256:92b7ee222e2b903e0a4b329a9943d432b3767f2d5029dbe4ca59fb75223bbe2e \\\n    --hash=sha256:938f756c2b9374bbcc262a37eea521d8a0e6458162f2a9c26329cc87fdf06534 \\\n    --hash=sha256:9756d9b9d4547e091f99d554fbba0d2a920aab98caa82a8fb3d3d9bee3c9ae85 \\\n    --hash=sha256:98b88a2bf26965f2015a771381624dd4b0839034b70d406dc74fd8be4cc053e3 \\\n    --hash=sha256:9b751a6306f330801665ae69270a8a3993654a85569b3469662efaad6cf5cc50 \\\n    --hash=sha256:a2a450bcce4931b295fc0848f384834c3f9b00edfc2150baafb4488c27953de6 \\\n    --hash=sha256:a3814760a1a700f3cfd2f977249f1032301d0a12c92aba74605cfa6ce9f78489 \\\n    --hash=sha256:a5abcbba9f4b463a45c8ca8b7720891200658f6f46894f79517e6cd11f3405ca \\\n    --hash=sha256:a6db7458ab89c7d80bc1f4e930cc9df6edee2200127cfa6f6e080cf619eddfbd \\\n    --hash=sha256:ad497f38a0d6c329cb621774788583ee12321863cd4bd9feee1effd60f2ad133 \\\n    --hash=sha256:ad9509ffb2396483ceacb1eee9134724443ee45b92141105a4645857244aecc8 \\\n    --hash=sha256:bbcba75fe879ad6fd2e0d6a8d937f34a571f116a0e4db37df8079e738ea95c71 \\\n    --hash=sha256:c10d85e81d0b9ef87970ecbdbfaeec14a361a7fa947118817fcea8e45335fa46 \\\n    --hash=sha256:c15b2271c44da77ee9d822552201180779e5e942f3a71fb74e026bf6172ff287 \\\n    --hash=sha256:ca37057625693d097543bd88076ceebeb248291df9d6ca8481349efc0b05dcd0 \\\n    --hash=sha256:cc3a145479a76ad0ed646434d09216d33d08eef0d8c9a11f5ae5cdc37caa3540 \\\n    --hash=sha256:ccf10f16ab498d20e28bc2b5c1306e9c1512f2840f7b6a67000a517a4b37d5ee \\\n    --hash=sha256:cd464ba806e27ee24a91362ba3621bfc39dbbb8b79f2e1340201615197370f7c \\\n    --hash=sha256:d007aa39a52d62373bd23428ba4a2546eed0e7643d7bf2e41ddcefd54519842c \\\n    --hash=sha256:d0666afbe984f6933fe72cd1f1c3560d8c55880a0bdd728ad774006eb4241ecd \\\n    --hash=sha256:d07502cc14ecd64f52b2a74ebbc106893d9a9717120057ea9ea1fd6568a747e7 \\\n    --hash=sha256:d489d9778522fbd0f8d6a5c6e48e3514f11be81cb0a5954bdda06f7e1594b321 \\\n    --hash=sha256:df7db76400bf46ec6a0a73192b14c8295bdb9812053f4fe53f4e789f3ea66bbb \\\n    --hash=sha256:e3538bc9fe1b902bef51372462e3d7c96fce2b566642512138a480b7adc9d508 \\\n    --hash=sha256:e87fd812899aa78252866ae03a048e77bd11b80fb4878ce27c23cade239b42b2 \\\n    --hash=sha256:ecdb8173e6c7aa09eee342ac62e193e6904923bd232e76b4157ac0bfa670609f \\\n    --hash=sha256:f244b8e541f414664889e2c87cac11a07b918cb4b540c36f7ada7bfa76571ea2 \\\n    --hash=sha256:f4065145bf69de124accdd17ea5f4dc770da0a6a6e440c53f6e0a8c27b3e635c \\\n    --hash=sha256:f420bfe862fb357a6d76f2065447ef6f484bc489292ac91e29bc65d2d7a2c84d \\\n    --hash=sha256:f6ddd90d9fb4b501c97a4458f1c1720e42432c26cb76d28177c5b5ad4e332601 \\\n    --hash=sha256:fa73e8c2656a3653ae6c307b3f4e878a21f87859a9afab228280ddccd7369d71 \\\n    --hash=sha256:fadbb8f1d4140825069db3fedbbb843290fd5f5bc0a5dbd7eaf81d91bf1b003b \\\n    --hash=sha256:fb3d0cc5cdb926090748ea60172fa8a213cec728bd6c54eae18b96040fcd6227 \\\n    --hash=sha256:fb46bb0f24813e6cede6cc07b1961d4b04f331f7112a23b5e21f567da4ee50aa \\\n    --hash=sha256:fd36c119c5d6551bce374fcb5c19269638f8d09862445f85a5a48596fd59f4bb\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   aiohttp-cors\n    #   pytest-aiohttp\naiohttp-cors==0.7.0 \\\n    --hash=sha256:0451ba59fdf6909d0e2cd21e4c0a43752bc0703d33fc78ae94d9d9321710193e \\\n    --hash=sha256:4d39c6d7100fd9764ed1caf8cebf0eb01bf5e3f24e2e073fda6234bc48b19f5d\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\naiorwlock==1.3.0 \\\n    --hash=sha256:45baf8e4fa9a23e0bb325fbd67da80de1fd7ae1d4f59a6381754c60cec7b289b \\\n    --hash=sha256:83f12d87df4b9728a0b8fda1756585ab0d652b107bab59c6084e1b1ad692ab45\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\naiosignal==1.3.1 \\\n    --hash=sha256:54cd96e15e1649b75d6c87526a6ff0b6c1b0dd3459f43d9ca11d48c339b68cfc \\\n    --hash=sha256:f8376fb07dd1e86a584e4fcdec80b36b7f81aac666ebc724e2c090300dd83b17\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   aiohttp\naiosqlite==0.19.0 \\\n    --hash=sha256:95ee77b91c8d2808bd08a59fbebf66270e9090c3d92ffbf260dc0db0b979577d \\\n    --hash=sha256:edba222e03453e094a3ce605db1b970c4b3376264e56f32e2a4959f948d66a96\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ypy-websocket\nannotated-types==0.6.0 \\\n    --hash=sha256:0641064de18ba7a25dee8f96403ebc39113d0cb953a01429249d5c7564666a43 \\\n    --hash=sha256:563339e807e53ffd9c267e99fc6d9ea23eb8443c08f112651963e24e22f84a5d\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   pydantic\nanyio==3.7.1 \\\n    --hash=sha256:44a3c9aba0f5defa43261a8b3efb97891f2bd7d804e0e1f56419befa1adfc780 \\\n    --hash=sha256:91dee416e570e92c64041bd18b900d1d6fa78dff7048769ce5ac5ddad004fbb5\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Flappy Bird Environment - Shell Command\nDESCRIPTION: Command to install the flappy_bird_gymnasium package as a prerequisite for running the Flappy Bird example.\nSOURCE: https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamerv3/README.md#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install flappy_bird_gymnasium\n```\n\n----------------------------------------\n\nTITLE: Local Storage Configuration for Single Node Cluster\nDESCRIPTION: Demonstrates configuring local storage for a single-node experiment with a custom storage path\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/persistent-storage.rst#2025-04-12_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom ray import train\nfrom ray.train.torch import TorchTrainer\n\ntrainer = TorchTrainer(\n    ...,\n    run_config=train.RunConfig(\n        storage_path=\"/tmp/custom/storage/path\",\n        name=\"experiment_name\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Example of KUBERAY_GEN_RAY_START_CMD Environment Variable\nDESCRIPTION: This shell command example shows the format of the KUBERAY_GEN_RAY_START_CMD environment variable that KubeRay injects into Ray containers, which contains the generated Ray start command without the ulimit command.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/pod-command.md#2025-04-12_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# Example of the environment variable `KUBERAY_GEN_RAY_START_CMD` in the head Pod.\nray start --head  --dashboard-host=0.0.0.0  --num-cpus=1  --block  --metrics-export-port=8080  --memory=2147483648\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret from netrc File\nDESCRIPTION: Bash command to create a Kubernetes secret from the contents of a local .netrc file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/runtime_env_auth.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic netrc-secret --from-file=.netrc=\"$HOME/.netrc\"\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Cluster with Kind\nDESCRIPTION: This command creates a Kubernetes cluster using Kind with a specific node image version.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/mobilenet-rayservice.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n----------------------------------------\n\nTITLE: Execute hyperparameter tuning\nDESCRIPTION: Executes the hyperparameter tuning process using the configured Tuner.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntune_results = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Logging Non-Reducible Values with MetricsLogger in Python\nDESCRIPTION: Logs non-reducible values such as sequential images using 'reduce=None'. The 'clear_on_reduce=True' flag ensures that logged values are cleared after each reduction to prevent memory leaks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/metrics-logger.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlogger.log_value(\"some_items\", value=\"a\", reduce=None, clear_on_reduce=True)\nlogger.log_value(\"some_items\", value=\"b\")\nlogger.log_value(\"some_items\", value=\"c\")\nlogger.log_value(\"some_items\", value=\"d\")\n\nlogger.peek(\"some_items\")  # expect a list: [\"a\", \"b\", \"c\", \"d\"]\n\nlogger.reduce()\nlogger.peek(\"some_items\")  # expect an empty list: []\n```\n\n----------------------------------------\n\nTITLE: Memory NN Example with Ray Tune in Python\nDESCRIPTION: This snippet demonstrates the implementation of a Memory Neural Network example using Ray Tune in Python.  It involves setting up the necessary configurations, defining the search space, and running the Tune experiment to optimize the model's performance. The code requires Ray and potentially other dependencies defined within the referenced file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/pbt_memnn_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\n.. literalinclude:: /../../python/ray/tune/examples/pbt_memnn_example.py\n\n```\n\n----------------------------------------\n\nTITLE: Implementing CIFAR10 Data Loaders\nDESCRIPTION: Functions to load and preprocess CIFAR10 dataset with data normalization and train/validation splits. Includes thread-safe data downloading with FileLock.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-pytorch-cifar.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef load_data(data_dir=\"./data\"):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    with FileLock(os.path.expanduser(\"~/.data.lock\")):\n        trainset = torchvision.datasets.CIFAR10(\n            root=data_dir, train=True, download=True, transform=transform)\n\n        testset = torchvision.datasets.CIFAR10(\n            root=data_dir, train=False, download=True, transform=transform)\n\n    return trainset, testset\n\ndef create_dataloaders(trainset, batch_size, num_workers=8):\n    train_size = int(len(trainset) * 0.8)\n    train_subset, val_subset = random_split(\n        trainset, [train_size, len(trainset) - train_size])\n\n    train_loader = torch.utils.data.DataLoader(\n        train_subset,\n        batch_size=batch_size, \n        shuffle=True,\n        num_workers=num_workers\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_subset,\n        batch_size=batch_size,\n        shuffle=False, \n        num_workers=num_workers\n    )\n    return train_loader, val_loader\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Classifier on Breast Cancer Dataset\nDESCRIPTION: Complete implementation of XGBoost training pipeline including data loading, preprocessing, model training, and evaluation on the breast cancer dataset.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-xgboost.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n\ndef train_breast_cancer(config):\n    # Load dataset\n    data, labels = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    # Split into train and test set\n    train_x, test_x, train_y, test_y = train_test_split(data, labels, test_size=0.25)\n    # Build input matrices for XGBoost\n    train_set = xgb.DMatrix(train_x, label=train_y)\n    test_set = xgb.DMatrix(test_x, label=test_y)\n    # Train the classifier\n    results = {}\n    bst = xgb.train(\n        config,\n        train_set,\n        evals=[(test_set, \"eval\")],\n        evals_result=results,\n        verbose_eval=False,\n    )\n    return results\n\n\nresults = train_breast_cancer(\n    {\"objective\": \"binary:logistic\", \"eval_metric\": [\"logloss\", \"error\"]}\n)\naccuracy = 1.0 - results[\"eval\"][\"error\"][-1]\nprint(f\"Accuracy: {accuracy:.4f}\")\n```\n\n----------------------------------------\n\nTITLE: Listing Python-DateUtil Package with Hash Values\nDESCRIPTION: Definition for the Python-DateUtil package dependency with version 2.8.2 and corresponding SHA256 hash values. The comment indicates this is required by multiple packages including botocore, matplotlib, pandas, and typepy.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_28\n\nLANGUAGE: text\nCODE:\n```\npython-dateutil==2.8.2 \\\n    --hash=sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86 \\\n    --hash=sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   botocore\n    #   matplotlib\n    #   pandas\n    #   typepy\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost with Fractional GPUs in Python\nDESCRIPTION: A code snippet demonstrating how to configure Ray Tune to use fractional GPUs for XGBoost training. This approach allows multiple trials to share the same GPU by allocating only a portion of the GPU memory to each trial.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-xgboost.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": [\"logloss\", \"error\"],\n    \"tree_method\": \"gpu_hist\",\n    \"max_depth\": tune.randint(1, 9),\n    \"min_child_weight\": tune.choice([1, 2, 3]),\n    \"subsample\": tune.uniform(0.5, 1.0),\n    \"eta\": tune.loguniform(1e-4, 1e-1),\n}\n\ntuner = tune.Tuner(\n    tune.with_resources(train_breast_cancer, resources={\"cpu\": 1, \"gpu\": 0.1}),\n    tune_config=tune.TuneConfig(num_samples=1 if SMOKE_TEST else 10),\n    param_space=config,\n)\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Citing RLlib - Second BibTeX Entry\nDESCRIPTION: This BibTeX entry is for citing the \"RLlib: Abstractions for Distributed Reinforcement Learning\" paper, presented at ICML 2018. It provides the necessary information, including title, authors, booktitle, year, and URL, to cite the paper appropriately in academic research.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_11\n\nLANGUAGE: BibTeX\nCODE:\n```\n@inproceedings{liang2018rllib,\n    title={{\\RLlib}: Abstractions for Distributed Reinforcement Learning},\n    author={\n        Eric Liang and\n        Richard Liaw and\n        Robert Nishihara and\n        Philipp Moritz and\n        Roy Fox and\n        Ken Goldberg and\n        Joseph E. Gonzalez and\n        Michael I. Jordan and\n        Ion Stoica,\n    },\n    booktitle = {International Conference on Machine Learning ({ICML})},\n    year={2018},\n    url={https://arxiv.org/pdf/1712.09381}\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Shellingham Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the shellingham package dependency with version 1.5.4 and SHA-256 hashes for verification. Comments indicate this package is required by typer and is constrained by requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_39\n\nLANGUAGE: pip\nCODE:\n```\nshellingham==1.5.4 \\\n    --hash=sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686 \\\n    --hash=sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   typer\n```\n\n----------------------------------------\n\nTITLE: Package Source Configuration for Ray Dependencies\nDESCRIPTION: Configuration for Python package sources, specifying the primary PyPI index URL, additional index URL for PyTorch CUDA 12.1 packages, and additional find-links URL for PyG packages compatible with PyTorch 2.5.1.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cu121\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html\n```\n\n----------------------------------------\n\nTITLE: Check Application Status - Console\nDESCRIPTION: Console command 'serve status' to check the health and deployment status of Serve applications and their components. Provides a snapshot of healthy or running states, replica counts, and timestamps of last deployment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/multi-app.md#2025-04-12_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ serve status\nproxies:\n  2e02a03ad64b3f3810b0dd6c3265c8a00ac36c13b2b0937cbf1ef153: HEALTHY\napplications:\n  app1:\n    status: RUNNING\n    message: ''\n    last_deployed_time_s: 1693267064.0735464\n    deployments:\n      downloader:\n        status: HEALTHY\n        replica_states:\n          RUNNING: 1\n        message: ''\n      ImageClassifier:\n        status: HEALTHY\n        replica_states:\n          RUNNING: 1\n        message: ''\n  app2:\n    status: RUNNING\n    message: ''\n    last_deployed_time_s: 1693267064.0735464\n    deployments:\n      Translator:\n        status: HEALTHY\n        replica_states:\n          RUNNING: 1\n        message: ''\n```\n\n----------------------------------------\n\nTITLE: Package Hash Requirements Configuration\nDESCRIPTION: Package dependency configuration listing SHA256 hashes for dependency verification. Contains requirements for packages like multidict 6.0.5 and networkx 3.2.1 with their corresponding hash values for security verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:a40821a89dc373d6427e2b44b572efc36a2778d3f543299e2f24eb1a5de65415 \\\n--hash=sha256:b291f0ee7961a597cbbcc77709374087fa2a9afe7bdb6a40dbbd9b127e79afee \\\n--hash=sha256:b573a43ef7c368ba4ea06050a957c2a7550f729c31f11dd616d2ac4aba99888d \\\n--hash=sha256:b610ff0f24e9f11c9ae653c67ff8cc03c075131401b3e5ef4b82570d1728f8a9 \\\n--hash=sha256:bdf38ba2d393c7911ae989c3bbba510ebbcdf4ecbdbfec36272abe350c454075\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Data Package with pip\nDESCRIPTION: Command to install the Ray Data package and its dependencies using pip.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"ray[data]\"\n```\n\n----------------------------------------\n\nTITLE: Defining Configuration Space - Python\nDESCRIPTION: This snippet illustrates how to define the hyperparameter search space using ConfigSpace, tailored for BOHB requirements.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconfig_space = CS.ConfigurationSpace()\\nconfig_space.add_hyperparameter(\\n    CS.Constant(\"steps\", 100)\\n)\\nconfig_space.add_hyperparameter(\\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\\n)\\nconfig_space.add_hyperparameter(\\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\\n)\\nconfig_space.add_hyperparameter(\\n    CS.CategoricalHyperparameter(\\n        \"activation\", choices=[\"relu\", \"tanh\"]\\n    )\\n)\n```\n\n----------------------------------------\n\nTITLE: Modifying RayJob Metadata for Kueue Integration\nDESCRIPTION: YAML snippet showing the necessary modifications to RayJob metadata for integration with Kueue. This includes specifying the queue name and priority class.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.md#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n  generateName: dev-pytorch-text-classifier-\n  labels:\n    kueue.x-k8s.io/queue-name: user-queue\n    kueue.x-k8s.io/priority-class: dev-priority\n```\n\n----------------------------------------\n\nTITLE: Managing RayCluster Resources with kubectl-ray\nDESCRIPTION: Commands to create worker groups, get cluster status, worker groups, and node information using the kubectl-ray plugin.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kubectl-plugin.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl ray create workergroup example-group --ray-cluster raycluster-sample --worker-memory 5Gi\nkubectl ray get cluster\nkubectl ray get workergroup\nkubectl ray get nodes\n```\n\n----------------------------------------\n\nTITLE: Specifying TensorBoard-Data-Server Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the tensorboard-data-server package dependency with version 0.7.2 and multiple SHA-256 hashes for verification. Comments indicate this package is required by tensorboard and is constrained by requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_46\n\nLANGUAGE: pip\nCODE:\n```\ntensorboard-data-server==0.7.2 \\\n    --hash=sha256:7e0610d205889588983836ec05dc098e80f97b7e7bbff7e994ebb78f578d0ddb \\\n    --hash=sha256:9fe5d24221b29625dbc7328b0436ca7fc1c23de4acf4d272f1180856e32f9f60 \\\n    --hash=sha256:ef687163c24185ae9754ed5650eb5bc4d84ff257aabdc33f0cc6f74d8ba54530\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   tensorboard\n```\n\n----------------------------------------\n\nTITLE: Specifying Pillow Package Dependency\nDESCRIPTION: Defines the Pillow package dependency with version 10.3.0 and multiple SHA-256 hashes. Pillow is an imaging library that may be used for image processing tasks in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_18\n\nLANGUAGE: Text\nCODE:\n```\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    # ... (additional hashes omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Handling Ingress Connection Error in Python\nDESCRIPTION: This snippet shows a typical error message when encountering issues connecting to a Ray Cluster through an Ingress. It includes the gRPC error details which can be useful for debugging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/ray-client.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ngrpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n    status = StatusCode.INVALID_ARGUMENT\n    details = \"\"\n    debug_error_string = \"{\"created\":\"@1628668820.164591000\",\"description\":\"Error received from peer ipv4:10.233.120.107:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"\",\"grpc_status\":3}\"\n>\nGot Error from logger channel -- shutting down: <_MultiThreadedRendezvous of RPC that terminated with:\n    status = StatusCode.INVALID_ARGUMENT\n    details = \"\"\n    debug_error_string = \"{\"created\":\"@1628668820.164713000\",\"description\":\"Error received from peer ipv4:10.233.120.107:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"\",\"grpc_status\":3}\"\n>\n```\n\n----------------------------------------\n\nTITLE: Defining Cloud Storage Tool Dependencies for Ray Project\nDESCRIPTION: This snippet specifies the required version of the cloud storage tool used in the Ray project. It includes s3fs, which is a Pythonic file interface to S3 compatible storage services.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/core-requirements.txt#2025-04-12_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Cloud storage tools\ns3fs==2023.5.0\n```\n\n----------------------------------------\n\nTITLE: Creating reStructuredText Table of Contents for Ray ML Libraries\nDESCRIPTION: This snippet defines a hidden table of contents (toctree) in reStructuredText format that organizes links to various Ray ML library integrations documentation pages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :hidden:\n\n    joblib\n    multiprocessing\n    ray-collective\n    dask-on-ray\n    raydp\n    mars-on-ray\n    modin/index\n    Ray Workflows (Deprecated) <../workflows/index>\n```\n\n----------------------------------------\n\nTITLE: Configuring IAM Instance Profile for Ray Worker Nodes in YAML\nDESCRIPTION: This YAML snippet shows how to configure the IAM instance profile for Ray worker nodes in the cluster configuration file. It specifies the instance type and the IAM instance profile name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/aws.md#2025-04-12_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nray.worker.default:\n    node_config:\n        InstanceType: c5a.large\n        IamInstanceProfile:\n            Name: ray-autoscaler-cloudwatch-v1\n```\n\n----------------------------------------\n\nTITLE: Example RLlink PING Message Structure\nDESCRIPTION: Shows the structure of a simple PING message in the RLlink protocol. The message consists of an 8-byte header encoding the size of the body (16 bytes) followed by the JSON body containing the message type.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/external-envs.rst#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n00000016{\"type\": \"PING\"}\n```\n\n----------------------------------------\n\nTITLE: Applying Kueue Resources\nDESCRIPTION: Command to apply the Kueue resource configurations to the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f kueue-resources.yaml\n```\n\n----------------------------------------\n\nTITLE: Deprecated Metric Reporting in Ray Train V2 using Python\nDESCRIPTION: This snippet illustrates the deprecated method of reporting free-floating metrics directly from workers which writes to log files. This feature will not be supported in Ray Train V2. Users are advised to transition to reporting metrics directly to external tools.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/monitoring-logging.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n.. literalinclude:: ../doc_code/metric_logging.py\n    :language: python\n    :start-after: __report_callback_start__\n    :end-before: __report_callback_end__\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Submitting Ray Job using CLI\nDESCRIPTION: Command for submitting a Ray job to the cluster using the Ray Jobs CLI. This specifies the Ray cluster address as an environment variable and sets the working directory to the current directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_start.txt#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRAY_ADDRESS='http://.+:8265' ray job submit --working-dir . -- python my_script.py\n```\n\n----------------------------------------\n\nTITLE: Configuring External Expert Data Schema in RLlib\nDESCRIPTION: Demonstrates how to configure RLlib to read external expert data with a custom schema mapping for observations, actions, rewards, and other data columns.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.bc import BCConfig\nfrom ray.rlib.core.columns import Columns\n\nconfig = (\n    BCConfig()\n    ...\n    .offline_data(\n        input_=[<input_path>],\n        input_read_schema={\n            Columns.OBS: \"o_t\",\n            Columns.ACTIONS: \"a_t\",\n            Columns.REWARDS: \"r_t\",\n            Columns.NEXT_OBS: \"o_tp1\",\n            Columns.INFOS: \"i_t\",\n            \"done\": \"d_t\",\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying PyCParser Package Dependency with Hash Verification\nDESCRIPTION: Defines the pycparser package dependency at version 2.21 with SHA256 hash verification. This package is required by cffi for C code parsing functionality.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_29\n\nLANGUAGE: plaintext\nCODE:\n```\npycparser==2.21 \\\n    --hash=sha256:8ee45429555515e1f6b185e78100aea234072576aa43ab53aefcae078162fca9 \\\n    --hash=sha256:e644fdec12f7872f86c58ff790da456218b10f863970249516d60a5eaca77206\n```\n\n----------------------------------------\n\nTITLE: Connecting to Multiple Ray Clusters in Python\nDESCRIPTION: This Python code demonstrates how to connect to multiple Ray clusters in a single Python process using Ray Client. It shows initialization, context switching, and proper disconnection.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/ray-client.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n# Create a default client.\nray.init(\"ray://<head_node_host_cluster>:10001\")\n\n# Connect to other clusters.\ncli1 = ray.init(\"ray://<head_node_host_cluster_1>:10001\", allow_multiple=True)\ncli2 = ray.init(\"ray://<head_node_host_cluster_2>:10001\", allow_multiple=True)\n\n# Data is put into the default cluster.\nobj = ray.put(\"obj\")\n\nwith cli1:\n    obj1 = ray.put(\"obj1\")\n\nwith cli2:\n    obj2 = ray.put(\"obj2\")\n\nwith cli1:\n    assert ray.get(obj1) == \"obj1\"\n    try:\n        ray.get(obj2)  # Cross-cluster ops not allowed.\n    except:\n        print(\"Failed to get object which doesn't belong to this cluster\")\n\nwith cli2:\n    assert ray.get(obj2) == \"obj2\"\n    try:\n        ray.get(obj1)  # Cross-cluster ops not allowed.\n    except:\n        print(\"Failed to get object which doesn't belong to this cluster\")\nassert \"obj\" == ray.get(obj)\ncli1.disconnect()\ncli2.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Specifying Protobuf Package Dependency with Hash Verification\nDESCRIPTION: Defines the protobuf package dependency at version 3.20.3 with SHA256 hash verification. This package is required by several components including Google API Core, gRPC tools, OpenTelemetry, and the Ray framework itself.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\nprotobuf==3.20.3 \\\n    --hash=sha256:03038ac1cfbc41aa21f6afcbcd357281d7521b4157926f30ebecc8d4ea59dcb7 \\\n    --hash=sha256:28545383d61f55b57cf4df63eebd9827754fd2dc25f80c5253f9184235db242c \\\n    --hash=sha256:2e3427429c9cffebf259491be0af70189607f365c2f41c7c3764af6f337105f2 \\\n    --hash=sha256:398a9e0c3eaceb34ec1aee71894ca3299605fa8e761544934378bbc6c97de23b \\\n    --hash=sha256:44246bab5dd4b7fbd3c0c80b6f16686808fab0e4aca819ade6e8d294a29c7050 \\\n    --hash=sha256:447d43819997825d4e71bf5769d869b968ce96848b6479397e29fc24c4a5dfe9 \\\n    --hash=sha256:67a3598f0a2dcbc58d02dd1928544e7d88f764b47d4a286202913f0b2801c2e7 \\\n    --hash=sha256:74480f79a023f90dc6e18febbf7b8bac7508420f2006fabd512013c0c238f454 \\\n    --hash=sha256:819559cafa1a373b7096a482b504ae8a857c89593cf3a25af743ac9ecbd23480 \\\n    --hash=sha256:899dc660cd599d7352d6f10d83c95df430a38b410c1b66b407a6b29265d66469 \\\n    --hash=sha256:8c0c984a1b8fef4086329ff8dd19ac77576b384079247c770f29cc8ce3afa06c \\\n    --hash=sha256:9aae4406ea63d825636cc11ffb34ad3379335803216ee3a856787bcf5ccc751e \\\n    --hash=sha256:a7ca6d488aa8ff7f329d4c545b2dbad8ac31464f1d8b1c87ad1346717731e4db \\\n    --hash=sha256:b6cc7ba72a8850621bfec987cb72623e703b7fe2b9127a161ce61e61558ad905 \\\n    --hash=sha256:bf01b5720be110540be4286e791db73f84a2b721072a3711efff6c324cdf074b \\\n    --hash=sha256:c02ce36ec760252242a33967d51c289fd0e1c0e6e5cc9397e2279177716add86 \\\n    --hash=sha256:d9e4432ff660d67d775c66ac42a67cf2453c27cb4d738fc22cb53b5d84c135d4 \\\n    --hash=sha256:daa564862dd0d39c00f8086f88700fdbe8bc717e993a21e90711acfed02f2402 \\\n    --hash=sha256:de78575669dddf6099a8a0f46a27e82a1783c557ccc38ee620ed8cc96d3be7d7 \\\n    --hash=sha256:e64857f395505ebf3d2569935506ae0dfc4a15cb80dc25261176c784662cdcc4 \\\n    --hash=sha256:f4bd856d702e5b0d96a00ec6b307b0f51c1982c2bf9c0052cf9019e9a544ba99 \\\n    --hash=sha256:f4c42102bc82a51108e449cbb32b19b180022941c727bac0cfd50170341f16ee\n```\n\n----------------------------------------\n\nTITLE: Specifying PyASN1 Package Dependency with Hash Verification\nDESCRIPTION: Defines the pyasn1 package dependency at version 0.5.1 with SHA256 hash verification. This package is required by oauth2client, pyasn1-modules, and rsa components.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_25\n\nLANGUAGE: plaintext\nCODE:\n```\npyasn1==0.5.1 \\\n    --hash=sha256:4439847c58d40b1d0a573d07e3856e95333f1976294494c325775aeca506eb58 \\\n    --hash=sha256:6d391a96e59b23130a5cfa74d6fd7f388dbbe26cc8f1edf39fdddf08d9d6676c\n```\n\n----------------------------------------\n\nTITLE: Configuring PrometheusRule for Custom Metrics in YAML\nDESCRIPTION: This YAML configuration defines a PrometheusRule to create a custom metric using a recording rule. It calculates the availability of the Ray Global Control Store (GCS) over a 30-day period.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md#2025-04-12_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: ray-cluster-gcs-rules\n  namespace: prometheus-system\n  labels:\n    release: prometheus\nspec:\n  groups:\n  - name: ray-cluster-main-staging-gcs.rules\n    interval: 30s\n    rules:\n    - record: ray_gcs_availability_30d\n      expr: |\n      (\n        100 * (\n          sum(rate(ray_gcs_update_resource_usage_time_bucket{container=\"ray-head\", le=\"20.0\"}[30d]))\n          /\n          sum(rate(ray_gcs_update_resource_usage_time_count{container=\"ray-head\"}[30d]))\n        )\n      )\n```\n\n----------------------------------------\n\nTITLE: Netty Decoder Channel Read Complete\nDESCRIPTION: Thread stack trace showing Netty's ByteToMessageDecoder handling channel read completion. This represents the end of a read operation in the Netty pipeline where decoded messages are ready for application processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_112\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j]\n```\n\n----------------------------------------\n\nTITLE: Defining Standard Python Package Dependencies with Version Pinning\nDESCRIPTION: This snippet shows several Python package dependencies (debugpy, decorator, defusedxml, etc.) with exact version numbers and SHA256 hash verification for secure installation. Each package includes comments indicating which part of the project requires the dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_8\n\nLANGUAGE: pip\nCODE:\n```\ndebugpy==1.8.0 \\\n    --hash=sha256:125b9a637e013f9faac0a3d6a82bd17c8b5d2c875fb6b7e2772c5aba6d082332 \\\n    --hash=sha256:12af2c55b419521e33d5fb21bd022df0b5eb267c3e178f1d374a63a2a6bdccd0 \\\n    --hash=sha256:3c6fb41c98ec51dd010d7ed650accfd07a87fe5e93eca9d5f584d0578f28f35f \\\n    --hash=sha256:46ab6780159eeabb43c1495d9c84cf85d62975e48b6ec21ee10c95767c0590aa \\\n    --hash=sha256:57161629133113c97b387382045649a2b985a348f0c9366e22217c87b68b73c6 \\\n    --hash=sha256:5d9de202f5d42e62f932507ee8b21e30d49aae7e46d5b1dd5c908db1d7068637 \\\n    --hash=sha256:60009b132c91951354f54363f8ebdf7457aeb150e84abba5ae251b8e9f29a8a6 \\\n    --hash=sha256:61eab4a4c8b6125d41a34bad4e5fe3d2cc145caecd63c3fe953be4cc53e65bf8 \\\n    --hash=sha256:7fb95ca78f7ac43393cd0e0f2b6deda438ec7c5e47fa5d38553340897d2fbdfb \\\n    --hash=sha256:8cd0197141eb9e8a4566794550cfdcdb8b3db0818bdf8c49a8e8f8053e56e38b \\\n    --hash=sha256:9c9b0ac1ce2a42888199df1a1906e45e6f3c9555497643a85e0bf2406e3ffbc4 \\\n    --hash=sha256:a64093656c4c64dc6a438e11d59369875d200bd5abb8f9b26c1f5f723622e153 \\\n    --hash=sha256:a8b7a2fd27cd9f3553ac112f356ad4ca93338feadd8910277aff71ab24d8775f \\\n    --hash=sha256:b05a6b503ed520ad58c8dc682749113d2fd9f41ffd45daec16e558ca884008cd \\\n    --hash=sha256:bdc5ef99d14b9c0fcb35351b4fbfc06ac0ee576aeab6b2511702e5a648a2e595 \\\n    --hash=sha256:e3412f9faa9ade82aa64a50b602544efcba848c91384e9f93497a458767e6926 \\\n    --hash=sha256:ef54404365fae8d45cf450d0544ee40cefbcb9cb85ea7afe89a963c27028261e \\\n    --hash=sha256:ef9ab7df0b9a42ed9c878afd3eaaff471fce3fa73df96022e1f5c9f8f8c87ada\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   ipykernel\ndecoration/requirements_compiled_ray_test_py311_cpu.txt\n    #   ipython\ndefusedxml==0.7.1 \\\n    --hash=sha256:1bb3032db185915b62d7c6209c5a8792be6a32ab2fedacc84e01b52c51aa3e69 \\\n    --hash=sha256:a352e7e428770286cc899e2542b6cdaedb2b4953ff269a210103ec58f6198a61\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   nbconvert\ndepyf==0.18.0 \\\n    --hash=sha256:007294d5bac19a38a0767d747be0f49b9ffdcea0394a822644142df22b33a3e1 \\\n    --hash=sha256:b99f0c383be949ae45d5d606fe444c71f375b55a57b8d6b20e7856670d52130d\n    # via vllm\ndill==0.3.9 \\\n    --hash=sha256:468dff3b89520b474c0397703366b7b95eebe6303f108adf9b19da1f702be87a \\\n    --hash=sha256:81aa267dddf68cbfe8029c42ca9ec6a4ab3b22371d1c450abc54422577b4512c\n    # via depyf\ndiskcache==5.6.3 \\\n    --hash=sha256:2c3a3fa2743d8535d832ec61c2054a1641f41775aa7c556758a109941e33e4fc \\\n    --hash=sha256:5e31b2d5fbad117cc363ebaf6b689474db18a1f6438bc82358b024abd4c2ca19\n    # via outlines\ndistlib==0.3.7 \\\n    --hash=sha256:2e24928bc811348f0feb63014e97aaae3037f2cf48712d51ae61df7fd6075057 \\\n    --hash=sha256:9dafe54b34a028eafd95039d5e5d4851a13734540f1331060d31c9916e7147a8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   virtualenv\ndistro==1.9.0 \\\n    --hash=sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed \\\n    --hash=sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2\n    # via openai\ndnspython==2.7.0 \\\n    --hash=sha256:b4c34b7d10b51bcc3a5071e7b8dee77939f1e878477eeecc965e9835f63c6c86\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with SHA256 Hashes\nDESCRIPTION: Requirements file listing Python package dependencies with their versions and SHA256 hashes for package verification. Includes dependencies like matplotlib, matplotlib-inline, mbstrdecoder, mdit-py-plugins, mdurl, and memray with their specific version constraints.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nmatplotlib==3.7.4 \\\n    --hash=sha256:0037d066cca1f4bda626c507cddeb6f7da8283bc6a214da2db13ff2162933c52 \\\n    --hash=sha256:0604880e4327114054199108b7390f987f4f40ee5ce728985836889e11a780ba \\\nmatplotlib-inline==0.1.6 \\\n    --hash=sha256:f1f41aab5328aa5aaea9b16d083b128102f8712542f819fe7e6a420ff581b311 \\\n    --hash=sha256:f887e5f10ba98e8d2b150ddcf4702c1e5f8b3a20005eb0f74bfdbd360ee6f304\nmbstrdecoder==1.1.3 \\\n    --hash=sha256:d66c1ed3f2dc4e7c5d87cd44a75be10bc5af4250f95b38bbaedd7851308ce938 \\\n    --hash=sha256:dcfd2c759322eb44fe193a9e0b1b86c5b87f3ec5ea8e1bb43b3e9ae423f1e8fe\n```\n\n----------------------------------------\n\nTITLE: Defining Pinned Python Dependencies with Hash Verification\nDESCRIPTION: A pip requirements file that specifies exact package versions with SHA256 hash verification. Each package includes multiple hash options for different distribution formats and is annotated with comments showing which other packages depend on it.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy==1.26.4 \\\n    --hash=sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b \\\n    --hash=sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818 \\\n    --hash=sha256:1af303d6b2210eb850fcf03064d364652b7120803a0b872f5211f5234b399f20 \\\n    --hash=sha256:1dda2e7b4ec9dd512f84935c5f126c8bd8b9f2fc001e9f54af255e8c5f16b0e0 \\\n    --hash=sha256:2a02aba9ed12e4ac4eb3ea9421c420301a0c6460d9830d74a9df87efa4912010 \\\n    --hash=sha256:2e4ee3380d6de9c9ec04745830fd9e2eccb3e6cf790d39d7b98ffd19b0dd754a \\\n    --hash=sha256:3373d5d70a5fe74a2c1bb6d2cfd9609ecf686d47a2d7b1d37a8f3b6bf6003aea \\\n    --hash=sha256:47711010ad8555514b434df65f7d7b076bb8261df1ca9bb78f53d3b2db02e95c \\\n    --hash=sha256:4c66707fabe114439db9068ee468c26bbdf909cac0fb58686a42a24de1760c71 \\\n    --hash=sha256:50193e430acfc1346175fcbdaa28ffec49947a06918b7b92130744e81e640110 \\\n    --hash=sha256:52b8b60467cd7dd1e9ed082188b4e6bb35aa5cdd01777621a1658910745b90be \\\n    --hash=sha256:60dedbb91afcbfdc9bc0b1f3f402804070deed7392c23eb7a7f07fa857868e8a \\\n    --hash=sha256:62b8e4b1e28009ef2846b4c7852046736bab361f7aeadeb6a5b89ebec3c7055a \\\n    --hash=sha256:666dbfb6ec68962c033a450943ded891bed2d54e6755e35e5835d63f4f6931d5 \\\n    --hash=sha256:675d61ffbfa78604709862923189bad94014bef562cc35cf61d3a07bba02a7ed \\\n    --hash=sha256:679b0076f67ecc0138fd2ede3a8fd196dddc2ad3254069bcb9faf9a79b1cebcd \\\n    --hash=sha256:7349ab0fa0c429c82442a27a9673fc802ffdb7c7775fad780226cb234965e53c \\\n    --hash=sha256:7ab55401287bfec946ced39700c053796e7cc0e3acbef09993a9ad2adba6ca6e \\\n    --hash=sha256:7e50d0a0cc3189f9cb0aeb3a6a6af18c16f59f004b866cd2be1c14b36134a4a0 \\\n    --hash=sha256:95a7476c59002f2f6c590b9b7b998306fba6a5aa646b1e22ddfeaf8f78c3a29c \\\n    --hash=sha256:96ff0b2ad353d8f990b63294c8986f1ec3cb19d749234014f4e7eb0112ceba5a \\\n    --hash=sha256:9fad7dcb1aac3c7f0584a5a8133e3a43eeb2fe127f47e3632d43d677c66c102b \\\n    --hash=sha256:9ff0f4f29c51e2803569d7a51c2304de5554655a60c5d776e35b4a41413830d0 \\\n    --hash=sha256:a354325ee03388678242a4d7ebcd08b5c727033fcff3b2f536aea978e15ee9e6 \\\n    --hash=sha256:a4abb4f9001ad2858e7ac189089c42178fcce737e4169dc61321660f1a96c7d2 \\\n    --hash=sha256:ab47dbe5cc8210f55aa58e4805fe224dac469cde56b9f731a4c098b91917159a \\\n    --hash=sha256:afedb719a9dcfc7eaf2287b839d8198e06dcd4cb5d276a3df279231138e83d30 \\\n    --hash=sha256:b3ce300f3644fb06443ee2222c2201dd3a89ea6040541412b8fa189341847218 \\\n    --hash=sha256:b97fe8060236edf3662adfc2c633f56a08ae30560c56310562cb4f95500022d5 \\\n    --hash=sha256:bfe25acf8b437eb2a8b2d49d443800a5f18508cd811fea3181723922a8a82b07 \\\n    --hash=sha256:cd25bcecc4974d09257ffcd1f098ee778f7834c3ad767fe5db785be9a4aa9cb2 \\\n    --hash=sha256:d209d8969599b27ad20994c8e41936ee0964e6da07478d6c35016bc386b66ad4 \\\n    --hash=sha256:d5241e0a80d808d70546c697135da2c613f30e28251ff8307eb72ba696945764 \\\n    --hash=sha256:edd8b5fe47dab091176d21bb6de568acdd906d1887a4584a15a9a96a1dca06ef \\\n    --hash=sha256:f870204a840a60da0b12273ef34f7051e98c3b5961b61b0c2c1be6dfd64fbcd3 \\\n    --hash=sha256:ffa75af20b44f8dba823498024771d5ac50620e6915abac414251bd971b4529f\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements.txt\n    #   cupy-cuda12x\n    #   gymnasium\n    #   imageio\n    #   pandas\n    #   pyarrow\n    #   scikit-image\n    #   scipy\n    #   tensorboardx\n    #   tifffile\n```\n\n----------------------------------------\n\nTITLE: Getting and Setting State for LearnerGroup in Python\nDESCRIPTION: This snippet demonstrates how to get and set the complete state or just the RLModule weights for all learners in a LearnerGroup. The state includes neural network weights and optimizer states, which can be retrieved or updated as needed. This is important for managing consistent states across distributed learners.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-learner.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Get the LearnerGroup's RLModule weights and optimizer states.\nstate = learner_group.get_state()\nlearner_group.set_state(state)\n\n# Only get the RLModule weights.\nweights = learner_group.get_weights()\nlearner_group.set_weights(weights)\n```\n\n----------------------------------------\n\nTITLE: Installing xformers Package with Platform-Specific Constraints\nDESCRIPTION: This snippet defines the installation for xformers version 0.0.29.post2 with platform-specific constraints (x86_64 architecture on Linux) and SHA256 hash verification. The package is required by vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_49\n\nLANGUAGE: text\nCODE:\n```\nxformers==0.0.29.post2 ; platform_machine == 'x86_64' and sys_platform == 'linux' \\\n    --hash=sha256:0d0eb14db56cf08ec3fb9cb36ed5e98de1303411571539ca4dc080c5861e2744 \\\n    --hash=sha256:2eed954ce0491d379f19ea38796027d367e259a90d1fcc9f4166331c1c27ce87 \\\n    --hash=sha256:6ca3d1a6db6f2abff25c1154adee96987f77f4dfd5141771805afa5fc13e9395 \\\n    --hash=sha256:a3ddb47abce3810d3928e8f48b290c0423c7939764a217c2b35ac8124a3cf641 \\\n    --hash=sha256:bbf0e9505f6b2e2b7738eeb3c22e94c45e6297fbdae66626febb0dbfe28c5050 \\\n    --hash=sha256:c3e19aa15de0242c27096e2cb72636123c4475096a9397f4f331eb08c67d193b \\\n    --hash=sha256:eb1db57f05b595ed9f1d0f8cc83a8e54d2c0737a16982238a01e93bdd0f2a4f5 \\\n    --hash=sha256:eb73626de82953fa7673a19ddcff3ef37d5de5f4e3230fe18dfd99c52460c55d \\\n    --hash=sha256:f4379dda52efd4e7beb9a3bdae183f6c9857a77f04d58ed2e000ce92b05f5d92\n    # via vllm\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Collection of Python package dependencies with specific versions and SHA256 hashes for secure package verification. Each package includes version constraints and cryptographic hashes to ensure package integrity.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nasttokens==2.4.1 \\\n    --hash=sha256:051ed49c3dcae8913ea7cd08e46a606dba30b79993209636c4875bc1d637bc24 \\\n    --hash=sha256:b03869718ba9a6eb027e134bfdf69f38a236d681c83c160d510768af11254ba0\n\nasync-timeout==4.0.3 \\\n    --hash=sha256:4640d96be84d82d02ed59ea2b7105a0f7b33abe8703703cd0ab0bf87c427522f \\\n    --hash=sha256:7405140ff1230c310e51dc27b3145b9092d659ce68ff733fb0cefe3ee42be028\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Tune with Cloud Storage and Checkpoint Management (Python)\nDESCRIPTION: This comprehensive example demonstrates configuring Ray Tune with cloud storage (S3), custom experiment naming, and checkpoint management. It sets up a Tuner with specific storage path, checkpoint configuration, and experiment name.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-storage.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport ray\nfrom ray import tune\nfrom your_module import my_trainable\n\n\ntuner = tune.Tuner(\n    my_trainable,\n    run_config=tune.RunConfig(\n        # Name of your experiment\n        name=\"my-tune-exp\",\n        # Configure how experiment data and checkpoints are persisted.\n        # We recommend cloud storage checkpointing as it survives the cluster when\n        # instances are terminated and has better performance.\n        storage_path=\"s3://my-checkpoints-bucket/path/\",\n        checkpoint_config=tune.CheckpointConfig(\n            # We'll keep the best five checkpoints at all times\n            # (with the highest AUC scores, a metric reported by the trainable)\n            checkpoint_score_attribute=\"max-auc\",\n            checkpoint_score_order=\"max\",\n            num_to_keep=5,\n        ),\n    ),\n)\n# This starts the run!\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Installing Python Package Dependencies with Hash Verification\nDESCRIPTION: This snippet shows how Python package dependencies are specified with their versions and SHA256 hash values for security verification. The file includes dependencies like widgetsnbextension, wrapt, xformers, and xgrammar with their specific versions and hash values.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_46\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:c24ba103ecf45861e2e1f933d40b2d93f5d52d8228870c3e7bf1299cd1cb8ff1 \\\n    --hash=sha256:c348abc5924caa02a62896300e32ea80a81521f91d6db2e853e6b1994017c9f6 \\\n    --hash=sha256:c53f97032b87a406044a1c33d1e9290cc38b117a8062e8a8b285175d7e2f99c9 \\\n    --hash=sha256:c7cd4b1015d2f60dfe539ee6c95bc968d5d5fad92ab01bb5501a77393da4f596 \\\n    --hash=sha256:c86dc2068f1c5ca2065aca34f257bbf4f78caf566eb230f692ad347da191f0a1 \\\n    --hash=sha256:c8c5c8e1bac05ef3c23722e591ef4f688f528235e2480f157a9cfe0a19081375 \\\n    --hash=sha256:ca36151289a15b39d8d683fd8b7abbe26fc50be311066c5f8dcf3cb8cee107ab \\\n    --hash=sha256:cc8821a03bcfb36e4e4705316f6b66af28450357af8a575dc8f4b09bf02a3dee \\\n    --hash=sha256:cccc18077acd34c8072578394ec79563664b1c205f7a86a62e94fafc7b59001f \\\n    --hash=sha256:d2244d8ab24374bed366f9ff206e2619345f9cd7fe79aad5225f53faac28b6b1 \\\n    --hash=sha256:d4c22992e24f12de340ca5f824121a5b3e1a37ad4360b4e1aaf15e9d1c42582d \\\n    --hash=sha256:dd24c4d256558429aeeb8d6c24ebad4e982ac52c50bc3670ae8646c181263965 \\\n    --hash=sha256:e413352a921f5ad5d66f9e2869b977e88d5103fc528b6deb8423028a2befd842 \\\n    --hash=sha256:ee06405ea2e67366a661ed313e14cf2a86e84142a3462852eb96348f7219cee3 \\\n    --hash=sha256:f83eca8cbfd168e424dfa3b3b5c955d6c281e8fc09feb9d870886ff8d03683c7 \\\n    --hash=sha256:fb915101dfbf318486364ce85662bb7b020840f68138014972c08331458d41f3 \\\n    --hash=sha256:ffc02b159b65c05f2ed9ec176b715b66918a674bd4daed48a9a7a590dd4be1aa \\\n    --hash=sha256:ffc5ae23ada6515f31604f700009e2df90b091b67d463a8401c1d8a37f76c1d7\n    # via uvicorn\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Serializers for Existing Classes\nDESCRIPTION: Shows how to register custom serializers and deserializers for classes that cannot be modified directly. This example demonstrates serializing a class with an unserializable threading.Lock attribute.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/serialization.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport threading\n\nclass A:\n    def __init__(self, x):\n        self.x = x\n        self.lock = threading.Lock()  # could not be serialized!\n\ntry:\n  ray.get(ray.put(A(1)))  # fail!\nexcept TypeError:\n  pass\n\ndef custom_serializer(a):\n    return a.x\n\ndef custom_deserializer(b):\n    return A(b)\n\n# Register serializer and deserializer for class A:\nray.util.register_serializer(\n  A, serializer=custom_serializer, deserializer=custom_deserializer)\nray.get(ray.put(A(1)))  # success!\n\n# You can deregister the serializer at any time.\nray.util.deregister_serializer(A)\ntry:\n  ray.get(ray.put(A(1)))  # fail!\nexcept TypeError:\n  pass\n\n# Nothing happens when deregister an unavailable serializer.\nray.util.deregister_serializer(A)\n```\n\n----------------------------------------\n\nTITLE: Package Hash Verification for Python Dependencies in Ray Project\nDESCRIPTION: This snippet contains SHA-256 hash verification values for Python package dependencies used in the Ray project. Each line includes a hash value preceded by '--hash=sha256:' for verifying package integrity during installation. The file includes a comment indicating these dependencies are referenced in the requirements_compiled_rayllm_test_py311_cu124.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_41\n\nLANGUAGE: plain text\nCODE:\n```\n--hash=sha256:88a19f62ff30117e706ebc9090b8ecc79aeb77d0b1f5ec10d2d27a12bc9f66d0 \\\n--hash=sha256:8d39d351e7faf01483cc7ff7c0213c412e38e5a340238826be7e0e4da450fdc8 \\\n--hash=sha256:90adb47ad432332d4f0bc28f83a5963f426ce9a1a8809f5e584e704b82685dcb \\\n--hash=sha256:913829534200eb0f789d45349e55203a091f45c37a2674678744ae52fae23efa \\\n--hash=sha256:93b2e109287f93db79210f86deb6b9bbb81ac32fc97236b16f7433db7fc437d8 \\\n--hash=sha256:9d41beda9dc97ca9ab0b9888cb71f7539124bc05df02c0cff6e5acc5a19dcc6e \\\n--hash=sha256:a440a2a624683108a1b454705ecd7afc1c3438a08e890a1513d468671d90a04e \\\n--hash=sha256:a4bb030cf46a434ec0225bddbebd4b89e6471814ca851abb8696170adb163985 \\\n--hash=sha256:a9ca04806f3be0ac6d558fffc2fdf8fcef767e0489d2684a21912cc4ed0cd1b8 \\\n--hash=sha256:ac1801c45cbf77b6c99242eeff4fffb5e4e73a800b5c4ad4fc0be5def634d2e1 \\\n--hash=sha256:ac36703a585e0929b032fbaab0707b75dc12703766d0b53486eabd5139ebadd5 \\\n--hash=sha256:b1771de9944d875f1b98a745bc547e684b863abf8f8287da8466cf470ef52690 \\\n--hash=sha256:b464c4ab4bfcb41e3bfd3f1c26600d038376c2de3297760dfe064d2cb7ea8e10 \\\n--hash=sha256:b4f6450109834af88cb4cc5ecddfc5380ebb9c228695afc11915a0bf82116789 \\\n--hash=sha256:b57f4f58099328dfb26c6a771d09fb20dbbae81d20cfb66141251ea063bd101b \\\n--hash=sha256:b643562c12680b01e17239be267bc306bbc6aac1f34f6444d1bded0c5ce438ca \\\n--hash=sha256:b958ddd075ddba5b09bb0be8a6d9906d2ce933aee81100db289badbeb966f54e \\\n--hash=sha256:b9d60031cf568c627d028239693fd718025719c02c9f55df0a53e587aab951b5 \\\n--hash=sha256:ba23302c0c61a9999784e73809427c9dbedd79f66a13d84ad1b1943802eaaf59 \\\n--hash=sha256:ba87babd629f8af77f557b61e49e7c7cac36f22f871156b91e10a6e9d4f829e9 \\\n--hash=sha256:c017a3b6df3a1bd45b9fa49a0f54005e53fbcad16633870104b66fa1a30a29d8 \\\n--hash=sha256:c1e1cc06da1491e6734f0ea1e6294ce00792193c463350626571c287c9a704db \\\n--hash=sha256:c654d5207c78e0bd6d749f6dae1dcbbfde3403ad3a4b11f3c5544d9906969dde \\\n--hash=sha256:c69697d3adff5aa4f874b19c0e4ed65180ceed6318ec856ebc423aa5850d84f7 \\\n--hash=sha256:c7d79f7d9aabd6011004e33b22bc13056a3e3fb54794d138af57f5ee9d9032cb \\\n--hash=sha256:ccaa3a4b521b780a7e771cc336a2dba389a0861592bbce09a476190bb0c8b4b3 \\\n--hash=sha256:ccd17349166b1bee6e529b4add61727d3f55edb7babbe4069b5764c9587a8cc6 \\\n--hash=sha256:ce1af883b94304f493698b00d0f006d56aea98aeb49d75ec7d98cd4a777e9285 \\\n--hash=sha256:d0e883008013c0e4aef84dcfe2a0b172c4d23c2669412cf5b3371003941f72bb \\\n--hash=sha256:d980e0325b6eddc81331d3f4551e2a333999fb176fd153e075c6d1c2530aa8a8 \\\n--hash=sha256:e17c9361d46a4d5addf777c6dd5eab0715a7684c2f11b88c67ac37edfba6c482 \\\n--hash=sha256:e2c08cc9b16f4f4bc522771d96734c7901e7ebef70c6c5c35dd0f10845270bcd \\\n--hash=sha256:e35ef8683211db69ffe129a25d5634319a677570ab6b2eba4afa860f54eeaf75 \\\n--hash=sha256:e3b9fd71836999aad54084906f8663dffcd2a7fb5cdafd6c37713b2e72be1760 \\\n--hash=sha256:ef9f7768395923c3039055c14334ba4d926f3baf7b776c923c93d80195624782 \\\n--hash=sha256:f52a265001d830bc425f82ca9eabda94a64a4d753b07d623a9f2863fde532b53 \\\n--hash=sha256:f91c4803173928a25e1a55b943c81f55b8872f0018be83e3ad4938adffb77dd2 \\\n--hash=sha256:fbd6748e8ab9b41171bb95c6142faf068f5ef1511935a0aa07025438dd9a9bc1 \\\n--hash=sha256:fe57328fbc1bfd0bd0514470ac692630f3901c0ee39052ae47acd1d90a436719 \\\n--hash=sha256:fea09ca13323376a2fdfb353a5fa2e59f90cd18d7ca4eaa1fd31f0a8b4f91e62\n# via\n#   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n#   aiohttp\n```\n\n----------------------------------------\n\nTITLE: Installing Ray Package\nDESCRIPTION: Command to install Ray using pip package manager\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install ray\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution Stack Trace with JavaScript Function Call\nDESCRIPTION: This stack trace shows the execution path leading to a JavaScript function call within the Vert.x application. It demonstrates how Java code interacts with JavaScript functions through the Mozilla JavaScript engine.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_84\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j]\n```\n\n----------------------------------------\n\nTITLE: Printing best hyperparameters\nDESCRIPTION: This snippet prints the best hyperparameters found during the Tune experiment, retrieved from the results object using results.get_best_result().config.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n```\n\n----------------------------------------\n\nTITLE: Ray Cluster Management Commands\nDESCRIPTION: Shell commands for managing Ray clusters and executing applications.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/getting-started.rst#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ ray up -y config.yaml\n$ ray exec config.yaml 'python -c \"import ray; ray.init()\"'\n```\n\n----------------------------------------\n\nTITLE: Retrieving RayCluster Status\nDESCRIPTION: Gets the status of deployed RayClusters\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nkubectl get rayclusters\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with SHA256 Hashes\nDESCRIPTION: Detailed requirements file listing Python packages with their versions and SHA256 hashes for security verification. Includes dependencies like gymnasium, h11, idna, imageio, jinja2, and other packages with their respective hash values.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:7b2c86457145ce14c38e5bf6bdc19ef88e66c5fee2c3d83285c5aef026ba93b3 \\\n--hash=sha256:7d69ce1f324dc2d71e40c9261d3fdbe7d4c9d60f332069ff9b2a4d8a257c7b2b \\\n--hash=sha256:802d84fd3d50614170649853d121baaaa305de7b65b3e01759247e768d691ddf \\\n# via\n#   -c python/requirements_compiled_ray_test_py311_cpu.txt\n#   -r python/requirements.txt\n#   opentelemetry-exporter-otlp-proto-grpc\n```\n\n----------------------------------------\n\nTITLE: Monitoring Job Progress and Cluster Status\nDESCRIPTION: Shell commands to monitor the job's progress, observe pod status, and check Ray cluster status using kubectl and Ray CLI tools.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/ml-example.md#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# Substitute the Ray Job's submission id.\nray job logs 'raysubmit_xxxxxxxxxxxxxxxx' --follow --address http://127.0.0.1:8265\n\n# If you're on MacOS, first `brew install watch`.\nwatch -n 1 kubectl get pod\n\n# Substitute the name of your Ray cluster's head pod.\nwatch -n 1 kubectl exec -it raycluster-xgboost-benchmark-head-xxxxx -- ray status\n```\n\n----------------------------------------\n\nTITLE: Specifying pyarrow Package Version with Hash Verification\nDESCRIPTION: This snippet defines the pyarrow package requirement with version 14.0.2 along with multiple SHA-256 hash values for verification during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_22\n\nLANGUAGE: text\nCODE:\n```\npyarrow==14.0.2 \\\n    --hash=sha256:059bd8f12a70519e46cd64e1ba40e97eae55e0cbe1695edd95384653d7626b23 \\\n    --hash=sha256:06ff1264fe4448e8d02073f5ce45a9f934c0f3db0a04460d0b01ff28befc3696 \\\n    --hash=sha256:1e6987c5274fb87d66bb36816afb6f65707546b3c45c44c28e3c4133c010a881 \\\n    --hash=sha256:209bac546942b0d8edc8debda248364f7f668e4aad4741bae58e67d40e5fcf75 \\\n    --hash=sha256:20e003a23a13da963f43e2b432483fdd8c38dc8882cd145f09f21792e1cf22a1 \\\n    --hash=sha256:22a768987a16bb46220cef490c56c671993fbee8fd0475febac0b3e16b00a10e \\\n    --hash=sha256:2cc61593c8e66194c7cdfae594503e91b926a228fba40b5cf25cc593563bcd07 \\\n    --hash=sha256:2dbba05e98f247f17e64303eb876f4a80fcd32f73c7e9ad975a83834d81f3fda \\\n    --hash=sha256:32356bfb58b36059773f49e4e214996888eeea3a08893e7dbde44753799b2a02 \\\n    --hash=sha256:36cef6ba12b499d864d1def3e990f97949e0b79400d08b7cf74504ffbd3eb025 \\\n    --hash=sha256:37c233ddbce0c67a76c0985612fef27c0c92aef9413cf5aa56952f359fcb7379 \\\n    --hash=sha256:3c0fa3bfdb0305ffe09810f9d3e2e50a2787e3a07063001dcd7adae0cee3601a \\\n    --hash=sha256:3f16111f9ab27e60b391c5f6d197510e3ad6654e73857b4e394861fc79c37200 \\\n    --hash=sha256:52809ee69d4dbf2241c0e4366d949ba035cbcf48409bf404f071f624ed313a2b \\\n    --hash=sha256:5c1da70d668af5620b8ba0a23f229030a4cd6c5f24a616a146f30d2386fec422 \\\n    --hash=sha256:63ac901baec9369d6aae1cbe6cca11178fb018a8d45068aaf5bb54f94804a866 \\\n    --hash=sha256:64df2bf1ef2ef14cee531e2dfe03dd924017650ffaa6f9513d7a1bb291e59c15 \\\n    --hash=sha256:66e986dc859712acb0bd45601229021f3ffcdfc49044b64c6d071aaf4fa49e98 \\\n    --hash=sha256:6dd4f4b472ccf4042f1eab77e6c8bce574543f54d2135c7e396f413046397d5a \\\n    --hash=sha256:75ee0efe7a87a687ae303d63037d08a48ef9ea0127064df18267252cfe2e9541 \\\n    --hash=sha256:76fc257559404ea5f1306ea9a3ff0541bf996ff3f7b9209fc517b5e83811fa8e \\\n    --hash=sha256:78ea56f62fb7c0ae8ecb9afdd7893e3a7dbeb0b04106f5c08dbb23f9c0157591 \\\n    --hash=sha256:87482af32e5a0c0cce2d12eb3c039dd1d853bd905b04f3f953f147c7a196915b \\\n    --hash=sha256:87e879323f256cb04267bb365add7208f302df942eb943c93a9dfeb8f44840b1 \\\n    --hash=sha256:a01d0052d2a294a5f56cc1862933014e696aa08cc7b620e8c0cce5a5d362e976 \\\n    --hash=sha256:a25eb2421a58e861f6ca91f43339d215476f4fe159eca603c55950c14f378cc5 \\\n    --hash=sha256:a51fee3a7db4d37f8cda3ea96f32530620d43b0489d169b285d774da48ca9785 \\\n    --hash=sha256:a898d134d00b1eca04998e9d286e19653f9d0fcb99587310cd10270907452a6b \\\n    --hash=sha256:b0c4a18e00f3a32398a7f31da47fefcd7a927545b396e1f15d0c85c2f2c778cd \\\n    --hash=sha256:ba9fe808596c5dbd08b3aeffe901e5f81095baaa28e7d5118e01354c64f22807 \\\n    --hash=sha256:c65bf4fd06584f058420238bc47a316e80dda01ec0dfb3044594128a6c2db794 \\\n    --hash=sha256:c87824a5ac52be210d32906c715f4ed7053d0180c1060ae3ff9b7e560f53f944 \\\n    --hash=sha256:e354fba8490de258be7687f341bc04aba181fc8aa1f71e4584f9890d9cb2dec2 \\\n    --hash=sha256:e4b123ad0f6add92de898214d404e488167b87b5dd86e9a434126bc2b7a5578d \\\n    --hash=sha256:f7d029f20ef56673a9730766023459ece397a05001f4e4d13805111d7c2108c0 \\\n    --hash=sha256:fc0de7575e841f1595ac07e5bc631084fd06ca8b03c0f2ecece733d23cd5102a\n    # via\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Ray Workflows API Documentation in RST\nDESCRIPTION: This snippet defines a table of contents for Ray Workflows API documentation using Sphinx's toctree directive. It includes links to execution.rst and management.rst files with a maxdepth of 1.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/api/api.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 1\n\n    execution.rst\n    management.rst\n```\n\n----------------------------------------\n\nTITLE: Accessing Ray Cluster Logs using CLI\nDESCRIPTION: Command to access logs from a Ray cluster. Only logs from alive nodes are available through this API. Requires Ray installation with 'default' extras.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/reference/cli.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nray logs\n```\n\n----------------------------------------\n\nTITLE: Accessing Shell on Docker Head Node\nDESCRIPTION: Command to get a shell on the head node of the Docker-based fake multinode cluster for direct interaction and debugging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/fake-autoscaler.rst#2025-04-12_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ docker exec -it fake_docker_fffffffffffffffffffffffffffffffffffffffffffffffffff00000_1 bash\n```\n\n----------------------------------------\n\nTITLE: Sending Updated Client Request\nDESCRIPTION: Python code for making a client request after configuration update to verify language change\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/inplace-updates.md#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Python client request code snippet after update\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Service Discovery\nDESCRIPTION: Command to list available Kubernetes services.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/stable-diffusion-rayservice.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get services\n```\n\n----------------------------------------\n\nTITLE: Including External Python File for Ax Integration with Ray Tune\nDESCRIPTION: This snippet demonstrates how to include an external Python file in reStructuredText documentation. It references the Ax example file from the Ray Tune examples directory that shows how to use Ray Tune with Facebook's Ax platform.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/ax_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n:orphan:\n\nAX Example\n~~~~~~~~~~\n\n.. literalinclude:: /../../python/ray/tune/examples/ax_example.py\n```\n\n----------------------------------------\n\nTITLE: Setting Configuration Variables\nDESCRIPTION: Configures utility variables for controlling output and defining test scope.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# For benchmarking purposes, we can print the times of various operations.\n# In order to reduce clutter in the output, this is set to False by default.\nPRINT_TIMES = False\n\n\ndef print_time(msg: str):\n    if PRINT_TIMES:\n        print(msg)\n\n\n# To speed things up, we'll only use a small subset of the full dataset consisting of two last months of 2019.\n# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\nSMOKE_TEST = True\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependency Specification with Hashes for yarl in Ray Project\nDESCRIPTION: This section specifies the yarl package at version 1.18.3 with comprehensive SHA-256 hashes for verification. The yarl package is an asynchronous URL toolkit commonly used with aiohttp and related packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_52\n\nLANGUAGE: pip\nCODE:\n```\nyarl==1.18.3 \\\n    --hash=sha256:00e5a1fea0fd4f5bfa7440a47eff01d9822a65b4488f7cff83155a0f31a2ecba \\\n    --hash=sha256:02ddb6756f8f4517a2d5e99d8b2f272488e18dd0bfbc802f31c16c6c20f22193 \\\n    --hash=sha256:045b8482ce9483ada4f3f23b3774f4e1bf4f23a2d5c912ed5170f68efb053318 \\\n    --hash=sha256:09c7907c8548bcd6ab860e5f513e727c53b4a714f459b084f6580b49fa1b9cee \\\n    --hash=sha256:0b0cad37311123211dc91eadcb322ef4d4a66008d3e1bdc404808992260e1a0e \\\n    --hash=sha256:0b3c92fa08759dbf12b3a59579a4096ba9af8dd344d9a813fc7f5070d86bbab1 \\\n    --hash=sha256:0fb2171a4486bb075316ee754c6d8382ea6eb8b399d4ec62fde2b591f879778a \\\n    --hash=sha256:1a74a13a4c857a84a845505fd2d68e54826a2cd01935a96efb1e9d86c728e186 \\\n    --hash=sha256:1d407181cfa6e70077df3377938c08012d18893f9f20e92f7d2f314a437c30b1 \\\n    --hash=sha256:1dd4bdd05407ced96fed3d7f25dbbf88d2ffb045a0db60dbc247f5b3c5c25d50 \\\n    --hash=sha256:25b411eddcfd56a2f0cd6a384e9f4f7aa3efee14b188de13048c25b5e91f1640 \\\n    --hash=sha256:2d06d3005e668744e11ed80812e61efd77d70bb7f03e33c1598c301eea20efbb \\\n    --hash=sha256:2ec9bbba33b2d00999af4631a3397d1fd78290c48e2a3e52d8dd72db3a067ac8 \\\n    --hash=sha256:3236da9272872443f81fedc389bace88408f64f89f75d1bdb2256069a8730ccc \\\n    --hash=sha256:35098b24e0327fc4ebdc8ffe336cee0a87a700c24ffed13161af80124b7dc8e5 \\\n    --hash=sha256:41f7ce59d6ee7741af71d82020346af364949314ed3d87553763a2df1829cc58 \\\n    --hash=sha256:436c4fc0a4d66b2badc6c5fc5ef4e47bb10e4fd9bf0c79524ac719a01f3607c2 \\\n    --hash=sha256:4891ed92157e5430874dad17b15eb1fda57627710756c27422200c52d8a4e393 \\\n    --hash=sha256:4ac515b860c36becb81bb84b667466885096b5fc85596948548b667da3bf9f24 \\\n    --hash=sha256:5094d9206c64181d0f6e76ebd8fb2f8fe274950a63890ee9e0ebfd58bf9d787b \\\n    --hash=sha256:54d6921f07555713b9300bee9c50fb46e57e2e639027089b1d795ecd9f7fa910 \\\n    --hash=sha256:578e281c393af575879990861823ef19d66e2b1d0098414855dd367e234f5b3c \\\n    --hash=sha256:5a3f356548e34a70b0172d8890006c37be92995f62d95a07b4a42e90fba54272 \\\n    --hash=sha256:602d98f2c2d929f8e697ed274fbadc09902c4025c5a9963bf4e9edfc3ab6f7ed \\\n    --hash=sha256:61b1a825a13bef4a5f10b1885245377d3cd0bf87cba068e1d9a88c2ae36880e1 \\\n    --hash=sha256:61e5e68cb65ac8f547f6b5ef933f510134a6bf31bb178be428994b0cb46c2a04 \\\n    --hash=sha256:61ee62ead9b68b9123ec24bc866cbef297dd266175d53296e2db5e7f797f902d\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how Python package dependencies are specified with their versions and SHA256 hash values for security and reproducibility. It includes comments indicating the source of the requirements.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_18\n\nLANGUAGE: Plain Text\nCODE:\n```\npygments==2.18.0 \\\n    --hash=sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199 \\\n    --hash=sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ipython\n    #   nbconvert\n    #   rich\n```\n\n----------------------------------------\n\nTITLE: Package Hash Verification for Zipp Python Package\nDESCRIPTION: This snippet specifies SHA-256 hash verification values for the zipp package version 3.21.0. It includes two hash values for package integrity verification and indicates that this requirement comes from the requirements_compiled_rayllm_test_py311_cu124.txt file and is used by importlib-metadata.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_42\n\nLANGUAGE: plain text\nCODE:\n```\nzipp==3.21.0 \\\n--hash=sha256:2c9958f6430a2040341a52eb608ed6dd93ef4392e02ffe219417c1b28b5dd1f4 \\\n--hash=sha256:ac1bbe05fd2991f160ebce24ffbac5f6d11d83dc90891255885223d42b3cd931\n# via\n#   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n#   importlib-metadata\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with ByteBuf Release\nDESCRIPTION: This stack trace shows Netty's event processing flow including buffer release operations. The trace demonstrates memory management in Netty with reference counted ByteBuf objects being properly released after processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_43\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/buffer/AbstractReferenceCountedByteBuf:.release_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Java Netty NIO Event Processing with JavaScript Wrap Method Stack Trace\nDESCRIPTION: A variation of the stack trace showing the processing path through Netty and Vertx with a different JavaScript method call. This trace involves the JavaScript wrap method instead of wrapAsJavaObject.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_51\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/WrapFactory:.wrap_[j];java/util/HashMap:.get_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Specifying PyArrow Package Dependency with Hash Verification\nDESCRIPTION: Defines the pyarrow package dependency at version 14.0.2 with SHA256 hash verification. This package is listed in the main requirements.txt file and provides Arrow functionality for efficient columnar data processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_24\n\nLANGUAGE: plaintext\nCODE:\n```\npyarrow==14.0.2 \\\n    --hash=sha256:059bd8f12a70519e46cd64e1ba40e97eae55e0cbe1695edd95384653d7626b23 \\\n    --hash=sha256:06ff1264fe4448e8d02073f5ce45a9f934c0f3db0a04460d0b01ff28befc3696 \\\n    --hash=sha256:1e6987c5274fb87d66bb36816afb6f65707546b3c45c44c28e3c4133c010a881 \\\n    --hash=sha256:209bac546942b0d8edc8debda248364f7f668e4aad4741bae58e67d40e5fcf75 \\\n    --hash=sha256:20e003a23a13da963f43e2b432483fdd8c38dc8882cd145f09f21792e1cf22a1 \\\n    --hash=sha256:22a768987a16bb46220cef490c56c671993fbee8fd0475febac0b3e16b00a10e \\\n    --hash=sha256:2cc61593c8e66194c7cdfae594503e91b926a228fba40b5cf25cc593563bcd07 \\\n    --hash=sha256:2dbba05e98f247f17e64303eb876f4a80fcd32f73c7e9ad975a83834d81f3fda \\\n    --hash=sha256:32356bfb58b36059773f49e4e214996888eeea3a08893e7dbde44753799b2a02 \\\n    --hash=sha256:36cef6ba12b499d864d1def3e990f97949e0b79400d08b7cf74504ffbd3eb025 \\\n    --hash=sha256:37c233ddbce0c67a76c0985612fef27c0c92aef9413cf5aa56952f359fcb7379 \\\n    --hash=sha256:3c0fa3bfdb0305ffe09810f9d3e2e50a2787e3a07063001dcd7adae0cee3601a \\\n    --hash=sha256:3f16111f9ab27e60b391c5f6d197510e3ad6654e73857b4e394861fc79c37200 \\\n    --hash=sha256:52809ee69d4dbf2241c0e4366d949ba035cbcf48409bf404f071f624ed313a2b \\\n    --hash=sha256:5c1da70d668af5620b8ba0a23f229030a4cd6c5f24a616a146f30d2386fec422 \\\n    --hash=sha256:63ac901baec9369d6aae1cbe6cca11178fb018a8d45068aaf5bb54f94804a866 \\\n    --hash=sha256:64df2bf1ef2ef14cee531e2dfe03dd924017650ffaa6f9513d7a1bb291e59c15 \\\n    --hash=sha256:66e986dc859712acb0bd45601229021f3ffcdfc49044b64c6d071aaf4fa49e98 \\\n    --hash=sha256:6dd4f4b472ccf4042f1eab77e6c8bce574543f54d2135c7e396f413046397d5a \\\n    --hash=sha256:75ee0efe7a87a687ae303d63037d08a48ef9ea0127064df18267252cfe2e9541 \\\n    --hash=sha256:76fc257559404ea5f1306ea9a3ff0541bf996ff3f7b9209fc517b5e83811fa8e \\\n    --hash=sha256:78ea56f62fb7c0ae8ecb9afdd7893e3a7dbeb0b04106f5c08dbb23f9c0157591 \\\n    --hash=sha256:87482af32e5a0c0cce2d12eb3c039dd1d853bd905b04f3f953f147c7a196915b \\\n    --hash=sha256:87e879323f256cb04267bb365add7208f302df942eb943c93a9dfeb8f44840b1 \\\n    --hash=sha256:a01d0052d2a294a5f56cc1862933014e696aa08cc7b620e8c0cce5a5d362e976 \\\n    --hash=sha256:a25eb2421a58e861f6ca91f43339d215476f4fe159eca603c55950c14f378cc5 \\\n    --hash=sha256:a51fee3a7db4d37f8cda3ea96f32530620d43b0489d169b285d774da48ca9785 \\\n    --hash=sha256:a898d134d00b1eca04998e9d286e19653f9d0fcb99587310cd10270907452a6b \\\n    --hash=sha256:b0c4a18e00f3a32398a7f31da47fefcd7a927545b396e1f15d0c85c2f2c778cd \\\n    --hash=sha256:ba9fe808596c5dbd08b3aeffe901e5f81095baaa28e7d5118e01354c64f22807 \\\n    --hash=sha256:c65bf4fd06584f058420238bc47a316e80dda01ec0dfb3044594128a6c2db794 \\\n    --hash=sha256:c87824a5ac52be210d32906c715f4ed7053d0180c1060ae3ff9b7e560f53f944 \\\n    --hash=sha256:e354fba8490de258be7687f341bc04aba181fc8aa1f71e4584f9890d9cb2dec2 \\\n    --hash=sha256:e4b123ad0f6add92de898214d404e488167b87b5dd86e9a434126bc2b7a5578d \\\n    --hash=sha256:f7d029f20ef56673a9730766023459ece397a05001f4e4d13805111d7c2108c0 \\\n    --hash=sha256:fc0de7575e841f1595ac07e5bc631084fd06ca8b03c0f2ecece733d23cd5102a\n```\n\n----------------------------------------\n\nTITLE: Listing PyParsing Package with Hash Values\nDESCRIPTION: Definition for the PyParsing package dependency with version 3.1.1 and corresponding SHA256 hash values. The comment indicates this is required by httplib2 and matplotlib packages through the compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_24\n\nLANGUAGE: text\nCODE:\n```\npyparsing==3.1.1 \\\n    --hash=sha256:32c7c0b711493c72ff18a981d24f28aaf9c1fb7ed5e9667c9e84e3db623bdbfb \\\n    --hash=sha256:ede28a1a32462f5a9705e07aea48001a08f7cf81a021585011deba701581a0db\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   httplib2\n    #   matplotlib\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: A requirements specification listing Python package dependencies with their exact versions and SHA256 hash values for verification. These hashes ensure package integrity during installation and prevent supply chain attacks.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_12\n\nLANGUAGE: text\nCODE:\n```\njiter==0.8.2 \\\n    --hash=sha256:025337859077b41548bdcbabe38698bcd93cfe10b06ff66617a48ff92c9aec60 \\\n    --hash=sha256:03c9df035d4f8d647f8c210ddc2ae0728387275340668fb30d2421e17d9a0841 \\\n    --hash=sha256:08d4c92bf480e19fc3f2717c9ce2aa31dceaa9163839a311424b6862252c943e \\\n    --hash=sha256:0cf5dfa9956d96ff2efb0f8e9c7d055904012c952539a774305aaaf3abdf3d6c \\\n    --hash=sha256:14601dcac4889e0a1c75ccf6a0e4baf70dbc75041e51bcf8d0e9274519df6887 \\\n    --hash=sha256:180a8aea058f7535d1c84183c0362c710f4750bef66630c05f40c93c2b152a0f \\\n    --hash=sha256:1c0dfbd1be3cbefc7510102370d86e35d1d53e5a93d48519688b1bf0f761160a \\\n    --hash=sha256:2dd61c5afc88a4fda7d8b2cf03ae5947c6ac7516d32b7a15bf4b49569a5c076b \\\n    --hash=sha256:317b25e98a35ffec5c67efe56a4e9970852632c810d35b34ecdd70cc0e47b3b6 \\\n    --hash=sha256:32475a42b2ea7b344069dc1e81445cfc00b9d0e3ca837f0523072432332e9f74 \\\n    --hash=sha256:37b2998606d6dadbb5ccda959a33d6a5e853252d921fec1792fc902351bb4e2c \\\n    --hash=sha256:3ac9f578c46f22405ff7f8b1f5848fb753cc4b8377fbec8470a7dc3997ca7566 \\\n    --hash=sha256:3b94a33a241bee9e34b8481cdcaa3d5c2116f575e0226e421bed3f7a6ea71cff \\\n    --hash=sha256:4a9220497ca0cb1fe94e3f334f65b9b5102a0b8147646118f020d8ce1de70105 \\\n    --hash=sha256:4ab9a87f3784eb0e098f84a32670cfe4a79cb6512fd8f42ae3d0709f06405d18 \\\n    --hash=sha256:5127dc1abd809431172bc3fbe8168d6b90556a30bb10acd5ded41c3cfd6f43b6 \\\n    --hash=sha256:5672a86d55416ccd214c778efccf3266b84f87b89063b582167d803246354be4 \\\n    --hash=sha256:580ccf358539153db147e40751a0b41688a5ceb275e6f3e93d91c9467f42b2e3 \\\n    --hash=sha256:58dc9bc9767a1101f4e5e22db1b652161a225874d66f0e5cb8e2c7d1c438b587 \\\n    --hash=sha256:5a90a923338531b7970abb063cfc087eebae6ef8ec8139762007188f6bc69a9f \\\n    --hash=sha256:653cf462db4e8c41995e33d865965e79641ef45369d8a11f54cd30888b7e6ff1 \\\n    --hash=sha256:66227a2c7b575720c1871c8800d3a0122bb8ee94edb43a5685aa9aceb2782d44 \\\n    --hash=sha256:6e5337bf454abddd91bd048ce0dca5134056fc99ca0205258766db35d0a2ea43 \\\n    --hash=sha256:70bf4c43652cc294040dbb62256c83c8718370c8b93dd93d934b9a7bf6c4f53c \\\n    --hash=sha256:711e408732d4e9a0208008e5892c2966b485c783cd2d9a681f3eb147cf36c7ef \\\n    --hash=sha256:76e324da7b5da060287c54f2fabd3db5f76468006c811831f051942bf68c9d44 \\\n    --hash=sha256:789361ed945d8d42850f919342a8665d2dc79e7e44ca1c97cc786966a21f627a \\\n    --hash=sha256:79aec8172b9e3c6d05fd4b219d5de1ac616bd8da934107325a6c0d0e866a21b6 \\\n    --hash=sha256:7efe4853ecd3d6110301665a5178b9856be7e2a9485f49d91aa4d737ad2ae49e \\\n    --hash=sha256:7f22b16b35d5c1df9dfd58843ab2cd25e6bf15191f5a236bed177afade507bfc \\\n    --hash=sha256:83c0efd80b29695058d0fd2fa8a556490dbce9804eac3e281f373bbc99045f6c \\\n    --hash=sha256:859e8eb3507894093d01929e12e267f83b1d5f6221099d3ec976f0c995cb6bd9 \\\n    --hash=sha256:8b9931fd36ee513c26b5bf08c940b0ac875de175341cbdd4fa3be109f0492586 \\\n    --hash=sha256:8bd2a824d08d8977bb2794ea2682f898ad3d8837932e3a74937e93d62ecbb637 \\\n    --hash=sha256:8f2d5ed877f089862f4c7aacf3a542627c1496f972a34d0474ce85ee7d939c27 \\\n    --hash=sha256:8ffc86ae5e3e6a93765d49d1ab47b6075a9c978a2b3b80f0f32628f39caa0c88 \\\n    --hash=sha256:92249669925bc1c54fcd2ec73f70f2c1d6a817928480ee1c65af5f6b81cdf12d \\\n    --hash=sha256:99d9a1eded738299ba8e106c6779ce5c3893cffa0e32e4485d680588adae6db8 \\\n    --hash=sha256:9c63eaef32b7bebac8ebebf4dabebdbc6769a09c127294db6babee38e9f405b9 \\\n    --hash=sha256:9e1fa156ee9454642adb7e7234a383884452532bc9d53d5af2d18d98ada1d79c \\\n    --hash=sha256:a2ecaa3c23e7a7cf86d00eda3390c232f4d533cd9ddea4b04f5d0644faf642c5 \\\n    --hash=sha256:a6c710d657c8d1d2adbbb5c0b0c6bfcec28fd35bd6b5f016395f9ac43e878a15 \\\n    --hash=sha256:a9584de0cd306072635fe4b89742bf26feae858a0683b399ad0c2509011b9dc0 \\\n    --hash=sha256:ab7f43235d71e03b941c1630f4b6e3055d46b6cb8728a17663eaac9d8e83a865 \\\n    --hash=sha256:af102d3372e917cffce49b521e4c32c497515119dc7bd8a75665e90a718bbf08 \\\n    --hash=sha256:b25bd626bde7fb51534190c7e3cb97cee89ee76b76d7585580e22f34f5e3f393 \\\n    --hash=sha256:b2dd880785088ff2ad21ffee205e58a8c1ddabc63612444ae41e5e4b321b39c0 \\\n    --hash=sha256:b426f72cd77da3fec300ed3bc990895e2dd6b49e3bfe6c438592a3ba660e41ca \\\n    --hash=sha256:ba5bdf56969cad2019d4e8ffd3f879b5fdc792624129741d3d83fc832fef8c7d \\\n    --hash=sha256:bf55846c7b7a680eebaf9c3c48d630e1bf51bdf76c68a5f654b8524335b0ad29 \\\n    --hash=sha256:ca1f08b8e43dc3bd0594c992fb1fd2f7ce87f7bf0d44358198d6da8034afdf84 \\\n    --hash=sha256:ca29b6371ebc40e496995c94b988a101b9fbbed48a51190a4461fcb0a68b4a36 \\\n    --hash=sha256:ca8577f6a413abe29b079bc30f907894d7eb07a865c4df69475e868d73e71c7b \\\n    --hash=sha256:cadcc978f82397d515bb2683fc0d50103acff2a180552654bb92d6045dec2c49 \\\n    --hash=sha256:cd646c827b4f85ef4a78e4e58f4f5854fae0caf3db91b59f0d73731448a970c6 \\\n    --hash=sha256:cd73d3e740666d0e639f678adb176fad25c1bcbdae88d8d7b857e1783bb4212d \\\n    --hash=sha256:cde031d8413842a1e7501e9129b8e676e62a657f8ec8166e18a70d94d4682855 \\\n    --hash=sha256:ce0820f4a3a59ddced7fce696d86a096d5cc48d32a4183483a17671a61edfddc \\\n    --hash=sha256:d20be8b7f606df096e08b0b1b4a3c6f0515e8dac296881fe7461dfa0fb5ec817 \\\n    --hash=sha256:d21974d246ed0181558087cd9f76e84e8321091ebfb3a93d4c341479a736f099 \\\n    --hash=sha256:d33f94615fcaf872f7fd8cd98ac3b429e435c77619777e8a449d9d27e01134d1 \\\n    --hash=sha256:d35c864c2dff13dfd79fb070fc4fc6235d7b9b359efe340e1261deb21b9fcb66 \\\n    --hash=sha256:d5c826a221851a8dc028eb6d7d6429ba03184fa3c7e83ae01cd6d3bd1d4bd17d \\\n    --hash=sha256:e41e75344acef3fc59ba4765df29f107f309ca9e8eace5baacabd9217e52a5ee \\\n    --hash=sha256:e52bf98c7e727dd44f7c4acb980cb988448faeafed8433c867888268899b298b \\\n    --hash=sha256:e6ec2be506e7d6f9527dae9ff4b7f54e68ea44a0ef6b098256ddf895218a2f8f \\\n    --hash=sha256:e725edd0929fa79f8349ab4ec7f81c714df51dc4e991539a578e5018fa4a7152 \\\n    --hash=sha256:eaa58399c01db555346647a907b4ef6d4f584b123943be6ed5588c3f2359c9f4 \\\n    --hash=sha256:eb21aaa9a200d0a80dacc7a81038d2e476ffe473ffdd9c91eb745d623561de05 \\\n    --hash=sha256:ecff0dc14f409599bbcafa7e470c00b80f17abc14d1405d38ab02e4b42e55b57 \\\n    --hash=sha256:f557c55bc2b7676e74d39d19bcb8775ca295c7a028246175d6a8b431e70835e5 \\\n    --hash=sha256:f7200b8f7619d36aa51c803fd52020a2dfbea36ffec1b5e22cab11fd34d95a6d \\\n    --hash=sha256:f9d471356dc16f84ed48768b8ee79f29514295c7295cb41e1133ec0b2b8d637d \\\n    --hash=sha256:fc5adda618205bd4678b146612ce44c3cbfdee9697951f2c0ffdef1f26d72b63 \\\n    --hash=sha256:fc9043259ee430ecd71d178fccabd8c332a3bf1e81e50cae43cc2b28d19e4cb7 \\\n    --hash=sha256:ffd9fee7d0775ebaba131f7ca2e2d83839a62ad65e8e02fe2bd8fc975cedeb9e\n    # via openai\n```\n\n----------------------------------------\n\nTITLE: Generating Code from Natural Language Intents with a Fine-tuned LLM in Python\nDESCRIPTION: Demonstrates how to use the fine-tuned language model to generate code snippets from natural language intents. The code loops through each test case, formats a prompt with the intent, and uses the generator to produce code with sampling for diversity.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor case in testcases:\n    prompt = PROMPT_TEMPLATE.format(intent=case[\"intent\"], snippet=\"\")\n    output = generator(prompt, max_new_tokens=30, do_sample=True)\n    print(output[0][\"generated_text\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Processing with Metaflow Foreach\nDESCRIPTION: Defines a Metaflow workflow that processes a list of media titles in parallel using the foreach feature. The workflow starts with a list of titles, processes each title independently, joins the results, and prints the processed titles.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/workflow/examples/comparisons/metaflow/foreach_metaflow.py.txt#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom metaflow import FlowSpec, step\n\n\nclass ForeachFlow(FlowSpec):\n    @step\n    def start(self):\n        self.titles = [\"Stranger Things\", \"House of Cards\", \"Narcos\"]\n        self.next(self.a, foreach=\"titles\")\n\n    @step\n    def a(self):\n        self.title = \"%s processed\" % self.input\n        self.next(self.join)\n\n    @step\n    def join(self, inputs):\n        self.results = [input.title for input in inputs]\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print(\"\\n\".join(self.results))\n\n\nif __name__ == \"__main__\":\n    ForeachFlow()\n```\n\n----------------------------------------\n\nTITLE: Displaying Actor State Transition Diagram in ASCII Art\nDESCRIPTION: This ASCII art diagram illustrates the possible state transitions for actors in the Ray system. It shows the five states (DEPENDENCIES_UNREADY, PENDING_CREATION, ALIVE, RESTARTING, and DEAD) and the numbered transitions between them.\nSOURCE: https://github.com/ray-project/ray/blob/master/src/ray/design_docs/actor_states.rst#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n                                                                         3\n   0                            1                       2          ------------->\n ---->DEPENDENCIES_UNREADY-------->PENDING_CREATION-------->ALIVE                RESTARTING\n               |                            |                  |   <-------------      |\n               |                            |                  |         4             |\n               |                            |                  |                       |\n             8 |                         7  |                6 |                       | 5\n               |                            |                  |                       |\n               |                            |                  |                       |\n               |                            |                  |                       |\n               |                            v                  |                       |\n                -------------------------->DEAD<---------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Executing Commands Before Ray Start in RayCluster Configuration\nDESCRIPTION: This YAML snippet demonstrates how to execute custom commands before the Ray start command without overriding the container command completely. This approach is useful for setting up environment variables needed by Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/pod-command.md#2025-04-12_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# https://github.com/ray-project/kuberay/ray-operator/config/samples/ray-cluster.head-command.yaml\n    rayStartParams:\n        ...\n    #pod template\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:2.8.0\n          resources:\n            ...\n          ports:\n            ...\n          # `command` and `args` will become a part of `spec.containers.0.args` in the head Pod.\n          command: [\"echo 123\"]\n          args: [\"456\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Result Buffer Time in Ray Tune\nDESCRIPTION: TUNE_RESULT_BUFFER_MIN_TIME_S sets the minimum time (in seconds) to buffer results. Defaults to 0 seconds.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_RESULT_BUFFER_MIN_TIME_S=0\n```\n\n----------------------------------------\n\nTITLE: Verifying Nsight System Installation in Bash\nDESCRIPTION: Command to verify that the Nsight System CLI is correctly installed by checking its version. This is a prerequisite for GPU profiling with Nsight on Ray.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/profiling.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ nsys --version\n\n# NVIDIA Nsight Systems version 2022.4.1.21-0db2c85\n```\n\n----------------------------------------\n\nTITLE: Creating AKS Cluster with System Nodepool\nDESCRIPTION: Initializes an AKS cluster with a system nodepool using Standard_D8s_v3 VM size and 3 nodes\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/azure-aks-gpu-cluster.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naz aks create \\\n   -g kuberay-rg \\\n   -n kuberay-gpu-cluster \\\n   --nodepool-name system \\\n   --node-vm-size Standard_D8s_v3 \\\n   --node-count 3\n```\n\n----------------------------------------\n\nTITLE: Configuring KubeRay for netrc Secret Usage\nDESCRIPTION: YAML configuration for KubeRay to mount the netrc secret as a volume and set the NETRC environment variable for both head and worker groups.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/runtime_env_auth.md#2025-04-12_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nheadGroupSpec:\n    ...\n    containers:\n        - name: ...\n          image: rayproject/ray:latest\n          ...\n          volumeMounts:\n            - mountPath: \"/home/ray/netrcvolume/\"\n              name: netrc-kuberay\n              readOnly: true\n          env:\n            - name: NETRC\n              value: \"/home/ray/netrcvolume/.netrc\"\n    volumes:\n        - name: netrc-kuberay\n          secret:\n            secretName: netrc-secret\n\nworkerGroupSpecs:\n    ...\n    containers:\n        - name: ...\n          image: rayproject/ray:latest\n          ...\n          volumeMounts:\n            - mountPath: \"/home/ray/netrcvolume/\"\n              name: netrc-kuberay\n              readOnly: true\n          env:\n            - name: NETRC\n              value: \"/home/ray/netrcvolume/.netrc\"\n    volumes:\n        - name: netrc-kuberay\n          secret:\n            secretName: netrc-secret\n```\n\n----------------------------------------\n\nTITLE: Listing PyTest Package with Hash Values\nDESCRIPTION: Definition for the PyTest package dependency with version 7.4.4 and corresponding SHA256 hash values. The comment indicates this is required through both compiled requirements and ML BYOD requirements for Python 3.9.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_27\n\nLANGUAGE: text\nCODE:\n```\npytest==7.4.4 \\\n    --hash=sha256:2cf0005922c6ace4a3e2ec8b4080eb0d9753fdc93107415332f50ce9e7994280 \\\n    --hash=sha256:b090cdf5ed60bf4c45261be03239c2c1c22df034fbffe691abe93cd80cea01d8\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_ml_byod_3.9.in\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how Python package dependencies are specified with exact versions and SHA256 hash values for security. It includes packages like matplotlib-inline, mdit-py-plugins, and mdurl.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_17\n\nLANGUAGE: Text\nCODE:\n```\nmatplotlib-inline==0.1.6 \\\n    --hash=sha256:f1f41aab5328aa5aaea9b16d083b128102f8712542f819fe7e6a420ff581b311 \\\n    --hash=sha256:f887e5f10ba98e8d2b150ddcf4702c1e5f8b3a20005eb0f74bfdbd360ee6f304\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   ipykernel\n    #   ipython\nmdit-py-plugins==0.4.2 \\\n    --hash=sha256:0c673c3f889399a33b95e88d2f0d111b4447bdfea7f237dab2d488f459835636 \\\n    --hash=sha256:5f2cd1fdb606ddf152d37ec30e46101a60512bc0e5fa1a7002c36647b09e26b5\n    # via jupytext\nmdurl==0.1.2 \\\n    --hash=sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8 \\\n    --hash=sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   markdown-it-py\n```\n\n----------------------------------------\n\nTITLE: Setting Ray Docker Image and SSH Configuration for Multinode Setup\nDESCRIPTION: Configure environment variables for specifying the Ray Docker image and indicating SSH availability for multinode deployment. This setup uses a pre-built image with OpenSSH server installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/fake-autoscaler.rst#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nRAY_DOCKER_IMAGE=\"rayproject/ray:multinode-py38\"\nRAY_HAS_SSH=1\n```\n\n----------------------------------------\n\nTITLE: Executing UV Pip Compile for Ray Dependencies with CUDA Support\nDESCRIPTION: Command to compile Python dependencies for Ray project with CUDA 12.4 support, using uv pip compile to generate a requirements file with hashes from the source requirements.txt. The command ensures proper dependency resolution with specific flags for security and compatibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cu124 --find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c python/requirements_compiled_ray_test_py311_cu124.txt python/requirements.txt -o python/requirements_compiled_ray_py311_cu124.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Notebook Parameters in Python\nDESCRIPTION: Shows how to set parameters for a notebook example. This code cell defines the number of workers and GPUs to be used in the example.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nnum_workers = 8\nnum_gpus = 2\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in HTML for Ray Dashboard Documentation\nDESCRIPTION: HTML iframe code for embedding YouTube videos demonstrating various Ray Dashboard features. The code uses responsive design with relative positioning and overflow handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/getting-started.rst#2025-04-12_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<div style=\"position: relative; height: 0; overflow: hidden; max-width: 100%; height: auto;\">\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/K2jLoIhlsnY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Installing Gymnasium Environments with Multiple Packages\nDESCRIPTION: Bash command to install Gymnasium environments with additional dependencies for Atari, ROM licenses, and MuJoCo support\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/index.rst#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install \"gymnasium[atari,accept-rom-license,mujoco]\"\n```\n\n----------------------------------------\n\nTITLE: Deleting RayCluster Resources After Benchmark\nDESCRIPTION: Removes all RayCluster custom resources from the default namespace after completing a benchmark experiment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md#2025-04-12_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl delete --all rayclusters.ray.io --namespace=default\n```\n\n----------------------------------------\n\nTITLE: Verifying Service Account Permissions After RBAC Configuration\nDESCRIPTION: Command to confirm that the ray-user service account now has access to the RayCluster resource after applying RBAC roles.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/kuberay-auth.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl auth can-i get rayclusters.ray.io/ray-cluster-with-auth --as=system:serviceaccount:default:ray-user\n```\n\n----------------------------------------\n\nTITLE: Executing serve run Command via CLI\nDESCRIPTION: This CLI command demonstrates how to use `serve run` to execute a Ray Serve application via HTTP, similar to `uvicorn`. Running this blocks the terminal and is used for local deployments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/dev-workflow.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n\"serve run local_dev:app\"\n```\n\n----------------------------------------\n\nTITLE: Defining Search Space - Python\nDESCRIPTION: This code snippet establishes the hyperparameter search space, setting bounds and choices for various parameters that the optimization algorithm will explore.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsearch_space = {\\n    \"steps\": 100,\\n    \"width\": tune.uniform(0, 20),\\n    \"height\": tune.uniform(-100, 100),\\n    \"activation\": tune.choice([\"relu\", \"tanh\"]),\\n}\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Specifies exact versions and SHA256 hashes for Python packages including pygments, pyopenssl, python-dateutil, python-dotenv and others. Each package includes verification hashes and references to requirement files where they are used.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_13\n\nLANGUAGE: text\nCODE:\n```\npygments==2.18.0 \\\n    --hash=sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199 \\\n    --hash=sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a\n```\n\nLANGUAGE: text\nCODE:\n```\npyopenssl==24.2.1 \\\n    --hash=sha256:4247f0dbe3748d560dcbb2ff3ea01af0f9a1a001ef5f7c4c647956ed8cbf0e95 \\\n    --hash=sha256:967d5719b12b243588573f39b0c677637145c7a1ffedcd495a487e58177fbb8d\n```\n\n----------------------------------------\n\nTITLE: Specifying Starlette Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the starlette package dependency with version 0.37.2 and SHA-256 hashes for verification. Comments indicate this package is required by fastapi and is constrained by requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_43\n\nLANGUAGE: pip\nCODE:\n```\nstarlette==0.37.2 \\\n    --hash=sha256:6fe59f29268538e5d0d182f2791a479a0c64638e6935d1c6989e63fb2699c6ee \\\n    --hash=sha256:9af890290133b79fc3db55474ade20f6220a364a0402e0b556e7cd5e1e093823\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   fastapi\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Ray Tune Training Function in Python\nDESCRIPTION: This code snippet defines a simple training function for Ray Tune, which simulates training and evaluation with placeholder logic for data, model, and epochs.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune_get_data_in_and_out.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport time\nimport pandas as pd\n\n\ndef training_function(config):\n    # For now, we have nothing here.\n    data = None\n    model = {\"hyperparameter_a\": None, \"hyperparameter_b\": None}\n    epochs = 0\n\n    # Simulate training & evaluation - we obtain back a \"metric\" and a \"trained_model\".\n    for epoch in range(epochs):\n        # Simulate doing something expensive.\n        time.sleep(1)\n        metric = (0.1 + model[\"hyperparameter_a\"] * epoch / 100) ** (\n            -1\n        ) + model[\"hyperparameter_b\"] * 0.1 * data[\"A\"].sum()\n        trained_model = {\"state\": model, \"epoch\": epoch}\n```\n\n----------------------------------------\n\nTITLE: PBT Example with Ray Tune\nDESCRIPTION: This Python code snippet demonstrates how to use Ray Tune to implement Population Based Training (PBT). It defines a training function that performs a simple optimization task and configures the PBT tuner to explore a hyperparameter space. The example showcases the key components of PBT, including population management and exploration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/pbt_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\":orphan:\\n\\nPBT Example\\n~~~~~~~~~~\\n\\n.. literalinclude:: /../../python/ray/tune/examples/pbt_example.py\"\n```\n\n----------------------------------------\n\nTITLE: Package Definition for multidict in Python\nDESCRIPTION: Defines the multidict package at version 6.0.5 with its cryptographic hash values for verification. Each hash value ensures the integrity of the package when it's downloaded and installed, providing security against compromised packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_22\n\nLANGUAGE: plaintext\nCODE:\n```\nmultidict==6.0.5 \\\n    --hash=sha256:01265f5e40f5a17f8241d52656ed27192be03bfa8764d88e8220141d1e4b3556 \\\n    --hash=sha256:0275e35209c27a3f7951e1ce7aaf93ce0d163b28948444bec61dd7badc6d3f8c \\\n    --hash=sha256:04bde7a7b3de05732a4eb39c94574db1ec99abb56162d6c520ad26f83267de29 \\\n    --hash=sha256:04da1bb8c8dbadf2a18a452639771951c662c5ad03aefe4884775454be322c9b \\\n    --hash=sha256:09a892e4a9fb47331da06948690ae38eaa2426de97b4ccbfafbdcbe5c8f37ff8 \\\n    --hash=sha256:0d63c74e3d7ab26de115c49bffc92cc77ed23395303d496eae515d4204a625e7 \\\n    --hash=sha256:107c0cdefe028703fb5dafe640a409cb146d44a6ae201e55b35a4af8e95457dd \\\n    --hash=sha256:141b43360bfd3bdd75f15ed811850763555a251e38b2405967f8e25fb43f7d40 \\\n    --hash=sha256:14c2976aa9038c2629efa2c148022ed5eb4cb939e15ec7aace7ca932f48f9ba6 \\\n    --hash=sha256:19fe01cea168585ba0f678cad6f58133db2aa14eccaf22f88e4a6dccadfad8b3 \\\n    --hash=sha256:1d147090048129ce3c453f0292e7697d333db95e52616b3793922945804a433c \\\n    --hash=sha256:1d9ea7a7e779d7a3561aade7d596649fbecfa5c08a7674b11b423783217933f9 \\\n    --hash=sha256:215ed703caf15f578dca76ee6f6b21b7603791ae090fbf1ef9d865571039ade5 \\\n    --hash=sha256:21fd81c4ebdb4f214161be351eb5bcf385426bf023041da2fd9e60681f3cebae \\\n    --hash=sha256:220dd781e3f7af2c1c1053da9fa96d9cf3072ca58f057f4c5adaaa1cab8fc442 \\\n    --hash=sha256:228b644ae063c10e7f324ab1ab6b548bdf6f8b47f3ec234fef1093bc2735e5f9 \\\n    --hash=sha256:29bfeb0dff5cb5fdab2023a7a9947b3b4af63e9c47cae2a10ad58394b517fddc \\\n    --hash=sha256:2f4848aa3baa109e6ab81fe2006c77ed4d3cd1e0ac2c1fbddb7b1277c168788c \\\n    --hash=sha256:2faa5ae9376faba05f630d7e5e6be05be22913782b927b19d12b8145968a85ea \\\n    --hash=sha256:2ffc42c922dbfddb4a4c3b438eb056828719f07608af27d163191cb3e3aa6cc5 \\\n    --hash=sha256:37b15024f864916b4951adb95d3a80c9431299080341ab9544ed148091b53f50 \\\n    --hash=sha256:3cc2ad10255f903656017363cd59436f2111443a76f996584d1077e43ee51182 \\\n    --hash=sha256:3d25f19500588cbc47dc19081d78131c32637c25804df8414463ec908631e453 \\\n    --hash=sha256:403c0911cd5d5791605808b942c88a8155c2592e05332d2bf78f18697a5fa15e \\\n    --hash=sha256:411bf8515f3be9813d06004cac41ccf7d1cd46dfe233705933dd163b60e37600 \\\n    --hash=sha256:425bf820055005bfc8aa9a0b99ccb52cc2f4070153e34b701acc98d201693733 \\\n    --hash=sha256:435a0984199d81ca178b9ae2c26ec3d49692d20ee29bc4c11a2a8d4514c67eda \\\n    --hash=sha256:4a6a4f196f08c58c59e0b8ef8ec441d12aee4125a7d4f4fef000ccb22f8d7241 \\\n    --hash=sha256:4cc0ef8b962ac7a5e62b9e826bd0cd5040e7d401bc45a6835910ed699037a461 \\\n    --hash=sha256:51d035609b86722963404f711db441cf7134f1889107fb171a970c9701f92e1e \\\n    --hash=sha256:53689bb4e102200a4fafa9de9c7c3c212ab40a7ab2c8e474491914d2305f187e \\\n    --hash=sha256:55205d03e8a598cfc688c71ca8ea5f66447164efff8869517f175ea632c7cb7b \\\n    --hash=sha256:5c0631926c4f58e9a5ccce555ad7747d9a9f8b10619621f22f9635f069f6233e \\\n    --hash=sha256:5cb241881eefd96b46f89b1a056187ea8e9ba14ab88ba632e68d7a2ecb7aadf7 \\\n    --hash=sha256:60d698e8179a42ec85172d12f50b1668254628425a6bd611aba022257cac1386 \\\n    --hash=sha256:612d1156111ae11d14afaf3a0669ebf6c170dbb735e510a7438ffe2369a847fd \\\n    --hash=sha256:6214c5a5571802c33f80e6c84713b2c79e024995b9c5897f794b43e714daeec9 \\\n    --hash=sha256:6939c95381e003f54cd4c5516740faba40cf5ad3eeff460c3ad1d3e0ea2549bf \\\n    --hash=sha256:69db76c09796b313331bb7048229e3bee7928eb62bab5e071e9f7fcc4879caee \\\n    --hash=sha256:6bf7a982604375a8d49b6cc1b781c1747f243d91b81035a9b43a2126c04766f5 \\\n    --hash=sha256:766c8f7511df26d9f11cd3a8be623e59cca73d44643abab3f8c8c07620524e4a \\\n    --hash=sha256:76c0de87358b192de7ea9649beb392f107dcad9ad27276324c24c91774ca5271 \\\n    --hash=sha256:76f067f5121dcecf0d63a67f29080b26c43c71a98b10c701b0677e4a065fbd54 \\\n    --hash=sha256:7901c05ead4b3fb75113fb1dd33eb1253c6d3ee37ce93305acd9d38e0b5f21a4 \\\n    --hash=sha256:79660376075cfd4b2c80f295528aa6beb2058fd289f4c9252f986751a4cd0496 \\\n    --hash=sha256:79a6d2ba910adb2cbafc95dad936f8b9386e77c84c35bc0add315b856d7c3abb \\\n\n```\n\n----------------------------------------\n\nTITLE: Defining snowballstemmer dependency with version pinning and hash verification\nDESCRIPTION: Specifies snowballstemmer version 2.2.0 with SHA-256 hashes for verification. Comments indicate it's required by sphinx.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_39\n\nLANGUAGE: plaintext\nCODE:\n```\nsnowballstemmer==2.2.0 \\\n    --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \\\n    --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a\n    # via sphinx\n```\n\n----------------------------------------\n\nTITLE: Creating a DaemonSet for Ray Image Preloading in Kubernetes\nDESCRIPTION: YAML configuration for a Kubernetes DaemonSet that pre-pulls Ray container images on every node. This ensures images are cached locally before Ray pods are scheduled, reducing startup time.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/reduce-image-pull-latency.md#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ray-image-preloader\n  labels:\n    k8s-app: ray-image-preloader\nspec:\n  selector:\n    matchLabels:\n      k8s-app: ray-image-preloader\n  template:\n    metadata:\n      labels:\n        name: ray-image-preloader\n        k8s-app: ray-image-preloader\n    spec:\n      containers:\n      - image: rayproject/ray:2.40.0\n        name: ray-image-preloader\n        command: [ \"sleep\", \"inf\" ]\n```\n\n----------------------------------------\n\nTITLE: Specifying numpy Package Requirement\nDESCRIPTION: Defines the required version and hash values for the numpy package. This ensures a specific, verified version is installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_12\n\nLANGUAGE: Text\nCODE:\n```\nnumpy==1.26.4 \\\n    --hash=sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b \\\n    --hash=sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818 \\\n    --hash=sha256:1af303d6b2210eb850fcf03064d364652b7120803a0b872f5211f5234b399f20\n```\n\n----------------------------------------\n\nTITLE: Allocating Resources to PreLearner Actors\nDESCRIPTION: Configuration for allocating CPU resources to PreLearner actors in the post-processing stage.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .offline_data(\n        map_batches_kwargs={\n            \"concurrency\": 4,\n            \"num_cpus\": 2,\n        },\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Killing Serve Controller using Python\nDESCRIPTION: This code snippet shows how to use the Python interpreter to get a handle to the Serve controller actor and kill it, simulating a controller failure.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/production-guide/fault-tolerance.md#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> import ray\n>>> controller_handle = ray.get_actor(\"SERVE_CONTROLLER_ACTOR\", namespace=\"serve\")\n>>> ray.kill(controller_handle, no_restart=True)\n>>> exit()\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Cluster\nDESCRIPTION: Commands to initialize a Ray cluster by starting the head node and optionally connecting other nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/examples/data_juicer_distributed_data_processing.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Start a cluster as the head node\nray start --head\n\n# (Optional) Connect to the cluster on other nodes/machines.\nray start --address='{head_ip}:6379'\n```\n\n----------------------------------------\n\nTITLE: Checking Grafana Pod Status\nDESCRIPTION: Commands to verify Grafana pod status and set up port forwarding for local access.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-operator-logs.md#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Verify that the Grafana pod is running in the `default` namespace.\nkubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana\"\n# NAME                       READY   STATUS    RESTARTS   AGE\n# grafana-54d5d747fd-5fldc   1/1     Running   0          8m21s\n```\n\n----------------------------------------\n\nTITLE: Specifying Ray Project Dependencies with Version Constraints\nDESCRIPTION: A requirements list specifying package dependencies for the Ray project. It includes evaluate version 0.4.3, mosaicml with a Python version constraint (less than 3.12), and sentencepiece version 0.1.96.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/ml/train-test-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nevaluate==0.4.3\nmosaicml; python_version < \"3.12\"\nsentencepiece==0.1.96\n```\n\n----------------------------------------\n\nTITLE: Creating a Volcano Queue with Resource Limits\nDESCRIPTION: YAML definition to create a Volcano queue with a capacity of 4 CPUs and 6Gi of RAM. The queue has a weight of 1 for resource sharing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -f - <<EOF\napiVersion: scheduling.volcano.sh/v1beta1\nkind: Queue\nmetadata:\n  name: kuberay-test-queue\nspec:\n  weight: 1\n  capability:\n    cpu: 4\n    memory: 6Gi\nEOF\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Hash Verification\nDESCRIPTION: Detailed package dependency list with specific versions and SHA-256 hash verification. Each package includes its version, hash values for verification, and comments indicating which other packages require it as a dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nargon2-cffi==23.1.0 \\\n    --hash=sha256:879c3e79a2729ce768ebb7d36d4609e3a78a4ca2ec3a9f12286ca057e3d0db08 \\\n    --hash=sha256:c670642b78ba29641818ab2e68bd4e6a78ba53b7eff7b4c3815ae16abf91c7ea\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jupyter-server\n    #   nbclassic\n    #   notebook\n```\n\n----------------------------------------\n\nTITLE: Testing Pandas DataFrame Operations\nDESCRIPTION: Validates the generated code for DataFrame string manipulation, demonstrating the replacement of whitespace with underscores in a column.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({\"col\": [\"abc def ghi\", \" 12 3 456\", \"     \"]})\nprint(\"Before\\n\", df)\n\ndf[\"col\"] = df[\"col\"].str.replace(\" \", \"_\")\nprint(\"After\\n\", df)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Documentation Build Environment\nDESCRIPTION: Command to clean the documentation build environment before starting a new build.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmake clean\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Documentation Structure for Ray Project\nDESCRIPTION: RST configuration that sets up the documentation hierarchy and navigation structure for Ray's documentation. It includes hidden toctree directive and references to all major Ray components and their documentation sections.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/index.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:html_theme.sidebar_secondary.remove:\n\n.. title:: Welcome to Ray!\n\n.. toctree::\n   :hidden:\n\n   Overview <ray-overview/index>\n   Getting Started <ray-overview/getting-started>\n   Installation <ray-overview/installation>\n   Use Cases <ray-overview/use-cases>\n   Example Gallery <ray-overview/examples>\n   Ecosystem <ray-overview/ray-libraries>\n   Ray Core <ray-core/walkthrough>\n   Ray Data <data/data>\n   Ray Train <train/train>\n   Ray Tune <tune/index>\n   Ray Serve <serve/index>\n   Ray RLlib <rllib/index>\n   More Libraries <ray-more-libs/index>\n   Ray Clusters <cluster/getting-started>\n   Monitoring and Debugging <ray-observability/index>\n   Developer Guides <ray-contribute/index>\n   Glossary <ray-references/glossary>\n   Security <ray-security/index>\n```\n\n----------------------------------------\n\nTITLE: Installing Podman on Ubuntu\nDESCRIPTION: This Bash snippet provides commands to install Podman on Ubuntu 20.04 or older. It requires adding a software repository and obtaining the necessary GPG keys before installing via apt.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/multi-app-container.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo sh -c \"echo 'deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_20.04/ /' > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\"\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 4D64390375060AA4\nsudo apt-get update\nsudo apt-get install podman -y\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Processing\nDESCRIPTION: This snippet illustrates the execution path of Netty's NIO event loop, including processing selected keys, reading from channels, and handling channel events.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_1\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j]\n```\n\n----------------------------------------\n\nTITLE: Implementing Even Number Function - Code Output Style\nDESCRIPTION: Example showing how to write a code-output-style snippet that demonstrates the same even number checking function with separate code and output blocks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/writing-code-snippets.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef is_even(x):\n    return (x % 2) == 0\n\nprint(is_even(0))\nprint(is_even(1))\n```\n\n----------------------------------------\n\nTITLE: Listing RayCluster Pods\nDESCRIPTION: Lists all pods associated with the RayCluster using label selector\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkubectl get pods --selector=ray.io/cluster=raycluster-kuberay\n```\n\n----------------------------------------\n\nTITLE: MNIST PyTorch Trainable Example\nDESCRIPTION: This code snippet shows a complete example of training an MNIST model using PyTorch and Ray Tune. It defines a `Trainable` class that encapsulates the model training logic and uses Ray Tune's hyperparameter optimization features to find the best model configuration. The example requires `torch`, `torchvision`, and `ray[tune]` to be installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/mnist_pytorch_trainable.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"/../../python/ray/tune/examples/mnist_pytorch_trainable.py\"\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies: google-auth Configuration\nDESCRIPTION: Definition of the google-auth package dependency with version 2.23.4 and corresponding SHA256 hashes. The comment indicates this package is required via python/requirements_compiled_ray_test_py311_cu121.txt and google-api-core.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\ngoogle-auth==2.23.4 \\\n    --hash=sha256:79905d6b1652187def79d491d6e23d0cbb3a21d3c7ba0dbaa9c8a01906b13ff3 \\\n    --hash=sha256:d4bbc92fe4b8bfd2f3e8d88e5ba7085935da208ee38a134fc280e7ce682a05f2\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   google-api-core\n```\n\n----------------------------------------\n\nTITLE: Netty Buffer Operations\nDESCRIPTION: This snippet demonstrates various operations on Netty buffers, including releasing, reading bytes, and accessing internal NIO buffers.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_3\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/AbstractReferenceCountedByteBuf:.release_[j]\n```\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];io/netty/buffer/PooledByteBuf:.internalNioBuffer_[j]\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify Python package dependencies with their versions and hash values for verification. It includes packages like google-api-core, google-auth, and google-cloud-storage.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_7\n\nLANGUAGE: Plain Text\nCODE:\n```\ngoogle-api-core==1.34.0 \\\n    --hash=sha256:6fb380f49d19ee1d09a9722d0379042b7edb06c0112e4796c7a395078a043e71 \\\n    --hash=sha256:7421474c39d396a74dfa317dddbc69188f2336835f526087c7648f91105e32ff\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   google-cloud-core\n    #   google-cloud-storage\ngoogle-apitools==0.5.32 \\\n    --hash=sha256:b78f74116558e0476e19501b5b4b2ac7c93261a69c5449c861ea95cbc853c688 \\\n    --hash=sha256:c3763e52289f61e21c41d5531e20fbda9cc8484a088b8686fd460770db8bad13\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gsutil\ngoogle-auth[aiohttp]==2.23.4 \\\n    --hash=sha256:79905d6b1652187def79d491d6e23d0cbb3a21d3c7ba0dbaa9c8a01906b13ff3 \\\n    --hash=sha256:d4bbc92fe4b8bfd2f3e8d88e5ba7085935da208ee38a134fc280e7ce682a05f2\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gcsfs\n    #   google-api-core\n    #   google-auth-oauthlib\n    #   google-cloud-core\n    #   google-cloud-storage\n    #   gsutil\n    #   tensorboard\n```\n\n----------------------------------------\n\nTITLE: Package Hash Requirements\nDESCRIPTION: Package dependency specifications with SHA256 hash verification values. Used to ensure package integrity and security when installing dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:8aecb5a7f6f7f8fe9cac0bcadd39efaca8bbf8d1bf242e9f175cbe4c925116c3 \\\n--hash=sha256:91bbf398ac8bb7d65a5a52127407c05f75a18d7015a270fdd94bbcb04e65d573\n```\n\n----------------------------------------\n\nTITLE: Netty Channel Read/Write Stack Trace with TCP Send\nDESCRIPTION: Stack trace showing execution from Netty NIO through TCP message sending at kernel level\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_7\n\nLANGUAGE: stacktrace\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k]\n```\n\n----------------------------------------\n\nTITLE: Specifying TensorFlow Package with Platform-Specific Conditions in pip\nDESCRIPTION: This code snippet defines the tensorflow package dependency with version 2.15.1 and multiple SHA-256 hashes for verification. It includes platform-specific conditions to only install on compatible platforms (not on macOS ARM64) and for Python versions below 3.12.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_48\n\nLANGUAGE: pip\nCODE:\n```\ntensorflow==2.15.1 ; python_version < \"3.12\" and (sys_platform != \"darwin\" or platform_machine != \"arm64\") \\\n    --hash=sha256:10132acc072d59696c71ce7221d2d8e0e3ff1e6bc8688dbac6d7aed8e675b710 \\\n    --hash=sha256:30c5ef9c758ec9ff7ce2aff76b71c980bc5119b879071c2cc623b1591a497a1a \\\n    --hash=sha256:432788ac5d1234b9e9b7c7f73603a5655271a28c293329c52c7c0b9434a1184e \\\n    --hash=sha256:6761efe511e6ee0f893f60738fefbcc51d6dc386eeaaafea59d21899ef369ffd \\\n    --hash=sha256:89b5aa1022dec47e567512eaf4e1271b8e6c1ff1984e30d0d9127bd1093ed4c5 \\\n    --hash=sha256:8e5431d45ceb416c2b1b6de87378054fbac7d2ed35d45b102d89a786613fffdc \\\n\n```\n\n----------------------------------------\n\nTITLE: Specifying TensorBoard Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the tensorboard package dependency with version 2.15.2 and SHA-256 hash for verification. Comments indicate this package is required by tensorflow and is constrained by requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_45\n\nLANGUAGE: pip\nCODE:\n```\ntensorboard==2.15.2 \\\n    --hash=sha256:a6f6443728064d962caea6d34653e220e34ef8df764cb06a8212c17e1a8f0622\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   tensorflow\n```\n\n----------------------------------------\n\nTITLE: Comment About File Generation\nDESCRIPTION: Comment header indicating this file was automatically generated using the uv tool\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# This file was autogenerated by uv via the following command:\n```\n\n----------------------------------------\n\nTITLE: Installing py4j with Pinned Version and Hashes\nDESCRIPTION: Specifies py4j package with version 0.10.9.7 and SHA256 hashes for verification. Comments indicate this is required by the pyspark package.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_22\n\nLANGUAGE: pip\nCODE:\n```\npy4j==0.10.9.7 \\\n    --hash=sha256:0b6e5315bb3ada5cf62ac651d107bb2ebc02def3dee9d9548e3baac644ea8dbb \\\n    --hash=sha256:85defdfd2b2376eb3abf5ca6474b51ab7e0de341c75a02f46dc9b5976f5a5c1b\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   pyspark\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray and Parallelizing Functions with Java Tasks\nDESCRIPTION: Shows how to convert a Java static method into a Ray task that runs asynchronously in a remote worker process\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_23\n\nLANGUAGE: java\nCODE:\n```\nimport io.ray.api.ObjectRef;\nimport io.ray.api.Ray;\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class RayDemo {\n    public static int square(int x) {\n        return x * x;\n    }\n\n    public static void main(String[] args) {\n        Ray.init();\n        List<ObjectRef<Integer>> objectRefList = new ArrayList<>();\n        for (int i = 0; i < 4; i++) {\n            objectRefList.add(Ray.task(RayDemo::square, i).remote());\n        }\n        System.out.println(Ray.get(objectRefList));  // [0, 1, 4, 9]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Gaudi Docker Container\nDESCRIPTION: This Bash snippet shows how to pull and run the Intel Gaudi Docker container for model training. Dependencies such as Docker are required to execute this command.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/bert.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\ndocker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n```\n\n----------------------------------------\n\nTITLE: Package Configuration for Python Dependencies with CUDA Support\nDESCRIPTION: Configuration section of the requirements file specifying the package indices and linked wheel repositories for PyTorch and PyG with CUDA 12.4 support. This ensures that packages are downloaded from the correct sources with compatible CUDA versions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n--index-url https://pypi.org/simple\n--extra-index-url https://download.pytorch.org/whl/cu124\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html\n--find-links https://data.pyg.org/whl/torch-2.5.1+cu124.html\n```\n\n----------------------------------------\n\nTITLE: Starting Ray Head Node\nDESCRIPTION: Command to initialize the Ray head node on port 6379. This creates the main cluster coordinator that worker nodes will connect to.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nray start --head --port=6379\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with SHA256 Hashes\nDESCRIPTION: Package dependency specifications with exact versions and SHA256 hash validations for security. Includes pluggy 1.3.0, propcache 0.3.0, and protobuf 3.20.3 along with their dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\npluggy==1.3.0 \\\n    --hash=sha256:cf61ae8f126ac6f7c451172cf30e3e43d3ca77615509771b3a984a0730651e12 \\\n    --hash=sha256:d89c696a773f8bd377d18e5ecda92b7a3793cbe66c87060a6fb58c7b6e1061f7\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   pytest\n```\n\n----------------------------------------\n\nTITLE: Delete Kubernetes Cluster\nDESCRIPTION: Removes the entire Kind cluster as part of cleanup.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nkind delete cluster\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Cluster with Kind\nDESCRIPTION: Command to create a Kubernetes cluster using Kind with specified node version\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nkind create cluster --image=kindest/node:v1.26.0\n```\n\n----------------------------------------\n\nTITLE: Specifying oauth2client Package Requirement\nDESCRIPTION: Defines the required version and hash values for the oauth2client package. This ensures a specific, verified version is installed.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_14\n\nLANGUAGE: Text\nCODE:\n```\noauth2client==4.1.3 \\\n    --hash=sha256:b8a81cc5d60e2d364f0b1b98f958dbd472887acaf1a5b05e21c28c31a2d6d3ac \\\n    --hash=sha256:d486741e451287f69568a4d26d70d9acd73a2bbfa275746c535b4209891cccc6\n```\n\n----------------------------------------\n\nTITLE: Viewing Training Results\nDESCRIPTION: Retrieves and displays the results of all training trials as a pandas DataFrame.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/start.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresults_df = result_grid.get_dataframe()\nresults_df\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Hash Values\nDESCRIPTION: This snippet defines Python package dependencies with specific versions and hash values for security verification. It includes comments indicating the source of the requirements and related packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_4\n\nLANGUAGE: Text\nCODE:\n```\nattrs==25.1.0 \\\n    --hash=sha256:1c97078a80c814273a76b2a298a932eb681c87415c11dee0a6921de7f1b02c3e \\\n    --hash=sha256:c75a69e28a550a7e93789579c22aa26b0f5b83b75dc4e08fe092980051e1090a\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   aiohttp\n    #   jsonschema\n    #   referencing\n```\n\n----------------------------------------\n\nTITLE: Testing Regex Pattern Matching\nDESCRIPTION: Tests the generated code for regular expression pattern matching in XML content using re.findall().\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\nline = \"\"\"\n<bookstore>\n  <book category=\"fiction\">\n    <title>The Great Gatsby</title>\n    <author>F. Scott Fitzgerald</author>\n    <year>1925</year>\n  </book>\n  <book category=\"non-fiction\">\n    <title>Sapiens: A Brief History of Humankind</title>\n    <author>Yuval Noah Harari</author>\n    <year>2011</year>\n  </book>\n</bookstore>\n\"\"\"\nre.findall(\">.*<\", line)\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how to specify package dependencies with their corresponding SHA256 hash values for verification. It includes examples for lightning-utilities, llvmlite, lm-eval, locust, and lxml packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_8\n\nLANGUAGE: Text\nCODE:\n```\nlightning-utilities==0.11.2 \\\n    --hash=sha256:541f471ed94e18a28d72879338c8c52e873bb46f4c47644d89228faeb6751159 \\\n    --hash=sha256:adf4cf9c5d912fe505db4729e51d1369c6927f3a8ac55a9dff895ce5c0da08d9\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   pytorch-lightning\nllvmlite==0.42.0 \\\n    --hash=sha256:05cb7e9b6ce69165ce4d1b994fbdedca0c62492e537b0cc86141b6e2c78d5888 \\\n    --hash=sha256:08fa9ab02b0d0179c688a4216b8939138266519aaa0aa94f1195a8542faedb56 \\\n    --hash=sha256:3366938e1bf63d26c34fbfb4c8e8d2ded57d11e0567d5bb243d89aab1eb56098 \\\n    --hash=sha256:43d65cc4e206c2e902c1004dd5418417c4efa6c1d04df05c6c5675a27e8ca90e \\\n    --hash=sha256:70f44ccc3c6220bd23e0ba698a63ec2a7d3205da0d848804807f37fc243e3f77 \\\n    --hash=sha256:763f8d8717a9073b9e0246998de89929071d15b47f254c10eef2310b9aac033d \\\n    --hash=sha256:7e0c4c11c8c2aa9b0701f91b799cb9134a6a6de51444eff5a9087fc7c1384275 \\\n    --hash=sha256:81e674c2fe85576e6c4474e8c7e7aba7901ac0196e864fe7985492b737dbab65 \\\n    --hash=sha256:8d90edf400b4ceb3a0e776b6c6e4656d05c7187c439587e06f86afceb66d2be5 \\\n    --hash=sha256:a78ab89f1924fc11482209f6799a7a3fc74ddc80425a7a3e0e8174af0e9e2301 \\\n    --hash=sha256:ae511caed28beaf1252dbaf5f40e663f533b79ceb408c874c01754cafabb9cbf \\\n    --hash=sha256:b2fce7d355068494d1e42202c7aff25d50c462584233013eb4470c33b995e3ee \\\n    --hash=sha256:bb3975787f13eb97629052edb5017f6c170eebc1c14a0433e8089e5db43bcce6 \\\n    --hash=sha256:bdd3888544538a94d7ec99e7c62a0cdd8833609c85f0c23fcb6c5c591aec60ad \\\n    --hash=sha256:c35da49666a21185d21b551fc3caf46a935d54d66969d32d72af109b5e7d2b6f \\\n    --hash=sha256:c5bece0cdf77f22379f19b1959ccd7aee518afa4afbd3656c6365865f84903f9 \\\n    --hash=sha256:d0936c2067a67fb8816c908d5457d63eba3e2b17e515c5fe00e5ee2bace06040 \\\n    --hash=sha256:d47494552559e00d81bfb836cf1c4d5a5062e54102cc5767d5aa1e77ccd2505c \\\n    --hash=sha256:d7599b65c7af7abbc978dbf345712c60fd596aa5670496561cc10e8a71cebfb2 \\\n    --hash=sha256:ebe66a86dc44634b59a3bc860c7b20d26d9aaffcd30364ebe8ba79161a9121f4 \\\n    --hash=sha256:f92b09243c0cc3f457da8b983f67bd8e1295d0f5b3746c7a1861d7a99403854a\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   numba\nlm-eval==0.4.0 \\\n    --hash=sha256:2dac56039b191c2dfb0011329ec9082e474006a15575db45468b88753923b34b\n    # via -r release/ray_release/byod/requirements_ml_byod_3.9.in\nlocust==2.18.0 \\\n    --hash=sha256:55036b2601ad7a2725885ceafb28f90390128a9a5dc631809da462f53b37cd56 \\\n    --hash=sha256:f8d668c2c33518c705664bc869791d58fc98ba8f1aadbf2335be36e4e681feae\n    # via -r release/ray_release/byod/requirements_ml_byod_3.9.in\nlxml==4.9.4 \\\n    --hash=sha256:00e91573183ad273e242db5585b52670eddf92bacad095ce25c1e682da14ed91 \\\n    --hash=sha256:01bf1df1db327e748dcb152d17389cf6d0a8c5d533ef9bab781e9d5037619229 \\\n    --hash=sha256:056a17eaaf3da87a05523472ae84246f87ac2f29a53306466c22e60282e54ff8 \\\n    --hash=sha256:0a08c89b23117049ba171bf51d2f9c5f3abf507d65d016d6e0fa2f37e18c0fc5 \\\n    --hash=sha256:1343df4e2e6e51182aad12162b23b0a4b3fd77f17527a78c53f0f23573663545 \\\n    --hash=sha256:1449f9451cd53e0fd0a7ec2ff5ede4686add13ac7a7bfa6988ff6d75cff3ebe2 \\\n    --hash=sha256:16b9ec51cc2feab009e800f2c6327338d6ee4e752c76e95a35c4465e80390ccd \\\n    --hash=sha256:1f10f250430a4caf84115b1e0f23f3615566ca2369d1962f82bef40dd99cd81a \\\n    --hash=sha256:231142459d32779b209aa4b4d460b175cadd604fed856f25c1571a9d78114771 \\\n    --hash=sha256:232fd30903d3123be4c435fb5159938c6225ee8607b635a4d3fca847003134ba \\\n    --hash=sha256:23d891e5bdc12e2e506e7d225d6aa929e0a0368c9916c1fddefab88166e98b20 \\\n    --hash=sha256:266f655d1baff9c47b52f529b5f6bec33f66042f65f7c56adde3fcf2ed62ae8b \\\n    --hash=sha256:273473d34462ae6e97c0f4e517bd1bf9588aa67a1d47d93f760a1282640e24ac \\\n    --hash=sha256:2bd9ac6e44f2db368ef8986f3989a4cad3de4cd55dbdda536e253000c801bcc7\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Cluster\nDESCRIPTION: Basic setup to start a local Ray cluster\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/examples/gentle_walkthrough.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nray.init()\n```\n\n----------------------------------------\n\nTITLE: Self-Play Example with OpenSpiel\nDESCRIPTION: A simpler version of the league-based self-play implementation that enables agents to improve through direct competition in two-player games.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Self-play with OpenSpiel\n# This script allows for direct self-play between agents.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Output Example of Best Hyperparameters Found\nDESCRIPTION: Shows an example output revealing the best hyperparameters identified by the Ray Tune process. It includes hyperparameters such as 'objective', 'metric', and optimized values like 'num_leaves' and 'learning_rate'. Indicates the completion of the Hyperparameter tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/lightgbm_example.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nBest hyperparameters found were: {'objective': 'binary', 'metric': ['binary_error', 'binary_logloss'], 'verbose': -1, 'boosting_type': 'gbdt', 'num_leaves': 622, 'learning_rate': 0.003721286118355498}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Hashes\nDESCRIPTION: This snippet shows how Python package dependencies are specified in a requirements.txt file, including version constraints and SHA256 hashes for security verification. It demonstrates the format for multiple packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nalabaster==0.7.16 \\\n    --hash=sha256:75a8b99c28a5dad50dd7f8ccdd447a121ddb3892da9e53d1ca5cca3106d58d65 \\\n    --hash=sha256:b46733c07dce03ae4e150330b975c75737fa60f0a7c591b6c8bf4928a28e2c92\n    # via sphinx\nannotated-types==0.6.0 \\\n    --hash=sha256:0641064de18ba7a25dee8f96403ebc39113d0cb953a01429249d5c7564666a43 \\\n    --hash=sha256:563339e807e53ffd9c267e99fc6d9ea23eb8443c08f112651963e24e22f84a5d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   pydantic\nanyio==3.7.1 \\\n    --hash=sha256:44a3c9aba0f5defa43261a8b3efb97891f2bd7d804e0e1f56419befa1adfc780 \\\n    --hash=sha256:91dee416e570e92c64041bd18b900d1d6fa78dff7048769ce5ac5ddad004fbb5\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   httpx\n    #   jupyter-server\n    #   openai\n    #   starlette\n    #   watchfiles\n```\n\n----------------------------------------\n\nTITLE: Setting the number of samples\nDESCRIPTION: This snippet sets the number of hyperparameter combinations to be tried out during the Tune run. The initial value is 1000, but is reduced to 10 in a subsequent snippet, likely for smoke tests.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 1000\n\n```\n\n----------------------------------------\n\nTITLE: Ray Tune Basic Example in Python\nDESCRIPTION: This snippet shows a basic example of using Ray Tune for hyperparameter optimization in Python. It demonstrates how to define a training function, configure the search space, and run the tuning process using Tune's run method. The example aims to provide a starting point for users to explore hyperparameter tuning with Ray Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/tune_basic_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"/../../python/ray/tune/examples/tune_basic_example.py\"\n```\n\n----------------------------------------\n\nTITLE: Initializing MetricsLogger in RLlib Python\nDESCRIPTION: Demonstrates creating a MetricsLogger instance and logging scalar values with reduction strategies. Allows tracking and aggregating metrics over time with configurable windowing and reduction methods.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/utils.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.utils.metrics.metrics_logger import MetricsLogger\n\nlogger = MetricsLogger()\n\n# Log a scalar float value under the `loss` key. By default, all logged\n# values under that key are averaged, once `reduce()` is called.\nlogger.log_value(\"loss\", 0.05, reduce=\"mean\", window=2)\nlogger.log_value(\"loss\", 0.1)\nlogger.log_value(\"loss\", 0.2)\n\nlogger.peek(\"loss\")  # expect: 0.15 (mean of last 2 values: 0.1 and 0.2)\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Client with Namespace and Runtime Environment\nDESCRIPTION: This code shows how to initialize Ray Client with additional arguments like namespace and runtime environment. It sets up a connection to a specific cluster with custom configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/ray-client.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Connects to an existing cluster at 1.2.3.4 listening on port 10001, using\n# the namespace \"my_namespace\". The Ray workers will run inside a cluster-side\n# copy of the local directory \"files/my_project\", in a Python environment with\n# `toolz` and `requests` installed.\nray.init(\n    \"ray://1.2.3.4:10001\",\n    namespace=\"my_namespace\",\n    runtime_env={\"working_dir\": \"files/my_project\", \"pip\": [\"toolz\", \"requests\"]},\n)\n#....\n```\n\n----------------------------------------\n\nTITLE: Netty TCP Transmission Size Goal Stack Trace\nDESCRIPTION: Call stack trace demonstrating Netty's network I/O path from NIO event loop through the TCP transmission size calculation. This trace shows how data flows from Java application code through JVM internals to kernel TCP stack focusing on transmission size optimizations.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_35\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];tcp_send_mss_[k];tcp_xmit_size_goal_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Versions and Hashes\nDESCRIPTION: This snippet shows how to specify Python package dependencies with exact versions and SHA256 hashes for security. It includes packages like starlette, tensorboardx, and typer, along with their specific versions and hash values.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_25\n\nLANGUAGE: Text\nCODE:\n```\nstarlette==0.37.2 \\\n    --hash=sha256:6fe59f29268538e5d0d182f2791a479a0c64638e6935d1c6989e63fb2699c6ee \\\n    --hash=sha256:9af890290133b79fc3db55474ade20f6220a364a0402e0b556e7cd5e1e093823\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   fastapi\ntensorboardx==2.6.2.2 \\\n    --hash=sha256:160025acbf759ede23fd3526ae9d9bfbfd8b68eb16c38a010ebe326dc6395db8 \\\n    --hash=sha256:c6476d7cd0d529b0b72f4acadb1269f9ed8b22f441e87a84f2a3b940bb87b666\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\ntyper==0.12.3 \\\n    --hash=sha256:070d7ca53f785acbccba8e7d28b08dcd88f79f1fbda035ade0aecec71ca5c914 \\\n    --hash=sha256:49e73131481d804288ef62598d97a1ceef3058905aa536a1134f90891ba35482\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Resource Status and Trial Results Table\nDESCRIPTION: A comprehensive status display showing node memory usage, resource allocation, and a table of completed trials with their performance metrics including iteration count, total time, timesteps, and reward scores.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.5/rllib_regression.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n== Status ==\nMemory usage on this node: 15.5/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/64 CPUs, 0.0/8 GPUs, 0.0/323.54 GiB heap, 0.0/98.39 GiB objects\nResult logdir: /home/ubuntu/ray_results/apex\nResult logdir: /home/ubuntu/ray_results/atari-a2c\nResult logdir: /home/ubuntu/ray_results/atari-basic-dqn\nResult logdir: /home/ubuntu/ray_results/atari-impala\nResult logdir: /home/ubuntu/ray_results/atari-ppo-tf\nResult logdir: /home/ubuntu/ray_results/atari-ppo-torch\nNumber of trials: 24 (24 TERMINATED)\n+-------------------------------------+------------+-------+--------+------------------+---------+----------+\n| Trial name                          | status     | loc   |   iter |   total time (s) |      ts |   reward |\n|-------------------------------------+------------+-------+--------+------------------+---------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_00000 | TERMINATED |       |    300 |          3609.38 | 7596000 |   371.22 |\n| IMPALA_BreakoutNoFrameskip-v4_00001 | TERMINATED |       |    300 |          3604.49 | 7561500 |   361.02 |\n| IMPALA_BreakoutNoFrameskip-v4_00002 | TERMINATED |       |    300 |          3605.28 | 7581500 |   358.73 |\n| IMPALA_BreakoutNoFrameskip-v4_00003 | TERMINATED |       |    300 |          3604.35 | 7567500 |   336.82 |\n| PPO_BreakoutNoFrameskip-v4_00004    | TERMINATED |       |    682 |          3601.14 | 3410000 |    66.3  |\n| PPO_BreakoutNoFrameskip-v4_00005    | TERMINATED |       |    951 |          3602.81 | 4755000 |    36.08 |\n| PPO_BreakoutNoFrameskip-v4_00006    | TERMINATED |       |    951 |          3601.49 | 4755000 |    72.78 |\n| PPO_BreakoutNoFrameskip-v4_00007    | TERMINATED |       |    950 |          3601.34 | 4750000 |    38.93 |\n| PPO_BreakoutNoFrameskip-v4_00008    | TERMINATED |       |     27 |          3726.15 |  135000 |     5.77 |\n| PPO_BreakoutNoFrameskip-v4_00009    | TERMINATED |       |     27 |          3626.05 |  135000 |     2.16 |\n| PPO_BreakoutNoFrameskip-v4_00010    | TERMINATED |       |     22 |          3645.07 |  110000 |     2.84 |\n| PPO_BreakoutNoFrameskip-v4_00011    | TERMINATED |       |     22 |          3622.54 |  110000 |     2.32 |\n| APEX_BreakoutNoFrameskip-v4_00012   | TERMINATED |       |    106 |          3620.95 | 5372640 |    59.43 |\n| APEX_BreakoutNoFrameskip-v4_00013   | TERMINATED |       |    107 |          3617.3  | 8081280 |    97.22 |\n| APEX_BreakoutNoFrameskip-v4_00014   | TERMINATED |       |     88 |          3601.26 | 6635200 |    79.08 |\n| APEX_BreakoutNoFrameskip-v4_00015   | TERMINATED |       |     89 |          3618.23 | 6583040 |    85.28 |\n| A2C_BreakoutNoFrameskip-v4_00016    | TERMINATED |       |    353 |          3603.02 | 3351500 |   126.25 |\n| A2C_BreakoutNoFrameskip-v4_00017    | TERMINATED |       |    353 |          3607.96 | 3339000 |   125.67 |\n| A2C_BreakoutNoFrameskip-v4_00018    | TERMINATED |       |    351 |          3605.17 | 3306000 |   224.59 |\n| A2C_BreakoutNoFrameskip-v4_00019    | TERMINATED |       |    353 |          3607.46 | 3676000 |   163.83 |\n| DQN_BreakoutNoFrameskip-v4_00020    | TERMINATED |       |     27 |          3654.34 |  280104 |    15.51 |\n| DQN_BreakoutNoFrameskip-v4_00021    | TERMINATED |       |     27 |          3660.45 |  280104 |    17.15 |\n| DQN_BreakoutNoFrameskip-v4_00022    | TERMINATED |       |     27 |          3615    |  280104 |    15.37 |\n| DQN_BreakoutNoFrameskip-v4_00023    | TERMINATED |       |     28 |          3655.79 |  290108 |    16.02 |\n+-------------------------------------+------------+-------+--------+------------------+---------+----------+\n```\n\n----------------------------------------\n\nTITLE: Dependency Hash Requirements\nDESCRIPTION: Package dependency specifications with SHA256 hash values for package verification. The file format follows pip requirements syntax with hash verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:a011a644f6d7d03736214d38832e030d8268bcff4a41f728e6030325fea3e400 \\\n--hash=sha256:a2913c5375154b6ef2e91c10b5720ea6e21007412f6437504ffea2109b5a33d7 \\\n--hash=sha256:a30596bae9403a342c978fb47d9b0ee277699fa53bbafad14706af51fe543d16 \\\n--hash=sha256:b03c2ae5d2f0fc05f9a2c0c997e1bc18c8229f392234e8a0194f202169ccd278\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with SHA256 Hashes\nDESCRIPTION: This snippet contains a list of Python package dependencies with their SHA256 hash values. The file uses pip's hash verification format to ensure package integrity during installation, with each hash value corresponding to different distribution formats of the packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_34\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:81b19725065dcb43df02b37e03278c011a09e49757287dca60c5aecdd5a0b8ed \\\n--hash=sha256:833b58d5d0b7e5b9832869f039203389ac7cbf01765639c7309fd50ef619e0b1 \\\n--hash=sha256:88bd7b6bd70a5b6803c1abf6bca012f7ed963e58c68d76ee20b9d751c74a3248 \\\n--hash=sha256:8ad85f7f4e20964db4daadcab70b47ab05c7c1cf2a7c1e51087bfaa83831854c \\\n--hash=sha256:8c0ce1e99116d5ab21355d8ebe53d9460366704ea38ae4d9f6933188f327b456 \\\n--hash=sha256:8d649d616e5c6a678b26d15ece345354f7c2286acd6db868e65fcc5ff7c24a77 \\\n--hash=sha256:903500616422a40a98a5a3c4ff4ed9d0066f3b4c951fa286018ecdf0750194ef \\\n--hash=sha256:9736af4641846491aedb3c3f56b9bc5568d92b0692303b5a305301a95dfd38b1 \\\n--hash=sha256:988635d122aaf2bdcef9e795435662bcd65b02f4f4c1ae37fbee7401c440b3a7 \\\n--hash=sha256:9cca3c2cdadb362116235fdbd411735de4328c61425b0aa9f872fd76d02c4e86 \\\n--hash=sha256:9e0fd32e0148dd5dea6af5fee42beb949098564cc23211a88d799e434255a1f4 \\\n--hash=sha256:9f3e6f9e05148ff90002b884fbc2a86bd303ae847e472f44ecc06c2cd2fcdb2d \\\n--hash=sha256:a85d2b46be66a71bedde836d9e41859879cc54a2a04fad1191eb50c2066f6e9d \\\n--hash=sha256:a9008dad07d71f68487c91e96579c8567c98ca4c3881b9b113bc7b33e9fd78b8 \\\n--hash=sha256:a9a52172be0b5aae932bef82a79ec0a0ce87288c7d132946d645eba03f0ad8a8 \\\n--hash=sha256:aa31fdcc33fef9eb2552cbcbfee7773d5a6792c137b359e82879c101e98584c5 \\\n--hash=sha256:acae32e13a4153809db37405f5eba5bac5fbe2e2ba61ab227926a22901051c0a \\\n--hash=sha256:b014c23646a467558be7da3d6b9fa409b2c567d2110599b7cf9a0c5992b3b471 \\\n--hash=sha256:b21bb4c09ffabfa0e85e3a6b623e19b80e7acd709b9f91452b8297ace2a8ab00 \\\n--hash=sha256:b5901a312f4d14c59918c221323068fad0540e34324925c8475263841dbdfe68 \\\n--hash=sha256:b9b7a708dd92306328117d8c4b62e2194d00c365f18eff11a9b53c6f923b01e3 \\\n--hash=sha256:d1967f46ea8f2db647c786e78d8cc7e4313dbd1b0aca360592d8027b8508e24d \\\n--hash=sha256:d52a25136894c63de15a35bc0bdc5adb4b0e173b9c0d07a2be9d3ca64a332735 \\\n--hash=sha256:d77c85fedff92cf788face9bfa3ebaa364448ebb1d765302e9af11bf449ca36d \\\n--hash=sha256:d79d7d5dc8a32b7093e81e97dad755127ff77bcc899e845f41bf71747af0c569 \\\n--hash=sha256:dbcda74c67263139358f4d188ae5faae95c30929281bc6866d00573783c422b7 \\\n--hash=sha256:ddaea91abf8b0d13443f6dac52e89051a5063c7d014710dcb4d4abb2ff811a59 \\\n--hash=sha256:dee0ce50c6a2dd9056c20db781e9c1cfd33e77d2d569f5d1d9321c641bb903d5 \\\n--hash=sha256:dee60e1de1898bde3b238f18340eec6148986da0455d8ba7848d50470a7a32fb \\\n--hash=sha256:e2f83e18fe2f4c9e7db597e988f72712c0c3676d337d8b101f6758107c42425b \\\n--hash=sha256:e3fb1677c720409d5f671e39bac6c9e0e422584e5f518bfd50aa4cbbea02433f \\\n--hash=sha256:ecee4132c6cd2ce5308e21672015ddfed1ff975ad0ac8d27168ea82e71413f55 \\\n--hash=sha256:ee2b1b1769f6707a8a445162ea16dddf74285c3964f605877a20e38545c3c462 \\\n--hash=sha256:ee6acae74a2b91865910eef5e7de37dc6895ad96fa23603d1d27ea69df545015 \\\n--hash=sha256:ef3f72c9666bba2bab70d2a8b79f2c6d2c1a42a7f7e2b0ec83bb2f9e383950af\n# via\n#   -c /tmp/ray-deps/requirements_compiled.txt\n#   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Smoke Test Settings for TorchTrainer\nDESCRIPTION: This code snippet modifies the scaling configuration for testing purposes, setting up CPU-only training with 8 workers and reducing the number of epochs to 1 when in smoke test mode.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif SMOKE_TEST:\n    scaling_config = ScalingConfig(\n        num_workers=8, use_gpu=False, resources_per_worker={\"CPU\": 1}\n    )\n    train_loop_config[\"num_epochs\"] = 1\n```\n\n----------------------------------------\n\nTITLE: Using allreduce Collective Communication in Python\nDESCRIPTION: Example showing how to perform an allreduce operation across multiple workers in a collective group. The example demonstrates creating two GPU workers and performing collective communication.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-more-libs/ray-collective.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport cupy\nimport ray.util.collective as col\n\n\n@ray.remote(num_gpus=1)\nclass Worker:\n    def __init__(self):\n        self.buffer = cupy.ones((10,), dtype=cupy.float32)\n\n    def compute(self):\n        col.allreduce(self.buffer, \"default\")\n        return self.buffer\n\n# Create two actors A and B and create a collective group following the previous example...\nA = Worker.remote()\nB = Worker.remote()\n# Invoke allreduce remotely\nray.get([A.compute.remote(), B.compute.remote()])\n```\n\n----------------------------------------\n\nTITLE: XGBoost with Dynamic Resources in Ray Tune\nDESCRIPTION: This example demonstrates how to train an XGBoost model using Ray Tune with dynamic resource allocation. The script showcases how to define a training function, set up the Tune search space, and run the optimization process. Dynamic resources are automatically managed by Ray Tune during the training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/xgboost_dynamic_resources_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\n.. literalinclude:: /../../python/ray/tune/examples/xgboost_dynamic_resources_example.py\n\n```\n\n----------------------------------------\n\nTITLE: Package Installation Command with UV\nDESCRIPTION: Command to compile Python requirements using UV package manager with specific configuration for Ray project with CUDA 12.1 support\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cu121 --find-links https://data.pyg.org/whl/torch-2.5.1+cu121.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c python/requirements_compiled_ray_test_py311_cu121.txt python/requirements.txt -o python/requirements_compiled_ray_py311_cu121.txt\n```\n\n----------------------------------------\n\nTITLE: Disabling Strict Metric Checking in Ray Tune\nDESCRIPTION: Setting TUNE_DISABLE_STRICT_METRIC_CHECKING to 1 disables the check that verifies metrics specified in Tuner() are actually reported in session.report().\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/env.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nTUNE_DISABLE_STRICT_METRIC_CHECKING=1\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with Hashes\nDESCRIPTION: Detailed listing of Python package dependencies with their versions and SHA256 hashes for security verification\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   aiohttp\n```\n\n----------------------------------------\n\nTITLE: Declaring Ray Java Dependencies in Maven\nDESCRIPTION: These XML snippets declare dependencies for Ray's API and runtime within a Maven project.  They specify the groupId, artifactId, and version, which is dynamically set by the `${ray.version}` property.  These dependencies are required to build and run Java code using Ray in local mode.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/installation.rst#2025-04-12_snippet_21\n\nLANGUAGE: xml\nCODE:\n```\n    <dependency>\n      <groupId>io.ray</groupId>\n      <artifactId>ray-api</artifactId>\n      <version>${ray.version}</version>\n    </dependency>\n    <dependency>\n      <groupId>io.ray</groupId>\n      <artifactId>ray-runtime</artifactId>\n      <version>${ray.version}</version>\n    </dependency>\n```\n\n----------------------------------------\n\nTITLE: Autoregressive Actions Example\nDESCRIPTION: Shows usage of autoregressive actions with RLlib, further details can be found in the referenced documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Auto-regressive actions\n# More information available in documentation.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Setting Model Name for Dolly-v2-7b\nDESCRIPTION: Defines the model name for loading the pre-trained dolly-v2-7b model from Hugging Face model hub. This 7B parameter model was derived from EleutherAI's Pythia-6.9b and fine-tuned on an instruction corpus.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMODEL_NAME = \"databricks/dolly-v2-7b\"\n```\n\n----------------------------------------\n\nTITLE: Installing PyZMQ 26.0.3 with Hash Verification\nDESCRIPTION: Package specification for PyZMQ 26.0.3 with SHA-256 hashes for secure installation. This version includes numerous hash values for verification across different platforms and architectures.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_43\n\nLANGUAGE: plaintext\nCODE:\n```\npyzmq==26.0.3 \\\n    --hash=sha256:01fbfbeb8249a68d257f601deb50c70c929dc2dfe683b754659569e502fbd3aa \\\n    --hash=sha256:0270b49b6847f0d106d64b5086e9ad5dc8a902413b5dbbb15d12b60f9c1747a4 \\\n    --hash=sha256:03c0ae165e700364b266876d712acb1ac02693acd920afa67da2ebb91a0b3c09 \\\n    --hash=sha256:068ca17214038ae986d68f4a7021f97e187ed278ab6dccb79f837d765a54d753 \\\n    --hash=sha256:082a2988364b60bb5de809373098361cf1dbb239623e39e46cb18bc035ed9c0c \\\n    --hash=sha256:0aaf982e68a7ac284377d051c742610220fd06d330dcd4c4dbb4cdd77c22a537 \\\n    --hash=sha256:0c0991f5a96a8e620f7691e61178cd8f457b49e17b7d9cfa2067e2a0a89fc1d5 \\\n    --hash=sha256:115f8359402fa527cf47708d6f8a0f8234f0e9ca0cab7c18c9c189c194dbf620 \\\n    --hash=sha256:15c59e780be8f30a60816a9adab900c12a58d79c1ac742b4a8df044ab2a6d920 \\\n    --hash=sha256:1b7d0e124948daa4d9686d421ef5087c0516bc6179fdcf8828b8444f8e461a77 \\\n    --hash=sha256:1c8eb19abe87029c18f226d42b8a2c9efdd139d08f8bf6e085dd9075446db450 \\\n    --hash=sha256:204e0f176fd1d067671157d049466869b3ae1fc51e354708b0dc41cf94e23a3a \\\n    --hash=sha256:2136f64fbb86451dbbf70223635a468272dd20075f988a102bf8a3f194a411dc \\\n    --hash=sha256:2b291d1230845871c00c8462c50565a9cd6026fe1228e77ca934470bb7d70ea0 \\\n    --hash=sha256:2c18645ef6294d99b256806e34653e86236eb266278c8ec8112622b61db255de \\\n    --hash=sha256:2cc4e280098c1b192c42a849de8de2c8e0f3a84086a76ec5b07bfee29bda7d18 \\\n    --hash=sha256:2ed8357f4c6e0daa4f3baf31832df8a33334e0fe5b020a61bc8b345a3db7a606 \\\n    --hash=sha256:3191d312c73e3cfd0f0afdf51df8405aafeb0bad71e7ed8f68b24b63c4f36500 \\\n    --hash=sha256:3401613148d93ef0fd9aabdbddb212de3db7a4475367f49f590c837355343972 \\\n    --hash=sha256:34106f68e20e6ff253c9f596ea50397dbd8699828d55e8fa18bd4323d8d966e6 \\\n    --hash=sha256:3516119f4f9b8671083a70b6afaa0a070f5683e431ab3dc26e9215620d7ca1ad \\\n    --hash=sha256:38ece17ec5f20d7d9b442e5174ae9f020365d01ba7c112205a4d59cf19dc38ee \\\n    --hash=sha256:3b4032a96410bdc760061b14ed6a33613ffb7f702181ba999df5d16fb96ba16a \\\n    --hash=sha256:3bf8b000a4e2967e6dfdd8656cd0757d18c7e5ce3d16339e550bd462f4857e59 \\\n    --hash=sha256:3e3070e680f79887d60feeda051a58d0ac36622e1759f305a41059eff62c6da7 \\\n    --hash=sha256:4496b1282c70c442809fc1b151977c3d967bfb33e4e17cedbf226d97de18f709 \\\n    --hash=sha256:44dd6fc3034f1eaa72ece33588867df9e006a7303725a12d64c3dff92330f625 \\\n    --hash=sha256:4adfbb5451196842a88fda3612e2c0414134874bffb1c2ce83ab4242ec9e027d \\\n    --hash=sha256:4b7c0c0b3244bb2275abe255d4a30c050d541c6cb18b870975553f1fb6f37527 \\\n    --hash=sha256:4c82a6d952a1d555bf4be42b6532927d2a5686dd3c3e280e5f63225ab47ac1f5 \\\n    --hash=sha256:5344b896e79800af86ad643408ca9aa303a017f6ebff8cee5a3163c1e9aec987 \\\n    --hash=sha256:5bde86a2ed3ce587fa2b207424ce15b9a83a9fa14422dcc1c5356a13aed3df9d \\\n    --hash=sha256:5bf6c237f8c681dfb91b17f8435b2735951f0d1fad10cc5dfd96db110243370b \\\n    --hash=sha256:5dbb9c997932473a27afa93954bb77a9f9b786b4ccf718d903f35da3232317de \\\n    --hash=sha256:69ea9d6d9baa25a4dc9cef5e2b77b8537827b122214f210dd925132e34ae9b12 \\\n    --hash=sha256:6b3146f9ae6af82c47a5282ac8803523d381b3b21caeae0327ed2f7ecb718798 \\\n    --hash=sha256:6bcb34f869d431799c3ee7d516554797f7760cb2198ecaa89c3f176f72d062be \\\n    --hash=sha256:6ca08b840fe95d1c2bd9ab92dac5685f949fc6f9ae820ec16193e5ddf603c3b2 \\\n    --hash=sha256:6ca7a9a06b52d0e38ccf6bca1aeff7be178917893f3883f37b75589d42c4ac20 \\\n    --hash=sha256:703c60b9910488d3d0954ca585c34f541e506a091a41930e663a098d3b794c67 \\\n    --hash=sha256:715bdf952b9533ba13dfcf1f431a8f49e63cecc31d91d007bc1deb914f47d0e4 \\\n    --hash=sha256:72b67f966b57dbd18dcc7efbc1c7fc9f5f983e572db1877081f075004614fcdd \\\n    --hash=sha256:74423631b6be371edfbf7eabb02ab995c2563fee60a80a30829176842e71722a \\\n    --hash=sha256:77a85dca4c2430ac04dc2a2185c2deb3858a34fe7f403d0a946fa56970cf60a1 \\\n    --hash=sha256:7821d44fe07335bea256b9f1f41474a642ca55fa671dfd9f00af8d68a920c2d4 \\\n    --hash=sha256:788f15721c64109cf720791714dc14afd0f449d63f3a5487724f024345067381 \\\n    --hash=sha256:7ca684ee649b55fd8f378127ac8462fb6c85f251c2fb027eb3c887e8ee347bcd \\\n    --hash=sha256:7daa3e1369355766dea11f1d8ef829905c3b9da886ea3152788dc25ee6079e02 \\\n    --hash=sha256:7e6bc96ebe49604df3ec2c6389cc3876cabe475e6bfc84ced1bf4e630662cb35 \\\n    --hash=sha256:80b12f25d805a919d53efc0a5ad7c0c0326f13b4eae981a5d7b7cc343318ebb7 \\\n    --hash=sha256:871587bdadd1075b112e697173e946a07d722459d20716ceb3d1bd6c64bd08ce \\\n    --hash=sha256:88b88282e55fa39dd556d7fc04160bcf39dea015f78e0cecec8ff4f06c1fc2b5 \\\n    --hash=sha256:8d7a498671ca87e32b54cb47c82a92b40130a26c5197d392720a1bce1b3c77cf \\\n    --hash=sha256:926838a535c2c1ea21c903f909a9a54e675c2126728c21381a94ddf37c3cbddf \\\n    --hash=sha256:971e8990c5cc4ddcff26e149398fc7b0f6a042306e82500f5e8db3b10ce69f84 \\\n    --hash=sha256:9b273ecfbc590a1b98f014ae41e5cf723932f3b53ba9367cfb676f838038b32c \\\n    --hash=sha256:a42db008d58530efa3b881eeee4991146de0b790e095f7ae43ba5cc612decbc5 \\\n    --hash=sha256:a72a84570f84c374b4c287183debc776dc319d3e8ce6b6a0041ce2e400de3f32 \\\n    --hash=sha256:ac97a21de3712afe6a6c071abfad40a6224fd14fa6ff0ff8d0c6e6cd4e2f807a \\\n    --hash=sha256:acb704195a71ac5ea5ecf2811c9ee19ecdc62b91878528302dd0be1b9451cc90 \\\n    --hash=sha256:b32bff85fb02a75ea0b68f21e2412255b5731f3f389ed9aecc13a6752f58ac97 \\\n    --hash=sha256:b3cd31f859b662ac5d7f4226ec7d8bd60384fa037fc02aee6ff0b53ba29a3ba8 \\\n\n```\n\n----------------------------------------\n\nTITLE: Syncing Model Checkpoint from S3\nDESCRIPTION: Downloads model checkpoint from AWS S3 to local storage using system command.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.system(f\"aws s3 sync s3://{checkpoint.path} /mnt/local_storage/\")\n```\n\n----------------------------------------\n\nTITLE: Computer Vision Support for Ray AI Applications\nDESCRIPTION: OpenCV Python headless package for computer vision capabilities without GUI dependencies. This package supports image processing tasks in Ray, particularly for machine learning models that process visual data.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_22\n\nLANGUAGE: pip\nCODE:\n```\nopencv-python-headless==4.11.0.86 \\\n    --hash=sha256:0e0a27c19dd1f40ddff94976cfe43066fbbe9dfbb2ec1907d66c19caef42a57b \\\n    --hash=sha256:48128188ade4a7e517237c8e1e11a9cdf5c282761473383e77beb875bb1e61ca \\\n    --hash=sha256:6c304df9caa7a6a5710b91709dd4786bf20a74d57672b3c31f7033cc638174ca \\\n    --hash=sha256:6efabcaa9df731f29e5ea9051776715b1bdd1845d7c9530065c7951d2a2899eb \\\n    --hash=sha256:996eb282ca4b43ec6a3972414de0e2331f5d9cda2b41091a49739c19fb843798 \\\n    --hash=sha256:a66c1b286a9de872c343ee7c3553b084244299714ebb50fbdcd76f07ebbe6c81 \\\n    --hash=sha256:f447d8acbb0b6f2808da71fddd29c1cdd448d2bc98f72d9bb78a7a898fc9621b\n    # via mistral-common\n```\n\n----------------------------------------\n\nTITLE: Installing rfc3339-validator Package with Hash Verification\nDESCRIPTION: This snippet specifies the installation of rfc3339-validator package version 0.1.4 with SHA256 hash verification. It also lists the packages that depend on this validator in the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_46\n\nLANGUAGE: pip\nCODE:\n```\nrfc3339-validator==0.1.4 \\\n    --hash=sha256:138a2abdf93304ad60530167e51d2dfb9549521a836871b88d7f4695d0022f6b \\\n    --hash=sha256:24f6ec1eda14ef823da9e36ec7113124b39c04d50a4d3d3a3c2859577e7791fa\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   jsonschema\n    #   jupyter-events\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters and Concurrency Limit in Python\nDESCRIPTION: Sets up initial hyperparameters for promising search space areas and configures concurrency limits using ConcurrencyLimiter for HyperOptSearch, allowing up to 4 concurrent trials.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/hyperopt_example.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninitial_params = [\n    {\"width\": 1, \"height\": 2, \"activation\": \"relu\"},\n    {\"width\": 4, \"height\": 2, \"activation\": \"tanh\"},\n]\nalgo = HyperOptSearch(points_to_evaluate=initial_params)\nalgo = ConcurrencyLimiter(algo, max_concurrent=4)\n```\n\n----------------------------------------\n\nTITLE: Ray Performance Metrics Configuration\nDESCRIPTION: JSON configuration containing task execution metrics, runtime information, and deployment URLs for a Ray project. Includes performance data like tasks per second (3.22), total tasks (1000), and execution time (610s).\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.7.0/benchmarks/many_nodes.txt#2025-04-12_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tasks_per_second\": 3.224712885579051,\n  \"num_tasks\": 1000,\n  \"time\": 610.1051273345947,\n  \"success\": \"1\",\n  \"_runtime\": 620.4832813739777,\n  \"_session_url\": \"https://beta.anyscale.com/o/anyscale-internal/projects/prj_2xR6uT6t7jJuu1aCwWMsle/clusters/ses_6f82dxdGaxTV4uZNSamTYGLY\",\n  \"_commit_url\": \"https://s3-us-west-2.amazonaws.com/ray-wheels/releases/1.7.0/2367a2cb9033913b68b1230316496ae273c25b54/ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\",\n  \"_stable\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Java Stack Trace with WrapFactory in Netty/Vert.x Processing\nDESCRIPTION: This stack trace extends the previous one to include Mozilla Rhino's WrapFactory operation, which is responsible for wrapping Java objects for use in JavaScript. This demonstrates the object conversion that occurs when passing values between Java and JavaScript in Vert.x.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_61\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/WrapFactory:.wrap_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries - Python\nDESCRIPTION: This code snippet imports essential libraries and modules necessary for setting up and executing the BOHB-based optimization with Ray Tune.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bohb_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\\nimport time\\nfrom pathlib import Path\\n\\nimport ray\\nfrom ray import tune\\nfrom ray.tune.schedulers.hb_bohb import HyperBandForBOHB\\nfrom ray.tune.search.bohb import TuneBOHB\\nimport ConfigSpace as CS\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PyTorch Lightning and Ray\nDESCRIPTION: This code snippet imports the necessary libraries for the project, including Ray, PyTorch, NumPy, PyTorch Lightning, and Hugging Face Transformers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/lightning_cola_advanced.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ray\nimport torch\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom datasets import load_dataset, load_metric\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Cluster for Tune on AWS (YAML)\nDESCRIPTION: YAML configuration file for setting up a Ray cluster on AWS to run distributed Tune experiments. Specifies instance types, AMI, and other cluster settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-distributed.rst#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Provider-specific config for worker nodes, e.g. instance type.\nworker_nodes:\n    InstanceType: m5.large\n    ImageId: ami-0b294f219d14e6a82 # Deep Learning AMI (Ubuntu) Version 21.0\n\n    # Run workers on spot by default. Comment this out to use on-demand.\n    InstanceMarketOptions:\n        MarketType: spot\n        SpotOptions:\n            MaxPrice: 1.0  # Max Hourly Price\n```\n\n----------------------------------------\n\nTITLE: Handling Read-Only Numpy Arrays in Ray Tasks\nDESCRIPTION: Illustrates the issue of \"assignment destination is read-only\" when working with numpy arrays in Ray tasks, due to zero-copy deserialization. The example shows how this can cause errors and suggests using .copy() as a workaround.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/objects/serialization.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: /ray-core/doc_code/deser.py\n```\n\n----------------------------------------\n\nTITLE: Restoring PyTorch Lightning Training State from Checkpoint\nDESCRIPTION: This snippet demonstrates how to restore training state for a PyTorch Lightning model from a Ray Train checkpoint using the Trainer.fit method.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/user-guides/checkpoints.rst#2025-04-12_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pytorch_lightning as pl\nimport ray.train as train\n\ndef train_func():\n    model = YourLightningModule()\n    trainer = pl.Trainer()\n\n    checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            ckpt_path = os.path.join(checkpoint_dir, \"checkpoint.ckpt\")\n            trainer.fit(model, ckpt_path=ckpt_path)\n    else:\n        trainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Establishing Ray Dashboard Connection with Port Forwarding from AWS Instance\nDESCRIPTION: Terminal output showing the process of connecting to a Ray dashboard by forwarding port 8265 from a remote AWS instance to localhost. The log includes verification of AWS settings, IP fetching, and confirmation of successful connection establishment.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/test_cli_patterns/test_ray_dashboard.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAttempting to establish dashboard locally at http://localhost:8265/ connected to remote port 8265\nChecking AWS environment settings\nFetched IP: .+\nForwarding ports\nubuntu@ip-.+:~$ exit\nSuccessfully established connection.\n```\n\n----------------------------------------\n\nTITLE: Analyzing Netty/Vert.x HTTP Request Processing with NativeFunction.initScriptFunction\nDESCRIPTION: Call stack trace showing the Java execution path from thread start through Netty's NIO event processing to Vert.x HTTP handlers and Mozilla JavaScript runtime. This trace ends with OptRuntime.call2 and NativeFunction.initScriptFunction for JavaScript function initialization.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_74\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/NativeFunction:.initScriptFunction_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Hashed Python Package Dependencies\nDESCRIPTION: A collection of Python package dependencies with pinned versions and SHA256 hash verification. Each entry includes the package name, version, hash values, and dependency chain information in comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\ntensorboardx==2.6.2.2 \\\n    --hash=sha256:160025acbf759ede23fd3526ae9d9bfbfd8b68eb16c38a010ebe326dc6395db8 \\\n    --hash=sha256:c6476d7cd0d529b0b72f4acadb1269f9ed8b22f441e87a84f2a3b940bb87b666\n```\n\n----------------------------------------\n\nTITLE: PB2 Example with Ray Tune\nDESCRIPTION: This Python script demonstrates the usage of PB2 (Population Based Bandits) within the Ray Tune framework. It showcases how to integrate PB2 for hyperparameter optimization and bandit-style exploration. The script provides a runnable example for understanding the practical application of PB2.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/pb2_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"/../../python/ray/tune/examples/pb2_example.py\"\n```\n\n----------------------------------------\n\nTITLE: Running Ray Shuffle on Single Node with 32 CPUs\nDESCRIPTION: Executes Ray's experimental shuffle operation on a single node with 32 CPUs, 200 partitions, 500MB partition size, and 20GB object store memory. Shows progress bars and memory statistics.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.4.0/data_processing_tests/streaming_shuffle.txt#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m ray.experimental.shuffle --num-cpus=32 --num-partitions=200 --partition-size=500e6 --object-store-memory=20e9\n```\n\n----------------------------------------\n\nTITLE: Initializing Model Paths for Stable Diffusion\nDESCRIPTION: Defines paths for the fine-tuned model checkpoint and optional LoRA-specific model paths. These variables need to be configured before running the generation pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/playground.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nTUNED_MODEL_PATH = \"/tmp/model-tuned\"\nORIG_MODEL_PATH = \"/tmp/model-orig/models--CompVis--stable-diffusion-v1-4/snapshots/b95be7d6f134c3a9e62ee616f310733567f069ce/\"\nLORA_WEIGHTS_DIR = \"/tmp/model-tuned\"\n```\n\n----------------------------------------\n\nTITLE: Creating a hidden toctree in Markdown/Sphinx\nDESCRIPTION: A hidden table of contents tree configuration for Sphinx documentation that lists all the KubeRay ecosystem integration pages without displaying them in the main content.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n\nk8s-ecosystem/ingress\nk8s-ecosystem/prometheus-grafana\nk8s-ecosystem/pyspy\nk8s-ecosystem/volcano\nk8s-ecosystem/yunikorn\nk8s-ecosystem/kueue\nk8s-ecosystem/istio\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Project Environment in YAML\nDESCRIPTION: Defines environment variables and runtime settings for a Ray project. It includes cluster name, Python version, and various Ray-specific configurations.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/project_files/project1/requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: ray_project\n\nenvironment_variables:\n  RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER: 1\n\nruntime_env:\n  env_vars:\n    RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER: 1\n    RAY_memory_monitor_refresh_ms: 0\n  py_modules:\n    - python-lib\n  conda: \"./.conda/env.yaml\"\n  pip: \"./requirements.txt\"\n  pythonpath: \".:..\"\n  working_dir: \".\"\n\ncluster:\n  app_config: app_config.yaml\n  compute_template: compute_tpl.yaml\n\n_version: \"1.0.5\"\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with SHA256 Hashes\nDESCRIPTION: Detailed package dependency specifications including version pins and SHA256 hash verifications. Includes dependency chain comments showing which packages require each dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_18\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:cbebcd5bcaf1eaf302617c114aa67569dd3f090dd0ce8ba9e35e9985b41ac35b \\\n--hash=sha256:cd6c8fca38178e12c00418de737aef1261576bd1b6e8c6134d3e729a4e858b38 \\\n--hash=sha256:ceb3b7e6a0135e092de86110c5a74e46bda4bd4fbfeeb3a3bcec79c0f861e450\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenCensus Dependency with Hash Verification\nDESCRIPTION: Defines the opencensus package dependency with version 0.11.3 and includes SHA256 hash validations. The package is referenced from requirements_compiled_ray_test_py311_cpu.txt and the main requirements.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nopencensus==0.11.3 \\\n    --hash=sha256:9c33d572059f0f0e874fc34c697a39a4193aa9cf3203f7e777df42e9edeea56a \\\n    --hash=sha256:af7a98bd51e63968144d772f346d696ed498a32dbdc4be267cd6011c4ce05da8\n```\n\n----------------------------------------\n\nTITLE: Pulling Docker Image for DreamBooth Fine-Tuning\nDESCRIPTION: This bash command pulls the Docker image used for DreamBooth fine-tuning from the Anyscale workspace templates repository. This image can be used as a base for building custom environments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/README.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull us-docker.pkg.dev/anyscale-workspace-templates/workspace-templates/dreambooth-finetuning:latest\n```\n\n----------------------------------------\n\nTITLE: Setting Ray Dashboard Address\nDESCRIPTION: Environment variable setup for connecting to Ray cluster dashboard.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/quickstart.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport RAY_ADDRESS=\"http://127.0.0.1:8265\"\n```\n\n----------------------------------------\n\nTITLE: Starting TensorBoard to Visualize Ray Tune Results (Bash)\nDESCRIPTION: This command starts TensorBoard to visualize results from Ray Tune trials. It specifies the log directory where Ray Tune stores the trial data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n\"tensorboard --logdir ~/ray_results\"\n```\n\n----------------------------------------\n\nTITLE: Defining pyasn1 and pyasn1-modules Dependencies\nDESCRIPTION: This snippet specifies both pyasn1 and pyasn1-modules dependencies with their respective versions and hash verifications. These are required by various authentication-related packages including oauth2client, rsa, and Google Auth for handling ASN.1 data structures.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_33\n\nLANGUAGE: pip\nCODE:\n```\npyasn1==0.5.1 \\\n    --hash=sha256:4439847c58d40b1d0a573d07e3856e95333f1976294494c325775aeca506eb58 \\\n    --hash=sha256:6d391a96e59b23130a5cfa74d6fd7f388dbbe26cc8f1edf39fdddf08d9d6676c\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   oauth2client\n    #   pyasn1-modules\n    #   rsa\npyasn1-modules==0.3.0 \\\n    --hash=sha256:5bd01446b736eb9d31512a30d46c1ac3395d676c6f3cafa4c03eb54b9925631c \\\n    --hash=sha256:d3ccd6ed470d9ffbc716be08bd90efbd44d0734bc9303818f7336070984a162d\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-auth\n    #   oauth2client\n```\n\n----------------------------------------\n\nTITLE: Verifying Zip File Contents\nDESCRIPTION: Bash command to check the contents of a zip file and ensure it contains only a single top-level directory\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nzipinfo -1 zip_file_name.zip\n# example_dir/\n# example_dir/my_file_1.txt\n# example_dir/subdir/my_file_2.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry Exporter OTLP Package with Hash Verification in Bash\nDESCRIPTION: Defines the OpenTelemetry exporter OTLP package version 1.1.0 with SHA256 hash verification for secure package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-exporter-otlp==1.1.0 \\\n    --hash=sha256:2a2135f87cdad417408d34fc6131879d5cee1d7af7546b4a1f67fd178b262f4e \\\n    --hash=sha256:61ee0a6e9a12dd7191aedca34a8a3e7cc4e8e92504a71adf390b6d2bcc36d0d4\n```\n\n----------------------------------------\n\nTITLE: Specifying prometheus-fastapi-instrumentator Package Dependency\nDESCRIPTION: This snippet specifies the prometheus-fastapi-instrumentator package version 7.0.2 with SHA-256 hashes for verification. It indicates that this dependency is required by vllm in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_35\n\nLANGUAGE: plaintext\nCODE:\n```\nprometheus-fastapi-instrumentator==7.0.2 \\\n    --hash=sha256:8a4d8fb13dbe19d2882ac6af9ce236e4e1f98dc48e3fa44fe88d8e23ac3c953f \\\n    --hash=sha256:975e39992acb7a112758ff13ba95317e6c54d1bbf605f9156f31ac9f2800c32d\n    # via vllm\n```\n\n----------------------------------------\n\nTITLE: Building a Single Documentation Subproject\nDESCRIPTION: Command to build only one part of the Ray documentation, such as Tune or Train, to speed up development when only working on a specific section.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/README.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nDOC_LIB=<project> sphinx-build -b html -d _build/doctrees  source _build/html\n```\n\n----------------------------------------\n\nTITLE: UV Pip Compile Command for Ray Dependencies\nDESCRIPTION: Command to compile Python package dependencies with specific constraints and settings for the Ray project. Uses uv pip to generate hashes and handles multiple requirement files.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nuv pip compile --generate-hashes --strip-extras --unsafe-package ray --unsafe-package grpcio-tools --unsafe-package setuptools --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cpu --find-links https://data.pyg.org/whl/torch-2.5.1+cpu.html --index-strategy unsafe-best-match --no-strip-markers --emit-index-url --emit-find-links -c /tmp/ray-deps/requirements_compiled.txt python/requirements.txt python/requirements/cloud-requirements.txt python/requirements/base-test-requirements.txt -o python/requirements_compiled_ray_test_py311_cpu.txt\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop with TCP Queue Cleaning\nDESCRIPTION: Stack trace showing Netty network operations with TCP retransmission queue management. This trace demonstrates how the TCP stack processes acknowledgments and cleans up the retransmission queue, which is essential for connection reliability and performance optimization.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_19\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k];ip_rcv_[k];ip_rcv_finish_[k];ip_local_deliver_[k];ip_local_deliver_finish_[k];tcp_v4_rcv_[k];tcp_v4_do_rcv_[k];tcp_rcv_established_[k];tcp_ack_[k];tcp_clean_rtx_queue_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Trainable Debugging Functions\nDESCRIPTION: Documentation listing for Tune's debugging utilities including serialization diagnosis and validation functions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_9\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n    :toctree: doc/\n\n    tune.utils.diagnose_serialization\n    tune.utils.validate_save_restore\n    tune.utils.util.validate_warmstart\n```\n\n----------------------------------------\n\nTITLE: Setting CPU Device for Smoke Test Mode\nDESCRIPTION: This conditional snippet changes the device from GPU to CPU when running in smoke test mode, ensuring the code can be tested in environments without GPU access.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/pytorch_resnet_finetune.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nif SMOKE_TEST:\n    device = torch.device(\"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Ingress for Ray Cluster in YAML\nDESCRIPTION: This YAML snippet shows the Ingress configuration required to resolve connection issues when using nginx-ingress-controller with a Ray Cluster. It enables underscores in headers and ignores invalid headers.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/ray-client.rst#2025-04-12_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n  annotations:\n     nginx.ingress.kubernetes.io/server-snippet: |\n       underscores_in_headers on;\n       ignore_invalid_headers on;\n```\n\n----------------------------------------\n\nTITLE: Getting Metrics from SingleAgentEnvRunner - Python\nDESCRIPTION: This method retrieves performance metrics for the agent's interactions with the environment. Metrics can include various statistics such as average reward and episode length, crucial for evaluating the agent's performance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_env_runner.rst#2025-04-12_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef get_metrics(self):\n    # Get the performance metrics of the agent\n    pass\n```\n\n----------------------------------------\n\nTITLE: Calculate max steps per epoch\nDESCRIPTION: Calculates the maximum number of steps per epoch based on the dataset size, batch size, and number of workers. This value is used to configure the training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"max_steps_per_epoch = ray_datasets[\\\"train\\\"].count() // (batch_size * num_workers)\"\n```\n\n----------------------------------------\n\nTITLE: Package SHA-256 Hash Verification for Ray Dependencies\nDESCRIPTION: A comprehensive list of SHA-256 hash values for Python package dependencies used in the Ray project. These hashes ensure the integrity and security of packages during installation using pip's hash verification feature.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:1dc93e8e4653bdb5910aed79f11e165c85732067614f180f70534f056da97db3 \\\n--hash=sha256:1e2d69948e4132813b8d1131f29f9101bc2c915f26089a6d632001a5c1349672 \\\n--hash=sha256:235a31ec7db685f5c82233bddf9858748b89b8119bf4538d514536c485c15fe0 \\\n--hash=sha256:27dcd6f46a21c18fa5e5deed92a43d4554e3df8d8ca5a47bf0615d6a5f39dbc9 \\\n--hash=sha256:28efb066cde83c479dfe5a48141a53bc7e5f13f785b92ddde336c716663039ee \\\n--hash=sha256:3476fae43db72bd11f29a5147ae2f3cb22e2f1a91d575ef130d2bf49afd21c46 \\\n--hash=sha256:36e17c4592231a7dbd2ed09027823ab295d2791b3b1efb2aee874b10548b7524 \\\n--hash=sha256:384d779f0d6f1b110eae74cb0659d9aa6ff35aaf547b3955abf2ab4c901c4819 \\\n--hash=sha256:38949d30b11ae5f95c3c91917ee7a6b239f5ec276f271f28638dec9156f82cfc \\\n--hash=sha256:3967e4ad1aa9da62fd53e346ed17d7b2e922cba5ab93bdd46febcac39be636fc \\\n--hash=sha256:3e7bf4442b310ff154b7bb9d81eb2c016b7d597e364f97d72b1acc3817a0fdc1 \\\n--hash=sha256:3f0c8c6dfa6605ab8ff0611995ee30d4f9fcff89966cf562733b4008a3d60d82 \\\n--hash=sha256:484ae3240666ad34cfa31eea7b8c6cd2f1fdaae21d73ce2974211df099a95d81 \\\n--hash=sha256:4a7b4f35de6a304b5533c238bee86b670b75b03d31b7797929caa7a624b5dda6 \\\n--hash=sha256:4cb14ce54d9b857be9591ac364cb08dc2d6a5c4318c1182cb1d02274029d590d \\\n--hash=sha256:4e71bc4416de195d6e9b4ee93ad3f2f6b2ce11d042b4d7a7ee00bbe0358bd0c2 \\\n--hash=sha256:52700dc63a4676669b341ba33520f4d6e43d3ca58d422e22ba66d1736b0a6e4c \\\n--hash=sha256:572efc93db7a4d27e404501975ca6d2d9775705c2d922390d878fcf768d92c87 \\\n--hash=sha256:576eb384292b139821c41995523654ad82d1916da6a60cff129c715a6223ea84 \\\n--hash=sha256:5b0bf0effb196ed76b7ad883848143427a73c355ae8e569fa538365064188b8e \\\n--hash=sha256:5b6ccc0c85916998d788b295765ea0e9cb9aac7e4a8ed71d12e7d8ac31c23c95 \\\n--hash=sha256:5ed82f5a7af3697b1c4786053736f24a0efd0a1b8a130d4c7bfee4b9ded0f08f \\\n--hash=sha256:6d4c80667de2e36970ebf74f42d1088cc9ee7ef5f4e8c35eee1b40eafd33ca5b \\\n--hash=sha256:730076207cb816138cf1af7f7237b208340a2c5e749707457d70705715c93b93 \\\n--hash=sha256:7687e22a31e976a0e7fc99c2f4d11ca45eff652a81eb8c8085e9609298916dcf \\\n--hash=sha256:822ea70dc4018c7e6223f13affd1c5c30c0f5c12ac1f96cd8e9949acddb48a61 \\\n--hash=sha256:84b0daf226913133f899ea9b30618722d45feffa67e4fe867b0b5ae83a34060c \\\n--hash=sha256:85765fdf4b27eb5086f05ac0491090fc76f4f2b28e09d9350c31aac25a5aaff8 \\\n--hash=sha256:8dd178c4c80706546702c59529ffc005681bd6dc2ea234c450661b205445a34d \\\n--hash=sha256:8f5b234f567cf76ee489502ceb7165c2a5cecec081db2b37e35332b537f8157c \\\n--hash=sha256:98bbd754a422a0b123c66a4c341de0474cad4a5c10c164ceed6ea090f3563db4 \\\n--hash=sha256:993584fc821c58d5993521bfdcd31a4adf025c7d745bbd4d12ccfecf695af5ba \\\n--hash=sha256:a40821a89dc373d6427e2b44b572efc36a2778d3f543299e2f24eb1a5de65415 \\\n--hash=sha256:b291f0ee7961a597cbbcc77709374087fa2a9afe7bdb6a40dbbd9b127e79afee \\\n--hash=sha256:b573a43ef7c368ba4ea06050a957c2a7550f729c31f11dd616d2ac4aba99888d \\\n--hash=sha256:b610ff0f24e9f11c9ae653c67ff8cc03c075131401b3e5ef4b82570d1728f8a9 \\\n--hash=sha256:bdf38ba2d393c7911ae989c3bbba510ebbcdf4ecbdbfec36272abe350c454075 \\\n--hash=sha256:bfef2bb6ef068827bbd021017a107194956918ab43ce4d6dc945ffa13efbc25f \\\n--hash=sha256:cab3db8bab4b7e635c1c97270d7a4b2a90c070b33cbc00c99ef3f9be03d3e1f7 \\\n--hash=sha256:cb70766519500281815dfd7a87d3a178acf7ce95390544b8c90587d76b227681 \\\n--hash=sha256:cca1b62fe70d761a282496b96a5e51c44c213e410a964bdffe0928e611368329 \\\n--hash=sha256:ccf9a39706b604d884d2cb1e27fe973bc55f2890c52f38df742bc1d79ab9f5e1 \\\n--hash=sha256:dc43f1ec66eb8440567186ae2f8c447d91e0372d793dfe8c222aec857b81a8cf \\\n--hash=sha256:dd632777ff3beaaf629f1ab4396caf7ba0bdd075d948a69460d13d44357aca4c \\\n--hash=sha256:e45ae4927759289c30ccba8d9fdce62bb414977ba158286b5ddaf8df2cddb5c5 \\\n--hash=sha256:e50ebce52f41370707f1e21a59514e3375e3edd6e1832f5e5235237db933c98b \\\n--hash=sha256:ebbbba226f0a108a7366bf4b59bf0f30a12fd5e75100c630267d94d7f0ad20e5 \\\n--hash=sha256:ec79ff6159dffcc30853b2ad612ed572af86c92b5168aa3fc01a67b0fa40665e \\\n--hash=sha256:f0936e08e0003f66bfd97e74ee530427707297b0d0361247e9b4f59ab78ddc8b \\\n--hash=sha256:f26a07a6e877c76a88e3cecac8531908d980d3d5067ff69213653649ec0f60ad \\\n--hash=sha256:f64e376cd20d3f030190e8c32e1c64582eba56ac6dc7d5b0b49a9d44021b52fd \\\n--hash=sha256:f6ffbc252eb0d229aeb2f9ad051200668fc3a9aaa8994e49f0cb2ffe2b7867e7 \\\n--hash=sha256:f9a7c509542db4eceed3dcf21ee5267ab565a83555c9b88a8109dcecc4709002 \\\n--hash=sha256:ff1d0899f104f3921d94579a5638847f783c9b04f2d5f229392ca77fba5b82fc\n# via\n#   -c /tmp/ray-deps/requirements_compiled.txt\n#   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Listing PyU2F Package with Hash Values\nDESCRIPTION: Definition for the PyU2F package dependency with version 0.1.5 and corresponding SHA256 hash value. The comment indicates this is required by the google-reauth package through the compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_31\n\nLANGUAGE: text\nCODE:\n```\npyu2f==0.1.5 \\\n    --hash=sha256:a3caa3a11842fc7d5746376f37195e6af5f17c0a15737538bb1cebf656fb306b\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   google-reauth\n```\n\n----------------------------------------\n\nTITLE: Making Inference Requests to Stable Diffusion Model\nDESCRIPTION: Python code to send image generation requests to the deployed Stable Diffusion model endpoint and save the output image.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/aws-neuron-core-inference-stable-diffusion.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nprompt = \"a zebra is dancing in the grass, river, sunlit\"\ninput = \"%20\".join(prompt.split(\" \"))\nresp = requests.get(f\"http://127.0.0.1:8000/imagine?prompt={input}\")\nprint(\"Write the response to `output.png`.\")\nwith open(\"output.png\", \"wb\") as f:\n    f.write(resp.content)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Tune with Cloud Storage (Python)\nDESCRIPTION: This snippet demonstrates how to configure Ray Tune to use cloud storage (e.g., AWS S3) for persistent storage. It sets up a Tuner with a specified storage path in an S3 bucket.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-storage.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\n\ntuner = tune.Tuner(\n    trainable,\n    run_config=tune.RunConfig(\n        name=\"experiment_name\",\n        storage_path=\"s3://bucket-name/sub-path/\",\n    )\n)\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Adding Data to SingleAgentEpisode in Python\nDESCRIPTION: This section explains methods for appending data to the SingleAgentEpisode, allowing for the addition of environment resets, environment steps, and temporary timestep data.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_episode.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n    Methods:\n    - add_env_reset(): adds a reset event for the environment to the episode.\n    - add_env_step(): logs a step taken in the environment.\n    - add_temporary_timestep_data(): records temporary data associated with the current timestep.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Vicuna-13B Model with Ray Train in Python\nDESCRIPTION: This snippet demonstrates how to initiate the fine-tuning process for the Vicuna-13B model using Ray Train. It calls the fit method on the TorchTrainer instance to start distributed training.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Server Log Output\nDESCRIPTION: Example log output showing successful startup of the Ray Serve application including HTTP proxy initialization and deployment status.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/text-classification.md#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n(ServeController pid=362, ip=10.0.44.233) INFO 2023-03-08 16:44:57,579 controller 362 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-7396d5a9efdb59ee01b7befba448433f6c6fc734cfa5421d415da1b3' on node '7396d5a9efdb59ee01b7befba448433f6c6fc734cfa5421d415da1b3' listening on '127.0.0.1:8000'\n(ServeController pid=362, ip=10.0.44.233) INFO 2023-03-08 16:44:57,588 controller 362 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-a30ea53938547e0bf88ce8672e578f0067be26a7e26d23465c46300b' on node 'a30ea53938547e0bf88ce8672e578f0067be26a7e26d23465c46300b' listening on '127.0.0.1:8000'\n(ProxyActor pid=439, ip=10.0.44.233) INFO:     Started server process [439]\n(ProxyActor pid=5779) INFO:     Started server process [5779]\n(ServeController pid=362, ip=10.0.44.233) INFO 2023-03-08 16:44:59,362 controller 362 deployment_state.py:1333 - Adding 1 replica to deployment 'APIIngress'.\n2023-03-08 16:45:01,316 SUCC <string>:93 -- Deployed Serve app successfully.\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx autopydantic_model directive for model documentation\nDESCRIPTION: This reStructuredText template configures how Pydantic models are documented using the autopydantic_model directive. It sets up display options to exclude Config class, hide validator and field summaries, include JSON examples, and maintain source order for summary lists while excluding undocumented members.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/autosummary/autopydantic_show_json.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autopydantic_model:: {{ fullname }}\n    :inherited-members: BaseModel\n    :exclude-members: Config\n    :model-show-config-summary: False\n    :model-show-validator-summary: False\n    :model-show-field-summary: False\n    :field-list-validators: False\n    :model-show-json: True\n    :model-summary-list-order: bysource\n    :undoc-members: \n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies\nDESCRIPTION: This snippet defines Python package dependencies with specific versions and SHA256 hash checksums. It includes packages like filelock, frozenlist, fsspec, and others required for the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_7\n\nLANGUAGE: Text\nCODE:\n```\nfilelock==3.17.0 \\\n    --hash=sha256:533dc2f7ba78dc2f0f531fc6c4940addf7b70a481e269a5a3b93be94ffbe8338 \\\n    --hash=sha256:ee4e77401ef576ebb38cd7f13b9b28893194acc20a8e68e18730ba9c0e54660e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   huggingface-hub\n    #   ray\n    #   torch\n    #   transformers\n    #   virtualenv\n    #   vllm\nfrozenlist==1.4.1 \\\n    --hash=sha256:04ced3e6a46b4cfffe20f9ae482818e34eba9b5fb0ce4056e4cc9b6e212d09b7 \\\n    --hash=sha256:0633c8d5337cb5c77acbccc6357ac49a1770b8c487e5b3505c57b949b4b82e98 \\\n    --hash=sha256:068b63f23b17df8569b7fdca5517edef76171cf3897eb68beb01341131fbd2ad \\\n    --hash=sha256:0c250a29735d4f15321007fb02865f0e6b6a41a6b88f1f523ca1596ab5f50bd5 \\\n    --hash=sha256:1979bc0aeb89b33b588c51c54ab0161791149f2461ea7c7c946d95d5f93b56ae \\\n    --hash=sha256:1a4471094e146b6790f61b98616ab8e44f72661879cc63fa1049d13ef711e71e \\\n    --hash=sha256:1b280e6507ea8a4fa0c0a7150b4e526a8d113989e28eaaef946cc77ffd7efc0a \\\n    --hash=sha256:1d0ce09d36d53bbbe566fe296965b23b961764c0bcf3ce2fa45f463745c04701 \\\n    --hash=sha256:20b51fa3f588ff2fe658663db52a41a4f7aa6c04f6201449c6c7c476bd255c0d \\\n    --hash=sha256:23b2d7679b73fe0e5a4560b672a39f98dfc6f60df63823b0a9970525325b95f6 \\\n    --hash=sha256:23b701e65c7b36e4bf15546a89279bd4d8675faabc287d06bbcfac7d3c33e1e6 \\\n    --hash=sha256:2471c201b70d58a0f0c1f91261542a03d9a5e088ed3dc6c160d614c01649c106 \\\n    --hash=sha256:27657df69e8801be6c3638054e202a135c7f299267f1a55ed3a598934f6c0d75 \\\n    --hash=sha256:29acab3f66f0f24674b7dc4736477bcd4bc3ad4b896f5f45379a67bce8b96868 \\\n    --hash=sha256:32453c1de775c889eb4e22f1197fe3bdfe457d16476ea407472b9442e6295f7a \\\n    --hash=sha256:3a670dc61eb0d0eb7080890c13de3066790f9049b47b0de04007090807c776b0 \\\n    --hash=sha256:3e0153a805a98f5ada7e09826255ba99fb4f7524bb81bf6b47fb702666484ae1 \\\n    --hash=sha256:410478a0c562d1a5bcc2f7ea448359fcb050ed48b3c6f6f4f18c313a9bdb1826 \\\n    --hash=sha256:442acde1e068288a4ba7acfe05f5f343e19fac87bfc96d89eb886b0363e977ec \\\n    --hash=sha256:48f6a4533887e189dae092f1cf981f2e3885175f7a0f33c91fb5b7b682b6bab6 \\\n    --hash=sha256:4f57dab5fe3407b6c0c1cc907ac98e8a189f9e418f3b6e54d65a718aaafe3950 \\\n    --hash=sha256:4f9c515e7914626b2a2e1e311794b4c35720a0be87af52b79ff8e1429fc25f19 \\\n    --hash=sha256:55fdc093b5a3cb41d420884cdaf37a1e74c3c37a31f46e66286d9145d2063bd0 \\\n    --hash=sha256:5667ed53d68d91920defdf4035d1cdaa3c3121dc0b113255124bcfada1cfa1b8 \\\n    --hash=sha256:590344787a90ae57d62511dd7c736ed56b428f04cd8c161fcc5e7232c130c69a \\\n    --hash=sha256:5a7d70357e7cee13f470c7883a063aae5fe209a493c57d86eb7f5a6f910fae09 \\\n    --hash=sha256:5c3894db91f5a489fc8fa6a9991820f368f0b3cbdb9cd8849547ccfab3392d86 \\\n    --hash=sha256:5c849d495bf5154cd8da18a9eb15db127d4dba2968d88831aff6f0331ea9bd4c \\\n    --hash=sha256:64536573d0a2cb6e625cf309984e2d873979709f2cf22839bf2d61790b448ad5 \\\n    --hash=sha256:693945278a31f2086d9bf3df0fe8254bbeaef1fe71e1351c3bd730aa7d31c41b \\\n    --hash=sha256:6db4667b187a6742b33afbbaf05a7bc551ffcf1ced0000a571aedbb4aa42fc7b \\\n    --hash=sha256:6eb73fa5426ea69ee0e012fb59cdc76a15b1283d6e32e4f8dc4482ec67d1194d \\\n    --hash=sha256:722e1124aec435320ae01ee3ac7bec11a5d47f25d0ed6328f2273d287bc3abb0 \\\n    --hash=sha256:7268252af60904bf52c26173cbadc3a071cece75f873705419c8681f24d3edea \\\n    --hash=sha256:74fb4bee6880b529a0c6560885fce4dc95936920f9f20f53d99a213f7bf66776 \\\n    --hash=sha256:780d3a35680ced9ce682fbcf4cb9c2bad3136eeff760ab33707b71db84664e3a \\\n    --hash=sha256:82e8211d69a4f4bc360ea22cd6555f8e61a1bd211d1d5d39d3d228b48c83a897 \\\n    --hash=sha256:89aa2c2eeb20957be2d950b85974b30a01a762f3308cd02bb15e1ad632e22dc7 \\\n    --hash=sha256:8aefbba5f69d42246543407ed2461db31006b0f76c4e32dfd6f42215a2c41d09 \\\n    --hash=sha256:96ec70beabbd3b10e8bfe52616a13561e58fe84c0101dd031dc78f250d5128b9 \\\n    --hash=sha256:9750cc7fe1ae3b1611bb8cfc3f9ec11d532244235d75901fb6b8e42ce9229dfe \\\n    --hash=sha256:9acbb16f06fe7f52f441bb6f413ebae6c37baa6ef9edd49cdd567216da8600cd \\\n    --hash=sha256:9d3e0c25a2350080e9319724dede4f31f43a6c9779be48021a7f4ebde8b2d742 \\\n    --hash=sha256:a06339f38e9ed3a64e4c4e43aec7f59084033647f908e4259d279a52d3757d09 \\\n    --hash=sha256:a0cb6f11204443f27a1628b0e460f37fb30f624be6051d490fa7d7e26d4af3d0 \\\n    --hash=sha256:a7496bfe1da7fb1a4e1cc23bb67c58fab69311cc7d32b5a99c2007b4b2a0e932 \\\n    --hash=sha256:a828c57f00f729620a442881cc60e57cfcec6842ba38e1b19fd3e47ac0ff8dc1 \\\n    --hash=sha256:a9b2de4cf0cdd5bd2dee4c4f63a653c61d2408055ab77b151c1957f221cabf2a \\\n    --hash=sha256:b46c8ae3a8f1f41a0d2ef350c0b6e65822d80772fe46b653ab6b6274f61d4a49 \\\n    --hash=sha256:b7e3ed87d4138356775346e6845cccbe66cd9e207f3cd11d2f0b9fd13681359d \\\n    --hash=sha256:b7f2f9f912dca3934c1baec2e4585a674ef16fe00218d833856408c48d5beee7 \\\n    --hash=sha256:ba60bb19387e13597fb059f32cd4d59445d7b18b69a745b8f8e5db0346f33480 \\\n    --hash=sha256:beee944ae828747fd7cb216a70f120767fc9f4f00bacae8543c14a6831673f89 \\\n    --hash=sha256:bfa4a17e17ce9abf47a74ae02f32d014c5e9404b6d9ac7f729e01562bbee601e \\\n    --hash=sha256:c037a86e8513059a2613aaba4d817bb90b9d9b6b69aace3ce9c877e8c8ed402b \\\n    --hash=sha256:c302220494f5c1ebeb0912ea782bcd5e2f8308037b3c7553fad0e48ebad6ad82 \\\n    --hash=sha256:c6321c9efe29975232da3bd0af0ad216800a47e93d763ce64f291917a381b8eb \\\n    --hash=sha256:c757a9dd70d72b076d6f68efdbb9bc943665ae954dad2801b874c8c69e185068 \\\n    --hash=sha256:c99169d4ff810155ca50b4da3b075cbde79752443117d89429595c2e8e37fed8 \\\n    --hash=sha256:c9c92be9fd329ac801cc420e08452b70e7aeab94ea4233a4804f0915c14eba9b \\\n    --hash=sha256:cc7b01b3754ea68a62bd77ce6020afaffb44a590c2289089289363472d13aedb \\\n    --hash=sha256:db9e724bebd621d9beca794f2a4ff1d26eed5965b004a97f1f1685a173b869c2 \\\n    --hash=sha256:dca69045298ce5c11fd539682cff879cc1e664c245d1c64da929813e54241d11 \\\n    --hash=sha256:dd9b1baec094d91bf36ec729445f7769d0d0cf6b64d04d86e45baf89e2b9059b \\\n    --hash=sha256:e02a0e11cf6597299b9f3bbd3f93d79217cb90cfd1411aec33848b13f5c656cc \\\n    --hash=sha256:e6a20a581f9ce92d389a8c7d7c3dd47c81fd5d6e655c8dddf341e14aa48659d0 \\\n    --hash=sha256:e7004be74cbb7d9f34553a5ce5fb08be14fb33bc86f332fb71cbe5216362a497 \\\n    --hash=sha256:e774d53b1a477a67838a904131c4b0eef6b3d8a651f8b138b04f748fccfefe17 \\\n    --hash=sha256:edb678da49d9f72c9f6c609fbe41a5dfb9a9282f9e6a2253d5a91e0fc382d7c0 \\\n    --hash=sha256:f146e0911cb2f1da549fc58fc7bcd2b836a44b79ef871980d605ec392ff6b0d2 \\\n    --hash=sha256:f56e2333dda1fe0f909e7cc59f021eba0d2307bc6f012a1ccf2beca6ba362439 \\\n    --hash=sha256:f9a3ea26252bd92f570600098783d1371354d89d5f6b7dfd87359d669f2109b5 \\\n    --hash=sha256:f9aa1878d1083b276b0196f2dfbe00c9b7e752475ed3b682025ff20c1c1f51ac \\\n    --hash=sha256:fb3c2db03683b5767dedb5769b8a40ebb47d6f7f45b1b3e3b4b51ec8ad9d9825 \\\n    --hash=sha256:fbeb989b5cc29e8daf7f976b421c220f1b8c731cbf22b9130d8815418ea45887 \\\n    --hash=sha256:fde5bd59ab5357e3853313127f4d3565fc7dad314a74d7b5d43c22c6a5ed2ced \\\n    --hash=sha256:fe1a06da377e3a1062ae5fe0926e12b84eceb8a50b350ddca72dc85015873f74\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   aiohttp\n    #   aiosignal\n    #   ray\nfsspec==2023.5.0 \\\n    --hash=sha256:51a4ad01a5bb66fcc58036e288c0d53d3975a0df2a5dc59a93b59bade0391f2a \\\n    --hash=sha256:b3b56e00fb93ea321bc9e5d9cf6f8522a0198b20eb24e02774d329e9c6fb84ce\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   huggingface-hub\n    #   torch\ngguf==0.10.0 \\\n    --hash=sha256:52a30ef26328b419ffc47d9269fc580c238edf1c8a19b5ea143c323e04a038c1 \\\n    --hash=sha256:706089fba756a06913227841b4a6c8398360fa991569fd974e663a92b224e33f\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   vllm\ngoogle-api-core==1.34.0 \\\n    --hash=sha256:6fb380f49d19ee1d09a9722d0379042b7edb06c0112e4796c7a395078a043e71 \\\n    --hash=sha256:7421474c39d396a74dfa317dddbc69188f2336835f526087c7648f91105e32ff\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   opencensus\ngoogle-auth==2.23.4 \\\n    --hash=sha256:79905d6b1652187def79d491d6e23d0cbb3a21d3c7ba0dbaa9c8a01906b13ff3 \\\n    --hash=sha256:d4bbc92fe4b8bfd2f3e8d88e5ba7085935da208ee38a134fc280e7ce682a05f2\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   google-api-core\ngoogleapis-common-protos==1.61.0 \\\n    --hash=sha256:22f1915393bb3245343f6efe87f6fe868532efc12aa26b391b15132e1279f1c0 \\\n    --hash=sha256:8a64866a97f6304a7179873a465d6eee97b7a24ec6cfd78e0f575e96b821240b\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   google-api-core\n    #   opentelemetry-exporter-otlp-proto-grpc\ngrpcio==1.66.2 \\\n    --hash=sha256:02697eb4a5cbe5a9639f57323b4c37bcb3ab2d48cec5da3dc2f13334d72790dd \\\n    --hash=sha256:03b0b307ba26fae695e067b94cbb014e27390f8bc5ac7a3a39b7723fed085604 \\\n    --hash=sha256:05bc2ceadc2529ab0b227b1310d249d95d9001cd106aa4d31e8871ad3c428d73 \\\n    --hash=sha256:06de8ec0bd71be123eec15b0e0d457474931c2c407869b6c349bd9bed4adbac3 \\\n    --hash=sha256:0be4e0490c28da5377283861bed2941d1d20ec017ca397a5df4394d1c31a9b50 \\\n    --hash=sha256:12fda97ffae55e6526825daf25ad0fa37483685952b5d0f910d6405c87e3adb6 \\\n    --hash=sha256:1caa38fb22a8578ab8393da99d4b8641e3a80abc8fd52646f1ecc92bcb8dee34 \\\n    --hash=sha256:2018b053aa15782db2541ca01a7edb56a0bf18c77efed975392583725974b249 \\\n    --hash=sha256:20657d6b8cfed7db5e11b62ff7dfe2e12064ea78e93f1434d61888834bc86d75 \\\n    --hash=sha256:2335c58560a9e92ac58ff2bc5649952f9b37d0735608242973c7a8b94a6437d8 \\\n    --hash=sha256:31fd163105464797a72d901a06472860845ac157389e10f12631025b3e4d0453 \\\n    --hash=sha256:38b68498ff579a3b1ee8f93a05eb48dc2595795f2f62716e797dc24774c1aaa8 \\\n    --hash=sha256:3b00efc473b20d8bf83e0e1ae661b98951ca56111feb9b9611df8efc4fe5d55d \\\n```\n\n----------------------------------------\n\nTITLE: Managing Episode Chunks in SingleAgentEpisode in Python\nDESCRIPTION: This segment describes methods for manipulating episode data, including cutting, slicing, concatenating, and converting episodes to NumPy format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_episode.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n    Methods:\n    - cut(): cuts the episode into specified segments.\n    - slice(): slices a portion of the episode.\n    - concat_episode(): concatenates multiple episodes into one.\n    - to_numpy(): converts the episode data to NumPy format.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ray for Job Submission in Python\nDESCRIPTION: Installs the latest Ray release with default components for interacting with remote clusters using Ray Job Submission.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! pip install -U \"ray[default]\"\n```\n\n----------------------------------------\n\nTITLE: Tearing Down KubeRay Operator\nDESCRIPTION: Python command to tear down the KubeRay operator after testing is complete.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/kuberay/README.md#2025-04-12_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython setup/teardown_kuberay.py\n```\n\n----------------------------------------\n\nTITLE: Inspecting Kueue ClusterQueue Status\nDESCRIPTION: Commands to check the status of ClusterQueue resources, showing pending workloads and resource utilization.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/kueue.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get clusterqueues.kueue.x-k8s.io\nNAME            COHORT   PENDING WORKLOADS\ncluster-queue            1\n\n$ kubectl get clusterqueues.kueue.x-k8s.io cluster-queue -o yaml\nStatus:\n  Admitted Workloads:  1 # Workloads admitted by queue.\n  Conditions:\n    Last Transition Time:  2024-02-28T22:41:28Z\n    Message:               Can admit new workloads\n    Reason:                Ready\n    Status:                True\n    Type:                  Active\n  Flavors Reservation:\n    Name:  default-flavor\n    Resources:\n      Borrowed:  0\n      Name:      cpu\n      Total:     2\n      Borrowed:  0\n      Name:      memory\n      Total:     4Gi\n  Flavors Usage:\n    Name:  default-flavor\n    Resources:\n      Borrowed:         0\n      Name:             cpu\n      Total:            2\n      Borrowed:         0\n      Name:             memory\n      Total:            4Gi\n  Pending Workloads:    1\n  Reserving Workloads:  1\n```\n\n----------------------------------------\n\nTITLE: Specifying Outlines and Outlines-Core Dependencies with Hash Verification\nDESCRIPTION: This snippet shows the dependency specification for Outlines and Outlines-Core packages with version pinning and multiple hash verifications. The comments indicate that these are dependencies for vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_26\n\nLANGUAGE: plaintext\nCODE:\n```\noutlines==0.1.11 \\\n    --hash=sha256:0997bd9da1cc050e430bd08995dc7d4bd855918bafa4531e49d3f37110a23aba \\\n    --hash=sha256:f5a5f2242ed9802d3aab7a92789bf4008d734c576be9258cc0a297f690124727\n    # via vllm\noutlines-core==0.1.26 \\\n    --hash=sha256:00f409f72c11f6ffadb57066950dd384d5388015028c1a1a615c9a64988dae3e \\\n    --hash=sha256:11ff56af56cb54c563b7f25d86cd9ee77f3fed825f1d4dccd9449bb1e4e89538 \\\n    --hash=sha256:15a3684fa29564da2db03934cf0097bef3e871f70d3af0ef2b52fdb886da2e09 \\\n    --hash=sha256:19f462f6b00935708677ad27cb4df55e0e17f6ffe713ab750f5f2683b090f95d \\\n    --hash=sha256:1e0ea28a76da31d25b6f53242bf13e1b59a0241badf82353c88f55e1cf81b128 \\\n    --hash=sha256:2f8641aab4a6bd84516907492ce82099503129da01b3c29c1dc9ad50320bae77 \\\n    --hash=sha256:3f59aeccea21ed6ff3cf52102fd163f26d279821c20e5127ddd18d4ea4d0c8d2 \\\n    --hash=sha256:481c4301341e77cc8f1832d616784adb4d461b4fec65878e7c0d2cba7163a189 \\\n    --hash=sha256:64e01c0cfa9ba371634d7c3f6ea1862397cef98e4509fe98e3f57faa721a72d6 \\\n    --hash=sha256:6a962a7452e7ac170fa04d405342cadae2d28fafa5b1830cef7aa610257ed32f \\\n    --hash=sha256:7b7849cf40028319ebb9d8ba0fe4c590ef5888eebe524a81b3af30aaa06ea21c \\\n    --hash=sha256:8cc8c87d89bd267356f8149c9066cbb98970425ec162997fbf195c3f1feb7009 \\\n    --hash=sha256:9525321b48700dcaaabf60bcdc951e45f9357ba3fb3e1bfc81b662d7d4170e7c \\\n    --hash=sha256:9b36bff12779e58883747116893a17b3551bbd10865878b951b03a44d112229a \\\n    --hash=sha256:9d792a43ed9d8a4e1b38f4d83fe99db442d57aad4404c2edf98b710892eda47e \\\n    --hash=sha256:a3c4196148e47f455f1ace78e329d5b97e531cbc406456d681592952adae7e17 \\\n    --hash=sha256:a84b7cd2fb6268bf990dd3d479ffb4fa0bace6f571cb85b15b6cdb44b84f5b69 \\\n    --hash=sha256:a8932044a3d9329be53a226118850638f85b4d7842f9b863d0a123f23de220cd \\\n    --hash=sha256:ad8564ecd7b64bcb840596c5049ff1c1a96346de494302ffcc0f2b188c15675e \\\n    --hash=sha256:b6787b07b7c673fc3087d2b537719ecac8e03b10a47d032dd1926985c32885b0 \\\n    --hash=sha256:bba56604efdbc5932c7a8a88c2b8b0d0c740ab883b0012fb5464a9736796802b \\\n    --hash=sha256:e86a1bb46adc5cbf6dfd7a7fe4105e0e2a4c6e041732a053126b41c521a1f223 \\\n    --hash=sha256:f19765c151abfc970996368080aeea6d2a19e927817fe4e2af6726e639be3de4 \\\n    --hash=sha256:f38d290a7f6e5e12cbfcaee03269dfc0dbda49b360024b4279d1aba251fdc346 \\\n    --hash=sha256:f54633bca50055d42ea4d94ae06dcbe52d3d76a9b621b75723b1177d0d952953\n    # via outlines\n```\n\n----------------------------------------\n\nTITLE: Installing rich Package with Hash Verification\nDESCRIPTION: This snippet specifies the installation of rich package version 13.3.2 with SHA256 hash verification. It also lists the packages that depend on rich in the project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_48\n\nLANGUAGE: pip\nCODE:\n```\nrich==13.3.2 \\\n    --hash=sha256:91954fe80cfb7985727a467ca98a7618e5dd15178cc2da10f553b36a93859001 \\\n    --hash=sha256:a104f37270bf677148d8acb07d33be1569eeee87e2d1beb286a4e9113caf6f2f\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   memray\n    #   typer\n```\n\n----------------------------------------\n\nTITLE: Defining Outlines Packages for Text Generation in Ray LLM\nDESCRIPTION: Package specifications for Outlines libraries that provide structured text generation capabilities. These packages are used in conjunction with VLLM for controlled text generation with language models.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_21\n\nLANGUAGE: pip\nCODE:\n```\noutlines==0.1.11 \\\n    --hash=sha256:0997bd9da1cc050e430bd08995dc7d4bd855918bafa4531e49d3f37110a23aba \\\n    --hash=sha256:f5a5f2242ed9802d3aab7a92789bf4008d734c576be9258cc0a297f690124727\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Dependency Specification with Hash Verification\nDESCRIPTION: Detailed package specifications with SHA256 hashes for security verification. The example shows aiohappyeyeballs and aiohttp packages with their exact versions, hash values, and package dependency relationships.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   aiohttp\n```\n\n----------------------------------------\n\nTITLE: Specifying google-api-core Package Dependency with Hash Values\nDESCRIPTION: Definition of the google-api-core package dependency at version 1.34.0 with associated hash values for verification. The comment indicates this package is required by multiple Google Cloud components.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_10\n\nLANGUAGE: text\nCODE:\n```\ngoogle-api-core==1.34.0 \\\n    --hash=sha256:6fb380f49d19ee1d09a9722d0379042b7edb06c0112e4796c7a395078a043e71 \\\n    --hash=sha256:7421474c39d396a74dfa317dddbc69188f2336835f526087c7648f91105e32ff\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   google-cloud-core\n    #   google-cloud-storage\n    #   opencensus\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with Channel Flush Operation\nDESCRIPTION: This stack trace shows Netty's NIO event loop processing with a focus on the channel flush operation. The trace follows the path from event loop processing through read operations and channelReadComplete events, ending at the ChannelDuplexHandler's flush method.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_40\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j] 2\n```\n\n----------------------------------------\n\nTITLE: gRPC Context Client Implementation\nDESCRIPTION: Demonstrates the client-side implementation for accessing gRPC context attributes set by the server. Shows how to retrieve custom status codes and metadata from the response.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/grpc-guide.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n:start-after: __begin_grpc_context_client__\n:end-before: __end_grpc_context_client__\n:language: python\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Ray Cluster Resources\nDESCRIPTION: Commands to clean up the Ray cluster and associated Volcano queue\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/k8s-ecosystem/volcano.md#2025-04-12_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete raycluster test-cluster-1\nkubectl delete queue kuberay-test-queue\n```\n\n----------------------------------------\n\nTITLE: Testing Actor Creator Failure Behavior in Python\nDESCRIPTION: This code snippet demonstrates how to test the behavior of actors when their creator fails in Ray. It shows the difference between detached and non-detached actors in terms of fault tolerance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/actors.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# __actor_creator_failure_begin__\n# __actor_creator_failure_end__\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for Debugging Guides in Markdown\nDESCRIPTION: A toctree configuration for organizing debugging guide documentation in a hidden format. It lists various debugging-related pages that are referenced in the main document.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/debug-apps/index.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n\ngeneral-debugging\ndebug-memory\ndebug-hangs\ndebug-failures\noptimize-performance\n../../ray-distributed-debugger\nray-debugging\n```\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Configuration Example\nDESCRIPTION: YAML configuration showing two Ray Serve applications setup with their routes and runtime environments\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice.md#2025-04-12_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nserveConfigV2: |\n  applications:\n    - name: fruit_app\n      import_path: fruit.deployment_graph\n      route_prefix: /fruit\n      runtime_env:\n        working_dir: \"https://github.com/ray-project/test_dag/archive/....zip\"\n      deployments: ...\n    - name: math_app\n      import_path: conditional_dag.serve_dag\n      route_prefix: /calc\n      runtime_env:\n        working_dir: \"https://github.com/ray-project/test_dag/archive/....zip\"\n      deployments: ...\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Ray Tune and Forecasting\nDESCRIPTION: Installs the required packages for the project using pip. This ensures all nodes in the distributed cluster have access to the necessary dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/start.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --user -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Scaling XGBoost Training Across Multiple CPU Nodes\nDESCRIPTION: Configures XGBoost training to scale across multiple nodes with multiple CPUs. This example shows how to utilize 4 nodes with 8 CPUs each for distributed training.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nscaling_config = ScalingConfig(\n    num_workers=4,\n    resources_per_worker={\"CPU\": 8},\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Optimum-Habana Environment\nDESCRIPTION: Commands to clone the Optimum-Habana repository and install the required dependencies for Stable Diffusion fine-tuning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/sd.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/optimum-habana.git\npip install ray[train,serve] optimum-habana\ncd optimum-habana/\npip install -r examples/stable-diffusion/requirements.txt\npip install -r examples/stable-diffusion/training/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining pure-eval Dependency for Ray Project\nDESCRIPTION: This snippet specifies the pure-eval package dependency with version 0.2.2 and hash verification. The package is required by stack-data for providing pure evaluation functionality in the Ray development environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_30\n\nLANGUAGE: pip\nCODE:\n```\npure-eval==0.2.2 \\\n    --hash=sha256:01eaab343580944bc56080ebe0a674b39ec44a945e6d09ba7db3cb8cec289350 \\\n    --hash=sha256:2b45320af6dfaa1750f543d714b6d1c520a1688dec6fd24d339063ce0aaa9ac3\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   stack-data\n```\n\n----------------------------------------\n\nTITLE: Specifying prometheus-fastapi-instrumentator 7.0.2 Dependency with Hash Verification\nDESCRIPTION: Defines the prometheus-fastapi-instrumentator package at version 7.0.2 with SHA256 hash verification values. This is imported from requirements_compiled_rayllm_test_py311_cu121.txt and used by vllm.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\nprometheus-fastapi-instrumentator==7.0.2 \\\n    --hash=sha256:8a4d8fb13dbe19d2882ac6af9ce236e4e1f98dc48e3fa44fe88d8e23ac3c953f \\\n    --hash=sha256:975e39992acb7a112758ff13ba95317e6c54d1bbf605f9156f31ac9f2800c32d\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorch ResNet Model\nDESCRIPTION: Loading a pre-trained ResNet152 model and setting up image preprocessing transforms.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/01_batch_inference/start.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchvision.models import ResNet152_Weights\nfrom torchvision import transforms\nfrom torchvision import models\n\nweights = ResNet152_Weights.IMAGENET1K_V1\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = models.resnet152(weights=weights).to(device)\nmodel.eval()\n\nimagenet_transforms = weights.transforms\ntransform = transforms.Compose([transforms.ToTensor(), imagenet_transforms()])\n```\n\n----------------------------------------\n\nTITLE: Ray Task Definition with Breakpoint for Debugging\nDESCRIPTION: Python code sample that defines a Ray task with a breakpoint for distributed debugging. This example demonstrates how to embed breakpoints in a Ray task and includes options for triggering exceptions for post-mortem debugging.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/ray-distributed-debugger.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# This file is referenced but not explicitly shown in the documentation\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray Client Connection in Python\nDESCRIPTION: This snippet demonstrates how to connect to a remote Ray cluster using Ray Client. It initializes the connection and defines a simple remote function.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/ray-client.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# You can run this code outside of the Ray cluster!\nimport ray\n\n# Starting the Ray client. This connects to a remote Ray cluster.\nray.init(\"ray://<head_node_host>:10001\")\n\n# Normal Ray code follows\n@ray.remote\ndef do_work(x):\n    return x ** x\n\ndo_work.remote(2)\n#....\n```\n\n----------------------------------------\n\nTITLE: Loading Tiny Shakespeare Dataset for GPT-J Fine-Tuning\nDESCRIPTION: Loads the tiny_shakespeare dataset from Hugging Face and converts it to Ray Data format for distributed preprocessing and training.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nprint(\"Loading tiny_shakespeare dataset\")\ncurrent_dataset = load_dataset(\"tiny_shakespeare\")\ncurrent_dataset\n```\n\nLANGUAGE: python\nCODE:\n```\nimport ray.data\n\nray_datasets = {\n    \"train\": ray.data.from_huggingface(current_dataset[\"train\"]),\n    \"validation\": ray.data.from_huggingface(current_dataset[\"validation\"]),\n}\n\nray_datasets\n```\n\n----------------------------------------\n\nTITLE: Docker Pull Command for Custom Image\nDESCRIPTION: Command to pull the base Docker image for customizing the deployment environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/03_serving_stable_diffusion/README.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull us-docker.pkg.dev/anyscale-workspace-templates/workspace-templates/serve-stable-diffusion-model-ray-serve:latest\n```\n\n----------------------------------------\n\nTITLE: Specifying StatsForcast Package Version\nDESCRIPTION: Defines the exact version 1.5.0 of the statsforecast Python package as a dependency using pip requirements file format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\nstatsforecast==1.5.0\n```\n\n----------------------------------------\n\nTITLE: Package Hash Verification List\nDESCRIPTION: Comprehensive list of SHA256 hashes for package version verification, primarily focused on dependency management and security validation. Includes hash entries for multiple package versions and their distributions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_40\n\nLANGUAGE: txt\nCODE:\n```\n--hash=sha256:00e5a1fea0fd4f5bfa7440a47eff01d9822a65b4488f7cff83155a0f31a2ecba \\\n--hash=sha256:02ddb6756f8f4517a2d5e99d8b2f272488e18dd0bfbc802f31c16c6c20f22193 \\\n--hash=sha256:045b8482ce9483ada4f3f23b3774f4e1bf4f23a2d5c912ed5170f68efb053318\n```\n\n----------------------------------------\n\nTITLE: Get initial parameter values\nDESCRIPTION: This snippet calls the `get_init_theta()` function (defined in `pbt_visualization_utils.py`) to retrieve initial values for the model parameters (theta). It then prints these initial values.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization.ipynb#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"theta_0 = get_init_theta()\nprint(f\\\"Initial parameter values: theta = {theta_0}\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Launches the test runner in interactive watch mode for running application tests.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/README.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm test\n```\n\n----------------------------------------\n\nTITLE: List Ray Nodes Command\nDESCRIPTION: Command to list all nodes in a Ray cluster using either CLI or Python SDK.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nray list nodes\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import list_nodes\nlist_nodes()\n```\n\n----------------------------------------\n\nTITLE: Listing Ray Project Package Dependencies\nDESCRIPTION: This section lists the package dependencies for the Ray project, including version constraints and hashes for verification. It includes comments indicating the source of each dependency.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\naiofiles==22.1.0 \\\n    --hash=sha256:1142fa8e80dbae46bb6339573ad4c8c0841358f79c6eb50a493dceca14621bad \\\n    --hash=sha256:9107f1ca0b2a5553987a94a3c9959fe5b491fdf731389aa5b7b1bd0733e32de6\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   ypy-websocket\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   aiohttp\naiohttp==3.11.16 \\\n    --hash=sha256:004511d3413737700835e949433536a2fe95a7d0297edd911a1e9705c5b5ea43 \\\n    --hash=sha256:0902e887b0e1d50424112f200eb9ae3dfed6c0d0a19fc60f633ae5a57c809656 \\\n    --hash=sha256:09b00dd520d88eac9d1768439a59ab3d145065c91a8fab97f900d1b5f802895e \\\n    --hash=sha256:0a2f451849e6b39e5c226803dcacfa9c7133e9825dcefd2f4e837a2ec5a3bb98 \\\n    --hash=sha256:0a950c2eb8ff17361abd8c85987fd6076d9f47d040ebffce67dce4993285e973 \\\n    --hash=sha256:0ad1fb47da60ae1ddfb316f0ff16d1f3b8e844d1a1e154641928ea0583d486ed \\\n    --hash=sha256:13ceac2c5cdcc3f64b9015710221ddf81c900c5febc505dbd8f810e770011540 \\\n    --hash=sha256:14461157d8426bcb40bd94deb0450a6fa16f05129f7da546090cebf8f3123b0f \\\n    --hash=sha256:16f8a2c9538c14a557b4d309ed4d0a7c60f0253e8ed7b6c9a2859a7582f8b1b8 \\\n    --hash=sha256:17ae4664031aadfbcb34fd40ffd90976671fa0c0286e6c4113989f78bebab37a \\\n    --hash=sha256:1ce63ae04719513dd2651202352a2beb9f67f55cb8490c40f056cea3c5c355ce \\\n    --hash=sha256:23a15727fbfccab973343b6d1b7181bfb0b4aa7ae280f36fd2f90f5476805682 \\\n    --hash=sha256:2540ddc83cc724b13d1838026f6a5ad178510953302a49e6d647f6e1de82bc34 \\\n    --hash=sha256:37dcee4906454ae377be5937ab2a66a9a88377b11dd7c072df7a7c142b63c37c \\\n    --hash=sha256:38bea84ee4fe24ebcc8edeb7b54bf20f06fd53ce4d2cc8b74344c5b9620597fd \\\n    --hash=sha256:3ab3367bb7f61ad18793fea2ef71f2d181c528c87948638366bf1de26e239183 \\\n    --hash=sha256:3ad1d59fd7114e6a08c4814983bb498f391c699f3c78712770077518cae63ff7 \\\n    --hash=sha256:3b4e6db8dc4879015b9955778cfb9881897339c8fab7b3676f8433f849425913 \\\n    --hash=sha256:3e061b09f6fa42997cf627307f220315e313ece74907d35776ec4373ed718b86 \\\n    --hash=sha256:42864e70a248f5f6a49fdaf417d9bc62d6e4d8ee9695b24c5916cb4bb666c802 \\\n    --hash=sha256:493910ceb2764f792db4dc6e8e4b375dae1b08f72e18e8f10f18b34ca17d0979 \\\n    --hash=sha256:4d0c970c0d602b1017e2067ff3b7dac41c98fef4f7472ec2ea26fd8a4e8c2149 \\\n    --hash=sha256:54eb3aead72a5c19fad07219acd882c1643a1027fbcdefac9b502c267242f955 \\\n    --hash=sha256:56a3443aca82abda0e07be2e1ecb76a050714faf2be84256dae291182ba59049 \\\n    --hash=sha256:576f5ca28d1b3276026f7df3ec841ae460e0fc3aac2a47cbf72eabcfc0f102e1 \\\n    --hash=sha256:58ede86453a6cf2d6ce40ef0ca15481677a66950e73b0a788917916f7e35a0bb \\\n    --hash=sha256:61c721764e41af907c9d16b6daa05a458f066015abd35923051be8705108ed17 \\\n    --hash=sha256:634d96869be6c4dc232fc503e03e40c42d32cfaa51712aee181e922e61d74814 \\\n    --hash=sha256:696ef00e8a1f0cec5e30640e64eca75d8e777933d1438f4facc9c0cdf288a810 \\\n    --hash=sha256:69a2cbd61788d26f8f1e626e188044834f37f6ae3f937bd9f08b65fc9d7e514e \\\n    --hash=sha256:6a792ce34b999fbe04a7a71a90c74f10c57ae4c51f65461a411faa70e154154e \\\n    --hash=sha256:6ac13b71761e49d5f9e4d05d33683bbafef753e876e8e5a7ef26e937dd766713 \\\n    --hash=sha256:6fdec0213244c39973674ca2a7f5435bf74369e7d4e104d6c7473c81c9bcc8c4 \\\n    --hash=sha256:72b1b03fb4655c1960403c131740755ec19c5898c82abd3961c364c2afd59fe7 \\\n    --hash=sha256:745f1ed5e2c687baefc3c5e7b4304e91bf3e2f32834d07baaee243e349624b24 \\\n    --hash=sha256:776c8e959a01e5e8321f1dec77964cb6101020a69d5a94cd3d34db6d555e01f7 \\\n    --hash=sha256:780df0d837276276226a1ff803f8d0fa5f8996c479aeef52eb040179f3156cbd \\\n    --hash=sha256:78e6e23b954644737e385befa0deb20233e2dfddf95dd11e9db752bdd2a294d3 \\\n    --hash=sha256:7951decace76a9271a1ef181b04aa77d3cc309a02a51d73826039003210bdc86 \\\n    --hash=sha256:7ba92a2d9ace559a0a14b03d87f47e021e4fa7681dc6970ebbc7b447c7d4b7cd \\\n    --hash=sha256:7f6428fee52d2bcf96a8aa7b62095b190ee341ab0e6b1bcf50c615d7966fd45b \\\n    --hash=sha256:87944bd16b7fe6160607f6a17808abd25f17f61ae1e26c47a491b970fb66d8cb \\\n    --hash=sha256:87a6e922b2b2401e0b0cf6b976b97f11ec7f136bfed445e16384fbf6fd5e8602 \\\n    --hash=sha256:8cb0688a8d81c63d716e867d59a9ccc389e97ac7037ebef904c2b89334407180 \\\n    --hash=sha256:8df6612df74409080575dca38a5237282865408016e65636a76a2eb9348c2567 \\\n    --hash=sha256:911a6e91d08bb2c72938bc17f0a2d97864c531536b7832abee6429d5296e5b27 \\\n    --hash=sha256:92b7ee222e2b903e0a4b329a9943d432b3767f2d5029dbe4ca59fb75223bbe2e \\\n    --hash=sha256:938f756c2b9374bbcc262a37eea521d8a0e6458162f2a9c26329cc87fdf06534 \\\n    --hash=sha256:9756d9b9d4547e091f99d554fbba0d2a920aab98caa82a8fb3d3d9bee3c9ae85 \\\n    --hash=sha256:98b88a2bf26965f2015a771381624dd4b0839034b70d406dc74fd8be4cc053e3 \\\n    --hash=sha256:9b751a6306f330801665ae69270a8a3993654a85569b3469662efaad6cf5cc50 \\\n    --hash=sha256:a2a450bcce4931b295fc0848f384834c3f9b00edfc2150baafb4488c27953de6 \\\n    --hash=sha256:a3814760a1a700f3cfd2f977249f1032301d0a12c92aba74605cfa6ce9f78489 \\\n    --hash=sha256:a5abcbba9f4b463a45c8ca8b7720891200658f6f46894f79517e6cd11f3405ca \\\n    --hash=sha256:a6db7458ab89c7d80bc1f4e930cc9df6edee2200127cfa6f6e080cf619eddfbd \\\n    --hash=sha256:ad497f38a0d6c329cb621774788583ee12321863cd4bd9feee1effd60f2ad133 \\\n    --hash=sha256:ad9509ffb2396483ceacb1eee9134724443ee45b92141105a4645857244aecc8 \\\n    --hash=sha256:bbcba75fe879ad6fd2e0d6a8d937f34a571f116a0e4db37df8079e738ea95c71 \\\n    --hash=sha256:c10d85e81d0b9ef87970ecbdbfaeec14a361a7fa947118817fcea8e45335fa46 \\\n    --hash=sha256:c15b2271c44da77ee9d822552201180779e5e942f3a71fb74e026bf6172ff287 \\\n    --hash=sha256:ca37057625693d097543bd88076ceebeb248291df9d6ca8481349efc0b05dcd0 \\\n    --hash=sha256:cc3a145479a76ad0ed646434d09216d33d08eef0d8c9a11f5ae5cdc37caa3540 \\\n    --hash=sha256:ccf10f16ab498d20e28bc2b5c1306e9c1512f2840f7b6a67000a517a4b37d5ee \\\n    --hash=sha256:cd464ba806e27ee24a91362ba3621bfc39dbbb8b79f2e1340201615197370f7c \\\n    --hash=sha256:d007aa39a52d62373bd23428ba4a2546eed0e7643d7bf2e41ddcefd54519842c \\\n    --hash=sha256:d0666afbe984f6933fe72cd1f1c3560d8c55880a0bdd728ad774006eb4241ecd \\\n    --hash=sha256:d07502cc14ecd64f52b2a74ebbc106893d9a9717120057ea9ea1fd6568a747e7 \\\n    --hash=sha256:d489d9778522fbd0f8d6a5c6e48e3514f11be81cb0a5954bdda06f7e1594b321 \\\n    --hash=sha256:df7db76400bf46ec6a0a73192b14c8295bdb9812053f4fe53f4e789f3ea66bbb \\\n    --hash=sha256:e3538bc9fe1b902bef51372462e3d7c96fce2b566642512138a480b7adc9d508 \\\n    --hash=sha256:e87fd812899aa78252866ae03a048e77bd11b80fb4878ce27c23cade239b42b2 \\\n    --hash=sha256:ecdb8173e6c7aa09eee342ac62e193e6904923bd232e76b4157ac0bfa670609f \\\n    --hash=sha256:f244b8e541f414664889e2c87cac11a07b918cb4b540c36f7ada7bfa76571ea2 \\\n    --hash=sha256:f4065145bf69de124accdd17ea5f4dc770da0a6a6e440c53f6e0a8c27b3e635c \\\n    --hash=sha256:f420bfe862fb357a6d76f2065447ef6f484bc489292ac91e29bc65d2d7a2c84d \\\n    --hash=sha256:f6ddd90d9fb4b501c97a4458f1c1720e42432c26cb76d28177c5b5ad4e332601 \\\n    --hash=sha256:fa73e8c2656a3653ae6c307b3f4e878a21f87859a9afab228280ddccd7369d71 \\\n    --hash=sha256:fadbb8f1d4140825069db3fedbbb843290fd5f5bc0a5dbd7eaf81d91bf1b003b \\\n    --hash=sha256:fb3d0cc5cdb926090748ea60172fa8a213cec728bd6c54eae18b96040fcd6227 \\\n    --hash=sha256:fb46bb0f24813e6cede6cc07b1961d4b04f331f7112a23b5e21f567da4ee50aa \\\n    --hash=sha256:fd36c119c5d6551bce374fcb5c19269638f8d09862445f85a5a48596fd59f4bb\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements/llm/llm-test-requirements.txt\n    #   -r python/requirements.txt\n    #   aiohttp-cors\n    #   pytest-aiohttp\n    #   vllm\naiohttp-cors==0.7.0 \\\n    --hash=sha256:0451ba59fdf6909d0e2cd21e4c0a43752bc0703d33fc78ae94d9d9321710193e \\\n    --hash=sha256:4d39c6d7100fd9764ed1caf8cebf0eb01bf5e3f24e2e073fda6234bc48b19f5d\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\naiorwlock==1.3.0 \\\n    --hash=sha256:45baf8e4fa9a23e0bb325fbd67da80de1fd7ae1d4f59a6381754c60cec7b289b \\\n    --hash=sha256:83f12d87df4b9728a0b8fda1756585ab0d652b107bab59c6084e1b1ad692ab45\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   -r python/requirements.txt\naiosignal==1.3.1 \\\n    --hash=sha256:54cd96e15e1649b75d6c87526a6ff0b6c1b0dd3459f43d9ca11d48c339b68cfc \\\n    --hash=sha256:f8376fb07dd1e86a584e4fcdec80b36b7f81aac666ebc724e2c090300dd83b17\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   aiohttp\n    #   ray\naiosqlite==0.19.0 \\\n    --hash=sha256:95ee77b91c8d2808bd08a59fbebf66270e9090c3d92ffbf260dc0db0b979577d \\\n    --hash=sha256:edba222e03453e094a3ce605db1b970c4b3376264e56f32e2a4959f948d66a96\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   ypy-websocket\nairportsdata==20241001 \\\n    --hash=sha256:67d71cf2c5378cc17ff66b62b1e11aa2444043949c894543ac8fd8dafce192fd \\\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Hash Verification\nDESCRIPTION: Package dependency declarations with SHA256 hash verification to ensure package integrity. Each package specifies version constraints and hash values for verification during installation. Includes dependencies like cupy-cuda12x, debugpy, decorator, and other packages required for the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:a011a644f6d7d03736214d38832e030d8268bcff4a41f728e6030325fea3e400 \\\n--hash=sha256:a2913c5375154b6ef2e91c10b5720ea6e21007412f6437504ffea2109b5a33d7 \\\n--hash=sha256:a30596bae9403a342c978fb47d9b0ee277699fa53bbafad14706af51fe543d16 \\\n# via python/requirements_compiled_ray_test_py311_cu124.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Version Constraints and Hashes\nDESCRIPTION: This code snippet defines Python package dependencies with exact version numbers and SHA256 hashes for security verification. It includes packages like absl-py, aiobotocore, and aiohttp among others.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cpu\n--find-links https://data.pyg.org/whl/torch-2.3.0+cpu.html\n\nabsl-py==1.4.0 \\\n    --hash=sha256:0d3fe606adfa4f7db64792dd4c7aee4ee0c38ab75dfd353b7a83ed3e957fcb47 \\\n    --hash=sha256:d2c244d01048ba476e7c080bd2c6df5e141d211de80223460d5b3b8a2a58433d\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   tensorboard\n    #   tensorflow\naiobotocore==2.5.0 \\\n    --hash=sha256:6a5b397cddd4f81026aa91a14c7dd2650727425740a5af8ba75127ff663faf67 \\\n    --hash=sha256:9a2a022d7b78ec9a2af0de589916d2721cddbf96264401b78d7a73c1a1435f3b\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   s3fs\naiohappyeyeballs==2.6.1 \\\n    --hash=sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558 \\\n    --hash=sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   aiohttp\naiohttp==3.11.16 \\\n    --hash=sha256:004511d3413737700835e949433536a2fe95a7d0297edd911a1e9705c5b5ea43 \\\n    --hash=sha256:0902e887b0e1d50424112f200eb9ae3dfed6c0d0a19fc60f633ae5a57c809656 \\\n    --hash=sha256:09b00dd520d88eac9d1768439a59ab3d145065c91a8fab97f900d1b5f802895e \\\n    --hash=sha256:0a2f451849e6b39e5c226803dcacfa9c7133e9825dcefd2f4e837a2ec5a3bb98 \\\n    --hash=sha256:0a950c2eb8ff17361abd8c85987fd6076d9f47d040ebffce67dce4993285e973 \\\n    --hash=sha256:0ad1fb47da60ae1ddfb316f0ff16d1f3b8e844d1a1e154641928ea0583d486ed \\\n    --hash=sha256:13ceac2c5cdcc3f64b9015710221ddf81c900c5febc505dbd8f810e770011540 \\\n    --hash=sha256:14461157d8426bcb40bd94deb0450a6fa16f05129f7da546090cebf8f3123b0f \\\n    --hash=sha256:16f8a2c9538c14a557b4d309ed4d0a7c60f0253e8ed7b6c9a2859a7582f8b1b8 \\\n    --hash=sha256:17ae4664031aadfbcb34fd40ffd90976671fa0c0286e6c4113989f78bebab37a \\\n    --hash=sha256:1ce63ae04719513dd2651202352a2beb9f67f55cb8490c40f056cea3c5c355ce \\\n    --hash=sha256:23a15727fbfccab973343b6d1b7181bfb0b4aa7ae280f36fd2f90f5476805682 \\\n    --hash=sha256:2540ddc83cc724b13d1838026f6a5ad178510953302a49e6d647f6e1de82bc34 \\\n    --hash=sha256:37dcee4906454ae377be5937ab2a66a9a88377b11dd7c072df7a7c142b63c37c \\\n    --hash=sha256:38bea84ee4fe24ebcc8edeb7b54bf20f06fd53ce4d2cc8b74344c5b9620597fd \\\n    --hash=sha256:3ab3367bb7f61ad18793fea2ef71f2d181c528c87948638366bf1de26e239183 \\\n    --hash=sha256:3ad1d59fd7114e6a08c4814983bb498f391c699f3c78712770077518cae63ff7 \\\n    --hash=sha256:3b4e6db8dc4879015b9955778cfb9881897339c8fab7b3676f8433f849425913 \\\n    --hash=sha256:3e061b09f6fa42997cf627307f220315e313ece74907d35776ec4373ed718b86 \\\n    --hash=sha256:42864e70a248f5f6a49fdaf417d9bc62d6e4d8ee9695b24c5916cb4bb666c802 \\\n    --hash=sha256:493910ceb2764f792db4dc6e8e4b375dae1b08f72e18e8f10f18b34ca17d0979 \\\n    --hash=sha256:4d0c970c0d602b1017e2067ff3b7dac41c98fef4f7472ec2ea26fd8a4e8c2149 \\\n    --hash=sha256:54eb3aead72a5c19fad07219acd882c1643a1027fbcdefac9b502c267242f955 \\\n    --hash=sha256:56a3443aca82abda0e07be2e1ecb76a050714faf2be84256dae291182ba59049 \\\n    --hash=sha256:576f5ca28d1b3276026f7df3ec841ae460e0fc3aac2a47cbf72eabcfc0f102e1 \\\n    --hash=sha256:58ede86453a6cf2d6ce40ef0ca15481677a66950e73b0a788917916f7e35a0bb \\\n    --hash=sha256:61c721764e41af907c9d16b6daa05a458f066015abd35923051be8705108ed17 \\\n    --hash=sha256:634d96869be6c4dc232fc503e03e40c42d32cfaa51712aee181e922e61d74814 \\\n    --hash=sha256:696ef00e8a1f0cec5e30640e64eca75d8e777933d1438f4facc9c0cdf288a810 \\\n    --hash=sha256:69a2cbd61788d26f8f1e626e188044834f37f6ae3f937bd9f08b65fc9d7e514e \\\n    --hash=sha256:6a792ce34b999fbe04a7a71a90c74f10c57ae4c51f65461a411faa70e154154e \\\n    --hash=sha256:6ac13b71761e49d5f9e4d05d33683bbafef753e876e8e5a7ef26e937dd766713 \\\n    --hash=sha256:6fdec0213244c39973674ca2a7f5435bf74369e7d4e104d6c7473c81c9bcc8c4 \\\n    --hash=sha256:72b1b03fb4655c1960403c131740755ec19c5898c82abd3961c364c2afd59fe7 \\\n    --hash=sha256:745f1ed5e2c687baefc3c5e7b4304e91bf3e2f32834d07baaee243e349624b24 \\\n    --hash=sha256:776c8e959a01e5e8321f1dec77964cb6101020a69d5a94cd3d34db6d555e01f7 \\\n    --hash=sha256:780df0d837276276226a1ff803f8d0fa5f8996c479aeef52eb040179f3156cbd \\\n    --hash=sha256:78e6e23b954644737e385befa0deb20233e2dfddf95dd11e9db752bdd2a294d3 \\\n    --hash=sha256:7951decace76a9271a1ef181b04aa77d3cc309a02a51d73826039003210bdc86 \\\n    --hash=sha256:7ba92a2d9ace559a0a14b03d87f47e021e4fa7681dc6970ebbc7b447c7d4b7cd \\\n    --hash=sha256:7f6428fee52d2bcf96a8aa7b62095b190ee341ab0e6b1bcf50c615d7966fd45b \\\n    --hash=sha256:87944bd16b7fe6160607f6a17808abd25f17f61ae1e26c47a491b970fb66d8cb \\\n    --hash=sha256:87a6e922b2b2401e0b0cf6b976b97f11ec7f136bfed445e16384fbf6fd5e8602 \\\n    --hash=sha256:8cb0688a8d81c63d716e867d59a9ccc389e97ac7037ebef904c2b89334407180 \\\n    --hash=sha256:8df6612df74409080575dca38a5237282865408016e65636a76a2eb9348c2567 \\\n    --hash=sha256:911a6e91d08bb2c72938bc17f0a2d97864c531536b7832abee6429d5296e5b27 \\\n    --hash=sha256:92b7ee222e2b903e0a4b329a9943d432b3767f2d5029dbe4ca59fb75223bbe2e \\\n    --hash=sha256:938f756c2b9374bbcc262a37eea521d8a0e6458162f2a9c26329cc87fdf06534 \\\n    --hash=sha256:9756d9b9d4547e091f99d554fbba0d2a920aab98caa82a8fb3d3d9bee3c9ae85 \\\n    --hash=sha256:98b88a2bf26965f2015a771381624dd4b0839034b70d406dc74fd8be4cc053e3 \\\n    --hash=sha256:9b751a6306f330801665ae69270a8a3993654a85569b3469662efaad6cf5cc50 \\\n    --hash=sha256:a2a450bcce4931b295fc0848f384834c3f9b00edfc2150baafb4488c27953de6 \\\n    --hash=sha256:a3814760a1a700f3cfd2f977249f1032301d0a12c92aba74605cfa6ce9f78489 \\\n    --hash=sha256:a5abcbba9f4b463a45c8ca8b7720891200658f6f46894f79517e6cd11f3405ca \\\n    --hash=sha256:a6db7458ab89c7d80bc1f4e930cc9df6edee2200127cfa6f6e080cf619eddfbd \\\n    --hash=sha256:ad497f38a0d6c329cb621774788583ee12321863cd4bd9feee1effd60f2ad133 \\\n    --hash=sha256:ad9509ffb2396483ceacb1eee9134724443ee45b92141105a4645857244aecc8 \\\n    --hash=sha256:bbcba75fe879ad6fd2e0d6a8d937f34a571f116a0e4db37df8079e738ea95c71 \\\n    --hash=sha256:c10d85e81d0b9ef87970ecbdbfaeec14a361a7fa947118817fcea8e45335fa46 \\\n    --hash=sha256:c15b2271c44da77ee9d822552201180779e5e942f3a71fb74e026bf6172ff287 \\\n    --hash=sha256:ca37057625693d097543bd88076ceebeb248291df9d6ca8481349efc0b05dcd0 \\\n    --hash=sha256:cc3a145479a76ad0ed646434d09216d33d08eef0d8c9a11f5ae5cdc37caa3540 \\\n    --hash=sha256:ccf10f16ab498d20e28bc2b5c1306e9c1512f2840f7b6a67000a517a4b37d5ee \\\n    --hash=sha256:cd464ba806e27ee24a91362ba3621bfc39dbbb8b79f2e1340201615197370f7c \\\n    --hash=sha256:d007aa39a52d62373bd23428ba4a2546eed0e7643d7bf2e41ddcefd54519842c \\\n    --hash=sha256:d0666afbe984f6933fe72cd1f1c3560d8c55880a0bdd728ad774006eb4241ecd \\\n    --hash=sha256:d07502cc14ecd64f52b2a74ebbc106893d9a9717120057ea9ea1fd6568a747e7 \\\n    --hash=sha256:d489d9778522fbd0f8d6a5c6e48e3514f11be81cb0a5954bdda06f7e1594b321 \\\n    --hash=sha256:df7db76400bf46ec6a0a73192b14c8295bdb9812053f4fe53f4e789f3ea66bbb \\\n    --hash=sha256:e3538bc9fe1b902bef51372462e3d7c96fce2b566642512138a480b7adc9d508 \\\n    --hash=sha256:e87fd812899aa78252866ae03a048e77bd11b80fb4878ce27c23cade239b42b2 \\\n    --hash=sha256:ecdb8173e6c7aa09eee342ac62e193e6904923bd232e76b4157ac0bfa670609f \\\n    --hash=sha256:f244b8e541f414664889e2c87cac11a07b918cb4b540c36f7ada7bfa76571ea2 \\\n    --hash=sha256:f4065145bf69de124accdd17ea5f4dc770da0a6a6e440c53f6e0a8c27b3e635c \\\n    --hash=sha256:f420bfe862fb357a6d76f2065447ef6f484bc489292ac91e29bc65d2d7a2c84d \\\n    --hash=sha256:f6ddd90d9fb4b501c97a4458f1c1720e42432c26cb76d28177c5b5ad4e332601 \\\n    --hash=sha256:fa73e8c2656a3653ae6c307b3f4e878a21f87859a9afab228280ddccd7369d71 \\\n    --hash=sha256:fadbb8f1d4140825069db3fedbbb843290fd5f5bc0a5dbd7eaf81d91bf1b003b \\\n    --hash=sha256:fb3d0cc5cdb926090748ea60172fa8a213cec728bd6c54eae18b96040fcd6227 \\\n    --hash=sha256:fb46bb0f24813e6cede6cc07b1961d4b04f331f7112a23b5e21f567da4ee50aa \\\n    --hash=sha256:fd36c119c5d6551bce374fcb5c19269638f8d09862445f85a5a48596fd59f4bb\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   aiobotocore\n    #   gcsfs\n    #   google-auth\n    #   s3fs\naioitertools==0.11.0 \\\n    --hash=sha256:04b95e3dab25b449def24d7df809411c10e62aab0cbe31a50ca4e68748c43394 \\\n    --hash=sha256:42c68b8dd3a69c2bf7f2233bf7df4bb58b557bca5252ac02ed5187bbc67d6831\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   aiobotocore\naiosignal==1.3.1 \\\n    --hash=sha256:54cd96e15e1649b75d6c87526a6ff0b6c1b0dd3459f43d9ca11d48c339b68cfc \\\n    --hash=sha256:f8376fb07dd1e86a584e4fcdec80b36b7f81aac666ebc724e2c090300dd83b17\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   aiohttp\nale-py==0.10.1 \\\n    --hash=sha256:076a44a61c2518b844f765692a91d0a6b383c6592b5fdabd94fd24d4c62a54ef \\\n    --hash=sha256:0835ee11004efeb5a9805a09c1525242f737257a8a4f5f4f0b9b3e047e6dca86 \\\n    --hash=sha256:12617edc9799c73570df67a731a4293bcfd500f413e0bfa867b53fc411fa7629 \\\n    --hash=sha256:24b9e61a4e868a4266f8a0ef7809cc20cecedb8c10d515d14ff6078950d51d8b \\\n    --hash=sha256:24f7aa19e1b3b1540516942020a95f57964af71285497620e58f03b2c113424e \\\n    --hash=sha256:3971a8552d2f982f569c87152479901574a9fe86410e5d1a26276e7ffccb59e1 \\\n    --hash=sha256:3d82d81715f15598b9db50529da971d36117cda027af9d112bd2ea22cefe3bcb \\\n    --hash=sha256:43d63b262f4b3bfcd567ce736a5648b4193470b2691bc14e38ac0c05dfe2a7e2 \\\n    --hash=sha256:4dd55a52e074497f1143785a215a50706afba3111be8b4923d46cc507c16be8f \\\n    --hash=sha256:4f3aaea36c1671812c21b5f7c5dcf9f5f9c726f5b10cbe7a657a844de963bb55 \\\n    --hash=sha256:5d4f326236c95736182323a480363c7b98959fc9a4ba09d2aa5b152faa6a2d59 \\\n    --hash=sha256:6f0a3da4ff47f913b5c61e66571fe7fb92fc569e5babdf4b0eeee348aac1d457 \\\n    --hash=sha256:771d5a1cd5a50d2cf226eba45c418fb7a18b453bd332b6a2189310030eda421a \\\n    --hash=sha256:7733d521921452b9e644e9e31e4d5b1ba612305473c5ba0266cafb7eff6a5461 \\\n    --hash=sha256:82c676030b8b6543cb6969a905ff841ae6f086a2efe707542d014ef6ca4ada4e \\\n    --hash=sha256:92a31bd44687c6a3595fcdac35bc3238e305dd604171ba6a9cb7912bc83c99ee\n\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Defines exact versions and SHA256 hashes for Python package dependencies. Includes package relationships and source requirements files. Used for deterministic and secure package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_23\n\nLANGUAGE: text\nCODE:\n```\npyasn1==0.5.1 \\\n    --hash=sha256:4439847c58d40b1d0a573d07e3856e95333f1976294494c325775aeca506eb58 \\\n    --hash=sha256:6d391a96e59b23130a5cfa74d6fd7f388dbbe26cc8f1edf39fdddf08d9d6676c\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies with SHA256 Hashes\nDESCRIPTION: This snippet shows a section of a requirements or constraints file containing Python package dependencies with specific versions and SHA256 hashes for secure installation. Each package includes version constraints, hash values, and comments indicating which parts of the Ray project require these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_35\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:8d1e3e10dfbfcd58119ba5a4d3c7d519182b970a2aebaf0b6f539f55ae16058d \\\n    --hash=sha256:9c64ebe9cf376cba0c31aed138e15ed179a1d128612dd241cdf299d159e5e882 \\\n    --hash=sha256:a6ad7b8aadccd4e4dd7f315a07bef1bca41d194eeaf4ec600d20dea02d242fce \\\n    --hash=sha256:afe80544ef46730ea1b11cc655da27038bbaa7159dc5af4bc35bbc32982262f2 \\\n    --hash=sha256:b587ee5d23369a0e881da6e37f78371dce4238cf7638a455db4b633a1a1c62d6 \\\n    --hash=sha256:ce28eb1c397dba437ec39b9ab18f2101806f388c7a0cf9cdfd8f09294ad1c799 \\\n    --hash=sha256:d7fda067837df94e0a614d93d3a38fb6868958d37f7f50afe2a534524f2660cb \\\n    --hash=sha256:de489e3ed315bdba55c9d1554a2e89faa65d212e365ab81bc323fa52681fc60e \\\n    --hash=sha256:fb471f757fc45102a87e5d86e87dc2c8c78b34ad4f203679a46520f1d863b9da \\\n    --hash=sha256:fc2c7931008a911e3060c77ea8933f63f7367c0f3af04f82db3a04808ad2cd2c\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   statsforecast\nsympy==1.13.1 \\\n    --hash=sha256:9cebf7e04ff162015ce31c9c6c9144daa34a93bd082f54fd8f12deca4f47515f \\\n    --hash=sha256:db36cdc64bf61b9b24578b6f7bab1ecdd2452cf008f34faa33776680c26d66f8\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   torch\ntabledata==1.3.3 \\\n    --hash=sha256:4abad1c996d8607e23b045b44dc0c5f061668f3c37585302c5f6c84c93a89962 \\\n    --hash=sha256:c90daaba9a408e4397934b3ff2f6c06797d5289676420bf520c741ad43e6ff91\n    # via pytablewriter\ntabulate==0.9.0 \\\n    --hash=sha256:0095b12bf5966de529c0feb1fa08671671b3368eec77d7ef7ab114be2c068b3c \\\n    --hash=sha256:024ca478df22e9340661486f85298cff5f6dcdba14f3813e8830015b9ed1948f\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   sacrebleu\ntblib==3.0.0 \\\n    --hash=sha256:80a6c77e59b55e83911e1e607c649836a69c103963c5f28a46cbeef44acf8129 \\\n    --hash=sha256:93622790a0a29e04f0346458face1e144dc4d32f493714c6c3dff82a4adb77e6\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_ml_byod_3.9.in\ntcolorpy==0.1.6 \\\n    --hash=sha256:8c15cb3167f30b0a433d72297e9d68667c825bd9e2af41c8dd7dfbd3d7f7e207 \\\n    --hash=sha256:8cea0bf5f8cf03f77528a9acfbf312df935573892ba5ea3b2516e61fa54de9a5\n    # via pytablewriter\ntensorboardx==2.6.2.2 \\\n    --hash=sha256:160025acbf759ede23fd3526ae9d9bfbfd8b68eb16c38a010ebe326dc6395db8 \\\n    --hash=sha256:c6476d7cd0d529b0b72f4acadb1269f9ed8b22f441e87a84f2a3b940bb87b666\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_ml_byod_3.9.in\n    #   pytorch-lightning\nthreadpoolctl==3.1.0 \\\n    --hash=sha256:8b99adda265feb6773280df41eece7b2e6561b772d21ffd52e372f999024907b \\\n    --hash=sha256:a335baacfaa4400ae1f0d8e3a58d6674d2f8828e3716bb2802c44955ad391380\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   scikit-learn\ntiktoken==0.7.0 \\\n    --hash=sha256:03c6c40ff1db0f48a7b4d2dafeae73a5607aacb472fa11f125e7baf9dce73704 \\\n    --hash=sha256:084cec29713bc9d4189a937f8a35dbdfa785bd1235a34c1124fe2323821ee93f \\\n    --hash=sha256:09ed925bccaa8043e34c519fbb2f99110bd07c6fd67714793c21ac298e449410 \\\n    --hash=sha256:0bc603c30b9e371e7c4c7935aba02af5994a909fc3c0fe66e7004070858d3f8f \\\n    --hash=sha256:1063c5748be36344c7e18c7913c53e2cca116764c2080177e57d62c7ad4576d1 \\\n    --hash=sha256:1077266e949c24e0291f6c350433c6f0971365ece2b173a23bc3b9f9defef6b6 \\\n    --hash=sha256:10c7674f81e6e350fcbed7c09a65bca9356eaab27fb2dac65a1e440f2bcfe30f \\\n    --hash=sha256:131b8aeb043a8f112aad9f46011dced25d62629091e51d9dc1adbf4a1cc6aa98 \\\n    --hash=sha256:13c94efacdd3de9aff824a788353aa5749c0faee1fbe3816df365ea450b82311 \\\n    --hash=sha256:20295d21419bfcca092644f7e2f2138ff947a6eb8cfc732c09cc7d76988d4a89 \\\n    --hash=sha256:21a20c3bd1dd3e55b91c1331bf25f4af522c525e771691adbc9a69336fa7f702 \\\n    --hash=sha256:2398fecd38c921bcd68418675a6d155fad5f5e14c2e92fcf5fe566fa5485a858 \\\n    --hash=sha256:2bcb28ddf79ffa424f171dfeef9a4daff61a94c631ca6813f43967cb263b83b9 \\\n    --hash=sha256:2ee92776fdbb3efa02a83f968c19d4997a55c8e9ce7be821ceee04a1d1ee149c \\\n    --hash=sha256:485f3cc6aba7c6b6ce388ba634fbba656d9ee27f766216f45146beb4ac18b25f \\\n    --hash=sha256:54031f95c6939f6b78122c0aa03a93273a96365103793a22e1793ee86da31685 \\\n    --hash=sha256:5d4511c52caacf3c4981d1ae2df85908bd31853f33d30b345c8b6830763f769c \\\n    --hash=sha256:71c55d066388c55a9c00f61d2c456a6086673ab7dec22dd739c23f77195b1908 \\\n    --hash=sha256:79383a6e2c654c6040e5f8506f3750db9ddd71b550c724e673203b4f6b4b4590 \\\n    --hash=sha256:811229fde1652fedcca7c6dfe76724d0908775b353556d8a71ed74d866f73f7b \\\n    --hash=sha256:861f9ee616766d736be4147abac500732b505bf7013cfaf019b85892637f235e \\\n    --hash=sha256:86b6e7dc2e7ad1b3757e8a24597415bafcfb454cebf9a33a01f2e6ba2e663992 \\\n    --hash=sha256:8a81bac94769cab437dd3ab0b8a4bc4e0f9cf6835bcaa88de71f39af1791727a \\\n    --hash=sha256:8c46d7af7b8c6987fac9b9f61041b452afe92eb087d29c9ce54951280f899a97 \\\n    --hash=sha256:8d57f29171255f74c0aeacd0651e29aa47dff6f070cb9f35ebc14c82278f3b25 \\\n    --hash=sha256:8e58c7eb29d2ab35a7a8929cbeea60216a4ccdf42efa8974d8e176d50c9a3df5 \\\n    --hash=sha256:8f5f6afb52fb8a7ea1c811e435e4188f2bef81b5e0f7a8635cc79b0eef0193d6 \\\n    --hash=sha256:959d993749b083acc57a317cbc643fb85c014d055b2119b739487288f4e5d1cb \\\n    --hash=sha256:c72baaeaefa03ff9ba9688624143c858d1f6b755bb85d456d59e529e17234769 \\\n    --hash=sha256:cabc6dc77460df44ec5b879e68692c63551ae4fae7460dd4ff17181df75f1db7 \\\n    --hash=sha256:d20b5c6af30e621b4aca094ee61777a44118f52d886dbe4f02b70dfe05c15350 \\\n    --hash=sha256:d427614c3e074004efa2f2411e16c826f9df427d3c70a54725cae860f09e4bf4 \\\n    --hash=sha256:d6d73ea93e91d5ca771256dfc9d1d29f5a554b83821a1dc0891987636e0ae226 \\\n    --hash=sha256:e215292e99cb41fbc96988ef62ea63bb0ce1e15f2c147a61acc319f8b4cbe5bf \\\n    --hash=sha256:e54be9a2cd2f6d6ffa3517b064983fb695c9a9d8aa7d574d1ef3c3f931a99225 \\\n    --hash=sha256:fffdcb319b614cf14f04d02a52e26b1d1ae14a570f90e9b55461a72672f7b13d\n    # via\n    #   -r release/ray_release/byod/requirements_ml_byod_3.9.in\n    #   openai-whisper\ntokenizers==0.15.2 \\\n    --hash=sha256:0143e7d9dcd811855c1ce1ab9bf5d96d29bf5e528fd6c7824d0465741e8c10fd \\\n    --hash=sha256:02272fe48280e0293a04245ca5d919b2c94a48b408b55e858feae9618138aeda \\\n    --hash=sha256:02458bee6f5f3139f1ebbb6d042b283af712c0981f5bc50edf771d6b762d5e4f \\\n    --hash=sha256:054c1cc9c6d68f7ffa4e810b3d5131e0ba511b6e4be34157aa08ee54c2f8d9ee \\\n    --hash=sha256:05a77cbfebe28a61ab5c3891f9939cc24798b63fa236d84e5f29f3a85a200c00 \\\n    --hash=sha256:064ff87bb6acdbd693666de9a4b692add41308a2c0ec0770d6385737117215f2 \\\n    --hash=sha256:06cd0487b1cbfabefb2cc52fbd6b1f8d4c37799bd6c6e1641281adaa6b2504a7 \\\n    --hash=sha256:0774bccc6608eca23eb9d620196687c8b2360624619623cf4ba9dc9bd53e8b51 \\\n    --hash=sha256:0cf6b7f1d4dc59af960e6ffdc4faffe6460bbfa8dce27a58bf75755ffdb2526d \\\n    --hash=sha256:0ef06b9707baeb98b316577acb04f4852239d856b93e9ec3a299622f6084e4be \\\n    --hash=sha256:0ff110ecc57b7aa4a594396525a3451ad70988e517237fe91c540997c4e50e29 \\\n    --hash=sha256:107089f135b4ae7817affe6264f8c7a5c5b4fd9a90f9439ed495f54fcea56fb4 \\\n    --hash=sha256:112a1dd436d2cc06e6ffdc0b06d55ac019a35a63afd26475205cb4b1bf0bfbff \\\n    --hash=sha256:13ca3611de8d9ddfbc4dc39ef54ab1d2d4aaa114ac8727dfdc6a6ec4be017378 \\\n    --hash=sha256:158be8ea8554e5ed69acc1ce3fbb23a06060bd4bbb09029431ad6b9a466a7121 \\\n    --hash=sha256:1cf75d32e8d250781940d07f7eece253f2fe9ecdb1dc7ba6e3833fa17b82fcbc \\\n    --hash=sha256:1ddba9a2b0c8c81633eca0bb2e1aa5b3a15362b1277f1ae64176d0f6eba78ab1 \\\n    --hash=sha256:20ea60479de6fc7b8ae756b4b097572372d7e4032e2521c1bbf3d90c90a99ff0 \\\n    --hash=sha256:2277c36d2d6cdb7876c274547921a42425b6810d38354327dd65a8009acf870c \\\n    --hash=sha256:237d1bf3361cf2e6463e6c140628e6406766e8b27274f5fcc62c747ae3c6f094 \\\n    --hash=sha256:2735ecbbf37e52db4ea970e539fd2d450d213517b77745114f92867f3fc246eb \\\n    --hash=sha256:2ef09bbc16519f6c25d0c7fc0c6a33a6f62923e263c9d7cca4e58b8c61572afb \\\n    --hash=sha256:32e16bdeffa7c4f46bf2152172ca511808b952701d13e7c18833c0b73cb5c23f \\\n    --hash=sha256:361abdc068e8afe9c5b818769a48624687fb6aaed49636ee39bec4e95e1a215b \\\n    --hash=sha256:37aaec5a52e959892870a7c47cef80c53797c0db9149d458460f4f31e2fb250e \\\n    --hash=sha256:3835738be1de66624fff2f4f6f6684775da4e9c00bde053be7564cbf3545cc66 \\\n    --hash=sha256:38bfb0204ff3246ca4d5e726e8cc8403bfc931090151e6eede54d0e0cf162ef0 \\\n    --hash=sha256:38d7ab43c6825abfc0b661d95f39c7f8af2449364f01d331f3b51c94dcff7221 \\\n    --hash=sha256:3b919afe4df7eb6ac7cafd2bd14fb507d3f408db7a68c43117f579c984a73843 \\\n    --hash=sha256:3ef5dd1d39797044642dbe53eb2bc56435308432e9c7907728da74c69ee2adca \\\n    --hash=sha256:3f5e64b0389a2be47091d8cc53c87859783b837ea1a06edd9d8e04004df55a5c \\\n    --hash=sha256:40b6a4c78da863ff26dbd5ad9a8ecc33d8a8d97b535172601cf00aee9d7ce9ce \\\n    --hash=sha256:41e39b41e5531d6b2122a77532dbea60e171ef87a3820b5a3888daa847df4153 \\\n    --hash=sha256:44f2a832cd0825295f7179eaf173381dc45230f9227ec4b44378322d900447c9 \\\n    --hash=sha256:454c203164e07a860dbeb3b1f4a733be52b0edbb4dd2e5bd75023ffa8b49403a \\\n    --hash=sha256:4620cca5c2817177ee8706f860364cc3a8845bc1e291aaf661fb899e5d1c45b0 \\\n    --hash=sha256:473c83c5e2359bb81b0b6fde870b41b2764fcdd36d997485e07e72cc3a62264a \\\n    --hash=sha256:48e2b9335be2bc0171df9281385c2ed06a15f5cf121c44094338306ab7b33f2c \\\n    --hash=sha256:494fdbe5932d3416de2a85fc2470b797e6f3226c12845cadf054dd906afd0442 \\\n    --hash=sha256:4b19a808d8799fda23504a5cd31d2f58e6f52f140380082b352f877017d6342b \\\n    --hash=sha256:4c4b89038a684f40a6b15d6b09f49650ac64d951ad0f2a3ea9169687bbf2a8ba \\\n    --hash=sha256:4e022fe65e99230b8fd89ebdfea138c24421f91c1a4f4781a8f5016fd5cdfb4d \\\n    --hash=sha256:4eeb12daf02a59e29f578a865f55d87cd103ce62bd8a3a5874f8fdeaa82e336b \\\n    --hash=sha256:4fe1f74a902bee74a3b25aff180fbfbf4f8b444ab37c4d496af7afd13a784ed2 \\\n    --hash=sha256:508711a108684111ec8af89d3a9e9e08755247eda27d0ba5e3c50e9da1600f6d \\\n    --hash=sha256:5179c271aa5de9c71712e31cb5a79e436ecd0d7532a408fa42a8dbfa4bc23fd9\n```\n\n----------------------------------------\n\nTITLE: Package Requirements List with Dependencies\nDESCRIPTION: A comprehensive requirements.txt style listing showing package names, versions, and dependency relationships. Each entry includes the package name, version constraint, and comments indicating which other requirement files or packages depend on it.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled.txt#2025-04-12_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\njschema-to-python==1.2.3\n    # via cfn-lint\njson5==0.9.14\n    # via jupyterlab-server\njsondiff==2.0.0\n    # via moto\njsonpatch==1.32\n    # via\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements/test-requirements.txt\n    #   cfn-lint\njsonpickle==3.0.2\n    # via jschema-to-python\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with JavaScript Scope Access\nDESCRIPTION: This stack trace shows Netty's event processing flow with JavaScript scope resolution. The trace illustrates how HTTP server handlers in Vert.x interact with Mozilla's Rhino JavaScript engine, accessing parent scopes during request processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_48\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/ScriptableObject:.getParentScope_[j] 1\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation Directives for Ray DataContext\nDESCRIPTION: ReStructuredText directives defining the documentation structure for Ray's DataContext API, including module reference, class documentation, and method listing.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/data_context.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _data-context-api:\n\nGlobal configuration\n====================\n\n.. currentmodule:: ray.data\n\n.. autoclass:: DataContext\n\n.. autosummary::\n   :nosignatures:\n   :toctree: doc/\n\n   DataContext.get_current\n```\n\n----------------------------------------\n\nTITLE: Listing PySpark Package with Hash Values\nDESCRIPTION: Definition for the PySpark package dependency with version 3.4.1 and corresponding SHA256 hash value. The comment indicates this is required by the Petastorm package through the compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_25\n\nLANGUAGE: text\nCODE:\n```\npyspark==3.4.1 \\\n    --hash=sha256:72cd66ab8cf61a75854e5a753f75bea35ee075c3a96f9de4e2a66d02ec7fc652\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   petastorm\n```\n\n----------------------------------------\n\nTITLE: Ray Client Object Mapping Example\nDESCRIPTION: Demonstrates the object type mappings between server-side Ray objects and their client-side counterparts, showing how different Ray core objects are represented in the client context.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/util/client/ARCHITECTURE.md#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nObjectRef <-> ClientObjectRef\nActorID <-> ClientActorRef\nRemoteFunc <-> ClientRemoteFunc\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents in Sphinx Documentation\nDESCRIPTION: Sphinx toctree directive that defines the documentation structure, including job submission and autoscaling reference pages with a maximum depth of 2 levels.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/index.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n:maxdepth: '2'\n\njob-submission/index\nautoscaling/reference\n```\n\n----------------------------------------\n\nTITLE: Executing and retrieving result from request task\nDESCRIPTION: This snippet executes the Ray remote task 'reqs' and prints the result. It calls the task remotely and retrieves the HTTP status code from ray.io.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(ray.get(reqs.remote()))\n```\n\n----------------------------------------\n\nTITLE: Running Ray Serving Test\nDESCRIPTION: This command runs tests for the Ray serving module using Pytest, which is a required dependency. It checks for proper behavior of `ray.serve` related scripts.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npytest python/ray/serve/tests\n```\n\n----------------------------------------\n\nTITLE: Specifying Pexpect Package with Hash Verification for Non-Windows Platforms\nDESCRIPTION: Defines the pexpect package version 4.8.0 with SHA256 hash verification for non-Windows platforms and notes that it's required by ipython.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_27\n\nLANGUAGE: text\nCODE:\n```\npexpect==4.8.0 ; sys_platform != 'win32' \\\n    --hash=sha256:0b48a55dcb3c05f3329815901ea4fc1537514d6ba867a152b581d69ae3710937 \\\n    --hash=sha256:fc65a43959d153d0114afe13997d439c22823a27cefceb5ff35c2178c6784c0c\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ipython\n```\n\n----------------------------------------\n\nTITLE: Regex Package Dependency with Extensive Hash Verification\nDESCRIPTION: Defines the regex package (version 2024.11.6) with numerous hash signatures for security verification. The comments show it's required by tiktoken and transformers packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_29\n\nLANGUAGE: txt\nCODE:\n```\nregex==2024.11.6 \\\n    --hash=sha256:02a02d2bb04fec86ad61f3ea7f49c015a0681bf76abb9857f945d26159d2968c \\\n    --hash=sha256:02e28184be537f0e75c1f9b2f8847dc51e08e6e171c6bde130b2687e0c33cf60 \\\n    --hash=sha256:040df6fe1a5504eb0f04f048e6d09cd7c7110fef851d7c567a6b6e09942feb7d \\\n    --hash=sha256:068376da5a7e4da51968ce4c122a7cd31afaaec4fccc7856c92f63876e57b51d \\\n    --hash=sha256:06eb1be98df10e81ebaded73fcd51989dcf534e3c753466e4b60c4697a003b67 \\\n    --hash=sha256:072623554418a9911446278f16ecb398fb3b540147a7828c06e2011fa531e773 \\\n    --hash=sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0 \\\n    --hash=sha256:08986dce1339bc932923e7d1232ce9881499a0e02925f7402fb7c982515419ef \\\n    --hash=sha256:0a86e7eeca091c09e021db8eb72d54751e527fa47b8d5787caf96d9831bd02ad \\\n    --hash=sha256:0c32f75920cf99fe6b6c539c399a4a128452eaf1af27f39bce8909c9a3fd8cbe \\\n    --hash=sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3 \\\n    --hash=sha256:1062b39a0a2b75a9c694f7a08e7183a80c63c0d62b301418ffd9c35f55aaa114 \\\n    --hash=sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4 \\\n    --hash=sha256:149f5008d286636e48cd0b1dd65018548944e495b0265b45e1bffecce1ef7f39 \\\n    --hash=sha256:164d8b7b3b4bcb2068b97428060b2a53be050085ef94eca7f240e7947f1b080e \\\n    --hash=sha256:167ed4852351d8a750da48712c3930b031f6efdaa0f22fa1933716bfcd6bf4a3 \\\n    --hash=sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7 \\\n    --hash=sha256:202eb32e89f60fc147a41e55cb086db2a3f8cb82f9a9a88440dcfc5d37faae8d \\\n    --hash=sha256:220902c3c5cc6af55d4fe19ead504de80eb91f786dc102fbd74894b1551f095e \\\n    --hash=sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a \\\n    --hash=sha256:2c89a8cc122b25ce6945f0423dc1352cb9593c68abd19223eebbd4e56612c5b7 \\\n    --hash=sha256:2d548dafee61f06ebdb584080621f3e0c23fff312f0de1afc776e2a2ba99a74f \\\n    --hash=sha256:2e34b51b650b23ed3354b5a07aab37034d9f923db2a40519139af34f485f77d0 \\\n    --hash=sha256:32f9a4c643baad4efa81d549c2aadefaeba12249b2adc5af541759237eee1c54 \\\n    --hash=sha256:3a51ccc315653ba012774efca4f23d1d2a8a8f278a6072e29c7147eee7da446b \\\n    --hash=sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c \\\n    --hash=sha256:40291b1b89ca6ad8d3f2b82782cc33807f1406cf68c8d440861da6304d8ffbbd \\\n    --hash=sha256:41758407fc32d5c3c5de163888068cfee69cb4c2be844e7ac517a52770f9af57 \\\n    --hash=sha256:4181b814e56078e9b00427ca358ec44333765f5ca1b45597ec7446d3a1ef6e34 \\\n    --hash=sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d \\\n    --hash=sha256:50153825ee016b91549962f970d6a4442fa106832e14c918acd1c8e479916c4f \\\n    --hash=sha256:5056b185ca113c88e18223183aa1a50e66507769c9640a6ff75859619d73957b \\\n    --hash=sha256:5071b2093e793357c9d8b2929dfc13ac5f0a6c650559503bb81189d0a3814519 \\\n    --hash=sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4 \\\n    --hash=sha256:52fb28f528778f184f870b7cf8f225f5eef0a8f6e3778529bdd40c7b3920796a \\\n    --hash=sha256:5478c6962ad548b54a591778e93cd7c456a7a29f8eca9c49e4f9a806dcc5d638 \\\n    --hash=sha256:5670bce7b200273eee1840ef307bfa07cda90b38ae56e9a6ebcc9f50da9c469b \\\n    --hash=sha256:5704e174f8ccab2026bd2f1ab6c510345ae8eac818b613d7d73e785f1310f839 \\\n    --hash=sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07 \\\n    --hash=sha256:5e7e351589da0850c125f1600a4c4ba3c722efefe16b297de54300f08d734fbf \\\n    --hash=sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff \\\n    --hash=sha256:658f90550f38270639e83ce492f27d2c8d2cd63805c65a13a14d36ca126753f0 \\\n    --hash=sha256:684d7a212682996d21ca12ef3c17353c021fe9de6049e19ac8481ec35574a70f \\\n    --hash=sha256:69ab78f848845569401469da20df3e081e6b5a11cb086de3eed1d48f5ed57c95 \\\n    --hash=sha256:6f44ec28b1f858c98d3036ad5d7d0bfc568bdd7a74f9c24e25f41ef1ebfd81a4 \\\n    --hash=sha256:70b7fa6606c2881c1db9479b0eaa11ed5dfa11c8d60a474ff0e095099f39d98e \\\n    --hash=sha256:764e71f22ab3b305e7f4c21f1a97e1526a25ebdd22513e251cf376760213da13 \\\n    --hash=sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519 \\\n    --hash=sha256:805e6b60c54bf766b251e94526ebad60b7de0c70f70a4e6210ee2891acb70bf2 \\\n    --hash=sha256:8447d2d39b5abe381419319f942de20b7ecd60ce86f16a23b0698f22e1b70008 \\\n    --hash=sha256:86fddba590aad9208e2fa8b43b4c098bb0ec74f15718bb6a704e3c63e2cef3e9 \\\n    --hash=sha256:89d75e7293d2b3e674db7d4d9b1bee7f8f3d1609428e293771d1a962617150cc \\\n    --hash=sha256:93c0b12d3d3bc25af4ebbf38f9ee780a487e8bf6954c115b9f015822d3bb8e48 \\\n    --hash=sha256:94d87b689cdd831934fa3ce16cc15cd65748e6d689f5d2b8f4f4df2065c9fa20 \\\n    --hash=sha256:9714398225f299aa85267fd222f7142fcb5c769e73d7733344efc46f2ef5cf89 \\\n    --hash=sha256:982e6d21414e78e1f51cf595d7f321dcd14de1f2881c5dc6a6e23bbbbd68435e \\\n    --hash=sha256:997d6a487ff00807ba810e0f8332c18b4eb8d29463cfb7c820dc4b6e7562d0cf \\\n    --hash=sha256:a03e02f48cd1abbd9f3b7e3586d97c8f7a9721c436f51a5245b3b9483044480b \\\n    --hash=sha256:a36fdf2af13c2b14738f6e973aba563623cb77d753bbbd8d414d18bfaa3105dd \\\n    --hash=sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84 \\\n    --hash=sha256:a7c2155f790e2fb448faed6dd241386719802296ec588a8b9051c1f5c481bc29 \\\n    --hash=sha256:a93c194e2df18f7d264092dc8539b8ffb86b45b899ab976aa15d48214138e81b \\\n    --hash=sha256:abfa5080c374a76a251ba60683242bc17eeb2c9818d0d30117b4486be10c59d3 \\\n    --hash=sha256:ac10f2c4184420d881a3475fb2c6f4d95d53a8d50209a2500723d831036f7c45 \\\n    --hash=sha256:ad182d02e40de7459b73155deb8996bbd8e96852267879396fb274e8700190e3 \\\n    --hash=sha256:b2837718570f95dd41675328e111345f9b7095d821bac435aac173ac80b19983 \\\n    --hash=sha256:b489578720afb782f6ccf2840920f3a32e31ba28a4b162e13900c3e6bd3f930e \\\n    --hash=sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7 \\\n    --hash=sha256:b85c2530be953a890eaffde05485238f07029600e8f098cdf1848d414a8b45e4 \\\n    --hash=sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e \\\n    --hash=sha256:ba9b72e5643641b7d41fa1f6d5abda2c9a263ae835b917348fc3c928182ad467 \\\n    --hash=sha256:bb26437975da7dc36b7efad18aa9dd4ea569d2357ae6b783bf1118dabd9ea577 \\\n    --hash=sha256:bb8f74f2f10dbf13a0be8de623ba4f9491faf58c24064f32b65679b021ed0001 \\\n    --hash=sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0 \\\n    --hash=sha256:bec9931dfb61ddd8ef2ebc05646293812cb6b16b60cf7c9511a832b6f1854b55 \\\n    --hash=sha256:c36f9b6f5f8649bb251a5f3f66564438977b7ef8386a52460ae77e6070d309d9 \\\n    --hash=sha256:cdf58d0e516ee426a48f7b2c03a332a4114420716d55769ff7108c37a09951bf \\\n    --hash=sha256:d1cee317bfc014c2419a76bcc87f071405e3966da434e03e13beb45f8aced1a6 \\\n    --hash=sha256:d22326fcdef5e08c154280b71163ced384b428343ae16a5ab2b3354aed12436e \\\n    --hash=sha256:d3660c82f209655a06b587d55e723f0b813d3a7db2e32e5e7dc64ac2a9e86fde \\\n    --hash=sha256:da8f5fc57d1933de22a9e23eec290a0d8a5927a5370d24bda9a6abe50683fe62 \\\n    --hash=sha256:df951c5f4a1b1910f1a99ff42c473ff60f8225baa1cdd3539fe2819d9543e9df \\\n    --hash=sha256:e5364a4502efca094731680e80009632ad6624084aff9a23ce8c8c6820de3e51 \\\n    --hash=sha256:ea1bfda7162605f6e8178223576856b3d791109f15ea99a9f95c16a7636fb5 \\\n    --hash=sha256:f02f93b92358ee3f78660e43b4b0091229260c5d5c408d17d60bf26b6c900e86 \\\n    --hash=sha256:f056bf21105c2515c32372bbc057f43eb02aae2fda61052e2f7622c801f0b4e2 \\\n    --hash=sha256:f1ac758ef6aebfc8943560194e9fd0fa18bcb34d89fd8bd2af18183afd8da3a2 \\\n    --hash=sha256:f2a19f302cd1ce5dd01a9099aaa19cae6173306d1302a43b627f62e21cf18ac0 \\\n    --hash=sha256:f654882311409afb1d780b940234208a252322c24a93b442ca714d119e68086c \\\n    --hash=sha256:f65557897fc977a44ab205ea871b690adaef6b9da6afda4790a2484b04293a5f \\\n    --hash=sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6 \\\n    --hash=sha256:fdabbfc59f2c6edba2a6622c647b716e34e8e3867e0ab975412c5c2f79b82da2 \\\n    --hash=sha256:fdd6028445d2460f33136c55eeb1f601ab06d74cb3347132e1c24250187500d9 \\\n    --hash=sha256:ff590880083d60acc0433f9c3f713c51f7ac6ebb9adf889c79a261ecf541aa91\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   tiktoken\n    #   transformers\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Thread Callstack with TCP Buffer Cleanup\nDESCRIPTION: Thread stack trace showing Netty NIO operations with focus on TCP buffer cleanup in the kernel. This trace captures network data reception and buffer management between Java and the kernel.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_109\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/socket/nio/NioSocketChannel:.doReadBytes_[j];sun/nio/ch/SocketChannelImpl:.read_[j];sun/nio/ch/FileDispatcherImpl:.read0_[j];read;system_call_fastpath_[k];sys_read_[k];vfs_read_[k];do_sync_read_[k];sock_aio_read_[k];sock_aio_read.part.13_[k];do_sock_read.isra.12_[k];inet_recvmsg_[k];tcp_recvmsg_[k];tcp_cleanup_rbuf_[k]\n```\n\n----------------------------------------\n\nTITLE: Documenting Ray Enable Usage Stats Command in reStructuredText\nDESCRIPTION: This snippet uses the click directive to generate documentation for the 'ray enable-usage-stats' command from the ray.scripts.scripts module.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/api/cli.rst#2025-04-12_snippet_6\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. click:: ray.scripts.scripts:enable_usage_stats\n   :prog: ray enable-usage-stats\n   :show-nested:\n```\n\n----------------------------------------\n\nTITLE: Setting Up SSH Port Forwarding for Ray Client in Bash\nDESCRIPTION: These bash commands demonstrate how to set up SSH port forwarding to connect to a Ray cluster using Ray Client. It uses the Ray Cluster launcher to set up and attach to the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/ray-client.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ray up cluster.yaml\n$ ray attach cluster.yaml -p 10001\n```\n\n----------------------------------------\n\nTITLE: Installing Ray RLlib and TensorFlow (Bash)\nDESCRIPTION: This command installs Ray RLlib and TensorFlow using pip. `-U` upgrades to the latest version, and `ray[rllib]` includes dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/getting-started.md#2025-04-12_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install -U \\\"ray[rllib]\\\" tensorflow  # or torch\"\n```\n\n----------------------------------------\n\nTITLE: Basic Job Submission\nDESCRIPTION: Example showing how to submit a job using JobSubmissionClient\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/sdk.rst#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.job_submission import JobSubmissionClient\n\n# If using a remote cluster, replace 127.0.0.1 with the head node's IP address or set up port forwarding.\nclient = JobSubmissionClient(\"http://127.0.0.1:8265\")\njob_id = client.submit_job(\n    # Entrypoint shell command to execute\n    entrypoint=\"python script.py\",\n    # Path to the local directory that contains the script.py file\n    runtime_env={\"working_dir\": \"./\"}\n)\nprint(job_id)\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Learners in AlgorithmConfig for Python\nDESCRIPTION: This snippet shows how to configure the number of learners to 4 using the AlgorithmConfig class in RLlib.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-offline.rst#2025-04-12_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n\nconfig = (\n    AlgorithmConfig()\n    .learners(num_learners=4)\n)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Training Logs for Llama-2 on HPUs\nDESCRIPTION: Training logs from DeepSpeed implementation showing per-epoch metrics including loss, gradient norm, learning rate and memory usage. Running on 4 HPUs with LoRA fine-tuning of Llama-2-70b-chat-hf model.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n(RayTrainWorker pid=50067) {'loss': 1.662, 'grad_norm': 0.36514782905578613, 'learning_rate': 9.938441702975689e-05, 'epoch': 0.16, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.46, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.6047, 'grad_norm': 0.396455317735672, 'learning_rate': 9.567727288213005e-05, 'epoch': 0.32, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.57, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.4974, 'grad_norm': 0.49250370264053345, 'learning_rate': 8.885729807284856e-05, 'epoch': 0.48, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.57, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.4078, 'grad_norm': 0.49840453267097473, 'learning_rate': 7.938926261462366e-05, 'epoch': 0.65, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.57, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.315, 'grad_norm': 0.3432576656341553, 'learning_rate': 6.7918397477265e-05, 'epoch': 0.81, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.59, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.2651, 'grad_norm': 0.32175061106681824, 'learning_rate': 5.522642316338268e-05, 'epoch': 0.97, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.59, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.1947, 'grad_norm': 0.3646097481250763, 'learning_rate': 4.2178276747988446e-05, 'epoch': 1.13, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.59, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.1534, 'grad_norm': 0.4598522186279297, 'learning_rate': 2.9663167846209998e-05, 'epoch': 1.29, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.59, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.1404, 'grad_norm': 0.2677183449268341, 'learning_rate': 1.8533980447508137e-05, 'epoch': 1.45, 'memory_allocated (GB)': 32.75, 'max_memory_allocated (GB)': 94.59, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.1283, 'grad_norm': 0.32087600231170654, 'learning_rate': 9.549150281252633e-06, 'epoch': 1.61, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.59, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.0877, 'grad_norm': 0.28305548429489136, 'learning_rate': 3.3209786751399187e-06, 'epoch': 1.77, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.59, 'total_memory_available (GB)': 94.62}\n(RayTrainWorker pid=50067) {'loss': 1.1238, 'grad_norm': 0.25713953375816345, 'learning_rate': 2.7390523158633554e-07, 'epoch': 1.94, 'memory_allocated (GB)': 32.86, 'max_memory_allocated (GB)': 94.59, 'total_memory_available (GB)': 94.62}\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Package Requirements with Constraints for Ray ML Environment\nDESCRIPTION: This requirements file ensures version consistency by constraining packages to match those in the ray-ml Docker image. It references the dl-cpu-requirements.txt file and specifies key machine learning libraries like TensorFlow and PyTorch.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ml_user_tests/tune_rllib/driver_requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-c ../../../python/requirements/ml/dl-cpu-requirements.txt\n\ntensorflow\ntorch\n# Need this library to unpickle errors\ntblib\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenTelemetry SDK and Semantic Conventions Dependencies\nDESCRIPTION: This snippet shows how to specify OpenTelemetry SDK and semantic conventions packages with exact version requirements. The comments indicate which parent requirements files include these dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_25\n\nLANGUAGE: plaintext\nCODE:\n```\nopentelemetry-sdk==1.1.0 \\\n    --hash=sha256:ba29274aab656572e97e0339afaad6f2bded4102324b1475ab7412079498df6e \\\n    --hash=sha256:da7dfa6188e8a39f34b99495260e6a1d398c86a9de064c7f0805db6f16733d94\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\nopentelemetry-semantic-conventions==0.20b0 \\\n    --hash=sha256:ecae7367203e5204c70518e6d24b438480d6a6f1e5c8ee9dc2145f176ff4452e \\\n    --hash=sha256:fac014ac2098b1a05fe58af77cbe74c825ff869d6d53d316c393cc77f507ec15\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   opentelemetry-sdk\n```\n\n----------------------------------------\n\nTITLE: Running DreamerV3 on Atari - Shell Command\nDESCRIPTION: Command line instruction to run DreamerV3 on Atari Pong environment using the provided example script.\nSOURCE: https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamerv3/README.md#2025-04-12_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd ray/rllib/tuned_examples/dreamerv3/\npython atari_100k.py --env ale_py:ALE/Pong-v5\n```\n\n----------------------------------------\n\nTITLE: ExecutionResources Documentation Definition\nDESCRIPTION: RST documentation defining the autosummary directive for ExecutionResources class documentation.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/api/execution_options.rst#2025-04-12_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n   :nosignatures:\n   :toctree: doc/\n   :template: autosummary/class_without_autosummary.rst\n\n   ExecutionResources\n```\n\n----------------------------------------\n\nTITLE: List Placement Groups Command\nDESCRIPTION: Command to list all placement groups in a Ray cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/user-guides/cli-sdk.rst#2025-04-12_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nray list placement-groups\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.util.state import list_placement_groups\nlist_placement_groups()\n```\n\n----------------------------------------\n\nTITLE: Updating Requirements for Ray BYOD Python 3.9\nDESCRIPTION: Shell command instruction for updating the requirements file using Bazel run command. This is used to manage Python package dependencies for the Ray project's BYOD (Bring Your Own Dependencies) configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbazel run //release:requirements_byod_3.9.update\n```\n\n----------------------------------------\n\nTITLE: Downloading Docker Retag Tool\nDESCRIPTION: Downloads the Docker Retag tool from GitHub using wget. The tool is version 0.0.2 and is saved in the docker/retag-lambda directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/docker/retag-lambda/README.md#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npushd docker/retag-lambda\nwget -q https://github.com/joshdk/docker-retag/releases/download/0.0.2/docker-retag\npopd\n```\n\n----------------------------------------\n\nTITLE: Custom Progress Reporter in Ray Tune\nDESCRIPTION: Illustrates the creation of a custom progress reporter in Ray Tune for tracking and displaying specific training metrics or status updates in a tailored format.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Custom progress reporter\n# This script provides a custom reporting mechanism for training metrics.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Specifying pluggy Package Dependency\nDESCRIPTION: This snippet specifies the pluggy package version 1.3.0 with SHA-256 hashes for verification. It indicates that this dependency is required by pytest in the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_33\n\nLANGUAGE: plaintext\nCODE:\n```\npluggy==1.3.0 \\\n    --hash=sha256:cf61ae8f126ac6f7c451172cf30e3e43d3ca77615509771b3a984a0730651e12 \\\n    --hash=sha256:d89c696a773f8bd377d18e5ecda92b7a3793cbe66c87060a6fb58c7b6e1061f7\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   pytest\n```\n\n----------------------------------------\n\nTITLE: Ray remote task using emoji\nDESCRIPTION: This snippet defines a Ray remote task that utilizes the 'emoji' package. The task 'f' uses the emoji.emojize function to convert a text string into its emoji representation. This demonstrates how uv-managed dependencies can be used in Ray tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport emoji\nimport ray\n\n@ray.remote\ndef f():\n    return emoji.emojize('Python is :thumbs_up:')\n\n# Execute 1000 copies of f across a cluster.\nprint(ray.get([f.remote() for _ in range(1000)]))\n```\n\n----------------------------------------\n\nTITLE: Pinned Python Package Requirements with SHA256 Hashes\nDESCRIPTION: A standard pip requirements file with pinned package versions and SHA256 hashes for security verification. Each package includes comments showing what depends on it, helping with dependency tracking and management.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_19\n\nLANGUAGE: pip\nCODE:\n```\n    --hash=sha256:67a3598f0a2dcbc58d02dd1928544e7d88f764b47d4a286202913f0b2801c2e7 \\\n    --hash=sha256:74480f79a023f90dc6e18febbf7b8bac7508420f2006fabd512013c0c238f454 \\\n    --hash=sha256:819559cafa1a373b7096a482b504ae8a857c89593cf3a25af743ac9ecbd23480 \\\n    --hash=sha256:899dc660cd599d7352d6f10d83c95df430a38b410c1b66b407a6b29265d66469 \\\n    --hash=sha256:8c0c984a1b8fef4086329ff8dd19ac77576b384079247c770f29cc8ce3afa06c \\\n    --hash=sha256:9aae4406ea63d825636cc11ffb34ad3379335803216ee3a856787bcf5ccc751e \\\n    --hash=sha256:a7ca6d488aa8ff7f329d4c545b2dbad8ac31464f1d8b1c87ad1346717731e4db \\\n    --hash=sha256:b6cc7ba72a8850621bfec987cb72623e703b7fe2b9127a161ce61e61558ad905 \\\n    --hash=sha256:bf01b5720be110540be4286e791db73f84a2b721072a3711efff6c324cdf074b \\\n    --hash=sha256:c02ce36ec760252242a33967d51c289fd0e1c0e6e5cc9397e2279177716add86 \\\n    --hash=sha256:d9e4432ff660d67d775c66ac42a67cf2453c27cb4d738fc22cb53b5d84c135d4 \\\n    --hash=sha256:daa564862dd0d39c00f8086f88700fdbe8bc717e993a21e90711acfed02f2402 \\\n    --hash=sha256:de78575669dddf6099a8a0f46a27e82a1783c557ccc38ee620ed8cc96d3be7d7 \\\n    --hash=sha256:e64857f395505ebf3d2569935506ae0dfc4a15cb80dc25261176c784662cdcc4 \\\n    --hash=sha256:f4bd856d702e5b0d96a00ec6b307b0f51c1982c2bf9c0052cf9019e9a544ba99 \\\n    --hash=sha256:f4c42102bc82a51108e449cbb32b19b180022941c727bac0cfd50170341f16ee\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n    #   google-api-core\n    #   googleapis-common-protos\n    #   tensorboard\n    #   tensorboardx\n    #   tensorflow\n```\n\n----------------------------------------\n\nTITLE: Specifying RPDS-PY Package Dependency with Hash Verification\nDESCRIPTION: A requirements entry for the rpds-py package (version 0.22.3) with multiple SHA256 hashes for verification. The entry is incomplete in the provided snippet, suggesting this is part of a larger requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_36\n\nLANGUAGE: plaintext\nCODE:\n```\nrpds-py==0.22.3 \\\n    --hash=sha256:009de23c9c9ee54bf11303a966edf4d9087cd43a6003672e6aa7def643d06518 \\\n    --hash=sha256:02fbb9c288ae08bcb34fb41d516d5eeb0455ac35b5512d03181d755d80810059 \\\n    --hash=sha256:0a0461200769ab3b9ab7e513f6013b7a97fdeee41c29b9db343f3c5a8e2b9e61 \\\n    --hash=sha256:0b09865a9abc0ddff4e50b5ef65467cd94176bf1e0004184eb915cbc10fc05c5 \\\n    --hash=sha256:0b8db6b5b2d4491ad5b6bdc2bc7c017eec108acbf4e6785f42a9eb0ba234f4c9 \\\n    --hash=sha256:0c150c7a61ed4a4f4955a96626574e9baf1adf772c2fb61ef6a5027e52803543 \\\n    --hash=sha256:0f3cec041684de9a4684b1572fe28c7267410e02450f4561700ca5a3bc6695a2\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with SHA256 Hashes\nDESCRIPTION: A requirements file listing Python packages with their corresponding SHA256 hash values for security verification. Each package includes version specifications and hash values for package integrity verification, along with comments about where they are referenced from.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:47e147cdda9037f94b399bf03bfd8a6b6b1f2f90be94a454e3386f006455a9b4 \\\n    --hash=sha256:6912a87782acdff6eb8bf01675ed01d60ca1f2551f8176a300a886f09e836a6a \\\n    --hash=sha256:6d4fd101f571a31acb1559ae1af30f30b1dc4b3186669f92ad780e17c81e91bc \\\n    --hash=sha256:74937acd22dc11b33946b67dca7680e6d103d6e90eeaaaf932603bec6fe7b03a \\\n    --hash=sha256:7a2872ee80dcf6b5dbdc838763d26554c2a18aa833d31a2635bff16aafefb9c9 \\\n    --hash=sha256:7d434ec7e2ce3cc8f452d1cd9a28591745de022f931d67be688a737320dfcead \\\n    --hash=sha256:977525a1e5f4059316b183fb4fd34fa858c9eade31f165427a3977c95e3ee749 \\\n    --hash=sha256:9cd2a7376f7b3367019b664c21f0c61766219faa3b03731113ead75107f3b66c \\\n    --hash=sha256:a289af9a1687c6cf463478f0fa8e8aa3b6fb813317b0d70bf1ed0759eab6f761 \\\n    --hash=sha256:ae2b5b5c3ef67354824fb75517c8db5fbe93bc02cd9671f3c62271626bc041d5 \\\n    --hash=sha256:bc9efc739cc6ed760f795806f67889923f7274276f0eb45092a1473e40d9b867 \\\n    --hash=sha256:c1da416ab53e4f7f3bc8d4eeba36d801cc1894b9fbfbf2022b29b6bad34a7df2 \\\n    --hash=sha256:d5bd550001d26450bd90777736c69d68c487d17bf371438f975229b2b8241a91 \\\n    --hash=sha256:df6509e1507ca0760787a199d19439cc887bfd82226f5af746d6977bd9f66844 \\\n    --hash=sha256:e0a9a1a39d4bf3517f2af9d23d479b4175ead205c592ceeb8b89af48a327ea57 \\\n    --hash=sha256:eccce86bba940bae0d8d48ed925f21dbb813519169246e2ab292b5092aba121f \\\n    --hash=sha256:f99b600aa7f65235a5a05d0b9a9f31150c390f31261f2a0ba678e26823ec38f7\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   numba\nlm-format-enforcer==0.10.11 \\\n    --hash=sha256:563e0dbc930a6d50fb687951506c5de098c6e962601be0ce723f3b7d0b916a1b \\\n    --hash=sha256:8ab371924e166a1df68f243aca73a8a647bea5909f37edd6a53a694e7e7c3274\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   vllm\nlz4==4.3.3 \\\n    --hash=sha256:01fe674ef2889dbb9899d8a67361e0c4a2c833af5aeb37dd505727cf5d2a131e \\\n    --hash=sha256:054b4631a355606e99a42396f5db4d22046a3397ffc3269a348ec41eaebd69d2 \\\n    --hash=sha256:0a136e44a16fc98b1abc404fbabf7f1fada2bdab6a7e970974fb81cf55b636d0 \\\n    --hash=sha256:0e9c410b11a31dbdc94c05ac3c480cb4b222460faf9231f12538d0074e56c563 \\\n    --hash=sha256:222a7e35137d7539c9c33bb53fcbb26510c5748779364014235afc62b0ec797f \\\n    --hash=sha256:24b3206de56b7a537eda3a8123c644a2b7bf111f0af53bc14bed90ce5562d1aa \\\n    --hash=sha256:2b901c7784caac9a1ded4555258207d9e9697e746cc8532129f150ffe1f6ba0d \\\n    --hash=sha256:2f7b1839f795315e480fb87d9bc60b186a98e3e5d17203c6e757611ef7dcef61 \\\n    --hash=sha256:30e8c20b8857adef7be045c65f47ab1e2c4fabba86a9fa9a997d7674a31ea6b6 \\\n    --hash=sha256:31ea4be9d0059c00b2572d700bf2c1bc82f241f2c3282034a759c9a4d6ca4dc2 \\\n    --hash=sha256:337cb94488a1b060ef1685187d6ad4ba8bc61d26d631d7ba909ee984ea736be1 \\\n    --hash=sha256:33c9a6fd20767ccaf70649982f8f3eeb0884035c150c0b818ea660152cf3c809 \\\n    --hash=sha256:363ab65bf31338eb364062a15f302fc0fab0a49426051429866d71c793c23394 \\\n    --hash=sha256:43cf03059c0f941b772c8aeb42a0813d68d7081c009542301637e5782f8a33e2 \\\n    --hash=sha256:56f4fe9c6327adb97406f27a66420b22ce02d71a5c365c48d6b656b4aaeb7775 \\\n    --hash=sha256:5d35533bf2cee56f38ced91f766cd0038b6abf46f438a80d50c52750088be93f \\\n    --hash=sha256:6756212507405f270b66b3ff7f564618de0606395c0fe10a7ae2ffcbbe0b1fba \\\n    --hash=sha256:6cdc60e21ec70266947a48839b437d46025076eb4b12c76bd47f8e5eb8a75dcc \\\n    --hash=sha256:abc197e4aca8b63f5ae200af03eb95fb4b5055a8f990079b5bdf042f568469dd \\\n    --hash=sha256:b14d948e6dce389f9a7afc666d60dd1e35fa2138a8ec5306d30cd2e30d36b40c \\\n    --hash=sha256:b47839b53956e2737229d70714f1d75f33e8ac26e52c267f0197b3189ca6de24 \\\n    --hash=sha256:b6d9ec061b9eca86e4dcc003d93334b95d53909afd5a32c6e4f222157b50c071 \\\n    --hash=sha256:b891880c187e96339474af2a3b2bfb11a8e4732ff5034be919aa9029484cd201 \\\n    --hash=sha256:bca8fccc15e3add173da91be8f34121578dc777711ffd98d399be35487c934bf \\\n    --hash=sha256:c81703b12475da73a5d66618856d04b1307e43428a7e59d98cfe5a5d608a74c6 \\\n    --hash=sha256:d2507ee9c99dbddd191c86f0e0c8b724c76d26b0602db9ea23232304382e1f21 \\\n    --hash=sha256:e36cd7b9d4d920d3bfc2369840da506fa68258f7bb176b8743189793c055e43d \\\n    --hash=sha256:e7d84b479ddf39fe3ea05387f10b779155fc0990125f4fb35d636114e1c63a2e \\\n    --hash=sha256:eac9af361e0d98335a02ff12fb56caeb7ea1196cf1a49dbf6f17828a131da807 \\\n    --hash=sha256:edfd858985c23523f4e5a7526ca6ee65ff930207a7ec8a8f57a01eae506aaee7 \\\n    --hash=sha256:ee9ff50557a942d187ec85462bb0960207e7ec5b19b3b48949263993771c6205 \\\n    --hash=sha256:f0e822cd7644995d9ba248cb4b67859701748a93e2ab7fc9bc18c599a52e4604 \\\n    --hash=sha256:f180904f33bdd1e92967923a43c22899e303906d19b2cf8bb547db6653ea6e7d \\\n    --hash=sha256:f1d18718f9d78182c6b60f568c9a9cec8a7204d7cb6fad4e511a2ef279e4cb05 \\\n    --hash=sha256:f4c7bf687303ca47d69f9f0133274958fd672efaa33fb5bcde467862d6c621f0 \\\n    --hash=sha256:f76176492ff082657ada0d0f10c794b6da5800249ef1692b35cf49b1e93e8ef7\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\nmarkdown-it-py==2.2.0 \\\n    --hash=sha256:5a35f8d1870171d9acc47b99612dc146129b631baf04970128b568f190d0cc30 \\\n    --hash=sha256:7c9a5e412688bc771c67432cbfebcdd686c93ce6484913dccf06cb5a0bea35a1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   rich\nmarkupsafe==2.1.3 \\\n    --hash=sha256:05fb21170423db021895e1ea1e1f3ab3adb85d1c2333cbc2310f2a26bc77272e \\\n    --hash=sha256:0a4e4a1aff6c7ac4cd55792abf96c915634c2b97e3cc1c7129578aa68ebd754e \\\n    --hash=sha256:10bbfe99883db80bdbaff2dcf681dfc6533a614f700da1287707e8a5d78a8431 \\\n    --hash=sha256:134da1eca9ec0ae528110ccc9e48041e0828d79f24121a1a146161103c76e686 \\\n    --hash=sha256:14ff806850827afd6b07a5f32bd917fb7f45b046ba40c57abdb636674a8b559c \\\n    --hash=sha256:1577735524cdad32f9f694208aa75e422adba74f1baee7551620e43a3141f559 \\\n    --hash=sha256:1b40069d487e7edb2676d3fbdb2b0829ffa2cd63a2ec26c4938b2d34391b4ecc \\\n    --hash=sha256:1b8dd8c3fd14349433c79fa8abeb573a55fc0fdd769133baac1f5e07abf54aeb \\\n    --hash=sha256:1f67c7038d560d92149c060157d623c542173016c4babc0c1913cca0564b9939 \\\n    --hash=sha256:282c2cb35b5b673bbcadb33a585408104df04f14b2d9b01d4c345a3b92861c2c \\\n    --hash=sha256:2c1b19b3aaacc6e57b7e25710ff571c24d6c3613a45e905b1fde04d691b98ee0 \\\n    --hash=sha256:2ef12179d3a291be237280175b542c07a36e7f60718296278d8593d21ca937d4 \\\n    --hash=sha256:338ae27d6b8745585f87218a3f23f1512dbf52c26c28e322dbe54bcede54ccb9 \\\n    --hash=sha256:3c0fae6c3be832a0a0473ac912810b2877c8cb9d76ca48de1ed31e1c68386575 \\\n    --hash=sha256:3fd4abcb888d15a94f32b75d8fd18ee162ca0c064f35b11134be77050296d6ba \\\n    --hash=sha256:42de32b22b6b804f42c5d98be4f7e5e977ecdd9ee9b660fda1a3edf03b11792d \\\n    --hash=sha256:47d4f1c5f80fc62fdd7777d0d40a2e9dda0a05883ab11374334f6c4de38adffd \\\n    --hash=sha256:504b320cd4b7eff6f968eddf81127112db685e81f7e36e75f9f84f0df46041c3 \\\n    --hash=sha256:525808b8019e36eb524b8c68acdd63a37e75714eac50e988180b169d64480a00 \\\n    --hash=sha256:56d9f2ecac662ca1611d183feb03a3fa4406469dafe241673d521dd5ae92a155 \\\n    --hash=sha256:5bbe06f8eeafd38e5d0a4894ffec89378b6c6a625ff57e3028921f8ff59318ac \\\n    --hash=sha256:65c1a9bcdadc6c28eecee2c119465aebff8f7a584dd719facdd9e825ec61ab52 \\\n    --hash=sha256:68e78619a61ecf91e76aa3e6e8e33fc4894a2bebe93410754bd28fce0a8a4f9f \\\n    --hash=sha256:69c0f17e9f5a7afdf2cc9fb2d1ce6aabdb3bafb7f38017c0b77862bcec2bbad8 \\\n    --hash=sha256:6b2b56950d93e41f33b4223ead100ea0fe11f8e6ee5f641eb753ce4b77a7042b \\\n    --hash=sha256:715d3562f79d540f251b99ebd6d8baa547118974341db04f5ad06d5ea3eb8007 \\\n    --hash=sha256:787003c0ddb00500e49a10f2844fac87aa6ce977b90b0feaaf9de23c22508b24 \\\n    --hash=sha256:7ef3cb2ebbf91e330e3bb937efada0edd9003683db6b57bb108c4001f37a02ea \\\n    --hash=sha256:8023faf4e01efadfa183e863fefde0046de576c6f14659e8782065bcece22198 \\\n    --hash=sha256:8758846a7e80910096950b67071243da3e5a20ed2546e6392603c096778d48e0 \\\n    --hash=sha256:8afafd99945ead6e075b973fefa56379c5b5c53fd8937dad92c662da5d8fd5ee \\\n    --hash=sha256:8c41976a29d078bb235fea9b2ecd3da465df42a562910f9022f1a03107bd02be \\\n    --hash=sha256:8e254ae696c88d98da6555f5ace2279cf7cd5b3f52be2b5cf97feafe883b58d2 \\\n    --hash=sha256:8f9293864fe09b8149f0cc42ce56e3f0e54de883a9de90cd427f191c346eb2e1 \\\n    --hash=sha256:9402b03f1a1b4dc4c19845e5c749e3ab82d5078d16a2a4c2cd2df62d57bb0707 \\\n    --hash=sha256:962f82a3086483f5e5f64dbad880d31038b698494799b097bc59c2edf392fce6 \\\n    --hash=sha256:9aad3c1755095ce347e26488214ef77e0485a3c34a50c5a5e2471dff60b9dd9c \\\n    --hash=sha256:9dcdfd0eaf283af041973bff14a2e143b8bd64e069f4c383416ecd79a81aab58 \\\n    --hash=sha256:aa57bd9cf8ae831a362185ee444e15a93ecb2e344c8e52e4d721ea3ab6ef1823 \\\n    --hash=sha256:aa7bd130efab1c280bed0f45501b7c8795f9fdbeb02e965371bbef3523627779 \\\n    --hash=sha256:ab4a0df41e7c16a1392727727e7998a467472d0ad65f3ad5e6e765015df08636 \\\n    --hash=sha256:ad9e82fb8f09ade1c3e1b996a6337afac2b8b9e365f926f5a61aacc71adc5b3c \\\n    --hash=sha256:af598ed32d6ae86f1b747b82783958b1a4ab8f617b06fe68795c7f026abbdcad \\\n    --hash=sha256:b076b6226fb84157e3f7c971a47ff3a679d837cf338547532ab866c57930dbee \\\n    --hash=sha256:b7ff0f54cb4ff66dd38bebd335a38e2c22c41a8ee45aa608efc890ac3e3931bc \\\n    --hash=sha256:bfce63a9e7834b12b87c64d6b155fdd9b3b96191b6bd334bf37db7ff1fe457f2 \\\n    --hash=sha256:c011a4149cfbcf9f03994ec2edffcb8b1dc2d2aede7ca243746df97a5d41ce48 \\\n    --hash=sha256:c9c804664ebe8f83a211cace637506669e7890fec1b4195b505c214e50dd4eb7 \\\n    --hash=sha256:ca379055a47383d02a5400cb0d110cef0a776fc644cda797db0c5696cfd7e18e \\\n    --hash=sha256:cb0932dc158471523c9637e807d9bfb93e06a95cbf010f1a38b98623b929ef2b \\\n    --hash=sha256:cd0f502fe016460680cd20aaa5a76d241d6f35a1c3350c474bac1273803893fa \\\n    --hash=sha256:ceb01949af7121f9fc39f7d27f91be8546f3fb112c608bc4029aef0bab86a2a5 \\\n    --hash=sha256:d080e0a5eb2529460b30190fcfcc4199bd7f827663f858a226a81bc27beaa97e \\\n    --hash=sha256:dd15ff04ffd7e05ffcb7fe79f1b98041b8ea30ae9234aed2a9168b5797c3effb \\\n    --hash=sha256:df0be2b576a7abbf737b1575f048c23fb1d769f267ec4358296f31c2479db8f9 \\\n\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset from NumPy Array - Ray\nDESCRIPTION: This code snippet illustrates creating a Ray dataset from a NumPy array using the `ray.data.from_numpy` function. The outer axis of the array is treated as the row dimension.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/loading-data.rst#2025-04-12_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport ray\n\narray = np.ones((3, 2, 2))\nds = ray.data.from_numpy(array)\n\nprint(ds)\n```\n\n----------------------------------------\n\nTITLE: Specifying Pillow Package with Hash Verification\nDESCRIPTION: Defines the pillow package version 10.3.0 with multiple SHA256 hash verifications for security verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_29\n\nLANGUAGE: text\nCODE:\n```\npillow==10.3.0 \\\n    --hash=sha256:048ad577748b9fa4a99a0548c64f2cb8d672d5bf2e643a739ac8faff1164238c \\\n    --hash=sha256:048eeade4c33fdf7e08da40ef402e748df113fd0b4584e32c4af74fe78baaeb2 \\\n    --hash=sha256:0ba26351b137ca4e0db0342d5d00d2e355eb29372c05afd544ebf47c0956ffeb \\\n    --hash=sha256:0ea2a783a2bdf2a561808fe4a7a12e9aa3799b701ba305de596bc48b8bdfce9d \\\n    --hash=sha256:1530e8f3a4b965eb6a7785cf17a426c779333eb62c9a7d1bbcf3ffd5bf77a4aa \\\n    --hash=sha256:16563993329b79513f59142a6b02055e10514c1a8e86dca8b48a893e33cf91e3 \\\n    --hash=sha256:19aeb96d43902f0a783946a0a87dbdad5c84c936025b8419da0a0cd7724356b1 \\\n    --hash=sha256:1a1d1915db1a4fdb2754b9de292642a39a7fb28f1736699527bb649484fb966a \\\n    --hash=sha256:1b87bd9d81d179bd8ab871603bd80d8645729939f90b71e62914e816a76fc6bd \\\n    --hash=sha256:1dfc94946bc60ea375cc39cff0b8da6c7e5f8fcdc1d946beb8da5c216156ddd8 \\\n    --hash=sha256:2034f6759a722da3a3dbd91a81148cf884e91d1b747992ca288ab88c1de15999 \\\n    --hash=sha256:261ddb7ca91fcf71757979534fb4c128448b5b4c55cb6152d280312062f69599 \\\n    --hash=sha256:2ed854e716a89b1afcedea551cd85f2eb2a807613752ab997b9974aaa0d56936 \\\n    --hash=sha256:3102045a10945173d38336f6e71a8dc71bcaeed55c3123ad4af82c52807b9375 \\\n    --hash=sha256:339894035d0ede518b16073bdc2feef4c991ee991a29774b33e515f1d308e08d \\\n    --hash=sha256:412444afb8c4c7a6cc11a47dade32982439925537e483be7c0ae0cf96c4f6a0b \\\n    --hash=sha256:4203efca580f0dd6f882ca211f923168548f7ba334c189e9eab1178ab840bf60 \\\n    --hash=sha256:45ebc7b45406febf07fef35d856f0293a92e7417ae7933207e90bf9090b70572 \\\n    --hash=sha256:4b5ec25d8b17217d635f8935dbc1b9aa5907962fae29dff220f2659487891cd3 \\\n    --hash=sha256:4c8e73e99da7db1b4cad7f8d682cf6abad7844da39834c288fbfa394a47bbced \\\n    --hash=sha256:4e6f7d1c414191c1199f8996d3f2282b9ebea0945693fb67392c75a3a320941f \\\n    --hash=sha256:4eaa22f0d22b1a7e93ff0a596d57fdede2e550aecffb5a1ef1106aaece48e96b \\\n    --hash=sha256:50b8eae8f7334ec826d6eeffaeeb00e36b5e24aa0b9df322c247539714c6df19 \\\n    --hash=sha256:50fd3f6b26e3441ae07b7c979309638b72abc1a25da31a81a7fbd9495713ef4f \\\n    --hash=sha256:51243f1ed5161b9945011a7360e997729776f6e5d7005ba0c6879267d4c5139d \\\n    --hash=sha256:5d512aafa1d32efa014fa041d38868fda85028e3f930a96f85d49c7d8ddc0383 \\\n    --hash=sha256:5f77cf66e96ae734717d341c145c5949c63180842a545c47a0ce7ae52ca83795 \\\n    --hash=sha256:6b02471b72526ab8a18c39cb7967b72d194ec53c1fd0a70b050565a0f366d355 \\\n    --hash=sha256:6fb1b30043271ec92dc65f6d9f0b7a830c210b8a96423074b15c7bc999975f57 \\\n    --hash=sha256:7161ec49ef0800947dc5570f86568a7bb36fa97dd09e9827dc02b718c5643f09 \\\n    --hash=sha256:72d622d262e463dfb7595202d229f5f3ab4b852289a1cd09650362db23b9eb0b \\\n    --hash=sha256:74d28c17412d9caa1066f7a31df8403ec23d5268ba46cd0ad2c50fb82ae40462 \\\n    --hash=sha256:78618cdbccaa74d3f88d0ad6cb8ac3007f1a6fa5c6f19af64b55ca170bfa1edf \\\n    --hash=sha256:793b4e24db2e8742ca6423d3fde8396db336698c55cd34b660663ee9e45ed37f \\\n    --hash=sha256:798232c92e7665fe82ac085f9d8e8ca98826f8e27859d9a96b41d519ecd2e49a \\\n    --hash=sha256:81d09caa7b27ef4e61cb7d8fbf1714f5aec1c6b6c5270ee53504981e6e9121ad \\\n    --hash=sha256:8ab74c06ffdab957d7670c2a5a6e1a70181cd10b727cd788c4dd9005b6a8acd9 \\\n    --hash=sha256:8eb0908e954d093b02a543dc963984d6e99ad2b5e36503d8a0aaf040505f747d \\\n    --hash=sha256:90b9e29824800e90c84e4022dd5cc16eb2d9605ee13f05d47641eb183cd73d45 \\\n    --hash=sha256:9797a6c8fe16f25749b371c02e2ade0efb51155e767a971c61734b1bf6293994 \\\n    --hash=sha256:9d2455fbf44c914840c793e89aa82d0e1763a14253a000743719ae5946814b2d \\\n    --hash=sha256:9d3bea1c75f8c53ee4d505c3e67d8c158ad4df0d83170605b50b64025917f338 \\\n    --hash=sha256:9e2ec1e921fd07c7cda7962bad283acc2f2a9ccc1b971ee4b216b75fad6f0463 \\\n    --hash=sha256:9e91179a242bbc99be65e139e30690e081fe6cb91a8e77faf4c409653de39451 \\\n    --hash=sha256:a0eaa93d054751ee9964afa21c06247779b90440ca41d184aeb5d410f20ff591 \\\n    --hash=sha256:a2c405445c79c3f5a124573a051062300936b0281fee57637e706453e452746c \\\n    --hash=sha256:aa7e402ce11f0885305bfb6afb3434b3cd8f53b563ac065452d9d5654c7b86fd \\\n    --hash=sha256:aff76a55a8aa8364d25400a210a65ff59d0168e0b4285ba6bf2bd83cf675ba32 \\\n    --hash=sha256:b09b86b27a064c9624d0a6c54da01c1beaf5b6cadfa609cf63789b1d08a797b9 \\\n    --hash=sha256:b14f16f94cbc61215115b9b1236f9c18403c15dd3c52cf629072afa9d54c1cbf \\\n    --hash=sha256:b50811d664d392f02f7761621303eba9d1b056fb1868c8cdf4231279645c25f5 \\\n    --hash=sha256:b7bc2176354defba3edc2b9a777744462da2f8e921fbaf61e52acb95bafa9828 \\\n    --hash=sha256:c78e1b00a87ce43bb37642c0812315b411e856a905d58d597750eb79802aaaa3 \\\n```\n\n----------------------------------------\n\nTITLE: Multiple Model Configuration in YAML\nDESCRIPTION: Configuration demonstrating how to define multiple applications with shared code but different parameters\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/app-builder-guide.md#2025-04-12_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napplications:\n  - name: Model1\n    import_path: my_module:my_model_code\n    args:\n      model_uri: s3://my_bucket/model_1\n  - name: Model2\n    import_path: my_module:my_model_code\n    args:\n      model_uri: s3://my_bucket/model_2\n```\n\n----------------------------------------\n\nTITLE: Launching Ray TorchTrainer Job\nDESCRIPTION: Shows how to initialize and launch a distributed training job using TorchTrainer with the configured scaling and storage settings\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/common/torch-configure-run.rst#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.train.torch import TorchTrainer\n\ntrainer = TorchTrainer(\n    train_func, scaling_config=scaling_config, run_config=run_config\n)\nresult = trainer.fit()\n```\n\n----------------------------------------\n\nTITLE: Custom Experiment Configuration with Ray Tune\nDESCRIPTION: Configures a custom experiment with Ray Tune, illustrating advanced options for customizing training and evaluation phases in reinforcement learning.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Custom experiment\n# This script showcases how to configure custom experiments in Ray Tune.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Running Build Script\nDESCRIPTION: Commands to make the build script executable and run it for creating the Docker image.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/aws-trainium/llama3.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x 0-kuberay-trn1-llama3-finetune-build-image.sh\n./0-kuberay-trn1-llama3-finetune-build-image.sh\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with Hash Values\nDESCRIPTION: Package requirements listing with exact versions and SHA256 hashes for verification. Includes dependencies like torchaudio, torchvision, transformers, and other ML-related packages with CPU variants.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:4027d982eb2781c93825ab9527f17fbbb12dbabf422298e4b954be60016f87d8 \\\n--hash=sha256:59e78aa0c690f70734e42670036d6b541930b8eabbaa18d94e090abf14cc4d91\n```\n\n----------------------------------------\n\nTITLE: Specifying Pathspec Package with Hash Verification\nDESCRIPTION: Defines the pathspec package version 0.11.2 with SHA256 hash verification and notes that it's required by cloud-requirements.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_26\n\nLANGUAGE: text\nCODE:\n```\npathspec==0.11.2 \\\n    --hash=sha256:1d6ed233af05e679efb96b1851550ea95bbb64b7c490b0f5aa52996c11e92a20 \\\n    --hash=sha256:e0d8d0ac2f12da61956eb2306b69f9469b42f4deb0f3cb6ed47b9cce9996ced3\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Netty Socket Channel Read Operation\nDESCRIPTION: Thread stack trace showing Netty's NioSocketChannel reading bytes from a socket. This represents the core I/O operation where Netty reads network data into its buffer system.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_113\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/socket/nio/NioSocketChannel:.doReadBytes_[j]\n```\n\n----------------------------------------\n\nTITLE: Injecting Private Key in Ray Configuration (Python)\nDESCRIPTION: During 'ray up' execution, the private key is injected into the configuration dictionary for SSH access to worker nodes.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/vsphere/ARCHITECTURE.md#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig[\"auth\"][\"ssh_private_key\"]\n```\n\n----------------------------------------\n\nTITLE: Installing Vale using pip (Python package manager)\nDESCRIPTION: Command to install Vale using pip, the Python package installer. This method is recommended for non-macOS systems.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/docs.md#2025-04-12_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install vale\n```\n\n----------------------------------------\n\nTITLE: Configuring RLlib Environment with AlgorithmConfig\nDESCRIPTION: Demonstrates how to use the environment configuration method of AlgorithmConfig to set up a reinforcement learning environment with specific parameters and settings.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/algorithm-config.rst#2025-04-12_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nconfig.environment(env=\"CartPole-v1\")\n```\n\n----------------------------------------\n\nTITLE: Testing Ray Serve Applications\nDESCRIPTION: Commands to test the deployed Ray Serve applications using curl\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/rayservice.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl run curl --image=radial/busyboxplus:curl -i --tty\ncurl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/fruit/ -d '[\"MANGO\", 2]'\ncurl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/calc/ -d '[\"MUL\", 3]'\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install the necessary Python packages including Ray Tune, scikit-learn, and XGBoost.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/tune-xgboost.ipynb#2025-04-12_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -q \"ray[tune]\" scikit-learn xgboost\n```\n\n----------------------------------------\n\nTITLE: Install Git-LFS\nDESCRIPTION: Command to install Git Large File Storage (LFS), which is required for storing large model files in a Git repository.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/transformers/huggingface_text_classification.ipynb#2025-04-12_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# !apt install git-lfs\n```\n\n----------------------------------------\n\nTITLE: Eager Installation Configuration\nDESCRIPTION: This snippet describes how to set the eager installation flag in a Ray runtime environment configuration. It distinguishes between eager and lazy installation scenarios.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\"eager_install\": false}\n```\n\n----------------------------------------\n\nTITLE: Configuring SSH Access for KubeRay Cluster\nDESCRIPTION: YAML configuration for enabling SSH access to a Ray head node in Kubernetes. It defines port 22 for SSH access to the ray-head container.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-observability/ray-distributed-debugger.rst#2025-04-12_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nports:\n- containerPort: 22\n  name: ssd\n```\n\n----------------------------------------\n\nTITLE: Accessing Bash Prompt in Kubernetes Pod\nDESCRIPTION: Kubernetes command to open an interactive bash shell inside a pod for direct debugging and inspection.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/kuberay/README.md#2025-04-12_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec -it <pod> bash\n```\n\n----------------------------------------\n\nTITLE: Specifying platformdirs 3.11.0 Dependency with Hash Verification\nDESCRIPTION: Defines the platformdirs package at version 3.11.0 with SHA256 hash verification values. This is imported from requirements_compiled_rayllm_test_py311_cu121.txt and used by virtualenv.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_26\n\nLANGUAGE: plaintext\nCODE:\n```\nplatformdirs==3.11.0 \\\n    --hash=sha256:cf8ee52a3afdb965072dcc652433e0c7e3e40cf5ea1477cd4b3b1d2eb75495b3 \\\n    --hash=sha256:e9d171d00af68be50e9202731309c4e658fd8bc76f55c11c7dd760d023bda68e\n```\n\n----------------------------------------\n\nTITLE: Seeding PyTorch and TensorFlow Random Number Generators\nDESCRIPTION: This snippet demonstrates how to seed the random number generators in PyTorch and TensorFlow for reproducible experiments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/faq.rst#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport tensorflow as tf\n\ntorch.manual_seed(0)\ntf.random.set_seed(0)\n```\n\n----------------------------------------\n\nTITLE: Set Custom Action Distribution Class (Placeholder)\nDESCRIPTION: This snippet shows how to set a custom action distribution class within the new RLlib API. `YOUR_DIST_CLASS` represents a placeholder for the user's custom distribution class.  It is used to configure the action distribution to be used by the policy.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/new-api-stack-migration-guide.rst#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"self.action_dist_cls = YOUR_DIST_CLASS\"\n```\n\n----------------------------------------\n\nTITLE: Monitoring Autoscaler Logs in Kubernetes\nDESCRIPTION: Command to view the last 20 lines of autoscaler logs from the head pod using kubectl\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs $HEAD_POD -c autoscaler | tail -n 20\n```\n\n----------------------------------------\n\nTITLE: Running Ray Java Tests with Bazel\nDESCRIPTION: Commands for running Ray Java tests using Bazel. The first command runs all Java tests with streamed output, while the second command runs custom tests specifically.\nSOURCE: https://github.com/ray-project/ray/blob/master/java/README.md#2025-04-12_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# To run ray tests.\nbazel test //java:all_tests --test_output=streamed\n# To run custom tests.\nbazel test //java:custom_test --test_output=streamed\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Requirements with Pinned Versions and Hashes\nDESCRIPTION: This code defines Python package dependencies with specific version requirements, SHA256 hash verification values, and comment annotations showing where each package is required in the project structure.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_37\n\nLANGUAGE: pip\nCODE:\n```\nstarlette==0.37.2 \\\n    --hash=sha256:6fe59f29268538e5d0d182f2791a479a0c64638e6935d1c6989e63fb2699c6ee \\\n    --hash=sha256:9af890290133b79fc3db55474ade20f6220a364a0402e0b556e7cd5e1e093823\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\n    #   fastapi\n    #   prometheus-fastapi-instrumentator\nsympy==1.13.1 \\\n    --hash=sha256:9cebf7e04ff162015ce31c9c6c9144daa34a93bd082f54fd8f12deca4f47515f \\\n    --hash=sha256:db36cdc64bf61b9b24578b6f7bab1ecdd2452cf008f34faa33776680c26d66f8\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   torch\ntensorboardx==2.6.2.2 \\\n    --hash=sha256:160025acbf759ede23fd3526ae9d9bfbfd8b68eb16c38a010ebe326dc6395db8 \\\n    --hash=sha256:c6476d7cd0d529b0b72f4acadb1269f9ed8b22f441e87a84f2a3b940bb87b666\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   -r python/requirements.txt\ntifffile==2024.7.21 \\\n    --hash=sha256:7f335b5d6ca49401fe0f1d87deb206f5dae47297e47b1ed52a676d05d6d26798 \\\n    --hash=sha256:818b577d49350421fb511f389f937984f9feaa2cd8177fa00823001920bf3483\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   scikit-image\ntiktoken==0.9.0 \\\n    --hash=sha256:03935988a91d6d3216e2ec7c645afbb3d870b37bcb67ada1943ec48678e7ee33 \\\n    --hash=sha256:11a20e67fdf58b0e2dea7b8654a288e481bb4fc0289d3ad21291f8d0849915fb \\\n    --hash=sha256:15a2752dea63d93b0332fb0ddb05dd909371ededa145fe6a3242f46724fa7990 \\\n    --hash=sha256:26113fec3bd7a352e4b33dbaf1bd8948de2507e30bd95a44e2b1156647bc01b4 \\\n    --hash=sha256:26242ca9dc8b58e875ff4ca078b9a94d2f0813e6a535dcd2205df5d49d927cc7 \\\n    --hash=sha256:27d457f096f87685195eea0165a1807fae87b97b2161fe8c9b1df5bd74ca6f63 \\\n    --hash=sha256:2b0e8e05a26eda1249e824156d537015480af7ae222ccb798e5234ae0285dbdb \\\n    --hash=sha256:2cf8ded49cddf825390e36dd1ad35cd49589e8161fdcb52aa25f0583e90a3e01 \\\n    --hash=sha256:3ebcec91babf21297022882344c3f7d9eed855931466c3311b1ad6b64befb3df \\\n    --hash=sha256:45556bc41241e5294063508caf901bf92ba52d8ef9222023f83d2483a3055348 \\\n    --hash=sha256:586c16358138b96ea804c034b8acf3f5d3f0258bd2bc3b0227af4af5d622e382 \\\n    --hash=sha256:5a62d7a25225bafed786a524c1b9f0910a1128f4232615bf3f8257a73aaa3b16 \\\n    --hash=sha256:5ea0edb6f83dc56d794723286215918c1cde03712cbbafa0348b33448faf5b95 \\\n    --hash=sha256:75f6d5db5bc2c6274b674ceab1615c1778e6416b14705827d19b40e6355f03e0 \\\n    --hash=sha256:8b3d80aad8d2c6b9238fc1a5524542087c52b860b10cbf952429ffb714bc1136 \\\n    --hash=sha256:92a5fb085a6a3b7350b8fc838baf493317ca0e17bd95e8642f95fc69ecfed1de \\\n    --hash=sha256:95e811743b5dfa74f4b227927ed86cbc57cad4df859cb3b643be797914e41794 \\\n    --hash=sha256:99376e1370d59bcf6935c933cb9ba64adc29033b7e73f5f7569f3aad86552b22 \\\n    --hash=sha256:a6600660f2f72369acb13a57fb3e212434ed38b045fd8cc6cdd74947b4b5d210 \\\n    --hash=sha256:b2a21133be05dc116b1d0372af051cd2c6aa1d2188250c9b553f9fa49301b336 \\\n    --hash=sha256:badb947c32739fb6ddde173e14885fb3de4d32ab9d8c591cbd013c22b4c31dd2 \\\n    --hash=sha256:c6386ca815e7d96ef5b4ac61e0048cd32ca5a92d5781255e13b31381d28667dc \\\n    --hash=sha256:cc156cb314119a8bb9748257a2eaebd5cc0753b6cb491d26694ed42fc7cb3139 \\\n    --hash=sha256:cd69372e8c9dd761f0ab873112aba55a0e3e506332dd9f7522ca466e817b1b7a \\\n    --hash=sha256:d02a5ca6a938e0490e1ff957bc48c8b078c88cb83977be1625b1fd8aac792c5d \\\n    --hash=sha256:d9c59ccc528c6c5dd51820b3474402f69d9a9e1d656226848ad68a8d5b2e5108 \\\n    --hash=sha256:e15b16f61e6f4625a57a36496d28dd182a8a60ec20a534c5343ba3cafa156ac7 \\\n    --hash=sha256:e5fd49e7799579240f03913447c0cdfa1129625ebd5ac440787afc4345990427 \\\n    --hash=sha256:e88f121c1c22b726649ce67c089b90ddda8b9662545a8aeb03cfef15967ddd03 \\\n    --hash=sha256:f0968d5beeafbca2a72c595e8385a1a1f8af58feaebb02b227229b69ca5357fd \\\n    --hash=sha256:f32cc56168eac4851109e9b5d327637f15fd662aa30dd79f964b7c39fbadd26e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   mistral-common\n    #   vllm\n    #   xgrammar\ntokenizers==0.21.0 \\\n    --hash=sha256:089d56db6782a73a27fd8abf3ba21779f5b85d4a9f35e3b493c7bbcbbf0d539b \\\n    --hash=sha256:3c4c93eae637e7d2aaae3d376f06085164e1660f89304c0ab2b1d08a406636b2 \\\n    --hash=sha256:400832c0904f77ce87c40f1a8a27493071282f785724ae62144324f171377273 \\\n    --hash=sha256:4145505a973116f91bc3ac45988a92e618a6f83eb458f49ea0790df94ee243ff \\\n    --hash=sha256:6b177fb54c4702ef611de0c069d9169f0004233890e0c4c5bd5508ae05abf193 \\\n    --hash=sha256:6b43779a269f4629bebb114e19c3fca0223296ae9fea8bb9a7a6c6fb0657ff8e \\\n    --hash=sha256:87841da5a25a3a5f70c102de371db120f41873b854ba65e52bccd57df5a3780c \\\n    --hash=sha256:9aeb255802be90acfd363626753fda0064a8df06031012fe7d52fd9a905eb00e \\\n    --hash=sha256:c87ca3dc48b9b1222d984b6b7490355a6fdb411a2d810f6f05977258400ddb74 \\\n    --hash=sha256:d8b09dbeb7a8d73ee204a70f94fc06ea0f17dcf0844f16102b9f414f0b7463ba \\\n    --hash=sha256:e84ca973b3a96894d1707e189c14a774b701596d579ffc7e69debfc036a61a04 \\\n    --hash=sha256:eb1702c2f27d25d9dd5b389cc1f2f51813e99f8ca30d9e25348db6585a97e24a \\\n    --hash=sha256:eb7202d231b273c34ec67767378cd04c767e967fda12d4a9e36208a34e2f137e \\\n    --hash=sha256:ee0894bf311b75b0c03079f33859ae4b2334d675d4e93f5a4132e1eae2834fe4 \\\n    --hash=sha256:f53ea537c925422a2e0e92a24cce96f6bc5046bbef24a1652a5edc8ba975f62e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   transformers\n    #   vllm\ntorch==2.6.0 \\\n    --hash=sha256:09e06f9949e1a0518c5b09fe95295bc9661f219d9ecb6f9893e5123e10696628 \\\n    --hash=sha256:265f70de5fd45b864d924b64be1797f86e76c8e48a02c2a3a6fc7ec247d2226c \\\n    --hash=sha256:2bb8987f3bb1ef2675897034402373ddfc8f5ef0e156e2d8cfc47cacafdda4a9 \\\n    --hash=sha256:46763dcb051180ce1ed23d1891d9b1598e07d051ce4c9d14307029809c4d64f7 \\\n    --hash=sha256:4874a73507a300a5d089ceaff616a569e7bb7c613c56f37f63ec3ffac65259cf \\\n    --hash=sha256:510c73251bee9ba02ae1cb6c9d4ee0907b3ce6020e62784e2d7598e0cfa4d6cc \\\n    --hash=sha256:56eeaf2ecac90da5d9e35f7f35eb286da82673ec3c582e310a8d1631a1c02341 \\\n    --hash=sha256:683410f97984103148e31b38a8631acf31c3034c020c0f4d26171e7626d8317a \\\n    --hash=sha256:6860df13d9911ac158f4c44031609700e1eba07916fff62e21e6ffa0a9e01961 \\\n    --hash=sha256:7979834102cd5b7a43cc64e87f2f3b14bd0e1458f06e9f88ffa386d07c7446e1 \\\n    --hash=sha256:7e1448426d0ba3620408218b50aa6ada88aeae34f7a239ba5431f6c8774b1239 \\\n    --hash=sha256:94fc63b3b4bedd327af588696559f68c264440e2503cc9e6954019473d74ae21 \\\n    --hash=sha256:9a610afe216a85a8b9bc9f8365ed561535c93e804c2a317ef7fabcc5deda0989 \\\n    --hash=sha256:9ea955317cfcd3852b1402b62af258ce735c2edeee42ca9419b6bc889e5ae053 \\\n    --hash=sha256:a0d5e1b9874c1a6c25556840ab8920569a7a4137afa8a63a32cee0bc7d89bd4b \\\n    --hash=sha256:b789069020c5588c70d5c2158ac0aa23fd24a028f34a8b4fcb8fcb4d7efcf5fb \\\n    --hash=sha256:bb2c6c3e65049f081940f5ab15c9136c7de40d3f01192541c920a07c7c585b7e \\\n    --hash=sha256:c4f103a49830ce4c7561ef4434cc7926e5a5fe4e5eb100c19ab36ea1e2b634ab \\\n    --hash=sha256:ccbd0320411fe1a3b3fec7b4d3185aa7d0c52adac94480ab024b5c8f74a0bf1d \\\n    --hash=sha256:ff96f4038f8af9f7ec4231710ed4549da1bdebad95923953a25045dcf6fd87e2\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   compressed-tensors\n    #   outlines\n    #   torchaudio\n    #   torchvision\n    #   vllm\n    #   xformers\n    #   xgrammar\ntorchaudio==2.6.0 \\\n    --hash=sha256:04803a969710bdb77a4ddfdb85a32fa9b9e0310dc91f7eb7e54d6083dd69bfab \\\n    --hash=sha256:0eda1cd876f44fc014dc04aa680db2fa355a83df5d834398db6dd5f5cd911f4c \\\n    --hash=sha256:0f0db5c997d031c34066d8be1c0ce7d2a1f2b6c016a92885b20b00bfeb17b753 \\\n    --hash=sha256:22798d5d8e37869bd5875d37f42270efbeb8ae94bda97fed40c1c5e0e1c62fa3 \\\n    --hash=sha256:377b177a3d683a9163e4cab5a06f0346dac9ff96fa527477338fd90fc6a2a4b6 \\\n    --hash=sha256:393fa74ec40d167f0170728ea21c9b5e0f830648fd02df7db2bf7e62f64245ec \\\n    --hash=sha256:52182f6de4e7b342d139e54b703185d428de9cce3c4cf914a9b2ab2359d192a3 \\\n    --hash=sha256:52f15185349c370fc1faa84e8b8b2782c007472db9d586a16bba314130b322f2 \\\n    --hash=sha256:6291d9507dc1d6b4ffe8843fbfb201e6c8270dd8c42ad70bb76226c0ebdcad56 \\\n    --hash=sha256:66f2e0bd5ab56fd81419d2f5afb74a9a70141688594646441756c8c24f424a73 \\\n    --hash=sha256:715aa21f6bdbd085454c313ae3a2c7cc07bf2e8cf05752f819afb5b4c57f4e6f \\\n    --hash=sha256:72e77055d8e742475c6dfacf59fab09b1fc94d4423e14897e188b67cad3851c6 \\\n    --hash=sha256:7d0e4b08c42325bf4b887de9a25c44ed882997001740e1bd7d901f65581cf1ab \\\n    --hash=sha256:86d6239792bf94741a41acd6fe3d549faaf0d50e7275d17d076a190bd007e2f9 \\\n    --hash=sha256:8c1a4d08e35a9ceaadadbff6e60bcb3442482f800369be350103dfd08b4ddf52 \\\n    --hash=sha256:9d8e07789452efdb8132d62afe21f2293a72805f26c2891c6c53e4e4df38ddf6 \\\n    --hash=sha256:b521ea9618fb4c29a6f8071628170c222291f46a48a3bf424cfeb488f54af714 \\\n    --hash=sha256:c12fc41241b8dfce3ccc1917f1c81a0f92f532d9917706600046f1eb21d2d765 \\\n    --hash=sha256:c6386bfa478afae2137715bb60f35520e3b05f5fc6d3bcc6969cf9cdfb11c09c \\\n    --hash=sha256:d855da878a28c2e5e6fb3d76fcddd544f4d957a320b29602cea5af2fe0ad1f3a\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   vllm\ntorchvision==0.21.0 \\\n    --hash=sha256:044ea420b8c6c3162a234cada8e2025b9076fa82504758cd11ec5d0f8cd9fa37 \\\n    --hash=sha256:084ac3f5a1f50c70d630a488d19bf62f323018eae1b1c1232f2b7047d3a7b76d \\\n    --hash=sha256:110d115333524d60e9e474d53c7d20f096dbd8a080232f88dddb90566f90064c \\\n    --hash=sha256:3891cd086c5071bda6b4ee9d266bb2ac39c998c045c2ebcd1e818b8316fb5d41 \\\n    --hash=sha256:49bcfad8cfe2c27dee116c45d4f866d7974bcf14a5a9fbef893635deae322f2f \\\n    --hash=sha256:5045a3a5f21ec3eea6962fa5f2fa2d4283f854caec25ada493fcf4aab2925467 \\\n    --hash=sha256:5083a5b1fec2351bf5ea9900a741d54086db75baec4b1d21e39451e00977f1b1 \\\n    --hash=sha256:54454923a50104c66a9ab6bd8b73a11c2fc218c964b1006d5d1fe5b442c3dcb6 \\\n    --hash=sha256:54815e0a56dde95cc6ec952577f67e0dc151eadd928e8d9f6a7f821d69a4a734\n```\n\n----------------------------------------\n\nTITLE: Ray Training Status Table Output\nDESCRIPTION: ASCII table showing the status of 4 IMPALA training runs on different Atari environments, including metrics like iterations, timesteps, and rewards. All trials are marked as TERMINATED with varying performance across different games.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.1/stress_tests/application_stress_test.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n== Status ==\nMemory usage on this node: 37.0/480.3 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/640 CPUs, 0/8 GPUs, 0.0/2526.95 GiB heap, 0.0/128.42 GiB objects\nResult logdir: /home/ubuntu/ray_results/atari-impala\nNumber of trials: 4 (4 TERMINATED)\n+---------------------------------------------+------------+-------+-----------------------------+--------+------------------+-------------+----------+\n| Trial name                                  | status     | loc   | env                         |   iter |   total time (s) |   timesteps |   reward |\n|---------------------------------------------+------------+-------+-----------------------------+--------+------------------+-------------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_511a1830      | TERMINATED |       | BreakoutNoFrameskip-v4      |     39 |      699.024 |     3050000 |   44.21  |\n| IMPALA_BeamRiderNoFrameskip-v4_511b6726     | TERMINATED |       | BeamRiderNoFrameskip-v4     |     37 |      664.972 |     3083500 |  449.72  |\n| IMPALA_QbertNoFrameskip-v4_511bd44a         | TERMINATED |       | QbertNoFrameskip-v4         |     40 |      707.975 |     3088000 |  495.464 |\n| IMPALA_SpaceInvadersNoFrameskip-v4_511c4858 | TERMINATED |       | SpaceInvadersNoFrameskip-v4 |     38 |      680.926 |     3004500 |  380.35  |\n+---------------------------------------------+------------+-------+-----------------------------+--------+------------------+-------------+----------+\n```\n\n----------------------------------------\n\nTITLE: Specifying SortedContainers Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the sortedcontainers package dependency with version 2.4.0 and SHA-256 hashes for verification. Comments indicate this package is required by distributed and is constrained by requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_42\n\nLANGUAGE: pip\nCODE:\n```\nsortedcontainers==2.4.0 \\\n    --hash=sha256:25caa5a06cc30b6b83d11423433f65d1f9d76c4c6a0c90e3379eaa43b9bfdb88 \\\n    --hash=sha256:a163dcaede0f1c021485e957a39245190e74249897e2ae4b2aa38595db237ee0\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   distributed\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with SHA256 Hashes\nDESCRIPTION: A requirements file listing Python package dependencies and their corresponding SHA256 hash values for verification. Each package has a specific version and one or more hash values to ensure package integrity during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99 \\\n--hash=sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c \\\n# via requests\nclick==8.1.7 \\\n--hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28 \\\n--hash=sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de\n```\n\n----------------------------------------\n\nTITLE: Specifying Pickleshare Library Dependency\nDESCRIPTION: This snippet defines the Pickleshare library dependency with hash verification. It's used by IPython for object persistence and is referenced from the requirements_compiled.txt file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\npickleshare==0.7.5 \\\n    --hash=sha256:87683d47965c1da65cdacaf31c8441d12b8044cdec9aca500cd78fc2c683afca \\\n    --hash=sha256:9649af414d74d4df115d5d718f82acb59c9d418196b7b4290ed47a12ce62df56\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ipython\n```\n\n----------------------------------------\n\nTITLE: Ray Performance Benchmark Metrics\nDESCRIPTION: Detailed performance metrics for Ray operations including Plasma Store operations, task execution, and actor calls. Each metric includes operations per second with standard deviation. Note indicates performance changes related to reference counting and pinning implementation.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.1/microbenchmark.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsingle client get calls (Plasma Store) per second 12550.57 +- 1835.19\nsingle client put calls (Plasma Store) per second 6791.78 +- 176.65\nsingle client put gigabytes per second 13.36 +- 5.7\nmulti client put calls (Plasma Store) per second 13503.59 +- 179.1\nmulti client put gigabytes per second 16.22 +- 1.36\nsingle client tasks sync per second 1295.56 +- 42.81\nsingle client tasks async per second 14825.7 +- 358.92\nmulti client tasks async per second 43699.93 +- 627.98\n1:1 actor calls sync per second 2194.35 +- 57.18\n1:1 actor calls async per second 6873.68 +- 87.3\n1:1 actor calls concurrent per second 7285.91 +- 50.57\n1:n actor calls async per second 13290.25 +- 140.2\nn:n actor calls async per second 45354.88 +- 678.84\nn:n actor calls with arg async per second 13668.97 +- 105.62\n```\n\n----------------------------------------\n\nTITLE: Successful Deletion Response Example\nDESCRIPTION: This HTTP response is an example of a successful deletion request to the `/api/serve/applications/` endpoint. The response indicates that the DELETE operation was successful.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/api/index.md#2025-04-12_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n```\n\n----------------------------------------\n\nTITLE: Specifying Blake3 Package with Multiple Hash Values\nDESCRIPTION: This snippet defines the blake3 package with a specific version and multiple hash values for different architectures or build configurations. It demonstrates comprehensive security verification for package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_5\n\nLANGUAGE: Text\nCODE:\n```\nblake3==1.0.4 \\\n    --hash=sha256:00605aa59923205c6a4f21131840840eb2d9a754c59b163357d890566755b97a \\\n    --hash=sha256:08f46c2f1c5f369f07409e3e4ff248bcb22617cd741f2224873d85982dd6034e \\\n    --hash=sha256:09b2c66bc2c797e9d783521ec22b1e9a6c74e3ddb98bdd0dcd4fcc2213fb27ec \\\n    --hash=sha256:0c6477a4689b374e846fd5330839c0d27d932fa62c2d2d6b731a28798d0348a0 \\\n    --hash=sha256:0f5888e358ae4bba094d4595e1703dfc230d96dea6924e877c42c7a98beda7b5 \\\n    # ... (more hash values omitted for brevity)\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: Setting Up Ray Development Environment (Python)\nDESCRIPTION: Command to run a Python script that replaces installed Ray packages with local editable copies for development.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython python/ray/setup-dev.py\n```\n\n----------------------------------------\n\nTITLE: Ray Serve Deployment Configuration\nDESCRIPTION: Defines the Ray Serve application setup including model loading and FastAPI endpoint configuration\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/tutorials/aws-neuron-core-inference.md#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n__neuron_serve_code_start__\n__neuron_serve_code_end__\n```\n\n----------------------------------------\n\nTITLE: Examining Ray Cluster Pods\nDESCRIPTION: Kubernetes command to list all pods in the cluster, which is useful for inspecting the Ray cluster after a test run.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/kuberay/README.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods\n```\n\n----------------------------------------\n\nTITLE: Deleting Ray Cluster\nDESCRIPTION: Kubernetes command to delete the Ray cluster across all namespaces, useful for cleaning up resources after testing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/kuberay/README.md#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete raycluster -A\n```\n\n----------------------------------------\n\nTITLE: Specifying Py-CPUInfo Package Dependency with Hash Verification\nDESCRIPTION: Defines the py-cpuinfo package dependency at version 9.0.0 with SHA256 hash verification. This package is required by the vllm component for CPU information detection.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_22\n\nLANGUAGE: plaintext\nCODE:\n```\npy-cpuinfo==9.0.0 \\\n    --hash=sha256:3cdbbf3fac90dc6f118bfd64384f309edeadd902d7c8fb17f02ffa1fc3f49690 \\\n    --hash=sha256:859625bc251f64e21f077d099d4162689c762b5d6a4c3c97553d56241c9674d5\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Ray Data User Guides in reStructuredText\nDESCRIPTION: This snippet defines the structure of the Ray Data user guide documentation using reStructuredText syntax. It includes a title, introduction, and a table of contents linking to various subtopics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/data/user-guide.rst#2025-04-12_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _data_user_guide:\n\n===========\nUser Guides\n===========\n\nIf you're new to Ray Data, start with the\n:ref:`Ray Data Quickstart <data_quickstart>`.\nThis user guide helps you navigate the Ray Data project and\nshow you how achieve several tasks.\n\n.. toctree::\n    :maxdepth: 2\n\n    loading-data\n    inspecting-data\n    transforming-data\n    iterating-over-data\n    shuffling-data\n    saving-data\n    working-with-images\n    working-with-text\n    working-with-tensors\n    working-with-pytorch\n    working-with-llms\n    monitoring-your-workload\n    execution-configurations\n    batch_inference\n    performance-tips\n    custom-datasource-example\n```\n\n----------------------------------------\n\nTITLE: Package Dependency for yarl with SHA256 Hash Values\nDESCRIPTION: Defines the dependency for the yarl package with version 1.18.3 and includes multiple SHA256 hash values for package verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_40\n\nLANGUAGE: text\nCODE:\n```\nyarl==1.18.3 \\\n    --hash=sha256:00e5a1fea0fd4f5bfa7440a47eff01d9822a65b4488f7cff83155a0f31a2ecba \\\n    --hash=sha256:02ddb6756f8f4517a2d5e99d8b2f272488e18dd0bfbc802f31c16c6c20f22193 \\\n    --hash=sha256:045b8482ce9483ada4f3f23b3774f4e1bf4f23a2d5c912ed5170f68efb053318 \\\n    --hash=sha256:09c7907c8548bcd6ab860e5f513e727c53b4a714f459b084f6580b49fa1b9cee \\\n    --hash=sha256:0b0cad37311123211dc91eadcb322ef4d4a66008d3e1bdc404808992260e1a0e \\\n    --hash=sha256:0b3c92fa08759dbf12b3a59579a4096ba9af8dd344d9a813fc7f5070d86bbab1 \\\n    --hash=sha256:0fb2171a4486bb075316ee754c6d8382ea6eb8b399d4ec62fde2b591f879778a \\\n    --hash=sha256:1a74a13a4c857a84a845505fd2d68e54826a2cd01935a96efb1e9d86c728e186 \\\n    --hash=sha256:1d407181cfa6e70077df3377938c08012d18893f9f20e92f7d2f314a437c30b1 \\\n    --hash=sha256:1dd4bdd05407ced96fed3d7f25dbbf88d2ffb045a0db60dbc247f5b3c5c25d50 \\\n    --hash=sha256:25b411eddcfd56a2f0cd6a384e9f4f7aa3efee14b188de13048c25b5e91f1640 \\\n    --hash=sha256:2d06d3005e668744e11ed80812e61efd77d70bb7f03e33c1598c301eea20efbb \\\n    --hash=sha256:2ec9bbba33b2d00999af4631a3397d1fd78290c48e2a3e52d8dd72db3a067ac8 \\\n    --hash=sha256:3236da9272872443f81fedc389bace88408f64f89f75d1bdb2256069a8730ccc \\\n    --hash=sha256:35098b24e0327fc4ebdc8ffe336cee0a87a700c24ffed13161af80124b7dc8e5 \\\n    --hash=sha256:41f7ce59d6ee7741af71d82020346af364949314ed3d87553763a2df1829cc58 \\\n    --hash=sha256:436c4fc0a4d66b2badc6c5fc5ef4e47bb10e4fd9bf0c79524ac719a01f3607c2 \\\n    --hash=sha256:4891ed92157e5430874dad17b15eb1fda57627710756c27422200c52d8a4e393 \\\n    --hash=sha256:4ac515b860c36becb81bb84b667466885096b5fc85596948548b667da3bf9f24 \\\n    --hash=sha256:5094d9206c64181d0f6e76ebd8fb2f8fe274950a63890ee9e0ebfd58bf9d787b \\\n    --hash=sha256:54d6921f07555713b9300bee9c50fb46e57e2e639027089b1d795ecd9f7fa910 \\\n    --hash=sha256:578e281c393af575879990861823ef19d66e2b1d0098414855dd367e234f5b3c \\\n    --hash=sha256:5a3f356548e34a70b0172d8890006c37be92995f62d95a07b4a42e90fba54272 \\\n    --hash=sha256:602d98f2c2d929f8e697ed274fbadc09902c4025c5a9963bf4e9edfc3ab6f7ed \\\n    --hash=sha256:61b1a825a13bef4a5f10b1885245377d3cd0bf87cba068e1d9a88c2ae36880e1 \\\n    --hash=sha256:61e5e68cb65ac8f547f6b5ef933f510134a6bf31bb178be428994b0cb46c2a04 \\\n    --hash=sha256:61ee62ead9b68b9123ec24bc866cbef297dd266175d53296e2db5e7f797f902d \\\n    --hash=sha256:6333c5a377c8e2f5fae35e7b8f145c617b02c939d04110c76f29ee3676b5f9a5 \\\n    --hash=sha256:6748dbf9bfa5ba1afcc7556b71cda0d7ce5f24768043a02a58846e4a443d808d \\\n    --hash=sha256:67a283dd2882ac98cc6318384f565bffc751ab564605959df4752d42483ad889 \\\n    --hash=sha256:75674776d96d7b851b6498f17824ba17849d790a44d282929c42dbb77d4f17ae \\\n    --hash=sha256:757e81cae69244257d125ff31663249b3013b5dc0a8520d73694aed497fb195b \\\n    --hash=sha256:77a6e85b90a7641d2e07184df5557132a337f136250caafc9ccaa4a2a998ca2c \\\n    --hash=sha256:7c33dd1931a95e5d9a772d0ac5e44cac8957eaf58e3c8da8c1414de7dd27c576 \\\n    --hash=sha256:7df647e8edd71f000a5208fe6ff8c382a1de8edfbccdbbfe649d263de07d8c34 \\\n    --hash=sha256:7e2ee16578af3b52ac2f334c3b1f92262f47e02cc6193c598502bd46f5cd1477 \\\n    --hash=sha256:80316a8bd5109320d38eef8833ccf5f89608c9107d02d2a7f985f98ed6876990 \\\n    --hash=sha256:82123d0c954dc58db301f5021a01854a85bf1f3bb7d12ae0c01afc414a882ca2 \\\n    --hash=sha256:84b2deecba4a3f1a398df819151eb72d29bfeb3b69abb145a00ddc8d30094512 \\\n    --hash=sha256:8503ad47387b8ebd39cbbbdf0bf113e17330ffd339ba1144074da24c545f0069 \\\n    --hash=sha256:877d209b6aebeb5b16c42cbb377f5f94d9e556626b1bfff66d7b0d115be88d0a \\\n    --hash=sha256:8874027a53e3aea659a6d62751800cf6e63314c160fd607489ba5c2edd753cf6\n```\n\n----------------------------------------\n\nTITLE: Building Ray from Source\nDESCRIPTION: Commands to clone and build Ray from source code, with symlinks enabled for Windows compatibility.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n# cd to the directory under which the ray source tree will be downloaded.\ngit clone -c core.symlinks=true https://github.com/ray-project/ray.git\ncd ray\\python\npip install -e . --verbose\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with Hashes\nDESCRIPTION: Requirements file listing package dependencies with pinned versions and SHA256 hashes for package verification. Each entry includes the package name, version, and hash values along with comments indicating where the requirement originated from.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nentrypoints==0.4 \\\n    --hash=sha256:b706eddaa9218a19ebcd67b56818f05bb27589b1ca9e8d797b74affad4ccacd4 \\\n    --hash=sha256:f174b5ff827504fd3cd97cc3f8649f3693f51538c7e4bdf3ef002c8429d42f9f\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   jupyter-client\n    #   nbconvert\n```\n\n----------------------------------------\n\nTITLE: Installing Test Dependencies\nDESCRIPTION: Command to install dependencies required for running Ray unit tests.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npip install -c python/requirements_compiled.txt -r python/requirements/test-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Header Reading\nDESCRIPTION: Java stack trace showing Netty's HTTP header reading process. This trace demonstrates how Netty processes and parses HTTP headers during message decoding.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_102\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/handler/codec/http/HttpObjectDecoder:.decode_[j];io/netty/handler/codec/http/HttpObjectDecoder:.readHeaders_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with SHA256 Hashes\nDESCRIPTION: Package dependency declarations with corresponding SHA256 hashes for secure verification during installation. Includes version constraints and platform-specific conditions.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nfarama-notifications==0.0.4 \\\n    --hash=sha256:13fceff2d14314cf80703c8266462ebf3733c7d165336eee998fc58e545efd18 \\\n    --hash=sha256:14de931035a41961f7c056361dc7f980762a143d05791ef5794a751a2caf05ae\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   gymnasium\n```\n\n----------------------------------------\n\nTITLE: Query Serve Endpoints in Python\nDESCRIPTION: This Python script shows how to send HTTP POST requests to the Whisper and Resnet applications running on Ray Serve using the requests library and interpret their responses.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/multi-app-container.md#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import requests\n>>> audio_file = \"https://storage.googleapis.com/public-lyrebird-test/test_audio_22s.wav\"\n>>> resp = requests.post(\"http://localhost:8000/whisper\", json={\"filepath\": audio_file}) # doctest: +SKIP\n>>> resp.json() # doctest: +SKIP\n{\n    \"language\": \"en\",\n    \"language_probability\": 1,\n    \"duration\": 21.775,\n    \"transcript_text\": \" Well, think about the time of our ancestors. A ping, a ding, a russling in the bushes is like, whoo, that means an immediate response. Oh my gosh, what's that thing? Oh my gosh, I have to do it right now. And dude, it's not a tiger, right? Like, but our, our body treats stress as if it's life-threatening because to quote Robert Sapolsky or butcher his quote, he's a Robert Sapolsky is like one of the most incredible stress physiologists of\",\n    \"whisper_alignments\": [\n        [\n            0.0,\n            0.36,\n            \" Well,\",\n            0.3125\n        ],\n        ...\n    ]\n}\n\n>>> link_to_image = \"https://serve-resnet-benchmark-data.s3.us-west-1.amazonaws.com/000000000019.jpeg\"\n>>> resp = requests.post(\"http://localhost:8000/resnet\", json={\"uri\": link_to_image}) # doctest: +SKIP\n>>> resp.text # doctest: +SKIP\nox\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Hash Values\nDESCRIPTION: This snippet shows how Python package dependencies are specified in a requirements file, including version numbers and SHA256 hash values for security. It also includes comments indicating which other packages or requirements files depend on each package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_46\n\nLANGUAGE: Text\nCODE:\n```\nrequests==2.31.0 \\\n    --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \\\n    --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements/cloud-requirements.txt\n    #   -r python/requirements.txt\n    #   google-api-core\n    #   google-cloud-storage\n    #   huggingface-hub\n    #   jupyterlab-server\n    #   mistral-common\n    #   outlines\n    #   ray\n    #   sphinx\n    #   tiktoken\n    #   transformers\n    #   vllm\n```\n\n----------------------------------------\n\nTITLE: yarl Package Dependency with SHA256 Hashes\nDESCRIPTION: This snippet shows the yarl package dependency with version 1.18.3 and its SHA256 hash values. The package is a dependency for the Ray project with numerous hash verifications for different distribution formats.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_36\n\nLANGUAGE: text\nCODE:\n```\nyarl==1.18.3 \\\n    --hash=sha256:00e5a1fea0fd4f5bfa7440a47eff01d9822a65b4488f7cff83155a0f31a2ecba \\\n    --hash=sha256:02ddb6756f8f4517a2d5e99d8b2f272488e18dd0bfbc802f31c16c6c20f22193 \\\n    --hash=sha256:045b8482ce9483ada4f3f23b3774f4e1bf4f23a2d5c912ed5170f68efb053318 \\\n    --hash=sha256:09c7907c8548bcd6ab860e5f513e727c53b4a714f459b084f6580b49fa1b9cee \\\n    --hash=sha256:0b0cad37311123211dc91eadcb322ef4d4a66008d3e1bdc404808992260e1a0e \\\n```\n\n----------------------------------------\n\nTITLE: Package Hash Values Configuration\nDESCRIPTION: Collection of SHA256 hash values used to verify the integrity of Python package dependencies during installation. Each line contains the hash value for a specific version of a package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:2e81c7b9c8979ce92ed306c249d46894776a909505d8f5a4ba55b14206e3222f \\\n--hash=sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6 \\\n--hash=sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8\n```\n\n----------------------------------------\n\nTITLE: Specifying Ray Project Dependencies\nDESCRIPTION: This snippet lists the dependencies required for the Ray project. It includes AWS tools, IPython, visualization libraries, and error handling utilities. Some packages have specific version requirements, while others are left unpinned.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/docker/ray-docker-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Todo: Fix conflicts with pinned boto3/botocore\n# awscli\ngsutil\n\n# Requirements that are shipped in the ML docker image.\nipython==8.12.3\n\n# Needed for rich visualization for Ray Train and Ray Data.\n# Todo: Pin to >=8 when myst-parser is upgraded\n# ipywidgets>=8\nipywidgets\n\n# Needed for Ray Client error message serialization/deserialization.\ntblib\n```\n\n----------------------------------------\n\nTITLE: Specifying pydantic Package Version and Hashes\nDESCRIPTION: This snippet defines the version and hash values for the pydantic package. It specifies version 2.9.2 and includes hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu121.txt#2025-04-12_snippet_41\n\nLANGUAGE: Text\nCODE:\n```\npydantic==2.9.2 \\\n    --hash=sha256:d155cef71265d1e9807ed1c32b4c8deec042a44a50a4188b25ac67ecd81a9c0f \\\n    --hash=sha256:f048cec7b26778210e28a0459867920654d48e5e62db0958433636cde4254f12\n```\n\n----------------------------------------\n\nTITLE: Specifying GPU Resources\nDESCRIPTION: Example showing how to request GPU resources for a job\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/running-applications/job-submission/sdk.rst#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\njob_id = client.submit_job(\n    entrypoint=\"python script.py\",\n    runtime_env={\n        \"working_dir\": \"./\",\n    }\n    # Reserve 1 GPU for the entrypoint script\n    entrypoint_num_gpus=1\n)\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Ray Dashboard\nDESCRIPTION: Sets up port forwarding to access the Ray Dashboard port on the local machine.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md#2025-04-12_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n! kubectl port-forward service/service-ray-cluster 8265:8265\n```\n\n----------------------------------------\n\nTITLE: Shutting down Ray\nDESCRIPTION: Shuts down the Ray cluster. This is important to do when the Ray application is finished to release resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/nevergrad_example.ipynb#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n\"ray.shutdown()\"\n```\n\n----------------------------------------\n\nTITLE: Parameter Configuration Functions\nDESCRIPTION: Documentation listing for Tune's parameter configuration functions including tune.with_parameters.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/api/trainable.rst#2025-04-12_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n    :toctree: doc/\n\n    tune.with_parameters\n```\n\n----------------------------------------\n\nTITLE: Defining soupsieve dependency with version pinning and hash verification\nDESCRIPTION: Specifies soupsieve version 2.5 with SHA-256 hashes for verification. Comments indicate it's required by compiled ray test requirements and beautifulsoup4.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cpu.txt#2025-04-12_snippet_40\n\nLANGUAGE: plaintext\nCODE:\n```\nsoupsieve==2.5 \\\n    --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \\\n    --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   beautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Spark Submit Command for Basic Example\nDESCRIPTION: Bash command to submit the Ray on Spark application to a Spark standalone cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/spark.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nspark-submit \\\n  --master spark://{spark_master_IP}:{spark_master_port} \\\n  path/to/ray-on-spark-example1.py\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Commands to install linter dependencies and test requirements for Ray development.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/development.rst#2025-04-12_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npip install -c python/requirements_compiled.txt -r python/requirements/lint-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Profiling Tools\nDESCRIPTION: Commands for installing required profiling tools including google-perftools and graphviz on Ubuntu systems.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/profiling.rst#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install google-perftools libgoogle-perftools-dev\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install graphviz\n```\n\n----------------------------------------\n\nTITLE: Including Slurm Template Script Reference\nDESCRIPTION: RestructuredText directive that includes and displays the contents of a Slurm template bash script from the cluster documentation code directory. The script is formatted as bash code.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/user-guides/community/slurm-template.rst#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. literalinclude:: /cluster/doc_code/slurm-template.sh\n    :language: bash\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependency with SHA256 Hash Values\nDESCRIPTION: A listing of Python package dependencies with their SHA256 hash values for package verification. The format includes package name, version, platform constraints, and multiple hash values along with comments about which requirements file includes these packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_37\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:51ffd53c53c4442415b613497a34ba0aa7b99ac07f1e4a62db5dcd640ae6c3c3 \\\n--hash=sha256:5294fcb410ed0a45d5d1cdedc4e51a60aab5b2b3193999028ea94afc2f554b05 \\\n--hash=sha256:56e3efe356416bc67a8e093607315951d76910f03d2b3ad49c4ade9207bf710d \\\n--hash=sha256:5d3cc75ef3e17490042c47e0523aee1bcc4eacd2482796107fd59dd1100a44bc \\\n--hash=sha256:5e6ee18a53dd5743e6155b8ff7e8e477c25b29b440f87f65be8165275c87fef0 \\\n--hash=sha256:67a04754d121ea5ca39ddedc3f77071651fb5b0bc6b973c71c515415b44ed9c5 \\\n--hash=sha256:7394c0b7d460569c9285fa089a429f58465db930012566c03046f9e3ab0ed181 \\\n--hash=sha256:789c43bf4a10cd067c24c321238e800b8b2716c863ddb2294d2fed886fa5a689 \\\n--hash=sha256:7ac67b542505186b3bbdaffbc303292e1ee9c8729e5d5df243c1f20f4bb9057e \\\n--hash=sha256:8561c48b0090993e3b2a54db480cab1d23eb2c5735067213bb90f402806339f5 \\\n--hash=sha256:86bfb52a9cfbcc09aba2b71388b0a20ea5c52b6517c0b2e316222435a8cdab72 \\\n--hash=sha256:8711682a629bbcaf492f5e0af72d378e976ea1d127a2d47584fa1c2c080b436b \\\n--hash=sha256:89da58e4005e153b03fe8b8794330e3f6a9774ee9e1c3bd5bc52eb098c3b0c4f \\\n--hash=sha256:89f72524033abbfde880ad338fd3c2c16e31ae232323ebdfbc745cbb1b3dcc03 \\\n--hash=sha256:8bf1ab71f9f23b0a1d52ec1682a3907e0c208c12fef9c3e99d2b80166b17905f \\\n--hash=sha256:8d7bbbe2cd6ed80aceef2a14e9f1c1b61683194c216472ed5ff33b700e784e37 \\\n--hash=sha256:94c4a9b01eede952442c088d415861b0cf2053cbd696b863f6d5022d4e4e2453 \\\n--hash=sha256:98dcf978d4c6048965d1762abd534c9d53bae981a035bfe486690ba11f49bbbb \\\n--hash=sha256:a4cc73a6ae0a6751b76e69cece9d0311f054da9b22df6a12f2c53111735657c8 \\\n--hash=sha256:a9f8e33747b1332db11cf7fcf4a9512bef9748cb5eb4d3f7fbc8c30d75dc6ffc \\\n--hash=sha256:ace960769d60037ca9625b4c578a6f28a14301bd2a1ff13bb00e824ac9f73e55 \\\n--hash=sha256:ae721bcc8e69846af00b7a77a220614d9b2ec57d25017a6bbde3a99473e41ce8 \\\n--hash=sha256:aea01f40995fa0945c020228ab919b8dfc93fc8a9f2d3d705ab5b793f32d9e99 \\\n--hash=sha256:b499caef4bca9cbd0bd23cd3386f5113ee7378094a3cb613a2fa543260fe9506 \\\n--hash=sha256:b89504227a5311610e4be16071465885a0a3d6b0e82e305ef46d9b064ce5fb72 \\\n--hash=sha256:bd66b4865c8b853b8cca7379afb692fc7f52cf898786537dfb5e5e2d64f0a47f \\\n--hash=sha256:bfcd3acc1a81f106abac6afd42327d2cf1e77ec905ae11dc1d9142a006a496b6 \\\n--hash=sha256:c24ba103ecf45861e2e1f933d40b2d93f5d52d8228870c3e7bf1299cd1cb8ff1 \\\n--hash=sha256:c348abc5924caa02a62896300e32ea80a81521f91d6db2e853e6b1994017c9f6 \\\n--hash=sha256:c53f97032b87a406044a1c33d1e9290cc38b117a8062e8a8b285175d7e2f99c9 \\\n--hash=sha256:c7cd4b1015d2f60dfe539ee6c95bc968d5d5fad92ab01bb5501a77393da4f596 \\\n--hash=sha256:c86dc2068f1c5ca2065aca34f257bbf4f78caf566eb230f692ad347da191f0a1 \\\n--hash=sha256:c8c5c8e1bac05ef3c23722e591ef4f688f528235e2480f157a9cfe0a19081375 \\\n--hash=sha256:ca36151289a15b39d8d683fd8b7abbe26fc50be311066c5f8dcf3cb8cee107ab \\\n--hash=sha256:cc8821a03bcfb36e4e4705316f6b66af28450357af8a575dc8f4b09bf02a3dee \\\n--hash=sha256:cccc18077acd34c8072578394ec79563664b1c205f7a86a62e94fafc7b59001f \\\n--hash=sha256:d2244d8ab24374bed366f9ff206e2619345f9cd7fe79aad5225f53faac28b6b1 \\\n--hash=sha256:d4c22992e24f12de340ca5f824121a5b3e1a37ad4360b4e1aaf15e9d1c42582d \\\n--hash=sha256:dd24c4d256558429aeeb8d6c24ebad4e982ac52c50bc3670ae8646c181263965 \\\n--hash=sha256:e413352a921f5ad5d66f9e2869b977e88d5103fc528b6deb8423028a2befd842 \\\n--hash=sha256:ee06405ea2e67366a661ed313e14cf2a86e84142a3462852eb96348f7219cee3 \\\n--hash=sha256:f83eca8cbfd168e424dfa3b3b5c955d6c281e8fc09feb9d870886ff8d03683c7 \\\n--hash=sha256:fb915101dfbf318486364ce85662bb7b020840f68138014972c08331458d41f3 \\\n--hash=sha256:ffc02b159b65c05f2ed9ec176b715b66918a674bd4daed48a9a7a590dd4be1aa \\\n--hash=sha256:ffc5ae23ada6515f31604f700009e2df90b091b67d463a8401c1d8a37f76c1d7\n# via\n#   -c python/requirements_compiled_rayllm_test_py311_cu124.txt\n#   uvicorn\n```\n\n----------------------------------------\n\nTITLE: Testing with Bazel\nDESCRIPTION: This command runs tests using Bazel, focusing on building and testing the entire project codebase. It requires Bazel to be installed and configured in the environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/getting-involved.rst#2025-04-12_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nbazel test --build_tests_only //:all\n```\n\n----------------------------------------\n\nTITLE: Defining psutil Dependency for Ray Project\nDESCRIPTION: This snippet specifies the psutil package dependency with version 5.9.6 and includes hash verifications. The psutil package is required by ipykernel and provides system utilization information needed by Ray for monitoring processes and resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_28\n\nLANGUAGE: pip\nCODE:\n```\npsutil==5.9.6 \\\n    --hash=sha256:10e8c17b4f898d64b121149afb136c53ea8b68c7531155147867b7b1ac9e7e28 \\\n    --hash=sha256:18cd22c5db486f33998f37e2bb054cc62fd06646995285e02a51b1e08da97017 \\\n    --hash=sha256:3ebf2158c16cc69db777e3c7decb3c0f43a7af94a60d72e87b2823aebac3d602 \\\n    --hash=sha256:51dc3d54607c73148f63732c727856f5febec1c7c336f8f41fcbd6315cce76ac \\\n    --hash=sha256:6e5fb8dc711a514da83098bc5234264e551ad980cec5f85dabf4d38ed6f15e9a \\\n    --hash=sha256:70cb3beb98bc3fd5ac9ac617a327af7e7f826373ee64c80efd4eb2856e5051e9 \\\n    --hash=sha256:748c9dd2583ed86347ed65d0035f45fa8c851e8d90354c122ab72319b5f366f4 \\\n    --hash=sha256:91ecd2d9c00db9817a4b4192107cf6954addb5d9d67a969a4f436dbc9200f88c \\\n    --hash=sha256:92e0cc43c524834af53e9d3369245e6cc3b130e78e26100d1f63cdb0abeb3d3c \\\n    --hash=sha256:a6f01f03bf1843280f4ad16f4bde26b817847b4c1a0db59bf6419807bc5ce05c \\\n    --hash=sha256:c69596f9fc2f8acd574a12d5f8b7b1ba3765a641ea5d60fb4736bf3c08a8214a \\\n    --hash=sha256:ca2780f5e038379e520281e4c032dddd086906ddff9ef0d1b9dcf00710e5071c \\\n    --hash=sha256:daecbcbd29b289aac14ece28eca6a3e60aa361754cf6da3dfb20d4d32b6c7f57 \\\n    --hash=sha256:e4b92ddcd7dd4cdd3f900180ea1e104932c7bce234fb88976e2a3b296441225a \\\n    --hash=sha256:fb8a697f11b0f5994550555fcfe3e69799e5b060c8ecf9e2f75c69302cc35c0d \\\n    --hash=sha256:ff18b8d1a784b810df0b0fff3bcb50ab941c3b8e2c8de5726f9c71c601c611aa\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   ipykernel\n```\n\n----------------------------------------\n\nTITLE: Getting Kubernetes Cluster Credentials\nDESCRIPTION: Retrieves credentials for the GKE cluster to use with kubectl, configuring kubectl to communicate with the cluster.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gke-gcs-bucket.md#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngcloud container clusters get-credentials cloud-bucket-cluster --zone us-west1-b --project my-project-id\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with ConcurrentHashMap Access\nDESCRIPTION: This stack trace shows Netty's event processing flow with access to a ConcurrentHashMap. The trace demonstrates how Vert.x handlers use concurrent collections to manage state or lookups during message processing in the event loop.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_45\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];java/util/concurrent/ConcurrentHashMap:.get_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Package Requirements with SHA256 Hashes\nDESCRIPTION: Package dependency specifications including version constraints and SHA256 hashes for verification. Includes mdurl 0.1.2, memray 1.10.0 (non-Windows only), and msgpack 1.0.7 with their respective hash values.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cpu.txt#2025-04-12_snippet_7\n\nLANGUAGE: requirements.txt\nCODE:\n```\nmdurl==0.1.2 \\\n    --hash=sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8 \\\n    --hash=sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cpu.txt\n    #   markdown-it-py\nmemray==1.10.0 ; sys_platform != 'win32' \\\n    --hash=sha256:0a21745fb516b7a6efcd40aa7487c59e9313fcfc782d0193fcfcf00b48426874 \\\n    --hash=sha256:22f2a47871c172a0539bd72737bb6b294fc10c510464066b825d90fcd3bb4916\nmsgpack==1.0.7 \\\n    --hash=sha256:04ad6069c86e531682f9e1e71b71c1c3937d6014a7c3e9edd2aa81ad58842862 \\\n    --hash=sha256:0bfdd914e55e0d2c9e1526de210f6fe8ffe9705f2b1dfcc4aecc92a4cb4b533d\n```\n\n----------------------------------------\n\nTITLE: Querying Ray Cluster Status via Command Line\nDESCRIPTION: This command-line example demonstrates how to query the status of a Ray cluster using the ray status command. It shows how to specify the address of the cluster to get its current status.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/project_files/requirements_project/requirements.txt#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nray status --address='localhost:6379'\n```\n\n----------------------------------------\n\nTITLE: Generating Benchmark Result Figure\nDESCRIPTION: Runs a Python script to generate a figure visualizing the benchmark results, which is saved as 'benchmark_result.png' in the scripts directory.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md#2025-04-12_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# (path: benchmark/memory_benchmark/scripts)\npython3 experiment_figures.py\n# The output image `benchmark_result.png` will be stored in `scripts/`.\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with SHA256 Hashes\nDESCRIPTION: Lists package dependencies with their exact versions and SHA256 hash values for verification. Each entry specifies the package name, version, and associated hash values to ensure package integrity.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_4\n\nLANGUAGE: txt\nCODE:\n```\n--hash=sha256:a760153f4e66edd6214df0a69e7eb90206c8ddd8083734ac430e852453a58e06 \\\n--hash=sha256:a764b697fd1cb01b92a18240f9afd291b1f33ede3c9cdc59dd92ba87a5f4f8f3 \\\n--hash=sha256:af18fcd2a37aa51c24cedbb82f4934f39a9a4ea11a84d34c1ab63df94a28fdd1 \\\n# via vllm\nbleach==6.1.0 \\\n--hash=sha256:0a31f1837963c41d46bbf1331b8778e1308ea0791db03cc4e7357b97cf42a8fe \\\n--hash=sha256:3225f354cfc436b9789c66c4ee030194bee0568fbf9cbdad3bc8b5c26c5f12b6\n```\n\n----------------------------------------\n\nTITLE: Configuring markdown-it-py package dependency with hash verification\nDESCRIPTION: Specification for the markdown-it-py package (version 2.2.0) with SHA256 hash values for verification. Comments indicate this dependency is required by Jupytext, mdit-py-plugins, and rich packages.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_15\n\nLANGUAGE: plain\nCODE:\n```\nmarkdown-it-py==2.2.0 \\\n    --hash=sha256:5a35f8d1870171d9acc47b99612dc146129b631baf04970128b568f190d0cc30 \\\n    --hash=sha256:7c9a5e412688bc771c67432cbfebcdd686c93ce6484913dccf06cb5a0bea35a1\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu124.txt\n    #   jupytext\n    #   mdit-py-plugins\n    #   rich\n```\n\n----------------------------------------\n\nTITLE: Installing Grafana Using Helm\nDESCRIPTION: Commands to install Grafana with custom datasource configuration using Helm.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/persist-kuberay-operator-logs.md#2025-04-12_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n\nhelm install grafana grafana/grafana --version 8.6.2 -f datasource-config.yaml\n```\n\n----------------------------------------\n\nTITLE: Specifying Partial-json-parser Package with Hash Verification in Bash\nDESCRIPTION: Defines the partial-json-parser package version 0.2.1.1.post5 with SHA256 hash verification for secure package installation. Required for the vllm package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\npartial-json-parser==0.2.1.1.post5 \\\n    --hash=sha256:627715aaa3cb3fb60a65b0d62223243acaa6c70846520a90326fef3a2f0b61ca \\\n    --hash=sha256:992710ac67e90b367921d52727698928040f7713ba7ecb33b96371ea7aec82ca\n```\n\n----------------------------------------\n\nTITLE: Autogenerated Comment Header in Markdown\nDESCRIPTION: A comment indicating that the file was automatically generated using the uv tool.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# This file was autogenerated by uv via the following command:\n```\n\n----------------------------------------\n\nTITLE: MsgPack Package Dependency with Hash Verification\nDESCRIPTION: This snippet defines the msgpack package with version 1.0.7 and extensive SHA256 hash verification codes. It indicates that distributed and locust packages require this dependency through a compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_13\n\nLANGUAGE: pip\nCODE:\n```\nmsgpack==1.0.7 \\\n    --hash=sha256:04ad6069c86e531682f9e1e71b71c1c3937d6014a7c3e9edd2aa81ad58842862 \\\n    --hash=sha256:0bfdd914e55e0d2c9e1526de210f6fe8ffe9705f2b1dfcc4aecc92a4cb4b533d \\\n    --hash=sha256:1dc93e8e4653bdb5910aed79f11e165c85732067614f180f70534f056da97db3 \\\n    --hash=sha256:1e2d69948e4132813b8d1131f29f9101bc2c915f26089a6d632001a5c1349672 \\\n    --hash=sha256:235a31ec7db685f5c82233bddf9858748b89b8119bf4538d514536c485c15fe0 \\\n    --hash=sha256:27dcd6f46a21c18fa5e5deed92a43d4554e3df8d8ca5a47bf0615d6a5f39dbc9 \\\n    --hash=sha256:28efb066cde83c479dfe5a48141a53bc7e5f13f785b92ddde336c716663039ee \\\n    --hash=sha256:3476fae43db72bd11f29a5147ae2f3cb22e2f1a91d575ef130d2bf49afd21c46 \\\n    --hash=sha256:36e17c4592231a7dbd2ed09027823ab295d2791b3b1efb2aee874b10548b7524 \\\n    --hash=sha256:384d779f0d6f1b110eae74cb0659d9aa6ff35aaf547b3955abf2ab4c901c4819 \\\n    --hash=sha256:38949d30b11ae5f95c3c91917ee7a6b239f5ec276f271f28638dec9156f82cfc \\\n    --hash=sha256:3967e4ad1aa9da62fd53e346ed17d7b2e922cba5ab93bdd46febcac39be636fc \\\n    --hash=sha256:3e7bf4442b310ff154b7bb9d81eb2c016b7d597e364f97d72b1acc3817a0fdc1 \\\n    --hash=sha256:3f0c8c6dfa6605ab8ff0611995ee30d4f9fcff89966cf562733b4008a3d60d82 \\\n    --hash=sha256:484ae3240666ad34cfa31eea7b8c6cd2f1fdaae21d73ce2974211df099a95d81 \\\n    --hash=sha256:4a7b4f35de6a304b5533c238bee86b670b75b03d31b7797929caa7a624b5dda6 \\\n    --hash=sha256:4cb14ce54d9b857be9591ac364cb08dc2d6a5c4318c1182cb1d02274029d590d \\\n    --hash=sha256:4e71bc4416de195d6e9b4ee93ad3f2f6b2ce11d042b4d7a7ee00bbe0358bd0c2 \\\n    --hash=sha256:52700dc63a4676669b341ba33520f4d6e43d3ca58d422e22ba66d1736b0a6e4c \\\n    --hash=sha256:572efc93db7a4d27e404501975ca6d2d9775705c2d922390d878fcf768d92c87 \\\n    --hash=sha256:576eb384292b139821c41995523654ad82d1916da6a60cff129c715a6223ea84 \\\n    --hash=sha256:5b0bf0effb196ed76b7ad883848143427a73c355ae8e569fa538365064188b8e \\\n    --hash=sha256:5b6ccc0c85916998d788b295765ea0e9cb9aac7e4a8ed71d12e7d8ac31c23c95 \\\n    --hash=sha256:5ed82f5a7af3697b1c4786053736f24a0efd0a1b8a130d4c7bfee4b9ded0f08f \\\n    --hash=sha256:6d4c80667de2e36970ebf74f42d1088cc9ee7ef5f4e8c35eee1b40eafd33ca5b \\\n    --hash=sha256:730076207cb816138cf1af7f7237b208340a2c5e749707457d70705715c93b93 \\\n    --hash=sha256:7687e22a31e976a0e7fc99c2f4d11ca45eff652a81eb8c8085e9609298916dcf \\\n    --hash=sha256:822ea70dc4018c7e6223f13affd1c5c30c0f5c12ac1f96cd8e9949acddb48a61 \\\n    --hash=sha256:84b0daf226913133f899ea9b30618722d45feffa67e4fe867b0b5ae83a34060c \\\n    --hash=sha256:85765fdf4b27eb5086f05ac0491090fc76f4f2b28e09d9350c31aac25a5aaff8 \\\n    --hash=sha256:8dd178c4c80706546702c59529ffc005681bd6dc2ea234c450661b205445a34d \\\n    --hash=sha256:8f5b234f567cf76ee489502ceb7165c2a5cecec081db2b37e35332b537f8157c \\\n    --hash=sha256:98bbd754a422a0b123c66a4c341de0474cad4a5c10c164ceed6ea090f3563db4 \\\n    --hash=sha256:993584fc821c58d5993521bfdcd31a4adf025c7d745bbd4d12ccfecf695af5ba \\\n    --hash=sha256:a40821a89dc373d6427e2b44b572efc36a2778d3f543299e2f24eb1a5de65415 \\\n    --hash=sha256:b291f0ee7961a597cbbcc77709374087fa2a9afe7bdb6a40dbbd9b127e79afee \\\n    --hash=sha256:b573a43ef7c368ba4ea06050a957c2a7550f729c31f11dd616d2ac4aba99888d \\\n    --hash=sha256:b610ff0f24e9f11c9ae653c67ff8cc03c075131401b3e5ef4b82570d1728f8a9 \\\n    --hash=sha256:bdf38ba2d393c7911ae989c3bbba510ebbcdf4ecbdbfec36272abe350c454075 \\\n    --hash=sha256:bfef2bb6ef068827bbd021017a107194956918ab43ce4d6dc945ffa13efbc25f \\\n    --hash=sha256:cab3db8bab4b7e635c1c97270d7a4b2a90c070b33cbc00c99ef3f9be03d3e1f7 \\\n    --hash=sha256:cb70766519500281815dfd7a87d3a178acf7ce95390544b8c90587d76b227681 \\\n    --hash=sha256:cca1b62fe70d761a282496b96a5e51c44c213e410a964bdffe0928e611368329 \\\n    --hash=sha256:ccf9a39706b604d884d2cb1e27fe973bc55f2890c52f38df742bc1d79ab9f5e1 \\\n    --hash=sha256:dc43f1ec66eb8440567186ae2f8c447d91e0372d793dfe8c222aec857b81a8cf \\\n    --hash=sha256:dd632777ff3beaaf629f1ab4396caf7ba0bdd075d948a69460d13d44357aca4c \\\n    --hash=sha256:e45ae4927759289c30ccba8d9fdce62bb414977ba158286b5ddaf8df2cddb5c5 \\\n    --hash=sha256:e50ebce52f41370707f1e21a59514e3375e3edd6e1832f5e5235237db933c98b \\\n    --hash=sha256:ebbbba226f0a108a7366bf4b59bf0f30a12fd5e75100c630267d94d7f0ad20e5 \\\n    --hash=sha256:ec79ff6159dffcc30853b2ad612ed572af86c92b5168aa3fc01a67b0fa40665e \\\n    --hash=sha256:f0936e08e0003f66bfd97e74ee530427707297b0d0361247e9b4f59ab78ddc8b \\\n    --hash=sha256:f26a07a6e877c76a88e3cecac8531908d980d3d5067ff69213653649ec0f60ad \\\n    --hash=sha256:f64e376cd20d3f030190e8c32e1c64582eba56ac6dc7d5b0b49a9d44021b52fd \\\n    --hash=sha256:f6ffbc252eb0d229aeb2f9ad051200668fc3a9aaa8994e49f0cb2ffe2b7867e7 \\\n    --hash=sha256:f9a7c509542db4eceed3dcf21ee5267ab565a83555c9b88a8109dcecc4709002 \\\n    --hash=sha256:ff1d0899f104f3921d94579a5638847f783c9b04f2d5f229392ca77fba5b82fc\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   distributed\n    #   locust\n```\n\n----------------------------------------\n\nTITLE: Including BOHB Example for Ray Tune with restructuredtext\nDESCRIPTION: A directive that includes the BOHB example Python file from the Ray Tune examples directory. This restructuredtext command references the implementation example for the BOHB optimization algorithm in Ray's hyperparameter tuning library.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/includes/bohb_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. literalinclude:: /../../python/ray/tune/examples/bohb_example.py\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version Pinning and Hash Verification\nDESCRIPTION: This requirements file lists Python package dependencies with exact versions and SHA256 hashes for secure installation. Each package includes comments indicating which parent packages require it as a dependency. The file follows pip's requirements format with hash verification for enhanced security.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_26\n\nLANGUAGE: pip\nCODE:\n```\n--hash=sha256:e53d19c2bf7d0d1e6998a7e693c7e87300dd971808e6618964621ccd0e01fe4e \\\n    --hash=sha256:e560fd75aaf3e5693b91bcaddd8b314f4d57e99aef8a6c6dc692f935cc1e6bbf \\\n    --hash=sha256:ec5060592d83454e8063e487696ac3783cc48c9a329498bafae0d972bc7816c9 \\\n    --hash=sha256:ecc2920630283e0783c22e2ac94427f8cca29a04cfdf331467d4f661f4072dac \\\n    --hash=sha256:ed7161bccab7696a473fe7ddb619c1d75963732b37da4618ba12e60899fefe4f \\\n    --hash=sha256:ee0bd3a7b2e184e88d25c9baa6a9dc609ba25b76daae942edfb14499ac7ec374 \\\n    --hash=sha256:ee25f1ac091def37c4b59d192bbe3a206298feeb89132a470325bf76ad122a1e \\\n    --hash=sha256:efa44f64c37cc30c9f05932c740a8b40ce359f51882c70883cc95feac842da4d \\\n    --hash=sha256:f47d52fd9b2ac418c4890aad2f6d21a6b96183c98021f0a48497a904199f006e \\\n    --hash=sha256:f857034dc68d5ceb30fb60afb6ff2103087aea10a01b613985610e007053a121 \\\n    --hash=sha256:fb91d20fa2d3b13deea98a690534697742029f4fb83673a3501ae6e3746508b5 \\\n    --hash=sha256:fddb8870bdb83456a489ab67c6b3040a8d5a55069aa6f72f9d872235fbc52f54\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   aiohttp\n    #   yarl\n```\n\n----------------------------------------\n\nTITLE: Configuring TOC for Ray Clusters VM Documentation\nDESCRIPTION: Sphinx/RST table of contents directive that specifies the structure and navigation for Ray Clusters VM documentation, including links to CLI reference and cluster configuration pages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/vms/references/index.md#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{toctree}\n:caption: \"Reference documentation for Ray Clusters on VMs:\"\n:maxdepth: '2'\n:name: ray-clusters-vms-reference\n\nray-cluster-cli\nray-cluster-configuration\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies with Security Hashes\nDESCRIPTION: Pinned versions of Python packages with SHA-256 hashes for security verification. Includes cffi 1.16.0, chardet 5.2.0, and charset-normalizer 3.3.2 with their dependency relationships noted in comments.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_3\n\nLANGUAGE: txt\nCODE:\n```\ncffi==1.16.0 \\\n    --hash=sha256:0c9ef6ff37e974b73c25eecc13952c55bceed9112be2d9d938ded8e856138bcc \\\n    --hash=sha256:131fd094d1065b19540c3d72594260f118b231090295d8c34e19a7bbcf2e860a \\\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   cryptography\nchardet==5.2.0 \\\n    --hash=sha256:1b3b6ff479a8c414bc3fa2c0852995695c4a026dcd6d0633b2dd092ca39c1cf7 \\\n    --hash=sha256:e1cf59446890a00105fe7b7912492ea04b6e6f06d4b742b2c788469e34c82970\n    # via mbstrdecoder\ncharset-normalizer==3.3.2 \\\n    --hash=sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027 \\\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Ray Dashboard\nDESCRIPTION: Commands to set up port forwarding for accessing the Ray dashboard through a web browser.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/observability.md#2025-04-12_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl port-forward $RAY_POD -n $YOUR_NAMESPACE 8265:8265\n```\n\n----------------------------------------\n\nTITLE: Java Netty NIO Event Processing with JavaScript Property Access\nDESCRIPTION: Stack trace showing the execution path through to JavaScript object property access. This trace illustrates how the JavaScript engine accesses object properties during HTTP request handling.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_54\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.getObjectProp_[j] 2\n```\n\n----------------------------------------\n\nTITLE: Package Hash Verification Block\nDESCRIPTION: SHA-256 hash verification block for package dependencies used in the Ray project. Contains multiple hash definitions for security verification of package downloads.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:1f71c10d1e88467126f0efd484bd44bca5e14c664ec2ede64c32f20875c0d413 \\\n--hash=sha256:2424ff4c4ac7f6b8177b53c17ed5d8fa74ae5955656867f5a8affaca36a27abb \\\n--hash=sha256:2bce03af1ce5a5567ab89bd90d11e7bbdff56b8af3acbbec1faded8f44cb06da\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Samples for the Evolution in Python\nDESCRIPTION: This snippet sets the total number of hyperparameter combinations (samples) to be tried out during the tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb#2025-04-12_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 1000\n\n# We override here for our smoke tests.\nnum_samples = 10\n```\n\n----------------------------------------\n\nTITLE: Defining Python Test Dependencies\nDESCRIPTION: Lists the required Python testing packages including pytest and its async testing extensions pytest-asyncio and pytest-aiohttp.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements/base-test-requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npytest\npytest-asyncio\npytest-aiohttp\n```\n\n----------------------------------------\n\nTITLE: Numpy Package Requirements\nDESCRIPTION: Version specification and hash verification for numpy package v1.26.4, required by multiple dependencies including ale-py, bokeh, dask, and others.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nnumpy==1.26.4 \\\n--hash=sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b \\\n--hash=sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818\n```\n\n----------------------------------------\n\nTITLE: Package Dependency Hashes\nDESCRIPTION: List of package dependencies with their corresponding SHA256 hashes used for package verification. Includes packages like pygments, pyopenssl, pyparsing, pyspark, pytest, python-dateutil, pytz, pyu2f, and pyyaml with their specific versions and hash values.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_29\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:3d5639516376dce1940ea36edf408c554475369f5da2abd45d44621cb616f769 \\\n--hash=sha256:42c6dcb030aefb668a2b7009c85b27f90e51e6a3b4d5c9bc4c57631292015b0d \\\n--hash=sha256:4a7cd62e831afe623fbb7aabbb4fe583212115b3ef38a9f6b71869ba644624a2 \\\n# Additional hashes omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Configuring Checkpointing for GBDT Training\nDESCRIPTION: Sets up a checkpoint configuration for XGBoost and LightGBM training with Ray. This configuration saves a checkpoint every 10 iterations and retains only the latest checkpoint, which is useful for resuming training after interruptions.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Configure checkpointing to save progress during training\nrun_config = RunConfig(\n    checkpoint_config=CheckpointConfig(\n        # Checkpoint every 10 iterations.\n        checkpoint_frequency=10,\n        # Only keep the latest checkpoint and delete the others.\n        num_to_keep=1,\n    )\n    ## If running in a multi-node cluster, this is where you\n    ## should configure the run's persistent storage that is accessible\n    ## across all worker nodes with `storage_path=\"s3://...\"`\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Pickleshare Package with Hash Verification in Bash\nDESCRIPTION: Defines the pickleshare package version 0.7.5 with SHA256 hash verification for secure package installation. Required by ipython.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\npickleshare==0.7.5 \\\n    --hash=sha256:87683d47965c1da65cdacaf31c8441d12b8044cdec9aca500cd78fc2c683afca \\\n    --hash=sha256:9649af414d74d4df115d5d718f82acb59c9d418196b7b4290ed47a12ce62df56\n```\n\n----------------------------------------\n\nTITLE: SHA256 Hash Dependencies\nDESCRIPTION: List of SHA256 hashes for package dependencies verification. Used to ensure package integrity and security during installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n--hash=sha256:6939c95381e003f54cd4c5516740faba40cf5ad3eeff460c3ad1d3e0ea2549bf \\\n--hash=sha256:69db76c09796b313331bb7048229e3bee7928eb62bab5e071e9f7fcc4879caee \\\n--hash=sha256:6bf7a982604375a8d49b6cc1b781c1747f243d91b81035a9b43a2126c04766f5\n```\n\n----------------------------------------\n\nTITLE: JavaScript Object Property Check in Netty/Vert.x Request Processing\nDESCRIPTION: This stack trace shows the process of checking for the existence of a property on a JavaScript object during the execution of server-side JavaScript code in response to an HTTP request in a Netty/Vert.x environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_77\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.newObject_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/optimizer/OptRuntime:.call2_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/IdScriptableObject:.has_[j]\n```\n\n----------------------------------------\n\nTITLE: Netty HTTP Stack Trace with Native Java Method Invocation\nDESCRIPTION: Thread stack trace showing JavaScript to Java interop in Vert.x. This trace demonstrates how JavaScript code invokes Java methods through Mozilla Rhino's MemberBox when processing HTTP requests.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_89\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Channel Write Path with Direct Hardware Transmission\nDESCRIPTION: Stack trace showing complete path of a Netty NIO write operation from Java through JNI and kernel networking stack to hardware transmission\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_8\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_hard_start_xmit_[k]\n```\n\n----------------------------------------\n\nTITLE: Specifying Protobuf Package Version and Hashes\nDESCRIPTION: Defines the required version of the protobuf package (3.20.3) along with its hash values for verification. This is used to ensure consistent and secure package installation.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu124.txt#2025-04-12_snippet_15\n\nLANGUAGE: Text\nCODE:\n```\nprotobuf==3.20.3 \\\n    --hash=sha256:03038ac1cfbc41aa21f6afcbcd357281d7521b4157926f30ebecc8d4ea59dcb7 \\\n    --hash=sha256:28545383d61f55b57cf4df63eebd9827754fd2dc25f80c5253f9184235db242c \\\n    --hash=sha256:2e3427429c9cffebf259491be0af70189607f365c2f41c7c3764af6f337105f2 \\\n    --hash=sha256:398a9e0c3eaceb34ec1aee71894ca3299605fa8e761544934378bbc6c97de23b \\\n    --hash=sha256:44246bab5dd4b7fbd3c0c80b6f16686808fab0e4aca819ade6e8d294a29c7050 \\\n    --hash=sha256:447d43819997825d4e71bf5769d869b968ce96848b6479397e29fc24c4a5dfe9 \\\n    --hash=sha256:67a3598f0a2dcbc58d02dd1928544e7d88f764b47d4a286202913f0b2801c2e7 \\\n    --hash=sha256:74480f79a023f90dc6e18febbf7b8bac7508420f2006fabd512013c0c238f454 \\\n    --hash=sha256:819559cafa1a373b7096a482b504ae8a857c89593cf3a25af743ac9ecbd23480 \\\n    --hash=sha256:899dc660cd599d7352d6f10d83c95df430a38b410c1b66b407a6b29265d66469 \\\n    --hash=sha256:8c0c984a1b8fef4086329ff8dd19ac77576b384079247c770f29cc8ce3afa06c \\\n    --hash=sha256:9aae4406ea63d825636cc11ffb34ad3379335803216ee3a856787bcf5ccc751e \\\n    --hash=sha256:a7ca6d488aa8ff7f329d4c545b2dbad8ac31464f1d8b1c87ad1346717731e4db \\\n    --hash=sha256:b6cc7ba72a8850621bfec987cb72623e703b7fe2b9127a161ce61e61558ad905 \\\n    --hash=sha256:bf01b5720be110540be4286e791db73f84a2b721072a3711efff6c324cdf074b \\\n    --hash=sha256:c02ce36ec760252242a33967d51c289fd0e1c0e6e5cc9397e2279177716add86 \\\n    --hash=sha256:d9e4432ff660d67d775c66ac42a67cf2453c27cb4d738fc22cb53b5d84c135d4 \\\n    --hash=sha256:daa564862dd0d39c00f8086f88700fdbe8bc717e993a21e90711acfed02f2402 \\\n    --hash=sha256:de78575669dddf6099a8a0f46a27e82a1783c557ccc38ee620ed8cc96d3be7d7 \\\n    --hash=sha256:e64857f395505ebf3d2569935506ae0dfc4a15cb80dc25261176c784662cdcc4 \\\n    --hash=sha256:f4bd856d702e5b0d96a00ec6b307b0f51c1982c2bf9c0052cf9019e9a544ba99 \\\n    --hash=sha256:f4c42102bc82a51108e449cbb32b19b180022941c727bac0cfd50170341f16ee\n```\n\n----------------------------------------\n\nTITLE: Configuring Toctree for KubeRay Benchmarks in Markdown\nDESCRIPTION: A Markdown toctree configuration that hides the actual benchmark documentation files from the main navigation while making them accessible through references. This snippet organizes the documentation structure for KubeRay benchmarks.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/benchmarks.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n\nbenchmarks/memory-scalability-benchmark\n```\n```\n\n----------------------------------------\n\nTITLE: Defining py-spy Package Requirement with Python Version Constraint\nDESCRIPTION: This snippet specifies the py-spy package at version 0.4.0 with a Python version constraint that prevents installation on Python 3.12 or newer. It includes multiple SHA-256 hash values for verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_py311_cu121.txt#2025-04-12_snippet_21\n\nLANGUAGE: text\nCODE:\n```\npy-spy==0.4.0 ; python_full_version < '3.12' \\\n    --hash=sha256:47cdda4c34d9b6cb01f3aaeceb2e88faf57da880207fe72ff6ff97e9bb6cc8a9 \\\n    --hash=sha256:77d8f637ade38367d944874776f45b703b7ac5938b1f7be8891f3a5876ddbb96 \\\n    --hash=sha256:806602ce7972782cc9c1e383f339bfc27bfb822d42485e6a3e0530ae5040e1f0 \\\n    --hash=sha256:87573e64dbfdfc89ba2e0f5e2f525aa84e0299c7eb6454b47ea335fde583a7a0 \\\n    --hash=sha256:8bf2f3702cef367a489faa45177b41a6c31b2a3e5bd78c978d44e29340152f5a \\\n    --hash=sha256:c5f06ffce4c9c98b7fc9f5e67e5e7db591173f1351837633f3f23d9378b1d18a \\\n    --hash=sha256:eee3d0bde85ca5cf4f01f012d461180ca76c24835a96f7b5c4ded64eb6a008ab \\\n    --hash=sha256:f2cf3f7130e7d780471faa5957441d3b4e0ec39a79b2c00f4c33d494f7728428\n    # via\n    #   -c python/requirements_compiled_ray_test_py311_cu121.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: HTTP Headers Manipulation Stack Trace in Netty\nDESCRIPTION: Thread stack trace showing HTTP header manipulation in Netty. This trace illustrates how DefaultHttpHeaders.set() is called during HTTP request processing in a Vert.x application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_90\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vhello_js_1:.call_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/NativeJavaMethod:.call_[j];org/mozilla/javascript/MemberBox:.invoke_[j];io/netty/handler/codec/http/DefaultHttpHeaders:.set_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Installing Kueue Manifests\nDESCRIPTION: Command to install the latest version of Kueue using kubectl.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.md#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/v0.8.2/manifests.yaml\n```\n\n----------------------------------------\n\nTITLE: Executing XGBoost Sweep Test in Python\nDESCRIPTION: This code snippet shows the main execution and error handling for an XGBoost sweep test. It captures the runtime and compares it against a predefined limit, raising an AssertionError if exceeded.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.3.0/tune_tests/scalability_tests/test_xgboost_sweep.txt#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nFile \"workloads/test_xgboost_sweep.py\", line 99, in <module>\n    main()\nFile \"workloads/test_xgboost_sweep.py\", line 86, in main\n    f\"The {name} test took {time_taken:.2f} seconds, but should not \" \\\nAssertionError: The large xgboost sweep test took 3187.07 seconds, but should not have exceeded 2600.00 seconds. Test failed.\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with SHA256 Hashes\nDESCRIPTION: Package dependency specifications with pinned versions and SHA256 hash verification. Each package includes its source requirements file reference and dependency chain.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu121.txt#2025-04-12_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:f1d18718f9d78182c6b60f568c9a9cec8a7204d7cb6fad4e511a2ef279e4cb05 \\\n--hash=sha256:f4c7bf687303ca47d69f9f0133274958fd672efaa33fb5bcde467862d6c621f0 \\\n--hash=sha256:f76176492ff082657ada0d0f10c794b6da5800249ef1692b35cf49b1e93e8ef7\n# via\n#   -c /tmp/ray-deps/requirements_compiled.txt\n#   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Grid Search in Ray Tune\nDESCRIPTION: Demonstrates how to use tune.grid_search to perform grid search over a boolean parameter.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-search-spaces.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntuner = tune.Tuner(\n    trainable,\n    param_space={\"bar\": tune.grid_search([True, False])})\nresults = tuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Memory Cleanup\nDESCRIPTION: Cleans up GPU memory by deleting the pipeline and emptying CUDA cache.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/playground.ipynb#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndel pipeline \ntorch.cuda.empty_cache()\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with Complete Channel Processing Flow\nDESCRIPTION: This stack trace shows the complete flow of Netty's NIO event loop from thread initialization through channel processing, including read operations, channelReadComplete events, and flush operations. The trace ends with recycling operations in Netty's utility classes.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_39\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/util/Recycler:.recycle_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Displaying Forecast Plot for First Partition\nDESCRIPTION: Locates and displays the forecast plot generated for the first dataset partition ('H1') during the training process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/02_many_model_training/start.ipynb#2025-04-12_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\nimport os\n\nfor result in result_grid:\n    if result.config[\"data_partition_id\"] == \"H1\":\n        display(Image(os.path.join(result.path, \"prediction.png\")))\n        break\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: List of Python package dependencies with exact versions and SHA256 hashes for reproducible and secure installations. Includes packages like boto3, cffi, beautifulsoup4 and their dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/requirements_buildkite.txt#2025-04-12_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nbackports-tarfile==1.1.1 \\\n    --hash=sha256:73e0179647803d3726d82e76089d01d8549ceca9bace469953fcb4d97cf2d417 \\\n    --hash=sha256:9c2ef9696cb73374f7164e17fc761389393ca76777036f5aad42e8b93fcd8009\nbazel-runfiles==0.31.0 \\\n    --hash=sha256:8f460c52879ebd9b6dbc8fea89637b90d271746eabf4f8382334879d80366663\nbeautifulsoup4==4.12.3 \\\n    --hash=sha256:74e3d1928edc070d21748185c46e3fb33490f22f52a3addee9aee0f4f7781051 \\\n    --hash=sha256:b80878c9f40111313e55da8ba20bdba06d8fa3969fc68304167741bbf9e082ed\n```\n\n----------------------------------------\n\nTITLE: Specifying Sniffio Package with Hash Verification in pip\nDESCRIPTION: This code snippet defines the sniffio package dependency with version 1.3.1 and SHA-256 hashes for verification. Comments indicate this package is required by anyio and is constrained by requirements_compiled.txt.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_41\n\nLANGUAGE: pip\nCODE:\n```\nsniffio==1.3.1 \\\n    --hash=sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2 \\\n    --hash=sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   anyio\n```\n\n----------------------------------------\n\nTITLE: Java Thread Execution Stack Trace with Netty, Vert.x, and Mozilla JavaScript\nDESCRIPTION: This stack trace shows the execution path of a Java thread handling an HTTP request in a Vert.x application. It includes Netty's NIO processing, Vert.x HTTP server handling, and JavaScript execution via Mozilla's engine. The trace ends with object property manipulation in the JavaScript runtime.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_80\n\nLANGUAGE: Java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.setObjectProp_[j];org/mozilla/javascript/IdScriptableObject:.put_[j];org/mozilla/javascript/ScriptableObject:.getSlot_[j];org/mozilla/javascript/ScriptableObject:.createSlot_[j]\n```\n\n----------------------------------------\n\nTITLE: Listing PyOpenSSL Package with Hash Values\nDESCRIPTION: Definition for the PyOpenSSL package dependency with version 24.2.1 and corresponding SHA256 hash values. The comment indicates this is required by GCS OAuth2 Boto Plugin and gsutil packages through the compiled requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_23\n\nLANGUAGE: text\nCODE:\n```\npyopenssl==24.2.1 \\\n    --hash=sha256:4247f0dbe3748d560dcbb2ff3ea01af0f9a1a001ef5f7c4c647956ed8cbf0e95 \\\n    --hash=sha256:967d5719b12b243588573f39b0c677637145c7a1ffedcd495a487e58177fbb8d\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   gcs-oauth2-boto-plugin\n    #   gsutil\n```\n\n----------------------------------------\n\nTITLE: Testing Generated Code for Sending System Signals in Python\nDESCRIPTION: Tests a generated code snippet that sends a SIGUSR1 signal to the current process. The code imports the necessary os and signal modules and uses os.kill() with the current process ID to send the signal.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os, signal\n\nos.kill(os.getpid(), signal.SIGUSR1)  # Terminate the current process~\n```\n\n----------------------------------------\n\nTITLE: Installing pyarrow with Pinned Version and Hashes\nDESCRIPTION: Specifies pyarrow package with version 14.0.2 and multiple SHA256 hashes for verification. Comments indicate this is required by the petastorm package and also directly specified in requirements_byod_3.9.in.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_byod_3.9.txt#2025-04-12_snippet_23\n\nLANGUAGE: pip\nCODE:\n```\npyarrow==14.0.2 \\\n    --hash=sha256:059bd8f12a70519e46cd64e1ba40e97eae55e0cbe1695edd95384653d7626b23 \\\n    --hash=sha256:06ff1264fe4448e8d02073f5ce45a9f934c0f3db0a04460d0b01ff28befc3696 \\\n    --hash=sha256:1e6987c5274fb87d66bb36816afb6f65707546b3c45c44c28e3c4133c010a881 \\\n    --hash=sha256:209bac546942b0d8edc8debda248364f7f668e4aad4741bae58e67d40e5fcf75 \\\n    --hash=sha256:20e003a23a13da963f43e2b432483fdd8c38dc8882cd145f09f21792e1cf22a1 \\\n    --hash=sha256:22a768987a16bb46220cef490c56c671993fbee8fd0475febac0b3e16b00a10e \\\n    --hash=sha256:2cc61593c8e66194c7cdfae594503e91b926a228fba40b5cf25cc593563bcd07 \\\n    --hash=sha256:2dbba05e98f247f17e64303eb876f4a80fcd32f73c7e9ad975a83834d81f3fda \\\n    --hash=sha256:32356bfb58b36059773f49e4e214996888eeea3a08893e7dbde44753799b2a02 \\\n    --hash=sha256:36cef6ba12b499d864d1def3e990f97949e0b79400d08b7cf74504ffbd3eb025 \\\n    --hash=sha256:37c233ddbce0c67a76c0985612fef27c0c92aef9413cf5aa56952f359fcb7379 \\\n    --hash=sha256:3c0fa3bfdb0305ffe09810f9d3e2e50a2787e3a07063001dcd7adae0cee3601a \\\n    --hash=sha256:3f16111f9ab27e60b391c5f6d197510e3ad6654e73857b4e394861fc79c37200 \\\n    --hash=sha256:52809ee69d4dbf2241c0e4366d949ba035cbcf48409bf404f071f624ed313a2b \\\n    --hash=sha256:5c1da70d668af5620b8ba0a23f229030a4cd6c5f24a616a146f30d2386fec422 \\\n    --hash=sha256:63ac901baec9369d6aae1cbe6cca11178fb018a8d45068aaf5bb54f94804a866 \\\n    --hash=sha256:64df2bf1ef2ef14cee531e2dfe03dd924017650ffaa6f9513d7a1bb291e59c15 \\\n    --hash=sha256:66e986dc859712acb0bd45601229021f3ffcdfc49044b64c6d071aaf4fa49e98 \\\n    --hash=sha256:6dd4f4b472ccf4042f1eab77e6c8bce574543f54d2135c7e396f413046397d5a \\\n    --hash=sha256:75ee0efe7a87a687ae303d63037d08a48ef9ea0127064df18267252cfe2e9541 \\\n    --hash=sha256:76fc257559404ea5f1306ea9a3ff0541bf996ff3f7b9209fc517b5e83811fa8e \\\n    --hash=sha256:78ea56f62fb7c0ae8ecb9afdd7893e3a7dbeb0b04106f5c08dbb23f9c0157591 \\\n    --hash=sha256:87482af32e5a0c0cce2d12eb3c039dd1d853bd905b04f3f953f147c7a196915b \\\n    --hash=sha256:87e879323f256cb04267bb365add7208f302df942eb943c93a9dfeb8f44840b1 \\\n    --hash=sha256:a01d0052d2a294a5f56cc1862933014e696aa08cc7b620e8c0cce5a5d362e976 \\\n    --hash=sha256:a25eb2421a58e861f6ca91f43339d215476f4fe159eca603c55950c14f378cc5 \\\n    --hash=sha256:a51fee3a7db4d37f8cda3ea96f32530620d43b0489d169b285d774da48ca9785 \\\n    --hash=sha256:a898d134d00b1eca04998e9d286e19653f9d0fcb99587310cd10270907452a6b \\\n    --hash=sha256:b0c4a18e00f3a32398a7f31da47fefcd7a927545b396e1f15d0c85c2f2c778cd \\\n    --hash=sha256:ba9fe808596c5dbd08b3aeffe901e5f81095baaa28e7d5118e01354c64f22807 \\\n    --hash=sha256:c65bf4fd06584f058420238bc47a316e80dda01ec0dfb3044594128a6c2db794 \\\n    --hash=sha256:c87824a5ac52be210d32906c715f4ed7053d0180c1060ae3ff9b7e560f53f944 \\\n    --hash=sha256:e354fba8490de258be7687f341bc04aba181fc8aa1f71e4584f9890d9cb2dec2 \\\n    --hash=sha256:e4b123ad0f6add92de898214d404e488167b87b5dd86e9a434126bc2b7a5578d \\\n    --hash=sha256:f7d029f20ef56673a9730766023459ece397a05001f4e4d13805111d7c2108c0 \\\n    --hash=sha256:fc0de7575e841f1595ac07e5bc631084fd06ca8b03c0f2ecece733d23cd5102a\n    # via\n    #   -c release/ray_release/byod/requirements_compiled.txt\n    #   -r release/ray_release/byod/requirements_byod_3.9.in\n    #   petastorm\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Ray in Python\nDESCRIPTION: This snippet demonstrates how to shut down Ray. The cell is marked to be removed in the rendered output.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/_templates/template.ipynb#2025-04-12_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nray.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Performance Test Stage Output\nDESCRIPTION: Raw performance test output showing execution times across 4 stages. Includes metrics like total time, average/max/min iteration times, and actor creation time.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.5/stress_tests/test_many_tasks.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nStage 0 results:\n\tTotal time: 21.52851891517639\nStage 1 results:\n\tTotal time: 151.462096452713\n\tAverage iteration time: 15.146199059486388\n\tMax iteration time: 16.154610872268677\n\tMin iteration time: 14.686655044555664\nStage 2 results:\n\tTotal time: 1099.6661903858185\n\tAverage iteration time: 219.9327588558197\n\tMax iteration time: 236.58895802497864\n\tMin iteration time: 201.1623387336731\nStage 3 results:\n\tActor creation time: 0.09940552711486816\n\tTotal time: 3180.518438100815\n```\n\n----------------------------------------\n\nTITLE: MultiRLModule Forward Method Implementation\nDESCRIPTION: This snippet shows the forward method for the MultiRLModule class, which processes observations through a shared encoder and then passes the embeddings to the appropriate policy networks based on agent ID and forward mode.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef forward(\n    self,\n    observations,\n    *,\n    module_id=\"p1\",\n    mode=RLModule.Forward.INFERENCE,\n    timestep=None,\n    explore=None,\n    feature_callbacks=None,\n):\n    # We are sharing a common encoder between the two policies, so we need\n    # to extract the observation for our encoder from the observations dict\n    # (this dict has one entry for each agent).\n\n    # Get the encoder's output: Embedding that is shared by the two policies.\n    if module_id == \"p1\":\n        embedding = self.encoder_module.encode(observations[\"p1\"], timestep=timestep)\n        # Then use the embedding to compute actions via the policy module.\n        return self.policy_p1.forward_given_embeddings(\n            embeddings=embedding,\n            mode=mode,\n            timestep=timestep,\n            explore=explore,\n            feature_callbacks=feature_callbacks,\n        )\n    else:  # module_id == \"p2\"\n        embedding = self.encoder_module.encode(observations[\"p2\"], timestep=timestep)\n        # Then use the embedding to compute actions via the policy module.\n        return self.policy_p2.forward_given_embeddings(\n            embeddings=embedding,\n            mode=mode,\n            timestep=timestep,\n            explore=explore,\n            feature_callbacks=feature_callbacks,\n        )\n```\n\n----------------------------------------\n\nTITLE: Specifying Ray Project Dependencies for Machine Learning\nDESCRIPTION: This snippet defines the dependencies for the Ray project's machine learning components. It constrains versions based on a specific requirements file and includes torch and tensorflow as direct dependencies.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ml_user_tests/train/driver_requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\n-c ../../../python/requirements/ml/dl-cpu-requirements.txt\n\ntorch\ntensorflow\n```\n\n----------------------------------------\n\nTITLE: Ray Actor Initialization Performance Log\nDESCRIPTION: Console output showing Ray performance test results with metrics on actor creation speed. Successfully started 10,000 actors with a rate of approximately 323 actors per second.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/1.6.0/benchmarks/many_actors.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nSuccess! Started 10000 actors in 30.97493314743042s. (322.84169758828256 actors/s)\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Stack Trace with HTTP Server Message Processing\nDESCRIPTION: This stack trace shows Netty's event processing flow with HTTP server message handling. The trace illustrates how HTTP requests are processed in Vert.x's DefaultHttpServer implementation after being decoded by Netty's pipeline.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_47\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j] 3\n```\n\n----------------------------------------\n\nTITLE: Splitting Shakespeare Dataset into Sentences\nDESCRIPTION: Transforms the dataset by splitting the original paragraphs into multiple sentences. This is done using the Ray Data map_batches function with the split_text function defined earlier.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/dolly_v2_lightning_fsdp_finetuning/lightning-llm-finetuning-7b.ipynb#2025-04-12_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# First split the dataset into multiple sentences.\ntrain_ds = train_ds.map_batches(split_text, batch_format=\"pandas\")\ntrain_ds.take(10)\n```\n\n----------------------------------------\n\nTITLE: Hidden Table of Contents in reStructuredText\nDESCRIPTION: This code snippet defines a hidden table of contents for the document, listing various getting started guides for KubeRay components.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started.md#2025-04-12_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n```{toctree}\n:hidden:\n\ngetting-started/kuberay-operator-installation\ngetting-started/raycluster-quick-start\ngetting-started/rayjob-quick-start\ngetting-started/rayservice-quick-start\n```\n```\n\n----------------------------------------\n\nTITLE: Visualizing Ray Docker Image Hierarchy\nDESCRIPTION: This ASCII diagram illustrates the hierarchy of Ray Docker images, showing how CPU and GPU variants are built from different base images. It demonstrates the layered structure and dependencies between images.\nSOURCE: https://github.com/ray-project/ray/blob/master/docker/README.md#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nubuntu:22.04\n base-deps:cpu\n     ray:cpu\n\nnvidia/cuda\n base-deps:cudaXXX\n     ray:cudaXXX\n         ray-llm:cudaXXX\n```\n\n----------------------------------------\n\nTITLE: Defining the SingleAgentEpisode Constructor in Python\nDESCRIPTION: This snippet provides an overview of the constructor for the SingleAgentEpisode class, which initializes an episode object for a single agent in a reinforcement learning environment.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/package_ref/env/single_agent_episode.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n    SingleAgentEpisode class constructor\n    Initializes the SingleAgentEpisode instance with the specified parameters.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Test Cases\nDESCRIPTION: Sets up test cases from CoNaLa's test split to evaluate the model's code generation capabilities across different programming tasks.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/vicuna_13b_lightning_deepspeed_finetuning/vicuna_13b_lightning_deepspeed_finetune.ipynb#2025-04-12_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntestcases = [\n    {\n        \"intent\": \"replace white spaces in colunm 'col' of dataframe `df` with '_'\",\n    },\n    {\n        \"intent\": \"search for occurrences of regex pattern '>.*<' in xml string `line`\",\n    },\n    {\n        \"intent\": \"send a signal `signal.SIGUSR1` to the current process\",\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Defining fsspec Package with Cryptographic Hashes in requirements.txt\nDESCRIPTION: This snippet defines the fsspec package dependency with version 2023.5.0 and its SHA256 hashes for verification. It also includes comments indicating which other requirement files depend on this package.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cpu.txt#2025-04-12_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nfsspec==2023.5.0 \\\n    --hash=sha256:51a4ad01a5bb66fcc58036e288c0d53d3975a0df2a5dc59a93b59bade0391f2a \\\n    --hash=sha256:b3b56e00fb93ea321bc9e5d9cf6f8522a0198b20eb24e02774d329e9c6fb84ce\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Profiling Environment\nDESCRIPTION: Environment variable setup for enabling CPU profiling of the Ray Raylet component.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-contribute/profiling.rst#2025-04-12_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport PERFTOOLS_PATH=/usr/lib/x86_64-linux-gnu/libprofiler.so\nexport PERFTOOLS_LOGFILE=/tmp/pprof.out\nexport RAY_RAYLET_PERFTOOLS_PROFILER=1\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements List\nDESCRIPTION: Defines the exact versions of Python packages required for the project, including ML frameworks like PyTorch and JAX, along with Hugging Face libraries for model development.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/templates/05_dreambooth_finetuning/dreambooth/requirements.txt#2025-04-12_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate==0.20.3\nbitsandbytes==0.39.1\ndiffusers==0.19.3\nflax==0.6.11\nipywidgets\nhuggingface_hub==0.27.0\njax==0.4.13\njaxlib==0.4.13\nnumpy==1.24.4\ntorch==2.0.1\ntorchvision==0.15.2\ntransformers==4.30.2\n```\n\n----------------------------------------\n\nTITLE: Java Stack Trace with ScriptableObject Slot Access in Rhino\nDESCRIPTION: This stack trace shows the internal slot-based property lookup mechanism in Mozilla Rhino during JavaScript execution in Vert.x. It reveals how Rhino manages the activation object for function execution and property lookups when processing HTTP requests.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_64\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.createFunctionActivation_[j];org/mozilla/javascript/IdScriptableObject:.get_[j];org/mozilla/javascript/ScriptableObject:.getSlot_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Selecting a Sample Location for Model Testing\nDESCRIPTION: Extracts a sample location ID from the index of the final DataFrame. This location will be used to demonstrate loading and testing a model from the saved checkpoints.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/batch_tuning.ipynb#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Choose a dropoff location\nsample_location_id = final_df.index[0]\nsample_location_id\n```\n\n----------------------------------------\n\nTITLE: Training Results Table Output\nDESCRIPTION: ASCII table displaying training metrics for different RL algorithms including trial names, status, iterations, total time, timesteps and rewards. Shows results for 24 trials across different implementations (IMPALA, PPO, APEX, A2C, DQN) of which 16 were terminated and 8 had errors.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.4/rllib_regression.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n+-------------------------------------+------------+-------+--------+------------------+---------+-----------+\n| Trial name                          | status     | loc   |   iter |   total time (s) |      ts |    reward |\n|-------------------------------------+------------+-------+--------+------------------+---------+-----------|\n| IMPALA_BreakoutNoFrameskip-v4_00000 | TERMINATED |       |    293 |         3612.08  | 7270500 | 351.08    |\n| IMPALA_BreakoutNoFrameskip-v4_00001 | TERMINATED |       |    293 |         3612.11  | 7252500 | 181.3     |\n| IMPALA_BreakoutNoFrameskip-v4_00002 | TERMINATED |       |    293 |         3610.22  | 7283000 | 377.12    |\n| IMPALA_BreakoutNoFrameskip-v4_00003 | TERMINATED |       |    293 |         3612.02  | 7238000 |   1.9927  |\n```\n\n----------------------------------------\n\nTITLE: Creating a Single-Host v4 TPU Node Pool\nDESCRIPTION: Adds a TPU node pool with a single-host v4 TPU topology (2x2x1) to the GKE cluster. Includes autoscaling configuration with a machine type of ct4p-hightpu-4t.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/user-guides/gcp-gke-tpu-cluster.md#2025-04-12_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container node-pools create v4-4 \\\n  --zone $ZONE \\\n  --cluster $CLUSTER_NAME \\\n  --num-nodes 1 \\\n  --min-nodes 0 \\\n  --max-nodes 10 \\\n  --enable-autoscaling \\\n  --machine-type ct4p-hightpu-4t \\\n  --tpu-topology 2x2x1\n```\n\n----------------------------------------\n\nTITLE: Importing required libraries\nDESCRIPTION: This snippet imports the required libraries, including time, ray, and specific modules from ray.tune for defining and running the experiment, as well as the BayesOptSearch algorithm.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/bayesopt_example.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport ray\nfrom ray import tune\nfrom ray.tune.search import ConcurrencyLimiter\nfrom ray.tune.search.bayesopt import BayesOptSearch\n\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Table of Contents in Markdown\nDESCRIPTION: This snippet defines a hidden table of contents (toctree) in Markdown format that links to various example pages for KubeRay deployments. The toctree is configured as hidden and lists all the available example documentation pages.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/examples.md#2025-04-12_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n\nexamples/ml-example\nexamples/gpu-training-example\nexamples/mnist-training-example\nexamples/stable-diffusion-rayservice\nexamples/tpu-serve-stable-diffusion\nexamples/mobilenet-rayservice\nexamples/text-summarizer-rayservice\nexamples/rayjob-batch-inference-example\nexamples/rayjob-kueue-priority-scheduling\nexamples/rayjob-kueue-gang-scheduling\nexamples/distributed-checkpointing-with-gcsfuse\nexamples/modin-example\nexamples/vllm-rayservice\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Java static method and class for cross-language use\nDESCRIPTION: Defines a Java static method 'add' and a class 'Counter' that will be called from Python in a cross-language context.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_4\n\nLANGUAGE: java\nCODE:\n```\npackage io.ray.demo;\n\npublic class Math {\n\n  public static int add(int a, int b) {\n    return a + b;\n  }\n}\n```\n\nLANGUAGE: java\nCODE:\n```\npackage io.ray.demo;\n\n// A regular Java class.\npublic class Counter {\n\n  private int value = 0;\n\n  public int increment() {\n    this.value += 1;\n    return this.value;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Ray driver with multiple directories in Java\nDESCRIPTION: Shows how to set multiple directories for code search path in Java using command-line arguments.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/cross-language.rst#2025-04-12_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath <classpath> \\\n    -Dray.address=<address> \\\n    -Dray.job.code-search-path=/path/to/jars:/path/to/pys \\\n    <classname> <args>\n```\n\n----------------------------------------\n\nTITLE: RPDS-PY Persistent Data Structures Library Dependency\nDESCRIPTION: Specifies the rpds-py package (version 0.22.3) with multiple hash signatures for secure verification. This package provides persistent data structures for Python.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_32\n\nLANGUAGE: txt\nCODE:\n```\nrpds-py==0.22.3 \\\n    --hash=sha256:009de23c9c9ee54bf11303a966edf4d9087cd43a6003672e6aa7def643d06518 \\\n    --hash=sha256:02fbb9c288ae08bcb34fb41d516d5eeb0455ac35b5512d03181d755d80810059 \\\n    --hash=sha256:0a0461200769ab3b9ab7e513f6013b7a97fdeee41c29b9db343f3c5a8e2b9e61 \\\n    --hash=sha256:0b09865a9abc0ddff4e50b5ef65467cd94176bf1e0004184eb915cbc10fc05c5 \\\n    --hash=sha256:0b8db6b5b2d4491ad5b6bdc2bc7c017eec108acbf4e6785f42a9eb0ba234f4c9 \\\n    --hash=sha256:0c150c7a61ed4a4f4955a96626574e9baf1adf772c2fb61ef6a5027e52803543 \\\n    --hash=sha256:0f3cec041684de9a4684b1572fe28c7267410e02450f4561700ca5a3bc6695a2 \\\n\n```\n\n----------------------------------------\n\nTITLE: Handling Events in Ray Workflows\nDESCRIPTION: Demonstrates how to work with events in Ray Workflows, including sleep events and custom event listeners. It shows how to create tasks that wait for specific events before execution.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/workflows/key-concepts.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\n# Sleep is a special type of event.\nsleep_task = workflow.sleep(1)\n\n# `wait_for_events` allows for pluggable event listeners.\nevent_task = workflow.wait_for_event(workflow.event_listener.TimerListener, time.time() + 2)\n\n@ray.remote\ndef gather(*args):\n    return args\n\n# If a task's arguments include events, the task won't be executed until all\n# of the events have occurred.\nworkflow.run(gather.bind(sleep_task, event_task, \"hello world\"))\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hashes\nDESCRIPTION: Detailed package dependencies listing with specific versions and SHA256 hashes for security verification. Includes packages like psutil, py-cpuinfo, py-spy, pyarrow, and various other dependencies needed for the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu124.txt#2025-04-12_snippet_27\n\nLANGUAGE: plaintext\nCODE:\n```\npsutil==5.9.6 \\\n    --hash=sha256:10e8c17b4f898d64b121149afb136c53ea8b68c7531155147867b7b1ac9e7e28 \\\n    --hash=sha256:18cd22c5db486f33998f37e2bb054cc62fd06646995285e02a51b1e08da97017\n```\n\n----------------------------------------\n\nTITLE: Setting Global Variables for Ray Setup\nDESCRIPTION: Defines key variables for initializing a Ray cluster with 16 workers, each using 1 GPU and 8 CPUs. These parameters configure the resources Ray will use during the fine-tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"EleutherAI/gpt-j-6B\"\nuse_gpu = True\nnum_workers = 16\ncpus_per_worker = 8\n```\n\n----------------------------------------\n\nTITLE: TensorFlow MNIST Model Tuning with Ray Tune\nDESCRIPTION: This code snippet demonstrates how to tune the hyperparameters of a TensorFlow MNIST model using Ray Tune and Ray Train. It leverages `literalinclude` to include the content of the `tune_tensorflow_mnist_example.py` file.  The example shows how to define a training function, configure the search space, and run the hyperparameter tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/tf/tune_tensorflow_mnist_example.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\n.. literalinclude:: /../../python/ray/train/examples/tf/tune_tensorflow_mnist_example.py\n\n```\n\n----------------------------------------\n\nTITLE: Java Netty NIO Event Processing with Virtual Table Chunks Access\nDESCRIPTION: Stack trace showing the execution path through to virtual table chunks access. This trace illustrates the low-level operation of the JavaScript engine during property lookup in HTTP request processing.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_56\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelRead_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelRead_[j];org/vertx/java/core/net/impl/VertxHandler:.channelRead_[j];org/vertx/java/core/http/impl/DefaultHttpServer$ServerHandler:.doMessageReceived_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/gen/file__home_bgregg_testtest_vert_x_2_1_4_sys_mods_io_vertx_lang_js_1_1_0_[j];org/mozilla/javascript/ScriptRuntime:.getObjectProp_[j];vtable chunks_[j] 1\n```\n\n----------------------------------------\n\nTITLE: Updating vSphere Configurations (Python)\nDESCRIPTION: Validates and updates the vSphere configurations provided by the user in the YAML file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/vsphere/ARCHITECTURE.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Update vSphere Configs\n```\n\n----------------------------------------\n\nTITLE: Training Results ASCII Table\nDESCRIPTION: ASCII-formatted table showing performance comparison of different RL algorithms (IMPALA, PPO, APEX, A2C, DQN) on BreakoutNoFrameskip-v4 environment, including metrics like trial status, iterations, runtime, timesteps and rewards.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.7.7/rllib_regression.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n+----------------------------------------+------------+-------+--------+------------------+-------------+----------+\n| Trial name                             | status     | loc   |   iter |   total time (s) |   timesteps |   reward |\n|----------------------------------------+------------+-------+--------+------------------+-------------+----------|\n| IMPALA_BreakoutNoFrameskip-v4_4445a400 | TERMINATED |       |    294 |          3600.28 |     6183500 |   352.15 |\n| IMPALA_BreakoutNoFrameskip-v4_44461b92 | TERMINATED |       |    292 |          3610.1  |     6178500 |   147.14 |\n| IMPALA_BreakoutNoFrameskip-v4_44467b28 | TERMINATED |       |    293 |          3611.05 |     6200500 |   275.1  |\n| IMPALA_BreakoutNoFrameskip-v4_4446e1bc | TERMINATED |       |    294 |          3611.16 |     6161500 |   322.95 |\n| PPO_BreakoutNoFrameskip-v4_44474a4e    | TERMINATED |       |    549 |          3603.09 |     2745000 |    36.41 |\n| PPO_BreakoutNoFrameskip-v4_4447b740    | TERMINATED |       |    545 |          3604.43 |     2725000 |    16.83 |\n| PPO_BreakoutNoFrameskip-v4_44481ea6    | TERMINATED |       |    549 |          3605.4  |     2745000 |    52.36 |\n| PPO_BreakoutNoFrameskip-v4_444885a8    | TERMINATED |       |    546 |          3605.27 |     2730000 |    34.94 |\n| APEX_BreakoutNoFrameskip-v4_4448feac   | TERMINATED |       |    113 |          3629.93 |     3552960 |    13.47 |\n| APEX_BreakoutNoFrameskip-v4_44497684   | TERMINATED |       |    112 |          3615.5  |     3539360 |    28.65 |\n| APEX_BreakoutNoFrameskip-v4_4449f01e   | TERMINATED |       |    113 |          3621.85 |     3524640 |    28.17 |\n| APEX_BreakoutNoFrameskip-v4_444a5dd8   | TERMINATED |       |    112 |          3616.41 |     3447520 |    20.56 |\n| A2C_BreakoutNoFrameskip-v4_444acb7e    | TERMINATED |       |    349 |          3604.38 |     2981000 |   108.8  |\n| A2C_BreakoutNoFrameskip-v4_444b3a50    | TERMINATED |       |    349 |          3602.3  |     2967500 |   145.82 |\n| A2C_BreakoutNoFrameskip-v4_444b9ebe    | TERMINATED |       |    349 |          3605.71 |     2990500 |   127.3  |\n| A2C_BreakoutNoFrameskip-v4_444c0016    | TERMINATED |       |    349 |          3602.58 |     2970000 |   112.92 |\n| DQN_BreakoutNoFrameskip-v4_444c6830    | TERMINATED |       |     30 |          3662.81 |      300000 |    18.71 |\n| DQN_BreakoutNoFrameskip-v4_444cdeb4    | TERMINATED |       |     30 |          3608.95 |      300000 |    17.45 |\n| DQN_BreakoutNoFrameskip-v4_444d5038    | TERMINATED |       |     31 |          3658.65 |      310000 |    17.43 |\n| DQN_BreakoutNoFrameskip-v4_444dbfb4    | TERMINATED |       |     31 |          3696.8  |      310000 |    18.63 |\n+----------------------------------------+------------+-------+--------+------------------+-------------+----------+\n```\n\n----------------------------------------\n\nTITLE: Deleting RayJob\nDESCRIPTION: kubectl command to delete the RayJob configuration.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete -f https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operator/config/samples/ray-job.sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Package Hash List - Requirements File\nDESCRIPTION: A comprehensive list of SHA256 hash values for package dependencies, structured as pip requirements file format with hash verification\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_36\n\nLANGUAGE: text\nCODE:\n```\n--hash=sha256:21287bcdd299fdc3328cc0fbbdeaa46838a1c05391264e51ddb38a3f5b09611f \\\n--hash=sha256:23cfd9ca09acaf07a43e5a695143d9a21bf00f5b49b15c07d5388cadf1f9ce11 \\\n--hash=sha256:248d3e83d119770f96003271fe41e049dd4ae52da2feb8f832b7a20e791d2920\n```\n\n----------------------------------------\n\nTITLE: Connecting to Existing Ray Cluster via Command Line\nDESCRIPTION: This command-line example shows how to connect to an existing Ray cluster using the ray start command. It demonstrates specifying the address of the head node and setting custom resources.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/tests/project_files/requirements_project/requirements.txt#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nray start --address='localhost:6379' --resources='{\"Custom1\": 1, \"Custom2\": 2}'\n```\n\n----------------------------------------\n\nTITLE: Migrating ModelV2 to RLModule by Configuration\nDESCRIPTION: Shows the process of migrating a ModelV2 setup to the new RLModule API stack using configuration, facilitating transition to newer functionalities.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rllib-examples.rst#2025-04-12_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Migrate ModelV2 to RLModule by config\n# This script demonstrates migration through configuration settings.\n```\n\n```\n\n----------------------------------------\n\nTITLE: Defining hash-verified Python dependencies in requirements.txt for Ray project\nDESCRIPTION: This requirements file specifies exact package versions with SHA256 hash verification to ensure secure and reproducible installations. It includes comments indicating which packages depend on each requirement through the Ray project structure.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cpu.txt#2025-04-12_snippet_4\n\nLANGUAGE: pip\nCODE:\n```\n--hash=sha256:070a54e890cefb5b3739d19f30f5a5ec840ffc9c50ffa7d23cc9fc1a38ebbfc5 \\\n    --hash=sha256:6a6effda93f4e1ce9f618779b2dd1d9d84f1e32812c23a29b3fff6fd7f63fa5e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   depyf\nattrs==25.1.0 \\\n    --hash=sha256:1c97078a80c814273a76b2a298a932eb681c87415c11dee0a6921de7f1b02c3e \\\n    --hash=sha256:c75a69e28a550a7e93789579c22aa26b0f5b83b75dc4e08fe092980051e1090a\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   aiohttp\n    #   jsonschema\n    #   referencing\nbackoff==1.10.0 \\\n    --hash=sha256:5e73e2cbe780e1915a204799dba0a01896f45f4385e636bcca7a0614d879d0cd \\\n    --hash=sha256:b8fba021fac74055ac05eb7c7bfce4723aedde6cd0a504e5326bcb0bdd6d19a4\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   opentelemetry-exporter-otlp-proto-grpc\nblake3==1.0.4 \\\n    --hash=sha256:00605aa59923205c6a4f21131840840eb2d9a754c59b163357d890566755b97a \\\n    --hash=sha256:08f46c2f1c5f369f07409e3e4ff248bcb22617cd741f2224873d85982dd6034e \\\n    --hash=sha256:09b2c66bc2c797e9d783521ec22b1e9a6c74e3ddb98bdd0dcd4fcc2213fb27ec \\\n    --hash=sha256:0c6477a4689b374e846fd5330839c0d27d932fa62c2d2d6b731a28798d0348a0 \\\n    --hash=sha256:0f5888e358ae4bba094d4595e1703dfc230d96dea6924e877c42c7a98beda7b5 \\\n    --hash=sha256:105730671403972fb5292dcaff0b78881075f583cd7b5e1589919b0b0f93f86a \\\n    --hash=sha256:1509d898c7930451720f3667b1f733434db1090f295b6d947f88140face1c596 \\\n    --hash=sha256:1524b1cabb034f1c9dc2621f3c06c10d2a4608391cf04e5db182aa5d7a82fdbe \\\n    --hash=sha256:1575c9c39632107e96d4b830d03646310d4c1eb07473ced1f68dd82c3af89d49 \\\n    --hash=sha256:17fb8c25d62b3dc35c2c4d59f3b2f3234814b2aa374c0b9bea3d326184bf9268 \\\n    --hash=sha256:1845c2c8a611c30e43a88843f202663ce35a3d4d61a28064bf99a9adf975ab74 \\\n    --hash=sha256:1c66288e957625892303d683f7581fab56b567623f4c58bff159e8e92d042a8b \\\n    --hash=sha256:1d48407451ad537f7a8d9210a8468a600e453662832c6a60b99405d9d792c97e \\\n    --hash=sha256:1dbdca6def64c5fbcd7aae7403fc0e408506f91fac631efb2b604cac1bff97c4 \\\n    --hash=sha256:1e3018d12e16faea2e08f210123a9c2e603de6c1b80b381624cffd536e1022d1 \\\n    --hash=sha256:20e90f313c524bd98d68f3d1e0495ae00e570a164ee9a09ac21ded49c082c276 \\\n    --hash=sha256:222234ebea46d16ac981b0da528dd6e57e8ea37cef168e9f669894f660a18e09 \\\n    --hash=sha256:2492bbd5f9d305c586c3addb8e247e9c4ebb6048e5fe3f6baddaca224e858dd1 \\\n    --hash=sha256:27835e72adf624754f6380635111d5c17685fd8db04f6573aebb4f6442b139ae \\\n    --hash=sha256:2aeacc45ab0eebd91697a523e8c04542cff7d09b6a6c397d4a868f879950f539 \\\n    --hash=sha256:407327ed661ccb943c4361fb647daa6264cc6bdc52f29de56e4dc62c2132e287 \\\n    --hash=sha256:407d3a527853d662f79fa99b4ec88478fc9b800420194ed495a961635d2ab77e \\\n    --hash=sha256:41795136af622eb113247ccb09819e388948fc0aa052da02448c9f477c02721f \\\n    --hash=sha256:43ebbf2af260f645eb961b045ed4e9ddcdcf3fb49744c8f2e0ba1e1c28e88782 \\\n    --hash=sha256:4e5f23d483a0e22a46991031a659cd65e58a84c2b737544e5a126fd49ffece68 \\\n    --hash=sha256:512c7515a42398a5b01d758c53e315d295a1403b09786d9579d7f8dba4907865 \\\n    --hash=sha256:524ca0bf368b35d91254cbb16af5351beaee6c22a3a236d355b9471a61b3b9ff \\\n    --hash=sha256:5404a99dcd9d5974ec09a6cc3e66e730ed7b8f65f353dea88b614ca4ed8dcb02 \\\n    --hash=sha256:5447a5731ee408809a5e2582a3bed3069b570046017ddddf9942d71c8afdc2ee \\\n    --hash=sha256:54d792827498d664b4e0687ca35cde8bbdc616e6766421378179b89914a65a6e \\\n    --hash=sha256:5624985511c1e209aede209142c09c81a4163cf230f218aff09f04ddd9e773a1 \\\n    --hash=sha256:66dbc4383586232ddc135936c1f395848358981152dcc7b94710664c21621491 \\\n    --hash=sha256:6a45e4c5df4ce654d42897ce2d5bd7dab0a5e84b06ffcb9248ed0b537520967a \\\n    --hash=sha256:6bf7cbee22d7f9e4d60fcb9b2ae3270c40beea71fc7ee7d7d7eef539749a6aab \\\n    --hash=sha256:7240572bfd4e3ecd0ab24144551053c02eb3995e00342fcb40eb25619678e556 \\\n    --hash=sha256:7592124471fb1c8c67f94776c480743c182aff92952ceb5f5c793a632a1a1436 \\\n    --hash=sha256:77dd01c07d2f327a97233841c5c9295b3ef5ac372c5649843d413fe588bf41a9 \\\n    --hash=sha256:785ef236f8da4ab4f233d02c403fc1bc6eab093edad1ca5903dd9dbb2b1c8e26 \\\n    --hash=sha256:78f4724d0a9f6bebd0fccf27e4afaed1ca4b6645740ee425d3621defe27c4e64 \\\n    --hash=sha256:7a1ab4bb7869fd38b7be2a88557d28cfe63d44b194bf2bf27e4ff08c5f2483ea \\\n    --hash=sha256:8241e372dfcb01ebe3947b7d5e22af1af5682fc37631153fe6ed747a603edb26 \\\n    --hash=sha256:846895cbe050c8d0ba94c7a8df4f89f023db82e5f8d35c76def177e410a1ba97 \\\n    --hash=sha256:87794eed0b25de3713d57faa82a5e3257d0b51cba7831f7de98884b73d4c41af \\\n    --hash=sha256:89e21eb0929b1bd35867dd450c27600af42ecf1cd7a08c5496ad29baaa35cb8b \\\n    --hash=sha256:8a99749c02d76b7aa5d931c3b80528ef6a68149e6bef424769dd5e461d39a4f0 \\\n    --hash=sha256:8b514764be91cce5825e1a3dd393004a112f8acbf1c782aaa43c057c40837a01 \\\n    --hash=sha256:8e83ddd16ae0a3641ba6d7b0ed582f0b7fcdefbf95638e82ee2480ab209342d7 \\\n    --hash=sha256:8faf42585fbd6ea189ee15b3d148f64dd3a8ced5aa26bed90a7438a7cb7094a3 \\\n    --hash=sha256:94cc36d0e69dc118db3c288c196533603d0f3413017070b455fe63ef0075dca2 \\\n    --hash=sha256:95b2223177be6e269ab5f39bf1f2c186dc4852d546f15500bb7dcc114cf681f0 \\\n    --hash=sha256:97134b7c407e6c4ddcff1813577763b4e370397f9ba20cf0db3d0fff13b4edf5 \\\n    --hash=sha256:a3d1a39fed926d8b6fb0efdf0295297ff92246e1c28e5dca7f2d7185ad4593be \\\n    --hash=sha256:a5c5c0a2f17220ad493f2a116b3ca83aae039926c0abbf520bc32b44e6edebdb \\\n    --hash=sha256:a760153f4e66edd6214df0a69e7eb90206c8ddd8083734ac430e852453a58e06 \\\n    --hash=sha256:a764b697fd1cb01b92a18240f9afd291b1f33ede3c9cdc59dd92ba87a5f4f8f3 \\\n    --hash=sha256:af18fcd2a37aa51c24cedbb82f4934f39a9a4ea11a84d34c1ab63df94a28fdd1 \\\n    --hash=sha256:afba60a70ac75f26fb8fb95502b80b37cab7a624daae6e1a1b952457ff0e7528 \\\n    --hash=sha256:b11bffad2c020cc0049e02990caa924cc9c8b5ab6032bf3dbd60706638993bc5 \\\n    --hash=sha256:b691e44df67ce61b3573f31e4d304eeb4ffa87c4e05eb1f3f4a2a6981b875c96 \\\n    --hash=sha256:b8720b726802c534e1e53e7fb8f53cbd4ee5a052b8903934d210feeb69c6438d \\\n    --hash=sha256:baad3e55f7e1d8c820be370071fc80d6ed4cc7a738cbce4bc462772738869f57 \\\n    --hash=sha256:bb2689cbef663d823011eeddec29c23d1c1f773ac867bfa854fb0590771a309d \\\n    --hash=sha256:c00c483e3d86c2587b7c1e4c65f519fd8745a0963cd6e3630d1bf24692c57fa2 \\\n    --hash=sha256:c213768763faee5348bf7622b906b47b60a31baa44ad6837f6ec7587a4b3d4c1 \\\n    --hash=sha256:c40e2badab95569681759273013ea19349c438dfc3c50a5d2e5c88e1b3879ba5 \\\n    --hash=sha256:cbd2782b2034021de468dcd466d732411a957efe3cf989d2f5c1e07a708a5874 \\\n    --hash=sha256:d09816c855043fe6a498108f6e0ec0ced2d5c1e65bc8a8c24012d773ac4e3208 \\\n    --hash=sha256:d1c52d9492896560b40fee414c02e23e2d868a4ef280574f67049be3b66cbbd2 \\\n    --hash=sha256:d2a0e30369b1e9f24f81c6a666e347309aa746e85a7e986e472156995dc3751c \\\n    --hash=sha256:d8e89c286ee110b2e325b179954eb2176d4a6315caef2eb8b44bcac7374da2b0 \\\n    --hash=sha256:d97685ff806592fa2cb35143a3bdb255db58385cbf9c1a3222b4b127ade1714d \\\n    --hash=sha256:dbaf16fd19f93a2b5d2eadab82dca3161e2bf418606144df7edaf20bc38eda7c \\\n    --hash=sha256:e3087e019603657cda6d5e4b8cb250d6cbcf935e8230a31291eb15d3ee8a341e \\\n    --hash=sha256:e53f76390144272ecfe34da0466e1df66c3252e4e8a3b44b12d75c8acd393397 \\\n    --hash=sha256:e55e38da0f57aa924c3125ffc98df72c36b2d212a2b7eb8f1d71169746f14689 \\\n    --hash=sha256:e93d952635a96225dda9f0b94bb115a7f1c1777db38f8a49cb902bf9433dd436 \\\n    --hash=sha256:ea806c10ad6d7c83f3543a22f31fe4892896a1daf58f9e4e3d76ae25ec469a3a \\\n    --hash=sha256:f0488a0f730383939bc9c6453220b15b8c2cda702a2ce626e6fd5e3add3f8da8 \\\n    --hash=sha256:fae37ec23f25fdbb8c2a34dd9b309a8f9fdce9ff7685cabb1fde7e16f012cf67 \\\n    --hash=sha256:fb866a8e0632f35fe9c8e24b751752c2df4abbaf20a36e85a76883a382ccbfd9 \\\n    --hash=sha256:fbc00208e9ebd4595290a684609a7a0557ca892f28870f44df4e433d4758e9b8 \\\n    --hash=sha256:fc9da486d47f399ac2aba8dfdfaf60cc7a507d8434623cee8f81f47852db594d \\\n    --hash=sha256:fe01393d535a7ddea39f0332453434fe214fa135e05e5b792a99dd7782acf429 \\\n    --hash=sha256:fedc326cac4476d2eab88413a4bf56e491040ae11ea98ddadaa5487cecda9b93 \\\n    --hash=sha256:ff0e96f61b16b365ad5bb7c6272754f83d8a59c95d3b2f70c3bb6324ddf5bc0c\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   vllm\ncachetools==5.3.2 \\\n    --hash=sha256:086ee420196f7b2ab9ca2db2520aca326318b68fe5ba8bc4d49cca91add450f2 \\\n    --hash=sha256:861f35a13a451f94e301ce2bec7cac63e881232ccce7ed67fab9b5df4d3beaa1\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   google-auth\n    #   vllm\ncertifi==2025.1.31 \\\n    --hash=sha256:3d5da6925056f6f18f119200434a4780a94263f10d1c21d032a6f6b2baa20651 \\\n    --hash=sha256:ca78db4565a652026a4db2bcdf68f2fb589ea80d0be70e03929ed730746b84fe\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cpu.txt\n    #   httpcore\n    #   httpx\n    #   requests\ncffi==1.16.0 ; platform_python_implementation != 'PyPy' \\\n    --hash=sha256:0c9ef6ff37e974b73c25eecc13952c55bceed9112be2d9d938ded8e856138bcc \\\n    --hash=sha256:131fd094d1065b19540c3d72594260f118b231090295d8c34e19a7bbcf2e860a \\\n    --hash=sha256:1b8ebc27c014c59692bb2664c7d13ce7a6e9a629be20e54e7271fa696ff2b417 \\\n    --hash=sha256:2c56b361916f390cd758a57f2e16233eb4f64bcbeee88a4881ea90fca14dc6ab \\\n    --hash=sha256:2d92b25dbf6cae33f65005baf472d2c245c050b1ce709cc4588cdcdd5495b520 \\\n    --hash=sha256:31d13b0f99e0836b7ff893d37af07366ebc90b678b6664c955b54561fc36ef36 \\\n    --hash=sha256:32c68ef735dbe5857c810328cb2481e24722a59a2003018885514d4c09af9743 \\\n    --hash=sha256:3686dffb02459559c74dd3d81748269ffb0eb027c39a6fc99502de37d501faa8 \\\n    --hash=sha256:582215a0e9adbe0e379761260553ba11c58943e4bbe9c36430c4ca6ac74b15ed \\\n    --hash=sha256:5b50bf3f55561dac5438f8e70bfcdfd74543fd60df5fa5f62d94e5867deca684 \\\n    --hash=sha256:5bf44d66cdf9e893637896c7faa22298baebcd18d1ddb6d2626a6e39793a1d56 \\\n    --hash=sha256:6602bc8dc6f3a9e02b6c22c4fc1e47aa50f8f8e6d3f78a5e16ac33ef5fefa324 \\\n    --hash=sha256:673739cb539f8cdaa07d92d02efa93c9ccf87e345b9a0b556e3ecc666718468d \\\n```\n\n----------------------------------------\n\nTITLE: Defining the Search Space for Tune Experiments\nDESCRIPTION: This code defines the search space for a grid sweep experiment, creating configurations for multiple models. It generates a list of configurations that will be passed to each trial instance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-run.rst#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Generate configurations for grid sweep\nNUM_MODELS = 100\nconfig = {\n    \"model_id\": tune.grid_search([f\"model_{i}\" for i in range(NUM_MODELS)])\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing RLModuleSpec for Single Model\nDESCRIPTION: Demonstrates creating an RLModuleSpec for a single RLModule, specifying the module class, observation and action spaces, along with model configuration. The spec is then used to build the RLModule instance.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/rllib/rl-modules.rst#2025-04-12_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom ray.rllib.algorithms.bc.torch.default_bc_torch_rl_module import DefaultBCTorchRLModule\nfrom ray.rllib.core.rl_module.rl_module import RLModuleSpec\n\n# Create an env object to know the spaces.\nenv = gym.make(\"CartPole-v1\")\n\n# First construct the spec.\nspec = RLModuleSpec(\n    module_class=DefaultBCTorchRLModule,\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    # A custom dict that's accessible inside your class as `self.model_config`.\n    model_config={\"fcnet_hiddens\": [64]},\n)\n\n# Then, build the RLModule through the spec's `build()` method.\nrl_module = spec.build()\n```\n\n----------------------------------------\n\nTITLE: PBT Visualization Utilities in Ray Tune\nDESCRIPTION: This snippet includes the entire content of the `pbt_visualization_utils.py` file. It provides utility functions used to create visualizations of Population Based Training runs performed using Ray Tune. The actual content of this file is defined below.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/examples/pbt_visualization/pbt_visualization_utils.rst#2025-04-12_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\n\"\"\"Utilities for PBT Visualization Example.\n\nFor example usage, see pbt_visualization.py\n\"\"\"\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\n\nfrom ray import tune\n\n\ndef load_population_data(path):\n    \"\"\"Load PBT population data from tune analysis path.\"\"\"\n    dfs = {}\n    for trial in os.listdir(path):\n        if trial.startswith(\"trial\"):  # Only load trials\n            try:\n                # Find log file, assuming only 1.\n                trial_path = os.path.join(path, trial)\n                files = [f for f in os.listdir(trial_path) if f.endswith(\".csv\")]\n                if len(files) != 1:\n                    raise ValueError(\n                        \"Expected 1 csv file, found {} in {}\".format(\n                            len(files), trial_path\n                        )\n                    )\n                file = files[0]\n                filepath = os.path.join(trial_path, file)\n                df = pd.read_csv(filepath)\n                trial_id = df[\"trial_id\"].iloc[0]\n                dfs[trial_id] = df\n            except Exception:\n                print(\"Problem loading trial\", trial)\n    df = pd.concat(dfs.values())\n    return df\n\n\ndef plot_time_vs_metric(df, metric=\"episode_reward_mean\", ax=None):\n    \"\"\"Plot performance vs. time.\n\n    Args:\n        df (pd.DataFrame): Population dataframe (output of\n            `load_population_data()`)\n        metric (str): Metric to plot\n        ax (plt.Axes): Matplotlib axis. Created if not passed.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    sns.lineplot(x=\"training_iteration\", y=metric, hue=\"trial_id\", data=df, ax=ax)\n    ax.set_xlabel(\"Time (Training Iteration)\")\n    ax.set_ylabel(metric)\n    return ax\n\n\ndef plot_hp_vs_metric(df, hp, metric=\"episode_reward_mean\", ax=None):\n    \"\"\"Plot hyperparameter vs. performance.\n\n    Args:\n        df (pd.DataFrame): Population dataframe (output of\n            `load_population_data()`)\n        hp (str): Hyperparameter to plot\n        metric (str): Metric to plot\n        ax (plt.Axes): Matplotlib axis. Created if not passed.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    sns.lineplot(x=hp, y=metric, hue=\"trial_id\", data=df, ax=ax)\n    ax.set_xlabel(hp)\n    ax.set_ylabel(metric)\n    return ax\n\n\ndef last_value(df, metric):\n    \"\"\"Get last reported value. Handle NAs by propagating previous value.\"\"\"\n    return df.ffill().groupby(\"trial_id\").last()[metric]\n\n\ndef plot_hp_importance(df, hps, metric=\"episode_reward_mean\", ax=None):\n    \"\"\"Plot hyperparameter value vs metric improvement.\n\n    Args:\n        df (pd.DataFrame): Population dataframe (output of\n            `load_population_data()`)\n        hps (list): List of hyperparameters to plot\n        metric (str): Metric to plot\n        ax (plt.Axes): Matplotlib axis. Created if not passed.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n\n    # Last reported metric value\n    perf = last_value(df, metric)\n\n    # Initial hyperparameter values\n    initial_hps = df.groupby(\"trial_id\").first()[hps]\n\n    # Hyperparameter importance\n    hp_importance = {hp: [] for hp in hps}\n    for trial_id in df[\"trial_id\"].unique():\n        other_trial_ids = [t for t in df[\"trial_id\"].unique() if t != trial_id]\n        for other_trial_id in other_trial_ids:\n            # Improvement in metric if hp was copied from other_trial_id\n            perf_change = perf[trial_id] - perf[other_trial_id]\n            for hp in hps:\n                hp_importance[hp].append(\n                    initial_hps.loc[other_trial_id, hp] - initial_hps.loc[trial_id, hp]\n                )\n\n    # Plotting\n    for hp in hps:\n        sns.regplot(\n            x=hp_importance[hp],\n            y=np.zeros(len(hp_importance[hp])),\n            marker=\"o\",\n            scatter_kws={\"alpha\": 0.2},\n            ax=ax,\n        )\n    ax.axvline(0, ls=\"--\", color=\"k\", alpha=0.5)\n\n    ax.set_yticks([])\n    ax.set_xlabel(\"Change in HP Value\")\n    ax.set_title(\"Change in Hyperparameter Value During Exploration\")\n    return ax\n\n```\n\n----------------------------------------\n\nTITLE: Viewing Training Metrics\nDESCRIPTION: Prints the metrics resulting from the XGBoost training process, including train and validation loss, error rates, and other training statistics.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/xgboost/distributed-xgboost-lightgbm.ipynb#2025-04-12_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(result.metrics)\n```\n\n----------------------------------------\n\nTITLE: Sending Initial Client Request\nDESCRIPTION: Python code for making an initial client request to the Serve application before configuration update\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/serve/advanced-guides/inplace-updates.md#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Python client request code snippet\n```\n\n----------------------------------------\n\nTITLE: Specifying gitpython Package Dependency with Hash Values\nDESCRIPTION: Definition of the gitpython package dependency at version 3.1.40 with associated hash values for verification. The comment indicates this package is required by the Ray project's cloud requirements file.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_9\n\nLANGUAGE: text\nCODE:\n```\ngitpython==3.1.40 \\\n    --hash=sha256:22b126e9ffb671fdd0c129796343a02bf67bf2994b35449ffc9321aa755e18a4 \\\n    --hash=sha256:cf14627d5a8049ffbf49915732e5eddbe8134c3bdb9d476e6182b676fc573f8a\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements/cloud-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Llama-2 Fine-tuning\nDESCRIPTION: Installs necessary Python packages for model training, including Ray, transformers, datasets, and Habana Gaudi-specific libraries\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/train/examples/intel_gaudi/llama.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install ray[train] notebook transformers datasets evaluate peft accelerate scikit-learn optimum-habana\n\npip install git+https://github.com/HabanaAI/DeepSpeed.git@1.20.0\n```\n\n----------------------------------------\n\nTITLE: Displaying TaskID Layouts for Different Task Types\nDESCRIPTION: A table showing the layout of TaskID for different types of tasks in Ray, including Normal Task, Actor Task, Actor Creation Task, and Driver Task. It illustrates how the task unique bytes and actor id components vary for each task type.\nSOURCE: https://github.com/ray-project/ray/blob/master/src/ray/design_docs/id_specification.md#2025-04-12_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n+-------------------+-----------------+------------+---------------------------+-----------------+\n|                   | Normal Task     | Actor Task | Actor Creation Task       | Driver Task     |\n+-------------------+-----------------+------------+---------------------------+-----------------+\n| task unique bytes | random          | random     | nil                       | nil             |\n+-------------------+-----------------+------------+---------------------------+-----------------+\n| actor id          | dummy actor id* | actor id   | Id of the actor to create | dummy actor id* |\n+-------------------+-----------------+------------+---------------------------+-----------------+\nNote: Dummy actor id is an `ActorID` whose unique part is nil.\n```\n\n----------------------------------------\n\nTITLE: Netty NIO Event Loop Call Trace with Network Packet Backlog Processing\nDESCRIPTION: Call stack trace showing the Netty NIO event loop with kernel network packet backlog processing. This trace illustrates how received packets are processed through the Linux kernel's networking subsystem after a write operation is performed from the Java Netty application.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/ray/dashboard/client/public/speedscope-1.5.3/perf-vertx-stacks-01-collapsed-all.3e0a632c.txt#2025-04-12_snippet_12\n\nLANGUAGE: java\nCODE:\n```\njava;start_thread;java_start;JavaThread::run;JavaThread::thread_main_inner;thread_entry;JavaCalls::call_virtual;JavaCalls::call_virtual;JavaCalls::call_helper;call_stub_[j];Interpreter_[j];Interpreter_[j];io/netty/channel/nio/NioEventLoop:.run_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeys_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKeysOptimized_[j];io/netty/channel/nio/NioEventLoop:.processSelectedKey_[j];io/netty/channel/nio/AbstractNioByteChannel$NioByteUnsafe:.read_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];io/netty/handler/codec/ByteToMessageDecoder:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.fireChannelReadComplete_[j];org/vertx/java/core/net/impl/VertxHandler:.channelReadComplete_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelDuplexHandler:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/ChannelOutboundHandlerAdapter:.flush_[j];io/netty/channel/AbstractChannelHandlerContext:.flush_[j];io/netty/channel/DefaultChannelPipeline$HeadContext:.flush_[j];io/netty/channel/AbstractChannel$AbstractUnsafe:.flush0_[j];io/netty/channel/nio/AbstractNioByteChannel:.doWrite_[j];io/netty/buffer/PooledUnsafeDirectByteBuf:.readBytes_[j];sun/nio/ch/SocketChannelImpl:.write_[j];sun/nio/ch/FileDispatcherImpl:.write0_[j];write;system_call_fastpath_[k];sys_write_[k];vfs_write_[k];do_sync_write_[k];sock_aio_write_[k];do_sock_write.isra.10_[k];inet_sendmsg_[k];tcp_sendmsg_[k];__tcp_push_pending_frames_[k];tcp_write_xmit_[k];tcp_transmit_skb_[k];ip_queue_xmit_[k];ip_local_out_[k];ip_output_[k];ip_finish_output_[k];dev_queue_xmit_[k];local_bh_enable_[k];do_softirq_[k];call_softirq_[k];__do_softirq_[k];net_rx_action_[k];process_backlog_[k];__netif_receive_skb_[k] 1\n```\n\n----------------------------------------\n\nTITLE: Submitting a Ray job with uv\nDESCRIPTION: This snippet shows how to submit a Ray job using the `ray job submit` command with uv. The `--working-dir .` argument specifies the current directory as the working directory, and `uv run test.py` executes the 'test.py' script within the uv environment defined by the `pyproject.toml` file.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/ray-core/handling-dependencies.rst#2025-04-12_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\nray job submit --working-dir . -- uv run test.py\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Hash Verification\nDESCRIPTION: Detailed package requirements listing with SHA256 hash verification values. Includes dependencies like markdown-it-py, markupsafe, matplotlib-inline, mdurl, memray, mistune, and msgpack with their specific versions and hash values for secure package verification.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_ray_test_py311_cu124.txt#2025-04-12_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:f1d18718f9d78182c6b60f568c9a9cec8a7204d7cb6fad4e511a2ef279e4cb05 \\\n    --hash=sha256:f4c7bf687303ca47d69f9f0133274958fd672efaa33fb5bcde467862d6c621f0 \\\n    --hash=sha256:f76176492ff082657ada0d0f10c794b6da5800249ef1692b35cf49b1e93e8ef7\n    # via\n    #   -c /tmp/ray-deps/requirements_compiled.txt\n    #   -r python/requirements.txt\nmarkdown-it-py==2.2.0 \\\n    --hash=sha256:5a35f8d1870171d9acc47b99612dc146129b631baf04970128b568f190d0cc30 \\\n    --hash=sha256:7c9a5e412688bc771c67432cbfebcdd686c93ce6484913dccf06cb5a0bea35a1\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for GPT-J Fine-Tuning\nDESCRIPTION: Imports numpy, pandas, and os libraries for data manipulation and file operations in the GPT-J fine-tuning process.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/air_examples/gptj_deepspeed_finetuning/gptj_deepspeed_fine_tuning.ipynb#2025-04-12_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport os\n```\n\n----------------------------------------\n\nTITLE: Performance Log Output for Ray Project Stages\nDESCRIPTION: Performance measurement results displaying execution times and statistics across 4 stages (0-3) of a Ray project. Includes total runtime, iteration statistics (average, max, min), and actor creation timing.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/release_logs/0.8.2/stress_tests/test_many_tasks.txt#2025-04-12_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nStage 0 results:\\n\\tTotal time: 22.579216480255127\\nStage 1 results:\\n\\tTotal time: 154.41431832313538\\n\\tAverage iteration time: 15.441423058509827\\n\\tMax iteration time: 15.943994760513306\\n\\tMin iteration time: 15.029884099960327\\nStage 2 results:\\n\\tTotal time: 646.7662391662598\\n\\tAverage iteration time: 129.35279755592347\\n\\tMax iteration time: 134.80017256736755\\n\\tMin iteration time: 121.44297170639038\\nStage 3 results:\\n\\tActor creation time: 0.0635519027709961\\n\\tTotal time: 3464.0461547374725\n```\n\n----------------------------------------\n\nTITLE: Specifying Jiter Dependency for Ray Project\nDESCRIPTION: Defines the Jiter package dependency with version 0.8.2 and numerous verification hashes. This dependency is utilized by the OpenAI component within the Ray project.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_py311_cu121.txt#2025-04-12_snippet_9\n\nLANGUAGE: pip\nCODE:\n```\njiter==0.8.2 \\\n    --hash=sha256:025337859077b41548bdcbabe38698bcd93cfe10b06ff66617a48ff92c9aec60 \\\n    --hash=sha256:03c9df035d4f8d647f8c210ddc2ae0728387275340668fb30d2421e17d9a0841 \\\n    --hash=sha256:08d4c92bf480e19fc3f2717c9ce2aa31dceaa9163839a311424b6862252c943e \\\n    --hash=sha256:0cf5dfa9956d96ff2efb0f8e9c7d055904012c952539a774305aaaf3abdf3d6c \\\n    --hash=sha256:14601dcac4889e0a1c75ccf6a0e4baf70dbc75041e51bcf8d0e9274519df6887 \\\n    --hash=sha256:180a8aea058f7535d1c84183c0362c710f4750bef66630c05f40c93c2b152a0f \\\n    --hash=sha256:1c0dfbd1be3cbefc7510102370d86e35d1d53e5a93d48519688b1bf0f761160a \\\n    --hash=sha256:2dd61c5afc88a4fda7d8b2cf03ae5947c6ac7516d32b7a15bf4b49569a5c076b \\\n    --hash=sha256:317b25e98a35ffec5c67efe56a4e9970852632c810d35b34ecdd70cc0e47b3b6 \\\n    --hash=sha256:32475a42b2ea7b344069dc1e81445cfc00b9d0e3ca837f0523072432332e9f74 \\\n    --hash=sha256:37b2998606d6dadbb5ccda959a33d6a5e853252d921fec1792fc902351bb4e2c \\\n    --hash=sha256:3ac9f578c46f22405ff7f8b1f5848fb753cc4b8377fbec8470a7dc3997ca7566 \\\n    --hash=sha256:3b94a33a241bee9e34b8481cdcaa3d5c2116f575e0226e421bed3f7a6ea71cff \\\n    --hash=sha256:4a9220497ca0cb1fe94e3f334f65b9b5102a0b8147646118f020d8ce1de70105 \\\n    --hash=sha256:4ab9a87f3784eb0e098f84a32670cfe4a79cb6512fd8f42ae3d0709f06405d18 \\\n    --hash=sha256:5127dc1abd809431172bc3fbe8168d6b90556a30bb10acd5ded41c3cfd6f43b6 \\\n    --hash=sha256:5672a86d55416ccd214c778efccf3266b84f87b89063b582167d803246354be4 \\\n    --hash=sha256:580ccf358539153db147e40751a0b41688a5ceb275e6f3e93d91c9467f42b2e3 \\\n    --hash=sha256:58dc9bc9767a1101f4e5e22db1b652161a225874d66f0e5cb8e2c7d1c438b587 \\\n    --hash=sha256:5a90a923338531b7970abb063cfc087eebae6ef8ec8139762007188f6bc69a9f \\\n    --hash=sha256:653cf462db4e8c41995e33d865965e79641ef45369d8a11f54cd30888b7e6ff1 \\\n    --hash=sha256:66227a2c7b575720c1871c8800d3a0122bb8ee94edb43a5685aa9aceb2782d44 \\\n    --hash=sha256:6e5337bf454abddd91bd048ce0dca5134056fc99ca0205258766db35d0a2ea43 \\\n    --hash=sha256:70bf4c43652cc294040dbb62256c83c8718370c8b93dd93d934b9a7bf6c4f53c \\\n    --hash=sha256:711e408732d4e9a0208008e5892c2966b485c783cd2d9a681f3eb147cf36c7ef \\\n    --hash=sha256:76e324da7b5da060287c54f2fabd3db5f76468006c811831f051942bf68c9d44 \\\n    --hash=sha256:789361ed945d8d42850f919342a8665d2dc79e7e44ca1c97cc786966a21f627a \\\n    --hash=sha256:79aec8172b9e3c6d05fd4b219d5de1ac616bd8da934107325a6c0d0e866a21b6 \\\n    --hash=sha256:7efe4853ecd3d6110301665a5178b9856be7e2a9485f49d91aa4d737ad2ae49e \\\n    --hash=sha256:7f22b16b35d5c1df9dfd58843ab2cd25e6bf15191f5a236bed177afade507bfc \\\n    --hash=sha256:83c0efd80b29695058d0fd2fa8a556490dbce9804eac3e281f373bbc99045f6c \\\n    --hash=sha256:859e8eb3507894093d01929e12e267f83b1d5f6221099d3ec976f0c995cb6bd9 \\\n    --hash=sha256:8b9931fd36ee513c26b5bf08c940b0ac875de175341cbdd4fa3be109f0492586 \\\n    --hash=sha256:8bd2a824d08d8977bb2794ea2682f898ad3d8837932e3a74937e93d62ecbb637 \\\n    --hash=sha256:8f2d5ed877f089862f4c7aacf3a542627c1496f972a34d0474ce85ee7d939c27 \\\n    --hash=sha256:8ffc86ae5e3e6a93765d49d1ab47b6075a9c978a2b3b80f0f32628f39caa0c88 \\\n    --hash=sha256:92249669925bc1c54fcd2ec73f70f2c1d6a817928480ee1c65af5f6b81cdf12d \\\n    --hash=sha256:99d9a1eded738299ba8e106c6779ce5c3893cffa0e32e4485d680588adae6db8 \\\n    --hash=sha256:9c63eaef32b7bebac8ebebf4dabebdbc6769a09c127294db6babee38e9f405b9 \\\n    --hash=sha256:9e1fa156ee9454642adb7e7234a383884452532bc9d53d5af2d18d98ada1d79c \\\n    --hash=sha256:a2ecaa3c23e7a7cf86d00eda3390c232f4d533cd9ddea4b04f5d0644faf642c5 \\\n    --hash=sha256:a6c710d657c8d1d2adbbb5c0b0c6bfcec28fd35bd6b5f016395f9ac43e878a15 \\\n    --hash=sha256:a9584de0cd306072635fe4b89742bf26feae858a0683b399ad0c2509011b9dc0 \\\n    --hash=sha256:ab7f43235d71e03b941c1630f4b6e3055d46b6cb8728a17663eaac9d8e83a865 \\\n    --hash=sha256:af102d3372e917cffce49b521e4c32c497515119dc7bd8a75665e90a718bbf08 \\\n    --hash=sha256:b25bd626bde7fb51534190c7e3cb97cee89ee76b76d7585580e22f34f5e3f393 \\\n    --hash=sha256:b2dd880785088ff2ad21ffee205e58a8c1ddabc63612444ae41e5e4b321b39c0 \\\n    --hash=sha256:b426f72cd77da3fec300ed3bc990895e2dd6b49e3bfe6c438592a3ba660e41ca \\\n    --hash=sha256:ba5bdf56969cad2019d4e8ffd3f879b5fdc792624129741d3d83fc832fef8c7d \\\n    --hash=sha256:bf55846c7b7a680eebaf9c3c48d630e1bf51bdf76c68a5f654b8524335b0ad29 \\\n    --hash=sha256:ca1f08b8e43dc3bd0594c992fb1fd2f7ce87f7bf0d44358198d6da8034afdf84 \\\n    --hash=sha256:ca29b6371ebc40e496995c94b988a101b9fbbed48a51190a4461fcb0a68b4a36 \\\n    --hash=sha256:ca8577f6a413abe29b079bc30f907894d7eb07a865c4df69475e868d73e71c7b \\\n    --hash=sha256:cadcc978f82397d515bb2683fc0d50103acff2a180552654bb92d6045dec2c49 \\\n    --hash=sha256:cd646c827b4f85ef4a78e4e58f4f5854fae0caf3db91b59f0d73731448a970c6 \\\n    --hash=sha256:cd73d3e740666d0e639f678adb176fad25c1bcbdae88d8d7b857e1783bb4212d \\\n    --hash=sha256:cde031d8413842a1e7501e9129b8e676e62a657f8ec8166e18a70d94d4682855 \\\n    --hash=sha256:ce0820f4a3a59ddced7fce696d86a096d5cc48d32a4183483a17671a61edfddc \\\n    --hash=sha256:d20be8b7f606df096e08b0b1b4a3c6f0515e8dac296881fe7461dfa0fb5ec817 \\\n    --hash=sha256:d21974d246ed0181558087cd9f76e84e8321091ebfb3a93d4c341479a736f099 \\\n    --hash=sha256:d33f94615fcaf872f7fd8cd98ac3b429e435c77619777e8a449d9d27e01134d1 \\\n    --hash=sha256:d35c864c2dff13dfd79fb070fc4fc6235d7b9b359efe340e1261deb21b9fcb66 \\\n    --hash=sha256:d5c826a221851a8dc028eb6d7d6429ba03184fa3c7e83ae01cd6d3bd1d4bd17d \\\n    --hash=sha256:e41e75344acef3fc59ba4765df29f107f309ca9e8eace5baacabd9217e52a5ee \\\n    --hash=sha256:e52bf98c7e727dd44f7c4acb980cb988448faeafed8433c867888268899b298b \\\n    --hash=sha256:e6ec2be506e7d6f9527dae9ff4b7f54e68ea44a0ef6b098256ddf895218a2f8f \\\n    --hash=sha256:e725edd0929fa79f8349ab4ec7f81c714df51dc4e991539a578e5018fa4a7152 \\\n    --hash=sha256:eaa58399c01db555346647a907b4ef6d4f584b123943be6ed5588c3f2359c9f4 \\\n    --hash=sha256:eb21aaa9a200d0a80dacc7a81038d2e476ffe473ffdd9c91eb745d623561de05 \\\n    --hash=sha256:ecff0dc14f409599bbcafa7e470c00b80f17abc14d1405d38ab02e4b42e55b57 \\\n    --hash=sha256:f557c55bc2b7676e74d39d19bcb8775ca295c7a028246175d6a8b431e70835e5 \\\n    --hash=sha256:f7200b8f7619d36aa51c803fd52020a2dfbea36ffec1b5e22cab11fd34d95a6d \\\n    --hash=sha256:f9d471356dc16f84ed48768b8ee79f29514295c7295cb41e1133ec0b2b8d637d \\\n    --hash=sha256:fc5adda618205bd4678b146612ce44c3cbfdee9697951f2c0ffdef1f26d72b63 \\\n    --hash=sha256:fc9043259ee430ecd71d178fccabd8c332a3bf1e81e50cae43cc2b28d19e4cb7 \\\n    --hash=sha256:ffd9fee7d0775ebaba131f7ca2e2d83839a62ad65e8e02fe2bd8fc975cedeb9e\n    # via\n    #   -c python/requirements_compiled_rayllm_test_py311_cu121.txt\n    #   openai\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with SHA256 Hashes\nDESCRIPTION: A requirements.txt style file containing package dependencies with their versions and SHA256 hashes for verification. Each package entry includes multiple hash options for different distributions and indicates which other requirements files or packages depend on it.\nSOURCE: https://github.com/ray-project/ray/blob/master/release/ray_release/byod/requirements_ml_byod_3.9.txt#2025-04-12_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n--hash=sha256:ae5f4161f18c61806f411a13b0310bea87f987c7d2ecdbdaad0e94eb2e404238 \\\n--hash=sha256:aed38f6e4fb3f5d6bf81bfa990a07806be9d83cf7bacef998ab1a9bd660a581f \\\n--hash=sha256:b01b88d45a6fcb69667cd6d2f7a9aeb4bf53760d7fc536bf679ec94fe9f3ff3d \\\n[...additional hashes omitted for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding\nDESCRIPTION: Establishes port forwarding for the Ray Dashboard\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.ipynb#2025-04-12_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nkubectl port-forward service/raycluster-kuberay-head-svc 8265:8265 > /dev/null &\n```\n\n----------------------------------------\n\nTITLE: Using Ray Object References with Initial Tune Experiment\nDESCRIPTION: Example demonstrating how to use Ray object references with Tune experiments, specifically working with pretrained models that are stored in the Ray object store and passed to trials.\nSOURCE: https://github.com/ray-project/ray/blob/master/doc/source/tune/tutorials/tune-fault-tolerance.rst#2025-04-12_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Set up some large pretrained models\nmodel_1 = create_large_pretrained_model(\"resnet50\")\nmodel_2 = create_large_pretrained_model(\"densenet121\")\n\n# Put the models in Ray's object store to avoid copying\nmodel_1_ref = ray.put(model_1)\nmodel_2_ref = ray.put(model_2)\n\n# Our param space references these objects\nparam_space = {\n    \"model\": tune.grid_search([model_1_ref, model_2_ref]),\n}\n\n# Original Tuner setup\ntuner = tune.Tuner(\n    trainable,\n    param_space=param_space,\n    run_config=RunConfig(\n        name=\"tune_fault_tolerance_guide\",\n        storage_path=\"~/ray_results\",\n    ),\n)\n\ntuner.fit()\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Requirements with Hash Verification\nDESCRIPTION: This code provides package specifications with SHA256 hash verification to ensure package integrity during installation. It includes version pinning and comments indicating which packages depend on these requirements.\nSOURCE: https://github.com/ray-project/ray/blob/master/python/requirements_compiled_rayllm_test_py311_cu124.txt#2025-04-12_snippet_44\n\nLANGUAGE: pip\nCODE:\n```\n--hash=sha256:f056bf21105c2515c32372bbc057f43eb02aae2fda61052e2f7622c801f0b4e2 \\\n    --hash=sha256:f1ac758ef6aebfc8943560194e9fd0fa18bcb34d89fd8bd2af18183afd8da3a2 \\\n    --hash=sha256:f2a19f302cd1ce5dd01a9099aaa19cae6173306d1302a43b627f62e21cf18ac0 \\\n    --hash=sha256:f654882311409afb1d780b940234208a252322c24a93b442ca714d119e68086c \\\n    --hash=sha256:f65557897fc977a44ab205ea871b690adaef6b9da6afda4790a2484b04293a5f \\\n    --hash=sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6 \\\n    --hash=sha256:fdabbfc59f2c6edba2a6622c647b716e34e8e3867e0ab975412c5c2f79b82da2 \\\n    --hash=sha256:fdd6028445d2460f33136c55eeb1f601ab06d74cb3347132e1c24250187500d9 \\\n    --hash=sha256:ff590880083d60acc0433f9c3f713c51f7ac6ebb9adf889c79a261ecf541aa91\n    # via\n    #   tiktoken\n    #   transformers\n```"
  }
]
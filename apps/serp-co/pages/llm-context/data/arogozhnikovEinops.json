[
  {
    "owner": "arogozhnikov",
    "repo": "einops",
    "content": "TITLE: Implementing Vision Transformer with Class Token Using einops\nDESCRIPTION: Implements a Vision Transformer (ViT) with a class token using einops.pack and unpack. Provides a clean way to combine class tokens with patch tokens for transformer processing.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef vit_einops(class_tokens, patch_tokens):\n    input_packed, ps = pack([class_tokens, patch_tokens], 'b * c')\n    output_packed = transformer_mock(input_packed)\n    return unpack(output_packed, ps, 'b * c_out')\n\nclass_token_emb, patch_tokens_emb = vit_einops(class_tokens, patch_tokens)\n\nclass_token_emb.shape, patch_tokens_emb.shape\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Sequential Model with Einops Reduce Layer\nDESCRIPTION: Creates a neural network using PyTorch's Sequential container with convolutional layers, pooling, and an Einops Reduce layer for combined pooling and flattening. The model processes image data through multiple layers including Conv2D, MaxPool2D, and Linear layers with a ReLU activation.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\nfrom einops.layers.torch import Reduce\n\nmodel = Sequential(\n    Conv2d(3, 6, kernel_size=5),\n    MaxPool2d(kernel_size=2),\n    Conv2d(6, 16, kernel_size=5),\n    # combined pooling and flattening in a single step\n    Reduce('b c (h 2) (w 2) -> b (c h w)', 'max'), \n    Linear(16*5*5, 120), \n    ReLU(),\n    Linear(120, 10), \n    # In flax, the {'axis': value} syntax for specifying values for axes is mandatory:\n    # Rearrange('(b1 b2) d -> b1 b2 d', {'b1': 12}), \n)\n```\n\n----------------------------------------\n\nTITLE: Using core einops operations in Python\nDESCRIPTION: Demonstrates the usage of rearrange, reduce, and repeat operations from einops for tensor manipulation.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import rearrange, reduce, repeat\n# rearrange elements according to the pattern\noutput_tensor = rearrange(input_tensor, 't b c -> b c t')\n# combine rearrangement and reduction\noutput_tensor = reduce(input_tensor, 'b c (h h2) (w w2) -> b h w c', 'mean', h2=2, w2=2)\n# copy along a new axis\noutput_tensor = repeat(input_tensor, 'h w -> h w c', c=3)\n```\n\n----------------------------------------\n\nTITLE: Optimized ResMLP Implementation\nDESCRIPTION: Final optimization of ResMLP using EinMix with improved code organization and clarity.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef init(layer: Mix, scale=1.):\n    layer.weight.data[:] = scale\n    if layer.bias is not None:\n        layer.bias.data[:] = 0\n    return layer\n\nclass ResMLP_Blocks3(nn.Module):\n    def __init__(self, nb_patches, dim, layerscale_init):\n        super().__init__()\n        self.branch_patches = nn.Sequential(\n            init(Mix('b t c -> b t c', weight_shape='c', c=dim), scale=layerscale_init),\n            Mix('b t c -> b t0 c', weight_shape='t t0', bias_shape='t0', t=nb_patches, t0=nb_patches),\n            init(Mix('b t c -> b t c', weight_shape='c', bias_shape='c', c=dim)),\n        )\n\n        self.branch_channels = nn.Sequential(\n            init(Mix('b t c -> b t c', weight_shape='c', c=dim), scale=layerscale_init),\n            nn.Linear(dim, 4 * dim),\n            nn.GELU(),\n            nn.Linear(4 * dim, dim),\n            init(Mix('b t c -> b t c', weight_shape='c', bias_shape='c', c=dim)),\n        )\n\n    def forward(self, x):\n        x = x + self.branch_patches(x)\n        x = x + self.branch_channels(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Reimplemented Vision Permutator with EinMix in Python\nDESCRIPTION: A simplified implementation of the WeightedPermuteMLP module using EinMix. This code replaces each of the complex permute and reshape operations with a single EinMix layer, making the code more readable and less error-prone.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass WeightedPermuteMLP_new(nn.Module):\n    def __init__(self, H, W, C, seg_len):\n        super().__init__()\n        assert C % seg_len == 0, f\"can't divide {C} into segments of length {seg_len}\"\n        self.mlp_c = Mix('b h w c -> b h w c0', weight_shape='c c0', bias_shape='c0', c=C, c0=C)\n        self.mlp_h = Mix('b h w (n c) -> b h0 w (n c0)', weight_shape='h c h0 c0', bias_shape='h0 c0',\n                            h=H, h0=H, c=seg_len, c0=seg_len)\n        self.mlp_w = Mix('b h w (n c) -> b h w0 (n c0)', weight_shape='w c w0 c0', bias_shape='w0 c0',\n                            w=W, w0=W, c=seg_len, c0=seg_len)\n        self.proj = nn.Linear(C, C)\n\n    def forward(self, x):\n        x = self.mlp_c(x) + self.mlp_h(x) + self.mlp_w(x)\n        return self.proj(x)\n```\n\n----------------------------------------\n\nTITLE: Basic Dimension Transposition with Einops Rearrange in Python\nDESCRIPTION: Using einops rearrange to transpose dimensions in a tensor, swapping height and width while preserving the color channel order.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# rearrange, as the name suggests, rearranges elements\n# below we swapped height and width.\n# In other words, transposed first two axes (dimensions)\nrearrange(ims[0], \"h w c -> w h c\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# we could use more verbose names for axes, and result is the same:\nrearrange(ims[0], \"height width color -> width height color\")\n# when you operate on same set of axes many times,\n# you usually come up with short names.\n# That's what we do throughout tutorial - we'll use b (for batch), h, w, and c\n```\n\n----------------------------------------\n\nTITLE: Implementing Vision Transformer with Class Token Using Vanilla Code\nDESCRIPTION: Implements the same Vision Transformer functionality but with traditional NumPy operations for comparison. Shows the manual reshaping, concatenating, and splitting required without einops.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef vit_vanilla(class_tokens, patch_tokens):\n    b, h, w, c = patch_tokens.shape\n    class_tokens_b1c = class_tokens[:, np.newaxis, :]\n    patch_tokens_btc = np.reshape(patch_tokens, [b, -1, c])\n    input_packed = np.concatenate([class_tokens_b1c, patch_tokens_btc], axis=1)\n    output_packed = transformer_mock(input_packed)\n    class_token_emb = np.squeeze(output_packed[:, :1, :], 1)\n    patch_tokens_emb = np.reshape(output_packed[:, 1:, :], [b, h, w, -1])\n    return class_token_emb, patch_tokens_emb\n\nclass_token_emb2, patch_tokens_emb2 = vit_vanilla(class_tokens, patch_tokens)\nassert np.allclose(class_token_emb, class_token_emb2)\nassert np.allclose(patch_tokens_emb, patch_tokens_emb2)\n```\n\n----------------------------------------\n\nTITLE: Reimplemented MLPMixer Patch Embeddings with EinMix in Python\nDESCRIPTION: A simplified implementation of MLPMixer's patch embeddings using EinMix. This implementation handles images of different dimensions in a single operation, splitting the image into patches and reorganizing them into a sequence of vectors.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef patcher(patch_size=16, in_channels=3, num_features=128):\n    return Mix('b c_in (h hp) (w wp) -> b (h w) c', weight_shape='c_in hp wp c', bias_shape='c',\n                  c=num_features, hp=patch_size, wp=patch_size, c_in=in_channels)\n```\n\n----------------------------------------\n\nTITLE: Composing Dimensions with Einops Rearrange in Python\nDESCRIPTION: Demonstrating how to merge dimensions together using einops rearrange. This allows flattening multiple dimensions into a single dimension based on pattern expressions.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# einops allows seamlessly composing batch and height to a new height dimension\n# We just rendered all images by collapsing to 3d tensor!\nrearrange(ims, \"b h w c -> (b h) w c\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# or compose a new dimension of batch and width\nrearrange(ims, \"b h w c -> h (b w) c\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# resulting dimensions are computed very simply\n# length of newly composed axis is a product of components\n# [6, 96, 96, 3] -> [96, (6 * 96), 3]\nrearrange(ims, \"b h w c -> h (b w) c\").shape\n```\n\nLANGUAGE: python\nCODE:\n```\n# we can compose more than two axes.\n# let's flatten 4d array into 1d, resulting array has as many elements as the original\nrearrange(ims, \"b h w c -> (b h w c)\").shape\n```\n\n----------------------------------------\n\nTITLE: Advanced Einops Pattern Examples for Image Manipulation in Python\nDESCRIPTION: A collection of complex einops pattern examples demonstrating creative tensor manipulations for images, including interweaving, composing, splitting, and other advanced transformations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# interweaving pixels of different pictures\n# all letters are observable\nrearrange(ims, \"(b1 b2) h w c -> (h b1) (w b2) c \", b1=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# interweaving along vertical for couples of images\nrearrange(ims, \"(b1 b2) h w c -> (h b1) (b2 w) c\", b1=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# interweaving lines for couples of images\n# exercise: achieve the same result without einops in your favourite framework\nreduce(ims, \"(b1 b2) h w c -> h (b2 w) c\", \"max\", b1=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# color can be also composed into dimension\n# ... while image is downsampled\nreduce(ims, \"b (h 2) (w 2) c -> (c h) (b w)\", \"mean\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# disproportionate resize\nreduce(ims, \"b (h 4) (w 3) c -> (h) (b w)\", \"mean\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# spilt each image in two halves, compute mean of the two\nreduce(ims, \"b (h1 h2) w c -> h2 (b w)\", \"mean\", h1=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# split in small patches and transpose each patch\nrearrange(ims, \"b (h1 h2) (w1 w2) c -> (h1 w2) (b w1 h2) c\", h2=8, w2=8)\n```\n\nLANGUAGE: python\nCODE:\n```\n# stop me someone!\nrearrange(ims, \"b (h1 h2 h3) (w1 w2 w3) c -> (h1 w2 h3) (b w1 h2 w3) c\", h2=2, w2=2, w3=2, h3=2)\n```\n\nLANGUAGE: python\nCODE:\n```\nrearrange(ims, \"(b1 b2) (h1 h2) (w1 w2) c -> (h1 b1 h2) (w1 b2 w2) c\", h1=3, w1=3, b2=3)\n```\n\nLANGUAGE: python\nCODE:\n```\n# patterns can be arbitrarily complicated\nreduce(ims, \"(b1 b2) (h1 h2 h3) (w1 w2 w3) c -> (h1 w1 h3) (b1 w2 h2 w3 b2) c\", \"mean\", h2=2, w1=2, w3=2, h3=2, b2=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# subtract background in each image individually and normalize\n# pay attention to () - this is composition of 0 axis, a dummy axis with 1 element.\nim2 = reduce(ims, \"b h w c -> b () () c\", \"max\") - ims\nim2 /= reduce(im2, \"b h w c -> b () () c\", \"max\")\nrearrange(im2, \"b h w c -> h (b w) c\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# pixelate: first downscale by averaging, then upscale back using the same pattern\naveraged = reduce(ims, \"b (h h2) (w w2) c -> b h w c\", \"mean\", h2=6, w2=8)\nrepeat(averaged, \"b h w c -> (h h2) (b w w2) c\", h2=6, w2=8)\n```\n\nLANGUAGE: python\nCODE:\n```\nrearrange(ims, \"b h w c -> w (b h) c\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# let's bring color dimension as part of horizontal axis\n```\n\n----------------------------------------\n\nTITLE: Decomposing Dimensions with Einops Rearrange in Python\nDESCRIPTION: Splitting dimensions into multiple new dimensions using einops rearrange. This allows restructuring a tensor by breaking apart dimensions based on specified factors.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# decomposition is the inverse process - represent an axis as a combination of new axes\n# several decompositions possible, so b1=2 is to decompose 6 to b1=2 and b2=3\nrearrange(ims, \"(b1 b2) h w c -> b1 b2 h w c \", b1=2).shape\n```\n\nLANGUAGE: python\nCODE:\n```\n# finally, combine composition and decomposition:\nrearrange(ims, \"(b1 b2) h w c -> (b1 h) (b2 w) c \", b1=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# slightly different composition: b1 is merged with width, b2 with height\n# ... so letters are ordered by w then by h\nrearrange(ims, \"(b1 b2) h w c -> (b2 h) (b1 w) c \", b1=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# move part of width dimension to height.\n# we should call this width-to-height as image width shrunk by 2 and height doubled.\n# but all pixels are the same!\n# Can you write reverse operation (height-to-width)?\nrearrange(ims, \"b h (w w2) c -> (h w2) (b w) c\", w2=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Universal Predict with Auto-Batching\nDESCRIPTION: Creates a universal prediction function that handles both single images and batches of images using pack and unpack. Shows how to implement auto-batching for ML models.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import reduce\ndef image_classifier(images_bhwc):\n    # mock for image classifier\n    predictions = reduce(images_bhwc, 'b h w c -> b c', 'mean', h=100, w=200, c=3)\n    return predictions\n\n\ndef universal_predict(x):\n    x_packed, ps = pack([x], '* h w c')\n    predictions_packed = image_classifier(x_packed)\n    [predictions] = unpack(predictions_packed, ps, '* cls')\n    return predictions\n```\n\n----------------------------------------\n\nTITLE: Cross-Framework Tensor Rearrangement\nDESCRIPTION: Demonstration of identical tensor rearrangement syntax across multiple deep learning frameworks including PyTorch, TensorFlow, Gluon, CuPy, JAX, and Paddle.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# pytorch:\nrearrange(ims, 'b h w c -> w (b h) c')\n# tensorflow:\nrearrange(ims, 'b h w c -> w (b h) c')\n# gluon:\nrearrange(ims, 'b h w c -> w (b h) c')\n# cupy:\nrearrange(ims, 'b h w c -> w (b h) c')\n# jax:\nrearrange(ims, 'b h w c -> w (b h) c')\n# paddle:\nrearrange(ims, 'b h w c -> w (b h) c')\n```\n\n----------------------------------------\n\nTITLE: Advanced EinMix Usage Example: Group-wise Mixing in Python\nDESCRIPTION: An example of using EinMix for group-wise mixing patterns in neural networks. This demonstrates how to implement channel groups for mixing, providing an architecture that would be much harder to implement without EinMix.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n'b hw (group c) -> b hw_out (group c)', weight_shape='group hw hw_out'\n```\n\n----------------------------------------\n\nTITLE: Parsing Tensor Shapes with einops.parse_shape in Python\nDESCRIPTION: The parse_shape function takes a tensor and a pattern string as input. It returns a dictionary where keys are dimension names from the pattern, and values are corresponding dimensions of the tensor. This function is useful for extracting specific dimensions from complex tensor shapes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/api/parse_shape.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import parse_shape\n\nx = torch.randn(2, 3, 5, 7)\nparse_shape(x, 'batch height width channels')\n# returns {'batch': 2, 'height': 3, 'width': 5, 'channels': 7}\n\nparse_shape(x, 'batch _ _ channels')\n# returns {'batch': 2, 'channels': 7}\n```\n\n----------------------------------------\n\nTITLE: Stacking RGB and Depth Images Using einops.pack\nDESCRIPTION: Example of packing RGB and depth images together using einops.pack. Creates random RGB and depth images with shapes (h,w,3) and (h,w) respectively, then stacks them with proper axis alignment.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import pack, unpack\n\nh, w = 100, 200\n# image_rgb is 3-dimensional (h, w, 3) and depth is 2-dimensional (h, w)\nimage_rgb = np.random.random([h, w, 3])\nimage_depth = np.random.random([h, w])\n# but we can stack them\nimage_rgbd, ps = pack([image_rgb, image_depth], 'h w *')\n```\n\n----------------------------------------\n\nTITLE: Creating Sequential Neural Network with Einops and PyTorch\nDESCRIPTION: Demonstrates building a neural network using PyTorch's Sequential container with Einops' Reduce layer for combined pooling and flattening operations. The model includes convolutional layers, max pooling, linear layers, and ReLU activation.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\nfrom einops.layers.torch import Reduce\n\nmodel = Sequential(\n    Conv2d(3, 6, kernel_size=5),\n    MaxPool2d(kernel_size=2),\n    Conv2d(6, 16, kernel_size=5),\n    # combined pooling and flattening in a single step\n    Reduce('b c (h 2) (w 2) -> b (c h w)', 'max'), \n    Linear(16*5*5, 120), \n    ReLU(),\n    Linear(120, 10), \n    # In flax, the {'axis': value} syntax for specifying values for axes is mandatory:\n    # Rearrange('(b1 b2) d -> b1 b2 d', {'b1': 12}), \n)\n```\n\n----------------------------------------\n\nTITLE: Using einops.repeat Function with Various Backends\nDESCRIPTION: The einops.repeat function allows repeating tensors along existing or new dimensions according to specified patterns. It supports various tensor frameworks like numpy, pytorch, tensorflow, jax, and others through backend-specific implementations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/api/repeat.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef repeat(tensor, pattern, **axes_lengths):\n    \"\"\"repeats tensor according to the provided pattern.\n\n    Args:\n        tensor: input tensor\n        pattern: string, the compositional pattern for repeating dimensions\n        **axes_lengths: parameters for the pattern, each axis name used within pattern should be\n            provided as a keyword with corresponding length.\n\n    Returns:\n        Result of repeating tensor according to the pattern.\n\n    Examples::  \n        >>> x = numpy.ones([1, 2, 1])\n        >>> repeat(x, 'h w c -> h w c t', t=3)\n        # result has shape [1, 2, 1, 3]\n\n        >>> repeat(x, 'h w c -> (repeat h)', repeat=2)\n        # result has shape [2, 2, 1]\n\n        >>> repeat(x, 'h w c -> h w (repeat c)', repeat=2)\n        # result has shape [1, 2, 2]\n\n        >>> repeat(x, 'h w c -> (h h2) w c', h2=2)\n        # result has shape [2, 2, 1]\n    \"\"\"\n    return _apply_recipe(_get_backend(tensor), _prepare_recipe(pattern, axes_lengths), tensor)\n```\n\n----------------------------------------\n\nTITLE: Rearranging Tensor Dimensions with Einops across Frameworks in Python\nDESCRIPTION: This snippet demonstrates how to use the rearrange function from einops to reorder tensor dimensions. It shows that the same einops operation can be used across different frameworks like PyTorch, TensorFlow, Gluon, CuPy, JAX, and Paddle without changes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# pytorch:\nrearrange(ims, 'b h w c -> w (b h) c')\n# tensorflow:\nrearrange(ims, 'b h w c -> w (b h) c')\n# gluon:\nrearrange(ims, 'b h w c -> w (b h) c')\n# cupy:\nrearrange(ims, 'b h w c -> w (b h) c')\n# jax:\nrearrange(ims, 'b h w c -> w (b h) c')\n# paddle:\nrearrange(ims, 'b h w c -> w (b h) c')\n```\n\n----------------------------------------\n\nTITLE: Basic Tensor Transpose with Einops\nDESCRIPTION: Demonstrates the difference between traditional transpose and einops rearrange syntax for better readability.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ny = x.transpose(0, 2, 3, 1)\n```\n\nLANGUAGE: python\nCODE:\n```\ny = rearrange(x, 'b c h w -> b h w c')\n```\n\n----------------------------------------\n\nTITLE: Comparing Traditional Transposition with Einops Rearrange in Python\nDESCRIPTION: A comparison between traditional tensor transposition code and einops' more readable pattern-based approach using the rearrange function.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ny = x.transpose(0, 2, 3, 1)\n```\n\nLANGUAGE: python\nCODE:\n```\ny = rearrange(x, 'b c h w -> b h w c')\n```\n\n----------------------------------------\n\nTITLE: Using pack and unpack operations in einops\nDESCRIPTION: Shows how to use pack and unpack functions for reversibly combining multiple tensors into one.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import pack, unpack\n# pack and unpack allow reversibly 'packing' multiple tensors into one.\n# Packed tensors may be of different dimensionality:\npacked,  ps = pack([class_token_bc, image_tokens_bhwc, text_tokens_btc], 'b * c')\nclass_emb_bc, image_emb_bhwc, text_emb_btc = unpack(transformer(packed), ps, 'b * c')\n```\n\n----------------------------------------\n\nTITLE: Universal Image Classifier Implementation\nDESCRIPTION: Implementation of a universal predict function that handles both single images and batches using pack/unpack.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/4-pack-and-unpack.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import reduce\ndef image_classifier(images_bhwc):\n    # mock for image classifier\n    predictions = reduce(images_bhwc, 'b h w c -> b c', 'mean', h=100, w=200, c=3)\n    return predictions\n\n\ndef universal_predict(x):\n    x_packed, ps = pack([x], '* h w c')\n    predictions_packed = image_classifier(x_packed)\n    [predictions] = unpack(predictions_packed, ps, '* cls')\n    return predictions\n```\n\n----------------------------------------\n\nTITLE: Axis Decomposition Examples\nDESCRIPTION: Shows how to decompose axes into multiple dimensions with specific sizes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nrearrange(ims, \"(b1 b2) h w c -> b1 b2 h w c \", b1=2).shape\n```\n\n----------------------------------------\n\nTITLE: Basic Axis Rearrangement\nDESCRIPTION: Shows how to swap height and width dimensions using einops rearrange.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrearrange(ims[0], \"h w c -> w h c\")\n```\n\n----------------------------------------\n\nTITLE: Unpacking Multiple Outputs for Reinforcement Learning\nDESCRIPTION: Example of using einops.unpack to extract multiple prediction outputs for reinforcement learning, including action logits, reward expectations, Q-values, and entropy predictions.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\naction_logits, reward_expectation, q_values, expected_entropy_after_action = \\\n    unpack(predictions_btc, [[n_actions], [], [n_actions], [n_actions]], 'b step *')\n```\n\n----------------------------------------\n\nTITLE: Combining Reduce and Repeat Operations in Einops with Python\nDESCRIPTION: Demonstrating how reduce and repeat operations can be used together, showing they are conceptually opposite operations by first increasing elements then reducing back.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrepeated = repeat(ims, \"b h w c -> b h new_axis w c\", new_axis=2)\nreduced = reduce(repeated, \"b h new_axis w c -> b h w c\", \"min\")\nassert numpy.array_equal(ims, reduced)\n```\n\n----------------------------------------\n\nTITLE: Reducing Tensor Dimensions with Einops in Python\nDESCRIPTION: This snippet shows how to use the reduce function from einops to downsample a tensor along specific dimensions. It reduces the horizontal axis by a factor of 3 and the vertical axis by a factor of 3 using mean reduction.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nreduce(ims, \"b (h h2) (w w2) c -> (h w2) (b w c)\", \"mean\", h2=3, w2=3)\n```\n\n----------------------------------------\n\nTITLE: Different Ways of Manually Unpacking Tensors\nDESCRIPTION: Shows various ways to unpack a tensor by manually specifying how to split dimensions. Demonstrates flexible splitting options by controlling the output shapes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# simple unpack by splitting the axis. Results are (h, w, 3) and (h, w, 1)\nrgb, depth = unpack(image_rgbd, [[3], [1]], 'h w *')\n# different split, both outputs have shape (h, w, 2)\nrg, bd = unpack(image_rgbd, [[2], [2]], 'h w *')\n# unpack to 4 tensors of shape (h, w). More like 'unstack over last axis'\n[r, g, b, d] = unpack(image_rgbd, [[], [], [], []], 'h w *')\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Vision Transformer Example\nDESCRIPTION: Sets up random input data for a Vision Transformer (ViT) example, including patch tokens and class tokens with appropriate shapes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# preparations\nbatch, height, width, c = 6, 16, 16, 256\npatch_tokens = np.random.random([batch, height, width, c])\nclass_tokens = np.zeros([batch, c])\n```\n\n----------------------------------------\n\nTITLE: Repeating Elements with Einops Repeat in Python\nDESCRIPTION: Using einops repeat to duplicate elements along existing or new dimensions, providing functionality similar to numpy.repeat and numpy.tile but with more intuitive syntax.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# repeat along a new axis. New axis can be placed anywhere\nrepeat(ims[0], \"h w c -> h new_axis w c\", new_axis=5).shape\n```\n\nLANGUAGE: python\nCODE:\n```\n# shortcut\nrepeat(ims[0], \"h w c -> h 5 w c\").shape\n```\n\nLANGUAGE: python\nCODE:\n```\n# repeat along w (existing axis)\nrepeat(ims[0], \"h w c -> h (repeat w) c\", repeat=3)\n```\n\nLANGUAGE: python\nCODE:\n```\n# repeat along two existing axes\nrepeat(ims[0], \"h w c -> (2 h) (2 w) c\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# order of axes matters as usual - you can repeat each element (pixel) 3 times\n# by changing order in parenthesis\nrepeat(ims[0], \"h w c -> h (w repeat) c\", repeat=3)\n```\n\n----------------------------------------\n\nTITLE: Stacking RGB and Depth Images\nDESCRIPTION: Demonstrates how to stack RGB and depth images using einops.pack function.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/4-pack-and-unpack.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import pack, unpack\n\nh, w = 100, 200\n# image_rgb is 3-dimensional (h, w, 3) and depth is 2-dimensional (h, w)\nimage_rgb = np.random.random([h, w, 3])\nimage_depth = np.random.random([h, w])\n# but we can stack them\nimage_rgbd, ps = pack([image_rgb, image_depth], 'h w *')\n```\n\n----------------------------------------\n\nTITLE: Original ResMLP implementation with PyTorch\nDESCRIPTION: Reference implementation of ResMLP from the original paper, showing the Affine transformation, MLP block, and ResMLP_Blocks class using standard PyTorch modules.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# No norm layer\nclass Affine(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.ones(dim))\n        self.beta = nn.Parameter(torch.zeros(dim))\n\n    def forward(self, x):\n        return self.alpha * x + self.beta\n\n\nclass Mlp(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, 4 * dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(4 * dim, dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        return x\n\nclass ResMLP_Blocks(nn.Module):\n    def __init__(self, nb_patches, dim, layerscale_init):\n        super().__init__()\n        self.affine_1 = Affine(dim)\n        self.affine_2 = Affine(dim)\n        self.linear_patches = nn.Linear(nb_patches, nb_patches) #Linear layer on patches\n        self.mlp_channels = Mlp(dim) #MLP on channels\n        self.layerscale_1 = nn.Parameter(layerscale_init * torch.ones((dim))) # LayerScale\n        self.layerscale_2 = nn.Parameter(layerscale_init * torch.ones((dim))) # parameters\n\n    def forward(self, x):\n        res_1 = self.linear_patches(self.affine_1(x).transpose(1,2)).transpose(1,2)\n        x = x + self.layerscale_1 * res_1\n        res_2 = self.mlp_channels(self.affine_2(x))\n        x = x + self.layerscale_2 * res_2\n        return x\n```\n\n----------------------------------------\n\nTITLE: Advanced EinMix Usage Example: Local Patch Mixing in Python\nDESCRIPTION: An example of how to implement local mixing within patches using EinMix. This pattern splits a tensor into patches and performs mixing within each patch and channel, enabling complex local operations in a single line of code.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n'b c (h hI) (w wI) -> b c (h hO) (w wO)', weight_shape='c hI wI hO wO'\n```\n\n----------------------------------------\n\nTITLE: Using einops layers in a PyTorch model\nDESCRIPTION: Demonstrates how to use einops layers within a PyTorch Sequential model, replacing the need for manual flattening.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\nfrom einops.layers.torch import Rearrange\n\nmodel = Sequential(\n    ...,\n    Conv2d(6, 16, kernel_size=5),\n    MaxPool2d(kernel_size=2),\n    # flattening without need to write forward\n    Rearrange('b c h w -> b (c h w)'),\n    Linear(16*5*5, 120),\n    ReLU(),\n    Linear(120, 10),\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced EinMix Usage Example: Subgrid Mixing in Python\nDESCRIPTION: An example of implementing subgrid-based mixing with EinMix. This pattern splits an image into subgrids and densely connects tokens within each subgrid, providing a computationally efficient alternative to all-to-all connections.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n'b c (hI h) (wI w) -> b c (hO h) (wO w)', weight_shape='c hI wI hO wO'\n```\n\n----------------------------------------\n\nTITLE: Using Einops Reduce for Dimension Reduction in Python\nDESCRIPTION: Introducing einops reduce operation which collapses dimensions using reduction operations like mean, min, max, sum, and prod. This provides explicit syntax for dimension reduction.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nx.mean(-1)\n```\n\nLANGUAGE: python\nCODE:\n```\nreduce(x, 'b h w c -> b h w', 'mean')\n```\n\nLANGUAGE: python\nCODE:\n```\n# average over batch\nreduce(ims, \"b h w c -> h w c\", \"mean\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# the previous is identical to familiar:\nims.mean(axis=0)\n# but is so much more readable\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example of reducing of several axes\n# besides mean, there are also min, max, sum, prod\nreduce(ims, \"b h w c -> h w\", \"min\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# this is mean-pooling with 2x2 kernel\n# image is split into 2x2 patches, each patch is averaged\nreduce(ims, \"b (h h2) (w w2) c -> h (b w) c\", \"mean\", h2=2, w2=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# max-pooling is similar\n# result is not as smooth as for mean-pooling\nreduce(ims, \"b (h h2) (w w2) c -> h (b w) c\", \"max\", h2=2, w2=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# yet another example. Can you compute result shape?\nreduce(ims, \"(b1 b2) h w c -> (b2 h) (b1 w)\", \"mean\", b1=2)\n```\n\n----------------------------------------\n\nTITLE: Common Deep Learning Operations\nDESCRIPTION: Implementation of common neural network operations using einops including flattening and space-to-depth transformations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Flattening\ny = rearrange(x, \"b c h w -> b (c h w)\")\n\n# space-to-depth\ny = rearrange(x, \"b c (h h1) (w w1) -> b (h1 w1 c) h w\", h1=2, w1=2)\n\n# depth-to-space\ny = rearrange(x, \"b (h1 w1 c) h w -> b c (h h1) (w w1)\", h1=2, w1=2)\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying Image Data\nDESCRIPTION: Loads test images from a numpy file and displays their shape and individual images.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nims = numpy.load(\"./resources/test_images.npy\", allow_pickle=False)\nprint(ims.shape, ims.dtype)\n```\n\n----------------------------------------\n\nTITLE: Packing Different Modalities for Transformer Input\nDESCRIPTION: Shows how to combine tokens from different modalities (text, image, task, static) into a single tensor for processing by a transformer model.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nall_inputs = [text_tokens_btc, image_bhwc, task_token_bc, static_tokens_bnc]\ninputs_packed, ps = pack(all_inputs, 'b * c')\n```\n\n----------------------------------------\n\nTITLE: Implementing TokenMixer with EinMix in Python\nDESCRIPTION: A concise implementation of MLPMixer's TokenMixer using EinMix to mix dimensions. This implementation reduces code complexity by allowing mixing on any axis, not just the last one, and follows a clear sequential structure without residual connections.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef TokenMixer(num_features: int, n_patches: int, expansion_factor: int, dropout: float):\n    n_hidden = n_patches * expansion_factor\n    return nn.Sequential(\n        nn.LayerNorm(num_features),\n        Mix('b hw c -> b hid c', weight_shape='hw hid', bias_shape='hid', hw=n_patches, hidden=n_hidden),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        Mix('b hid c -> b hw c', weight_shape='hid hw', bias_shape='hw',  hw=n_patches, hidden=n_hidden),\n        nn.Dropout(dropout),\n    )\n```\n\n----------------------------------------\n\nTITLE: Backpropagation Example\nDESCRIPTION: Shows how to perform backpropagation through Einops operations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ny0 = x\ny1 = reduce(y0, \"b c h w -> b c\", \"max\")\ny2 = rearrange(y1, \"b c -> c b\")\ny3 = reduce(y2, \"c b -> \", \"sum\")\n\nif flavour == \"tensorflow\":\n    print(reduce(tape.gradient(y3, x), \"b c h w -> \", \"sum\"))\nelse:\n    y3.backward()\n    print(reduce(x.grad, \"b c h w -> \", \"sum\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing TokenMixer with EinMix in Python\nDESCRIPTION: A simplified implementation of TokenMixer from MLPMixer using EinMix. This implementation allows mixing on any axis, not just the last one, and uses a sequential structure for better readability.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef TokenMixer(num_features: int, n_patches: int, expansion_factor: int, dropout: float):\n    n_hidden = n_patches * expansion_factor\n    return nn.Sequential(\n        nn.LayerNorm(num_features),\n        Mix('b hw c -> b hid c', weight_shape='hw hid', bias_shape='hid', hw=n_patches, hidden=n_hidden),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        Mix('b hid c -> b hw c', weight_shape='hid hw', bias_shape='hw',  hw=n_patches, hidden=n_hidden),\n        nn.Dropout(dropout),\n    )\n```\n\n----------------------------------------\n\nTITLE: Tensor Format Conversion\nDESCRIPTION: Demonstrates converting between BCHW and BHWC formats using Einops rearrange.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ny = rearrange(x, \"b c h w -> b h w c\")\nguess(y.shape)\n```\n\n----------------------------------------\n\nTITLE: Packing Real and Fake Images for GAN Processing\nDESCRIPTION: Demonstrates using pack and unpack in a GAN context to combine real and fake images for processing, then separating the predictions for different handling.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ninput_ims, ps = pack([true_images, fake_images], '* h w c')\ntrue_pred, fake_pred = unpack(model(input_ims), ps, '* c')\n```\n\n----------------------------------------\n\nTITLE: Traditional Method for Unpacking Detection Model Outputs\nDESCRIPTION: Shows a traditional approach to extracting multiple outputs from a detection model by manually tracking offsets and reshaping tensors.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef loss_detection(model_output_bhwc, mask_h: int, mask_w: int, n_classes: int):\n    output = model_output_bhwc\n\n    confidence = output[..., 0].sigmoid()\n    bbox_x_shift = output[..., 1].sigmoid()\n    bbox_y_shift = output[..., 2].sigmoid()\n    bbox_w = output[..., 3]\n    bbox_h = output[..., 4]\n    mask_logits = output[..., 5: 5 + mask_h * mask_w]\n    mask_logits = mask_logits.reshape([*mask_logits.shape[:-1], mask_h, mask_w])\n    class_logits = output[..., 5 + mask_h * mask_w:]\n    assert class_logits.shape[-1] == n_classes, class_logits.shape[-1]\n\n    # downstream computations\n    return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits\n```\n\n----------------------------------------\n\nTITLE: Reimplemented Vision Permutator with EinMix in Python\nDESCRIPTION: A simplified implementation of Vision Permutator using EinMix. This implementation replaces the complex chains of tensor manipulations with three concise EinMix layers, making the code more readable and flexible while maintaining performance.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass WeightedPermuteMLP_new(nn.Module):\n    def __init__(self, H, W, C, seg_len):\n        super().__init__()\n        assert C % seg_len == 0, f\"can't divide {C} into segments of length {seg_len}\"\n        self.mlp_c = Mix('b h w c -> b h w c0', weight_shape='c c0', bias_shape='c0', c=C, c0=C)\n        self.mlp_h = Mix('b h w (n c) -> b h0 w (n c0)', weight_shape='h c h0 c0', bias_shape='h0 c0',\n                            h=H, h0=H, c=seg_len, c0=seg_len)\n        self.mlp_w = Mix('b h w (n c) -> b h w0 (n c0)', weight_shape='w c w0 c0', bias_shape='w0 c0',\n                            w=W, w0=W, c=seg_len, c0=seg_len)\n        self.proj = nn.Linear(C, C)\n\n    def forward(self, x):\n        x = self.mlp_c(x) + self.mlp_h(x) + self.mlp_w(x)\n        return self.proj(x)\n```\n\n----------------------------------------\n\nTITLE: Performance Testing of Vision Permutator Implementations in Python\nDESCRIPTION: Code for benchmarking the performance of the original and reimplemented Vision Permutator using timeit. This tests both regular and JIT-scripted versions of both implementations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nx = torch.zeros([32, 32, 32, 128])\n\nfor layer in [\n    WeightedPermuteMLP(H=32, W=32, C=128, S=4),\n    WeightedPermuteMLP_new(H=32, W=32, C=128, seg_len=4),\n    # scripted versions\n    torch.jit.script(WeightedPermuteMLP(H=32, W=32, C=128, S=4)),\n    torch.jit.script(WeightedPermuteMLP_new(H=32, W=32, C=128, seg_len=4)),\n]:\n    %timeit -n 10 y = layer(x)\n```\n\n----------------------------------------\n\nTITLE: Reimplemented MLPMixer Patch Embedding with EinMix in Python\nDESCRIPTION: A simplified implementation of MLPMixer patch embedding using EinMix. This code replaces the Conv2d and reshaping operations with a single EinMix operation, which can handle images of different dimensions as long as they can be divided into patches.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef patcher(patch_size=16, in_channels=3, num_features=128):\n    return Mix('b c_in (h hp) (w wp) -> b (h w) c', weight_shape='c_in hp wp c', bias_shape='c',\n                  c=num_features, hp=patch_size, wp=patch_size, c_in=in_channels)\n```\n\n----------------------------------------\n\nTITLE: Stacking and Concatenating Arrays with Einops Rearrange in Python\nDESCRIPTION: Using einops rearrange to handle lists of arrays by stacking or concatenating them along specified dimensions, providing a more readable alternative to numpy.stack and numpy.concatenate.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# rearrange can also take care of lists of arrays with the same shape\nx = list(ims)\nprint(type(x), \"with\", len(x), \"tensors of shape\", x[0].shape)\n# that's how we can stack inputs\n# \"list axis\" becomes first (\"b\" in this case), and we left it there\nrearrange(x, \"b h w c -> b h w c\").shape\n```\n\nLANGUAGE: python\nCODE:\n```\n# but new axis can appear in the other place:\nrearrange(x, \"b h w c -> h w c b\").shape\n```\n\nLANGUAGE: python\nCODE:\n```\n# that's equivalent to numpy stacking, but written more explicitly\nnumpy.array_equal(rearrange(x, \"b h w c -> h w c b\"), numpy.stack(x, axis=3))\n```\n\nLANGUAGE: python\nCODE:\n```\n# ... or we can concatenate along axes\nrearrange(x, \"b h w c -> h (b w) c\").shape\n```\n\nLANGUAGE: python\nCODE:\n```\n# which is equivalent to concatenation\nnumpy.array_equal(rearrange(x, \"b h w c -> h (b w) c\"), numpy.concatenate(x, axis=1))\n```\n\n----------------------------------------\n\nTITLE: Reducing Tensor Dimensions with Mean Operation\nDESCRIPTION: Example of using einops reduce operation to downsample horizontal axis by 3x while computing mean values.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nreduce(ims, \"b (h h2) (w w2) c -> (h w2) (b w c)\", \"mean\", h2=3, w2=3)\n```\n\n----------------------------------------\n\nTITLE: Testing Universal Predict with Different Input Shapes\nDESCRIPTION: Tests the universal_predict function with different input shapes: single image, batch of images, and batch of videos. Demonstrates the flexibility of the auto-batching implementation.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# works with a single image\nprint(universal_predict(np.zeros([h, w, 3])).shape)\n# works with a batch of images\nbatch = 5\nprint(universal_predict(np.zeros([batch, h, w, 3])).shape)\n# or even a batch of videos\nn_frames = 7\nprint(universal_predict(np.zeros([batch, n_frames, h, w, 3])).shape)\n```\n\n----------------------------------------\n\nTITLE: Adding and Removing Dimensions with Einops in Python\nDESCRIPTION: Adding and removing singleton dimensions (axes of length 1) in tensors using einops rearrange, providing functionality similar to numpy.expand_dims and numpy.squeeze.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nx = rearrange(ims, \"b h w c -> b 1 h w 1 c\")  # functionality of numpy.expand_dims\nprint(x.shape)\nprint(rearrange(x, \"b 1 h w 1 c -> b h w c\").shape)  # functionality of numpy.squeeze\n```\n\nLANGUAGE: python\nCODE:\n```\n# compute max in each image individually, then show a difference\nx = reduce(ims, \"b h w c -> b () () c\", \"max\") - ims\nrearrange(x, \"b h w c -> h (b w) c\")\n```\n\n----------------------------------------\n\nTITLE: Original ResMLP Implementation\nDESCRIPTION: Reference implementation of ResMLP including Affine, MLP, and ResMLP_Blocks classes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# No norm layer\nclass Affine(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.ones(dim))\n        self.beta = nn.Parameter(torch.zeros(dim))\n\n    def forward(self, x):\n        return self.alpha * x + self.beta\n\n\nclass Mlp(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, 4 * dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(4 * dim, dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        return x\n\nclass ResMLP_Blocks(nn.Module):\n    def __init__(self, nb_patches, dim, layerscale_init):\n        super().__init__()\n        self.affine_1 = Affine(dim)\n        self.affine_2 = Affine(dim)\n        self.linear_patches = nn.Linear(nb_patches, nb_patches) #Linear layer on patches\n        self.mlp_channels = Mlp(dim) #MLP on channels\n        self.layerscale_1 = nn.Parameter(layerscale_init * torch.ones((dim))) # LayerScale\n        self.layerscale_2 = nn.Parameter(layerscale_init * torch.ones((dim))) # parameters\n\n    def forward(self, x):\n        res_1 = self.linear_patches(self.affine_1(x).transpose(1,2)).transpose(1,2)\n        x = x + self.layerscale_1 * res_1\n        res_2 = self.mlp_channels(self.affine_2(x))\n        x = x + self.layerscale_2 * res_2\n        return x\n```\n\n----------------------------------------\n\nTITLE: Using einops.unpack for Detection Model Outputs\nDESCRIPTION: Reimplements the detection model output extraction using einops.unpack. Eliminates manual offset tracking and shape checking, making the code more readable and maintainable.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef loss_detection_einops(model_output, mask_h: int, mask_w: int, n_classes: int):\n    confidence, bbox_x_shift, bbox_y_shift, bbox_w, bbox_h, mask_logits, class_logits \\\n        = unpack(model_output, [[]] * 5 + [[mask_h, mask_w], [n_classes]], 'b h w *')\n\n    confidence = confidence.sigmoid()\n    bbox_x_shift = bbox_x_shift.sigmoid()\n    bbox_y_shift = bbox_y_shift.sigmoid()\n\n    # downstream computations\n    return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits\n```\n\n----------------------------------------\n\nTITLE: First refactoring of ResMLP using EinMix\nDESCRIPTION: Rewritten implementation of ResMLP using EinMix to handle affine transformations and linear operations on patches, with explicit shape specifications and initialization.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef Mlp(dim):\n    return nn.Sequential(\n        nn.Linear(dim, 4 * dim),\n        nn.GELU(),\n        nn.Linear(4 * dim, dim),\n    )\n\ndef init(Mix_layer, scale=1.):\n    Mix_layer.weight.data[:] = scale\n    if Mix_layer.bias is not None:\n        Mix_layer.bias.data[:] = 0\n    return Mix_layer\n\nclass ResMLP_Blocks2(nn.Module):\n    def __init__(self, nb_patches, dim, layerscale_init):\n        super().__init__()\n\n        self.affine1 = init(Mix('b t c -> b t c', weight_shape='c', bias_shape='c', c=dim))\n        self.affine2 = init(Mix('b t c -> b t c', weight_shape='c', bias_shape='c', c=dim))\n        self.mix_patches = Mix('b t c -> b t0 c', weight_shape='t t0', bias_shape='t0', t=nb_patches, t0=nb_patches)\n        self.mlp_channels = Mlp(dim)\n        self.linear1 = init(Mix('b t c -> b t c', weight_shape='c', c=dim), scale=layerscale_init)\n        self.linear2 = init(Mix('b t c -> b t c', weight_shape='c', c=dim), scale=layerscale_init)\n\n    def forward(self, x):\n        res1 = self.mix_patches(self.affine1(x))\n        x = x + self.linear1(res1)\n        res2 = self.mlp_channels(self.affine2(x))\n        x = x + self.linear2(res2)\n        return x\n```\n\n----------------------------------------\n\nTITLE: EinMix Equivalent Implementation\nDESCRIPTION: Shows how to implement the same linear layer using EinMix instead of einsum.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmix_channels = Mix('t b c -> t b c_out', weight_shape='c c_out', ...)\nresult = mix_channels(embeddings)\n```\n\n----------------------------------------\n\nTITLE: Reduction Operations with Einops\nDESCRIPTION: Demonstrates various reduction operations (mean, min, max) on tensor dimensions.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nreduce(ims, \"b h w c -> h w c\", \"mean\")\n```\n\n----------------------------------------\n\nTITLE: Backpropagation Example\nDESCRIPTION: Demonstrates gradient computation through einops operations in both PyTorch and TensorFlow.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ny0 = x\ny1 = reduce(y0, \"b c h w -> b c\", \"max\")\ny2 = rearrange(y1, \"b c -> c b\")\ny3 = reduce(y2, \"c b -> \", \"sum\")\n\nif flavour == \"tensorflow\":\n    print(reduce(tape.gradient(y3, x), \"b c h w -> \", \"sum\"))\nelse:\n    y3.backward()\n    print(reduce(x.grad, \"b c h w -> \", \"sum\"))\n```\n\n----------------------------------------\n\nTITLE: Importing Einops Core Functions\nDESCRIPTION: Basic imports of the main einops transformation functions.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import rearrange, reduce\n```\n\n----------------------------------------\n\nTITLE: Dimension Order Effects in Einops Rearrange with Python\nDESCRIPTION: Demonstrating how the order of dimensions in compositions affects the output arrangement of the tensor, similar to lexicographic sorting.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# compare with the next example\nrearrange(ims, \"b h w c -> h (b w) c\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# order of axes in composition is different\n# rule is just as for digits in the number: leftmost digit is the most significant,\n# while neighboring numbers differ in the rightmost axis.\n\n# you can also think of this as lexicographic sort\nrearrange(ims, \"b h w c -> h (w b) c\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# what if b1 and b2 are reordered before composing to width?\nrearrange(ims, \"(b1 b2) h w c -> h (b1 b2 w) c \", b1=2)  # produces 'einops'\nrearrange(ims, \"(b1 b2) h w c -> h (b2 b1 w) c \", b1=2)  # produces 'eoipns'\n```\n\n----------------------------------------\n\nTITLE: Universal Tensor Repetition with Einops across Frameworks\nDESCRIPTION: Shows how to use Einops' repeat function with the same syntax across multiple frameworks (NumPy, PyTorch, TensorFlow, JAX, CuPy) to tile an image along its width dimension.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrepeat(image, 'h w -> h (tile w)', tile=2)  # in numpy\nrepeat(image, 'h w -> h (tile w)', tile=2)  # in pytorch\nrepeat(image, 'h w -> h (tile w)', tile=2)  # in tf\nrepeat(image, 'h w -> h (tile w)', tile=2)  # in jax\nrepeat(image, 'h w -> h (tile w)', tile=2)  # in cupy\n... (etc.)\n```\n\n----------------------------------------\n\nTITLE: Unpacking RGBD Image to Original Components\nDESCRIPTION: Demonstrates using einops.unpack to reverse the packing operation, retrieving the original RGB and depth images with their correct shapes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# remove 1-axis in depth image during unpacking. Results are (h, w, 3) and (h, w)\nunpacked_rgb, unpacked_depth = unpack(image_rgbd, ps, 'h w *')\nunpacked_rgb.shape, unpacked_depth.shape\n```\n\n----------------------------------------\n\nTITLE: Comparing Framework-Specific vs Einops Approach for Tensor Repetition\nDESCRIPTION: This snippet demonstrates how the same tensor repetition operation requires different syntax in NumPy versus PyTorch, and then shows how Einops provides a consistent API across all frameworks.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnp.tile(image, (1, 2))    # in numpy\nimage.repeat(1, 2)        # pytorch's repeat ~ numpy's tile\n```\n\n----------------------------------------\n\nTITLE: Original MLPMixer Patch Embedding Implementation in Python\nDESCRIPTION: The original implementation of patch embedding for MLPMixer that splits an image into patches and projects each patch into an embedding using Conv2d. This code checks if the image dimensions are divisible by the patch size.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef check_sizes(image_size, patch_size):\n    sqrt_num_patches, remainder = divmod(image_size, patch_size)\n    assert remainder == 0, \"`image_size` must be divisibe by `patch_size`\"\n    num_patches = sqrt_num_patches ** 2\n    return num_patches\n\nclass Patcher(nn.Module):\n    def __init__(\n        self,\n        image_size=256,\n        patch_size=16,\n        in_channels=3,\n        num_features=128,\n    ):\n        _num_patches = check_sizes(image_size, patch_size)\n        super().__init__()\n        # per-patch fully-connected is equivalent to strided conv2d\n        self.patcher = nn.Conv2d(\n            in_channels, num_features, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        patches = self.patcher(x)\n        batch_size, num_features, _, _ = patches.shape\n        patches = patches.permute(0, 2, 3, 1)\n        patches = patches.view(batch_size, -1, num_features)\n\n        return patches\n```\n\n----------------------------------------\n\nTITLE: Comparing einsum and EinMix implementations of a linear layer\nDESCRIPTION: Demonstrates the difference between using torch.einsum and EinMix to implement a linear layer for sequence modeling, highlighting EinMix's benefits for weight management.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nweight = <...create tensor...>\nresult = torch.einsum('tbc,cd->tbd', embeddings, weight)\n```\n\nLANGUAGE: python\nCODE:\n```\nmix_channels = Mix('t b c -> t b c_out', weight_shape='c c_out', ...)\nresult = mix_channels(embeddings)\n```\n\n----------------------------------------\n\nTITLE: Detection Model Output Processing\nDESCRIPTION: Demonstrates processing multiple outputs from a detection model using einops unpack.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/4-pack-and-unpack.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef loss_detection_einops(model_output, mask_h: int, mask_w: int, n_classes: int):\n    confidence, bbox_x_shift, bbox_y_shift, bbox_w, bbox_h, mask_logits, class_logits \\\n        = unpack(model_output, [[]] * 5 + [[mask_h, mask_w], [n_classes]], 'b h w *')\n\n    confidence = confidence.sigmoid()\n    bbox_x_shift = bbox_x_shift.sigmoid()\n    bbox_y_shift = bbox_y_shift.sigmoid()\n\n    # downstream computations\n    return confidence, bbox_x_shift, bbox_y_shift, bbox_h, bbox_w, mask_logits, class_logits\n```\n\n----------------------------------------\n\nTITLE: Basic Einsum Example\nDESCRIPTION: Demonstrates a linear layer implementation using torch.einsum for sequence modeling.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nweight = <...create tensor...>\nresult = torch.einsum('tbc,cd->tbd', embeddings, weight)\n```\n\n----------------------------------------\n\nTITLE: Element Repetition with Einops\nDESCRIPTION: Demonstrates repeating elements along existing or new axes using einops repeat.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrepeat(ims[0], \"h w c -> h new_axis w c\", new_axis=5).shape\n```\n\n----------------------------------------\n\nTITLE: Vision Transformer Implementation\nDESCRIPTION: Implementation of a Vision Transformer (ViT) with class token using einops pack/unpack.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/4-pack-and-unpack.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef vit_einops(class_tokens, patch_tokens):\n    input_packed, ps = pack([class_tokens, patch_tokens], 'b * c')\n    output_packed = transformer_mock(input_packed)\n    return unpack(output_packed, ps, 'b * c_out')\n\nclass_token_emb, patch_tokens_emb = vit_einops(class_tokens, patch_tokens)\n```\n\n----------------------------------------\n\nTITLE: Tensor Stacking and Concatenation\nDESCRIPTION: Shows how to stack and concatenate arrays using einops rearrange.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nx = list(ims)\nrearrange(x, \"b h w c -> b h w c\").shape\n```\n\n----------------------------------------\n\nTITLE: Original MLPMixer Patch Embeddings Implementation in Python\nDESCRIPTION: Original implementation of MLPMixer's patch embeddings from vision transformers. This code splits an image into patches and linearly projects each patch into an embedding using a convolutional layer with stride equal to patch size.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef check_sizes(image_size, patch_size):\n    sqrt_num_patches, remainder = divmod(image_size, patch_size)\n    assert remainder == 0, \"`image_size` must be divisibe by `patch_size`\"\n    num_patches = sqrt_num_patches ** 2\n    return num_patches\n\nclass Patcher(nn.Module):\n    def __init__(\n        self,\n        image_size=256,\n        patch_size=16,\n        in_channels=3,\n        num_features=128,\n    ):\n        _num_patches = check_sizes(image_size, patch_size)\n        super().__init__()\n        # per-patch fully-connected is equivalent to strided conv2d\n        self.patcher = nn.Conv2d(\n            in_channels, num_features, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        patches = self.patcher(x)\n        batch_size, num_features, _, _ = patches.shape\n        patches = patches.permute(0, 2, 3, 1)\n        patches = patches.view(batch_size, -1, num_features)\n\n        return patches\n```\n\n----------------------------------------\n\nTITLE: Initializing Random Tensor Data\nDESCRIPTION: Creates a random tensor with specific dimensions using NumPy.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nx = np.random.RandomState(42).normal(size=[10, 32, 100, 200])\n```\n\n----------------------------------------\n\nTITLE: Examining Pack Shape Information\nDESCRIPTION: Examines the packed shapes (ps) information returned by the pack function. This metadata stores how tensors were packed, enabling proper unpacking later.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# now let's see what PS keeps.\n# PS means Packed Shapes, not PlayStation or Post Script\nps\n```\n\n----------------------------------------\n\nTITLE: Checking Shapes After Packing RGB and Depth Images\nDESCRIPTION: Code to inspect the shapes of the original RGB image, depth image, and the resulting packed RGBD image. Demonstrates how pack aligns dimensions correctly.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# as you see, pack properly appended depth as one more layer\n# and correctly aligned axes!\n# this won't work off the shelf with np.concatenate or torch.cat or alike\nimage_rgb.shape, image_depth.shape, image_rgbd.shape\n```\n\n----------------------------------------\n\nTITLE: Performance Testing Code\nDESCRIPTION: Code snippet for comparing performance between different ResMLP implementations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nx = torch.zeros([32, 128, 128])\nfor layer in [\n    ResMLP_Blocks(128, dim=128, layerscale_init=1.),\n    ResMLP_Blocks2(128, dim=128, layerscale_init=1.),\n    ResMLP_Blocks3(128, dim=128, layerscale_init=1.),\n    # scripted versions\n    torch.jit.script(ResMLP_Blocks(128, dim=128, layerscale_init=1.)),\n    torch.jit.script(ResMLP_Blocks2(128, dim=128, layerscale_init=1.)),\n    torch.jit.script(ResMLP_Blocks3(128, dim=128, layerscale_init=1.)),\n]:\n    %timeit -n 10 y = layer(x)\n```\n\n----------------------------------------\n\nTITLE: Original Vision Permutator Implementation in Python\nDESCRIPTION: The original implementation of the WeightedPermuteMLP module from the Vision Permutator paper. This code performs separate operations on height, width, and channel dimensions, using complex reshape and permute operations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass WeightedPermuteMLP(nn.Module):\n    def __init__(self, H, W, C, S):\n        super().__init__()\n\n        self.proj_h = nn.Linear(H * S, H * S)\n        self.proj_w = nn.Linear(W * S, W * S)\n        self.proj_c = nn.Linear(C, C)\n        self.proj = nn.Linear(C, C)\n        self.S = S\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n        S = self.S\n        N = C // S\n        x_h = x.reshape(B, H, W, N, S).permute(0, 3, 2, 1, 4).reshape(B, N, W, H*S)\n        x_h = self.proj_h(x_h).reshape(B, N, W, H, S).permute(0, 3, 2, 1, 4).reshape(B, H, W, C)\n\n        x_w = x.reshape(B, H, W, N, S).permute(0, 1, 3, 2, 4).reshape(B, H, N, W*S)\n        x_w = self.proj_w(x_w).reshape(B, H, N, W, S).permute(0, 1, 3, 2, 4).reshape(B, H, W, C)\n\n        x_c = self.proj_c(x)\n\n        x = x_h + x_w + x_c\n        x = self.proj(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Advanced Axis Composition\nDESCRIPTION: Demonstrates composing multiple axes into new dimensions using einops rearrange.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrearrange(ims, \"b h w c -> (b h) w c\")\n```\n\n----------------------------------------\n\nTITLE: Original Vision Permutator Implementation in Python\nDESCRIPTION: Original implementation of the Vision Permutator (ViP) from the paper. This module operates on spatial dimensions separately and splits channels into groups called 'segments'. The implementation is complex with multiple reshape and permute operations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass WeightedPermuteMLP(nn.Module):\n    def __init__(self, H, W, C, S):\n        super().__init__()\n\n        self.proj_h = nn.Linear(H * S, H * S)\n        self.proj_w = nn.Linear(W * S, W * S)\n        self.proj_c = nn.Linear(C, C)\n        self.proj = nn.Linear(C, C)\n        self.S = S\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n        S = self.S\n        N = C // S\n        x_h = x.reshape(B, H, W, N, S).permute(0, 3, 2, 1, 4).reshape(B, N, W, H*S)\n        x_h = self.proj_h(x_h).reshape(B, N, W, H, S).permute(0, 3, 2, 1, 4).reshape(B, H, W, C)\n\n        x_w = x.reshape(B, H, W, N, S).permute(0, 1, 3, 2, 4).reshape(B, H, N, W*S)\n        x_w = self.proj_w(x_w).reshape(B, H, N, W, S).permute(0, 1, 3, 2, 4).reshape(B, H, W, C)\n\n        x_c = self.proj_c(x)\n\n        x = x_h + x_w + x_c\n        x = self.proj(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Loading Test Images for Einops Manipulation in Python\nDESCRIPTION: Loading a batch of images from a numpy file to demonstrate tensor operations. The images are stored as a 4D tensor with batch, height, width, and color dimensions.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nims = numpy.load(\"./resources/test_images.npy\", allow_pickle=False)\n# There are 6 images of shape 96x96 with 3 color channels packed into tensor\nprint(ims.shape, ims.dtype)\n```\n\n----------------------------------------\n\nTITLE: Original TokenMixer implementation from MLPMixer\nDESCRIPTION: Reference implementation of the TokenMixer component from MLPMixer, showing the original PyTorch code with transpose operations and MLP architecture for mixing tokens.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.nn import functional as F\n\nclass MLP(nn.Module):\n    def __init__(self, num_features, expansion_factor, dropout):\n        super().__init__()\n        num_hidden = num_features * expansion_factor\n        self.fc1 = nn.Linear(num_features, num_hidden)\n        self.dropout1 = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(num_hidden, num_features)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.dropout1(F.gelu(self.fc1(x)))\n        x = self.dropout2(self.fc2(x))\n        return x\n\n\nclass TokenMixer(nn.Module):\n    def __init__(self, num_features, num_patches, expansion_factor, dropout):\n        super().__init__()\n        self.norm = nn.LayerNorm(num_features)\n        self.mlp = MLP(num_patches, expansion_factor, dropout)\n\n    def forward(self, x):\n        # x.shape == (batch_size, num_patches, num_features)\n        residual = x\n        x = self.norm(x)\n        x = x.transpose(1, 2)\n        # x.shape == (batch_size, num_features, num_patches)\n        x = self.mlp(x)\n        x = x.transpose(1, 2)\n        # x.shape == (batch_size, num_patches, num_features)\n        out = x + residual\n        return out\n```\n\n----------------------------------------\n\nTITLE: Importing NumPy for Demo Purposes\nDESCRIPTION: Basic imports needed for using einops with NumPy for demonstration purposes. The code initializes NumPy for tensor operations which will be used in subsequent examples.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# we'll use numpy for demo purposes\n# operations work the same way with other frameworks\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Displaying Individual Images from a Batch in Python\nDESCRIPTION: Accessing and displaying individual images from the loaded batch by indexing into the 4D tensor.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# display the first image (whole 4d tensor can't be rendered)\nims[0]\n```\n\nLANGUAGE: python\nCODE:\n```\n# second image in a batch\nims[1]\n```\n\n----------------------------------------\n\nTITLE: TokenMixer Implementation from MLPMixer\nDESCRIPTION: Original implementation of TokenMixer component from MLPMixer architecture.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.nn import functional as F\n\nclass MLP(nn.Module):\n    def __init__(self, num_features, expansion_factor, dropout):\n        super().__init__()\n        num_hidden = num_features * expansion_factor\n        self.fc1 = nn.Linear(num_features, num_hidden)\n        self.dropout1 = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(num_hidden, num_features)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.dropout1(F.gelu(self.fc1(x)))\n        x = self.dropout2(self.fc2(x))\n        return x\n\n\nclass TokenMixer(nn.Module):\n    def __init__(self, num_features, num_patches, expansion_factor, dropout):\n        super().__init__()\n        self.norm = nn.LayerNorm(num_features)\n        self.mlp = MLP(num_patches, expansion_factor, dropout)\n\n    def forward(self, x):\n        # x.shape == (batch_size, num_patches, num_features)\n        residual = x\n        x = self.norm(x)\n        x = x.transpose(1, 2)\n        # x.shape == (batch_size, num_features, num_patches)\n        x = self.mlp(x)\n        x = x.transpose(1, 2)\n        # x.shape == (batch_size, num_patches, num_features)\n        out = x + residual\n        return out\n```\n\n----------------------------------------\n\nTITLE: Importing Einops Core Operations in Python\nDESCRIPTION: Importing the three main operations from einops: rearrange for reshaping dimensions, reduce for collapsing dimensions, and repeat for expanding dimensions.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# we'll use three operations\nfrom einops import rearrange, reduce, repeat\n```\n\n----------------------------------------\n\nTITLE: Further refactoring of ResMLP with Sequential modules\nDESCRIPTION: Advanced refactoring of ResMLP that groups operations by branch (patches and channels) using nn.Sequential, making the code more readable and revealing potential optimizations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef init(layer: Mix, scale=1.):\n    layer.weight.data[:] = scale\n    if layer.bias is not None:\n        layer.bias.data[:] = 0\n    return layer\n\nclass ResMLP_Blocks3(nn.Module):\n    def __init__(self, nb_patches, dim, layerscale_init):\n        super().__init__()\n        self.branch_patches = nn.Sequential(\n            init(Mix('b t c -> b t c', weight_shape='c', c=dim), scale=layerscale_init),\n            Mix('b t c -> b t0 c', weight_shape='t t0', bias_shape='t0', t=nb_patches, t0=nb_patches),\n            init(Mix('b t c -> b t c', weight_shape='c', bias_shape='c', c=dim)),\n        )\n\n        self.branch_channels = nn.Sequential(\n            init(Mix('b t c -> b t c', weight_shape='c', c=dim), scale=layerscale_init),\n            nn.Linear(dim, 4 * dim),\n            nn.GELU(),\n            nn.Linear(4 * dim, dim),\n            init(Mix('b t c -> b t c', weight_shape='c', bias_shape='c', c=dim)),\n        )\n\n    def forward(self, x):\n        x = x + self.branch_patches(x)\n        x = x + self.branch_channels(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Importing EinMix from einops for PyTorch\nDESCRIPTION: Demonstrates importing the EinMix component from einops.layers.torch module for use in PyTorch implementations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom einops.layers.torch import EinMix as Mix\n```\n\n----------------------------------------\n\nTITLE: First ResMLP Rewrite with EinMix\nDESCRIPTION: First iteration of ResMLP rewritten using EinMix, simplifying the implementation with more explicit tensor operations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef Mlp(dim):\n    return nn.Sequential(\n        nn.Linear(dim, 4 * dim),\n        nn.GELU(),\n        nn.Linear(4 * dim, dim),\n    )\n\ndef init(Mix_layer, scale=1.):\n    Mix_layer.weight.data[:] = scale\n    if Mix_layer.bias is not None:\n        Mix_layer.bias.data[:] = 0\n    return Mix_layer\n\nclass ResMLP_Blocks2(nn.Module):\n    def __init__(self, nb_patches, dim, layerscale_init):\n        super().__init__()\n\n        self.affine1 = init(Mix('b t c -> b t c', weight_shape='c', bias_shape='c', c=dim))\n        self.affine2 = init(Mix('b t c -> b t c', weight_shape='c', bias_shape='c', c=dim))\n        self.mix_patches = Mix('b t c -> b t0 c', weight_shape='t t0', bias_shape='t0', t=nb_patches, t0=nb_patches)\n        self.mlp_channels = Mlp(dim)\n        self.linear1 = init(Mix('b t c -> b t c', weight_shape='c', c=dim), scale=layerscale_init)\n        self.linear2 = init(Mix('b t c -> b t c', weight_shape='c', c=dim), scale=layerscale_init)\n\n    def forward(self, x):\n        res1 = self.mix_patches(self.affine1(x))\n        x = x + self.linear1(res1)\n        res2 = self.mlp_channels(self.affine2(x))\n        x = x + self.linear2(res2)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison of Vision Permutator Implementations in Python\nDESCRIPTION: A code snippet to compare the performance of the original and reimplemented Vision Permutator models, including their JIT-scripted versions. This demonstrates that the EinMix implementation maintains performance while improving code clarity.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nx = torch.zeros([32, 32, 32, 128])\n\nfor layer in [\n    WeightedPermuteMLP(H=32, W=32, C=128, S=4),\n    WeightedPermuteMLP_new(H=32, W=32, C=128, seg_len=4),\n    # scripted versions\n    torch.jit.script(WeightedPermuteMLP(H=32, W=32, C=128, S=4)),\n    torch.jit.script(WeightedPermuteMLP_new(H=32, W=32, C=128, seg_len=4)),\n]:\n    %timeit -n 10 y = layer(x)\n```\n\n----------------------------------------\n\nTITLE: Importing Einops Operations\nDESCRIPTION: Imports the three main einops operations: rearrange, reduce, and repeat.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import rearrange, reduce, repeat\n```\n\n----------------------------------------\n\nTITLE: Running Einops Tests for Different Frameworks\nDESCRIPTION: Command for running Einops tests across specified frameworks. The command allows testing compatibility with NumPy, PyTorch, and JAX in this example, with an option to install dependencies automatically.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# pip install einops pytest\npython -m einops.tests.run_tests numpy pytorch jax --pip-install\n```\n\n----------------------------------------\n\nTITLE: Importing Einops Core Functions\nDESCRIPTION: Basic imports of essential Einops functions for tensor manipulation.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import rearrange, reduce\n```\n\n----------------------------------------\n\nTITLE: Verifying Equivalence of Detection Output Methods\nDESCRIPTION: Tests both the traditional and einops-based detection output handling methods to verify they produce identical results.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# check that results are identical\nimport torch\ndims = dict(mask_h=6, mask_w=8, n_classes=19)\nmodel_output = torch.randn([3, 5, 7, 5 + dims['mask_h'] * dims['mask_w'] + dims['n_classes']])\nfor a, b in zip(loss_detection(model_output, **dims), loss_detection_einops(model_output, **dims)):\n    assert torch.allclose(a, b)\n```\n\n----------------------------------------\n\nTITLE: Basic Tensor Rearrangement Example\nDESCRIPTION: Simple example of rearranging tensor dimensions using einops with numpy array.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrearrange(ims, 'b h w c -> w (b h) c')\n```\n\n----------------------------------------\n\nTITLE: Importing EinMix Module\nDESCRIPTION: Basic import of EinMix from einops.layers.torch package.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/3-einmix-layer.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom einops.layers.torch import EinMix as Mix\n```\n\n----------------------------------------\n\nTITLE: Importing einops layers for different frameworks\nDESCRIPTION: Shows how to import Rearrange and Reduce layers from einops for various deep learning frameworks.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom einops.layers.torch      import Rearrange, Reduce\nfrom einops.layers.tensorflow import Rearrange, Reduce\nfrom einops.layers.flax       import Rearrange, Reduce\nfrom einops.layers.paddle     import Rearrange, Reduce\n```\n\n----------------------------------------\n\nTITLE: Framework Selection Setup\nDESCRIPTION: Code to set up either PyTorch or TensorFlow backend for tensor operations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(\"selected {} backend\".format(flavour))\nif flavour == \"tensorflow\":\n    import tensorflow as tf\n\n    tape = tf.GradientTape(persistent=True)\n    tape.__enter__()\n    x = tf.Variable(x) + 0\nelse:\n    assert flavour == \"pytorch\"\n    import torch\n\n    x = torch.from_numpy(x)\n    x.requires_grad = True\n```\n\n----------------------------------------\n\nTITLE: Setting Up Numpy and Display Utilities for Einops Tutorial\nDESCRIPTION: Importing numpy and setting up utilities to display numpy arrays as images in Jupyter/IPython environment.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/1-einops-basics.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Examples are given for numpy. This code also setups ipython/jupyter\n# so that numpy arrays in the output are displayed as images\nimport numpy\nfrom utils import display_np_arrays_as_images\n\ndisplay_np_arrays_as_images()\n```\n\n----------------------------------------\n\nTITLE: Basic imports for ResMLP implementation\nDESCRIPTION: Essential imports for implementing and refactoring ResMLP architecture using PyTorch and einops.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# let's start\nimport torch\nfrom torch import nn\n```\n\n----------------------------------------\n\nTITLE: Data Initialization\nDESCRIPTION: Creating a random tensor with numpy for demonstration purposes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nx = np.random.RandomState(42).normal(size=[10, 32, 100, 200])\n```\n\n----------------------------------------\n\nTITLE: Using einsum in einops\nDESCRIPTION: Demonstrates the use of einsum operation in einops, which supports multi-lettered names for axes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom einops import einsum, pack, unpack\n# einsum is like ... einsum, generic and flexible dot-product\n# but 1) axes can be multi-lettered  2) pattern goes last 3) works with multiple frameworks\nC = einsum(A, B, 'b t1 head c, b t2 head c -> b head t1 t2')\n```\n\n----------------------------------------\n\nTITLE: Framework Selection Setup\nDESCRIPTION: Code to select and initialize either PyTorch or TensorFlow backend for tensor operations.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/2-einops-for-deep-learning.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# select \"tensorflow\" or \"pytorch\"\nflavour = \"pytorch\"\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(\"selected {}\".format(flavour))\nif flavour == \"tensorflow\":\n    import tensorflow as tf\n\n    tape = tf.GradientTape(persistent=True)\n    tape.__enter__()\n    x = tf.Variable(x) + 0\nelse:\n    assert flavour == \"pytorch\"\n    import torch\n\n    x = torch.from_numpy(x)\n    x.requires_grad = True\n```\n\n----------------------------------------\n\nTITLE: Styling Blockquotes and Paragraphs with CSS\nDESCRIPTION: This CSS snippet defines custom styles for blockquotes and paragraphs within the document. It sets background color, border color, margins, and padding for blockquotes, and aligns the text of paragraphs following blockquotes to the right.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/pages/testimonials.md#2025-04-19_snippet_0\n\nLANGUAGE: CSS\nCODE:\n```\n<style>\n.md-typeset blockquote {\n    background-color: rgba(128, 128, 128, 0.04);\n    border-color: #002ee380;\n    color: #333;\n    margin-top: 2.5em;\n    margin-bottom: -0.5em;\n    margin-right: 3em;\n    padding-right: 2em;\n}\nblockquote + p {\n    text-align: right;\n    padding-right: 5em;\n}\n</style>\n```\n\n----------------------------------------\n\nTITLE: Creating a Transformer Mock Function\nDESCRIPTION: Creates a simple mock for a transformer model that works with BTC-shaped tensors (batch, time, channels). This will be used in subsequent examples.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/4-pack-and-unpack.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef transformer_mock(x_btc):\n    # imagine this is a transformer model, a very efficient one\n    assert len(x_btc.shape) == 3\n    return x_btc\n```\n\n----------------------------------------\n\nTITLE: Environment Setup for Einops Tutorial\nDESCRIPTION: Sets up the environment with numpy and custom utilities for displaying numpy arrays as images.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/1-einops-basics.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom utils import display_np_arrays_as_images\n\ndisplay_np_arrays_as_images()\n```\n\n----------------------------------------\n\nTITLE: Performance testing of different ResMLP implementations\nDESCRIPTION: Code for benchmarking the performance of the original ResMLP implementation and the two refactored versions using EinMix, including JIT-scripted variants.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs/3-einmix-layer.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nx = torch.zeros([32, 128, 128])\nfor layer in [\n    ResMLP_Blocks(128, dim=128, layerscale_init=1.),\n    ResMLP_Blocks2(128, dim=128, layerscale_init=1.),\n    ResMLP_Blocks3(128, dim=128, layerscale_init=1.),\n    # scripted versions\n    torch.jit.script(ResMLP_Blocks(128, dim=128, layerscale_init=1.)),\n    torch.jit.script(ResMLP_Blocks2(128, dim=128, layerscale_init=1.)),\n    torch.jit.script(ResMLP_Blocks3(128, dim=128, layerscale_init=1.)),\n]:\n    %timeit -n 10 y = layer(x)\n```\n\n----------------------------------------\n\nTITLE: Importing Numpy for Basic Demo\nDESCRIPTION: Basic setup importing numpy for demonstration purposes.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/4-pack-and-unpack.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Installing einops via pip\nDESCRIPTION: Simple command to install the einops library using pip package manager.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install einops\n```\n\n----------------------------------------\n\nTITLE: Importing einops.asnumpy Module\nDESCRIPTION: This code snippet shows how to import the asnumpy module from the einops package. The asnumpy function is used to convert tensors from different machine learning frameworks to numpy arrays.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/docs_src/api/asnumpy.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: einops.asnumpy\n```\n\n----------------------------------------\n\nTITLE: Building and Serving Einops Documentation\nDESCRIPTION: Command for building and serving the Einops documentation using Hatch, which will make the documentation available on a local development server.\nSOURCE: https://github.com/arogozhnikov/einops/blob/main/README.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhatch run docs:serve  # Serving on http://localhost:8000/\n```"
  }
]
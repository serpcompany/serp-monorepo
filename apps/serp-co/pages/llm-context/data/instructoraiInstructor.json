[
  {
    "owner": "instructor-ai",
    "repo": "instructor",
    "content": "TITLE: Installing Instructor and Provider-Specific Packages\nDESCRIPTION: Commands for installing Instructor and additional packages for specific LLM providers using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/getting-started.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\n\n# For OpenAI (included by default)\npip install instructor\n\n# For Anthropic\npip install \"instructor[anthropic]\"\n\n# For other providers\npip install \"instructor[google-generativeai]\"  # For Google/Gemini\npip install \"instructor[vertexai]\"             # For Vertex AI\npip install \"instructor[cohere]\"               # For Cohere\npip install \"instructor[litellm]\"              # For LiteLLM (multiple providers)\npip install \"instructor[mistralai]\"            # For Mistral\n```\n\n----------------------------------------\n\nTITLE: Validating Form Data with Pydantic and Instructor in Python\nDESCRIPTION: This code demonstrates a complete implementation of a registration form validator using Pydantic models with Instructor for OpenAI integration. The class includes field-level validators for username format, email format, password complexity, and age verification, plus a model-level validator to ensure password confirmation matches. The validation rules enforce security and usability standards for registration forms.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/field_validation.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, field_validator, model_validator\nimport instructor\nfrom openai import OpenAI\nimport re\nfrom datetime import date, datetime\nfrom typing import Optional\n\nclient = instructor.from_openai(OpenAI())\n\nclass RegistrationForm(BaseModel):\n    username: str = Field(..., min_length=3, max_length=20)\n    email: str\n    password: str\n    confirm_password: str\n    birth_date: date\n    \n    @field_validator('username')\n    @classmethod\n    def validate_username(cls, v):\n        if not re.match(r'^[a-zA-Z0-9_]+$', v):\n            raise ValueError(\"Username can only contain letters, numbers, and underscores\")\n        return v\n    \n    @field_validator('email')\n    @classmethod\n    def validate_email(cls, v):\n        if not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', v):\n            raise ValueError(\"Invalid email format\")\n        return v\n    \n    @field_validator('password')\n    @classmethod\n    def validate_password(cls, v):\n        if len(v) < 8:\n            raise ValueError(\"Password must be at least 8 characters\")\n        if not re.search(r'[A-Z]', v):\n            raise ValueError(\"Password must contain at least one uppercase letter\")\n        if not re.search(r'[a-z]', v):\n            raise ValueError(\"Password must contain at least one lowercase letter\")\n        if not re.search(r'[0-9]', v):\n            raise ValueError(\"Password must contain at least one number\")\n        return v\n    \n    @field_validator('birth_date')\n    @classmethod\n    def validate_age(cls, v):\n        today = date.today()\n        age = today.year - v.year - ((today.month, today.day) < (v.month, v.day))\n        if age < 18:\n            raise ValueError(\"You must be at least 18 years old to register\")\n        return v\n    \n    @model_validator(mode='after')\n    def passwords_match(self):\n        if self.password != self.confirm_password:\n            raise ValueError(\"Passwords do not match\")\n        return self\n```\n\n----------------------------------------\n\nTITLE: Implementing KnowledgeGraph Class with Visualization\nDESCRIPTION: Defines the KnowledgeGraph class with visualization capabilities using Graphviz.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom graphviz import Digraph\nfrom IPython.display import display\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: list[Node] = Field(\n        ..., default_factory=list\n    )  # A list of nodes in the knowledge graph.\n    edges: list[Edge] = Field(\n        ..., default_factory=list\n    )  # A list of edges in the knowledge graph.\n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(name=str(node.id), label=node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(\n                str(edge.source), str(edge.target), label=edge.label, color=edge.color\n            )\n\n        return display(dot)\n```\n\n----------------------------------------\n\nTITLE: Structured Data Extraction with Instructor and Pydantic\nDESCRIPTION: Shows how to use Instructor with Pydantic models to extract structured customer data with validation and type safety.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/structured_outputs.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field, EmailStr\n\nclass Customer(BaseModel):\n    name: str = Field(description=\"Customer's full name\")\n    age: int = Field(description=\"Customer's age in years\", ge=0, le=120)\n    email: EmailStr = Field(description=\"Customer's email address\")\n\nclient = instructor.from_openai(OpenAI())\ncustomer = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract customer: John Doe, age 35, email: john@example.com\",\n        }\n    ],\n    response_model=Customer,  # This is the key part\n)\n\nprint(customer)  # Customer(name='John Doe', age=35, email='john@example.com')\nprint(f\"Name: {customer.name}, Age: {customer.age}, Email: {customer.email}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for OpenAI Integration\nDESCRIPTION: This snippet shows how to install Instructor AI for use with OpenAI's API.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\n```\n\n----------------------------------------\n\nTITLE: Complete Question Answering Implementation\nDESCRIPTION: Full implementation showing how to use Instructor with OpenAI to answer questions with citations and validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.from_openai(OpenAI())\n\n\ndef answer_question(question: str, text_chunk: str) -> AnswerWithCitation:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Answer the question: {question} with the text chunk: {text_chunk}\",\n            },\n        ],\n        response_model=AnswerWithCitation,\n        validation_context={\"text_chunk\": text_chunk},\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Field-Level Streaming with Instructor in Python\nDESCRIPTION: This code snippet demonstrates how to use the Instructor library to implement field-level streaming for partial responses. It defines data models, sets up OpenAI client with Instructor, and creates a stream of partial responses that are dynamically rendered using Rich console.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/partial_streaming.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\n\nclient = instructor.from_openai(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nPartialMeetingInfo = instructor.Partial[MeetingInfo]\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=PartialMeetingInfo,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)  # type: ignore\n\n\nfrom rich.console import Console\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n```\n\n----------------------------------------\n\nTITLE: Basic Person Data Extraction using Instructor and OpenAI\nDESCRIPTION: Complete example showing how to extract a person's name and age from text using Instructor with OpenAI. Demonstrates model definition, client setup, and data extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/first_extraction.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\n# 1. Define your data model\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# 2. Set up the Instructor client\nclient = instructor.from_openai(OpenAI())\n\n# 3. Extract structured data\nperson = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Person,\n    messages=[\n        {\"role\": \"user\", \"content\": \"John Doe is 30 years old\"}\n    ]\n)\n\n# 4. Use the structured data\nprint(f\"Name: {person.name}, Age: {person.age}\")\n# Output: Name: John Doe, Age: 30\n```\n\n----------------------------------------\n\nTITLE: Using Models with Instructor Client\nDESCRIPTION: Shows how to use defined models with the Instructor client for extracting data using OpenAI.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nforecast = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=WeatherForecast,\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather in New York today?\"}\n    ]\n)\n\nprint(forecast.model_dump_json(indent=2))\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Function Calling with OpenAI in Python\nDESCRIPTION: This snippet demonstrates how to use parallel function calling with OpenAI to simultaneously fetch weather information for multiple locations and perform a Google search. It uses the instructor library and Pydantic models to structure the function calls.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/parallel.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nimport openai\nimport instructor\n\nfrom typing import Iterable, Literal\nfrom pydantic import BaseModel\n\n\nclass Weather(BaseModel):\n    location: str\n    units: Literal[\"imperial\", \"metric\"]\n\n\nclass GoogleSearch(BaseModel):\n    query: str\n\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)\n\nfunction_calls = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You must always use tools\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather in toronto and dallas and who won the super bowl?\",\n        },\n    ],\n    response_model=Iterable[Weather | GoogleSearch],\n)\n\nfor fc in function_calls:\n    print(fc)\n    #> location='Toronto' units='metric'\n    #> location='Dallas' units='metric'\n    #> query='who won the super bowl 2023'\n```\n\n----------------------------------------\n\nTITLE: YouTube Transcript Processing and Clip Generation Implementation\nDESCRIPTION: Complete Python implementation for extracting YouTube video transcripts and generating meaningful clips. Includes Pydantic models for data validation, transcript fetching functionality, and OpenAI integration for clip generation with proper timing and descriptions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/youtube_clips.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom pydantic import BaseModel, Field\nfrom typing import List, Generator, Iterable\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef extract_video_id(url: str) -> str | None:\n    import re\n\n    match = re.search(r\"v=([a-zA-Z0-9_-]+)\", url)\n    if match:\n        return match.group(1)\n\n\nclass TranscriptSegment(BaseModel):\n    source_id: int\n    start: float\n    text: str\n\n\ndef get_transcript_with_timing(\n    video_id: str,\n) -> Generator[TranscriptSegment, None, None]:\n    \"\"\"\n    Fetches the transcript of a YouTube video along with the start and end times\n    for each text segment, and returns them as a list of Pydantic models.\n    \"\"\"\n    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n    for ii, segment in enumerate(transcript):\n        yield TranscriptSegment(\n            source_id=ii, start=segment[\"start\"], text=segment[\"text\"]\n        )\n\n\nclass YoutubeClip(BaseModel):\n    title: str = Field(description=\"Specific and informative title for the clip.\")\n    description: str = Field(\n        description=\"A detailed description of the clip, including notable quotes or phrases.\"\n    )\n    start: float\n    end: float\n\n\nclass YoutubeClips(BaseModel):\n    clips: List[YoutubeClip]\n\n\ndef yield_clips(segments: Iterable[TranscriptSegment]) -> Iterable[YoutubeClips]:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        stream=True,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are given a sequence of YouTube transcripts and your job\n                is to return notable clips that can be recut as smaller videos. Give very\n                specific titles and descriptions. Make sure the length of clips is proportional\n                to the length of the video. Note that this is a transcript and so there might\n                be spelling errors. Note that and correct any spellings. Use the context to\n                make sure you're spelling things correctly.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Let's use the following transcript segments.\\n{segments}\",\n            },\n        ],\n        response_model=instructor.Partial[YoutubeClips],\n        validation_context={\"segments\": segments},\n    )  # type: ignore\n\n\n# Example usage\nif __name__ == \"__main__\":\n    from rich.table import Table\n    from rich.console import Console\n    from rich.prompt import Prompt\n\n    console = Console()\n    url = Prompt.ask(\"Enter a YouTube URL\")\n\n    with console.status(\"[bold green]Processing YouTube URL...\") as status:\n        video_id = extract_video_id(url)\n\n        if video_id is None:\n            raise ValueError(\"Invalid YouTube video URL\")\n\n        transcript = list(get_transcript_with_timing(video_id))\n        status.update(\"[bold green]Generating clips...\")\n\n        for clip in yield_clips(transcript):\n            console.clear()\n\n            table = Table(title=\"Extracted YouTube Clips\", padding=(0, 1))\n\n            table.add_column(\"Title\", style=\"cyan\")\n            table.add_column(\"Description\", style=\"magenta\")\n            table.add_column(\"Start\", justify=\"right\", style=\"green\")\n            table.add_column(\"End\", justify=\"right\", style=\"green\")\n            for youtube_clip in clip.clips or []:\n                table.add_row(\n                    youtube_clip.title,\n                    youtube_clip.description,\n                    str(youtube_clip.start),\n                    str(youtube_clip.end),\n                )\n            console.print(table)\n```\n\n----------------------------------------\n\nTITLE: Implementing Knowledge Graph Generation with OpenAI and Pydantic in Python\nDESCRIPTION: A complete implementation for converting text to knowledge graphs using OpenAI's API and Pydantic models. The code defines data structures for nodes and edges, and includes a function to generate graph representations from input text. It uses instructor-patched OpenAI client for model response handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/building_knowledge_graphs.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\nimport instructor\n\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str = \"blue\"  # Default color set to blue\n\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"  # Default color for edges\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(default_factory=list)\n    edges: List[Edge] = Field(default_factory=list)\n\n\n# Patch the OpenAI client to add response_model support\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_graph(input_text: str) -> KnowledgeGraph:\n    \"\"\"Generates a knowledge graph from the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input_text}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )\n\n\nif __name__ == \"__main__\":\n    input_text = \"Jason is Sarah's friend and he is a doctor\"\n    graph = generate_graph(input_text)\n    print(graph.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"nodes\": [\n        {\n          \"id\": 1,\n          \"label\": \"Jason\",\n          \"color\": \"blue\"\n        },\n        {\n          \"id\": 2,\n          \"label\": \"Sarah\",\n          \"color\": \"blue\"\n        },\n        {\n          \"id\": 3,\n          \"label\": \"Doctor\",\n          \"color\": \"blue\"\n        }\n      ],\n      \"edges\": [\n        {\n          \"source\": 1,\n          \"target\": 2,\n          \"label\": \"is a friend of\",\n          \"color\": \"black\"\n        },\n        {\n          \"source\": 1,\n          \"target\": 3,\n          \"label\": \"is a\",\n          \"color\": \"black\"\n        }\n      ]\n    }\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Action Item Extraction System with Python\nDESCRIPTION: Core implementation of the action item extraction system using OpenAI API and Pydantic models. Includes definitions for priority enums, subtask and ticket models, and the main generation function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/action_items.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable, List, Optional\nfrom enum import Enum\nfrom pydantic import BaseModel\n\n\nclass PriorityEnum(str, Enum):\n    high = \"High\"\n    medium = \"Medium\"\n    low = \"Low\"\n\n\nclass Subtask(BaseModel):\n    \"\"\"Correctly resolved subtask from the given transcript\"\"\"\n\n    id: int\n    name: str\n\n\nclass Ticket(BaseModel):\n    \"\"\"Correctly resolved ticket from the given transcript\"\"\"\n\n    id: int\n    name: str\n    description: str\n    priority: PriorityEnum\n    assignees: List[str]\n    subtasks: Optional[List[Subtask]]\n    dependencies: Optional[List[int]]\n\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate(data: str) -> Iterable[Ticket]:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[Ticket],\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"The following is a transcript of a meeting...\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Create the action items for the following transcript: {data}\",\n            },\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Scrubbing PII Data from Documents\nDESCRIPTION: Demonstrates the process of sanitizing documents by replacing identified PII with placeholders using the scrub_data method. Shows how to handle multiple types of PII data including dates, SSNs, emails, and addresses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/pii.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npii_data = PIIDataExtraction(\n    private_data=[\n        {\"index\": 0, \"data_type\": \"date\", \"pii_value\": \"01/02/1980\"},\n        {\"index\": 1, \"data_type\": \"ssn\", \"pii_value\": \"123-45-6789\"},\n        {\"index\": 2, \"data_type\": \"email\", \"pii_value\": \"john.doe@email.com\"},\n        {\"index\": 3, \"data_type\": \"phone\", \"pii_value\": \"555-123-4567\"},\n        {\n            \"index\": 4,\n            \"data_type\": \"address\",\n            \"pii_value\": \"123 Main St, Springfield, IL, 62704\",\n        },\n    ]\n)\n\nEXAMPLE_DOCUMENT = \"\"\"\n# Fake Document with PII for Testing PII Scrubbing Model\n# He was born on 01/02/1980. His social security number is 123-45-6789. He has been using the email address john.doe@email.com for years, and he can always be reached at 555-123-4567.\n\"\"\"\nprint(pii_data.scrub_data(EXAMPLE_DOCUMENT))\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Anthropic and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with Anthropic's Claude model to extract structured data. It uses a Pydantic model for data extraction and Anthropic's client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_anthropic(Anthropic())\n\n# note that client.chat.completions.create will also work\nresp = client.messages.create(\n    model=\"claude-3-5-sonnet-20240620\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=ExtractUser,\n)\n\nassert isinstance(resp, ExtractUser)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Implementing Cumulative Reasoning Pipeline in Python with Instructor\nDESCRIPTION: Complete implementation of a Cumulative Reasoning system that uses three steps (Propose, Verify, Report) to perform logical reasoning. The code uses Pydantic models for structured outputs and async functions to interact with OpenAI's API through the Instructor library.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/self_criticism/cumulative_reason.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\nfrom typing import Literal\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass Proposition(BaseModel):\n    premise1: str\n    premise2: str\n    reasoning: str\n    proposition: str\n\n\nclass ProposerOutput(BaseModel):\n    reasoning: str\n    valid_propositions: list[Proposition] = Field(\n        description=\"Concise list of Propositions that are derived from the premises that are relevant to the hypothesis. Note that each Proposition is derived from two given premises at most\",\n        min_length=4,\n    )\n    prediction: Literal[\"False\", \"True\", \"Unknown\"]\n\n\nclass VerifiedProposition(BaseModel):\n    proposition: str\n    reasoning: str\n    is_valid: bool\n\n\nclass ReporterOutput(BaseModel):\n    reasoning: str\n    is_valid_hypothesis: bool\n\n\nasync def generate_propositions(premises: list[str], hypothesis: str) -> ProposerOutput:\n    formatted_premises = \"\\n- \".join(premises)\n    return await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    \"\"\"\n                Suppose you are one of the greatest AI\n                scientists, logicians, and mathematicians.\n\n                Let us think step by step. Please use\n                First-Order Logic (FOL) to deduce a list\n                of Propositions. Each Proposition is\n                derived from two given Premises and\n                should be logically correct. Most\n                importantly, each Proposition should\n                not duplicate the two premises that it\n                is derived from. Please make sure your\n                reasoning is directly deduced from the\n                Premises and Propositions rather than\n                introducing unsourced common knowledge\n                and unsourced information by common\n                sense reasoning.\n                \"\"\"\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                Premises:\n                {formatted_premises}\n\n                We want to deduce more Propositions to\n                determine the correctness of the following\n                Hypothesis:\n                Hypothesis: {hypothesis}\n                \"\"\"\n                ),\n            },\n        ],\n        response_model=ProposerOutput,\n        model=\"gpt-4o\",\n    )\n\n\nasync def verify_propositions(\n    premise_evaluation: ProposerOutput,\n) -> list[VerifiedProposition]:\n    async def create_verification_task(proposition: Proposition) -> VerifiedProposition:\n        return await client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                    Suppose you are one of the greatest AI\n                    scientists, logicians, and mathematicians.\n                    Let us think step by step. Please use\n                    First-Order Logic (FOL) to determine\n                    whether the deduction of two given\n                    Premises to a Proposition is valid or not,\n                    and reply with True or False.\n                    \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                    Premises:\n                    {proposition.premise1}\n                    {proposition.premise2}\n\n                    Proposition:\n                    {proposition.proposition}\n                    \"\"\",\n                },\n            ],\n            response_model=VerifiedProposition,\n            model=\"gpt-4o\",\n        )\n\n    tasks = [\n        create_verification_task(proposition)\n        for proposition in premise_evaluation.valid_propositions\n    ]\n\n    return await asyncio.gather(*tasks)\n\n\nasync def final_evaluation(\n    verification_result: list[str], hypothesis: str, premises: list[str]\n) -> ReporterOutput:\n    formatted_premises = \"\\n- \".join(premises)\n    formatted_propositions = \"\\n- \".join(verification_result)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                Suppose you are one of the greatest AI\n                scientists, logicians, and mathematicians.\n                Let us think step by step. Read and analyze\n                the \"Premises\" first, then use First-Order\n                Logic (FOL) to judge whether the \"Hypothesis\"\n                is True, False, or Unknown. Please make sure\n                your reasoning is directly deduced from the\n                \"Premises\" and \"Propositions\" rather than\n                introducing unsourced common knowledge and\n                unsourced information by common sense\n                reasoning.\n                \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Premises:\n                {formatted_premises}\n\n                Hypothesis: {hypothesis}\n                \"\"\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": f\"\"\"\n                Let's think step by step. From the premises,\n                we can deduce the following propositions:\n                {formatted_propositions}\n\n                Recall the Hypothesis: {hypothesis}\n                \"\"\",\n            },\n        ],\n        response_model=ReporterOutput,\n    )\n\n\nif __name__ == \"__main__\":\n    hypothesis = \"Hyraxes lay eggs\"\n    premises = [\n        \"The only types of mammals that lay eggs are platypuses and echidnas\",\n        \"Platypuses are not hyrax\",\n        \"Echidnas are not hyrax\",\n        \"No mammals are invertebrates\",\n        \"All animals are either vertebrates or invertebrates\",\n        \"Mammals are animals\",\n        \"Hyraxes are mammals\",\n        \"Grebes lay eggs\",\n        \"Grebes are not platypuses and also not echidnas\",\n    ]\n    premise_evaluation = asyncio.run(generate_propositions(premises, hypothesis))\n\n    verification_result = asyncio.run(verify_propositions(premise_evaluation))\n\n    filtered_propositions = [\n        proposition.proposition\n        for proposition in verification_result\n        if proposition.is_valid\n    ]\n\n    reporter_output = asyncio.run(\n        final_evaluation(filtered_propositions, hypothesis, premises)\n    )\n    print(reporter_output.model_dump_json(indent=2))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Pydantic Integration\nDESCRIPTION: Basic setup for using Pydantic with OpenAI's API to create structured responses. The code demonstrates how to initialize a client with response model validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/pydantic-is-still-all-you-need.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import from_openai\n\nclient = from_openai(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\", response_model=User, messages=[...]\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Structured Outputs with OpenAI and Instructor\nDESCRIPTION: Demonstrates how to use the Instructor library to create structured outputs from OpenAI's language models. It uses a Pydantic model to define the expected response structure and shows how to make a chat completion request.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Complex Nested Data Extraction with Instructor\nDESCRIPTION: Demonstrates advanced usage of Instructor for extracting complex nested data structures with multiple models and optional fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/structured_outputs.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n\nclass Contact(BaseModel):\n    email: Optional[str] = None\n    phone: Optional[str] = None\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    occupation: str\n    address: Address\n    contact: Contact\n    skills: List[str] = Field(description=\"List of professional skills\")\n\nperson = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n        Extract detailed information for this person:\n        John Smith is a 42-year-old software engineer living at 123 Main St, San Francisco, CA 94105.\n        His email is john.smith@example.com and phone is 555-123-4567.\n        John is skilled in Python, JavaScript, and cloud architecture.\n        \"\"\",\n        }\n    ],\n    response_model=Person,\n)\n\nprint(f\"Name: {person.name}\")\nprint(f\"Location: {person.address.city}, {person.address.state}\")\nprint(f\"Skills: {', '.join(person.skills)}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Fact Class with Source Validation in Python\nDESCRIPTION: Defines a Fact class using Pydantic that validates citations against source context using regex matching. Each fact contains a statement and supporting quotes that must exist in the original text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/exact_citations.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import Field, BaseModel, model_validator, ValidationInfo\nfrom typing import List\n\n\nclass Fact(BaseModel):\n    fact: str = Field(...)\n    substring_quote: List[str] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self, info: ValidationInfo) -> \"Fact\":\n        text_chunks = info.context.get(\"text_chunk\", None)\n        spans = list(self.get_spans(text_chunks))\n        self.substring_quote = [text_chunks[span[0] : span[1]] for span in spans]\n        return self\n\n    def get_spans(self, context):\n        for quote in self.substring_quote:\n            yield from self._get_span(quote, context)\n\n    def _get_span(self, quote, context):\n        for match in re.finditer(re.escape(quote), context):\n            yield match.span()\n```\n\n----------------------------------------\n\nTITLE: Adding Model Documentation\nDESCRIPTION: Demonstrates how to add documentation to models using docstrings and field descriptions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass Investment(BaseModel):\n    \"\"\"Represents an investment opportunity with risk and return details.\"\"\"\n    \n    name: str = Field(description=\"Name of the investment\")\n    amount: float = Field(description=\"Investment amount in USD\")\n    expected_return: float = Field(description=\"Expected annual return percentage\")\n    risk_level: str = Field(description=\"Risk level (low, medium, high)\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Self-Generated In-Context Learning for Sentiment Analysis with Instructor\nDESCRIPTION: This code demonstrates a complete implementation of Self-Generated In-Context Learning (SG-ICL) for sentiment analysis. It uses the Instructor library with OpenAI to first generate balanced examples of positive and negative reviews, then uses these examples as context to perform sentiment prediction on new inputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/few_shot/example_generation/sg_icl.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom typing import Literal\n\nn = 4  # num examples to generate per class\n\n\nclass GeneratedReview(BaseModel):\n    review: str\n    sentiment: Literal[\"positive\", \"negative\"]\n\n\nclass SentimentPrediction(BaseModel):\n    sentiment: Literal[\"positive\", \"negative\"]\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_sample(input_review, sentiment):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=GeneratedReview,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                           Generate a '{sentiment}' review similar to: {input_review}\n                           Generated review:\n                           \"\"\",\n            }\n        ],\n    )\n\n\ndef predict_sentiment(input_review, in_context_samples):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=SentimentPrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"\".join(\n                    [\n                        f\"Review: {sample.review}\\nSentiment: {sample.sentiment}\\n\\n\"\n                        for sample in in_context_samples\n                    ]\n                )\n                + f\"Review: {input_review}\\nSentiment:\",\n            }\n        ],\n    ).sentiment\n\n\nif __name__ == \"__main__\":\n    input_review = (\n        \"This movie was a rollercoaster of emotions, keeping me engaged throughout.\"\n    )\n\n    # Generate in-context samples\n    samples = [\n        generate_sample(input_review, sentiment)\n        for sentiment in ('positive', 'negative')\n        for _ in range(n)\n    ]\n    for sample in samples:\n        print(sample)\n        \"\"\"\n        review='This film was an enthralling experience from start to finish, leaving me captivated every moment.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='This film was an emotional journey that captivated me from start to finish.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='The film took me on an unforgettable journey, capturing my attention at every moment.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='This book was a riveting journey, capturing my attention from start to finish.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='The movie was a total letdown, failing to hold my interest from start to finish.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='This movie was a disjointed mess of emotions, leaving me confused throughout.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='The movie was an emotional rollercoaster, but it left me feeling more confused than engaged.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='This movie was a monotonous ride, failing to engage me at any point.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='This film was an emotional journey, captivating me from start to finish.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='This film captivated me from start to finish with its thrilling plot and emotional depth.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='This movie was a breathtaking journey, capturing my attention from start to finish.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='This movie was a chaotic mess of emotions, losing me at every turn.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='This movie was a confusing mess, leaving me disengaged throughout.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='This movie was a chore to sit through, leaving me bored most of the time.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='This movie was a mishmash of confusing scenes, leaving me frustrated throughout.' sentiment='negative'\n        \"\"\"\n\n    # Predict sentiment\n    print(predict_sentiment(input_review, samples))\n    #> positive\n```\n\n----------------------------------------\n\nTITLE: Dynamic Action Selection with Union Types\nDESCRIPTION: Implements a pattern for dynamic action selection using Union types with execute methods.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/unions.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import Union\n\n\nclass Search(BaseModel):\n    query: str\n\n    def execute(self):\n        # Implementation for search\n        return f\"Searching for: {self.query}\"\n\n\nclass Lookup(BaseModel):\n    key: str\n\n    def execute(self):\n        # Implementation for lookup\n        return f\"Looking up key: {self.key}\"\n\n\nclass Action(BaseModel):\n    action: Union[Search, Lookup]\n\n    def execute(self):\n        return self.action.execute()\n```\n\n----------------------------------------\n\nTITLE: Using Optional Attributes in Pydantic Models\nDESCRIPTION: This code snippet shows how to use Python's Optional type with Pydantic models to handle nullable fields. It defines a UserDetail class with an optional role field, setting a default value of None to prevent undesired defaults like empty strings.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n```\n\n----------------------------------------\n\nTITLE: Async Support with Gemini AI\nDESCRIPTION: Demonstrates using async support with the Gemini SDK to make non-blocking requests in asynchronous applications. The example shows extracting structured user information using an async function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nimport instructor\nfrom google import genai\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_user():\n    client = genai.Client()\n    client = instructor.from_genai(\n        client, mode=instructor.Mode.GENAI_TOOLS, use_async=True\n    )\n\n    response = await client.chat.completions.create(\n        model=\"gemini-2.0-flash-001\",\n        messages=[{\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"}],\n        response_model=User,\n    )\n    return response\n\n\nprint(asyncio.run(extract_user()))\n#> name = Jason age= 25\n```\n\n----------------------------------------\n\nTITLE: Image Analysis with OpenAI Vision\nDESCRIPTION: Example showing how to analyze images using OpenAI's vision capabilities with Instructor\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import Image\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\n\nclass ImageDescription(BaseModel):\n    objects: list[str] = Field(..., description=\"The objects in the image\")\n    scene: str = Field(..., description=\"The scene of the image\")\n    colors: list[str] = Field(..., description=\"The colors in the image\")\n\n\nclient = instructor.from_openai(OpenAI())\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/image.jpg\"\n# Multiple ways to load an image:\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=ImageDescription,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"What is in this image?\",\n                Image.from_url(url),\n            ],\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Least-to-Most Prompting with Instructor and OpenAI\nDESCRIPTION: A complete Python implementation of the Least-to-Most prompting technique using Instructor and OpenAI. The code demonstrates how to decompose a complex problem into subquestions, solve them sequentially, and use previous answers to solve subsequent questions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/decomposition/least_to_most.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom typing import Iterable\n\n\nclass Subquestion(BaseModel):\n    question: str\n\n\nclass Answer(BaseModel):\n    answer: int\n\n\nclass SubquestionWithAnswers(BaseModel):\n    question: str\n    answer: int\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef decompose(question):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Subquestion],\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Break this question down into subquestions to solve sequentially: {question}\",\n            }\n        ],\n    )\n\n\ndef solve(question, solved_questions, original_question):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Answer,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    <original_question>\n                    {original_question}\n                    </original_question>\n\n                    <solved_subquestions>\n                    {solved_questions}\n                    </solved_subquestions>\n\n                    Solve this next subquestion: {question}\n                    \"\"\",\n            }\n        ],\n    ).answer\n\n\nif __name__ == \"__main__\":\n    question = \"Four years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years old, how old is Kody?\"\n\n    # Stage 1: Decompose Question into Subquestions\n    subquestions = decompose(question)\n\n    # Stage 2: Sequentially Solve Subquestions\n    solved_questions = []\n    for subquestion in subquestions:\n        solved_questions.append(\n            SubquestionWithAnswers(\n                question=subquestion.question,\n                answer=solve(subquestion, solved_questions, question),\n            )\n        )\n\n    # Print\n    for item in solved_questions:\n        print(f\"{item.question} {item.answer}\")\n        #> How old is Mohamed currently? 60\n        #> How old was Mohamed four years ago? 56\n        #> How old was Kody four years ago if he was half as old as Mohamed? 28\n        #> How old is Kody currently? 32\n```\n\n----------------------------------------\n\nTITLE: Implementing Tab-CoT Structured Reasoning with Python and Instructor\nDESCRIPTION: Complete implementation of structured reasoning using Instructor and Pydantic models. The code sets up data models for reasoning steps, handles query processing, and formats the response as a structured table. It includes a practical example of solving a bakery inventory problem.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/thought_generation/chain_of_thought_zero_shot/tab_cot.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass ReasoningStep(BaseModel):\n    step: int = Field(description=\"The step number\")\n    subquestion: str = Field(description=\"Subquestion to solve\")\n    procedure: str = Field(\n        description=\"\"\"Any intermediate computation\n        that was done in the reasoning process. Leave\n        empty if no computation is needed\"\"\",\n    )\n    result: str\n\n\nclass Response(BaseModel):\n    reasoning: list[ReasoningStep] = Field(\n        description=\"reasoning steps to derive answer\",\n    )\n    correct_answer: int\n\n\ndef generate_structured_reasoning_response(query: str, context: str):\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                <system>\n                    <role>expert Question Answering system</role>\n                    <instruction>Make sure to output your reasoning in structured reasoning steps before generating a response to the user's query.</instruction>\n                </system>\n\n                <context>\n                    {context}\n                </context>\n\n                <query>\n                    {query}\n                </query>\n                \"\"\"\n                ),\n            },\n        ],\n    )\n    return response\n\n\nif __name__ == \"__main__\":\n    query = \"How many loaves of bread did they have left?\"\n    context = \"\"\"\n    The bakers at the Beverly Hills Bakery baked\n    200 loaves of bread on Monday morning. They\n    sold 93 loaves in the morning and 39 loaves\n    in the afternoon. A grocery store returned 6\n    unsold loaves.\n    \"\"\"\n\n    response = generate_structured_reasoning_response(query, context)\n    print(response.model_dump_json(indent=2))\n```\n\n----------------------------------------\n\nTITLE: Implementing Faithful Chain of Thought with Instructor and OpenAI\nDESCRIPTION: This code demonstrates how to use Instructor with OpenAI to generate executable reasoning steps for problem-solving. It defines a Pydantic model for reasoning steps, implements a function to generate these steps from a query, and executes the generated Python code to arrive at an answer.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/decomposition/faithful_cot.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass ReasoningStep(BaseModel):\n    id: int = Field(description=\"Unique ID\")\n    rationale: list[str] = Field(\n        description=\"\"\"Specific sections from prior reasoning\n        steps or the context that ground this reasoning step\"\"\"\n    )\n    dependencies: list[int] = Field(\n        description=\"\"\"IDs of prior reasoning steps that this\n        reasoning step depends on\"\"\"\n    )\n    eval_string: str = Field(\n        description=\"\"\"Python Code to execute to generate the\n        final evaluation\"\"\"\n    )\n\n\ndef generate_reasoning_steps(query: str) -> list[ReasoningStep]:\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are a world class AI who excels at\n                generating reasoning steps to answer a\n                question. You will be given a question\n                and you will generate a list of reasoning\n                steps that are needed to answer the\n                question.\n\n                At each point you should either\n                - declare a variable to be referenced\n                later on\n                - combine multiple variables together to\n                generate a new result that you should\n                store in another variable\n\n                The final answer should be stored in a\n                variable called `answer`.\n                \"\"\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n        model=\"gpt-4o\",\n        response_model=list[ReasoningStep],\n    )\n\n\nif __name__ == \"__main__\":\n    steps = generate_reasoning_steps(\n        \"\"\"If there are 3 cars in the parking lot and 2 more\n        cars arrive, how many cars are in the parking lot\n        after another 2 more arrive?\"\"\"\n    )\n\n    code = \"\\n\".join([step.eval_string for step in steps])\n    print(code)\n    \"\"\"\n    initial_cars = 3\n    arriving_cars = 2\n    cars_after_first_arrival = initial_cars + arriving_cars\n    final_car_count = cars_after_first_arrival + 2\n    answer = final_car_count\n    \"\"\"\n    exec(code)\n\n    local_vars = {}\n    exec(code, {}, local_vars)\n    print(local_vars.get(\"answer\"))\n    #> 7\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor and Using Jinja Templates\nDESCRIPTION: This snippet demonstrates how to initialize an OpenAI client with Instructor, define a Pydantic model, and use Jinja templating in the prompt. It shows how to pass context to the templating engine and extract information using a custom model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/templating.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"Extract the information from the\n        following text: `{{ data }}`\"\"\",\n        },\n    ],\n    response_model=User,\n    context={\"data\": \"John Doe is thirty years old\"},\n)\n\nprint(resp)\n#> name='John Doe' age=30\n```\n\n----------------------------------------\n\nTITLE: Implementing Universal Self Consistency with Instructor and OpenAI\nDESCRIPTION: A comprehensive implementation of Universal Self Consistency that uses Instructor with OpenAI to generate multiple reasoning paths and then select the most consistent one. It defines Pydantic models for responses, generates batch responses asynchronously, and uses a second LLM call to select the most consistent answer based on majority consensus.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/universal_self_consistency.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field, ValidationInfo, field_validator\nimport instructor\nfrom textwrap import dedent\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass Response(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n\nclass SelectedResponse(BaseModel):\n    most_consistent_response_id: int = Field(\n        description=\"\"\"The ID of the most consistent response that\n        was provided\"\"\"\n    )\n\n    @field_validator(\"most_consistent_response_id\")\n    @classmethod\n    def validate_id(cls, v: int, info: ValidationInfo):\n        context = info.context\n        number_responses = context.get(\"number_responses\", float(\"inf\"))\n\n        if v > number_responses:\n            raise ValueError(\n                f\"\"\"Most consistent response ID {v} is greater than the\n                number of responses {number_responses}. Please return a\n                valid id between 0 and {number_responses-1}\"\"\"\n            )\n        return v\n\n\nasync def generate_response(query: str) -> Response:\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[{\"role\": \"user\", \"content\": query}],\n    )\n\n\nasync def generate_batch_responses(query: str, no_responses: int):\n    coros = [generate_response(query) for _ in range(no_responses)]\n    return await asyncio.gather(*coros)\n\n\nasync def select_consistent_response(responses: list[Response], query: str):\n    formatted_responses = \"\\n\".join(\n        [\n            f\"Response {idx}: {response.chain_of_thought}. {response.answer}\"\n            for idx, response in enumerate(responses)\n        ]\n    )\n\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=SelectedResponse,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                <user query>\n                {query}\n                </user query>\n\n                {formatted_responses}\n\n                Evaluate these responses.\n                Select the most consistent response based on majority\n                consensus\n                \"\"\"\n                ),\n            }\n        ],\n        validation_context={\"number_responses\": len(responses)},\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"The three-digit number 'ab5' is divisible by 3. How many different\n     three-digit numbers can 'ab5' represent?\"\"\"\n    responses = asyncio.run(generate_batch_responses(query, 3))\n\n    for response in responses:\n        print(response.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"chain_of_thought\": \"A number is divisible by 3 if\n          the sum of its digits is divisible by 3. Given the\n          number 'ab5', we need to check how many different\n          values of 'a' and 'b', where both are digits (0-9)\n          can make the sum divisible by 3.\\n\\nThe sum of the\n          digits is a + b + 5.\\n\\nWe need to find pairs (a, b)\n          such that (a + b + 5) % 3 == 0.\",\n          \"answer\": \"30\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"chain_of_thought\": \"A number is divisible by 3 if\n          the sum of its digits is divisible by 3. Let's\n          denote the digits a and b. The number 'ab5' has\n          digits a, b, and 5. Therefore, the sum of the\n          digits is a + b + 5. Since the number is divisible\n          by 3, a + b + 5 must be divisible by 3.\\n\\nNow,\n          since a and b are single digits (0-9), we need to\n          find pairs (a, b) such that a + b + 5 is divisible\n          by 3. We will evaluate all possible combinations of\n          values for a and b to count how many valid pairs\n          (a, b) exist.\\n\\nLet's start by considering b's\n          values:\\n1. If b = 0, then a + 5 must be divisible\n          by 3.\\n2. If b = 1, then a + 6 must be divisible by\n          3.\\n3. If b = 2, then a + 7 must be divisible by\n          3.\\n4. If b = 3, then a + 8 must be divisible by\n          3.\\n5. If b = 4, then a + 9 must be divisible by\n          3.\\n6. If b = 5, then a + 10 must be divisible by\n          3.\\n7. If b = 6, then a + 11 must be divisible by\n          3.\\n8. If b = 7, then a + 12 must be divisible by\n          3.\\n9. If b = 8, then a + 13 must be divisible by\n          3.\\n10. If b = 9, then a + 14 must be divisible by\n          3.\\n\\nWe will find all corresponding a values for\n          each b and count the valid combinations.\\n\",\n          \"answer\": \"There are 30 different three-digit\n          numbers that 'ab5' can represent.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"chain_of_thought\": \"A number is divisible by 3 if\n          the sum of its digits is divisible by 3. The given\n          number is in the form 'ab5', where 'a' and 'b' are\n          digits from 0 to 9. To find the total number of\n          different three-digit numbers that 'ab5' can\n          represent, we need to determine all possible digit\n          combinations for 'a' and 'b' such that 'a + b + 5'\n          is divisible by 3.\",\n          \"answer\": \"30\"\n        }\n        \"\"\"\n\n    selected_response = asyncio.run(select_consistent_response(responses, query))\n    print(selected_response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"most_consistent_response_id\": 0\n    }\n    \"\"\"\n\n    print(\n        responses[selected_response.most_consistent_response_id].model_dump_json(\n            indent=2\n        )\n    )\n    \"\"\"\n    {\n      \"chain_of_thought\": \"A number is divisible by 3 if the sum of its digits is divisible by 3. Given the number 'ab5', we need to\n      check how many different values of 'a' and 'b', where both are digits (0-9) can make the sum divisible by 3.\\n\\nThe sum of the\n      digits is a + b + 5.\\n\\nWe need to find pairs (a, b) such that (a + b + 5) % 3 == 0.\",\n      \"answer\": \"30\"\n    }\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Instructor Installation\nDESCRIPTION: Python code example demonstrating how to verify the Instructor installation by performing a simple extraction using OpenAI.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_openai(OpenAI())\nperson = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Person,\n    messages=[\n        {\"role\": \"user\", \"content\": \"John Doe is 30 years old\"}\n    ]\n)\n\nprint(f\"Name: {person.name}, Age: {person.age}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Instructor with OpenAI Client and Hooks\nDESCRIPTION: Demonstrates configuring an OpenAI client with Instructor, setting up event hooks for logging and error handling, and extracting structured user information using a Pydantic model\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n\n# Initialize the OpenAI client with Instructor\nclient = instructor.from_openai(OpenAI())\n\n\n# Define hook functions\ndef log_kwargs(**kwargs):\n    print(f\"Function called with kwargs: {kwargs}\")\n\n\ndef log_exception(exception: Exception):\n    print(f\"An exception occurred: {str(exception)}\")\n\n\nclient.on(\"completion:kwargs\", log_kwargs)\nclient.on(\"completion:error\", log_exception)\n\nuser_info = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=UserInfo,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract the user name: 'John is 20 years old'\"}\n    ],\n)\n\nprint(f\"Name: {user_info.name}, Age: {user_info.age}\")\n```\n\n----------------------------------------\n\nTITLE: Advanced OpenAI Client Configuration with Instructor\nDESCRIPTION: Shows advanced configuration options for the OpenAI client using Instructor, including setting max retries, API key, and organization ID.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/client_setup.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(\n    OpenAI(),\n    mode=instructor.Mode.TOOLS,\n    max_retries=2,  # Number of retries for validation failures\n)\n\n# With API key explicitly defined\nclient = instructor.from_openai(\n    OpenAI(api_key=\"your-api-key\"),\n    mode=instructor.Mode.JSON\n)\n\n# With organization ID\nclient = instructor.from_openai(\n    OpenAI(\n        api_key=\"your-api-key\",\n        organization=\"org-...\"\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Contrastive Chain of Thought using Instructor and OpenAI\nDESCRIPTION: Python implementation of Contrastive Chain of Thought using Instructor library and OpenAI. The code defines a ChainOfThought model and a function that processes queries with both correct and incorrect reasoning examples to generate improved responses. Uses GPT-4 model and Pydantic for type validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/thought_generation/chain_of_thought_few_shot/contrastive.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass ChainOfThought(BaseModel):\n    chain_of_thought: str = Field(description=\"Incorrect reasoning for the answer\")\n    correct_answer: str\n\n\ndef contrastive_chain_of_thought(\n    query: str,\n    context: str,\n    example_prompt: str,\n    correct_examples: list[str],\n    incorrect_examples: list[str],\n):\n    correct_example_prompt = \"\\n\".join(\n        [f\"<Explanation>{example}</Explanation>\" for example in correct_examples]\n    )\n    incorrect_example_prompt = \"\\n\".join(\n        [\n            f\"<WrongExplanation>{example}</WrongExplanation>\"\n            for example in incorrect_examples\n        ]\n    )\n    \"\"\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ChainOfThought,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n            <prompt>\n                <role>system</role>\n                <context>\n                You are an expert question answering AI System.\n\n                You are about to be given some examples of incorrect\n                and correct reasoning for a question. You will then\n                be asked to correctly reason through another question\n                to generate a valid response.\n                </context>\n\n                <question>{example_prompt}</question>\n\n                <Explanations>\n                    {correct_example_prompt}\n                    {incorrect_example_prompt}\n                </Explanations>\n                <context>{context}</context>\n                <question>{query}</question>\n\n            </prompt>\n            \"\"\"\n                ),\n            }\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Client in Python with Instructor\nDESCRIPTION: Demonstrates how to set up an OpenAI client using Instructor, including both default TOOLS mode and JSON mode.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/client_setup.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\n# Default mode (TOOLS)\nclient = instructor.from_openai(OpenAI())\n\n# With JSON mode\nclient = instructor.from_openai(\n    OpenAI(),\n    mode=instructor.Mode.JSON  # Use JSON mode instead\n)\n```\n\n----------------------------------------\n\nTITLE: Using Enumerations in Pydantic Models for Standardized Fields\nDESCRIPTION: This snippet demonstrates how to use Enums for standardized fields in Pydantic models. It includes an 'OTHER' option as a fallback to signal uncertainty, helping prevent data misalignment.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum, auto\nfrom pydantic import BaseModel, Field\n\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Outputs with Anthropic and Instructor\nDESCRIPTION: Example of using Instructor with Anthropic's Claude model to generate structured output using Pydantic models. The code demonstrates how to patch the Anthropic client with Instructor and define Pydantic models to extract specific information from AI responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/structured-output-anthropic.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import List\nimport anthropic\nimport instructor\n\n# Patch the Anthropic client with Instructor\nanthropic_client = instructor.from_anthropic(create=anthropic.Anthropic())\n\n\n# Define your Pydantic models\nclass Properties(BaseModel):\n    name: str\n    value: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    properties: List[Properties]\n\n\n# Use the patched client to generate structured output\nuser_response = anthropic_client(\n    model=\"claude-3-7-sonnet-latest\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Create a user for a model with a name, age, and properties.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(user_response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"properties\": [\n    { \"name\": \"favorite_color\", \"value\": \"blue\" }\n  ]\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain of Verification (CoVe) with Instructor and OpenAI\nDESCRIPTION: A complete implementation of the Chain of Verification (CoVe) technique using instructor and OpenAI's API. The code defines Pydantic models for structured outputs and implements a four-step verification process: generating an initial response, creating verification questions, independently answering those questions, and validating the original response against the verification data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/self_criticism/chain_of_verification.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass QueryResponse(BaseModel):\n    correct_answer: str\n\n\nclass ValidationQuestions(BaseModel):\n    question: list[str] = Field(\n        description=\"\"\"A list of questions that need to be\n        answered to validate the response\"\"\"\n    )\n\n\nclass ValidationAnswer(BaseModel):\n    answer: str\n\n\nclass FinalResponse(BaseModel):\n    correct_answer: str\n\n\nasync def generate_initial_response(query: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=QueryResponse,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert question answering system\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n    )\n\n\nasync def generate_verification_questions(llm_response: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ValidationQuestions,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an expert AI system that excels at\n                generating follow up questions to validate a response.\n                These questions should validate key assumptions, facts\n                and other important portions of the generated response\"\"\",\n            },\n            {\"role\": \"user\", \"content\": llm_response},\n        ],\n    )\n\n\nasync def generate_verification_response(questions: list[str]):\n    async def verify_question(question: str) -> tuple[ValidationAnswer, str]:\n        return (\n            await client.chat.completions.create(\n                model=\"gpt-4o\",\n                response_model=ValidationAnswer,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"\"\"You are an expert AI system that\n                        excels at answering validation questions.\"\"\",\n                    },\n                    {\"role\": \"user\", \"content\": question},\n                ],\n            ),\n            question,\n        )\n\n    coros = [verify_question(question) for question in questions]\n    return await asyncio.gather(*coros)\n\n\nasync def generate_final_response(\n    answers: list[tuple[ValidationAnswer, str]],\n    initial_response: QueryResponse,\n    original_query: str,\n):\n    formatted_answers = \"\\n\".join(\n        [f\"Q: {question}\\nA: {answer.answer}\" for answer, question in answers]\n    )\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=FinalResponse,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an expert AI system that excels at\n                validating and verifying if an initial answer answers an\n                initial query based off some Verification Questions and\n                Answers provided. Return the original answer if it is\n                valid else generate a new response off the verification\n                questions and answers provided.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Initial query: {original_query}\n                Initial Answer : {initial_response.correct_answer}\n                Verification Questions and Answers:\n                {formatted_answers}\n            \"\"\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"What was the primary cause of the Mexican-American war and how long did it last?\"\n    initial_response = asyncio.run(generate_initial_response(query))\n    print(initial_response.model_dump_json())\n    \"\"\"\n    {\"correct_answer\":\"The primary cause of the Mexican-American War was\n    the annexation of Texas by the United States and the dispute over\n    whether Texas ended at the Nueces River (as the Mexicans claimed) or\n    the Rio Grande (as the U.S. claimed). The war lasted from April 25,\n    1846, to February 2, 1848, totaling nearly two years.\"}\n    \"\"\"\n\n    verification_questions = asyncio.run(\n        generate_verification_questions(initial_response.correct_answer)\n    )\n    print(verification_questions.model_dump_json())\n    \"\"\"\n    {\"question\":[\"Is it accurate that the primary cause of the\n    Mexican-American War was the annexation of Texas by the United\n    States?\",\"Was there a dispute over whether Texas ended at the Nueces\n    River or the Rio Grande?\",\"Did the Mexican-American War last from\n    April 25, 1846, to February 2, 1848?\",\"Is it correct to state that\n    the disagreement over the Texas border was between the Nueces River\n    and the Rio Grande?\",\"Was the Mexican claim that Texas ended at the\n    Nueces River while the U.S. claimed it was at the Rio Grande?\"]}\n    \"\"\"\n\n    responses = asyncio.run(\n        generate_verification_response(verification_questions.question)\n    )\n\n    final_answer = asyncio.run(\n        generate_final_response(responses, initial_response, query)\n    )\n    print(final_answer.model_dump_json())\n    \"\"\"\n    {\"correct_answer\":\"The primary cause of the Mexican-American War was\n    the annexation of Texas by the United States and the dispute over\n    whether Texas ended at the Nueces River (as the Mexicans claimed) or\n    the Rio Grande (as the U.S. claimed). The war lasted from April 25,\n    1846, to February 2, 1848, totaling nearly two years.\"}\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Basic Instructor Usage with OpenAI\nDESCRIPTION: Demonstrates core concept of using Instructor with OpenAI client for structured data extraction using Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Define your output structure\nclass User(BaseModel):\n    name: str\n    age: int\n\n# Patch your client\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract the user: John Doe is 30 years old.\"}\n    ]\n)\n\nprint(user.name)  # \"John Doe\"\nprint(user.age)   # 30\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Self-Verification Pipeline with Instructor\nDESCRIPTION: Complete implementation of a self-verification system using the instructor library. The code demonstrates forward reasoning to generate multiple candidates and backward verification using True-False Item Verification (TFV). It includes model definitions, LLM querying, response rewriting, and verification scoring.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/self_criticism/self_verification.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom typing import Literal\n\nclient = instructor.from_openai(OpenAI())\n\nn = 3  # Number of candidates to generate\nk = 5  # Number of times to verify\n\n\nclass Date(BaseModel):\n    month: int\n    day: int\n\n\nclass Candidate(BaseModel):\n    reasoning_steps: list[str]\n    month: str\n\n\nclass Rewritten(BaseModel):\n    declarative: str\n\n\nclass Verification(BaseModel):\n    correct: Literal[\"True\", \"False\"]\n\n\ndef query_llm(query, model):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=model,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Think step by step: {query}\",\n            }\n        ],\n    )\n\n\ndef rewrite(query, candidate):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Rewritten,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Please change the questions and answers into complete declarative sentences\n                    {query}\n                    The answer is {candidate.month}.\n                \"\"\",\n            }\n        ],\n    )\n\n\ndef verify(question):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Verification,\n        messages=[{\"role\": \"user\", \"content\": question}],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"What month is it now if it has been 3 weeks, 10 days, and 2 hours since May 1, 2024 6pm?\"\n\n    # Step 1: Forward Reasoning\n    candidates = [query_llm(query, Candidate) for _ in range(n)]\n\n    # Step 2: Backwards Verification\n    for candidate in candidates:\n        # 2.a Rewrite\n        rewritten = rewrite(query, candidate)\n        # 2.b Construct new questions\n        question = f\"{rewritten.declarative} Do it is correct (True or False)?\"\n        # 2.c Compute verification score\n        scores = [verify(question).correct for _ in range(k)]\n        verification_score = sum(1 for s in scores if s == \"True\")\n\n        print(f\"Candidate: {candidate.month}, Verification Score: {verification_score}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Step-Back Prompting with Python, OpenAI and Instructor\nDESCRIPTION: A complete implementation of step-back prompting using Python, OpenAI's API, and the Instructor library. The code defines Pydantic models for structuring responses and includes three main functions: generating step-back questions, querying with the abstract question, and producing final responses with context. Uses GPT-4 as the underlying model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom typing import Iterable, Literal\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Stepback(BaseModel):\n    original_question: str\n    abstract_question: str\n\n\nclass Education(BaseModel):\n    degree: Literal[\"Bachelors\", \"Masters\", \"PhD\"]\n    school: str\n    topic: str\n    year: int\n\n\nclass Response(BaseModel):\n    school: str\n\n\ndef generate_stepback_question():\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Stepback,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                You are an expert at world knowledge. Your task is to step back\n                and paraphrase a question to a more generic step-back question,\n                which is easier to answer.\n\n                Here are a few examples:\n                Original Question: Which position did Knox Cunningham hold from\n                May 1955 to Apr 1956?\n                Step-back Question: Which positions has Knox Cunningham held in\n                his career?\n                Original Question: Who was the spouse of Anna Karina from 1968\n                to 1974?\n                Step-back Question: Who were the spouses of Anna Karina?\n                Original Question: Which team did Thierry Audel play for from\n                2007 to 2008?\n                Step-back Question: Which teams did Thierry Audel play for in\n                his career?\n\n                Now, generate the step-back question for the following question:\n                Estella Leopold went to which school between Aug 1954 and\n                Nov 1954?\n                \"\"\",\n            },\n        ],\n    )\n\n\ndef ask_stepback_question(stepback):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Education],\n        messages=[\n            {\"role\": \"user\", \"content\": stepback.abstract_question},\n        ],\n    )\n\n\ndef get_final_response(stepback, stepback_response):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Q: {stepback.abstract_question},\n                A: {stepback_response}\n                Q: {stepback.original_question}\n                A:\n                \"\"\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    # Generate the step-back question\n    stepback = generate_stepback_question()\n    print(stepback.original_question)\n    #> Estella Leopold went to which school between Aug 1954 and Nov 1954?\n    print(stepback.abstract_question)\n    #> Which schools did Estella Leopold attend in her life?\n\n    # Ask the step-back question\n    stepback_response = ask_stepback_question(stepback)\n    for item in stepback_response:\n        print(item)\n\n    # Ask the original question, appended with context from the stepback response\n    print(get_final_response(stepback, stepback_response))\n    #> school='Yale University'\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Planning with OpenAI\nDESCRIPTION: Implements the query planning functionality using OpenAI's API and the previously defined Pydantic models. Includes system setup and the main query planning function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/planning-tasks.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\ndef query_planner(question: str) -> QueryPlan:\n    PLANNING_MODEL = \"gpt-4o-mini\"\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class query planning algorithm capable ofbreaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Consider: {question}\\nGenerate the correct query plan.\",\n        },\n    ]\n\n    root = client.chat.completions.create(\n        model=PLANNING_MODEL,\n        temperature=0,\n        response_model=QueryPlan,\n        messages=messages,\n        max_tokens=1000,\n    )\n    return root\n```\n\n----------------------------------------\n\nTITLE: Implementing Plan and Solve Reasoning with Instructor and OpenAI\nDESCRIPTION: This code demonstrates the two-step Plan and Solve approach for mathematical reasoning tasks. It defines Pydantic models for capturing reasoning and answers, implements functions to generate reasoning chains and extract final answers, and includes a complete example solving a percentage problem. The highlighted sections show the core Plan and Solve prompting strategy.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/decomposition/plan_and_solve.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Reasoning(BaseModel):\n    chain_of_thought: str\n\n\nclass Response(BaseModel):\n    correct_answer: str\n\n\ndef generate_reasoning(query: str):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                <user query>\n                {query}\n                </user query>\n\n                Let's first understand the problem,\n                extract relevant variables and their\n                corresponding numerals, and make a\n                complete plan. Then, let's carry out\n                the plan, calculate intermediate\n                variables (pay attention to correct\n                numerical calculation and commonsense),\n                solve the problem step by step, and\n                show the answer.\n                \"\"\",\n            },\n        ],\n        response_model=Reasoning,\n        model=\"gpt-4o\",\n    )\n\n\ndef extract_answer(query: str, reasoning: Reasoning):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                <user query>\n                    {query}\n                </user query>\n\n                Let's first understand the problem,\n                extract relevant variables and their\n                corresponding numerals, and make a\n                complete plan. Then, let's carry out\n                the plan, calculate intermediate\n                variables (pay attention to correct\n                numerical calculation and commonsense),\n                solve the problem step by step, and\n                show the answer.\n\n                <reasoning>\n                {reasoning.chain_of_thought}\n                </reasoning>\n\n                Therefore the answer (arabic numerals) is\n                \"\"\",\n            }\n        ],\n        model=\"gpt-4o\",\n        response_model=Response,\n    )\n\n\nif __name__ == \"__main__\":\n    query = (\n        \"In a dance class of 20 students, 20% enrolled \"\n        \"in contemporary dance, 25% of the remaining \"\n        \"enrolled in jazz dance and the rest enrolled \"\n        \"in hip-hop dance. What percentage of the entire \"\n        \"students enrolled in hip-hop dance?\"\n    )\n\n    reasoning = generate_reasoning(query)\n    print(reasoning.model_dump_json(indent=2))\n    \"\"\"\n    {\n    \"chain_of_thought\": \"Let's first break down the\n    problem:\\n\\n1. Total number of students = 20\\n2.\n    Percentage enrolled in contemporary dance = 20%\\n\\n\n    Step-by-Step Plan:\\n1. Calculate the number of\n    students enrolled in contemporary dance.\\n2.\n    Calculate the remaining students after contemporary\n    dance enrollment.\\n3. Calculate the percentage and\n    number of students from the remaining who enrolled in\n    jazz dance.\\n4. Determine the remaining students who\n    enrolled in hip-hop dance.\\n5. Finally, calculate the\n    percentage of the entire students who enrolled in\n    hip-hop dance.\\n\\nLet's carry out the plan:\\n\\n1.\n    Number of students enrolled in contemporary dance =\n    20% of 20 = (20/100) * 20 = 4\\n2. Remaining students\n    after contemporary = 20 - 4 = 16\\n3. Percentage of\n    remaining students enrolled in jazz dance = 25%\\n\n    Number of students enrolled in jazz dance = 25% of 16\n    = (25/100) * 16 = 4\\n4. Remaining students after\n    contemporary and jazz = 16 - 4 = 12\\n5. The number of\n    students enrolled in hip-hop dance = 12\\n6.\n    Percentage of entire students enrolled in hip-hop =\n    (Number of hip-hop students / Total students) *\n    100\\n   Percentage = (12 / 20) * 100 = 60%\\n\\nThus,\n    60% of the entire students enrolled in hip-hop dance.\"\n    }\n    \"\"\"\n\n    response = extract_answer(query, reasoning)\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"correct_answer\": \"60\"\n    }\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Gemini Files API for Audio Processing\nDESCRIPTION: Shows direct usage of the Gemini Files API for audio processing, demonstrating how to upload files and use them as parameters in chat completions with structured responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom google import genai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass Summary(BaseModel):\n    summary: str\n\n\nclient = genai.Client()\nclient = instructor.from_genai(client, mode=instructor.Mode.GENAI_TOOLS)\n\nfile1 = client.files.upload(\n    file=\"./gettysburg.wav\",\n)\n\n# As a parameter\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    system=\"Summarise the audio file.\",\n    messages=[\n        file1,\n    ],\n    response_model=Summary,\n)\n\nprint(response)\n# > summary=\"Abraham Lincoln's Gettysburg Address commences by stating that 87 years prior, the founding fathers created a new nation based on liberty and equality. It goes on to say that the Civil War is testing whether a nation so conceived can survive.\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Vertex AI and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with Google's Vertex AI to extract structured data. It uses a Pydantic model for data extraction and Vertex AI's generative model client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nfrom pydantic import BaseModel\n\nvertexai.init()\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n)\n\n# note that client.chat.completions.create will also work\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=ExtractUser,\n)\n\nassert isinstance(resp, ExtractUser)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Implementing Self-Correcting Pydantic Model with OpenAI API in Python\nDESCRIPTION: This code snippet defines a UserDetails Pydantic model with a field validator for the name field. It uses the instructor library to patch the OpenAI client and extract user details from a given text. The model ensures the extracted name is in uppercase, demonstrating self-correction on validation error.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/why.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert model.name == \"JASON\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Label Classification with OpenAI and Pydantic in Python\nDESCRIPTION: This code snippet demonstrates how to implement multi-label classification for support tickets using OpenAI's API and Pydantic. It defines a custom Pydantic model for multi-label predictions, sets up the OpenAI client with instructor patch, and provides a function to classify support tickets with multiple labels.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/multiple_classification.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\n\nfrom typing import List, Literal\nfrom pydantic import BaseModel, Field\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(openai.OpenAI())\n\nLABELS = Literal[\"ACCOUNT\", \"BILLING\", \"GENERAL_QUERY\"]\n\n\nclass MultiClassPrediction(BaseModel):\n    \"\"\"\n    A few-shot example of multi-label classification:\n    Examples:\n    - \"My account is locked and I can't access my billing info.\": ACCOUNT, BILLING\n    - \"I need help with my subscription.\": ACCOUNT\n    - \"How do I change my payment method?\": BILLING\n    - \"Can you tell me the status of my order?\": BILLING\n    - \"I have a question about the product features.\": GENERAL_QUERY\n    \"\"\"\n\n    labels: List[LABELS] = Field(\n        ...,\n        description=\"Only select the labels that apply to the support ticket.\",\n    )\n\n\ndef multi_classify(data: str) -> MultiClassPrediction:\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=MultiClassPrediction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are a support agent at a tech company. Only select the labels that apply to the support ticket.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following support ticket: <text>{data}</text>\",\n            },\n        ],\n    )  # type: ignore\n\n\nif __name__ == \"__main__\":\n    ticket = \"My account is locked and I can't access my billing info.\"\n    prediction = multi_classify(ticket)\n    assert {\"ACCOUNT\", \"BILLING\"} == {label for label in prediction.labels}\n    print(\"input:\", ticket)\n    #> input: My account is locked and I can't access my billing info.\n    print(\"labels:\", LABELS)\n    #> labels: typing.Literal['ACCOUNT', 'BILLING', 'GENERAL_QUERY']\n    print(\"prediction:\", prediction)\n    #> prediction: labels=['ACCOUNT', 'BILLING']\n```\n\n----------------------------------------\n\nTITLE: Basic User Profile Validation with Instructor and OpenAI\nDESCRIPTION: Demonstrates how to create a validated user profile model using Pydantic and Instructor. The example shows age validation ensuring users are at least 13 years old, with automatic retry functionality if validation fails.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/basics.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\n# Define a model with validation\nclass UserProfile(BaseModel):\n    name: str\n    age: int = Field(ge=13, description=\"User's age in years\")\n\n# Extract validated data\nclient = instructor.from_openai(OpenAI())\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"My name is Jane Smith and I'm 25 years old.\"}\n    ],\n    response_model=UserProfile\n)\n\nprint(f\"User: {response.name}, Age: {response.age}\")\n```\n\n----------------------------------------\n\nTITLE: Simple Data Extraction with Instructor and Pydantic\nDESCRIPTION: Demonstrates basic structured data extraction using Instructor. This example extracts contact information from unstructured text into a strongly-typed Pydantic model, providing a simple entry point for using the library.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclass Contact(BaseModel):\n    name: str\n    email: str\n    phone: str\n\nclient = instructor.from_openai(OpenAI())\n\ncontact = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Contact,\n    messages=[\n        {\"role\": \"user\", \"content\": \"My name is John Doe, email is john@example.com and phone is 555-123-4567\"}\n    ]\n)\n\nprint(contact.model_dump_json(indent=2))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor Patching for Tool Calling\nDESCRIPTION: This snippet demonstrates how to initialize an OpenAI client with Instructor patching using the TOOLS mode, which is the recommended mode for OpenAI clients.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n```\n\n----------------------------------------\n\nTITLE: Implementing Retries and Error Handling with Instructor for OpenAI API\nDESCRIPTION: Demonstrates how to configure retry logic for validation failures when extracting structured data from OpenAI completions. The example defines a strict user model with validation constraints and configures the client to retry up to 3 times if validation fails.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclass StrictUser(BaseModel):\n    name: str\n    age: int = Field(gt=0, lt=150)\n    email: str = Field(pattern=r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\")\n\n# Configure max retries\nclient = instructor.from_openai(\n    OpenAI(),\n    max_retries=3  # Will retry up to 3 times if validation fails\n)\n\ntry:\n    user = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=StrictUser,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract the user: John Doe is 30 years old.\"}\n        ]\n    )\nexcept instructor.exceptions.ValidationError as e:\n    print(f\"Validation failed: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Structure Validation with Instructor\nDESCRIPTION: This snippet demonstrates how to add validation to nested structures using Instructor. It includes field-level validation for email format and model-level validation to check if the name matches the email address.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/nested_structure.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, field_validator, model_validator\nimport instructor\nfrom openai import OpenAI\nimport re\n\nclient = instructor.from_openai(OpenAI())\n\nclass EmailContact(BaseModel):\n    email: str\n    \n    @field_validator('email')\n    @classmethod\n    def validate_email(cls, v):\n        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n        if not re.match(pattern, v):\n            raise ValueError(\"Invalid email format\")\n        return v\n\nclass Customer(BaseModel):\n    name: str\n    contact: EmailContact  # Nested structure with its own validation\n    \n    @model_validator(mode='after')\n    def validate_name_email_match(self):\n        name_part = self.name.lower().split()[0]\n        if name_part not in self.contact.email.lower():\n            print(f\"Warning: Email {self.contact.email} may not match name {self.name}\")\n        return self\n```\n\n----------------------------------------\n\nTITLE: Implementing Confidential Document Classification System\nDESCRIPTION: Complete Python implementation of a document classification system using Llama-cpp-python and instructor. Includes model initialization, query type definitions, and processing logic.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/local_classification.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_cpp import Llama  # type: ignore\nimport instructor\nfrom pydantic import BaseModel\nfrom enum import Enum\nfrom typing import Optional\n\nllm = Llama.from_pretrained(  # type: ignore\n    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",  \n    filename=\"*Q4_K_M.gguf\",\n    verbose=False,  \n    n_gpu_layers=-1,  \n)\n\ncreate = instructor.patch(\n    create=llm.create_chat_completion_openai_v1,  # type: ignore\n)\n\n\n# Define query types for document-related inquiries\nclass QueryType(str, Enum):\n    DOCUMENT_CONTENT = \"document_content\"\n    LAST_MODIFIED = \"last_modified\"\n    ACCESS_PERMISSIONS = \"access_permissions\"\n    RELATED_DOCUMENTS = \"related_documents\"\n\n\n# Define the structure for query responses\nclass QueryResponse(BaseModel):\n    query_type: QueryType\n    response: str\n    additional_info: Optional[str] = None\n\n\ndef process_confidential_query(query: str) -> QueryResponse:\n    prompt = f\"\"\"Analyze the following confidential document query and provide an appropriate response:\n    Query: {query}\n\n    Determine the type of query (document content, last modified, access permissions, or related documents),\n    provide a response, and include a confidence score and any additional relevant information.\n    Remember, you're handling confidential data, so be cautious about specific details.\n    \"\"\"\n\n    return create(\n        response_model=QueryResponse,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a secure AI assistant trained to handle confidential document queries.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n\n\n# Sample confidential document queries\nconfidential_queries = [\n    \"What are the key findings in the Q4 financial report?\",\n    \"Who last accessed the merger proposal document?\",\n    \"What are the access permissions for the new product roadmap?\",\n    \"Are there any documents related to Project X's budget forecast?\",\n    \"When was the board meeting minutes document last updated?\",\n]\n\n# Process each query and print the results\nfor query in confidential_queries:\n    response: QueryResponse = process_confidential_query(query)\n    print(f\"{query} : {response.query_type}\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Basic Nested Structure using Instructor and OpenAI API\nDESCRIPTION: This snippet demonstrates how to extract a simple nested structure representing a person with an address using Instructor and the OpenAI API. It defines Pydantic models for Address and Person, then uses the client to extract the data from a given text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/nested_structure.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\nfrom typing import List, Optional\n\n# Initialize the client\nclient = instructor.from_openai(OpenAI())\n\n# Define nested models\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    address: Address  # Nested structure\n\n# Extract the nested data\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        John Smith is 35 years old.\n        He lives at 123 Main Street, Boston, MA 02108.\n        \"\"\"}\n    ],\n    response_model=Person\n)\n\n# Access the nested data\nprint(f\"Name: {response.name}\")\nprint(f\"Age: {response.age}\")\nprint(f\"Address: {response.address.street}, {response.address.city}, \" \n      f\"{response.address.state} {response.address.zip_code}\")\n```\n\n----------------------------------------\n\nTITLE: Complete COSP Implementation with Example Selection in Python\nDESCRIPTION: This code provides a complete implementation of the COSP technique, including a COSPSelector class that handles response generation, metric calculation, and example selection. It computes combined quality scores to identify the most effective examples for few-shot learning.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/few_shot/cosp.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Tuple\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\nimport numpy as np\nfrom scipy.stats import entropy\n\nclass Example(BaseModel):\n    text: str\n    score: float = Field(description=\"Combined quality score\")\n    entropy: float = Field(description=\"Entropy of responses\")\n    repetitiveness: float = Field(description=\"Repetitiveness of responses\")\n\nclass COSPSelector:\n    def __init__(self, client: OpenAI, n_samples: int = 3):\n        self.client = instructor.from_openai(client)\n        self.n_samples = n_samples\n\n    def generate_responses(self, prompt: str) -> List[Response]:\n        return [\n            self.client.chat.completions.create(\n                model=\"gpt-4\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                response_model=Response\n            )\n            for _ in range(self.n_samples)\n        ]\n\n    def calculate_metrics(self, responses: List[Response]) -> Tuple[float, float]:\n        confidences = [r.confidence for r in responses]\n        entropy_score = entropy(confidences)\n\n        unique_responses = len(set(r.content for r in responses))\n        repetitiveness = 1 - (unique_responses / len(responses))\n\n        return entropy_score, repetitiveness\n\n    def select_examples(self, candidates: List[str], k: int) -> List[Example]:\n        examples = []\n\n        for text in candidates:\n            responses = self.generate_responses(text)\n            entropy_score, repetitiveness = self.calculate_metrics(responses)\n\n            # Combined score (lower is better)\n            score = entropy_score - repetitiveness\n\n            examples.append(Example(\n                text=text,\n                score=score,\n                entropy=entropy_score,\n                repetitiveness=repetitiveness\n            ))\n\n        # Sort by score (lower is better) and select top k\n        return sorted(examples, key=lambda x: x.score)[:k]\n```\n\n----------------------------------------\n\nTITLE: Pydantic Model Implementation\nDESCRIPTION: Implementation of data validation using Pydantic BaseModel with examples of validation and type casting.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/1-introduction.ipynb#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nperson = Person(name=\"Sam\", age=30)\nperson\n```\n\n----------------------------------------\n\nTITLE: Streaming with OpenAI Structured Outputs in Python\nDESCRIPTION: This example demonstrates the complexity of handling streaming with OpenAI's Structured Outputs. It shows how developers must manually handle JSON parsing with partial chunks and use a context manager, making streaming implementation challenging.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = openai.OpenAI()\nwith client.beta.chat.completions.stream(\n    response_format=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the following user: Jason is 25 years old.\",\n        },\n    ],\n    model=\"gpt-4o-mini\",\n) as stream:\n    for event in stream:\n        if event.type == \"content.delta\":\n            print(event.snapshot, flush=True, end=\"\\n\")\n            #> \n            #> {\"\n            #> {\"name\n            #> {\"name\":\"\n            #> {\"name\":\"Jason\n            #> {\"name\":\"Jason\",\"\n            #> {\"name\":\"Jason\",\"age\n            #> {\"name\":\"Jason\",\"age\":\n            #> {\"name\":\"Jason\",\"age\":25\n            #> {\"name\":\"Jason\",\"age\":25}\n            # >\n            #> {\"\n            #> {\"name\n            #> {\"name\":\"\n            #> {\"name\":\"Jason\n            #> {\"name\":\"Jason\",\"\n            #> {\"name\":\"Jason\",\"age\n            #> {\"name\":\"Jason\",\"age\":\n            #> {\"name\":\"Jason\",\"age\":25\n            #> {\"name\":\"Jason\",\"age\":25}\n```\n\n----------------------------------------\n\nTITLE: Implementing MCP Server with OpenAI Agent SDK\nDESCRIPTION: Python code demonstrating how to connect OpenAI's Agent SDK with an MCP server for Git repositories. This allows an AI agent to answer questions about a local git repository using the standardized MCP interface.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/why-care-about-mcps.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport shutil\n\nfrom agents import Agent, Runner, trace\nfrom agents.mcp import MCPServer, MCPServerStdio\n\nasync def run(mcp_server: MCPServer, directory_path: str):\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=f\"Answer questions about the git repository at {directory_path}, use that for repo_path\",\n        mcp_servers=[mcp_server],\n    )\n\n    question = input(\"Enter a question: \")\n\n    print(\"\\n\" + \"-\" * 40)\n    print(f\"Running: {question}\")\n    result = await Runner.run(starting_agent=agent, input=question)\n    print(result.final_output)\n\n    message = \"Summarize the last change in the repository.\"\n    print(\"\\n\" + \"-\" * 40)\n    print(f\"Running: {message}\")\n    result = await Runner.run(starting_agent=agent, input=message)\n    print(result.final_output)\n\nasync def main():\n    # Ask the user for the directory path\n    directory_path = input(\"Please enter the path to the git repository: \")\n\n    async with MCPServerStdio(\n        cache_tools_list=True,  # Cache the tools list, for demonstration\n        params={\"command\": \"uvx\", \"args\": [\"mcp-server-git\"]},\n    ) as server:\n        with trace(workflow_name=\"MCP Git Example\"):\n            await run(server, directory_path)\n\nif __name__ == \"__main__\":\n    if not shutil.which(\"uvx\"):\n        raise RuntimeError(\n            \"uvx is not installed. Please install it with `pip install uvx`.\"\n        )\n\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing YouTube Transcript Analysis with Instructor in Python\nDESCRIPTION: This Python code demonstrates the complete implementation of YouTube transcript analysis using the Instructor library. It includes the Chapter model definition, transcript extraction, and chapter extraction using OpenAI's GPT model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-transcripts.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\n# Set up OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n\nclass Chapter(BaseModel):\n    start_ts: float = Field(\n        ...,\n        description=\"The start timestamp indicating when the chapter starts in the video.\",\n    )\n    end_ts: float = Field(\n        ...,\n        description=\"The end timestamp indicating when the chapter ends in the video.\",\n    )\n    title: str = Field(\n        ..., description=\"A concise and descriptive title for the chapter.\"\n    )\n    summary: str = Field(\n        ...,\n        description=\"A brief summary of the chapter's content, don't use words like 'the speaker'\",\n    )\n\n\ndef get_youtube_transcript(video_id: str) -> str:\n    try:\n        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n        return [f\"ts={entry['start']} - {entry['text']}\" for entry in transcript]\n    except Exception as e:\n        print(f\"Error fetching transcript: {e}\")\n        return \"\"\n\n\ndef extract_chapters(transcript: str):\n    return client.chat.completions.create_iterable(\n        model=\"gpt-4o\",  # You can experiment with different models\n        response_model=Chapter,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Analyze the given YouTube transcript and extract chapters. For each chapter, provide a start timestamp, end timestamp, title, and summary.\",\n            },\n            {\"role\": \"user\", \"content\": transcript},\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    transcripts = get_youtube_transcript(\"jkrNMKz9pWU\")\n\n    for transcript in transcripts[:2]:\n        print(transcript)\n        #> ts=0.539 - hi I am Jeremy Howard from fast.ai and\n        #> ts=4.62 - this is a hacker's guide to language\n\n    formatted_transcripts = ''.join(transcripts)\n    chapters = extract_chapters(formatted_transcripts)\n\n    for chapter in chapters:\n        print(chapter.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"start_ts\": 0.539,\n          \"end_ts\": 9.72,\n          \"title\": \"Introduction\",\n          \"summary\": \"Jeremy Howard from fast.ai introduces the video, mentioning it as a hacker's guide to language models, focusing on a code-first approach.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"start_ts\": 9.72,\n          \"end_ts\": 65.6,\n          \"title\": \"Understanding Language Models\",\n          \"summary\": \"Explains the code-first approach to using language models, suggesting prerequisites such as prior deep learning knowledge and recommends the course.fast.ai for in-depth learning.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"start_ts\": 65.6,\n          \"end_ts\": 250.68,\n          \"title\": \"Basics of Language Models\",\n          \"summary\": \"Covers the concept of language models, demonstrating how they predict the next word in a sentence, and showcases OpenAI's text DaVinci for creative brainstorming with examples.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"start_ts\": 250.68,\n          \"end_ts\": 459.199,\n          \"title\": \"How Language Models Work\",\n          \"summary\": \"Dives deeper into how language models like ULMfit and others were developed, their training on datasets like Wikipedia, and the importance of learning various aspects of the world to predict the next word effectively.\"\n        }\n        \"\"\"\n        # ... other chapters\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI API for Question Decomposition in Python\nDESCRIPTION: This code snippet demonstrates how to use OpenAI's API to generate a structured response for decomposing a complex question. It uses the GPT-4 model and the previously defined QueryPlan response model to structure the output.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nretrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QueryPlan,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the difference between the population of jason's home country and canada?\",\n        },\n    ],\n)\n\nprint(retrieval.model_dump_json(indent=4))\n```\n\n----------------------------------------\n\nTITLE: Implementing Rephrase and Respond (RaR) with Instructor and OpenAI\nDESCRIPTION: This code demonstrates how to implement the Rephrase and Respond technique using the instructor and OpenAI libraries. It defines a Response model to capture both the rephrased question and the answer, then creates a function that sends the query to the GPT-4o model with instructions to rephrase and respond to the original question.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/zero_shot/rar.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Response(BaseModel):\n    rephrased_question: str\n    answer: str\n\n\ndef rephrase_and_respond(query):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"{query}\\nRephrase and expand the question, and respond.\"\"\",  # (1)!\n            }\n        ],\n        response_model=Response,\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"Take the last letters of the words in 'Edgar Bob' and concatinate them.\"\n\n    response = rephrase_and_respond(query)\n\n    print(response.rephrased_question)\n    \"\"\"\n    What are the last letters of each word in the name 'Edgar Bob', and what do you get when you concatenate them?\n    \"\"\"\n    print(response.answer)\n    \"\"\"\n    To find the last letters of each word in the name 'Edgar Bob', we look at 'Edgar' and 'Bob'. The last letter of 'Edgar' is 'r' and the last letter of 'Bob' is 'b'. Concatenating these letters gives us 'rb'.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Complex Schema Extraction with Nested Models and Validation\nDESCRIPTION: Demonstrates extraction of complex, nested data structures with validation constraints. This example shows how to extract structured information about a person including address, skills, and work experience with datetime handling and field validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom datetime import datetime\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n\nclass Experience(BaseModel):\n    company: str\n    position: str\n    start_date: datetime\n    end_date: Optional[datetime] = None\n    description: str\n\nclass Person(BaseModel):\n    name: str\n    age: int = Field(gt=0, lt=150)\n    email: str\n    phone: Optional[str] = None\n    address: Address\n    skills: List[str] = Field(min_items=1)\n    experience: List[Experience] = Field(min_items=0)\n\nclient = instructor.from_openai(OpenAI())\n\nperson = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Person,\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Extract information about Jane Smith who is 35 years old.\n        Email: jane.smith@example.com\n        Phone: 555-987-6543\n        Address: 123 Main St, Springfield, IL 62701\n        Skills: Python, Data Analysis, Machine Learning, Communication\n        \n        Work Experience:\n        - Data Scientist at TechCorp (2019-01-15 to 2023-04-30)\n          Led data science projects for major clients\n        - Junior Analyst at DataFirm (2015-06-01 to 2018-12-15)\n          Performed statistical analysis and created reports\n        \"\"\"}\n    ]\n)\n\nprint(person.model_dump_json(indent=2))\n```\n\n----------------------------------------\n\nTITLE: Implementing Reverse Chain of Thought Analysis System\nDESCRIPTION: Complete implementation of a system that analyzes LLM responses using Reverse Chain of Thought methodology. The code defines Pydantic models for structured responses and includes functions for generating responses, reconstructing prompts, deconstructing conditions, and providing feedback.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/self_criticism/reversecot.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass ReconstructedPrompt(BaseModel):\n    chain_of_thought: str\n    reconstructed_prompt: str = Field(\n        description=\"\"\"Reconstruction of a potential prompt\n        that could have been used to generate the reasoning\n        and final solution provided by the user\"\"\"\n    )\n\n\nclass ConditionList(BaseModel):\n    conditions: list[str] = Field(\n        description=\"\"\"Key information and conditions present\n        in the reasoning steps which are relevant to answering\n        the question\"\"\"\n    )\n\n\nclass ModelFeedback(BaseModel):\n    detected_inconsistencies: list[str] = Field(\n        description=\"\"\"Inconsistencies that were detected between\n        the original condition list and the reconstructed condition\n        list\"\"\"\n    )\n    feedback: str = Field(\n        description=\"\"\"Feedback on how to fix the inconsistencies\n        detected in the original condition list and the reconstructed\n        condition list\"\"\"\n    )\n    is_equal: bool\n\n\nclass ModelResponse(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"\"\"Logical Steps that were taken to derive\n        the final concluding statement\"\"\"\n    )\n    correct_answer: str\n\n\ndef generate_response(query: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are a helpful AI Question Answerer. You are\n                about to be passed a query by a User.\n\n                Make sure to generate a series of logical steps\n                and reason about the problem before generating\n                a solution.\n                \"\"\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n        response_model=ModelResponse,\n    )\n\n\ndef reconstruct_prompt(model_response: ModelResponse):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ReconstructedPrompt,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                    Give the concrete prompt (problem) that can\n                    generate this answer. The problem should\n                    contain all basic and necessary information\n                    and correspond to the answer. The problem\n                    can only ask for one result\n\n                    Reasoning: {model_response.chain_of_thought}\n                    Response: {model_response.correct_answer}\n                    \"\"\",\n            }\n        ],\n    )\n\n\ndef deconstruct_prompt_into_condition_list(prompt: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ConditionList,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are an expert AI system that excels at\n                analyzing and decomposing questions into their\n                constituent parts.\n\n                Please list the conditions of the problem given\n                below. There might be multiple conditions in the\n                problem so make sure to navigate through the\n                prompt incrementally, indentifying and extracting\n                the conditions necessary to answer the question\n                in your final response.\n                \"\"\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n\n\ndef generate_feedback(\n    original_condition_list: list[str], final_condition_list: list[str]\n):\n    formatted_original_conditions = \"\\n- \".join(original_condition_list)\n    formatted_final_conditions = \"\\n- \".join(final_condition_list)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ModelFeedback,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                You are an expert AI system that excels at\n                analyzing and comparing two lists of conditions.\n\n                Original Condition List:\n                {formatted_original_conditions}\n\n                Reconstructed Condition List:\n                {formatted_final_conditions}\n\n                Determine if the two condition lists are roughly\n                equivalent. If they are not, give targetted\n                feedback on what is missing from the reconstructed\n                condition list as compared to the original condition\n                list and how it can be fixed.\n                \"\"\",\n            }\n        ],\n    )\n\n\ndef revise_response(response: ModelResponse, feedback: ModelFeedback):\n    formatted_inconsistencies = \"\\n- \".join(feedback.detected_inconsistencies)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                Here are the mistakes and reasons in your answer\n                to the prompt\n\n                Original Response: {response.correct_answer}\n                You have overlooked some real conditions:\n                {formatted_inconsistencies}\n\n                Here are detailed reasons:\n                {feedback.feedback}\n\n                Generate a revised response that takes into account\n                the detailed feedback and includes the ignored\n                conditions\n                \"\"\",\n            }\n        ],\n        response_model=ModelResponse,\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Question Classification with LangSmith and Instructor\nDESCRIPTION: Complete implementation of an async question classification system using OpenAI, LangSmith, and Instructor. Includes enum definitions, model classes, and classification logic with rate limiting.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/batch_classification_langsmith.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport asyncio\n\nfrom langsmith import traceable\nfrom langsmith.wrappers import wrap_openai\n\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List\nfrom enum import Enum\n\n# Wrap the OpenAI client with LangSmith\nclient = wrap_openai(AsyncOpenAI())\n\n# Patch the client with instructor\nclient = instructor.from_openai(client)\n\n# Rate limit the number of requests\nsem = asyncio.Semaphore(5)\n\n\n# Use an Enum to define the types of questions\nclass QuestionType(Enum):\n    CONTACT = \"CONTACT\"\n    TIMELINE_QUERY = \"TIMELINE_QUERY\"\n    DOCUMENT_SEARCH = \"DOCUMENT_SEARCH\"\n    COMPARE_CONTRAST = \"COMPARE_CONTRAST\"\n    EMAIL = \"EMAIL\"\n    PHOTOS = \"PHOTOS\"\n    SUMMARY = \"SUMMARY\"\n\n\n# You can add more instructions and examples in the description\n# or you can put it in the prompt in `messages=[...]`\nclass QuestionClassification(BaseModel):\n    \"\"\"\n    Predict the type of question that is being asked.\n    Here are some tips on how to predict the question type:\n    CONTACT: Searches for some contact information.\n    TIMELINE_QUERY: \"When did something happen?\n    DOCUMENT_SEARCH: \"Find me a document\"\n    COMPARE_CONTRAST: \"Compare and contrast two things\"\n    EMAIL: \"Find me an email, search for an email\"\n    PHOTOS: \"Find me a photo, search for a photo\"\n    SUMMARY: \"Summarize a large amount of data\"\n    \"\"\"\n\n    # If you want only one classification, just change it to\n    #   `classification: QuestionType` rather than `classifications: List[QuestionType]``\n    chain_of_thought: str = Field(\n        ..., description=\"The chain of thought that led to the classification\"\n    )\n    classification: List[QuestionType] = Field(\n        description=f\"An accuracy and correct prediction predicted class of question. Only allowed types: {[t.value for t in QuestionType]}, should be used\",\n    )\n\n    @field_validator(\"classification\", mode=\"before\")\n    def validate_classification(cls, v):\n        # sometimes the API returns a single value, just make sure it's a list\n        if not isinstance(v, list):\n            v = [v]\n        return v\n\n\n@traceable(name=\"classify-question\")\nasync def classify(data: str) -> QuestionClassification:\n    \"\"\"\n    Perform multi-label classification on the input text.\n    Change the prompt to fit your use case.\n    Args:\n        data (str): The input text to classify.\n    \"\"\"\n    async with sem:  # some simple rate limiting\n        return data, await client.chat.completions.create(\n            model=\"gpt-4-turbo-preview\",\n            response_model=QuestionClassification,\n            max_retries=2,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Classify the following question: {data}\",\n                },\n            ],\n        )\n\n\nasync def main(questions: List[str]):\n    tasks = [classify(question) for question in questions]\n\n    for task in asyncio.as_completed(tasks):\n        question, label = await task\n        resp = {\n            \"question\": question,\n            \"classification\": [c.value for c in label.classification],\n            \"chain_of_thought\": label.chain_of_thought,\n        }\n        resps.append(resp)\n    return resps\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    questions = [\n        \"What was that ai app that i saw on the news the other day?\",\n        \"Can you find the trainline booking email?\",\n        \"what did I do on Monday?\",\n        \"Tell me about todays meeting and how it relates to the email on Monday\",\n    ]\n\n    resp = asyncio.run(main(questions))\n\n    for r in resp:\n        print(\"q:\", r[\"question\"])\n        #> q: what did I do on Monday?\n        print(\"c:\", r[\"classification\"])\n        #> c: ['SUMMARY']\n```\n\n----------------------------------------\n\nTITLE: Implementing Self-Refining Code Generation System in Python\nDESCRIPTION: A complete Python implementation of a self-refining system using OpenAI's GPT models to generate, evaluate, and improve code. The system includes Pydantic models for structured responses and feedback, functions for generating feedback and refining responses, and a main execution loop that demonstrates the refinement of a Fibonacci sequence implementation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/self_criticism/self_refine.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\nfrom typing import Optional\n\n\nclass Response(BaseModel):\n    code: str\n\n\nclass Feedback(BaseModel):\n    feedback: list[str] = Field(\n        description=\"A list of actions to take to improve the code.\"\n    )\n    done: bool\n\n\nclass Timestep(BaseModel):\n    response: str\n    feedback: Optional[list[str]] = Field(default_factory=list)\n    refined_response: Optional[str] = Field(default=\"\")\n\n\nclass History(BaseModel):\n    history: list[Timestep] = Field(default_factory=list)\n\n    def add(self, code, feedback, refined_code):\n        self.history.append(\n            Timestep(response=code, feedback=feedback, refined_response=refined_code)\n        )\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_feedback(response):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Feedback,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                        You are an expert Python coder.\n                        Provide feedback on this code.\n                        How can we make it (1) faster and (2) more readable?\n\n                        <code>\n                        {response.code}\n                        </code>\n\n                        If the code does not need to be improved, then indicate by setting \"done\" to True.\n                        \"\"\",\n            }\n        ],\n    )\n\n\ndef refine(response, feedback):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                        You are an expert Python coder.\n\n                        <response>\n                        {response.code}\n                        </response>\n\n                        <feedback>\n                        {feedback.feedback}\n                        </feedback>\n\n                        Refine your response.\n                        \"\"\",\n            }\n        ],\n    )\n\n\ndef stop_condition(feedback, history):\n    return feedback.done or len(history.history) >= 3\n\n\nif __name__ == \"__main__\":\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Write Python code to calculate the fibonacci sequence.\",\n            }\n        ],\n    )\n\n    history = History()\n\n    while True:\n        feedback = generate_feedback(response)\n        if stop_condition(feedback, history):\n            break\n        refined_response = refine(response, feedback)\n\n        # Save to history\n        history.add(response.code, feedback.feedback, refined_response.code)\n        response = refined_response\n```\n\n----------------------------------------\n\nTITLE: Extracting Customer Lead Information with OpenAI and Pydantic in Python\nDESCRIPTION: This code snippet defines a Lead model using Pydantic, creates a function to parse lead information from user messages using OpenAI's API, and demonstrates the extraction process with sample inputs. It includes error handling for invalid phone numbers and supports extracting multiple leads from a single message.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extract_contact_info.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom pydantic_extra_types.phone_numbers import PhoneNumber\nfrom typing import Iterable\n\n\nclass Lead(BaseModel):\n    name: str\n    phone_number: PhoneNumber = Field(\n        description=\"Needs to be a phone number with a country code. If none, assume +1\"\n    )\n\n    # Can define some function here to send Lead information to a database using an API\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef parse_lead_from_message(user_message: str):\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=Iterable[Lead],\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data extraction system that extracts a user's name and phone number from a message.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Extract the user's lead information from this user's message: {user_message}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    lead = parse_lead_from_message(\n        \"Yes, that would be great if someone can reach out my name is Patrick King 9175554587\"\n    )\n    assert all(isinstance(item, Lead) for item in lead)\n    for item in lead:\n        print(item.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"name\": \"Patrick King\",\n          \"phone_number\": \"tel:+1-917-555-4587\"\n        }\n        \"\"\"\n\n    # Invalid phone number example:\n    try:\n        lead2 = parse_lead_from_message(\n            \"Yes, that would be great if someone can reach out my name is Patrick King 9172234\"\n        )\n        assert all(isinstance(item, Lead) for item in lead2)\n        for item in lead2:\n            print(item.model_dump_json(indent=2))\n            \"\"\"\n            {\n              \"name\": \"Patrick King\",\n              \"phone_number\": \"tel:+1-917-223-4999\"\n            }\n            \"\"\"\n\n    except Exception as e:\n        print(\"ERROR:\", e)\n        \"\"\"\n        ERROR:\n        1 validation error for IterableLead\n        tasks.0.phone_number\n          value is not a valid phone number [type=value_error, input_value='+19172234', input_type=str]\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Asynchronous User Extraction with OpenAI\nDESCRIPTION: Example demonstrating async user information extraction using OpenAI client with Instructor\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import AsyncOpenAI\nimport instructor\nfrom pydantic import BaseModel\nimport asyncio\n\n# Initialize with API key\nclient = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# Enable instructor patches for async OpenAI client\nclient = instructor.from_openai(client)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nasync def extract_user():\n    user = await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n        ],\n        response_model=User,\n    )\n    return user\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)\n#> User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Complete Tag Classification Implementation\nDESCRIPTION: Full implementation of the tag classification system including imports, model definitions, and async processing functions with example usage.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\ntags = [\n    TagWithInstructions(id=0, name=\"personal\", instructions=\"Personal information\"),\n    TagWithInstructions(id=1, name=\"phone\", instructions=\"Phone number\"),\n    TagWithInstructions(id=2, name=\"email\", instructions=\"Email address\"),\n    TagWithInstructions(id=3, name=\"address\", instructions=\"Address\"),\n    TagWithInstructions(id=4, name=\"Other\", instructions=\"Other information\"),\n]\n\ntexts = [\n    \"What is your phone number?\",\n    \"What is your email address?\",\n    \"What is your address?\",\n    \"What is your privacy policy?\",\n]\n\nrequest = TagRequest(texts=texts, tags=tags)\n```\n\n----------------------------------------\n\nTITLE: Creating FastAPI POST Endpoint with Pydantic Models\nDESCRIPTION: Demonstrates setting up a FastAPI application with a POST endpoint that uses Pydantic models for request/response validation. Integrates with OpenAI's API using the instructor library for response handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/fastapi.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables response_model\nclient = instructor.from_openai(AsyncOpenAI())\napp = FastAPI()\n\n\nclass UserData(BaseModel):\n    # This can be the model for the input data\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@app.post(\"/endpoint\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -> UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n    return user_detail\n```\n\n----------------------------------------\n\nTITLE: Implementing Redis Cache Decorator for Pydantic Models\nDESCRIPTION: A decorator that implements Redis caching for functions returning Pydantic models. It handles serialization and deserialization of model data, with cache key generation based on function name and arguments.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/caching.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor Patching for Gemini Models via OpenAI SDK\nDESCRIPTION: This snippet demonstrates how to initialize an OpenAI client with Instructor patching for Gemini Models using the OpenAI SDK. It requires setting up Google Cloud authentication and specifying the Vertex AI endpoint.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport google.auth\nimport google.auth.transport.requests\nimport instructor\nfrom openai import OpenAI\n\ncreds, project = google.auth.default()\nauth_req = google.auth.transport.requests.Request()\ncreds.refresh(auth_req)\n\n# Pass the Vertex endpoint and authentication to the OpenAI SDK\nPROJECT = 'PROJECT_ID'\nLOCATION = 'LOCATION'\n\nbase_url = f'https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT}/locations/{LOCATION}/endpoints/openapi'\nclient = instructor.from_openai(\n    OpenAI(base_url=base_url, api_key=creds.token), mode=instructor.Mode.JSON\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Pydantic Model in Python\nDESCRIPTION: Demonstrates how to create a simple Pydantic model with basic field types for user data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n```\n\n----------------------------------------\n\nTITLE: Streaming Conference Information Extraction with Instructor and OpenAI\nDESCRIPTION: This comprehensive example demonstrates how to use Instructor and OpenAI to stream partial responses when extracting conference information. It includes model definitions, text processing, and visualization of streamed results.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/partial.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.from_openai(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = client.chat.completions.create_partial(\n    model=\"gpt-4\",\n    response_model=MeetingInfo,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n\nprint(extraction.model_dump_json(indent=2))\n\"\"\"\n{\n  \"users\": [\n    {\n      \"name\": \"John Doe\",\n      \"email\": \"johndoe@email.com\",\n      \"twitter\": \"@TechGuru44\"\n    },\n    {\n      \"name\": \"Jane Smith\",\n      \"email\": \"janesmith@email.com\",\n      \"twitter\": \"@DigitalDiva88\"\n    },\n    {\n      \"name\": \"Alex Johnson\",\n      \"email\": \"alexj@email.com\",\n      \"twitter\": \"@CodeMaster2023\"\n    }\n  ],\n  \"date\": \"2024-03-15\",\n  \"location\": \"Grand Tech Arena located at 4521 Innovation Drive\",\n  \"budget\": 50000,\n  \"deadline\": \"2024-02-20\"\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Asynchronous User Data Extraction with Vertex AI\nDESCRIPTION: Implementation of user data extraction using Instructor with Vertex AI in asynchronous mode. Shows how to use async/await pattern for non-blocking operations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/vertex.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport instructor\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nfrom pydantic import BaseModel\n\nvertexai.init()\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n    _async=True,\n)\n\nasync def extract_user():\n    user = await client.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract Jason is 25 years old.\",\n            }\n        ],\n        response_model=User,\n    )\n    return user\n\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)  # User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Generating structured output with Cohere and instructor\nDESCRIPTION: This example demonstrates how to use Cohere's command models with the instructor library to create a structured Group object for 'The Beatles' band. It defines Pydantic models for Person and Group, patches the Cohere client with instructor, and generates a structured response based on a given task.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cohere.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport cohere\nimport instructor\n\n\n# Patching the Cohere client with the instructor for enhanced capabilities\nclient = instructor.from_cohere(\n    cohere.Client(),\n    max_tokens=1000,\n    model=\"command-r-plus\",\n)\n\n\nclass Person(BaseModel):\n    name: str = Field(description=\"name of the person\")\n    country_of_origin: str = Field(description=\"country of origin of the person\")\n\n\nclass Group(BaseModel):\n    group_name: str = Field(description=\"name of the group\")\n    members: List[Person] = Field(description=\"list of members in the group\")\n\n\ntask = \"\"\"\nGiven the following text, create a Group object for 'The Beatles' band\n\nText:\nThe Beatles were an English rock band formed in Liverpool in 1960. With a line-up comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr, they are regarded as the most influential band of all time. The group were integral to the development of 1960s counterculture and popular music's recognition as an art form.\n\"\"\"\ngroup = client.messages.create(\n    response_model=Group,\n    messages=[{\"role\": \"user\", \"content\": task}],\n    temperature=0,\n)\n\nprint(group.model_dump_json(indent=2))\n\"\"\"\n{\n  \"group_name\": \"The Beatles\",\n  \"members\": [\n    {\n      \"name\": \"John Lennon\",\n      \"country_of_origin\": \"England\"\n    },\n    {\n      \"name\": \"Paul McCartney\",\n      \"country_of_origin\": \"England\"\n    },\n    {\n      \"name\": \"George Harrison\",\n      \"country_of_origin\": \"England\"\n    },\n    {\n      \"name\": \"Ringo Starr\",\n      \"country_of_origin\": \"England\"\n    }\n  ]\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: List Validation with Pydantic\nDESCRIPTION: Demonstrates how to add validation for both individual items and entire lists using Pydantic validators. Includes price validation and unique name constraints.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/list_extraction.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field, field_validator, model_validator\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    \n    @field_validator('price')\n    @classmethod\n    def validate_price(cls, v):\n        if v <= 0:\n            raise ValueError(\"Price must be greater than zero\")\n        return v\n\nclass ProductList(BaseModel):\n    products: List[Product] = Field(..., min_items=1)\n    \n    @model_validator(mode='after')\n    def validate_unique_names(self):\n        names = [p.name for p in self.products]\n        if len(names) != len(set(names)):\n            raise ValueError(\"All product names must be unique\")\n        return self\n\n# Extract list with validation\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"List of products: Headphones ($50), Speakers ($80), Earbuds ($30)\"}\n    ],\n    response_model=ProductList\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom MarkdownDataFrame Type for Table Extraction in Python\nDESCRIPTION: This code defines a custom MarkdownDataFrame type using Python's Annotated and InstanceOf types, along with Pydantic decorators. It includes a function to convert markdown to DataFrame and handles serialization and validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extracting_tables.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nimport pandas as pd\n\n\ndef md_to_df(data: Any) -> Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],\n    BeforeValidator(md_to_df),\n    PlainSerializer(lambda df: df.to_markdown()),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be seperate\",\n        }\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Responses with Writer and Instructor in Python\nDESCRIPTION: This snippet illustrates how to use the streaming capability with Writer and Instructor. It defines Pydantic models for User and MeetingInfo, and demonstrates incremental processing of responses using the create_partial method.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/writer-support.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom writerai import Writer\nfrom pydantic import BaseModel\n\n# Initialize Writer client\nclient = instructor.from_writer(Writer())\n\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nPartialMeetingInfo = instructor.Partial[MeetingInfo]\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"palmyra-x-004\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    response_model=PartialMeetingInfo,\n    stream=True,\n)  # type: ignore\n\n\nfor obj in extraction_stream:\n    print(obj)\n    #> date='March 15th, 2024' location='' budget=None deadline=None\n    #> date='March 15th, 2024' location='Grand Tech Arena, 4521 Innovation' budget=None deadline=None\n    #> date='March 15th, 2024' location='Grand Tech Arena, 4521 Innovation Drive' budget=50000 eadline='February 20th'\n```\n\n----------------------------------------\n\nTITLE: Streaming Data Extraction with OpenAI Tools Mode\nDESCRIPTION: Demonstrates how to implement streaming data extraction using OpenAI's Tools mode. Shows real-time extraction of User objects from text as tokens are received.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/iterable.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Iterable[User],\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Types and Classes for Table Extraction in Python\nDESCRIPTION: This code snippet defines custom types and classes for handling table extraction from images. It includes a MarkdownDataFrame type using Pydantic annotations and a Table class to organize extracted data. The code also sets up the OpenAI client with the instructor library for API interactions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/tables_from_vision.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom io import StringIO\nfrom typing import Annotated, Any, List\nfrom pydantic import (\n    BaseModel,\n    BeforeValidator,\n    PlainSerializer,\n    InstanceOf,\n    WithJsonSchema,\n)\nimport instructor\nimport pandas as pd\nfrom rich.console import Console\n\nconsole = Console()\nclient = instructor.from_openai(\n    client=OpenAI(),\n    mode=instructor.Mode.TOOLS,\n)\n\n\ndef md_to_df(data: Any) -> Any:\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Get rid of whitespaces\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .map(lambda x: x.strip())\n        )  # type: ignore\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],\n    BeforeValidator(md_to_df),\n    PlainSerializer(lambda x: x.to_markdown()),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n                The markdown representation of the table,\n                each one should be tidy, do not try to join tables\n                that should be seperate\"\"\",\n        }\n    ),\n]\n\n\nclass Table(BaseModel):\n    caption: str\n    dataframe: MarkdownDataFrame\n\n\nclass MultipleTables(BaseModel):\n    tables: List[Table]\n\n\nexample = MultipleTables(\n    tables=[\n        Table(\n            caption=\"This is a caption\",\n            dataframe=pd.DataFrame(\n                {\n                    \"Chart A\": [10, 40],\n                    \"Chart B\": [20, 50],\n                    \"Chart C\": [30, 60],\n                }\n            ),\n        )\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous User Data Extraction\nDESCRIPTION: Implementation of asynchronous user data extraction using AsyncOpenAI client\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cortex.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import AsyncOpenAI\nimport instructor\nfrom pydantic import BaseModel\nimport asyncio\n\n# Initialize with API key\nclient = from_openai(\n    openai.AsyncOpenAI(\n        base_url=\"http://localhost:39281/v1\",\n        api_key=\"this is a fake api key that doesn't matter\",\n    )\n)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nasync def extract_user():\n    user = await client.chat.completions.create(\n        model=\"llama3.2:3b-gguf-q4-km\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n        ],\n        response_model=User,\n    )\n    return user\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)\n#> User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Extraction with FastAPI, Instructor, and OpenAI\nDESCRIPTION: This snippet shows the basic setup for a FastAPI endpoint that uses Instructor and OpenAI to extract user details from a query. It defines Pydantic models for input and output, and uses an asynchronous OpenAI client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/full-fastapi-visibility.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom fastapi import FastAPI\nfrom openai import AsyncOpenAI\nimport instructor\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\napp = FastAPI()\nclient = instructor.from_openai(AsyncOpenAI())\n\n\n@app.post(\"/user\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -> UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n\n    return user_detail\n```\n\n----------------------------------------\n\nTITLE: Implementing KNN-based Example Selection for LLM Queries in Python\nDESCRIPTION: A complete implementation that demonstrates selecting relevant in-context examples using KNN and embeddings. The code handles embedding generation, distance calculation, example selection, and response generation using OpenAI's API and the Instructor library. It includes Pydantic models for type safety and structured responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/few_shot/exemplar_selection/knn.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nimport math\nfrom textwrap import dedent\n\n\nclass Example(BaseModel):\n    question: str\n    answer: str\n\n\nclass Response(BaseModel):\n    answer: str\n\n\noai = OpenAI()\nclient = instructor.from_openai(oai)\n\n\ndef distance(a: list[float], b: list[float]):\n    return 1 - sum(ai * bi for ai, bi in zip(a, b)) / (\n        math.sqrt(sum(ai**2 for ai in a)) * math.sqrt(sum(bi**2 for bi in b))\n    )\n\n\ndef embed_queries(queries: list[str]) -> list[tuple[list[float], str]]:\n    return [\n        (embedding_item.embedding, query)\n        for embedding_item, query in zip(\n            oai.embeddings.create(input=queries, model=\"text-embedding-3-large\").data,\n            queries,\n        )\n    ]\n\n\ndef knn(\n    embedded_examples: list[tuple[list[float], str]],\n    query_embedding: list[float],\n    k: int,\n):\n    distances = [\n        (distance(embedding, query_embedding), example)\n        for embedding, example in embedded_examples\n    ]\n    distances.sort(key=lambda x: x[0])\n    return distances[:k]\n\n\ndef generate_response(examples: list[str], query: str):\n    formatted_examples = \"\\n\".join(examples)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                    Respond to the following query with the most accurate\n                    and concise answer possible.\n                    <examples>\n                    {formatted_examples}\n                    </examples>\n                    <query>\n                    {query}\n                    </query>\n                \"\"\"\n                ),\n            }\n        ],\n    )\n\n\ndef generate_question_and_answer_pair(\n    questions: list[str], question_and_answers: list[dict[str, str]]\n) -> list[str]:\n    question_to_answer = {}\n\n    for question in question_and_answers:\n        question_to_answer[question[\"question\"]] = question[\"answer\"]\n\n    return [\n        dedent(\n            f\"\"\"\n        <example>\n        <question>{question}</question>\n        <answer>{question_to_answer[question]}</answer>\n        </example>\n        \"\"\"\n        )\n        for question in questions\n    ]\n\n\nif __name__ == \"__main__\":\n    examples = [\n        {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n        {\"question\": \"Who wrote Romeo and Juliet\", \"answer\": \"Shakespeare\"},\n        {\"question\": \"What is the capital of Germany?\", \"answer\": \"Berlin\"},\n    ]\n\n    query = \"What is the capital of Italy?\"\n\n    # Step 1 : Embed the Examples\n    embeddings = embed_queries([example[\"question\"] for example in examples] + [query])\n\n    embedded_examples = embeddings[:-1]\n    embedded_query = embeddings[-1]\n\n    # # Step 3: Find the k closest examples to the query\n    k_closest_examples = knn(embedded_examples, embedded_query[0], 2)\n\n    for example in k_closest_examples:\n        print(example)\n        #> (0.4013468481736857, 'What is the capital of France?')\n        #> (0.4471368596136872, 'What is the capital of Germany?')\n\n    # Step 4: Use these examples as in-context examples\n    formatted_examples = generate_question_and_answer_pair(\n        [example[1] for example in k_closest_examples], examples\n    )\n    response = generate_response(formatted_examples, query)\n    print(response.answer)\n    #> Rome\n```\n\n----------------------------------------\n\nTITLE: Defining Document Section Data Structures with Pydantic\nDESCRIPTION: Defines Section and StructuredDocument classes using Pydantic models to represent document segments. The Section class tracks title and line indices while StructuredDocument contains a list of sections.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/document_segmentation.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Section(BaseModel):\n    title: str = Field(description=\"main topic of this section of the document\")\n    start_index: int = Field(description=\"line number where the section begins\")\n    end_index: int = Field(description=\"line number where the section ends\")\n\n\nclass StructuredDocument(BaseModel):\n    \"\"\"obtains meaningful sections, each centered around a single concept/topic\"\"\"\n\n    sections: List[Section] = Field(description=\"a list of sections of the document\")\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Objects with Instructor\nDESCRIPTION: Demonstrates the `create_partial` method for streaming partial objects, showing how to incrementally generate and process structured data\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser_stream = client.chat.completions.create_partial(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n\nfor user in user_stream:\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Async Contextual Retrieval Implementation\nDESCRIPTION: Complete implementation of Anthropic's Contextual Retrieval technique using async processing, including Pydantic models, chunking, and async processing of document chunks\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/situate-context.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import AsyncInstructor, Mode, patch\nfrom anthropic import AsyncAnthropic\nfrom pydantic import BaseModel, Field\nimport asyncio\nfrom typing import List, Dict\n\n\nclass SituatedContext(BaseModel):\n    title: str = Field(..., description=\"The title of the document.\")\n    context: str = Field(\n        ..., description=\"The context to situate the chunk within the document.\"\n    )\n\n\nclient = AsyncInstructor(\n    create=patch(\n        create=AsyncAnthropic().beta.prompt_caching.messages.create,\n        mode=Mode.ANTHROPIC_TOOLS,\n    ),\n    mode=Mode.ANTHROPIC_TOOLS,\n)\n\n\nasync def situate_context(doc: str, chunk: str) -> str:\n    response = await client.chat.completions.create(\n        model=\"claude-3-haiku-20240307\",\n        max_tokens=1024,\n        temperature=0.0,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"<document>{{doc}}</document>\",\n                        \"cache_control\": {\"type\": \"ephemeral\"},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Here is the chunk we want to situate within the whole document\\n<chunk>{{chunk}}</chunk>\\nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\\nAnswer only with the succinct context and nothing else.\",\n                    },\n                ],\n            }\n        ],\n        response_model=SituatedContext,\n        context={\"doc\": doc, \"chunk\": chunk},\n    )\n    return response.context\n\n\ndef chunking_function(doc: str) -> List[str]:\n    chunk_size = 1000\n    overlap = 200\n    chunks = []\n    start = 0\n    while start < len(doc):\n        end = start + chunk_size\n        chunks.append(doc[start:end])\n        start += chunk_size - overlap\n    return chunks\n\n\nasync def process_chunk(doc: str, chunk: str) -> Dict[str, str]:\n    context = await situate_context(doc, chunk)\n    return {\"chunk\": chunk, \"context\": context}\n\n\nasync def process(doc: str) -> List[Dict[str, str]]:\n    chunks = chunking_function(doc)\n    tasks = [process_chunk(doc, chunk) for chunk in chunks]\n    results = await asyncio.gather(*tasks)\n    return results\n\n\n# Example usage\nasync def main():\n    document = \"Your full document text here...\"\n    processed_chunks = await process(document)\n    for i, item in enumerate(processed_chunks):\n        print(f\"Chunk {i + 1}:\")\n        print(f\"Text: {item['chunk'][:50]}...\")\n        print(f\"Context: {item['context']}\")\n        print()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Extracting Tidy Tables from Images Using Instructor and OpenAI in Python\nDESCRIPTION: Demonstrates how to use the instructor library with OpenAI to extract and convert untidy tables from images into tidy pandas DataFrames. The code defines Pydantic models to structure the output and uses a patched OpenAI client to process the image and return structured data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/tidy-data-from-messy-tables.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\nclass Table(BaseModel):\n    caption: str\n    dataframe: MarkdownDataFrame  # Custom type for handling tables\n\n\nclass TidyTables(BaseModel):\n    tables: list[Table]\n\n\n# Patch the OpenAI client with instructor\nclient = instructor.from_openai(OpenAI())\n\n\ndef extract_table(image_path: str) -> TidyTables:\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    \"Convert this untidy table to tidy format\",\n                    instructor.Image.from_path(image_path),\n                ],\n            }\n        ],\n        response_model=TidyTables,\n    )\n\n\nextracted_tables = extract_table(\"./untidy_table.png\")\n```\n\n----------------------------------------\n\nTITLE: Testing with Hooks using Mock Objects\nDESCRIPTION: Shows how to use hooks in unit testing scenarios with mock objects to inspect arguments and responses without modifying application code.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\nfrom unittest.mock import Mock\nimport instructor\nimport openai\n\n\nclass TestMyApp(unittest.TestCase):\n    def test_completion(self):\n        client = instructor.from_openai(openai.OpenAI())\n        mock_handler = Mock()\n\n        client.on(\"completion:response\", mock_handler)\n\n        result = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n            response_model=str,\n        )\n\n        mock_handler.assert_called_once()\n\n        response_arg = mock_handler.call_args[0][0]\n        self.assertEqual(response_arg.model, \"gpt-3.5-turbo\")\n```\n\n----------------------------------------\n\nTITLE: Using Audio Class with OpenAI for Audio Analysis in Python\nDESCRIPTION: This example demonstrates how to use the Audio class from Instructor to analyze an audio file using OpenAI's GPT-4 audio model. It loads an audio file from a URL and extracts a summary and transcript.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/multimodal.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\nfrom instructor.multimodal import Audio\nimport base64\n\n# Initialize the client\nclient = instructor.from_openai(OpenAI())\n\n\n# Define our response model\nclass AudioDescription(BaseModel):\n    summary: str\n    transcript: str\n\n\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/gettysburg.wav\"\n\n# Make the API call with the audio file\nresp = client.chat.completions.create(\n    model=\"gpt-4o-audio-preview\",\n    response_model=AudioDescription,\n    modalities=[\"text\"],\n    audio={\"voice\": \"alloy\", \"format\": \"wav\"},\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Extract the following information from the audio:\",\n                Audio.from_url(url),\n            ],\n        },\n    ],\n)\n\nprint(resp)\n```\n\n----------------------------------------\n\nTITLE: Complete Redis Cache Implementation with OpenAI Integration\nDESCRIPTION: Full implementation example showing Redis caching with OpenAI API integration, including a Pydantic model definition and practical usage with the instructor library. Demonstrates caching of AI-generated responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/caching.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -> UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing SimToM with Instructor and OpenAI\nDESCRIPTION: A complete implementation of the SimToM technique using Python with the Instructor and OpenAI libraries. The code demonstrates how to extract facts known to a specific entity and then generate responses from that entity's perspective using structured outputs with Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/zero_shot/simtom.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel, Field\nfrom typing import Iterable\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass KnownFact(BaseModel):\n    fact: str = Field(description=\"A fact that the given entity would know\")\n\n\nclass Response(BaseModel):\n    location: str\n\n\ndef generate_known_facts(entity, context, query) -> Iterable[KnownFact]:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[KnownFact],\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Given the following context, list\n                the facts that {entity} would know:\n\n                Context:\n                {context}\n                {query}\n\n                List only the facts relevant to {entity}.\n                \"\"\",\n            }\n        ],\n    )\n\n\ndef answer_question_based_on_facts(entity, query, known_facts) -> Response:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"You are {entity}. Answer the following question\n                based only on these facts you know:\n                {\" \".join([str(fact) for fact in known_facts])}\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Question: {query}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    entity = \"Alice\"\n    context = \"\"\"Alice puts the book on the table.\n        Alice leaves the room.\n        Bob moves the book to the shelf.\n        \"\"\"\n    query = f\"Where does {entity} think the book is?\"\n\n    known_facts = generate_known_facts(entity, context, query)\n    response = answer_question_based_on_facts(entity, query, known_facts)\n\n    for fact in known_facts:\n        print(fact)\n        #> fact='Alice puts the book on the table.'\n        #> fact='Alice leaves the room. Bob moves the book to the shelf.'\n    print(response.location)\n    #> On the table\n```\n\n----------------------------------------\n\nTITLE: LLM Topic Validation Model\nDESCRIPTION: Example of using LLM-based validation to ensure responses stay on topic using the instructor library.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import llm_validator\n\n\nclass AssistantMessage(BaseModel):\n    message: Annotated[\n        str,\n        AfterValidator(\n            llm_validator(\n                \"don't talk about any other topic except health best practices and topics\",\n                client=client,\n            )\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Streaming User Extraction with Tools Mode\nDESCRIPTION: Implements streaming extraction of User objects using OpenAI's API with Instructor in TOOLS mode. Shows how to process results as they stream in.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/lists.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Iterable[User],\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Streaming List Extraction\nDESCRIPTION: Shows how to stream list extraction results using Instructor's streaming capabilities. Demonstrates real-time processing of extracted tasks.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/list_extraction.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\nclass Task(BaseModel):\n    description: str\n    priority: str\n    deadline: str\n\n# Stream a list of tasks\nfor task in client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Generate a list of 5 sample tasks for a project manager\"}\n    ],\n    response_model=List[Task],\n    stream=True\n):\n    print(f\"Received task: {task.description} (Priority: {task.priority}, Deadline: {task.deadline})\")\n```\n\n----------------------------------------\n\nTITLE: Complex Nested Object Extraction Example\nDESCRIPTION: A real-world example showing how to extract nested structured data with optional fields. Demonstrates handling complex contact information including nested address data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/simple_object.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: Optional[str] = None\n    address: Optional[Address] = None\n\n# Extract structured data\nclient = instructor.from_openai(OpenAI())\ncontact = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Contact information:\n        Name: Sarah Johnson\n        Email: sarah.j@example.com\n        Phone: (555) 123-4567\n        Address: 123 Main St, Boston, MA 02108\n        \"\"\"}\n    ],\n    response_model=ContactInfo\n)\n\nprint(f\"Name: {contact.name}\")\nprint(f\"Email: {contact.email}\")\n```\n\n----------------------------------------\n\nTITLE: Sentiment Classification with Enum and Instructor\nDESCRIPTION: Shows how to perform sentiment analysis with structured results using Enum types. This example extracts sentiment classification along with confidence scores and explanations, demonstrating how to use Enum with Pydantic for constrained outputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom enum import Enum\n\nclass Sentiment(str, Enum):\n    POSITIVE = \"positive\"\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n\nclass SentimentAnalysis(BaseModel):\n    sentiment: Sentiment\n    confidence: float\n    explanation: str\n\nclient = instructor.from_openai(OpenAI())\n\nanalysis = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=SentimentAnalysis,\n    messages=[\n        {\"role\": \"user\", \"content\": \"I absolutely loved the new movie! It was fantastic!\"}\n    ]\n)\n\nprint(f\"Sentiment: {analysis.sentiment}\")\nprint(f\"Confidence: {analysis.confidence}\")\nprint(f\"Explanation: {analysis.explanation}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor Patching for Function Calling\nDESCRIPTION: This snippet demonstrates how to initialize an OpenAI client with Instructor patching using the TOOLS mode for function calling. Note that function calling is soon to be deprecated in favor of TOOL mode for OpenAI.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n```\n\n----------------------------------------\n\nTITLE: Implementing Emotion Prompting with Instructor and OpenAI\nDESCRIPTION: This code demonstrates how to implement emotion prompting with Python using the Instructor library and OpenAI. It defines an Album model, creates a function that adds emotional stimuli to a query, and processes the response as an iterable of structured Album objects.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/zero_shot/emotion_prompting.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom typing import Iterable\n\n\nclass Album(BaseModel):\n    name: str\n    artist: str\n    year: int\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef emotion_prompting(query, stimuli):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Album],\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                {query}\n                {stimuli}\n                \"\"\",\n            }\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"Provide me with a list of 3 musical albums from the 2000s.\"\n    stimuli = \"This is very important to my career.\"  # (1)!\n\n    albums = emotion_prompting(query, stimuli)\n\n    for album in albums:\n        print(album)\n        #> name='Kid A' artist='Radiohead' year=2000\n        #> name='The Marshall Mathers LP' artist='Eminem' year=2000\n        #> name='The College Dropout' artist='Kanye West' year=2004\n```\n\n----------------------------------------\n\nTITLE: Implementing List Item Validation in Python with Instructor\nDESCRIPTION: Demonstrates how to validate items in a list using Pydantic. This example includes validation for a list of tags, checking for minimum length, uniqueness, and converting to lowercase.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/field_validation.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field, field_validator\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass TagList(BaseModel):\n    tags: List[str] = Field(..., min_items=1, max_items=5)\n    \n    @field_validator('tags')\n    @classmethod\n    def validate_tags(cls, tags):\n        # Convert all tags to lowercase\n        tags = [tag.lower() for tag in tags]\n        \n        # Check for minimum length of each tag\n        for tag in tags:\n            if len(tag) < 2:\n                raise ValueError(\"Each tag must be at least 2 characters\")\n                \n        # Check for duplicates\n        if len(tags) != len(set(tags)):\n            raise ValueError(\"Tags must be unique\")\n            \n        return tags\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain of Density Summarization Function\nDESCRIPTION: Defines the main summarization function that uses OpenAI's GPT model to generate increasingly dense summaries through multiple iterations. Includes initial summary generation and subsequent refinements based on entity density.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chain-of-density.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(\n        model=\"gpt-4-0613\",\n        response_model=InitialSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"The generated summary should be about 80 words.\",\n            },\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\n                },\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create(\n            model=\"gpt-4-0613\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\n\n                Perform the following two tasks\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\n\n                Guidelines\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n                - Missing entities can appear anywhere in the new summary\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n                \"\"\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n                },\n                *missing_entity_message,\n            ],\n            max_retries=3,\n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n```\n\n----------------------------------------\n\nTITLE: Configuring instructor client with default parameters\nDESCRIPTION: Setting up an instructor client with default parameters that will be passed to every create call. This example shows how to specify model and temperature as defaults.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/version-1.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\n\nclient = instructor.from_openai(\n    openai.OpenAI(), model=\"gpt-4-turbo-preview\", temperature=0.2\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Multiple Levels of Nested Data with Instructor\nDESCRIPTION: This code snippet shows how to extract multiple levels of nested data using Instructor. It defines models for EmployeeDetails, ContactInfo, Address, and Person, demonstrating a more complex nested structure with optional fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/nested_structure.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\nfrom typing import List, Optional\n\nclient = instructor.from_openai(OpenAI())\n\nclass EmployeeDetails(BaseModel):\n    department: str\n    position: str\n    start_date: str\n\nclass ContactInfo(BaseModel):\n    phone: str\n    email: str\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    contact: ContactInfo  # First level nesting\n    address: Address      # First level nesting\n    employment: Optional[EmployeeDetails] = None  # Optional nested structure\n\n# Extract deeply nested data\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Employee Profile:\n        Name: Jane Doe\n        Age: 32\n        Phone: (555) 123-4567\n        Email: jane.doe@example.com\n        Address: 456 Oak Avenue, Chicago, IL 60601\n        Department: Engineering\n        Position: Senior Developer\n        Start Date: 2021-03-15\n        \"\"\"}\n    ],\n    response_model=Person\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Logfire with FastAPI and OpenAI for Enhanced Logging\nDESCRIPTION: This snippet demonstrates how to integrate Logfire with FastAPI and OpenAI. It configures Logfire to instrument both FastAPI and OpenAI, enabling detailed logging and monitoring of the application.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/full-fastapi-visibility.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom fastapi import FastAPI\nfrom openai import AsyncOpenAI\nimport instructor\nimport logfire\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\napp = FastAPI()\nopenai_client = AsyncOpenAI()\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\"))\nlogfire.instrument_openai(openai_client)\nlogfire.instrument_fastapi(app)\nclient = instructor.from_openai(openai_client)\n\n\n@app.post(\"/user\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -> UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n\n    return user_detail\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with OpenAI and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with OpenAI to extract structured data from natural language. It defines a Pydantic model and uses it to parse the API response.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n# Define your desired output structure\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data from natural language\nres = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=ExtractUser,\n    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n)\n\nassert res.name == \"John Doe\"\nassert res.age == 30\n```\n\n----------------------------------------\n\nTITLE: Nested List Extraction with Multiple Authors\nDESCRIPTION: Demonstrates extraction of nested lists where books can have multiple authors. Shows how to handle complex nested data structures with Lists inside model definitions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/list_extraction.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Author(BaseModel):\n    name: str\n    nationality: str\n\nclass Book(BaseModel):\n    title: str\n    authors: List[Author]  # Nested list of authors\n    publication_year: int\n\n# Extract data with nested lists\nbooks = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Book 1: \"Good Omens\" (1990)\n        Authors: Terry Pratchett (British), Neil Gaiman (British)\n        \n        Book 2: \"The Talisman\" (1984)\n        Authors: Stephen King (American), Peter Straub (American)\n        \"\"\"}\n    ],\n    response_model=List[Book]\n)\n\n# Access the nested data\nfor book in books:\n    author_names = \", \".join([author.name for author in book.authors])\n    print(f\"{book.title} ({book.publication_year}) by {author_names}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Output with IBM watsonx.ai and LiteLLM\nDESCRIPTION: This Python script demonstrates how to use IBM watsonx.ai with LiteLLM to generate structured output. It sets up the environment, defines a Pydantic model, and uses the Instructor library to create a structured Company object from given text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/watsonx.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport litellm\nfrom litellm import completion\nfrom pydantic import BaseModel, Field\n\nimport instructor\nfrom instructor import Mode\n\nlitellm.drop_params = True  # watsonx.ai doesn't support `json_mode`\n\nos.environ[\"WATSONX_URL\"] = \"https://us-south.ml.cloud.ibm.com\"\nos.environ[\"WATSONX_API_KEY\"] = \"\"\nos.environ[\"WATSONX_PROJECT_ID\"] = \"\"\n# Additional options: https://docs.litellm.ai/docs/providers/watsonx\n\n\nclass Company(BaseModel):\n    name: str = Field(description=\"name of the company\")\n    year_founded: int = Field(description=\"year the company was founded\")\n\n\nclient = instructor.from_litellm(completion, mode=Mode.JSON)\n\nresp = client.chat.completions.create(\n    model=\"watsonx/meta-llama/llama-3-8b-instruct\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\nGiven the following text, create a Company object:\n\nIBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems.\n\"\"\",\n        }\n    ],\n    project_id=os.environ[\"WATSONX_PROJECT_ID\"],\n    response_model=Company,\n)\n\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"IBM\",\n  \"year_founded\": 1911\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: PDF Processing with Anthropic and Instructor\nDESCRIPTION: Demonstrates PDF analysis using Anthropic through Instructor's multimodal interface. Shows how to extract structured data from PDFs with support for multiple loading methods.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import PDF\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom anthropic import Anthropic\n\n\nclass Receipt(BaseModel):\n    total: int\n    items: list[str]\n\n\nclient = instructor.from_anthropic(Anthropic())\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\n# Multiple ways to load an PDF:\nresponse = client.chat.completions.create(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=Receipt,\n    max_tokens=1000,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Extract out the total and line items from the invoice\",\n                # Option 1: Direct URL\n                PDF.from_url(url),\n            ],\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Responses with Mistral and Instructor\nDESCRIPTION: Python code demonstrating streaming of partial responses using Mistral and Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport instructor\nfrom mistralai import Mistral\nfrom instructor.dsl.partial import Partial\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n# Initialize with API key\nclient = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n\n# Enable instructor patches for Mistral client\ninstructor_client = instructor.from_mistral(client)\n\n# Stream partial responses\nmodel = instructor_client.chat.completions.create(\n    model=\"mistral-large-latest\",\n    response_model=Partial[UserExtract],\n    stream=True,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Jason Liu is 25 years old\"},\n    ],\n)\n\nfor partial_user in model:\n    print(f\"Received update: {partial_user}\")\n# Output might show:\n# Received update: UserExtract(name='Jason', age=None)\n# Received update: UserExtract(name='Jason Liu', age=None)\n# Received update: UserExtract(name='Jason Liu', age=25)\n```\n\n----------------------------------------\n\nTITLE: Using Instructor as a Function with Pydantic Return Types\nDESCRIPTION: This snippet demonstrates how to create a Python function that wraps the Instructor client to extract user details. It returns a strongly-typed Pydantic model from unstructured text using LLM processing.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef extract_user(str) -> UserDetails:\n    return client.chat.completions(\n           response_model=UserDetails,\n           messages=[]\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Instructor Distillation in Python\nDESCRIPTION: Example showing how to set up Instructor's distillation feature to generate training data for fine-tuning. Uses a simple multiplication function with logging and Pydantic models for structured output.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/distilation-part1.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions\n\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\n@instructions.distil\ndef fn(a: int, b: int) -> Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n\nfor _ in range(10):\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n```\n\n----------------------------------------\n\nTITLE: Implementing Complexity Based Consistency with instructor and OpenAI\nDESCRIPTION: A complete implementation of Complexity Based Consistency using Python's instructor library and OpenAI. The code defines Pydantic models for reasoning steps, implements async response generation, and includes logic to select responses based on reasoning complexity. It requires the instructor library, OpenAI API, and uses GPT-4 for generating responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/thought_generation/chain_of_thought_few_shot/complexity_based.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\nimport asyncio\nfrom collections import Counter\nimport random\n\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass ReasoningStep(BaseModel):\n    step: int = Field(..., description=\"The step number\")\n    subquestion: str = Field(..., description=\"Subquestion to solve\")\n    procedure: str = Field(\n        description=\"\"\"Any intermediate computation\n        that was done in the reasoning process. Leave\n        empty if no computation is needed\"\"\",\n    )\n    result: str\n\n\nclass Response(BaseModel):\n    reasoning: list[ReasoningStep] = Field(\n        description=\"reasoning steps to derive answer\",\n    )\n    correct_answer: int\n\n\nasync def generate_single_response(query: str, context: str) -> Response:\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are an expert Question Answering system. Make sure\n                to output your reasoning in structured reasoning steps\n                before generating a response to the user's query.\n\n\n                Context:\n                {context}\n\n                Query:\n                {query}\n                \"\"\"\n                ),\n            },\n        ],\n    )\n\n\nasync def complexity_based_consistency(\n    query: str, context: str, samples: int, top_k: int\n):\n    generated_responses = [\n        generate_single_response(query, context) for _ in range(samples)\n    ]\n    responses = await asyncio.gather(*generated_responses)\n    sorted_responses = sorted(responses, key=lambda x: len(x.reasoning), reverse=True)\n    top_responses = sorted_responses[:top_k]\n    return top_responses\n\n\nif __name__ == \"__main__\":\n    query = \"How many loaves of bread did they have left?\"\n    context = \"\"\"\n    The bakers at the Beverly Hills Bakery baked\n    200 loaves of bread on Monday morning. They\n    sold 93 loaves in the morning and 39 loaves\n    in the afternoon. A grocery store returned 6\n    unsold loaves.\n    \"\"\"\n\n    number_of_reasoning_chains = 5\n    top_k_to_sample = 3\n    response = asyncio.run(\n        complexity_based_consistency(\n            query, context, number_of_reasoning_chains, top_k_to_sample\n        )\n    )\n\n    answer_counts = Counter([res.correct_answer for res in response])\n\n    most_common_count = answer_counts.most_common(len(answer_counts))[0][1]\n    max_answers = [\n        answer for answer, count in answer_counts.items() if count == most_common_count\n    ]\n\n    final_answer = random.choice(max_answers)\n    print(final_answer)\n    #> 74\n```\n\n----------------------------------------\n\nTITLE: Customer Feedback Classification and Scoring with Langfuse and Instructor\nDESCRIPTION: This extensive example demonstrates how to use Langfuse with Instructor to classify customer feedback into categories and score its relevance. It uses asynchronous processing, rate limiting, and Langfuse's observability features.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/tracing_with_langfuse.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum\n\nimport asyncio\nimport instructor\n\nfrom langfuse import Langfuse\nfrom langfuse.openai import AsyncOpenAI\nfrom langfuse.decorators import langfuse_context, observe\n\nfrom pydantic import BaseModel, Field, field_validator\nimport os\n\n# Set your API keys Here\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-...\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-...\"\nos.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"\nos.environ[\"OPENAI_API_KEY] = \"sk-...\"\n\n\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n# Initialize Langfuse (needed for scoring)\nlangfuse = Langfuse()\n\n# Rate limit the number of requests\nsem = asyncio.Semaphore(5)\n\n\n# Define feedback categories\nclass FeedbackType(Enum):\n    PRAISE = \"PRAISE\"\n    SUGGESTION = \"SUGGESTION\"\n    BUG = \"BUG\"\n    QUESTION = \"QUESTION\"\n\n\n# Model for feedback classification\nclass FeedbackClassification(BaseModel):\n    feedback_text: str = Field(...)\n    classification: list[FeedbackType] = Field(\n        description=\"Predicted categories for the feedback\"\n    )\n    relevance_score: float = Field(\n        default=0.0,\n        description=\"Score of the query evaluating its relevance to the business between 0.0 and 1.0\",\n    )\n\n    # Make sure feedback type is list\n    @field_validator(\"classification\", mode=\"before\")\n    def validate_classification(cls, v):\n        if not isinstance(v, list):\n            v = [v]\n        return v\n\n\n@observe()  # Langfuse decorator to automatically log spans to Langfuse\nasync def classify_feedback(feedback: str):\n    \"\"\"\n    Classify customer feedback into categories and evaluate relevance.\n    \"\"\"\n    async with sem:  # simple rate limiting\n        response = await client.chat.completions.create(\n            model=\"gpt-4o\",\n            response_model=FeedbackClassification,\n            max_retries=2,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Classify and score this feedback: {feedback}\",\n                },\n            ],\n        )\n\n        # Retrieve observation_id of current span\n        observation_id = langfuse_context.get_current_observation_id()\n\n        return feedback, response, observation_id\n\n\ndef score_relevance(trace_id: str, observation_id: str, relevance_score: float):\n    \"\"\"\n    Score the relevance of a feedback query in Langfuse given the observation_id.\n    \"\"\"\n    langfuse.score(\n        trace_id=trace_id,\n        observation_id=observation_id,\n        name=\"feedback-relevance\",\n        value=relevance_score,\n    )\n\n\n@observe()  # Langfuse decorator to automatically log trace to Langfuse\nasync def main(feedbacks: list[str]):\n    tasks = [classify_feedback(feedback) for feedback in feedbacks]\n    results = []\n\n    for task in asyncio.as_completed(tasks):\n        feedback, classification, observation_id = await task\n        result = {\n            \"feedback\": feedback,\n            \"classification\": [c.value for c in classification.classification],\n            \"relevance_score\": classification.relevance_score,\n        }\n        results.append(result)\n\n        # Retrieve trace_id of current trace\n        trace_id = langfuse_context.get_current_trace_id()\n\n        # Score the relevance of the feedback in Langfuse\n        score_relevance(trace_id, observation_id, classification.relevance_score)\n\n    # Flush observations to Langfuse\n    langfuse_context.flush()\n    return results\n\n\nfeedback_messages = [\n    \"The chat bot on your website does not work.\",\n    \"Your customer service is exceptional!\",\n    \"Could you add more features to your app?\",\n    \"I have a question about my recent order.\",\n]\n\nfeedback_classifications = asyncio.run(main(feedback_messages))\n\nfor classification in feedback_classifications:\n    print(f\"Feedback: {classification['feedback']}\")\n    print(f\"Classification: {classification['classification']}\")\n    print(f\"Relevance Score: {classification['relevance_score']}\")\n\n\n\"\"\"\nFeedback: I have a question about my recent order.\nClassification: ['QUESTION']\nRelevance Score: 0.0\nFeedback: Could you add more features to your app?\nClassification: ['SUGGESTION']\nRelevance Score: 0.0\nFeedback: The chat bot on your website does not work.\nClassification: ['BUG']\nRelevance Score: 0.9\nFeedback: Your customer service is exceptional!\nClassification: ['PRAISE']\nRelevance Score: 0.9\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Question Classification Implementation with LangSmith and OpenAI\nDESCRIPTION: Implements a complete system for classifying questions using OpenAI's API with LangSmith integration. Includes async processing, custom types, and validation using Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/langsmith.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport asyncio\n\nfrom langsmith import traceable\nfrom langsmith.wrappers import wrap_openai\n\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List\nfrom enum import Enum\n\n# Wrap the OpenAI client with LangSmith\nclient = wrap_openai(AsyncOpenAI())\n\n# Patch the client with instructor\nclient = instructor.from_openai(client, mode=instructor.Mode.TOOLS)\n\n# Rate limit the number of requests\nsem = asyncio.Semaphore(5)\n\n\n# Use an Enum to define the types of questions\nclass QuestionType(Enum):\n    CONTACT = \"CONTACT\"\n    TIMELINE_QUERY = \"TIMELINE_QUERY\"\n    DOCUMENT_SEARCH = \"DOCUMENT_SEARCH\"\n    COMPARE_CONTRAST = \"COMPARE_CONTRAST\"\n    EMAIL = \"EMAIL\"\n    PHOTOS = \"PHOTOS\"\n    SUMMARY = \"SUMMARY\"\n\n\n# You can add more instructions and examples in the description\n# or you can put it in the prompt in `messages=[...]`\nclass QuestionClassification(BaseModel):\n    \"\"\"\n    Predict the type of question that is being asked.\n    Here are some tips on how to predict the question type:\n    CONTACT: Searches for some contact information.\n    TIMELINE_QUERY: \"When did something happen?\n    DOCUMENT_SEARCH: \"Find me a document\"\n    COMPARE_CONTRAST: \"Compare and contrast two things\"\n    EMAIL: \"Find me an email, search for an email\"\n    PHOTOS: \"Find me a photo, search for a photo\"\n    SUMMARY: \"Summarize a large amount of data\"\n    \"\"\"\n\n    # If you want only one classification, just change it to\n    #   `classification: QuestionType` rather than `classifications: List[QuestionType]``\n    chain_of_thought: str = Field(\n        ..., description=\"The chain of thought that led to the classification\"\n    )\n    classification: List[QuestionType] = Field(\n        description=f\"An accuracy and correct prediction predicted class of question. Only allowed types: {[t.value for t in QuestionType]}, should be used\",\n    )\n\n    @field_validator(\"classification\", mode=\"before\")\n    def validate_classification(cls, v):\n        # sometimes the API returns a single value, just make sure it's a list\n        if not isinstance(v, list):\n            v = [v]\n        return v\n\n\n@traceable(name=\"classify-question\")\nasync def classify(data: str) -> QuestionClassification:\n    \"\"\"\n    Perform multi-label classification on the input text.\n    Change the prompt to fit your use case.\n\n    Args:\n        data (str): The input text to classify.\n    \"\"\"\n    async with sem:  # some simple rate limiting\n        return data, await client.chat.completions.create(\n            model=\"gpt-4-turbo-preview\",\n            response_model=QuestionClassification,\n            max_retries=2,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Classify the following question: {data}\",\n                },\n            ],\n        )\n\n\nasync def main(questions: List[str]):\n    tasks = [classify(question) for question in questions]\n\n    for task in asyncio.as_completed(tasks):\n        question, label = await task\n        resp = {\n            \"question\": question,\n            \"classification\": [c.value for c in label.classification],\n            \"chain_of_thought\": label.chain_of_thought,\n        }\n        resps.append(resp)\n    return resps\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    questions = [\n        \"What was that ai app that i saw on the news the other day?\",\n        \"Can you find the trainline booking email?\",\n        \"what did I do on Monday?\",\n        \"Tell me about todays meeting and how it relates to the email on Monday\",\n    ]\n\n    resp = asyncio.run(main(questions))\n\n    for r in resp:\n        print(\"q:\", r[\"question\"])\n        #> q: what did I do on Monday?\n        print(\"c:\", r[\"classification\"])\n        #> c: ['SUMMARY']\n```\n\n----------------------------------------\n\nTITLE: Real-world Task Extraction Example\nDESCRIPTION: Comprehensive example showing how to extract action items from meeting notes, including assignees, due dates, and priorities. Demonstrates practical application of list extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/list_extraction.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\nfrom datetime import date\n\nclient = instructor.from_openai(OpenAI())\n\nclass Assignee(BaseModel):\n    name: str\n    email: Optional[str] = None\n\nclass ActionItem(BaseModel):\n    description: str = Field(..., description=\"The task that needs to be completed\")\n    assignee: Assignee = Field(..., description=\"The person responsible for the task\")\n    due_date: Optional[date] = Field(None, description=\"The deadline for the task\")\n    priority: str = Field(..., description=\"Priority level: Low, Medium, or High\")\n\n# Extract action items from meeting notes\naction_items = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Meeting Notes - Project Kickoff\n        Date: 2023-05-15\n        \n        Attendees: John (john@example.com), Sarah (sarah@example.com), Mike\n        \n        Discussion points:\n        1. John will prepare the project timeline by next Friday. This is high priority.\n        2. Sarah needs to contact the client for requirements clarification by Wednesday. Medium priority.\n        3. Mike is responsible for setting up the development environment. Due by tomorrow, high priority.\n        \"\"\"}\n    ],\n    response_model=List[ActionItem]\n)\n\n# Process the extracted action items\nfor item in action_items:\n    due_str = item.due_date.isoformat() if item.due_date else \"Not specified\"\n    print(f\"Task: {item.description}\")\n    print(f\"Assignee: {item.assignee.name} ({item.assignee.email or 'No email'})\")\n    print(f\"Due: {due_str}, Priority: {item.priority}\")\n    print(\"---\")\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-Powered Validation with Instructor\nDESCRIPTION: Demonstrates how to use the llm_validator from Instructor to create a more flexible and context-aware content moderation system using language models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\nclass UserMessage(BaseModel):\n    message: Annotated[\n        str, AfterValidator(llm_validator(\"don't say objectionable things\"))\n    ]\n\n\ntry:\n    UserMessage(\n        message=\"Violence is always acceptable, as long as we silence the witness\"\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Assertion failed, The statement promotes violence, which is objectionable. [type=assertion_error, input_value='Violence is always accep... we silence the witness', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Table Extraction Function Using OpenAI GPT-4 Vision in Python\nDESCRIPTION: This code snippet defines the extract function that uses the OpenAI GPT-4 Vision model to analyze images and extract table data. It sends a request to the OpenAI API with an image URL and instructions for table extraction, returning the results as a MultipleTables object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/tables_from_vision.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract(url: str) -> MultipleTables:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo\",\n        max_tokens=4000,\n        response_model=MultipleTables,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": url},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"\"\"\n                            First, analyze the image to determine the most appropriate headers for the tables.\n                            Generate a descriptive h1 for the overall image, followed by a brief summary of the data it contains.\n                            For each identified table, create an informative h2 title and a concise description of its contents.\n                            Finally, output the markdown representation of each table.\n                            Make sure to escape the markdown table properly, and make sure to include the caption and the dataframe.\n                            including escaping all the newlines and quotes. Only return a markdown table in dataframe, nothing else.\n                        \"\"\",\n                    },\n                ],\n            }\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Basic Prompt Template Implementation in Python with Instructor\nDESCRIPTION: Demonstrates the basic setup of a prompt template using Instructor and OpenAI to extract person information. Uses a Person model with name, age, and occupation fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/prompt_templates.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    occupation: str\n\n# Define a template with parameters\nprompt_template = \"\"\"\nExtract information about the person mentioned in the following {document_type}:\n\n{content}\n\nPlease provide their name, age, and occupation.\n\"\"\"\n\n# Use the template with specific values\ndocument_type = \"email\"\ncontent = \"Hi team, I'm introducing our new project manager, Sarah Johnson. She's 34 and has been in project management for 8 years.\"\n\nprompt = prompt_template.format(\n    document_type=document_type,\n    content=content\n)\n\n# Extract structured data using the formatted prompt\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": prompt}\n    ],\n    response_model=Person\n)\n```\n\n----------------------------------------\n\nTITLE: Rate-Limited Concurrent Processing with Semaphore and as_completed in Python\nDESCRIPTION: Demonstrates rate-limited concurrent processing using asyncio.Semaphore and asyncio.as_completed. This method combines the benefits of processing results as they become available with controlled concurrency.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/learn-async.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsem = asyncio.Semaphore(2)\n\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -> Person:\n    async with sem:\n        return await extract_person(text)\n\n\nasync def rate_limited_as_completed(sem: Semaphore):\n    all_persons = []\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person)\n```\n\n----------------------------------------\n\nTITLE: Implementing Maybe Pattern with OpenAI Integration\nDESCRIPTION: Demonstrates a complete implementation of the Maybe pattern including model definitions and an extraction function that uses OpenAI's API. The function handles both successful extractions and error cases, returning appropriately structured responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/maybe.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n# This enables the `response_model` keyword\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n\n\ndef extract(content: str) -> MaybeUser:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeUser,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\n        ],\n    )\n\n\nuser1 = extract(\"Jason is a 25-year-old scientist\")\nprint(user1.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": null,\n  \"error\": false,\n  \"message\": null\n}\n\"\"\"\n\nuser2 = extract(\"Unknown user\")\nprint(user2.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": null,\n  \"error\": true,\n  \"message\": \"Unknown user\"\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Logfire with OpenAI and Instructor for LLM Observability\nDESCRIPTION: Setup code to integrate Logfire with OpenAI and Instructor. This configuration records all Pydantic operations and instruments the OpenAI client for monitoring API calls.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\nimport logfire\n\n\nopenai_client = OpenAI()\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\"))  # (1)!\nlogfire.instrument_openai(openai_client)  # (2)!\nclient = instructor.from_openai(openai_client)\n```\n\n----------------------------------------\n\nTITLE: Nested Object Extraction with Cerebras and Instructor\nDESCRIPTION: Demonstrates how to extract nested objects using Instructor with Cerebras. It defines Address and User models with nested structures and extracts complex information from a given text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cerebras.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport instructor\nfrom cerebras.cloud.sdk import Cerebras\n\nclient = instructor.from_cerebras(Cerebras())\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\n# Create structured output with nested objects\nuser = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n        Extract: Jason is 25 years old.\n        He lives at 123 Main St, New York, USA\n        and has a summer house at 456 Beach Rd, Miami, USA\n    \"\"\",\n        }\n    ],\n    model=\"llama3.1-70b\",\n    response_model=User,\n)\n\nprint(user)\n#> {\n#>     'name': 'Jason',\n#>     'age': 25,\n#>     'addresses': [\n#>         {\n#>             'street': '123 Main St',\n#>             'city': 'New York',\n#>             'country': 'USA'\n#>         },\n#>         {\n#>             'street': '456 Beach Rd',\n#>             'city': 'Miami',\n#>             'country': 'USA'\n#>         }\n#>     ]\n#> }\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Models for Result Reranking in Python\nDESCRIPTION: Creates Pydantic models to structure the LLM output for reranking search results. The Label class tracks chunk IDs, reasoning, and relevancy scores, while RerankedResults sorts the labels by relevancy.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/llm-as-reranker.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Label(BaseModel):\n    chunk_id: int = Field(description=\"The unique identifier of the text chunk\")\n    chain_of_thought: str = Field(\n        description=\"The reasoning process used to evaluate the relevance\"\n    )\n    relevancy: int = Field(\n        description=\"Relevancy score from 0 to 10, where 10 is most relevant\",\n        ge=0,\n        le=10,\n    )\n\n\nclass RerankedResults(BaseModel):\n    labels: list[Label] = Field(description=\"List of labeled and ranked chunks\")\n\n    @field_validator(\"labels\")\n    @classmethod\n    def model_validate(cls, v: list[Label]) -> list[Label]:\n        return sorted(v, key=lambda x: x.relevancy, reverse=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Response Models with Pydantic\nDESCRIPTION: Shows how to create a Pydantic model with field descriptions for LLM data extraction\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/start-here.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    name: str = Field(description=\"The user's full name\")\n    age: int = Field(description=\"The user's age in years\")\n    # The descriptions help the LLM understand what to extract\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt Template Generation with Instructor and OpenAI\nDESCRIPTION: Python implementation of a prompt mining system using Instructor and OpenAI. It defines a PromptTemplate class and a function to generate multiple variations of prompt templates from a given input prompt. The system uses GPT-4 to generate more concise and understandable prompt templates.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/thought_generation/chain_of_thought_few_shot/prompt_mining.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\n\nclass PromptTemplate(BaseModel):\n    prompt_template: str = Field(\n        description=(\n            \"\"\"\n            A template that has the subject and object that we\n            want to extract from the prompt replaced with a\n            single placeholder of {subject} and {object}.\n            Rephrase the prompt if necessary to make it more\n            concise and easier to understand\n            \"\"\"\n        ),\n    )\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_prompt_templates(prompt: str):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are an expert prompt miner that excels at \"\n                    \"generating prompt templates which are more \"\n                    \"concise and easier to understand\\n\\nYou are \"\n                    \"about to be passed a prompt to extract 3 new \"\n                    \"prompt templates for\"\n                ),\n            },\n            {\"role\": \"system\", \"content\": prompt},\n        ],\n        response_model=list[PromptTemplate],\n        temperature=0,\n        max_retries=3,\n        model=\"gpt-4o\",\n    )\n\n\nif __name__ == \"__main__\":\n    prompt = \"France is the capital of Paris\"\n    prompt_template = generate_prompt_templates(prompt)\n    for prompt in prompt_template:\n        print(prompt)\n        #> prompt_template='{subject} is the capital of {object}'\n        #> prompt_template='The capital of {object} is {subject}'\n        #> prompt_template=\"{object}'s capital is {subject}\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Analogical Prompting using Instructor and OpenAI\nDESCRIPTION: A Python implementation of Analogical Prompting that uses Instructor library with OpenAI's API. The code defines Pydantic models for structuring responses and implements a function that processes queries using the analogical prompting technique. It includes example usage for calculating the area of a square given its vertices.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom textwrap import dedent\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass RelevantProblem(BaseModel):\n    problem_explanation: str\n    solution: str\n\n\nclass Response(BaseModel):\n    relevant_problems: list[RelevantProblem] = Field(\n        max_length=3,\n        min_length=3,\n    )\n    answer: RelevantProblem\n\n\ndef analogical_prompting(query: str):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                <problem>\n                {query}\n                </problem>\n\n                Relevant Problems: Recall three relevant and\n                distinct problems. For each problem, describe\n                it and explain the solution before solving\n                the problem\n                \"\"\"\n                ),\n            }\n        ],\n        model=\"gpt-4o\",\n        response_model=Response,\n    )\n\n\nif __name__ == \"__main__\":\n    query = (\n        \"What is the area of the square with the four \"\n        \"vertices at (-2, 2), (2, -2), (-2, -6), and \"\n        \"(-6, -2)?\"\n    )\n    response = analogical_prompting(query)\n    for problem in response.relevant_problems:\n        print(problem.model_dump_json(indent=2))\n\n    print(response.answer.model_dump_json(indent=2))\n```\n\n----------------------------------------\n\nTITLE: Implementing Automated CoT Example Selection in Python\nDESCRIPTION: A complete implementation that clusters and selects diverse examples for few-shot Chain of Thought prompting. Uses sentence transformers for embeddings, K-means clustering for grouping similar questions, and OpenAI's GPT for generating reasoning steps. Includes example selection criteria based on reasoning step length.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/thought_generation/chain_of_thought_few_shot/auto_cot.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport numpy as np\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom sklearn.cluster import KMeans\nfrom sentence_transformers import SentenceTransformer\n\nclient = instructor.patch(OpenAI())\nNUM_CLUSTERS = 2\n\n\nclass Example(BaseModel):\n    question: str\n    reasoning_steps: list[str]\n\n\nclass FinalAnswer(BaseModel):\n    reasoning_steps: list[str]\n    answer: int\n\n\ndef cluster_and_sort(questions, n_clusters=NUM_CLUSTERS):\n    # Cluster\n    embeddings = SentenceTransformer('all-MiniLM-L6-v2').encode(questions)\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(embeddings)\n\n    # Sort\n    sorted_clusters = [[] for _ in range(kmeans.n_clusters)]\n    for question, embedding, label in zip(questions, embeddings, kmeans.labels_):\n        center = kmeans.cluster_centers_[label]\n        distance = np.linalg.norm(embedding - center)\n        sorted_clusters[label].append((distance, question))\n    for cluster in sorted_clusters:\n        cluster.sort()  # Sort by distance\n\n    return sorted_clusters\n\n\ndef sample(cluster):\n    for question in cluster:\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            response_model=Example,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an AI assistant that generates step-by-step reasoning for mathematical questions.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Q: {question}\\nA: Let's think step by step.\",\n                },\n            ],\n        )\n        if (\n            len(response.reasoning_steps) <= 5\n        ):  # If we satisfy the selection criteria, we've found our question for this cluster\n            return response\n\n\nif __name__ == \"__main__\":\n    questions = [\n        \"How many apples are left if you have 10 apples and eat 3?\",\n        \"What's the sum of 5 and 7?\",\n        \"If you have 15 candies and give 6 to your friend, how many do you have left?\",\n        \"What's 8 plus 4?\",\n        \"You start with 20 stickers and use 8. How many stickers remain?\",\n        \"Calculate 6 added to 9.\",\n    ]\n\n    # Cluster and sort the questions\n    sorted_clusters = cluster_and_sort(questions)\n\n    # Sample questions that match selection criteria for each cluster\n    selected_examples = [sample(cluster) for cluster in sorted_clusters]\n    print(selected_examples)\n\n    # Use selected questions as examples for the LLM\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=FinalAnswer,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                {selected_examples}\n                If there are 10 books in my bad and I read 8 of them, how many books do I have left? Let's think step by step.\n                \"\"\",\n            }\n        ],\n    )\n\n    print(response.reasoning_steps)\n    print(response.answer)\n```\n\n----------------------------------------\n\nTITLE: Synchronous User Data Extraction with Vertex AI\nDESCRIPTION: Example of extracting structured user data using Instructor with Vertex AI in synchronous mode. Demonstrates basic setup and usage of the Pydantic model for type validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/vertex.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nfrom pydantic import BaseModel\n\nvertexai.init()\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n)\n\n# note that client.chat.completions.create will also work\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#> User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Extracting Information from Audio using OpenAI's Chat Completions API in Python\nDESCRIPTION: This code snippet demonstrates how to use the gpt-4o-audio-preview model to extract structured information from an audio file. It utilizes the instructor library to create a Person model and process the audio input, returning a structured Person object with name and age.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/openai-multimodal.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\nfrom instructor.multimodal import Audio\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-audio-preview\",\n    response_model=Person,\n    modalities=[\"text\"],\n    audio={\"voice\": \"alloy\", \"format\": \"wav\"},\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Extract the following information from the audio\",\n                Audio.from_path(\"./output.wav\"),\n            ],\n        },\n    ],\n)\n\nprint(resp)\n# Expected output: Person(name='Jason', age=20)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Information from Audio using OpenAI and Instructor in Python\nDESCRIPTION: This code snippet demonstrates how to use OpenAI's audio model with Instructor to extract structured information from an audio file. It defines a Pydantic model for the extracted data, processes the audio file, and uses OpenAI's API to extract the specified information.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/audio_extraction.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\nfrom instructor.multimodal import Audio\nimport base64\n\n# Initialize the OpenAI client with Instructor\nclient = instructor.from_openai(OpenAI())\n\n# Define the structure for extracted information\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Read and encode the audio file\nwith open(\"./output.wav\", \"rb\") as f:\n    encoded_string = base64.b64encode(f.read()).decode(\"utf-8\")\n\n# Extract information from the audio\nresp = client.chat.completions.create(\n    model=\"gpt-4-audio-preview\",\n    response_model=Person,\n    modalities=[\"text\"],\n    audio={\"voice\": \"alloy\", \"format\": \"wav\"},\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Extract the following information from the audio\",\n                Audio.from_path(\"./output.wav\"),\n            ],\n        },\n    ],\n)\n\nprint(resp)\n# Example output: Person(name='Jason', age=20)\n```\n\n----------------------------------------\n\nTITLE: Basic User Model with Pydantic Validation\nDESCRIPTION: Demonstrates basic validation using Pydantic with field constraints and custom validators for a user model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/validation.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field, field_validator\n\n\nclass User(BaseModel):\n    \"\"\"Model representing a user with validation rules.\"\"\"\n\n    name: str = Field(\n        ..., min_length=2, description=\"User's full name, minimum 2 characters\"\n    )\n    age: int = Field(..., ge=0, le=150, description=\"User's age between 0 and 150\")\n    emails: List[str] = Field(description=\"List of user's email addresses\")\n\n    @field_validator('emails')\n    @classmethod\n    def validate_emails(cls, v):\n        \"\"\"Validate that all email addresses contain an @ symbol.\"\"\"\n        if not all('@' in email for email in v):\n            raise ValueError('Invalid email format')\n        return v\n```\n\n----------------------------------------\n\nTITLE: Partial Streaming with Cerebras and Instructor\nDESCRIPTION: Shows how to use partial streaming with Instructor and Cerebras. It defines a Person model and streams the response as it's being generated, allowing for immediate processing of partial results.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cerebras.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom cerebras.cloud.sdk import Cerebras, AsyncCerebras\nfrom pydantic import BaseModel\nfrom typing import Iterable\n\nclient = instructor.from_cerebras(Cerebras(), mode=instructor.Mode.CEREBRAS_JSON)\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create_partial(\n    model=\"llama3.1-70b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Ivan is 27 and lives in Singapore\",\n        }\n    ],\n    response_model=Person,\n    stream=True,\n)\n\nfor person in resp:\n    print(person)\n    # > name=None age=None\n    # > name='Ivan' age=None\n    # > name='Ivan' age=27\n```\n\n----------------------------------------\n\nTITLE: Validating Outputs with Pydantic and LLM in Python\nDESCRIPTION: This snippet shows how to use Pydantic for output validation and LLM-based retry on failure. It defines a QuestionAnswer model with a custom validator to prevent objectionable content in answers.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instructor import llm_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\", client=client)),\n    ]\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior by encouraging evil and stealing. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Data Models for Citations and Answers\nDESCRIPTION: Creates Pydantic models to structure the output data. The Citation model captures relevant text with page numbers, while the Answer model combines citations with explanations and reasoning.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generating-pdf-citations.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Citation(BaseModel):\n    reason_for_relevance: str\n    text: list[str]\n    page_number: int\n\n\nclass Answer(BaseModel):\n    chain_of_thought: str\n    citations: list[Citation]\n    answer: str\n```\n\n----------------------------------------\n\nTITLE: Generating structured output using Instructor with LiteLLM (Asynchronous)\nDESCRIPTION: Python code showing how to use Instructor with LiteLLM to create structured, type-safe outputs asynchronously. It defines a User model and extracts information from a given text using asyncio.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/litellm.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom litellm import acompletion\nimport instructor\nfrom pydantic import BaseModel\nimport asyncio\n\n# Enable instructor patches for async\nclient = instructor.from_litellm(acompletion)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_user():\n    user = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n        ],\n        response_model=User,\n    )\n    return user\n\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)  # User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Implementing Content Moderation with Pydantic field_validator\nDESCRIPTION: Demonstrates how to use Pydantic's field_validator decorator to create a custom validator that checks for blacklisted words in a message.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError, field_validator\n\n\nclass UserMessage(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -> str:\n        for word in v.split():  # (1)!\n            if word.lower() in {'rob', 'steal'}:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain of Density Summarization with OpenAI and Instructor in Python\nDESCRIPTION: This function uses OpenAI's API and Instructor to generate increasingly concise and entity-dense summaries of an article through multiple iterations. It requires the OpenAI and Instructor libraries, as well as custom response models (InitialSummary and RewrittenSummary).\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=InitialSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"The generated summary should be about 80 words.\",\n            },\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for _i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\n                },\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create(\n            model=\"gpt-4-1106-preview\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\n\n                Perform the following two tasks\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\n\n                Guidelines\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n                - Missing entities can appear anywhere in the new summary\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n                \"\"\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n                },\n                *missing_entity_message,\n            ],\n            max_retries=3,\n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n```\n\n----------------------------------------\n\nTITLE: Implementing Single-Label Classification with OpenAI API in Python\nDESCRIPTION: This code snippet demonstrates how to perform single-label classification using the OpenAI API. It defines a ClassificationResponse model, a classify function, and includes examples of classifying text as SPAM or NOT_SPAM. The script uses the instructor library to patch the OpenAI client for enhanced functionality.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/single_classification.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nfrom openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n\nclass ClassificationResponse(BaseModel):\n    \"\"\"\n    A few-shot example of text classification:\n\n    Examples:\n    - \"Buy cheap watches now!\": SPAM\n    - \"Meeting at 3 PM in the conference room\": NOT_SPAM\n    - \"You've won a free iPhone! Click here\": SPAM\n    - \"Can you pick up some milk on your way home?\": NOT_SPAM\n    - \"Increase your followers by 10000 overnight!\": SPAM\n    \"\"\"\n\n    label: Literal[\"SPAM\", \"NOT_SPAM\"] = Field(\n        ...,\n        description=\"The predicted class label.\",\n    )\n\n\ndef classify(data: str) -> ClassificationResponse:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=ClassificationResponse,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: <text>{data}</text>\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    for text, label in [\n        (\"Hey Jason! You're awesome\", \"NOT_SPAM\"),\n        (\"I am a nigerian prince and I need your help.\", \"SPAM\"),\n    ]:\n        prediction = classify(text)\n        assert prediction.label == label\n        print(f\"Text: {text}, Predicted Label: {prediction.label}\")\n        #> Text: Hey Jason! You're awesome, Predicted Label: NOT_SPAM\n        #> Text: I am a nigerian prince and I need your help., Predicted Label: SPAM\n```\n\n----------------------------------------\n\nTITLE: Custom LLM Validator Implementation\nDESCRIPTION: Implementation of a custom validator using OpenAI and Instructor to validate content against specific rules.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.from_openai(OpenAI())\n\n\ndef validator(v):\n    statement = \"don't say objectionable things\"\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Does `{v}` follow the rules: {statement}\",\n            },\n        ],\n        # this comes from client = instructor.from_openai(OpenAI())\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return v\n```\n\n----------------------------------------\n\nTITLE: PDF Processing and Summarization using Gemini and Instructor\nDESCRIPTION: This Python script demonstrates how to use Gemini and Instructor to process a PDF file and extract a structured summary. It includes initialization of the Gemini client, definition of the output structure using Pydantic, file upload, and summary extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chat-with-your-pdf-with-gemini.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom google.ai.generativelanguage_v1beta.types.file import File\nfrom pydantic import BaseModel\nimport time\n\n# Initialize the client\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    )\n)\n\n\n# Define your output structure\nclass Summary(BaseModel):\n    summary: str\n\n\n# Upload the PDF\nfile = genai.upload_file(\"path/to/your.pdf\")\n\n# Wait for file to finish processing\nwhile file.state != File.State.ACTIVE:\n    time.sleep(1)\n    file = genai.get_file(file.name)\n    print(f\"File is still uploading, state: {file.state}\")\n\nprint(f\"File is now active, state: {file.state}\")\nprint(file)\n\nresp = client.chat.completions.create(\n    messages=[\n        {\"role\": \"user\", \"content\": [\"Summarize the following file\", file]},\n    ],\n    response_model=Summary,\n)\n\nprint(resp.summary)\n```\n\n----------------------------------------\n\nTITLE: Complete Instructor Implementation with OpenAI\nDESCRIPTION: A comprehensive implementation showing how to use Instructor with OpenAI, including model definition, client initialization, and data processing with proper error handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/templates/cookbook_template.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport os\nfrom typing import List, Optional, Dict, Any\n\n# Third-party imports\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\n# Local imports (if any)\n# from my_module import my_function\n\n# Set up environment\n# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"  # Uncomment and replace with your API key if not set\n\n# Define your models with proper type annotations\nclass MyResponseModel(BaseModel):\n    \"\"\"Model representing the structured output from the LLM.\"\"\"\n    field1: str = Field(description=\"Description of field1\")\n    field2: int = Field(description=\"Description of field2\")\n    optional_field: Optional[List[str]] = Field(None, description=\"Optional field example\")\n\n# Initialize the client with explicit mode\nclient = instructor.from_openai(\n    OpenAI(),\n    mode=instructor.Mode.JSON  # Always specify mode explicitly\n)\n\ndef process_data(input_text: str) -> MyResponseModel:\n    \"\"\"\n    Process input text and return structured data.\n    \n    Args:\n        input_text: The text to be processed\n        \n    Returns:\n        A structured MyResponseModel object\n    \"\"\"\n    try:\n        result = client.chat.completions.create(\n            model=\"gpt-4o\",  # Use a consistent, current model version\n            messages=[\n                {\"role\": \"system\", \"content\": \"Extract structured information from the user input.\"},\n                {\"role\": \"user\", \"content\": input_text}\n            ],\n            response_model=MyResponseModel,\n        )\n        return result\n    except instructor.exceptions.InstructorError as e:\n        # Handle validation/extraction errors\n        print(f\"Extraction error: {e}\")\n        raise\n    except Exception as e:\n        # Handle other errors (API, network, etc.)\n        print(f\"Unexpected error: {e}\")\n        raise\n\n# Example usage\nif __name__ == \"__main__\":\n    input_text = \"Field1 is example data and field2 is 42.\"\n    result = process_data(input_text)\n    print(result.model_dump_json(indent=2))\n\n# Expected output:\n# {\n#   \"field1\": \"example data\",\n#   \"field2\": 42,\n#   \"optional_field\": null\n# }\n```\n\n----------------------------------------\n\nTITLE: Image Analysis with Anthropic and Instructor\nDESCRIPTION: Shows how to analyze images using Anthropic's vision capabilities through Instructor's multimodal interface. Includes multiple methods for loading images and extracting structured information.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import Image\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom anthropic import Anthropic\n\n\nclass ImageDescription(BaseModel):\n    objects: list[str] = Field(..., description=\"The objects in the image\")\n    scene: str = Field(..., description=\"The scene of the image\")\n    colors: list[str] = Field(..., description=\"The colors in the image\")\n\n\nclient = instructor.from_anthropic(Anthropic())\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/image.jpg\"\n# Multiple ways to load an image:\nresponse = client.chat.completions.create(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=ImageDescription,\n    max_tokens=1000,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"What is in this image?\",\n                # Option 1: Direct URL with autodetection\n                Image.from_url(url),\n            ],\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Disk-Based Caching with diskcache in Python\nDESCRIPTION: This snippet shows how to use diskcache for persistent, large data caching in Python. It implements a custom decorator that caches function results to disk, focusing on functions that return Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/caching.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  # (4)\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #  (2)\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -> UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Using Image Class with OpenAI for Image Analysis in Python\nDESCRIPTION: This snippet demonstrates how to use the Image class from Instructor to analyze an image using OpenAI's GPT-4 model. It loads an image from a URL and generates a description with a list of items in the image.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/multimodal.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom instructor.multimodal import Image\nimport openai\nfrom pydantic import BaseModel\n\n\nclass ImageDescription(BaseModel):\n    description: str\n    items: list[str]\n\n\n# Use our sample image provided above.\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/image.jpg\"\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=ImageDescription,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"What is in this image?\",\n                Image.from_url(url),\n            ],\n        }\n    ],\n)\n\nprint(response)\n# > description='A bush with numerous clusters of blueberries surrounded by green leaves, under a cloudy sky.' items=['blueberries', 'green leaves', 'cloudy sky']\n```\n\n----------------------------------------\n\nTITLE: Complex Data Type Validation in Python\nDESCRIPTION: Demonstrates validation for complex data types including email and phone number formatting. Uses regex patterns for validation and includes data transformation logic.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/custom_validators.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, field_validator\nimport instructor\nfrom openai import OpenAI\nimport re\n\nclient = instructor.from_openai(OpenAI())\n\nclass Contact(BaseModel):\n    name: str\n    email: str\n    phone: str\n    \n    @field_validator('email')\n    @classmethod\n    def validate_email(cls, value):\n        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n        if not re.match(pattern, value):\n            raise ValueError(\"Invalid email format\")\n        return value\n    \n    @field_validator('phone')\n    @classmethod\n    def validate_phone(cls, value):\n        # Remove non-digit characters and validate\n        digits_only = re.sub(r'\\D', '', value)\n        if len(digits_only) < 10:\n            raise ValueError(\"Phone number must have at least 10 digits\")\n        return digits_only  # Return the cleaned version\n```\n\n----------------------------------------\n\nTITLE: Basic Streaming Implementation with Instructor and OpenAI\nDESCRIPTION: Demonstrates how to implement basic streaming functionality for structured responses using Instructor. The example shows streaming a UserProfile model with incremental field updates.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/streaming/basics.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Define your data structure\nclass UserProfile(BaseModel):\n    name: str\n    bio: str\n    interests: list[str]\n\n# Set up client\nclient = instructor.from_openai(OpenAI())\n\n# Enable streaming\nfor partial in client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Generate a profile for Alex Chen\"}\n    ],\n    response_model=UserProfile,\n    stream=True  # This enables streaming\n):\n    # Print each update as it arrives\n    print(\"\\nUpdate received:\")\n    \n    # Access available fields\n    if hasattr(partial, \"name\") and partial.name:\n        print(f\"Name: {partial.name}\")\n    if hasattr(partial, \"bio\") and partial.bio:\n        print(f\"Bio: {partial.bio[:30]}...\")\n    if hasattr(partial, \"interests\") and partial.interests:\n        print(f\"Interests: {', '.join(partial.interests)}\")\n```\n\n----------------------------------------\n\nTITLE: Basic List Extraction with Pydantic Models in Python\nDESCRIPTION: Demonstrates how to extract a list of people using a wrapper model and Pydantic BaseModel classes. Shows initialization of OpenAI client with Instructor and definition of data models for structured extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/list_extraction.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\n# Initialize the client\nclient = instructor.from_openai(OpenAI())\n\n# Define a single item model\nclass Person(BaseModel):\n    name: str = Field(..., description=\"The person's full name\")\n    age: int = Field(..., description=\"The person's age in years\")\n\n# Define a wrapper model for the list\nclass PeopleList(BaseModel):\n    people: List[Person] = Field(..., description=\"List of people mentioned in the text\")\n\n# Extract the list\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Here's information about some people:\n        - John Smith is 35 years old\n        - Mary Johnson is 28 years old\n        - Robert Davis is 42 years old\n        \"\"\"}\n    ],\n    response_model=PeopleList\n)\n\n# Access the extracted data\nfor i, person in enumerate(response.people):\n    print(f\"Person {i+1}: {person.name}, {person.age} years old\")\n```\n\n----------------------------------------\n\nTITLE: Adding Chunk ID Validation to Pydantic Model in Python\nDESCRIPTION: Enhances the Label model with a field validator that ensures the chunk_id is present in the chunks list. This is useful for working with complex identifiers and ensuring data integrity.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/llm-as-reranker.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Label(BaseModel):\n    chunk_id: int = Field(description=\"The unique identifier of the text chunk\")\n    ...\n\n    @field_validator(\"chunk_id\")\n    @classmethod\n    def validate_chunk_id(cls, v: int, info: ValidationInfo) -> int:\n        context = info.context\n        chunks = context[\"chunks\"]\n        if v not in [chunk[\"id\"] for chunk in chunks]:\n            raise ValueError(\n                f\"Chunk with id {v} not found, must be one of {[chunk['id'] for chunk in chunks]}\"\n            )\n        return v\n```\n\n----------------------------------------\n\nTITLE: Implementing Judge Function for Text Relevance Evaluation\nDESCRIPTION: Defines a function that uses the OpenAI API to judge the relevance between a question and a text, returning a structured Judgment object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/pairwise-llm-judge.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef judge_relevance(question: str, text: str) -> Judgment:\n    return client.chat.create(\n        model=\"gpt-4\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                    You are tasked with comparing a question and a piece of text to determine if they are relevant to each other or similar in some way. Your goal is to analyze the content, context, and potential connections between the two.\n\n                    To determine if the question and text are relevant or similar, please follow these steps:\n\n                    1. Carefully read and understand both the question and the text.\n                    2. Identify the main topic, keywords, and concepts in the question.\n                    3. Analyze the text for any mention of these topics, keywords, or concepts.\n                    4. Consider any potential indirect connections or implications that might link the question and text.\n                    5. Evaluate the overall context and purpose of both the question and the text.\n\n                    As you go through this process, please use a chain of thought approach. Write out your reasoning for each step inside <thought> tags.\n\n                    After your analysis, provide a boolean judgment on whether the question and text are similar or relevant to each other. Use \"true\" if they are similar or relevant, and \"false\" if they are not.\n\n                    Before giving your final judgment, provide a justification for your decision. Explain the key factors that led to your conclusion.\n\n                    Please ensure your analysis is thorough, impartial, and based on the content provided.\n                \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"\"\"\n                    Here is the question:\n\n                    <question>\n                    {{question}}\n                    </question>\n\n                    Here is the text:\n                    <text>\n                    {{text}}\n                    </text>\n                \"\"\",\n            },\n        ],\n        response_model=Judgment,\n        context={\"question\": question, \"text\": text},\n    )\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Groq and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with Groq to extract structured data. It uses a Pydantic model for data extraction and Groq's client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom groq import Groq\nfrom pydantic import BaseModel\n\nclient = instructor.from_groq(Groq())\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nresp = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    response_model=ExtractUser,\n    messages=[{\"role\": \"user\", \"content\": \"Extract Jason is 25 years old.\"}],\n)\n\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Iterable Extraction with Cerebras and Instructor\nDESCRIPTION: Demonstrates how to use iterable extraction with Instructor and Cerebras. It defines a Person model and extracts multiple users from a given sentence, streaming the results as they are generated.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cerebras.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom cerebras.cloud.sdk import Cerebras, AsyncCerebras\nfrom pydantic import BaseModel\nfrom typing import Iterable\n\nclient = instructor.from_cerebras(Cerebras(), mode=instructor.Mode.CEREBRAS_JSON)\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create_iterable(\n    model=\"llama3.1-70b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract all users from this sentence : Chris is 27 and lives in San Francisco, John is 30 and lives in New York while their college roomate Jessica is 26 and lives in London\",\n        }\n    ],\n    response_model=Person,\n    stream=True,\n)\n\nfor person in resp:\n    print(person)\n    # > Person(name='Chris', age=27)\n    # > Person(name='John', age=30)\n    # > Person(name='Jessica', age=26)\n```\n\n----------------------------------------\n\nTITLE: Implementing Self-Ask Technique with Instructor and OpenAI\nDESCRIPTION: This code demonstrates how to implement the Self-Ask technique using Instructor and OpenAI. It defines Pydantic models to structure the response format, creates a function to handle Self-Ask queries, and shows a practical example of determining who was US president when superconductivity was discovered.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/zero_shot/self_ask.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass FollowUp(BaseModel):\n    question: str = Field(description=\"The follow-up question\")\n    answer: str = Field(description=\"The answer to the follow-up question\")\n\n\nclass Response(BaseModel):\n    follow_ups_required: bool\n    follow_ups: list[FollowUp]\n    final_answer: str\n\n\ndef self_ask(query):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"Query: {query}\n                        Are follow-up questions needed?\n                        If so, generate follow-up questions, their answers, and then the final answer to the query.\n                        \"\"\",  # !(1)\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"Who was president of the U.S. when superconductivity was discovered?\"\n\n    response = self_ask(query)\n\n    print(response.follow_ups_required)\n    #> True\n    for follow_up in response.follow_ups:\n        print(follow_up)\n        \"\"\"\n        question='When was superconductivity discovered?' answer='Superconductivity was discovered in April 1911.'\n        \"\"\"\n        \"\"\"\n        question='Who was president of the U.S. in April 1911?' answer='William Howard Taft was the President of the United States in April 1911.'\n        \"\"\"\n    print(response.final_answer)\n    \"\"\"\n    William Howard Taft was president of the U.S. when superconductivity was discovered.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing COSP with Instructor and OpenAI API\nDESCRIPTION: Complete implementation of Chain of Self-adaptive Prompts (COSP) pattern including response models, similarity scoring, batch generation, and example selection. Uses OpenAI's API for embeddings and completions, with async operations for better performance.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/cosp.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI, OpenAI\nfrom collections import defaultdict, Counter\nimport asyncio\nfrom textwrap import dedent\nimport math\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass Response(BaseModel):\n    chain_of_thought: list[str]\n    answer: int\n\n\nclass ResponseScore(BaseModel):\n    query: str\n    response: Response\n    score: float\n\n    def format_response(self):\n        return dedent(\n            f\"\"\"\n            Q: {self.query}\n            A: {''.join(self.response.chain_of_thought)}. Therefore the answer is {self.response.answer}.\n            \"\"\"\n        )\n\n\ndef cosine_similarity(vec1: list[float], vec2: list[float]):\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n    magnitude2 = math.sqrt(sum(b * b for b in vec2))\n\n    if magnitude1 * magnitude2 == 0:\n        return 0  # Handle the case of zero vectors\n\n    return dot_product / (magnitude1 * magnitude2)\n\n\ndef score_repetitiveness(prediction: Response):\n    if len(prediction.chain_of_thought) == 1:\n        return 0\n\n    embedding = OpenAI().embeddings.create(\n        input=prediction.chain_of_thought, model=\"text-embedding-3-small\"\n    )\n    embedding = [item.embedding for item in embedding.data]\n\n    ttl = 0\n    num_comparisons = 0\n    for idx in range(len(embedding)):\n        for idx2 in range(idx + 1, len(embedding)):\n            ttl += cosine_similarity(embedding[idx], embedding[idx2])\n            num_comparisons += 1\n\n    return ttl / num_comparisons if num_comparisons > 0 else 0\n\n\nasync def generate_cot_response(query: str) -> tuple[Response, str]:\n    return (\n        await client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": query}],\n            response_model=Response,\n            temperature=0.4,\n        ),\n        query,\n    )\n\n\nasync def generate_batch_cot_responses(\n    queries: list[str], m: int\n) -> list[tuple[Response, str]]:\n    coros = [generate_cot_response(query) for query in queries for _ in range(m)]\n    return await asyncio.gather(*coros)\n\n\ndef score_entropy(predictions: list[Response]):\n    counter = Counter([prediction.answer for prediction in predictions])\n\n    prob = [counter[i] / len(predictions) for i in counter]\n\n    numer = -sum([p * math.log(p) for p in prob])\n    denom = math.log(len(predictions))\n\n    return numer / denom\n\n\ndef score_responses(\n    predictions: list[tuple[Response, str]], trade_off_param: float\n) -> list[ResponseScore]:\n    query_to_responses: dict[str, list[Response]] = defaultdict(list)\n    for prediction, query in predictions:\n        query_to_responses[query].append(prediction)\n\n    query_to_entropy = {\n        query: score_entropy(predictions)\n        for query, predictions in query_to_responses.items()\n    }\n\n    return [\n        ResponseScore(\n            query=query,\n            response=prediction,\n            score=query_to_entropy[query]\n            + trade_off_param * score_repetitiveness(prediction),\n        )\n        for prediction, query in predictions\n    ]\n\n\ndef get_top_k_examples(queries: list[ResponseScore], k: int):\n    \"\"\"\n    This gets the top k responses that have the minimum possible score\n    \"\"\"\n    sorted_responses = sorted(queries, key=lambda x: x.score)\n    return sorted_responses[:k]\n\n\nasync def generate_answer_with_examples(query: str, examples: list[ResponseScore]):\n    formatted_examples = \"\\n\".join([example.format_response() for example in examples])\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are a world class AI system that excels at answering user queries\n\n                <query>\n                {query}\n                </query>\n\n                <examples>\n                {formatted_examples}\n                </examples>\n                \"\"\"\n                ),\n            }\n        ],\n        response_model=Response,\n    )\n\n\nasync def generate_final_answers(\n    query: str, examples: list[ResponseScore], number_samples: int\n):\n    coros = [\n        generate_answer_with_examples(query, examples) for _ in range(number_samples)\n    ]\n\n    return await asyncio.gather(*coros)\n\n\nif __name__ == \"__main__\":\n    query = (\n        \"The schools debate team had 5 boys and 40 girls on it. \"\n        \"If they were split into groups of 9 how many groups \"\n        \"could they make?\"\n    )\n\n    example_questions = [\n        (\n            \"Debby's class is going on a field trip to the zoo. \"\n            \"If each van can hold 4 people and there are 2 students \"\n            \"and 6 adults going, how many vans will they need?\"\n        ),\n        (\n            \"Nancy had 80 files on her computer. She deleted 31 of \"\n            \"them and put the rest into folders with 7 files in each \"\n            \"one. How many folders did Nancy end up with?\"\n        ),\n        (\n            \"At the arcade, Tom won 32 tickets playing 'whack a mole' \"\n            \"and 25 tickets playing 'skee ball'. If he spent 7 of his \"\n            \"tickets on a hat, how many tickets does Tom have left?\"\n        ),\n    ]\n\n    m = 2  # Number of Reasoning Chains per example ( Step 1 )\n    k = 3  # Number of Examples to include in final prompt (Step 2)\n    n = 2  # Number of Reasoning Chains For Self-Consistency ( Step 2 )\n\n    # Step 1 : Generate the examples\n    responses = asyncio.run(generate_batch_cot_responses(example_questions, m))\n    scored_responses = score_responses(responses, 0.2)\n\n    chosen_examples = get_top_k_examples(scored_responses, k)\n\n    # Step 2 : Run Self-Consistency\n    final_responses = asyncio.run(generate_final_answers(query, chosen_examples, n))\n\n    c = Counter([response.answer for response in final_responses])\n    answer = c.most_common(1)[0][0]\n\n    print(answer)\n    #> 5\n```\n\n----------------------------------------\n\nTITLE: Comparing entity density of two summaries in Python\nDESCRIPTION: This snippet compares the entity density of two summaries to demonstrate the concept of entity density.  The `calculate_entity_density` function is used to compute the token count, entity count, and entity density for each summary, and the results are printed to show the difference in information density.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsummary_1 = \"\"\"\nThis article discusses an incident that occurred during the Chinese Grand Prix\ninvolving two racing drivers, Jenson Button and Pastor Maldonado. The two were \ncompeting for the 13th place when Button collided with Maldonado's vehicle, \ncausing damage to both cars. The incident resulted in a penalty for Button, \nwho was demoted to 14th place. Maldonado, on the other hand, had to retire from \nthe race due to the damage his car sustained.\n\"\"\"\n\nsummary_2 = \"\"\"\nJenson Button's McLaren collided with Pastor Maldonado's Lotus during the Chinese \nGrand Prix, causing front wing damage to Button's car and rear-end damage to \nMaldonado's, forcing his retirement. Button received a five-second penalty and \ntwo superlicence points, dropping himto 14th. Fernando Alonso advanced two places, \nwhile Button was lapped by Nico Rosberg and Alonso by Sebastian Vettel and \nKimi Raikkonen.\n\"\"\"\n\ncalculate_entity_density(summary_1), calculate_entity_density(summary_2)\n```\n\n----------------------------------------\n\nTITLE: Handling Token Limit Exceptions in OpenAI Requests\nDESCRIPTION: Demonstrates exception handling for context length exceeded scenarios using IncompleteOutputException. The code shows how to catch the exception and access the token count for implementing custom logic when token limits are exceeded.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/usage.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.exceptions import IncompleteOutputException\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\ntry:\n    client.chat.completions.create_with_completion(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserExtract,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n        ],\n    )\nexcept IncompleteOutputException as e:\n    token_count = e.last_completion.usage.total_tokens  # type: ignore\n    # your logic here\n```\n\n----------------------------------------\n\nTITLE: Defining Query and DateRange Models with Pydantic in Python\nDESCRIPTION: This snippet defines two Pydantic models, DateRange and Query, to structure data for queries. The Query model includes a method for reporting usage metrics alongside its attributes.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport instructor\n\nfrom openai import AsyncOpenAI\nfrom datetime import date\nfrom pydantic import BaseModel, Field\n\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\n    def report(self):\n        dct = self.model_dump()\n        dct[\"usage\"] = self._raw_response.usage.model_dump()\n        return dct\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Role Management with Enum in Pydantic\nDESCRIPTION: This snippet demonstrates using Python's Enum class with Pydantic to create a standardized set of user roles. The code defines a Role enum with predefined options and incorporates it into a UserDetail model with a descriptive field to guide role assignment.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/enums.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom enum import Enum\n\n\nclass Role(Enum):\n    PRINCIPAL = \"PRINCIPAL\"\n    TEACHER = \"TEACHER\"\n    STUDENT = \"STUDENT\"\n    OTHER = \"OTHER\"\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Complex Template Function for Product Review Extraction\nDESCRIPTION: Implements a more complex template function for extracting product review information with optional parameters. Uses a ProductReview model with multiple fields including lists.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/prompt_templates.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass ProductReview(BaseModel):\n    product_name: str\n    rating: int\n    pros: List[str]\n    cons: List[str]\n    summary: str\n\ndef create_review_extraction_prompt(\n    review_text: str,\n    product_category: str,\n    include_sentiment: bool = False\n) -> str:\n    sentiment_instruction = \"\"\"\n    Also include a brief sentiment analysis of the review.\n    \"\"\" if include_sentiment else \"\"\n    \n    return f\"\"\"\n    Extract product review information from the following {product_category} review:\n    \n    {review_text}\n    \n    Please identify:\n    - The name of the product being reviewed\n    - The numerical rating (1-5)\n    - A list of pros/positive points\n    - A list of cons/negative points\n    - A brief summary of the review\n    {sentiment_instruction}\n    \"\"\"\n\n# Use the template function\nreview_text = \"\"\"\nI recently purchased the UltraSound X300 headphones, and I'm mostly satisfied.\nThe sound quality is amazing and the battery lasts for days. They're also very\ncomfortable to wear for long periods. However, they're a bit pricey at $299, and\nthe Bluetooth occasionally disconnects. Overall, I'd give them 4 out of 5 stars.\n\"\"\"\n\nprompt = create_review_extraction_prompt(\n    review_text=review_text,\n    product_category=\"headphone\",\n    include_sentiment=True\n)\n\nreview = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": prompt}\n    ],\n    response_model=ProductReview\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Label Classification Function\nDESCRIPTION: This function performs multi-label classification on input text using the OpenAI API and the MultiClassPrediction model. It sends a chat completion request to classify the given support ticket into multiple categories.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/classification.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef multi_classify(data: str) -> MultiClassPrediction:\n    \"\"\"Perform multi-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=MultiClassPrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following support ticket: <ticket>{data}</ticket>\",\n            },\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Structured Data Extraction\nDESCRIPTION: Example of extracting structured data using the Instructor client with OpenAI's API.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/first_extraction.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nperson = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Person,\n    messages=[\n        {\"role\": \"user\", \"content\": \"John Doe is 30 years old\"}\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Endpoint with Instructor and FastAPI\nDESCRIPTION: This snippet defines a FastAPI endpoint that uses Instructor's Iterable support to stream multiple UserDetail objects. It utilizes AsyncOpenAI client and StreamingResponse for efficient data processing and delivery.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/full-fastapi-visibility.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Iterable\nfrom fastapi.responses import StreamingResponse\n\n\nclass MultipleUserData(BaseModel):\n    queries: list[str]\n\n\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    suppressed_client = AsyncOpenAI()\n    logfire.instrument_openai(\n        suppressed_client, suppress_other_instrumentation=False\n    )  # (1)!\n    client = instructor.from_openai(suppressed_client)\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": data.query},\n        ],\n    )\n\n    async def generate():\n        with logfire.span(\"Generating User Response Objects\"):\n            async for user in users:\n                resp_json = user.model_dump_json()\n                logfire.info(\"Returning user object\", value=resp_json)\n\n                yield resp_json\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Relevance Checking with Automatic Retrying in Instructor\nDESCRIPTION: This code snippet shows how to use validation context to ensure that an AI-generated answer is relevant to the given question. It utilizes Instructor's automatic retrying feature to re-ask the question if the initial response fails validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, field_validator, ValidationInfo\n\nclass RelevantAnswer(BaseModel):\n    answer: str\n    \n    @field_validator('answer')\n    def check_relevance(cls, answer: str, info: ValidationInfo) -> str:\n        question = info.context.get(\"question\", \"\")\n        if \"climate change\" in question.lower() and \"climate\" not in answer.lower():\n            raise ValueError(\"Answer doesn't address climate change as requested in the question\")\n        return answer\n\nclient = instructor.from_openai(\n    OpenAI(),\n    max_retries=2  # Will retry up to 2 times if validation fails\n)\n\nquestion = \"What are the major impacts of climate change?\"\n\nresult = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=RelevantAnswer,\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Answer the following question:\n\n        <question>\n        {{ question }}\n        </question>\n        \"\"\"}\n    ],\n    context={\"question\": question}\n)\n\nprint(result.answer)  # Guaranteed to mention climate change\n```\n\n----------------------------------------\n\nTITLE: Extracting Complex Organization Structure with Instructor\nDESCRIPTION: This comprehensive example demonstrates how to extract a complex organization structure using Instructor. It defines models for Employee, Department (with recursive sub-departments), and Organization, showcasing nested and recursive structures.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/nested_structure.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Employee(BaseModel):\n    name: str\n    title: str\n    \nclass Department(BaseModel):\n    name: str\n    head: Employee\n    employees: List[Employee]\n    sub_departments: List[\"Department\"] = []\n\n# Update for Pydantic's recursive model support\nDepartment.model_rebuild()\n\nclass Organization(BaseModel):\n    name: str\n    ceo: Employee\n    departments: List[Department]\n\n# Extract organization structure\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Acme Corporation\n        CEO: Jane Smith, Chief Executive Officer\n        \n        Departments:\n        \n        1. Engineering\n           Head: Bob Johnson, CTO\n           Employees:\n           - Sarah Lee, Senior Engineer\n           - Tom Brown, Software Developer\n           \n           Sub-departments:\n           - Frontend Team\n             Head: Lisa Wang, Frontend Lead\n             Employees:\n             - Mike Chen, UI Developer\n             - Ana Garcia, UX Designer\n           \n           - Backend Team\n             Head: David Kim, Backend Lead\n             Employees:\n             - James Wright, Database Engineer\n             - Rachel Patel, API Developer\n        \n        2. Marketing\n           Head: Michael Davis, CMO\n           Employees:\n           - Jennifer Miller, Marketing Specialist\n           - Robert Chen, Content Creator\n        \"\"\"}\n    ],\n    response_model=Organization\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Product Detection Function with GPT-4 Vision API\nDESCRIPTION: Defines a function that uses OpenAI's vision model to process a list of image URLs and identify products in each of them. It utilizes the instructor library to patch the OpenAI client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/image_to_ad_copy.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef read_images(image_urls: list[str]) -> IdentifiedProduct:\n    \"\"\"\n    Given a list of image URLs, identify the products in the images.\n    \"\"\"\n\n    logger.info(f\"Identifying products in images... {len(image_urls)} images\")\n\n    return client_image.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=IdentifiedProduct,\n        max_tokens=1024,  # can be changed\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Identify products using the given images and generate key features for each product.\",\n                    },\n                    *[\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n                        for url in image_urls\n                    ],\n                ],\n            }\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Streaming Product Recommendations with Instructor AI in Python\nDESCRIPTION: This snippet demonstrates how to use Instructor AI to stream product recommendations based on a user profile. It sets up the prompt, creates a streaming completion, and processes each recommendation as it arrives.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generator.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprompt = (\n    f\"Based on the following user profile:\\n{profile_data}\\nRank the following products from most relevant to least relevant:\\n\"\n    + '\\n'.join(\n        f\"{product['product_id']} {product['product_name']}\" for product in products\n    )\n)\n\nstart_perf = time.perf_counter()\nrecommendations_stream = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=True,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nfor product in recommendations_stream:\n    print(product)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    break\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Query Generation with Instructor\nDESCRIPTION: Demonstrates how to use Instructor to generate structured SearchQuery objects from natural language queries, including time-based constraints.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-timelines.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    response_model=SearchQuery,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a query generator for customer support tickets. The current date is 2024-02-17\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Show me customer support tickets opened in the past week.\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Distillation with Instructor for Function Fine-Tuning in Python\nDESCRIPTION: This code demonstrates how to use Instructor's distillation feature to prepare a dataset for fine-tuning. It creates a multiplication function that returns a Pydantic model and logs the interactions to a JSONL file for future fine-tuning.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/distillation.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions  # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -> Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n\n# Generate some data\nfor _ in range(10):\n    random.seed(42)\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n    #> a=754 b=214 result=161356\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Custom Validator in Python with Instructor\nDESCRIPTION: Demonstrates how to create a basic age validator using Pydantic field validators with Instructor. Includes validation logic to ensure age values fall within a reasonable range (0-120).\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/custom_validators.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, field_validator\nimport instructor\nfrom openai import OpenAI\n\n# Initialize the client\nclient = instructor.from_openai(OpenAI())\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    \n    @field_validator('age')\n    @classmethod\n    def validate_age(cls, value):\n        if value < 0 or value > 120:\n            raise ValueError(\"Age must be between 0 and 120\")\n        return value\n\n# Extract data with validation\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"The person's name is John and they are 150 years old.\"}\n    ],\n    response_model=Person\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Langfuse with Asynchronous OpenAI Client in Instructor\nDESCRIPTION: This snippet shows how to integrate Langfuse with an asynchronous OpenAI client using Instructor. It includes environment setup, client patching, and an asynchronous weather information extraction example.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/tracing_with_langfuse.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom langfuse.openai import openai\nfrom pydantic import BaseModel\nimport os\nimport asyncio\n\n# Set your API keys Here\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-\"\nos.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"\nos.environ[\"OPENAI_API_KEY] = \"sk-...\"\n\n\n# Patch Langfuse wrapper of synchronous OpenAI client with instructor\nclient = instructor.from_openai(openai.AsyncOpenAI())\n\n\nclass WeatherDetail(BaseModel):\n    city: str\n    temperature: int\n\n\nasync def main():\n    # Run synchronous OpenAI client\n    weather_info = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=WeatherDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": \"The weather in Paris is 18 degrees Celsius.\"},\n        ],\n    )\n\n    print(weather_info.model_dump_json(indent=2))\n    \"\"\"\n    {\n    \"city\": \"Paris\",\n    \"temperature\": 18\n    }\n    \"\"\"\n\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Creating AIResponse Model with Chain-of-Thought Validation in Python\nDESCRIPTION: This snippet defines an AIResponse Pydantic model with a model validator that uses the validate_chain_of_thought function to ensure the answer follows the chain of thought.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\nfrom pydantic import model_validator\n\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -> Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n```\n\n----------------------------------------\n\nTITLE: Partial Streaming with Field Updates\nDESCRIPTION: Implementation of partial streaming that allows tracking specific field updates as they are completed.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom instructor.dsl import partial\n\nclass LongReport(BaseModel):\n    executive_summary: str = partial()\n    detailed_analysis: str = partial()\n    conclusion: str = partial()\n\nclient = instructor.from_openai(OpenAI())\n\nfor chunk in client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=LongReport,\n    stream=True,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a detailed report on climate change impacts.\"}\n    ]\n):\n    # Each chunk will contain completed fields\n    if hasattr(chunk, 'executive_summary') and chunk.executive_summary:\n        print(\"Executive Summary Complete!\")\n    if hasattr(chunk, 'detailed_analysis') and chunk.detailed_analysis:\n        print(\"Analysis Complete!\")\n    if hasattr(chunk, 'conclusion') and chunk.conclusion:\n        print(\"Conclusion Complete!\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Self Calibration with Instructor and OpenAI\nDESCRIPTION: This code demonstrates how to implement Self Calibration for language models using the Instructor library. It defines a Pydantic model to structure the evaluation response and creates a function to assess whether a model's answer to a question is valid. The implementation includes chain-of-thought reasoning and a boolean validation flag.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/self_criticism/self_calibration.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass SelfCalibration(BaseModel):\n    chain_of_thought: str\n    is_valid_answer: bool = Field(description=\"Whether the answer is correct or not\")\n\n\ndef evaluate_model_output(original_prompt: str, model_response: str):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Question: {original_prompt}\n\n                {model_response}\n\n                Is this a valid answer to the question?\n                Make sure to examine the question\n                thoroughly and generate a complete\n                reasoning for why the answer is correct\n                or not before responding.\n                \"\"\",\n            }\n        ],\n        response_model=SelfCalibration,\n        model=\"gpt-4o\",\n    )\n\n\nif __name__ == \"__main__\":\n    original_prompt = \"\"\"\n    Question: Who was the third president of the\n    United States?\n    \"\"\"\n    model_response = \"\"\"\n    Here are some brainstormed ideas: James Monroe\n    Thomas Jefferson\n    Jefferson\n    Thomas Jefferson\n    George Washington\n    \"\"\"\n    response = evaluate_model_output(original_prompt, model_response)\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"Let's examine the question\n      carefully: 'Who was the third president of the\n      United States?'\\n\\nThe brainstormed ideas are:\n      \\n1. James Monroe\\n2. Thomas Jefferson\\n3.\n      Jefferson\\n4. Thomas Jefferson\\n5. George\n      Washington.\\n\\nTo determine the validity of these\n      answers, I'll cross-check with historical\n      records.\\n\\n1. James Monroe was not the third\n      president; he was the fifth president.\\n2. Thomas\n      Jefferson was indeed the third president of the\n      United States.\\n3. 'Jefferson' is a correct but\n      incomplete answer; it lacks the first name, though\n      it is commonly understood.\\n4. 'Thomas Jefferson'\n      is the full name and correct answer.\\n5. George\n      Washington was the first president, not the\n      third.\\n\\nTherefore, the correct, valid answer to\n      the question 'Who was the third president of the\n      United States?' is 'Thomas Jefferson,' and this\n      answer is correct.\",\n      \"is_valid_answer\": true\n    }\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Gemini and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with Google's Gemini model to extract structured data. It uses a Pydantic model for data extraction and Google's generative AI client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n    mode=instructor.Mode.GEMINI_JSON,\n)\n\n# note that client.chat.completions.create will also work\nresp = client.messages.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=ExtractUser,\n)\n\nassert isinstance(resp, ExtractUser)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Implementing System 2 Attention (S2A) with Instructor and OpenAI in Python\nDESCRIPTION: This code demonstrates a two-step implementation of the S2A technique using the instructor library with OpenAI models. It first asks the model to rewrite a prompt by extracting only relevant information, then passes that refined prompt back to get a final answer. The example shows how the technique filters out irrelevant context from a math problem.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/zero_shot/s2a.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Step1(BaseModel):\n    relevant_context: str = Field(..., description=\"Relevant context\")\n    user_query: str = Field(..., description=\"The question from the user\")\n\n\nclass Step2(BaseModel):\n    answer: int\n\n\ndef rewrite_prompt(query):\n    rewritten_prompt = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Step1,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Given the following text by a user, extract the part\n                    that is actually relevant to their question. Please\n                    include the actual question or query that the user\n                    is asking.\n\n                    Text by user:\n                    {query}\n                    \"\"\",  # (1)!\n            }\n        ],\n    )\n    return rewritten_prompt\n\n\ndef generate_final_response(rewritten_prompt):\n    final_response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Step2,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"{rewritten_prompt.relevant_context}\n                    Question: {rewritten_prompt.user_query}\"\"\",\n            }\n        ],\n    )\n    return final_response\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"Mary has 3 times as much candy as Megan.\n        Mary then adds 10 more pieces of candy to her collection.\n        Max is 5 years older than Mary.\n        If Megan has 5 pieces of candy, how many does Mary have in total?\n        \"\"\"\n\n    # Step 1: Rewrite the prompt\n    rewritten_prompt = rewrite_prompt(query)\n    print(rewritten_prompt.relevant_context)\n    \"\"\"\n    Mary has 3 times as much candy as Megan. Mary then adds 10 more pieces of candy to her collection. If Megan has 5 pieces of candy, how many does Mary have in total?\n    \"\"\"\n    print(rewritten_prompt.user_query)\n    #> how many does Mary have in total?\n\n    # Step 2: Generate the final response\n    final_response = generate_final_response(rewritten_prompt)\n    print(final_response.answer)\n    #> 25\n```\n\n----------------------------------------\n\nTITLE: Complete Receipt Processing Implementation with Example\nDESCRIPTION: Shows the complete implementation including models, validation, and extraction logic, along with a practical example using a sample receipt image URL.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extracting_receipts.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://templates.mediamodifier.com/645124ff36ed2f5227cbf871/supermarket-receipt-template.jpg\"\n\n\nreceipt = extract(url)\nprint(receipt)\n\"\"\"\nitems=[Item(name='Lorem ipsum', price=9.2, quantity=1), Item(name='Lorem ipsum dolor sit', price=19.2, quantity=1), Item(name='Lorem ipsum dolor sit amet', price=15.0, quantity=1), Item(name='Lorem ipsum', price=15.0, quantity=1), Item(name='Lorem ipsum', price=15.0, quantity=1), Item(name='Lorem ipsum dolor sit', price=15.0, quantity=1), Item(name='Lorem ipsum', price=19.2, quantity=1)] total=107.6\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Story Elements with Pydantic Models in Python\nDESCRIPTION: This code defines Pydantic models for representing people and their relationships in a story. It includes classes for People, Relationship, and Story, which can be used to structure extracted information from text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass People(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Story(BaseModel):\n    people: List[People]\n    relationships: List[Relationship]\n```\n\n----------------------------------------\n\nTITLE: Defining Single-Label Classification Structure with Pydantic\nDESCRIPTION: This snippet defines a Pydantic model for single-label classification of spam messages. It includes a Literal field for labels and uses the model's docstring for few-shot examples to improve classification accuracy.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/classification.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nfrom openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n\nclass ClassificationResponse(BaseModel):\n    \"\"\"\n    A few-shot example of text classification:\n    \n    Examples:\n    - \"Buy cheap watches now!\": SPAM\n    - \"Meeting at 3 PM in the conference room\": NOT_SPAM\n    - \"You've won a free iPhone! Click here\": SPAM\n    - \"Can you pick up some milk on your way home?\": NOT_SPAM\n    - \"Increase your followers by 10000 overnight!\": SPAM\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        ...,\n        description=\"The chain of thought that led to the prediction.\",\n    )\n    label: Literal[\"SPAM\", \"NOT_SPAM\"] = Field(\n        ...,\n        description=\"The predicted class label.\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Initial Summary Model with Pydantic for Chain of Density\nDESCRIPTION: This code defines a Pydantic model for the initial summary in the Chain of Density process. It includes detailed docstrings that are used by the LLM when generating outputs, specifying the desired characteristics of the summary.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chain-of-density.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Simplifying Error Handling with Instructor's Maybe Pattern\nDESCRIPTION: This code snippet shows how to use the instructor library to create a Maybe pattern dynamically from any BaseModel. It simplifies the process of creating a Maybe type for any class, streamlining error handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n\n\nMaybeUser = instructor.Maybe(UserDetail)\n```\n\n----------------------------------------\n\nTITLE: Custom Validation Error Messages with Pydantic\nDESCRIPTION: Shows how to implement custom error messages for validation using Pydantic Field properties. The example demonstrates price validation for a product model with a custom error message when the price is not greater than zero.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/basics.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass Product(BaseModel):\n    name: str\n    price: float = Field(\n        gt=0, \n        description=\"Product price in USD\",\n        json_schema_extra={\"error_msg\": \"Price must be greater than zero\"}\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Iterable Objects from Claude Responses\nDESCRIPTION: Shows how to use the create_iterable method to extract multiple instances of the same response model from a single prompt. This code extracts multiple User objects from text data and processes them sequentially.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport os\n\n# Third-party imports\nimport anthropic\nfrom instructor import from_anthropic\nfrom pydantic import BaseModel, Field\n\n# Set up environment (typically handled before script execution)\n# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Uncomment and replace with your API key if not set\n\n# Initialize client with explicit mode\nclient = from_anthropic(\n    anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\")),\n    mode=instructor.Mode.ANTHROPIC_TOOLS\n)\n\n# Define your model with proper annotations\nclass User(BaseModel):\n    \"\"\"Model representing a basic user.\"\"\"\n    name: str = Field(description=\"The user's full name\")\n    age: int = Field(description=\"The user's age in years\")\n\ntry:\n    # Create an iterable of user objects\n    users = client.chat.completions.create_iterable(\n        model=\"claude-3-haiku-20240307\",  # Use latest stable model\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Extract all users from the provided text into structured format.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"\"\"\n                Extract users:\n                1. Jason is 25 years old\n                2. Sarah is 30 years old\n                3. Mike is 28 years old\n                \"\"\",\n            },\n        ],\n        max_tokens=4096,\n        response_model=User,\n    )\n\n    # Process each user as it's extracted\n    for user in users:\n        print(user)\n\n    # Expected output:\n    # > name='Jason' age=25\n    # > name='Sarah' age=30\n    # > name='Mike' age=28\nexcept Exception as e:\n    print(f\"Error during iteration: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Field Descriptions in Pydantic Models\nDESCRIPTION: Shows how to add field descriptions to Pydantic models using the Field class. These descriptions help guide the AI model during extraction by providing context for each field.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/simple_object.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass Book(BaseModel):\n    title: str = Field(description=\"The full title of the book\")\n    author: str = Field(description=\"The author's full name\")\n    publication_year: int = Field(description=\"The year the book was published\")\n```\n\n----------------------------------------\n\nTITLE: Implementing In-Memory Caching with functools.cache in Python\nDESCRIPTION: This snippet demonstrates how to use functools.cache for simple in-memory caching in Python. It caches the result of an OpenAI API call that extracts user details, improving performance for repeated calls with the same input.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/caching.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport functools\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@functools.cache\ndef extract(data) -> UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n\n\nstart = time.perf_counter()  # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#> Time taken: 0.5008833750034682\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#> Time taken: 1.2920063454657793e-06\n```\n\n----------------------------------------\n\nTITLE: Implementing Persistent Caching with diskcache\nDESCRIPTION: Shows how to implement persistent caching using diskcache. This approach is suitable for applications needing cache persistence between sessions or dealing with large datasets.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/caching.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  # (4)\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #  (2)\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -> UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Uploading and Processing Full Audio File with Gemini\nDESCRIPTION: This snippet demonstrates how to upload an entire audio file and process it using Gemini. It includes downloading the audio, saving it locally, and using it with the Gemini model for summarization.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/multi_modal_gemini.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom pydub import AudioSegment\n\n# Download the audio file\nurl = \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\"\nresponse = requests.get(url)\n\n# Save the audio file locally\nwith open(\"sample.mp3\", \"wb\") as file:\n    file.write(response.content)\n\nsound = AudioSegment.from_mp3(\"sample.mp3\")  # (2)!\nsound = sound[:60000]\nsound.export(\n    \"sample.mp3\", format=\"mp3\"\n)  # Save the processed audio segment as sample.mp3\n\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n    mode=instructor.Mode.GEMINI_JSON,  # (1)!\n)\n\nmp3_file = genai.upload_file(\"./sample.mp3\")  # (2)!\n\n\nclass Description(BaseModel):\n    description: str\n\n\nresp = client.create(\n    response_model=Description,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Summarize what's happening in this audio file and who the main speaker is\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": mp3_file,  # (3)!\n        },\n    ],\n)\n\nprint(resp)\n```\n\n----------------------------------------\n\nTITLE: Example Usage with Web Article Processing\nDESCRIPTION: Demonstrates complete workflow by processing a Transformers tutorial article, including web scraping, preprocessing, segmentation and content extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/document_segmentation.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura import fetch_url, extract\n\nurl = 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html'\ndownloaded = fetch_url(url)\ndocument = extract(downloaded)\n\ndocument_with_line_numbers, line2text = doc_with_lines(document)\nstructured_doc = get_structured_document(document_with_line_numbers)\nsegments = get_sections_text(structured_doc, line2text)\n```\n\n----------------------------------------\n\nTITLE: Implementing Citation Validation with Instructor and OpenAI\nDESCRIPTION: This snippet demonstrates how to use validation context to check if a citation exists in a source document. It uses the Instructor library with OpenAI to generate a statement and citation, then validates the citation against the provided source text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, field_validator, ValidationInfo\n\nclass CitationCheck(BaseModel):\n    statement: str\n    citation: str\n    \n    @field_validator('citation')\n    def validate_citation(cls, citation: str, info: ValidationInfo) -> str:\n        # Access the validation context\n        source_text = info.context.get(\"source_document\", \"\")\n        \n        # Check if the citation actually exists in the source document\n        if citation not in source_text:\n            raise ValueError(f\"Citation '{citation}' not found in source document\")\n        return citation\n\nclient = instructor.from_openai(OpenAI())\n\nsource_document = \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\"\n\nresult = client.chat.completions.create(\n    model=\"gpt-4o\",\n    response_model=CitationCheck,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Make a statement about Earth and provide a citation from the text.\"}\n    ],\n    context={\"source_document\": source_document}\n)\n\nprint(f\"Statement: {result.statement}\")\nprint(f\"Citation: {result.citation} (verified to exist in source)\")\n```\n\n----------------------------------------\n\nTITLE: Using a Fine-Tuned Model for Function Dispatch in Python\nDESCRIPTION: This snippet shows how to use a fine-tuned model to replace the actual function implementation. It configures Instructor to dispatch calls to the model instead of executing the function code.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/distillation.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import Instructions\nfrom pydantic import BaseModel\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -> Multiply:\n    # now this code will be short circuited and the model will be used instead.\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n```\n\n----------------------------------------\n\nTITLE: Handling Retry Exceptions with Instructor and Tenacity in Python\nDESCRIPTION: Demonstrates how to catch and handle retry exceptions, accessing information about the retry attempts and last completion.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/retrying.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, field_validator\nimport openai\nimport instructor\nfrom instructor.exceptions import InstructorRetryException\nfrom tenacity import Retrying, retry_if_not_exception_type, stop_after_attempt\n\n# Patch the OpenAI client to enable response_model\nclient = instructor.from_openai(openai.OpenAI())\n\n\n# Define a Pydantic model for the user details\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"age\")\n    def validate_age(cls, v: int):\n        raise ValueError(f\"You will never succeed with {str(v)}\")\n\n\nretries = Retrying(\n    retry=retry_if_not_exception_type(ZeroDivisionError), stop=stop_after_attempt(3)\n)\n# Use the client to create a user detail\ntry:\n    user = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[{\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}],\n        max_retries=retries,\n    )\nexcept InstructorRetryException as e:\n    print(e.messages[-1][\"content\"])  # type: ignore\n    \"\"\"\n    Validation Error found:\n    1 validation error for UserDetail\n    age\n      Value error, You will never succeed with 25 [type=value_error, input_value=25, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    Recall the function correctly, fix the errors\n    \"\"\"\n\n    print(e.n_attempts)\n    #> 3\n\n    print(e.last_completion)\n    \"\"\"\n    ChatCompletion(\n        id='chatcmpl-B7YgHmfrWA8FxsSxvzUUvdSe2lo9h',\n        choices=[\n            Choice(\n                finish_reason='stop',\n                index=0,\n                logprobs=None,\n                message=ChatCompletionMessage(\n                    content=None,\n                    refusal=None,\n                    role='assistant',\n                    audio=None,\n                    function_call=None,\n                    tool_calls=[\n                        ChatCompletionMessageToolCall(\n                            id='call_zvTyhnBKPIhDXrOCxNzlsgzN',\n                            function=Function(\n                                arguments='{\"name\":\"Jason\",\"age\":25}', name='UserDetail'\n                            ),\n                            type='function',\n                        )\n                    ],\n                ),\n            )\n        ],\n        created=1741141309,\n        model='gpt-3.5-turbo-0125',\n        object='chat.completion',\n        service_tier='default',\n        system_fingerprint=None,\n        usage=CompletionUsage(\n            completion_tokens=30,\n            prompt_tokens=522,\n            total_tokens=552,\n            completion_tokens_details=CompletionTokensDetails(\n                audio_tokens=0, reasoning_tokens=0\n            ),\n            prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n        ),\n    )\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-based Validation with Pydantic\nDESCRIPTION: Shows how to integrate LLM-based validation into a Pydantic model using the instructor library. The validator checks for objectionable content in answers.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/reask_validation.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\n\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\", client=client)),\n    ]\n\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior by encouraging evil and stealing. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/assertion_error\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Basic Retry Implementation with Pydantic Model in Python\nDESCRIPTION: Demonstrates basic retry functionality using a Product model with field validation. Shows how to initialize an Instructor client with retry settings and validate product data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/retry_mechanisms.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field, field_validator\n\n# Initialize the client with max_retries\nclient = instructor.from_openai(\n    OpenAI(),\n    max_retries=2  # Will try up to 3 times (initial + 2 retries)\n)\n\nclass Product(BaseModel):\n    name: str\n    price: float = Field(..., gt=0)\n    \n    @field_validator('name')\n    @classmethod\n    def validate_name(cls, v):\n        if len(v) < 3:\n            raise ValueError(\"Product name must be at least 3 characters\")\n        return v\n\n# This will automatically retry if validation fails\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Product: Pen, Price: -5\"}\n    ],\n    response_model=Product\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Content Filtering with LLM Validation in Python\nDESCRIPTION: This snippet shows how to use a BeforeValidator with an LLM-based validator to filter objectionable content from the model's responses, with automatic reasking on validation failure.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BeforeValidator\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\", client=client)),\n    ]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n\nresp.answer\n```\n\n----------------------------------------\n\nTITLE: Person Model with Field Descriptions\nDESCRIPTION: Enhanced Person model using Pydantic Field for adding descriptive metadata to help guide the extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/first_extraction.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass Person(BaseModel):\n    name: str = Field(description=\"Person's full name\")\n    age: int = Field(description=\"Person's age in years\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Union Types with Pydantic\nDESCRIPTION: Demonstrates basic usage of Union types to allow a field to accept multiple types (string or integer) in a Pydantic model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/unions.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union\nfrom pydantic import BaseModel\n\n\nclass Response(BaseModel):\n    value: Union[str, int]  # Can be either string or integer\n```\n\n----------------------------------------\n\nTITLE: Basic List Streaming with Instructor and OpenAI\nDESCRIPTION: Demonstrates how to stream a list of book objects using Instructor and OpenAI. The example shows model definition, client initialization, and processing streamed responses using Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/streaming/lists.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\n# Initialize the client\nclient = instructor.from_openai(OpenAI())\n\nclass Book(BaseModel):\n    title: str = Field(..., description=\"Book title\")\n    author: str = Field(..., description=\"Book author\")\n    year: int = Field(..., description=\"Publication year\")\n\n# Stream a list of books\nfor book in client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"List 5 classic science fiction books\"}\n    ],\n    response_model=List[Book],  # Note: Using List directly\n    stream=True\n):\n    print(f\"Received: {book.title} by {book.author} ({book.year})\")\n```\n\n----------------------------------------\n\nTITLE: Generating FastAPI Application Code with create_app Function\nDESCRIPTION: Example usage of the create_app function to generate a FastAPI application for person extraction. The function takes API path, task name, JSON schema path, and a Jinja2 prompt template as inputs and generates the necessary FastAPI code and Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/codegen-from-schema/readme.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfastapi_code = create_app(\n    api_path=\"/api/v1/extract_person\",\n    task_name=\"extract_person\",\n    json_schema_path=\"./input.json\",\n    prompt_template=\"Extract the person from the following: {{biography}}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining KnowledgeGraph and Supporting Classes in Python\nDESCRIPTION: This snippet defines the Node, Edge, and KnowledgeGraph classes using Pydantic. It includes methods for updating the graph and visualizing it using Graphviz.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/knowledge_graph.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nimport instructor\nfrom openai import OpenAI\nfrom graphviz import Digraph\n\n\nclass Node(BaseModel, frozen=True):\n    id: int\n    label: str\n    color: str\n\n\nclass Edge(BaseModel, frozen=True):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: Optional[List[Node]] = Field(..., default_factory=list)\n    edges: Optional[List[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -> \"KnowledgeGraph\":\n        \"\"\"Updates the current graph with the other graph, deduplicating nodes and edges.\"\"\"\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),\n            edges=list(set(self.edges + other.edges)),\n        )\n\n    def draw(self, prefix: str = None):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:  # (1)!\n            dot.node(str(node.id), node.label, color=node.color)\n\n        for edge in self.edges:  # (2)!\n            dot.edge(\n                str(edge.source), str(edge.target), label=edge.label, color=edge.color\n            )\n        dot.render(prefix, format=\"png\", view=True)\n\n\nclient = instructor.from_openai(OpenAI())\n```\n\n----------------------------------------\n\nTITLE: Visualizing Validation Flow with Mermaid\nDESCRIPTION: A flowchart diagram showing the validation process flow in Instructor, including model definition, LLM requests, validation checks, and retry mechanisms.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/validation.md#2025-04-14_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    A[Define Pydantic Model] --> B[Send Request to LLM]\n    B --> C[LLM Generates Response]\n    C --> D{Validate Response}\n    \n    D -->|Valid| E[Return Pydantic Object]\n    D -->|Invalid| F{Auto-Retry Enabled?}\n    \n    F -->|Yes| G[Send Error Context to LLM]\n    F -->|No| H[Raise ValidationError]\n    \n    G --> I[LLM Generates New Response]\n    I --> J{Validate Again}\n    \n    J -->|Valid| E\n    J -->|Invalid| K{Max Retries Reached?}\n    \n    K -->|No| G\n    K -->|Yes| H\n    \n    classDef success fill:#d4edda,stroke:#c3e6cb,color:#155724;\n    classDef error fill:#f8d7da,stroke:#f5c6cb,color:#721c24;\n    classDef process fill:#e2f0fb,stroke:#b8daff,color:#004085;\n    classDef decision fill:#fff3cd,stroke:#ffeeba,color:#856404;\n    \n    class A,B,C,G,I process\n    class D,F,J,K decision\n    class E success\n    class H error\n```\n\n----------------------------------------\n\nTITLE: Implementing Re2 (Re-Reading) Technique with Instructor and OpenAI\nDESCRIPTION: This code demonstrates how to implement the Re2 (Re-Reading) technique using the Instructor library with OpenAI. It defines a Pydantic Response model and a re2 function that constructs a prompt asking the model to read the question again with a critical thinking prompt. The example solves a simple tennis ball counting problem.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/zero_shot/re2.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Response(BaseModel):\n    answer: int\n\n\ndef re2(query, thinking_prompt):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Read the question again: {query} {thinking_prompt}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"Roger has 5 tennis balls.\n        He buys 2 more cans of tennis balls.\n        Each can has 3 tennis balls.\n        How many tennis balls does he have now?\n        \"\"\"\n    thinking_prompt = \"Let's think step by step.\"\n\n    response = re2(query=query, thinking_prompt=thinking_prompt)\n    print(response.answer)\n    #> 11\n```\n\n----------------------------------------\n\nTITLE: Working with Complex Nested Pydantic Models\nDESCRIPTION: Demonstration of using Instructor with nested Pydantic models for more complex data structures.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/getting-started.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    addresses: List[Address]\n\nperson = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Person,\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Extract: John Smith is 35 years old. \n        He has homes at 123 Main St, Springfield, IL 62704 and \n        456 Oak Ave, Chicago, IL 60601.\n        \"\"\"}\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Executing a Query Understanding System with Instructor and OpenAI\nDESCRIPTION: This snippet demonstrates how to use the Instructor library with OpenAI to create a query understanding system. It sends a natural language query to GPT-4 and receives a structured MetaphorQuery response.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-and-beyond.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.from_openai(OpenAI())\n\nquery = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=MetaphorQuery,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\",\n        },\n        {\"role\": \"user\", \"content\": \"What are some recent developments in AI?\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Responses for Examples in COSP using Python\nDESCRIPTION: This code demonstrates how to generate multiple responses from a model for potential examples in COSP's first stage. It uses the instructor library with OpenAI to create responses with confidence scores for measuring consistency.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/few_shot/cosp.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclass Response(BaseModel):\n    content: str = Field(description=\"The model's response to the prompt\")\n    confidence: float = Field(description=\"Confidence score between 0 and 1\")\n\nclient = instructor.from_openai(OpenAI())\n\ndef generate_responses(prompt: str, n: int = 3) -> List[Response]:\n    responses = []\n    for _ in range(n):\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            response_model=Response\n        )\n        responses.append(response)\n    return responses\n```\n\n----------------------------------------\n\nTITLE: Example-Based Synthetic Data Generation with Field Examples\nDESCRIPTION: Enhanced synthetic data generation using Pydantic Field examples to guide the model towards generating celebrity-like names.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/fake-data.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str = Field(examples=[\"Timothee Chalamet\", \"Zendaya\"])\n    age: int\n\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Generate a {count} synthetic users\"},\n        ],\n    )\n\n\nfor user in generate_fake_users(5):\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Cohere and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with Cohere to extract structured data. It uses a Pydantic model for data extraction and Cohere's client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom cohere import Client\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_cohere(Client())\n\nresp = client.chat.completions.create(\n    response_model=ExtractUser,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n)\n\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Basic Instructor Usage Example\nDESCRIPTION: Demonstrates how to extract structured person data from text using Instructor with OpenAI\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/start-here.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Import the necessary libraries\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Define the structure you want\nclass Person:\n    name: str\n    age: int\n    city: str\n\n# Connect to the LLM with Instructor\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data\nperson = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Person,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract a person from: John is 30 years old and lives in New York.\"}\n    ]\n)\n\n# Now you have a structured object\nprint(f\"Name: {person.name}\")  # Name: John\nprint(f\"Age: {person.age}\")    # Age: 30\nprint(f\"City: {person.city}\")  # City: New York\n```\n\n----------------------------------------\n\nTITLE: Defining Structured Data Schema in Python with Pydantic\nDESCRIPTION: This snippet demonstrates how to define a structured data schema using Pydantic's BaseModel in Python. It's a key step in using the Instructor library for working with LLMs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/philosophy.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass StructuredData(BaseModel):\n```\n\n----------------------------------------\n\nTITLE: Processing PDFs with Gemini AI's Raw Bytes Method\nDESCRIPTION: Demonstrates how to extract structured information from PDF documents using Gemini models. The code shows multiple ways to load PDFs (URL, local file, base64) and extract specific data like invoice totals and line items.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import PDF\nfrom pydantic import BaseModel\nimport instructor\nfrom google.genai import Client\n\n\nclass Receipt(BaseModel):\n    total: int\n    items: list[str]\n\n\nclient = instructor.from_genai(Client())\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\n# Multiple ways to load an PDF:\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash\",\n    response_model=Receipt,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Extract out the total and line items from the invoice\",\n                # Option 1: Direct URL\n                PDF.from_url(url),\n                # Option 2: Local file\n                # PDF.from_path(\"path/to/local/invoice.pdf\"),\n                # Option 3: Base64 string\n                # PDF.from_base64(\"base64_encoded_string_here\")\n                # Option 4: Autodetect\n                # PDF.autodetect(<url|path|base64>)\n            ],\n        },\n    ],\n)\n\nprint(response)\n# > Receipt(total=220, items=['English Tea', 'Tofu'])\n```\n\n----------------------------------------\n\nTITLE: Basic Structured Data Extraction with Anthropic\nDESCRIPTION: Demonstrates how to set up and use Instructor with Anthropic to extract structured user data using Pydantic models. Shows error handling and model validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport os\nfrom typing import List\n\n# Third-party imports\nimport anthropic\nimport instructor\nfrom pydantic import BaseModel, Field\n\n# Define your models with proper type annotations\nclass Properties(BaseModel):\n    \"\"\"Model representing a key-value property.\"\"\"\n    name: str = Field(description=\"The name of the property\")\n    value: str = Field(description=\"The value of the property\")\n\n\nclass User(BaseModel):\n    \"\"\"Model representing a user with properties.\"\"\"\n    name: str = Field(description=\"The user's full name\")\n    age: int = Field(description=\"The user's age in years\")\n    properties: List[Properties] = Field(description=\"List of user properties\")\n\n# Initialize the client with explicit mode\nclient = instructor.from_anthropic(\n    anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\")),\n    mode=instructor.Mode.ANTHROPIC_TOOLS  # Using Anthropic's tool calling API\n)\n\ntry:\n    # Extract structured data\n    user_response = client.chat.completions.create(\n        model=\"claude-3-haiku-20240307\",  # Use latest stable model\n        max_tokens=1024,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Extract structured information based on the user's request.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Create a user for a model with a name, age, and properties.\",\n            }\n        ],\n        response_model=User,\n    )\n\n    # Print the result as formatted JSON\n    print(user_response.model_dump_json(indent=2))\n\nexcept instructor.exceptions.InstructorError as e:\n    print(f\"Validation error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Structured Output with llama-cpp-python\nDESCRIPTION: Demonstrates setting up llama-cpp-python with Instructor to generate structured outputs using Pydantic models. The code initializes a Llama model with specific configurations, patches it with Instructor capabilities, and extracts structured user details from text input.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/llama-cpp-python.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport llama_cpp\nimport instructor\nfrom llama_cpp.llama_speculative import LlamaPromptLookupDecoding\nfrom pydantic import BaseModel\n\n\nllama = llama_cpp.Llama(\n    model_path=\"../../models/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q4_K_M.gguf\",\n    n_gpu_layers=-1,\n    chat_format=\"chatml\",\n    n_ctx=2048,\n    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=2),\n    logits_all=True,\n    verbose=False,\n)\n\n\ncreate = instructor.patch(\n    create=llama.create_chat_completion_openai_v1,\n    mode=instructor.Mode.JSON_SCHEMA,\n)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract `Jason is 30 years old`\",\n        }\n    ],\n    response_model=UserDetail,\n)\n\nprint(user)\n#> name='Jason' age=30\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-based Validation with Pydantic\nDESCRIPTION: This code shows how to use Pydantic validators with Instructor's llm_validator to implement content filtering. It creates a model that validates answers against objectionable content before accepting them.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, BeforeValidator\nfrom instructor import llm_validator\n\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\")),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Synchronous User Extraction with Cerebras and Instructor\nDESCRIPTION: Demonstrates how to use Instructor with Cerebras for synchronous extraction of user information. It initializes the client, defines a User model, and extracts name and age from a given sentence.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cerebras.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom cerebras.cloud.sdk import Cerebras\nfrom pydantic import BaseModel\n\nclient = instructor.from_cerebras(Cerebras())\n\n# Enable instructor patches\nclient = instructor.from_cerebras(client)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n# Create structured output\nresp = client.chat.completions.create(\n    model=\"llama3.1-70b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the name and age of the person in this sentence: John Smith is 29 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#> User(name='John Smith', age=29)\n```\n\n----------------------------------------\n\nTITLE: Processing a PDF and Generating Citations with Gemini\nDESCRIPTION: Demonstrates the full workflow for analyzing a PDF, uploading it to Gemini, and generating structured citations. The code uploads the file, waits for processing, and then queries about export restrictions while enforcing the response structure.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generating-pdf-citations.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npdf_path = \"./10k.pdf\"\ndoc = pymupdf.open(pdf_path)\n\n# Upload the PDF\nfile = genai.upload_file(pdf_path)\n\n# Wait for file to finish processing\nwhile file.state != File.State.ACTIVE:\n    time.sleep(1)\n    file = genai.get_file(file.name)\n    print(f\"File is still uploading, state: {file.state}\")\n\nresp: Answer = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can answer questions about the provided pdf file. You will be given a question and a pdf file. Your job is to answer the question using the information in the pdf file. Provide all citations that are relevant to the question and make sure that the coordinates are accurate.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"What were all of the export restrictions announced by the USG in 2023? What chips did they affect?\",\n                file,\n            ],\n        },\n    ],\n    response_model=Answer,\n)\n\nprint(resp)\n# Answer(\n#     chain_of_thought=\"The question asks about export restrictions in 2023. Page 25 mentions the USG announcing licensing requirements for A100 and H100 chips in August 2022, and additional licensing requirements for a subset of these products in July 2023.\",\n#     citations=[\n#         Citation(\n#             reason_for_relevance=\"Describes the export licensing requirements and which chips they affect.\",\n#             text=[\n#                 \"In August 2022, the U.S. government, or the USG, announced licensing requirements that, with certain exceptions, impact exports to China (including Hong\",\n#                 \"Kong and Macau) and Russia of our A100 and H100 integrated circuits, DGX or any other systems or boards which incorporate A100 or H100 integrated circuits.\",\n#                 \"In July 2023, the USG informed us of an additional licensing requirement for a subset of A100 and H100 products destined to certain customers and other\",\n#                 \"regions, including some countries in the Middle East.\",\n#             ],\n#             page_number=25,\n#         )\n#     ],\n#     answer=\"In 2023, the U.S. government (USG) announced new licensing requirements for the export of certain chips to China, Russia, and other countries.  These chips included the A100 and H100 integrated circuits, the DGX system, and any other systems or boards incorporating the A100 or H100 chips.\",\n# )\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Streaming Data Extraction\nDESCRIPTION: Implements asynchronous streaming data extraction using AsyncOpenAI. Shows how to use async/await syntax with iterables for processing extracted data in real-time.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/iterable.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nasync def print_iterable_results():\n    model = await client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make two up people\"},\n        ],\n    )\n    async for m in model:\n        print(m)\n\n\nimport asyncio\n\nasyncio.run(print_iterable_results())\n```\n\n----------------------------------------\n\nTITLE: Defining Product Recommendation Model with Instructor AI in Python\nDESCRIPTION: This snippet sets up the Instructor AI client and defines Pydantic models for product recommendations. It uses the Iterable type to model a list of recommendations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generator.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\n\nclass ProductRecommendation(BaseModel):\n    product_id: str\n    product_name: str\n\n\nRecommendations = Iterable[ProductRecommendation]\n```\n\n----------------------------------------\n\nTITLE: Structured Output Generation with Ollama and Instructor\nDESCRIPTION: Python code demonstrating how to use Instructor with Ollama to generate structured outputs using Pydantic models. The example creates a Character model and uses it to extract structured information about Harry Potter.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/ollama.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    fact: List[str] = Field(..., description=\"A list of facts about the character\")\n\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model=\"llama3\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the Harry Potter\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Harry James Potter\",\n  \"age\": 37,\n  \"fact\": [\n    \"He is the chosen one.\",\n    \"He has a lightning-shaped scar on his forehead.\",\n    \"He is the son of James and Lily Potter.\",\n    \"He attended Hogwarts School of Witchcraft and Wizardry.\",\n    \"He is a skilled wizard and sorcerer.\",\n    \"He fought against Lord Voldemort and his followers.\",\n    \"He has a pet owl named Snowy.\"\n  ]\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Processing Video and Extracting Recommendations\nDESCRIPTION: Sends the video to Gemini for analysis and extracts structured recommendations using the defined Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/multimodal-gemini.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresp = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\"What places do they recommend in this video?\", file],\n        }\n    ],\n    response_model=Recommendations,\n)\n\nprint(resp)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Multi-class Text Classification with OpenAI in Python\nDESCRIPTION: This snippet demonstrates an asynchronous function for multi-class text classification using OpenAI's API. It uses the instructor library to create a client, defines allowed tags, and returns multiple tags for a given text input.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nimport asyncio\nfrom typing import Iterable\n\nclient = instructor.from_openai(\n    openai.AsyncOpenAI(),\n)\n\nasync def get_tags(text: List[str], tags: List[Tag]) -> List[Tag]:\n    allowed_tags = [(tag.id, tag.name) for tag in tags]\n    allowed_tags_str = \", \".join([f\"`{tag}`\" for tag in allowed_tags])\n\n    return await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world-class text tagging system.\",\n            },\n            {\"role\": \"user\", \"content\": f\"Describe the following text: `{text}`\"},\n            {\n                \"role\": \"user\",\n                \"content\": f\"Here are the allowed tags: {allowed_tags_str}\",\n            },\n        ],\n        response_model=Iterable[Tag],\n        validation_context={\"tags\": tags},\n    )\n\n\ntag_results = asyncio.run(get_tags(text, tags))\nfor tag in tag_results:\n    print(tag)\n    #> id=1 name='phone'\n```\n\n----------------------------------------\n\nTITLE: Defining PII Data Structures with Pydantic\nDESCRIPTION: Implements Pydantic models for representing PII data and extraction results. Includes a Data class for individual PII elements and a PIIDataExtraction class with scrubbing functionality.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/pii.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel\n\n\n# Define Schemas for PII data\nclass Data(BaseModel):\n    index: int\n    data_type: str\n    pii_value: str\n\n\nclass PIIDataExtraction(BaseModel):\n    \"\"\"\n    Extracted PII data from a document, all data_types should try to have consistent property names\n    \"\"\"\n\n    private_data: List[Data]\n\n    def scrub_data(self, content: str) -> str:\n        \"\"\"\n        Iterates over the private data and replaces the value with a placeholder in the form of\n        <{data_type}_{i}>\n        \"\"\"\n        for i, data in enumerate(self.private_data):\n            content = content.replace(data.pii_value, f\"<{data.data_type}_{i}>\")\n        return content\n```\n\n----------------------------------------\n\nTITLE: Advanced Progressive Validation Pattern in Python\nDESCRIPTION: Implements a two-step validation process using basic and detailed product models. Shows how to progressively validate and extract information with proper error handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/retry_mechanisms.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\n# Initialize with moderate retries\nclient = instructor.from_openai(\n    OpenAI(),\n    max_retries=2\n)\n\n# Basic validation first\nclass BasicProduct(BaseModel):\n    name: str\n    price: float = Field(..., gt=0)\n\n# Advanced validation second\nclass DetailedProduct(BasicProduct):\n    description: str = Field(..., min_length=10)\n    category: str\n    in_stock: bool\n\n# Two-step extraction with validation\ntry:\n    # First get basic fields\n    basic = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Product: Mini Pen, Price: $2.50\"}\n        ],\n        response_model=BasicProduct\n    )\n    \n    # Then get full details with context from the first step\n    detailed = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Provide more details about {basic.name} which costs ${basic.price}\"}\n        ],\n        response_model=DetailedProduct\n    )\nexcept Exception as e:\n    # Handle validation failures\n    print(f\"Validation failed: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Knowledge Graphs with OpenAI API\nDESCRIPTION: Implements a function to generate knowledge graphs using OpenAI's API and the Instructor library. The function takes an input query and returns a structured KnowledgeGraph object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/knowledge_graph.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\ndef generate_graph(input) -> KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Objects with Azure OpenAI and Instructor\nDESCRIPTION: This snippet demonstrates how to stream partial objects as they're generated using Azure OpenAI and instructor for structured output generation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/azure.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import from_openai\nfrom openai import AzureOpenAI\nfrom pydantic import BaseModel\nimport os\n\nclient = from_openai(\n    AzureOpenAI(\n        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n        api_version=\"2024-02-01\",\n        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    )\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    bio: str\n\n\n# Stream partial objects as they're generated\nuser = client.chat.completions.create_partial(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user profile for Jason, age 25\"},\n    ],\n    response_model=User,\n)\n\nfor user_partial in user:\n    print(user_partial)\n\n# > name='Jason' age=None bio='None'\n# > name='Jason' age=25 bio='A tech'\n# > name='Jason' age=25 bio='A tech enthusiast'\n# > name='Jason' age=25 bio='A tech enthusiast who loves coding, gaming, and exploring new'\n# > name='Jason' age=25 bio='A tech enthusiast who loves coding, gaming, and exploring new technologies'\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Model Structures\nDESCRIPTION: Shows how to create complex nested data structures using multiple Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: Optional[str] = None\n    country: str\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: List[Address]\n```\n\n----------------------------------------\n\nTITLE: Defining a Multi-Source Search Client for Personal Assistant\nDESCRIPTION: This code defines a model for querying multiple data sources (Gmail, Calendar) in a personal assistant context. It includes an enum for source types and methods to execute queries against different backends asynchronously.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-and-beyond.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ClientSource(enum.Enum):\n    GMAIL = \"gmail\"\n    CALENDAR = \"calendar\"\n\n\nclass SearchClient(BaseModel):\n    query: str\n    keywords: List[str]\n    email: str\n    source: ClientSource\n    start_date: datetime.date\n    end_date: datetime.date\n\n    async def execute(self) -> str:\n        if self.source == ClientSource.GMAIL:\n            ...\n        elif self.source == ClientSource.CALENDAR:\n            ...\n\n\nclass Retrieval(BaseModel):\n    queries: List[SearchClient]\n\n    async def execute(self) -> str:\n        return await asyncio.gather(*[query.execute() for query in self.queries])\n```\n\n----------------------------------------\n\nTITLE: Implementing Modular Chain of Thought in Python with Pydantic\nDESCRIPTION: This snippet demonstrates how to implement a modular 'chain of thought' approach using Pydantic models. It defines a Role class with a chain_of_thought field for step-by-step reasoning, and a UserDetail class that incorporates the Role.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass Role(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Think step by step to determine the correct title\"\n    )\n    title: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n```\n\n----------------------------------------\n\nTITLE: Tracking Token Usage in OpenAI Non-Streaming Requests\nDESCRIPTION: Shows how to retrieve and access token usage metrics from non-streaming OpenAI completions using the instructor library with Pydantic models. The code demonstrates creating a completion request and accessing the detailed usage statistics including prompt and completion tokens.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/usage.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(completion.usage)\n\"\"\"\nCompletionUsage(\n    completion_tokens=10,\n    prompt_tokens=82,\n    total_tokens=92,\n    completion_tokens_details=CompletionTokensDetails(\n        audio_tokens=0, reasoning_tokens=0\n    ),\n    prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Automatic Retries with Writer and Instructor for Schema Validation in Python\nDESCRIPTION: This snippet shows how to use automatic retries with Writer and Instructor for schema validation. It defines a User model with a custom validator and demonstrates how the system automatically retries requests that fail due to schema validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/writer-support.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom typing import Annotated\nfrom writerai import Writer\nfrom pydantic import BaseModel, AfterValidator, Field\n\n# Initialize Writer client\nclient = instructor.from_writer(Writer())\n\n\n# Example of model, that may require usage of retries\ndef uppercase_validator(v):\n    if v.islower():\n        raise ValueError(\"Name must be in uppercase\")\n    return v\n\n\nclass User(BaseModel):\n    name: Annotated[str, AfterValidator(uppercase_validator)] = Field(\n        ..., description=\"The name of the user\"\n    )\n    age: int\n\n\nuser = client.chat.completions.create(\n    model=\"palmyra-x-004\",\n    messages=[{\"role\": \"user\", \"content\": \"Extract: jason is 12\"}],\n    response_model=User,\n    max_retries=3,\n)\n\nprint(user)\n#> name='JASON' age=12\n```\n\n----------------------------------------\n\nTITLE: Defining Maybe Pattern Models with Pydantic\nDESCRIPTION: Defines two Pydantic models: UserDetail for storing user information and MaybeUser for implementing the Maybe pattern. UserDetail captures basic user information while MaybeUser provides a wrapper for handling potential errors and successful results.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/maybe.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n```\n\n----------------------------------------\n\nTITLE: Iterable Streaming with Fireworks\nDESCRIPTION: Demonstrates how to stream multiple structured objects from Fireworks AI responses\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/fireworks.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom fireworks.client import Fireworks\nimport instructor\nfrom pydantic import BaseModel\n\n\n# Enable instructor patches\nclient = instructor.from_fireworks(Fireworks())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Extract multiple users from text\nusers = client.chat.completions.create_iterable(\n    model=\"accounts/fireworks/models/llama-v3-8b-instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract users:\n            1. Jason is 25 years old\n            2. Sarah is 30 years old\n            3. Mike is 28 years old\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Choice Expansion with Context Tracking\nDESCRIPTION: Python function that recursively expands story choices while maintaining state. It uses asyncio for parallel processing and implements a semaphore to control API call rates. Each path tracks its complete history to ensure narrative consistency.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/consistent-stories.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def rewrite_choice(\n    client: instructor.AsyncInstructor,\n    choice: str,\n    story: GeneratedStory,\n    prev_choices: list[dict],  # Accumulator for path state\n    max_depth: int,\n    sem: asyncio.Semaphore\n) -> FinalStoryChoice:\n    # Each choice knows its entire path history\n    async with sem:\n        rewritten_choice = await client.chat.completions.create(\n            model=\"gpt-4o\",\n            response_model=RewrittenChoice,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": \"\"\"\n                Given this choice: {{ choice }}\n\n                Story context:\n                Setting: {{ story.setting }}\n                Plot: {{ story.plot_summary }}\n\n                Previous choices made in this path:\n                {% for prev in prev_choices %}\n                - {{ prev.choice_description }}\n                  Result: {{ prev.choice_consequences }}\n                {% endfor %}\n\n                Generate the next story beat and 2-4 new choices.\n                The story should end in {{ max_depth - len(prev_choices) }} more turns.\n                \"\"\"\n            }],\n            context={\n                \"choice\": choice,\n                \"story\": story,\n                \"prev_choices\": prev_choices,\n            }\n        )\n\n    # For terminal nodes (at max depth)\n    if len(prev_choices) == max_depth - 1:\n        return FinalStoryChoice(\n            choice_description=rewritten_choice.choice_description,\n            choice_consequences=rewritten_choice.choice_consequences,\n            choices=[]  # Terminal node\n        )\n\n    # Recursively expand child choices\n    child_choices = await asyncio.gather(*[\n        rewrite_choice(\n            client=client,\n            choice=new_choice,\n            story=story,\n            prev_choices=prev_choices + [{\n                \"choice_description\": rewritten_choice.choice_description,\n                \"choice_consequences\": rewritten_choice.choice_consequences\n            }],\n            max_depth=max_depth,\n            sem=sem\n        )\n        for new_choice in rewritten_choice.choices\n    ])\n\n    return FinalStoryChoice(\n        choice_description=rewritten_choice.choice_description,\n        choice_consequences=rewritten_choice.choice_consequences,\n        choices=child_choices\n    )\n```\n\n----------------------------------------\n\nTITLE: Basic Synchronous User Extraction with OpenAI\nDESCRIPTION: Example showing how to extract user information using OpenAI client with Instructor in synchronous mode\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\n\n# Initialize with API key\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# Enable instructor patches for OpenAI client\nclient = instructor.from_openai(client)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n# Create structured output\nuser = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n    ],\n    response_model=User,\n)\n\nprint(user)\n#> User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Defining a Metaphor Query Model in Python with Pydantic\nDESCRIPTION: This code snippet defines a Pydantic model for structured query representation in Metaphor Systems, including date ranges and domain filtering. It shows how to model complex search parameters including an async execution method.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-and-beyond.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n\n\nclass MetaphorQuery(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n    domains_allow_list: List[str]\n\n    async def execute():\n        return await metaphor.search(...)\n```\n\n----------------------------------------\n\nTITLE: Structured Data Extraction with Anyscale and Instructor\nDESCRIPTION: Complete Python example demonstrating how to initialize the Anyscale client with Instructor, define a Pydantic model for data structure, and extract structured information using the Mixtral model. Shows integration between OpenAI-compatible API, Instructor library, and Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anyscale.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Initialize the client with Anyscale base URL\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"https://api.endpoints.anyscale.com/v1\",\n        api_key=os.environ[\"ANYSCALE_API_KEY\"],\n    ),\n    mode=instructor.Mode.JSON_SCHEMA,\n)\n\n# Define your data structure\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n# Extract structured data\nuser = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(user)\n# Output: UserExtract(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Creating Composable Pydantic Schemas for LLM Responses\nDESCRIPTION: This snippet demonstrates how to create modular schemas by extending base Pydantic models. It shows a UserWithAddress class that inherits from UserDetails and adds an address field.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\nclass UserWithAddress(UserDetails):\n    address: str\n```\n\n----------------------------------------\n\nTITLE: Creating Search Query Model with Time Filter\nDESCRIPTION: Defines a SearchQuery model that combines a search string with the TimeFilter model for structured query representation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-timelines.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass SearchQuery(BaseModel):\n    query: str\n    time_filter: TimeFilter\n```\n\n----------------------------------------\n\nTITLE: Extracting Tables from Images using OpenAI Vision Model in Python\nDESCRIPTION: This function uses the OpenAI vision model to process an image URL and extract tables in markdown format. It utilizes the instructor library to patch the OpenAI client and returns an iterable of Table objects.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extracting_tables.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\n\n# Apply the patch to the OpenAI client to support response_model\n# Also use MD_JSON mode since the vision model does not support any special structured output mode\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.MD_JSON)\n\n\ndef extract_table(url: str) -> Iterable[Table]:\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=Iterable[Table],\n        max_tokens=1800,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Extract table from image.\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": url}},\n                ],\n            }\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining AdCopy Model in Python with Pydantic\nDESCRIPTION: Creates a Pydantic model for storing generated advertising copy. It includes fields for headline, ad copy text, and product name.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/image_to_ad_copy.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass AdCopy(BaseModel):\n    \"\"\"\n    Represents a generated ad copy.\n    \"\"\"\n\n    headline: str = Field(\n        description=\"A short, catchy, and memorable headline for the given product. The headline should invoke curiosity and interest in the product.\",\n    )\n    ad_copy: str = Field(\n        description=\"A long-form advertisement copy for the given product. This will be used in campaigns to promote the product with a persuasive message and a call-to-action with the objective of driving sales.\",\n    )\n    name: str = Field(description=\"The name of the product being advertised.\")\n```\n\n----------------------------------------\n\nTITLE: PDF Analysis with Anthropic Claude and Caching\nDESCRIPTION: Demonstrates how to analyze a PDF using Anthropic's Claude model with built-in caching functionality through PDFWithCacheControl. The code downloads a PDF, extracts invoice information including total and items using a Pydantic model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/multimodal.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom anthropic import Anthropic\nimport instructor\nfrom pydantic import BaseModel\nfrom instructor.multimodal import PDFWithCacheControl\n\n# Set up the client\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\nclient = instructor.from_anthropic(Anthropic())\n\n\n# Create a model for analyzing PDFs\nclass Invoice(BaseModel):\n    total: float\n    items: list[str]\n\n\n# Load and analyze a PDF\nresponse, completion = client.chat.completions.create_with_completion(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=Invoice,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Analyze this document\",\n                PDFWithCacheControl.from_url(url),\n            ],\n        }\n    ],\n    max_tokens=1000,\n)\n\nprint(response)\n# > Total = 220, items = ['English Tea', 'Tofu']\n\nprint(completion.usage.cache_creation_input_tokens)\n# > 2091\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing with Instructor for Concurrent Task Execution\nDESCRIPTION: Shows how to process multiple tasks concurrently using Instructor's parallel processing capabilities. This code analyzes multiple text samples simultaneously, extracting structured data for each one with customizable worker count.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom instructor.dsl.parallel import parallel\n\nclass Data(BaseModel):\n    summary: str\n    entities: list[str]\n    sentiment: str\n\nclient = instructor.from_openai(OpenAI())\n\n# Create parallel tasks\ntasks = [\n    {\"text\": \"Apple announces new iPhone with revolutionary features.\"},\n    {\"text\": \"Climate scientists warn of increasing global temperatures.\"},\n    {\"text\": \"Stock market hits record high amid economic recovery.\"}\n]\n\n# Process in parallel\nresults = parallel(\n    client=client,\n    model=\"gpt-3.5-turbo\",\n    response_model=Data,\n    prompts=[\n        [{\"role\": \"user\", \"content\": f\"Analyze this text: {task['text']}\"}]\n        for task in tasks\n    ],\n    max_workers=3\n)\n\nfor i, result in enumerate(results):\n    print(f\"Result {i+1}:\")\n    print(f\"  Summary: {result.summary}\")\n    print(f\"  Entities: {', '.join(result.entities)}\")\n    print(f\"  Sentiment: {result.sentiment}\")\n```\n\n----------------------------------------\n\nTITLE: Vision and Multimodal Data Extraction with Instructor\nDESCRIPTION: Shows how to extract structured data from images using GPT-4 Vision. This example demonstrates reading receipt information from an image, converting the unstructured visual data into a structured Pydantic model with items, prices, and totals.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport base64\nfrom typing import List\n\nclass Item(BaseModel):\n    name: str\n    price: float = Field(gt=0)\n    quantity: int = Field(gt=0)\n\nclass Receipt(BaseModel):\n    store_name: str\n    date: str\n    items: List[Item]\n    subtotal: float\n    tax: float\n    total: float\n\nclient = instructor.from_openai(OpenAI())\n\n# Load the receipt image\nwith open(\"receipt.jpg\", \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n\nreceipt = client.chat.completions.create(\n    model=\"gpt-4-vision-preview\",\n    response_model=Receipt,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Extract all information from this receipt\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                    }\n                }\n            ]\n        }\n    ]\n)\n\nprint(receipt.model_dump_json(indent=2))\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain of Thought with Instructor and Pydantic\nDESCRIPTION: Demonstrates how to implement Chain of Thought reasoning using Instructor framework with Pydantic models. The code shows the setup of a ReasonedAnswer model with chain-of-thought and final answer fields, integrated with OpenAI's API.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/index.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\n# Example implementing Chain of Thought with a field\nclass ReasonedAnswer(BaseModel):\n    \"\"\"Answer the following question with detailed reasoning.\"\"\"\n    \n    chain_of_thought: str = Field(\n        description=\"Step-by-step reasoning process to solve the problem\"\n    )\n    final_answer: str = Field(\n        description=\"The final conclusion after reasoning\"\n    )\n\nclient = instructor.from_openai(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=ReasonedAnswer,\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the cube root of 27?\"}\n    ]\n)\n\nprint(f\"Reasoning: {response.chain_of_thought}\")\nprint(f\"Answer: {response.final_answer}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI Moderation for Content Validation in Python\nDESCRIPTION: This code snippet defines a Response class with a message field that is validated using OpenAI's moderation endpoint. It demonstrates how to set up the OpenAI client, use the AfterValidator with the openai_moderation function, and handle exceptions for flagged content.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/moderation.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\n\nfrom instructor import openai_moderation\n\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, AfterValidator\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(openai_moderation(client=client))]\n\n\ntry:\n    Response(message=\"I want to make them suffer the consequences\")\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response\n    message\n      Value error, `I want to make them suffer the consequences` was flagged for violence [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n\ntry:\n    Response(message=\"I want to hurt myself.\")\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response\n    message\n      Value error, `I want to hurt myself.` was flagged for self_harm, self_harm_intent, self-harm, self-harm/intent [type=value_error, input_value='I want to hurt myself.', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Objects from Claude with Error Handling\nDESCRIPTION: Demonstrates how to stream partial objects as they're generated by the model. This allows for progressive rendering of the response as the model generates it, with each update containing the current state of the generated User object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    # Stream partial objects as they're generated\n    for partial_user in client.chat.completions.create_partial(\n        model=\"claude-3-haiku-20240307\",  # Use latest stable model\n        messages=[\n            {\"role\": \"system\", \"content\": \"Create a detailed user profile based on the information provided.\"},\n            {\"role\": \"user\", \"content\": \"Create a user profile for Jason, age 25\"},\n        ],\n        response_model=User,\n        max_tokens=4096,\n    ):\n        print(f\"Current state: {partial_user}\")\n\n    # Expected output:\n    # > Current state: name='Jason' age=None bio=None\n    # > Current state: name='Jason' age=25 bio='Jason is a 25-year-old with an adventurous spirit and a love for technology. He is'\n    # > Current state: name='Jason' age=25 bio='Jason is a 25-year-old with an adventurous spirit and a love for technology. He is always on the lookout for new challenges and opportunities to grow both personally and professionally.'\nexcept Exception as e:\n    print(f\"Error during streaming: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Basic Unstructured OpenAI API Call Example\nDESCRIPTION: Demonstrates a basic OpenAI API call without structured output, showing the inconsistency problem in response formatting.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/structured_outputs.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract customer: John Doe, age 35, email: john@example.com\",\n        }\n    ],\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Structures with Instructor for Comment Threads\nDESCRIPTION: This code snippet demonstrates how to work with recursive structures using Instructor, specifically for representing nested comment threads. It defines a Comment model that can contain replies, which are also Comments.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/nested_structure.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Comment(BaseModel):\n    text: str\n    author: str\n    replies: List[\"Comment\"] = []  # Recursive structure\n\n# Update the Comment class reference for Pydantic\nComment.model_rebuild()\n\nclass Post(BaseModel):\n    title: str\n    content: str\n    author: str\n    comments: List[Comment] = []\n\n# Extract recursive nested data\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Blog Post: \"Python Tips and Tricks\"\n        Author: John Smith\n        Content: Here are some helpful Python tips for beginners...\n        \n        Comments:\n        1. Alice: \"Great post! Very helpful.\"\n           - Bob: \"I agree, I learned a lot.\"\n             - Alice: \"Bob, did you try the last example?\"\n           - Charlie: \"Thanks for sharing this.\"\n        2. David: \"Could you explain the second tip more?\"\n           - John: \"Sure, I'll add more details.\"\n        \"\"\"}\n    ],\n    response_model=Post\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Single-Label Classification Function\nDESCRIPTION: This function performs single-label classification on input text using the OpenAI API and the previously defined ClassificationResponse model. It sends a chat completion request to classify the given text as either SPAM or NOT_SPAM.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/classification.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef classify(data: str) -> ClassificationResponse:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=ClassificationResponse,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: <text>{data}</text>\",\n            },\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Streaming with Vertex AI\nDESCRIPTION: Example of asynchronous streaming implementations combining both partial responses and iterable collections. Demonstrates advanced usage of async streaming capabilities.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/vertex.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nimport instructor\nfrom pydantic import BaseModel\nfrom instructor.dsl.partial import Partial\n\nvertexai.init()\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-2.0-flash\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n    _async=True,\n)\n\nasync def stream_partial():\n    response_stream = await client.chat.completions.create(\n        response_model=Partial[UserExtract],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Anibal is 23 years old\"},\n        ],\n    )\n    \n    async for partial_user in response_stream:\n        print(f\"Received update: {partial_user}\")\n\nasync def stream_iterable():\n    response_stream = client.chat.completions.create_iterable(\n        response_model=UserExtract,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make up two people\"},\n        ],\n    )\n    \n    async for user in response_stream:\n        print(f\"Generated user: {user}\")\n\n# Run async functions\nasyncio.run(stream_partial())\nasyncio.run(stream_iterable())\n```\n\n----------------------------------------\n\nTITLE: Generating Training Data for Fine-Tuning GPT-3.5 with Instructor\nDESCRIPTION: This code snippet demonstrates how to use the Instructor library to generate a .jsonl file for fine-tuning GPT-3.5. It includes setting up logging, defining a Pydantic model for the summary, and using the @instructions.distil decorator to capture function inputs and outputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chain-of-density.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom chain_of_density import summarize_article\nimport csv\nimport logging\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = instructor.Instructions(\n    name=\"Chain Of Density\",\n    finetune_format=\"messages\",\n    log_handlers=[logging.FileHandler(\"generated.jsonl\")],\n    openai_client=client,\n)\n\nclass GeneratedSummary(BaseModel):\n    \"\"\"\n    This represents a highly concise summary that includes as many entities as possible from the original source article.\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n\n    Guidelines\n    - Make every word count\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n    )\n\n@instructions.distil\ndef distil_summarization(text: str) -> GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1])\n\nwith open(\"train.csv\", \"r\") as file:\n    reader = csv.reader(file)\n    next(reader)  # Skip the header\n    for article, summary in reader:\n        # Run Distillisation to generate the values\n        distil_summarization(article)\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous User Extraction with FastAPI and Logfire\nDESCRIPTION: This snippet shows how to use asyncio with FastAPI and Logfire to process multiple user queries in parallel. It defines a new endpoint that accepts multiple queries and uses asyncio.gather to run extractions concurrently.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/full-fastapi-visibility.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\n\nclass MultipleUserData(BaseModel):\n    queries: list[str]\n\n\n@app.post(\"/many-users\", response_model=list[UserDetail])\nasync def extract_many_users(data: MultipleUserData):\n    async def extract_user(query: str):\n        user_detail = await client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            response_model=UserDetail,\n            messages=[\n                {\"role\": \"user\", \"content\": f\"Extract: `{query}`\"},\n            ],\n        )\n        logfire.info(\"/User returning\", value=user_detail)\n        return user_detail\n\n    coros = [extract_user(query) for query in data.queries]\n    return await asyncio.gather(*coros)\n```\n\n----------------------------------------\n\nTITLE: Logging OpenAI Requests and Responses in Python\nDESCRIPTION: This code snippet shows how to set up DEBUG level logging to view OpenAI requests and responses. It creates an OpenAI client with instructor, defines a Pydantic model for user details, and makes a request to extract information from a given text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/logging.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nimport logging\n\nfrom pydantic import BaseModel\n\n\n# Set logging to DEBUG\nlogging.basicConfig(level=logging.DEBUG)\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)  # type: ignore\n\n\"\"\"\n...\nDEBUG:instructor:Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\nDEBUG:instructor:Instructor Request: mode.value='tool_call', response_model=<class '__main__.UserDetail'>, new_kwargs={'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Extract Jason is 25 years old'}], 'tools': [{'type': 'function', 'function': {'name': 'UserDetail', 'description': 'Correctly extracted `UserDetail` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'UserDetail'}}}\nDEBUG:instructor:max_retries: 1\n...\nDEBUG:instructor:Instructor Pre-Response: ChatCompletion(id='chatcmpl-8zBxMxsOqm5Sj6yeEI38PnU2r6ncC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_E1cftF5U0zEjzIbWt3q0ZLbN', function=Function(arguments='{\"name\":\"Jason\",\"age\":25}', name='UserDetail'), type='function')]))], created=1709594660, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=9, prompt_tokens=81, total_tokens=90))\nDEBUG:httpcore.connection:close.started\nDEBUG:httpcore.connection:close.complete\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Output with MistralAI and Instructor\nDESCRIPTION: This Python script demonstrates how to use MistralAI with Instructor to generate structured output. It defines a Pydantic model for user details, sets up the MistralAI client with Instructor, and uses it to create a structured response based on a given prompt. The example extracts a name and age from the input text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/mistral.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pydantic import BaseModel\nfrom mistralai import Mistral\nfrom instructor import from_mistral, Mode\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\n# enables `response_model` in chat call\nclient = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n\ninstructor_client = from_mistral(\n    client=client,\n    model=\"mistral-large-latest\",\n    mode=Mode.MISTRAL_TOOLS,\n    max_tokens=1000,\n)\n\nresp = instructor_client.messages.create(\n    response_model=UserDetails,\n    messages=[{\"role\": \"user\", \"content\": \"Jason is 10\"}],\n    temperature=0,\n)\n\nprint(resp)\n#> name='Jason' age=10\n\n# output: UserDetails(name='Jason', age=10)\n```\n\n----------------------------------------\n\nTITLE: Citation Validation Model\nDESCRIPTION: Implementation of a Pydantic model that validates citations against provided context to prevent hallucination.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import ValidationInfo\n\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator(\"citation\")\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            context = context.get(\"text_chunk\")\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text\")\n        return v\n```\n\n----------------------------------------\n\nTITLE: Implementing Uncertainty-Routed Chain Of Thought with instructor and OpenAI\nDESCRIPTION: This Python implementation of Uncertainty-Routed Chain Of Thought generates multiple reasoning chains (8 in this example) and takes the majority answer if it exceeds a specific threshold (0.6). The code uses the instructor library with OpenAI's API to structure responses using a Pydantic model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/thought_generation/chain_of_thought_few_shot/uncertainty_routed_cot.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nimport instructor\nfrom textwrap import dedent\nfrom typing import Literal\nimport asyncio\nfrom collections import Counter\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass ChainOfThoughtResponse(BaseModel):\n    chain_of_thought: str\n    correct_answer: Literal[\"A\", \"B\", \"C\", \"D\"]\n\n\nasync def generate_response(query: str, options: dict[str, str]):\n    formatted_options = \"\\n\".join(\n        [f\"{key}:{answer}\" for key, answer in options.items()]\n    )\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ChainOfThoughtResponse,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are a a world class AI who excels at answering\n                complex questions. Choose one of the options below\n                that best answers the question you are about to be\n                asked\n                <question>\n                {query}\n                </question>\n\n                <options>\n                {formatted_options}\n                </options>\n                \"\"\"\n                ),\n            }\n        ],\n    )\n\n\nasync def generate_batch_responses(\n    query: str, options: dict[str, str], num_chains: int\n) -> list[ChainOfThoughtResponse]:\n    coros = [generate_response(query, options) for _ in range(num_chains)]\n    return await asyncio.gather(*coros)\n\n\nif __name__ == \"__main__\":\n    question = \"\"\"In a population of giraffes, an environmental\n    change occurs that favors individuals that are tallest. As a\n    result, more of the taller individuals are able to obtain\n    nutrients and survive to pass along their genetic information.\n    This is an example of\"\"\"\n\n    options = {\n        \"A\": \"directional selection\",\n        \"B\": \"stabilizing selection\",\n        \"C\": \"sexual selection\",\n        \"D\": \"disruptive selection\",\n    }\n\n    correct_answer = \"A\"\n    k = 8\n    threshold = 0.6\n\n    responses = asyncio.run(generate_batch_responses(question, options, k))\n    votes = Counter([response.correct_answer for response in responses])\n    print(votes)\n    #> Counter({'A': 8})\n\n    majority_vote_element, majority_vote_count = votes.most_common(1)[0]\n    print(majority_vote_element, majority_vote_count)\n    #> A 8\n    majority_threshold = majority_vote_count / k\n\n    if majority_threshold < threshold:\n        response = asyncio.run(generate_response(question, options))\n        response = response.correct_answer\n    else:\n        response = majority_vote_element\n\n    print(response)\n    #> A\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Responses in FastAPI\nDESCRIPTION: Shows how to implement streaming responses in FastAPI using Server-Sent Events (SSE). The endpoint streams UserDetail objects generated from OpenAI responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/fastapi.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n# Route to handle SSE events and return users\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": data.query},\n        ],\n    )\n\n    async def generate():\n        async for user in users:\n            resp_json = user.model_dump_json()\n            yield f\"data: {resp_json}\"\n        yield \"data: [DONE]\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor AI for Structured Output\nDESCRIPTION: This snippet shows how to initialize an OpenAI client with Instructor AI to enable structured output using a response model. It sets up the client and creates a chat completion request with a custom Retrieval model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-and-beyond.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Enables response_model in the openai client\nclient = instructor.from_openai(OpenAI())\n\nretrieval = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Retrieval,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"},\n        {\"role\": \"user\", \"content\": \"What do I have today?\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with Async Code in Python\nDESCRIPTION: Demonstrates how to use Instructor with asynchronous code, utilizing the AsyncOpenAI client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/faq.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = instructor.from_openai(AsyncOpenAI())\n\nasync def extract_data():\n    result = await client.chat.completions.create(\n        response_model=MyModel,\n        messages=[...]\n    )\n    return result\n\nasyncio.run(extract_data())\n```\n\n----------------------------------------\n\nTITLE: Processing Text with Cached Prompts\nDESCRIPTION: Demonstrates character extraction using prompt caching, showing how tokens are cached and reused across multiple requests.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/anthropic-prompt-caching.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor _ in range(2):\n    resp, completion = client.chat.completions.create_with_completion(\n        model=\"claude-3-haiku-20240307\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"<book>\" + book + \"</book>\",\n                        \"cache_control\": {\"type\": \"ephemeral\"},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Extract a character from the text given above\",\n                    },\n                ],\n            },\n        ],\n        response_model=Character,\n        max_tokens=1000,\n    )\n    assert isinstance(resp, Character)\n    print(completion.usage)\n    print(resp)\n```\n\n----------------------------------------\n\nTITLE: Multi-Field Validation for Reservations in Python\nDESCRIPTION: Shows how to validate multiple fields together using Pydantic's model_validator decorator, implementing business rules for hotel reservations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/field_level_validation.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, model_validator\nimport instructor\nfrom openai import OpenAI\nfrom datetime import date\n\nclient = instructor.from_openai(OpenAI())\n\nclass Reservation(BaseModel):\n    check_in: date\n    check_out: date\n    room_type: str\n    guests: int\n    \n    @model_validator(mode='after')\n    def validate_dates(self):\n        if self.check_out <= self.check_in:\n            raise ValueError(\"Check-out date must be after check-in date\")\n        \n        if self.room_type == \"Standard\" and self.guests > 2:\n            raise ValueError(\"Standard rooms can only fit 2 guests\")\n            \n        return self\n```\n\n----------------------------------------\n\nTITLE: Iterable Responses with Azure OpenAI and Instructor\nDESCRIPTION: This example shows how to extract multiple structured objects from text using Azure OpenAI and instructor's iterable response feature.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/azure.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import from_openai\nfrom openai import AzureOpenAI\nfrom pydantic import BaseModel\nimport os\n\nclient = from_openai(\n    AzureOpenAI(\n        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n        api_version=\"2024-02-01\",\n        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    )\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Extract multiple users from text\nusers = client.chat.completions.create_iterable(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract users:\n            1. Jason is 25 years old\n            2. Sarah is 30 years old\n            3. Mike is 28 years old\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n#> name='Jason' age=25\n# > name='Sarah' age=30\n# > name='Mike' age=28\n```\n\n----------------------------------------\n\nTITLE: Streaming Large Responses with Instructor in Python\nDESCRIPTION: Shows how to use the create_partial method to stream large responses, allowing for partial updates as the response is generated.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/faq.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstream = client.chat.completions.create_partial(\n    response_model=MyModel,\n    messages=[...]\n)\n\nfor partial in stream:\n    print(partial)  # Partial model with fields filled in as they're generated\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Hooks in Instructor AI\nDESCRIPTION: Shows how to extend the HookName enum and create custom event handlers. Includes hook registration and event emission patterns.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Protocol, Any\nfrom enum import Enum\nfrom instructor.hooks import Hooks, HookName\n\n\nclass CustomHookName(str, Enum):\n    CUSTOM_EVENT = \"custom:event\"\n\n    COMPLETION_KWARGS = HookName.COMPLETION_KWARGS.value\n    COMPLETION_RESPONSE = HookName.COMPLETION_RESPONSE.value\n    COMPLETION_ERROR = HookName.COMPLETION_ERROR.value\n    PARSE_ERROR = HookName.PARSE_ERROR.value\n    COMPLETION_LAST_ATTEMPT = HookName.COMPLETION_LAST_ATTEMPT.value\n\n\nhooks = Hooks()\n\n\ndef custom_handler(data):\n    print(f\"Custom event: {data}\")\n\n\nhooks.on(CustomHookName.CUSTOM_EVENT, custom_handler)\n\nhooks.emit(CustomHookName.CUSTOM_EVENT, {\"data\": \"value\"})\n```\n\n----------------------------------------\n\nTITLE: Structured Output with Together AI\nDESCRIPTION: Implementation of structured output extraction using Together AI platform with Instructor integration\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/open_source.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pydantic import BaseModel\n\nimport instructor\nimport openai\n\n\nclient = openai.OpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n)\n\nclient = instructor.from_openai(client, mode=instructor.Mode.TOOLS)\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserExtract), \"Should be instance of UserExtract\"\n\nprint(user)\n#> name='jason' age=25\n```\n\n----------------------------------------\n\nTITLE: Asynchronous SambaNova Integration\nDESCRIPTION: Example of using Instructor with SambaNova's LLM API asynchronously. Shows how to set up an async client and make async calls using asyncio.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/sambanova.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import AsyncOpenAI\nimport instructor\nimport os\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(\n    AsyncOpenAI(\n        base_url=\"https://api.sambanova.ai/v1\",\n        api_key=os.environ[\"SAMBANOVA_API_KEY\"]\n    )\n)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nasync def get_user():\n    user = await client.chat.completions.create(\n        model=\"Meta-Llama-3.1-405B-Instruct\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Ivan is 28\"},\n        ],\n        response_model=User,\n    )\n    return user\n\n# Run with asyncio\nimport asyncio\nuser = asyncio.run(get_user())\nprint(user)\n# > User(name='Ivan', age=28)\n```\n\n----------------------------------------\n\nTITLE: Generating structured output using Instructor with LiteLLM (Synchronous)\nDESCRIPTION: Python code demonstrating how to use Instructor with LiteLLM to create structured, type-safe outputs synchronously. It defines a User model and extracts information from a given text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/litellm.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom litellm import completion\nimport instructor\nfrom pydantic import BaseModel\n\n# Enable instructor patches\nclient = instructor.from_litellm(completion)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n# Create structured output\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",  # Can use any supported model\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n    ],\n    response_model=User,\n)\n\nprint(user)  # User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Using Pydantic Annotated for Content Moderation\nDESCRIPTION: Shows an alternative approach to content moderation using Pydantic's Annotated function and AfterValidator for custom validation logic.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\ndef message_cannot_have_blacklisted_words(value: str):\n    for word in value.split():\n        if word.lower() in {'rob', 'steal'}:\n            raise ValueError(f\"`{word}` was found in the message `{value}`\")\n    return value\n\n\nclass UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Language Detection and Summarization with Pydantic and OpenAI\nDESCRIPTION: This code snippet demonstrates how to use Pydantic, OpenAI's API, and the langdetect library to create summaries in the same language as the source text. It defines a base model for summaries and uses language detection to ensure correct language matching.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/matching-language.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom instructor import patch\nfrom openai import AsyncOpenAI\nfrom langdetect import detect\n\ndocs = # To see the text, expand the notes above.\n```\n\n----------------------------------------\n\nTITLE: Streaming Multiple Items with Instructor and Gemini in Python\nDESCRIPTION: Illustrates how to use Instructor with Gemini to stream multiple items with the same structure. This example generates and streams 10 random users with name and age.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/google-openai-client.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import from_openai\nfrom openai import OpenAI\nfrom instructor import Mode\nfrom pydantic import BaseModel\nimport os\n\nclient = from_openai(\n    OpenAI(\n        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n    ),\n    mode=Mode.MD_JSON,\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create_iterable(\n    model=\"gemini-1.5-flash\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate 10 random users\",\n        }\n    ],\n    response_model=User,\n)\n\nfor r in resp:\n    print(r)\n# name='Alice' age=25\n# name='Bob' age=32\n# name='Charlie' age=19\n# name='David' age=48\n# name='Emily' age=28\n# name='Frank' age=36\n# name='Grace' age=22\n# name='Henry' age=41\n# name='Isabella' age=30\n# name='Jack' age=27\n```\n\n----------------------------------------\n\nTITLE: Generating Question-Answer Pairs with Instructor\nDESCRIPTION: Implements the LLM-based generation of question-answer pairs using Instructor and OpenAI, with structured output handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-flashcards.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\n\ninstructor_client = instructor.from_openai(openai.OpenAI())\n\nsystem_prompt = \"\"\"Analyze the given YouTube transcript and generate question-answer pairs\nto help study and understand the topic better. Please rate all questions from 1 to 5\nbased on their difficulty.\"\"\"\n\nresponse = instructor_client.chat.completions.create_iterable(\n    model=\"gpt-4o-mini\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": transcript},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Search Query Structure with Pydantic in Python\nDESCRIPTION: This code snippet defines the Search class using Pydantic to structure search queries. It includes fields for query, type, and an async execute method for performing the search. The code also sets up the OpenAI client with instructor patch for response_model support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/search.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable, Literal\nfrom pydantic import BaseModel, Field\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n\nclass Search(BaseModel):\n    query: str = Field(..., description=\"Query to search for relevant content\")\n    type: Literal[\"web\", \"image\", \"video\"] = Field(..., description=\"Type of search\")\n\n    async def execute(self):\n        print(\n            f\"Searching for `{self.title}` with query `{self.query}` using `{self.type}`\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-based Reranker Function in Python\nDESCRIPTION: Creates a function that reranks search results using an LLM. It takes a query and chunks of text, sends them to the LLM with a structured prompt, and returns a RerankedResults object with relevancy scores.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/llm-as-reranker.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef rerank_results(query: str, chunks: list[dict]) -> RerankedResults:\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=RerankedResults,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are an expert search result ranker. Your task is to evaluate the relevance of each text chunk to the given query and assign a relevancy score.\n\n                For each chunk:\n                1. Analyze its content in relation to the query.\n                2. Provide a chain of thought explaining your reasoning.\n                3. Assign a relevancy score from 0 to 10, where 10 is most relevant.\n\n                Be objective and consistent in your evaluations.\n                \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"\"\"\n                <query>{{ query }}</query>\n\n                <chunks_to_rank>\n                {% for chunk in chunks %}\n                <chunk id=\"{{ chunk.id }}\">\n                    {{ chunk.text }}\n                </chunk>\n                {% endfor %}\n                </chunks_to_rank>\n\n                Please provide a RerankedResults object with a Label for each chunk.\n                \"\"\",\n            },\n        ],\n        context={\"query\": query, \"chunks\": chunks},\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Data Structures for Competitor Extraction in Python\nDESCRIPTION: This code defines Pydantic models for representing competitors, industries, and overall competition data. It creates structures for organizing extracted information from slides.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extract_slides.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\n\n\nclass Competitor(BaseModel):\n    name: str\n    features: Optional[List[str]]\n\n\n# Define models\nclass Industry(BaseModel):\n    \"\"\"\n    Represents competitors from a specific industry extracted from an image using AI.\n    \"\"\"\n\n    name: str = Field(description=\"The name of the industry\")\n    competitor_list: List[Competitor] = Field(\n        description=\"A list of competitors for this industry\"\n    )\n\n\nclass Competition(BaseModel):\n    \"\"\"\n    This class serves as a structured representation of\n    competitors and their qualities.\n    \"\"\"\n\n    industry_list: List[Industry] = Field(\n        description=\"A list of industries and their competitors\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Streaming User Extraction\nDESCRIPTION: Demonstrates asynchronous streaming implementation for user extraction using AsyncOpenAI client. Shows how to use async/await patterns with Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/lists.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nasync def print_iterable_results():\n    model = await client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make two up people\"},\n        ],\n    )\n    async for m in model:\n        print(m)\n\n\nimport asyncio\n\nasyncio.run(print_iterable_results())\n```\n\n----------------------------------------\n\nTITLE: Implementing QuestionAnswer Class with Validation in Python\nDESCRIPTION: Defines a QuestionAnswer class that manages a question and its answer composed of Facts. Includes validation to ensure all facts have supporting citations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/exact_citations.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass QuestionAnswer(BaseModel):\n    question: str = Field(...)\n    answer: List[Fact] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self) -> \"QuestionAnswer\":\n        self.answer = [fact for fact in self.answer if len(fact.substring_quote) > 0]\n        return self\n```\n\n----------------------------------------\n\nTITLE: Using Image Class with OpenAI and Autodetect Feature in Python\nDESCRIPTION: This example shows how to use the Image class with the autodetect_images feature, which allows passing URLs or file paths as normal strings. It demonstrates image analysis using OpenAI's GPT-4 model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/multimodal.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom instructor.multimodal import Image\nimport openai\nfrom pydantic import BaseModel\n\n\nclass ImageDescription(BaseModel):\n    description: str\n    items: list[str]\n\n\n# Download a sample image for demonstration\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/image.jpg\"\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=ImageDescription,\n    autodetect_images=True,  # Set this to True\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\"What is in this image?\", url],\n        }\n    ],\n)\n\nprint(response)\n# > description='A bush with numerous clusters of blueberries surrounded by green leaves, under a cloudy sky.' items=['blueberries', 'green leaves', 'cloudy sky']\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain of Thought Reasoning in Pydantic Models\nDESCRIPTION: This snippet demonstrates how to capture reasoning steps in LLM responses using Pydantic. It creates a TimeRange model with a chain_of_thought field to store the reasoning process alongside the actual time values.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass TimeRange(BaseModel):\n    chain_of_thought: str\n    start_time: int\n    end_time: int\n\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    work_time: TimeRange\n    leisure_time: TimeRange\n```\n\n----------------------------------------\n\nTITLE: Structured Data Extraction with Databricks\nDESCRIPTION: Python implementation showing how to extract structured data from Databricks models using instructor. Demonstrates client initialization, model definition with Pydantic, and data extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/databricks.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Initialize the client with Databricks base URL\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"https://your-databricks-workspace-url/serving-endpoints/your-endpoint-name/invocations\",\n        api_key=os.environ[\"DATABRICKS_API_KEY\"],\n    ),\n    mode=instructor.Mode.TOOLS,\n)\n\n# Define your data structure\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n# Extract structured data\nuser = client.chat.completions.create(\n    model=\"databricks-model\", # Your model name in Databricks\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(user)\n# Output: UserExtract(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Defining InitialSummary Data Class in Python\nDESCRIPTION: Creates a Pydantic model for an initial, verbose summary of an article. It specifies the expected length and style of the summary.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\\n        It should be roughly 80 words in length\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Pydantic Validators\nDESCRIPTION: Defines custom field validators for the RewrittenSummary model using NLTK and spaCy. Includes validation for minimum length, entity presence, absence checking, and entity density requirements.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chain-of-density.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n@field_validator(\"summary\")\ndef min_length(cls, v: str):\n    tokens = nltk.word_tokenize(v)\n    num_tokens = len(tokens)\n    if num_tokens < 60:\n        raise ValueError(\n            \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n        )\n    return v\n\n@field_validator(\"missing\")\ndef has_missing_entities(cls, missing_entities: List[str]):\n    if len(missing_entities) == 0:\n        raise ValueError(\n            \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n        )\n    return missing_entities\n\n@field_validator(\"absent\")\ndef has_no_absent_entities(cls, absent_entities: List[str]):\n    absent_entity_string = \",\".join(absent_entities)\n    if len(absent_entities) > 0:\n        print(f\"Detected absent entities of {absent_entity_string}\")\n        raise ValueError(\n            f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n        )\n    return absent_entities\n\n@field_validator(\"summary\")\ndef min_entity_density(cls, v: str):\n    tokens = nltk.word_tokenize(v)\n    num_tokens = len(tokens)\n\n    # Extract Entities\n    doc = nlp(v)\n    num_entities = len(doc.ents)\n\n    density = num_entities / num_tokens\n    if density < 0.08:\n        raise ValueError(\n            f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n        )\n\n    return v\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Results with Instructor and Gemini in Python\nDESCRIPTION: Shows how to use Instructor with Gemini to stream partial results as they're generated. This example creates a bedtime story with title and summary, streaming the output incrementally.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/google-openai-client.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import from_openai\nfrom openai import OpenAI\nfrom instructor import Mode\nfrom pydantic import BaseModel\nimport os\n\nclient = from_openai(\n    OpenAI(\n        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n    ),\n    mode=Mode.MD_JSON,\n)\n\n\nclass Story(BaseModel):\n    title: str\n    summary: str\n\n\nresp = client.chat.completions.create_partial(\n    model=\"gemini-1.5-flash\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a random bedtime story + 1 sentence summary\",\n        }\n    ],\n    response_model=Story,\n)\n\nfor r in resp:\n    print(r)\n\n\n# title = None summary = None\n# title='The Little Firefly Who Lost His Light' summary=None\n# title='The Little Firefly Who Lost His Light' summary='A tiny firefly learns the true meaning of friendship when he loses his glow and a wise old owl helps him find it again.'\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Output with Ollama and Instructor\nDESCRIPTION: Python code demonstrating how to use Instructor with Ollama to generate a structured character description. It defines a Pydantic model, sets up the OpenAI client with Instructor patch, and makes a request to the Llama 2 model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/ollama.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    fact: List[str] = Field(..., description=\"A list of facts about the character\")\n\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model=\"llama2\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the Harry Potter\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Harry James Potter\",\n  \"age\": 37,\n  \"fact\": [\n    \"He is the chosen one.\",\n    \"He has a lightning-shaped scar on his forehead.\",\n    \"He is the son of James and Lily Potter.\",\n    \"He attended Hogwarts School of Witchcraft and Wizardry.\",\n    \"He is a skilled wizard and sorcerer.\",\n    \"He fought against Lord Voldemort and his followers.\",\n    \"He has a pet owl named Snowy.\"\n  ]\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: User Model with Name Validation\nDESCRIPTION: Example showing how to implement a UserModel with name validation that ensures names are in uppercase.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, field_validator\n\n\nclass UserModel(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n```\n\n----------------------------------------\n\nTITLE: Using Enums with Pydantic for Structured LLM Outputs\nDESCRIPTION: This snippet shows how to incorporate Enum types in Pydantic models for LLM responses. It defines a Role enum and integrates it into a UserDetail model to constrain possible role values.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum, auto\n\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n```\n\n----------------------------------------\n\nTITLE: Creating Email with Template\nDESCRIPTION: Example of using the OpenAI client with Instructor to create an email using templated prompts and validation. Includes model configuration and template input handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/parea.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nemail = client.messages.create(\n    model=\"gpt-3.5-turbo\",\n    max_tokens=1024,\n    max_retries=3,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"I'm responding to a student's question. Here is the link to the documentation: {{doc_link1}} and {{doc_link2}}\",\n        }\n    ],\n    template_inputs={\n        \"doc_link1\": \"https://python.useinstructor.com/docs/tutorial/tutorial-1\",\n        \"doc_link2\": \"https://jxnl.github.io/docs/tutorial/tutorial-2\",\n    },\n    response_model=Email,\n)\nprint(email)\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Client with Instructor\nDESCRIPTION: This code snippet demonstrates how to configure the Azure OpenAI client and patch it with instructor for use in structured output generation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/azure.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import AzureOpenAI\nimport instructor\n\n# Configure Azure OpenAI client\nclient = AzureOpenAI(\n    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n    api_version=\"2024-02-01\",\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n)\n\n# Patch the client with instructor\nclient = instructor.from_openai(client)\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with API Model Distillation for Structured Output\nDESCRIPTION: A Python example demonstrating how to use Instructor with API Model Distillation by enabling the 'store' parameter. The code creates a Pydantic model for structured output and extracts user details from text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/openai-distilation-store.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Enable response_model and API Model Distillation\nclient = instructor.patch(OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n    def introduce(self):\n        return f\"Hello, I'm {self.name} and I'm {self.age} years old\"\n\n\n# Use the store parameter to enable API Model Distillation\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n    store=True,  # Enable API Model Distillation\n)\n```\n\n----------------------------------------\n\nTITLE: OpenAI API Integration with Redis Cache\nDESCRIPTION: Implementation example showing how to use the Redis cache decorator with OpenAI API calls. Includes a Pydantic model definition and a cached function that extracts user details from API responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/caching.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -> UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Iterable-based Data Extraction with OpenAI\nDESCRIPTION: Shows how to use Instructor with OpenAI to extract structured data using Iterables. Demonstrates the extraction of User objects from text using a non-streaming approach.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/iterable.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[User],\n    stream=False,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\n                         Correctly segment it into entitites\\\n                        Make sure the JSON is correct\",\n        },\n    ],\n)\nfor user in users:\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Cohere LLM Document Segmentation Implementation\nDESCRIPTION: Implements document segmentation using Cohere's LLM with instructor patch for structured output. Configures system prompt and creates function to extract StructuredDocument from preprocessed text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/document_segmentation.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport cohere\n\nclient = instructor.from_cohere(cohere.Client())\n\nsystem_prompt = f\"\"\"\\\nYou are a world class educator working on organizing your lecture notes.\nRead the document below and extract a StructuredDocument object from it where each section of the document is centered around a single concept/topic that can be taught in one lesson.\nEach line of the document is marked with its line number in square brackets (e.g. [1], [2], [3], etc). Use the line numbers to indicate section start and end.\n\"\"\"\n\ndef get_structured_document(document_with_line_numbers) -> StructuredDocument:\n    return client.chat.completions.create(\n        model=\"command-r-plus\",\n        response_model=StructuredDocument,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": document_with_line_numbers,\n            },\n        ],\n    )  # type: ignore\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields from Model Export\nDESCRIPTION: Shows how to use the exclude parameter to omit specific fields when exporting the model to JSON.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/fields.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom datetime import date\n\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Reasoning behind the date range.\", exclude=True\n    )\n    start_date: date\n    end_date: date\n\n\ndate_range = DateRange(\n    chain_of_thought=\"\"\"\n        I want to find the date range for the last 30 days.\n        Today is 2021-01-30 therefore the start date\n        should be 2021-01-01 and the end date is 2021-01-30\"\"\",\n    start_date=date(2021, 1, 1),\n    end_date=date(2021, 1, 30),\n)\nprint(date_range.model_dump_json())\n#> {\"start_date\":\"2021-01-01\",\"end_date\":\"2021-01-30\"}\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Output with Groq and Instructor AI\nDESCRIPTION: This Python script demonstrates how to use Groq with Instructor AI to generate structured output. It defines a Character model, initializes the Groq client, and uses it to generate information about Tesla in a structured format.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/groq.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom groq import Groq\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    fact: List[str] = Field(..., description=\"A list of facts about the subject\")\n\n\nclient = Groq(\n    api_key=os.environ.get('GROQ_API_KEY'),\n)\n\nclient = instructor.from_groq(client, mode=instructor.Mode.TOOLS)\n\nresp = client.chat.completions.create(\n    model=\"mixtral-8x7b-32768\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the company Tesla\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Tesla\",\n  \"fact\": [\n    \"electric vehicle manufacturer\",\n    \"solar panel producer\",\n    \"based in Palo Alto, California\",\n    \"founded in 2003 by Elon Musk\"\n  ]\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Citation Validation System\nDESCRIPTION: Demonstrates how to use the citation validation system with a sample question and context, showing expected output format.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/exact_citations.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"What did the author do during college?\"\ncontext = \"\"\"\nMy name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\nI went to an arts high school but in university I studied Computational Mathematics and physics.\nAs part of coop I worked at many companies including Stitchfix, Facebook.\nI also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Processing Inline Audio Segment with Gemini\nDESCRIPTION: This snippet shows how to process an inline audio segment using Gemini. It loads an MP3 file, creates an audio segment, and uses it for transcription and summarization.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/multi_modal_gemini.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\nfrom pydub import AudioSegment\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n    mode=instructor.Mode.GEMINI_JSON,  # (1)!\n)\n\n\nsound = AudioSegment.from_mp3(\"sample.mp3\")  # (2)!\nsound = sound[:60000]\n\n\nclass Transcription(BaseModel):\n    summary: str\n    exact_transcription: str\n\n\nresp = client.create(\n    response_model=Transcription,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Please transcribe this recording\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": {\n                \"mime_type\": \"audio/mp3\",\n                \"data\": sound.export().read(),  # (3)!\n            },\n        },\n    ],\n)\n\nprint(resp)\n```\n\n----------------------------------------\n\nTITLE: Using LLM for Citation Verification in Python\nDESCRIPTION: This example leverages OpenAI's LLM to validate citations. It defines a Pydantic model with a model validator that uses the LLM to check if the citation exists in the given context.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/citations.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Validation(BaseModel):\n    is_valid: bool\n    error_messages: Optional[str] = Field(None, description=\"Error messages if any\")\n\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @model_validator(mode=\"after\")\n    def substring_quote_exists(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following citation exist in the following context?\\n\\nCitation: {self.substring_quote}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n```\n\n----------------------------------------\n\nTITLE: Creating Retrieving Queries with OpenAI in Python\nDESCRIPTION: The code demonstrates how to create retrieval queries using the OpenAI client, where it sends structured messages to gather information regarding schedule and emails, based on user's requests.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nretrieval = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Retrieval,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"You are Jason's personal assistant.\n                He has two emails jason@work.com jason@personal.com\n                Today is {date.today()}\"\n        },\n        {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Context-Based Jinja Templating in Instructor\nDESCRIPTION: Demonstrates how to use the proposed context keyword in Instructor's create methods to render Jinja templates with dynamic content, showing separation of prompt structure from content and how to manage complex prompts with conditionals and loops.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/jinja-proposal.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclient.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n                You are a {{ role }} tasks with the following question \n\n                <question>\n                {{ question }}\n                </question>\n\n                Use the following context to answer the question, make sure to return [id] for every citation:\n\n                <context>\n                {% for chunk in context %}\n                  <context_chunk>\n                    <id>{{ chunk.id }}</id>\n                    <text>{{ chunk.text }}</text>\n                  </context_chunk>\n                {% endfor %}\n                </context>\n\n                {% if rules %}\n                Make sure to follow these rules:\n\n                {% for rule in rules %}\n                  * {{ rule }}\n                {% endfor %}\n                {% endif %}\n            \"\"\",\n        },\n    ],\n    context={\n        \"role\": \"professional educator\",\n        \"question\": \"What is the capital of France?\",\n        \"context\": [\n            {\"id\": 1, \"text\": \"Paris is the capital of France.\"},\n            {\"id\": 2, \"text\": \"France is a country in Europe.\"},\n        ],\n        \"rules\": [\"Use markdown.\"],\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Error Handling with Instructor\nDESCRIPTION: Demonstrates error handling for validation failures using Instructor with OpenAI integration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/validation.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field, validator\n\n\nclass User(BaseModel):\n    \"\"\"Model representing a user with validation rules.\"\"\"\n\n    name: str = Field(description=\"User's full name\")\n    age: int = Field(description=\"User's age in years\")\n\n    @validator('age')\n    def validate_age(cls, v):\n        \"\"\"Validate that age is a positive number.\"\"\"\n        if v < 0:\n            raise ValueError(\"Age cannot be negative\")\n        return v\n\n\nclient = instructor.from_openai(\n    OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")), mode=instructor.Mode.JSON\n)\n\ntry:\n    user = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=User,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Extract structured user information from the text.\",\n            },\n            {\"role\": \"user\", \"content\": \"Extract: John Doe, age: -5\"},\n        ],\n    )\n    print(user.model_dump_json(indent=2))\nexcept instructor.exceptions.InstructorValidationError as e:\n    print(f\"Validation error: {e}\")\nexcept Exception as e:\n    print(f\"Other error: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini Client with Instructor Patching for Tool Calling\nDESCRIPTION: This snippet shows how to initialize a Gemini client with Instructor patching using the GEMINI_TOOLS mode. It requires the 'jsonref' package to be installed and has some limitations with strict Pydantic validation and schema customizations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\n\nclient = instructor.from_gemini(\n    genai.GenerativeModel(), mode=instructor.Mode.GEMINI_TOOLS\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini Client with Instructor\nDESCRIPTION: Sets up the Gemini client using Instructor wrapper to enable structured outputs from the model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/multimodal-gemini.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Vision Model with Instructor and Logfire for Image Processing\nDESCRIPTION: Setting up Instructor with GPT-4V and Logfire monitoring to extract data from images. This configuration enables tracking of the entire process from request to response with detailed metrics.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport logfire\n\n\nopenai_client = OpenAI()\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\"))\nlogfire.instrument_openai(openai_client)\nclient = instructor.from_openai(openai_client, mode=instructor.Mode.MD_JSON)\n\n\n@logfire.instrument(\"extract-table\", extract_args=True)\ndef extract_table_from_image(url: str) -> Iterable[Table]:\n    return client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=Iterable[Table],\n        max_tokens=1800,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Extract out a table from the image. Only extract out the total number of skiiers.\",\n                    },\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": url}},\n                ],\n            }\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Models for User Extraction\nDESCRIPTION: Demonstrates how to create nested Pydantic models for structured data extraction. Defines a User model and a Users container model that holds a list of User objects.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/lists.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclass Users(BaseModel):\n    users: List[User]\n\n\nprint(Users.model_json_schema())\n```\n\n----------------------------------------\n\nTITLE: Implementing Role Prompting with OpenAI and Instructor\nDESCRIPTION: Demonstrates how to implement role prompting using the Instructor library with OpenAI's API. The code creates a structured response using Pydantic models and allows assigning specific roles to guide the model's output. The example shows how to generate a poem about coffee by assigning the role of a renowned poet to the model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/zero_shot/role_prompting.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Response(BaseModel):\n    poem: str\n\n\ndef role_prompting(query, role):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"{role} {query}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"Write me a short poem about coffee.\"\n    role = \"You are a renowned poet.\"\n\n    response = role_prompting(query, role)\n    print(response.poem)\n    \"\"\"\n    In the morning's gentle light,\n    A brew of warmth, dark and bright.\n    Awakening dreams, so sweet,\n    In every sip, the day we greet.\n\n    Through the steam, stories spin,\n    A liquid muse, caffeine within.\n    Moments pause, thoughts unfold,\n    In coffee's embrace, we find our gold.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Limiting the length of lists\nDESCRIPTION: This snippet focuses on generating data with a constraint on the list length. It defines Property with an index and limits the extracted properties list to exactly 5 by adding description to the properties field . The model is instructed to create a numbered list of arbitrary extracted properties with length of 5.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: list[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be exactly 5\",\n    )\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with Anthropic Models\nDESCRIPTION: Example of integrating Instructor with Anthropic's Claude models. This code shows how to extract structured data using the Anthropic client patched with Instructor functionality.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_anthropic(Anthropic())\n\n# note that client.chat.completions.create will also work\nresp = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    system=\"You are a world class AI that excels at extracting user data from a sentence\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Extracting data with Maybe Pattern\nDESCRIPTION: This snippet defines a function `extract` that uses the MaybeCharacter model to handle potential errors during data extraction. The function sends a prompt to the OpenAI API to extract data and returns a MaybeCharacter object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef extract(content: str) -> MaybeCharacter:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeCharacter,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor Patching for JSON Mode\nDESCRIPTION: This snippet shows how to initialize an OpenAI client with Instructor patching using the JSON mode. This mode uses OpenAI's JSON format for responses by setting response_format={\"type\": \"json_object\"}.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.JSON)\n```\n\n----------------------------------------\n\nTITLE: Product Model with Field Validation\nDESCRIPTION: Shows field-level validation using Pydantic Field constraints for a product model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/validation.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass Product(BaseModel):\n    \"\"\"Model representing a product with field validation constraints.\"\"\"\n\n    name: str = Field(\n        ...,\n        min_length=1,\n        max_length=100,\n        description=\"Product name between 1-100 characters\",\n    )\n    price: float = Field(..., gt=0, description=\"Product price, must be greater than 0\")\n    quantity: int = Field(\n        ..., ge=0, description=\"Available quantity, must be 0 or greater\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Enhancing DateRange Model with Chain of Thought Reasoning\nDESCRIPTION: This snippet shows how to add chain-of-thought reasoning to the DateRange model, allowing the language model to explicitly document its thinking process when determining the optimal time range for searches.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-and-beyond.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n    chain_of_thought: str = Field(\n        None,\n        description=\"Think step by step to plan what is the best time range to search in\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with llama-cpp-python and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with llama-cpp-python to extract structured data. It sets up a Llama model and uses Instructor to patch its chat completion function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport llama_cpp\nimport instructor\nfrom llama_cpp.llama_speculative import LlamaPromptLookupDecoding\nfrom pydantic import BaseModel\n\nllama = llama_cpp.Llama(\n    model_path=\"../../models/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q4_K_M.gguf\",\n    n_gpu_layers=-1,\n    chat_format=\"chatml\",\n    n_ctx=2048,\n    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=2),\n    logits_all=True,\n    verbose=False,\n)\n\ncreate = instructor.patch(\n    create=llama.create_chat_completion_openai_v1,\n    mode=instructor.Mode.JSON_SCHEMA,\n)\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nuser = create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract `Jason is 30 years old`\",\n        }\n    ],\n    response_model=ExtractUser,\n)\n\nassert user.name == \"Jason\"\nassert user.age == 30\n```\n\n----------------------------------------\n\nTITLE: Handling Nested Objects with AWS Bedrock\nDESCRIPTION: Example of processing complex nested data structures using Pydantic models with AWS Bedrock, including lists of nested objects\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/bedrock.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\nimport instructor\nfrom pydantic import BaseModel\n\n# Initialize the Bedrock client\nbedrock_client = boto3.client('bedrock-runtime')\n\n# Enable instructor patches for Bedrock client\nclient = instructor.from_bedrock(bedrock_client)\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\n# Create structured output with nested objects\nuser = client.converse(\n    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract: Jason is 25 years old.\n            He lives at 123 Main St, New York, USA\n            and has a summer house at 456 Beach Rd, Miami, USA\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n\nprint(user)\n#> User(\n#>     name='Jason',\n#>     age=25,\n#>     addresses=[\n#>         Address(street='123 Main St', city='New York', country='USA'),\n#>         Address(street='456 Beach Rd', city='Miami', country='USA')\n#>     ]\n#> )\n```\n\n----------------------------------------\n\nTITLE: Implementing Validation Hook with Instructor\nDESCRIPTION: Shows how to implement a validation hook with Instructor. This hook customizes the behavior of the validation process, allowing for retries and custom error handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cerebras.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import Instructor\n\ndef validation_hook(value, retry_count, exception):\n    print(f\"Validation failed {retry_count} times: {exception}\")\n    return retry_count < 3  # Retry up to 3 times\n\ninstructor.patch(client, validation_hook=validation_hook)\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Model with Examples for Synthetic QA Generation using OpenAI\nDESCRIPTION: This code snippet defines a Pydantic model 'SyntheticQA' with example question-answer pairs in its JSON schema. It then uses OpenAI's GPT model to generate synthetic data based on these examples. The script demonstrates how to incorporate examples into Pydantic models and use them with AI-powered data generation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/examples.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom typing import Iterable\nfrom pydantic import BaseModel, ConfigDict\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass SyntheticQA(BaseModel):\n    question: str\n    answer: str\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n                {\n                    \"question\": \"What is the largest planet in our solar system?\",\n                    \"answer\": \"Jupiter\",\n                },\n                {\n                    \"question\": \"Who wrote 'To Kill a Mockingbird'?\",\n                    \"answer\": \"Harper Lee\",\n                },\n                {\n                    \"question\": \"What element does 'O' represent on the periodic table?\",\n                    \"answer\": \"Oxygen\",\n                },\n            ]\n        }\n    )\n\n\ndef get_synthetic_data() -> Iterable[SyntheticQA]:\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Generate synthetic examples\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"Generate the exact examples you see in the examples of this prompt. \",\n            },\n        ],\n        response_model=Iterable[SyntheticQA],\n    )  # type: ignore\n\n\nif __name__ == \"__main__\":\n    for example in get_synthetic_data():\n        print(example)\n        #> question='What is the capital of France?' answer='Paris'\n        #> question='What is the largest planet in our solar system?' answer='Jupiter'\n        #> question=\"Who wrote 'To Kill a Mockingbird'?\" answer='Harper Lee'\n        \"\"\"\n        question=\"What element does 'O' represent on the periodic table?\" answer='Oxygen'\n        \"\"\"\n        \"\"\"\n        question=\"What element does 'O' represent on the periodic table?\" answer='Oxygen'\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Enums for Structured Classification\nDESCRIPTION: Demonstrates implementing enum-based classification for consistent value handling across applications.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/types.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom enum import Enum\n\n\nclass Label(str, Enum):\n    BILLING = \"BILLING\"\n    SHIPPING = \"SHIPPING\"\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Label,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Retries with Tenacity in Python\nDESCRIPTION: Demonstrates how to use AsyncRetrying for asynchronous retry logic in Python applications.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/retrying.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import AsyncRetrying, stop_after_attempt, wait_fixed\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\ntask = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=AsyncRetrying(\n        stop=stop_after_attempt(2),\n        wait=wait_fixed(1),\n    ),\n)\n\nimport asyncio\n\nresponse = asyncio.run(task)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with LiteLLM\nDESCRIPTION: Integration example of Instructor with LiteLLM for model-agnostic structured output extraction. Demonstrates how to use LiteLLM's completion function with Instructor to extract structured data from an Anthropic model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom litellm import completion\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_litellm(completion)\n\nresp = client.chat.completions.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Generating Ad Copy from Product Information using OpenAI GPT\nDESCRIPTION: This function takes a Product object and generates an AdCopy using OpenAI's GPT model. It uses the product's generated prompt as input and returns a structured AdCopy object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/image_to_ad_copy.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef generate_ad_copy(product: Product) -> AdCopy:\n    \"\"\"\n    Given a product, generate an ad copy for the product.\n    \"\"\"\n\n    logger.info(f\"Generating ad copy for product: {product.name}\")\n\n    return client_copy.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=AdCopy,\n        temperature=0.3,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert marketing assistant for all products. Your task is to generate an advertisement copy for a product using the name, description, and key features.\",\n            },\n            {\"role\": \"user\", \"content\": product.generate_prompt()},\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Knowledge Graph from Text Chunks in Python\nDESCRIPTION: This code snippet demonstrates how to process text chunks to generate a knowledge graph. It uses a list of text chunks and a generate_graph function to create a KnowledgeGraph object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntext_chunks = [\n    \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",\n    \"Professors are smart.\",\n    \"Sarah knows Jason and is a student of his.\",\n    \"Sarah is a student at the University of Toronto. and UofT is in Canada.\",\n]\n\ngraph: KnowledgeGraph = generate_graph(text_chunks)\n```\n\n----------------------------------------\n\nTITLE: Implementing Knowledge Graph Generation Function in Python\nDESCRIPTION: This function iteratively builds a knowledge graph from a list of text inputs using OpenAI's API. It updates the graph state with each iteration and visualizes the changes.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/knowledge_graph.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef generate_graph(input: List[str]) -> KnowledgeGraph:\n    cur_state = KnowledgeGraph()  # (1)!\n    num_iterations = len(input)\n    for i, inp in enumerate(input):\n        new_updates = client.chat.completions.create(\n            model=\"gpt-3.5-turbo-16k\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an iterative knowledge graph builder.\n                    You are given the current state of the graph, and you must append the nodes and edges\n                    to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Extract any new nodes and edges from the following:\n                    # Part {i}/{num_iterations} of the input:\n\n                    {inp}\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Here is the current state of the graph:\n                    {cur_state.model_dump_json(indent=2)}\"\"\",\n                },  # (2)!\n            ],\n            response_model=KnowledgeGraph,\n        )  # type: ignore\n\n        # Update the current state\n        cur_state = cur_state.update(new_updates)  # (3)!\n        cur_state.draw(prefix=f\"iteration_{i}\")\n    return cur_state\n```\n\n----------------------------------------\n\nTITLE: Combining Validation Context with Jinja Templating in Instructor\nDESCRIPTION: This example demonstrates how to use validation context in conjunction with Jinja templating in the Instructor library. It showcases template-based prompt generation and response validation using the same context, ensuring answers are grounded in provided information.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, field_validator, ValidationInfo\nfrom instructor.templating import template\n\nclass AnswerWithContext(BaseModel):\n    answer: str\n    \n    @field_validator('answer')\n    def validate_answer(cls, answer: str, info: ValidationInfo) -> str:\n        # Access the same context used in the template\n        context_doc = info.context.get(\"document\", \"\")\n        if len(context_doc) > 100 and not any(fact in answer for fact in context_doc.split('.')[:3]):\n            raise ValueError(\"Answer doesn't use key facts from the context document\")\n        return answer\n\nclient = instructor.from_openai(OpenAI(), max_retries=2)\n\n# Document to use in both template and validation\ncontext_document = \"\"\"\nThe James Webb Space Telescope (JWST) was launched on December 25, 2021. \nIt is the largest optical telescope in space and can observe objects too \nold, distant, or faint for the Hubble Space Telescope. The telescope is \nnamed after James E. Webb, who was the administrator of NASA from 1961 to 1968.\n\"\"\"\n\n# Use the template with variables from validation_context\nquestion = \"When was the James Webb Space Telescope launched and what can it do?\"\n\nresult = client.chat.completions.create(\n    model=\"gpt-4o\",\n    response_model=AnswerWithContext,\n    messages=[\n        {\n            \"role\": \"user\", \n            \"content\": \"\"\"\n            Please answer the following question based on this information:\n\n            {{ document }}\n\n            Question: {{ question }}\n            \"\"\"\n        }\n    ],\n    # Pass the same context to validation\n    context={\n        \"document\": context_document,\n        \"question\": question\n    }\n)\n\nprint(result.answer)  # Guaranteed to include facts from the context\n```\n\n----------------------------------------\n\nTITLE: Implementing Complex Field Validators in Python with Instructor\nDESCRIPTION: Shows how to use Pydantic's field_validator decorator for more complex validation logic. It includes examples of validating product names, SKUs, and prices with custom rules.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/field_validation.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, field_validator\nimport instructor\nfrom openai import OpenAI\nimport re\n\nclient = instructor.from_openai(OpenAI())\n\nclass Product(BaseModel):\n    name: str\n    sku: str\n    price: float\n    \n    @field_validator('name')\n    @classmethod\n    def validate_name(cls, v):\n        if len(v.strip()) < 3:\n            raise ValueError(\"Product name must be at least 3 characters\")\n        return v.strip()\n    \n    @field_validator('sku')\n    @classmethod\n    def validate_sku(cls, v):\n        if not re.match(r'^[A-Z]{3}-\\d{4}$', v):\n            raise ValueError(\"SKU must be in format XXX-0000\")\n        return v\n    \n    @field_validator('price')\n    @classmethod\n    def validate_price(cls, v):\n        if v <= 0:\n            raise ValueError(\"Price must be greater than zero\")\n        if v > 10000:\n            raise ValueError(\"Price exceeds maximum allowed value\")\n        return v\n\n# Extract validated data\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Product: Wireless Headphones, SKU: ABC-1234, Price: $79.99\"}\n    ],\n    response_model=Product\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Data Structures with Pydantic for Legal Entity Modeling\nDESCRIPTION: Defines the core data structures using Pydantic models for representing entities, properties, and document extractions. Includes Entity and Property classes with fields for ID, dependencies, and attributes.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/entity_resolution.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n    resolved_absolute_value: str\n\n\nclass Entity(BaseModel):\n    id: int = Field(\n        ...,\n        description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\",\n    )\n    subquote_string: List[str] = Field(\n        ...,\n        description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\",\n    )\n    entity_title: str\n    properties: List[Property] = Field(\n        ..., description=\"List of properties of the entity\"\n    )\n    dependencies: List[int] = Field(\n        ...,\n        description=\"List of entity ids that this entity depends  or relies on to resolve it\",\n    )\n\n\nclass DocumentExtraction(BaseModel):\n    entities: List[Entity] = Field(\n        ...,\n        description=\"Body of the answer, each fact should be a separate object with a body and a list of sources\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Structured Output Generation with Perplexity\nDESCRIPTION: Shows how to implement asynchronous structured output generation using Perplexity AI with AsyncOpenAI client and asyncio.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/perplexity.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import AsyncOpenAI\nimport instructor\nfrom pydantic import BaseModel\nimport asyncio\n\n# Initialize with API key\nclient = AsyncOpenAI(\n    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n    base_url=\"https://api.perplexity.ai\"\n)\n\n# Enable instructor patches for Perplexity client\nclient = instructor.from_perplexity(client)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_user():\n    user = await client.chat.completions.create(\n        model=\"sonar-medium-online\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n        ],\n        response_model=User,\n    )\n    return user\n\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)\n# > User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Defining Node and Edge Base Classes\nDESCRIPTION: Creates base classes for nodes and edges in the knowledge graph using Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Nested Lists with Instructor for Recipe Data\nDESCRIPTION: This snippet demonstrates how to extract nested list data using Instructor, specifically for a recipe. It defines models for Ingredient and Recipe, where Recipe contains a list of Ingredients and a list of steps.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/nested_structure.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\nfrom typing import List\n\nclient = instructor.from_openai(OpenAI())\n\nclass Ingredient(BaseModel):\n    name: str\n    amount: str\n    unit: str\n\nclass Recipe(BaseModel):\n    title: str\n    description: str\n    ingredients: List[Ingredient]  # Nested list of ingredients\n    steps: List[str]  # List of strings\n\n# Extract nested list data\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Recipe: Chocolate Chip Cookies\n        \n        Description: Classic homemade chocolate chip cookies that are soft in the middle and crispy on the edges.\n        \n        Ingredients:\n        - 2 1/4 cups all-purpose flour\n        - 1 teaspoon baking soda\n        - 1 teaspoon salt\n        - 1 cup butter\n        - 3/4 cup white sugar\n        - 3/4 cup brown sugar\n        - 2 eggs\n        - 2 teaspoons vanilla extract\n        - 2 cups chocolate chips\n        \n        Instructions:\n        1. Preheat oven to 375°F (190°C)\n        2. Mix flour, baking soda, and salt\n        3. Cream butter and sugars, then add eggs and vanilla\n        4. Gradually add dry ingredients\n        5. Stir in chocolate chips\n        6. Drop by rounded tablespoons onto ungreased baking sheets\n        7. Bake for 9 to 11 minutes or until golden brown\n        8. Cool on wire racks\n        \"\"\"}\n    ],\n    response_model=Recipe\n)\n```\n\n----------------------------------------\n\nTITLE: PDF Analysis with Google GenAI and New File Upload\nDESCRIPTION: Shows how to analyze a PDF using Google's Gemini model by first downloading and uploading the file. Uses PDFWithGenaiFile to handle file upload and processing with retry mechanisms.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/multimodal.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai import Client\nimport instructor\nfrom pydantic import BaseModel\nfrom instructor.multimodal import PDFWithGenaiFile\nimport requests\n\n# Set up the client\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\nclient = instructor.from_genai(Client())\n\nwith requests.get(url) as response:\n    pdf_data = response.content\n    with open(\"./invoice.pdf\", \"wb\") as f:\n        f.write(pdf_data)\n\n\n# Create a model for analyzing PDFs\nclass Invoice(BaseModel):\n    total: float\n    items: list[str]\n\n\n# Load and analyze a PDF\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash\",\n    response_model=Invoice,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Analyze this document\",\n                PDFWithGenaiFile.from_new_genai_file(\n                    file_path=\"./invoice.pdf\",\n                    retry_delay=10,\n                    max_retries=20,\n                ),\n            ],\n        }\n    ],\n)\n\nprint(response)\n# > Total = 220, items = ['English Tea', 'Tofu']\n```\n\n----------------------------------------\n\nTITLE: Advanced Jinja Syntax Usage in Instructor AI Prompts\nDESCRIPTION: This snippet demonstrates advanced usage of Jinja syntax in Instructor AI prompts. It shows how to render lists, use conditionals, and format complex prompts with multiple context elements and rules.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/templating.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Citation(BaseModel):\n    source_ids: list[int]\n    text: str\n\n\nclass Response(BaseModel):\n    answer: list[Citation]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n                You are a {{ role }} tasks with the following question\n\n                <question>\n                {{ question }}\n                </question>\n\n                Use the following context to answer the question, make sure to return [id] for every citation:\n\n                <context>\n                {% for chunk in context %}\n                  <context_chunk>\n                    <id>{{ chunk.id }}</id>\n                    <text>{{ chunk.text }}</text>\n                  </context_chunk>\n                {% endfor %}\n                </context>\n\n                {% if rules %}\n                Make sure to follow these rules:\n\n                {% for rule in rules %}\n                  * {{ rule }}\n                {% endfor %}\n                {% endif %}\n            \"\"\",\n        },\n    ],\n    response_model=Response,\n    context={\n        \"role\": \"professional educator\",\n        \"question\": \"What is the capital of France?\",\n        \"context\": [\n            {\"id\": 1, \"text\": \"Paris is the capital of France.\"},\n            {\"id\": 2, \"text\": \"France is a country in Europe.\"},\n        ],\n        \"rules\": [\"Use markdown.\"],\n    },\n)\n\nprint(resp)\n#> answer=[Citation(source_ids=[1], text='The capital of France is Paris.')]\n```\n\n----------------------------------------\n\nTITLE: Implementing Code-based Validation with Pydantic\nDESCRIPTION: Demonstrates how to create a Pydantic model with custom validation rules using the Annotated class. The example enforces a naming rule that requires spaces in names.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/reask_validation.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Annotated Types with Custom Field Descriptions\nDESCRIPTION: Demonstrates using Annotated types with Field descriptions for enhanced type information.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/types.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom typing import Annotated\nfrom pydantic import Field\n\nclient = instructor.from_openai(openai.OpenAI())\n\nUpperCaseStr = Annotated[str, Field(description=\"string must be upper case\")]\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UpperCaseStr,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the capital of france?\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced KnowledgeGraph with Update Functionality\nDESCRIPTION: Enhanced KnowledgeGraph class with methods for updating and combining graphs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass KnowledgeGraph(BaseModel):\n    nodes: Optional[list[Node]] = Field(..., default_factory=list)\n    edges: Optional[list[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -> \"KnowledgeGraph\":\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),\n            edges=list(set(self.edges + other.edges)),\n        )\n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(str(node.id), node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(\n                str(edge.source), str(edge.target), label=edge.label, color=edge.color\n            )\n\n        return display(dot)\n```\n\n----------------------------------------\n\nTITLE: Using Pydantic Validators with Context in Instructor AI\nDESCRIPTION: This example shows how to use the context parameter with Pydantic validators to implement dynamic validation rules and data transformations. It demonstrates redacting sensitive information based on regex patterns provided in the context.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/templating.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel, ValidationInfo, field_validator\nimport re\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Response(BaseModel):\n    text: str\n\n    @field_validator('text')\n    @classmethod\n    def redact_regex(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            redact_patterns = context.get('redact_patterns', [])\n            for pattern in redact_patterns:\n                v = re.sub(pattern, '****', v)\n        return v\n\n\nresponse = client.create(\n    model=\"gpt-4o\",\n    response_model=Response,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n                Write about a {{ topic }}\n\n                {% if banned_words %}\n                You must not use the following banned words:\n\n                <banned_words>\n                {% for word in banned_words %}\n                * {{ word }}\n                {% endfor %}\n                </banned_words>\n                {% endif %}\n              \"\"\",\n        },\n    ],\n    context={\n        \"topic\": \"jason and now his phone number is 123-456-7890\",\n        \"redact_patterns\": [\n            r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # Phone number pattern\n            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN pattern\n        ],\n    },\n    max_retries=3,\n)\n\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Testing LLM Validator with Sample Messages and Error Handling\nDESCRIPTION: Code that tests the LLM validator with different sample messages. It attempts to create Statement objects and catches ValidationError exceptions when objectionable content is detected.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    \"I think we should always treat violence as the best solution\",\n    \"There are some great pastries down the road at this bakery I know\",\n]\n\nfor message in messages:\n    try:\n        Statement(message=message)\n    except ValidationError as e:\n        print(e)\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Input Caching with Anthropic Client\nDESCRIPTION: Demonstrates how to implement caching for text input with the Anthropic client. This approach helps improve performance and reduce costs when processing large text files repeatedly by caching the context for subsequent calls.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport os\n\n# Third-party imports\nimport instructor\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel, Field\n\n# Set up environment (typically handled before script execution)\n# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Uncomment and replace with your API key if not set\n\n# Define your Pydantic model with proper annotations\nclass Character(BaseModel):\n    \"\"\"Model representing a character extracted from text.\"\"\"\n    name: str = Field(description=\"The character's full name\")\n    description: str = Field(description=\"A description of the character\")\n\n# Initialize client with explicit mode and prompt caching\nclient = instructor.from_anthropic(\n    Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\")),\n    mode=instructor.Mode.ANTHROPIC_TOOLS,\n    enable_prompt_caching=True  # Enable prompt caching\n)\n\ntry:\n    # Load your large context\n    with open(\"./book.txt\", \"r\") as f:\n        book = f.read()\n\n    # Make multiple calls using the cached context\n    for _ in range(2):\n        # The first time processes the large text, subsequent calls use the cache\n        resp, completion = client.chat.completions.create_with_completion(\n            model=\"claude-3-haiku-20240307\",  # Use latest stable model\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"Extract character information from the provided text.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"<book>\" + book + \"</book>\",\n                            \"cache_control\": {\"type\": \"ephemeral\"},  # Mark for caching\n                        },\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Extract a character from the text given above\",\n                        },\n                    ],\n                },\n            ],\n            response_model=Character,\n            max_tokens=1000,\n        )\n\n        # Process the result\n        print(f\"Character: {resp.name}\")\n        print(f\"Description: {resp.description}\")\n\n        # The completion contains the raw response\n        print(f\"Raw completion length: {len(completion)}\")\n\n    # Note: Second iteration should be faster due to cache hit\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Customizing Validation Error Messages in Python with Instructor\nDESCRIPTION: Demonstrates how to customize validation error messages for better feedback. This example includes custom error messages for credit card validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/field_validation.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass CreditCard(BaseModel):\n    number: str = Field(\n        ..., \n        pattern=r'^\\d{16}$',\n        json_schema_extra={\"error_msg\": \"Credit card number must be exactly 16 digits\"}\n    )\n    expiry_month: int = Field(\n        ..., \n        ge=1, \n        le=12,\n        json_schema_extra={\"error_msg\": \"Expiry month must be between 1 and 12\"}\n    )\n    expiry_year: int = Field(\n        ..., \n        ge=2023, \n        le=2030,\n        json_schema_extra={\"error_msg\": \"Expiry year must be between 2023 and 2030\"}\n    )\n    cvv: str = Field(\n        ..., \n        pattern=r'^\\d{3,4}$',\n        json_schema_extra={\"error_msg\": \"CVV must be 3 or 4 digits\"}\n    )\n```\n\n----------------------------------------\n\nTITLE: Streaming Structured Outputs with Cerebras Inference\nDESCRIPTION: Example demonstrating how to use streaming with Cerebras Inference in CEREBRAS_JSON mode. This allows processing multiple structured responses as they arrive, leveraging Cerebras's fast inference capabilities.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs-with-cerebras-inference.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom cerebras.cloud.sdk import Cerebras\nfrom pydantic import BaseModel\nfrom typing import Iterable\n\nclient = instructor.from_cerebras(Cerebras(), mode=instructor.Mode.CEREBRAS_JSON)\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create(\n    model=\"llama3.1-70b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract all users from this sentence : Chris is 27 and lives in San Francisco, John is 30 and lives in New York while their college roommate Jessica is 26 and lives in London\",\n        }\n    ],\n    response_model=Iterable[Person],\n    stream=True,\n)\n\nfor person in resp:\n    print(person)\n    #> Person(name='Chris', age=27)\n    #> Person(name='John', age=30)\n    #> Person(name='Jessica', age=26)\n```\n\n----------------------------------------\n\nTITLE: Implementing Model-level Validation in Python with Instructor\nDESCRIPTION: Demonstrates how to use Pydantic's model_validator for validating relationships between fields. This example checks if the end date is after the start date in a date range.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/field_validation.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, model_validator\nimport instructor\nfrom openai import OpenAI\nfrom datetime import date\n\nclient = instructor.from_openai(OpenAI())\n\nclass DateRange(BaseModel):\n    start_date: date\n    end_date: date\n    \n    @model_validator(mode='after')\n    def validate_date_range(self):\n        if self.end_date < self.start_date:\n            raise ValueError(\"End date must be after start date\")\n        return self\n```\n\n----------------------------------------\n\nTITLE: Processing Audio Files with Gemini AI\nDESCRIPTION: Shows how to analyze audio files with Gemini models to extract structured information like transcripts, summaries, speaker identification, and key points. The code demonstrates loading audio from URL or local path.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import Audio\nfrom pydantic import BaseModel\nimport instructor\nfrom google.genai import Client\n\n\nclass AudioDescription(BaseModel):\n    transcript: str\n    summary: str\n    speakers: list[str]\n    key_points: list[str]\n\n\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/gettysburg.wav\"\n\nclient = instructor.from_genai(Client())\n\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash\",\n    response_model=AudioDescription,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Please transcribe and analyze this audio:\",\n                # Multiple loading options:\n                Audio.from_url(url),\n                # Option 2: Local file\n                # Audio.from_path(\"path/to/local/audio.mp3\")\n            ],\n        },\n    ],\n)\n\nprint(response)\n# > transcript='Four score and seven years ago our fathers...\"]\n\n```\n\n----------------------------------------\n\nTITLE: Extracting Basic Structured Data with OpenAI\nDESCRIPTION: Python code demonstrating how to use Instructor with OpenAI to extract structured data based on a Pydantic model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/getting-started.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Define your output structure\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n# Create an instructor-patched client\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data\nuser_info = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserInfo,\n    messages=[\n        {\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}\n    ],\n)\n\nprint(f\"Name: {user_info.name}, Age: {user_info.age}\")\n# Output: Name: John Doe, Age: 30\n```\n\n----------------------------------------\n\nTITLE: Validation with Optional Fields\nDESCRIPTION: Demonstrates how to implement custom validation for optional fields using Pydantic validators.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/optional_fields.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom pydantic import BaseModel, field_validator\nimport instructor\nfrom openai import OpenAI\nimport re\n\nclient = instructor.from_openai(OpenAI())\n\nclass ContactInfo(BaseModel):\n    email: str\n    phone: Optional[str] = None\n    \n    @field_validator('phone')\n    @classmethod\n    def validate_phone(cls, v):\n        if v is not None and not re.match(r'^\\+?[1-9]\\d{1,14}$', v):\n            raise ValueError(\"Invalid phone format\")\n        return v\n```\n\n----------------------------------------\n\nTITLE: Asynchronous User Data Extraction with Gemini\nDESCRIPTION: Shows how to implement asynchronous user data extraction using Instructor with Gemini model. Uses asyncio for async operation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/google.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\nimport asyncio\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_user():\n    client = instructor.from_gemini(\n        client=genai.GenerativeModel(\n            model_name=\"models/gemini-1.5-flash-latest\",\n        ),\n        use_async=True,\n    )\n\n    user = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract Jason is 25 years old.\",\n            }\n        ],\n        response_model=User,\n    )\n    return user\n\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)  # User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Using Anthropic Reasoning Tools with Instructor in Python\nDESCRIPTION: This code snippet demonstrates how to use the `instructor.Mode.ANTHROPIC_REASONING_TOOLS` mode with the Anthropic client to get validated tool calls. It defines a Pydantic model `Answer` to represent the expected response structure and uses the `client.chat.completions.create` method with the appropriate model and parameters to trigger the reasoning process.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom anthropic import Anthropic\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass Answer(BaseModel):\n    answer: float\n\n\nclient = Anthropic()\nclient = instructor.from_anthropic(client, mode=instructor.Mode.ANTHROPIC_REASONING_TOOLS)\nresponse = client.chat.completions.create(\n    model=\"claude-3-7-sonnet-latest\",\n    response_model=Answer,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Which is larger, 9.11 or 9.8\",\n        },\n    ],\n    temperature=1,\n    max_tokens=2000,\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 1024},\n)\n\n\n# Assertions to validate the response\nassert isinstance(response, Answer)\nassert response.answer == 9.8\n```\n\n----------------------------------------\n\nTITLE: Basic Instructor Usage with OpenAI\nDESCRIPTION: Demonstrates the basic setup and usage of Instructor with OpenAI integration, including model definition, client initialization, and error handling. Uses Pydantic for data validation and structured responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/templates/concept_template.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport os\nfrom typing import List, Optional\n\n# Third-party imports\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\n# Set up environment (typically handled before script execution)\n# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"  # Uncomment and replace with your API key if not set\n\n# Define your model with proper annotations\nclass ExampleModel(BaseModel):\n    \"\"\"Model that demonstrates the concept in action.\"\"\"\n    field1: str = Field(description=\"Description of field1\")\n    field2: int = Field(description=\"Description of field2\")\n    # Additional fields demonstrating the concept\n\n# Initialize client with explicit mode\nclient = instructor.from_openai(\n    OpenAI(),\n    mode=instructor.Mode.JSON  # Always specify mode explicitly\n)\n\n# Use the concept with proper error handling\ntry:\n    result = client.chat.completions.create(\n        model=\"gpt-4o\",  # Use latest stable model\n        messages=[\n            {\"role\": \"system\", \"content\": \"Generate structured data based on the user request.\"},\n            {\"role\": \"user\", \"content\": \"Example prompt\"}\n        ],\n        response_model=ExampleModel,\n        # Concept-specific parameters\n    )\n    \n    print(result.model_dump_json(indent=2))\n    # Expected output:\n    # {\n    #   \"field1\": \"example value\",\n    #   \"field2\": 42\n    # }\nexcept instructor.exceptions.InstructorError as e:\n    print(f\"Validation error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Reiterating Instructions in Pydantic Field Descriptions\nDESCRIPTION: This snippet demonstrates how to reiterate complex instructions in Pydantic field descriptions. It defines a Role class with an instructions field that restates the rules for determining the title, improving clarity and performance.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass Role(BaseModel):\n    \"\"\"\n    Extract the role based on the following rules ...\n    \"\"\"\n\n    instructions: str = Field(\n        ...,\n        description=\"Restate the instructions and rules to correctly determine the title.\",\n    )\n    title: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Logging and Debugging Example\nDESCRIPTION: Complete example implementing various hooks for logging and debugging, including error counting functionality.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nimport pydantic\n\n\ndef log_completion_kwargs(*args, **kwargs) -> None:\n    print(\"## Completion kwargs:\")\n    print(kwargs)\n\n\ndef log_completion_response(response) -> None:\n    print(\"## Completion response:\")\n    print(response.model_dump())\n\n\ndef handle_completion_error(error: Exception) -> None:\n    print(f\"## Completion error: {error}\")\n    print(f\"Type: {type(error).__name__}\")\n    print(f\"Message: {str(error)}\")\n\n\ndef log_parse_error(error: Exception) -> None:\n    print(f\"## Parse error: {error}\")\n    print(f\"Type: {type(error).__name__}\")\n    print(f\"Message: {str(error)}\")\n\n\n# Handler for a custom logger that records how many errors have occurred\nclass ErrorCounter:\n    def __init__(self) -> None:\n        self.error_count = 0\n\n    def count_error(self, error: Exception) -> None:\n        self.error_count += 1\n        print(f\"Error count: {self.error_count}\")\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n# Register the hooks\nclient.on(\"completion:kwargs\", log_completion_kwargs)\nclient.on(\"completion:response\", log_completion_response)\nclient.on(\"completion:error\", handle_completion_error)\nclient.on(\"parse:error\", log_parse_error)\n\n# Example with error counter\nerror_counter = ErrorCounter()\nclient.on(\"completion:error\", error_counter.count_error)\nclient.on(\"parse:error\", error_counter.count_error)\n\n# Define a model for extraction\nclass User(pydantic.BaseModel):\n    name: str\n    age: int\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Output with Together AI and Instructor in Python\nDESCRIPTION: Demonstrates how to use the patched client to generate a structured output. It sends a prompt to extract user information and receives a response conforming to the UserExtract model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/together.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuser: UserExtract = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserExtract), \"Should be instance of UserExtract\"\nassert user.name.lower() == \"jason\"\nassert user.age == 25\n\nprint(user.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 25\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Schema for User Data Extraction\nDESCRIPTION: Demonstrates how to create Pydantic models for structured data extraction, defining a User class and a Users container class that holds a list of User objects. Shows the resulting JSON schema structure.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/iterable.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclass Users(BaseModel):\n    users: List[User]\n\n\nprint(Users.model_json_schema())\n```\n\n----------------------------------------\n\nTITLE: Handling Uncertain Responses with Maybe Type in Instructor\nDESCRIPTION: Shows how to use the Maybe type to handle uncertain or incomplete responses gracefully. This pattern allows code to cleanly handle cases where some expected fields might be missing from the LLM output, providing better error reporting and fallback options.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom instructor.dsl.maybe import Maybe\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    occupation: str\n\nclient = instructor.from_openai(OpenAI())\n\n# Use Maybe to handle potential missing information\nresult = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Maybe[Person],\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract info about Jane Doe who is 28 years old.\"}\n    ]\n)\n\nif result.value:\n    print(f\"Name: {result.value.name}, Age: {result.value.age}\")\n    if hasattr(result.value, 'occupation'):\n        print(f\"Occupation: {result.value.occupation}\")\n    else:\n        print(\"Occupation information not available\")\nelse:\n    print(f\"Unable to extract person. Reason: {result.reason}\")\n```\n\n----------------------------------------\n\nTITLE: Using Gemini Models via OpenAI Client\nDESCRIPTION: Advanced implementation that calls Google's Gemini models through the OpenAI client interface. Requires Google Auth setup, Vertex AI configuration, and the Google Auth library for authentication.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport google.auth\nimport google.auth.transport.requests\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\ncreds, project = google.auth.default()\nauth_req = google.auth.transport.requests.Request()\ncreds.refresh(auth_req)\n\n# Pass the Vertex endpoint and authentication to the OpenAI SDK\nPROJECT = 'PROJECT_ID'\nLOCATION = (\n    'LOCATION'  # https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations\n)\nbase_url = f'https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT}/locations/{LOCATION}/endpoints/openapi'\n\nclient = instructor.from_openai(\n    OpenAI(base_url=base_url, api_key=creds.token), mode=instructor.Mode.JSON\n)\n\n\n# JSON mode is req'd\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create(\n    model=\"google/gemini-1.5-flash-001\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced Retry Logic with Tenacity in Python\nDESCRIPTION: Shows how to use Tenacity's Retrying class to define custom retry logic with stop conditions and wait times.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/retrying.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import Retrying, stop_after_attempt, wait_fixed\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=Retrying(\n        stop=stop_after_attempt(2),  # (1)!\n        wait=wait_fixed(1),  # (2)!\n    ),  # (3)!\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Client with OpenAI and Instructor\nDESCRIPTION: Demonstrates how to use the async version of the OpenAI client with Instructor for asynchronous operations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/client_setup.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nasync def extract_user():\n    async_client = instructor.from_openai(AsyncOpenAI())\n    return await async_client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=User,\n        messages=[\n            {\"role\": \"user\", \"content\": \"John is 30 years old.\"}\n        ]\n    )\n\nuser = asyncio.run(extract_user())\n```\n\n----------------------------------------\n\nTITLE: PDF Processing with Cache Control\nDESCRIPTION: Shows how to implement PDF processing with cache control for improved performance when making multiple requests with the same PDF.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import PdfWithCacheControl\nfrom pydantic import BaseModel\nimport instructor\nfrom anthropic import Anthropic\n\n\nclass Receipt(BaseModel):\n    total: int\n    items: list[str]\n\n\nclient = instructor.from_anthropic(Anthropic())\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\n# Multiple ways to load an PDF:\nresponse, completion = client.chat.completions.create_with_completion(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=Receipt,\n    max_tokens=1000,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Extract out the total and line items from the invoice\",\n                # Option 1: Direct URL\n                PdfWithCacheControl.from_url(url),\n            ],\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Together AI Client with Instructor Patch in Python\nDESCRIPTION: Sets up the Together AI client with Instructor patch to enable structured outputs. It configures the OpenAI client with Together's base URL and API key, then applies the Instructor patch to support response_model parameter.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/together.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport openai\nfrom pydantic import BaseModel\nimport instructor\n\nclient = openai.OpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n)\n\n# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.create methods to support the response_model parameter\nclient = instructor.from_openai(client, mode=instructor.Mode.TOOLS)\n```\n\n----------------------------------------\n\nTITLE: Iterative Knowledge Graph Generation\nDESCRIPTION: Implements iterative graph generation function that processes input in chunks and updates the graph progressively.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef generate_graph(input: list[str]) -> KnowledgeGraph:\n    cur_state = KnowledgeGraph()\n\n    for i, inp in enumerate(input):\n        new_updates = client.chat.completions.create(\n            model=\"gpt-4-1106-preview\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an iterative knowledge graph builder.\n                    You are given the current state of the graph, and you must append the nodes and edges \n                    to it Do not provide any duplicates and try to reuse nodes as much as possible.\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Extract any new nodes and edges from the following:\n                    # Part {i}/{len(input)} of the input:\n\n                    {inp}\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Here is the current state of the graph:\n                    {cur_state.model_dump_json(indent=2)}\"\"\",\n                },\n            ],\n            response_model=KnowledgeGraph,\n        )\n\n        cur_state = cur_state.update(new_updates)\n        cur_state.visualize_knowledge_graph()\n\n    return cur_state\n```\n\n----------------------------------------\n\nTITLE: Section Text Extraction Function\nDESCRIPTION: Creates a function to reconstruct section text from line indices and the line mapping dictionary. Returns segments with title, content, and line range information.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/document_segmentation.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_sections_text(structured_doc, line2text):\n    segments = []\n    for s in structured_doc.sections:\n        contents = []\n        for line_id in range(s.start_index, s.end_index):\n            contents.append(line2text.get(line_id, ''))\n        segments.append(\n            {\n                \"title\": s.title,\n                \"content\": \"\\n\".join(contents),\n                \"start\": s.start_index,\n                \"end\": s.end_index,\n            }\n        )\n    return segments\n```\n\n----------------------------------------\n\nTITLE: Asynchronous User Extraction with Cerebras and Instructor\nDESCRIPTION: Shows how to use Instructor with Cerebras for asynchronous extraction of user information. It initializes an async client, defines a User model, and extracts name and age from a given sentence using an async function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cerebras.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cerebras.cloud.sdk import AsyncCerebras\nimport instructor\nfrom pydantic import BaseModel\nimport asyncio\n\n# Initialize async client\nclient = AsyncCerebras()\n\n# Enable instructor patches\nclient = instructor.from_cerebras(client)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nasync def extract_user():\n    resp = await client.chat.completions.create(\n        model=\"llama3.1-70b\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract the name and age of the person in this sentence: John Smith is 29 years old.\",\n            }\n        ],\n        response_model=User,\n    )\n    return resp\n\n# Run async function\nresp = asyncio.run(extract_user())\nprint(resp)\n#> User(name='John Smith', age=29)\n```\n\n----------------------------------------\n\nTITLE: Implementing Depth Validation for Recursive Schemas\nDESCRIPTION: Demonstrates how to add depth validation to prevent infinite recursion in recursive schemas using Pydantic's model validator.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/recursive.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import model_validator\n\n\nclass RecursiveNodeWithDepth(RecursiveNode):\n    @model_validator(mode='after')\n    def validate_depth(self) -> \"RecursiveNodeWithDepth\":\n        def check_depth(node: \"RecursiveNodeWithDepth\", current_depth: int = 0) -> int:\n            if current_depth > 10:  # Maximum allowed depth\n                raise ValueError(\"Maximum depth exceeded\")\n            return max(\n                [check_depth(child, current_depth + 1) for child in node.children],\n                default=current_depth,\n            )\n\n        check_depth(self)\n        return self\n```\n\n----------------------------------------\n\nTITLE: Defining a Pydantic Model for Structured Output\nDESCRIPTION: Creates a Pydantic BaseModel class representing a user profile with annotated fields. This model defines the structure of the data to be extracted from the LLM's response, including name, age, and biographical information.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass User(BaseModel):\n    \"\"\"Model representing a user profile.\"\"\"\n    name: str = Field(description=\"The user's full name\")\n    age: int = Field(description=\"The user's age in years\")\n    bio: str = Field(description=\"A biographical description of the user\")\n```\n\n----------------------------------------\n\nTITLE: Concurrent Processing with asyncio.as_completed in Python\nDESCRIPTION: Demonstrates using asyncio.as_completed to handle tasks as they complete. This method allows processing of results as soon as they become available, which can be useful for large datasets or streaming data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/learn-async.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def as_completed():\n    all_persons = []\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person)\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with OpenAI API for Pydantic Model Extraction\nDESCRIPTION: Shows how to use Instructor to simplify working with OpenAI models and extract Pydantic objects from prompts, including automatic retries for invalid responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor  # pip install instructor\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.from_openai(OpenAI())  # (1)!\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n    max_retries=3,  # (2)!\n)\n\nassert user.name == \"Jason\"  # (3)!\nassert user.age == 25\n```\n\n----------------------------------------\n\nTITLE: Basic Person Object Extraction using Instructor AI and OpenAI\nDESCRIPTION: Demonstrates the basic pattern of extracting a person's information using a Pydantic model with Instructor AI integration. Shows how to define a simple schema and extract structured data from text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/simple_object.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\n# Define the structure you want to extract\nclass Person(BaseModel):\n    name: str\n    age: int\n    occupation: str\n\n# Extract the structured data\nclient = instructor.from_openai(OpenAI())\nperson = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"John Smith is a 35-year-old software engineer.\"}\n    ],\n    response_model=Person\n)\n\nprint(f\"Name: {person.name}\")\nprint(f\"Age: {person.age}\")\nprint(f\"Occupation: {person.occupation}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Union Types for Multiple Response Types\nDESCRIPTION: Demonstrates using Union types to handle multiple possible response structures.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/types.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Add(BaseModel):\n    a: int\n    b: int\n\n\nclass Weather(BaseModel):\n    location: str\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Union[Add, Weather],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is 5 + 5?\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Audio File with Gemini Using Content Lists\nDESCRIPTION: This snippet demonstrates how to process an audio file using Gemini by passing content as a list. It includes both a normal user message and a file object in the content list for flexible input handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/multi_modal_gemini.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n    mode=instructor.Mode.GEMINI_JSON,  # (1)!\n)\n\nmp3_file = genai.upload_file(\"./sample.mp3\")  # (2)!\n\n\nclass Description(BaseModel):\n    description: str\n\n\ncontent = [\n    \"Summarize what's happening in this audio file and who the main speaker is\",\n    mp3_file,  # (3)!\n]\n\nresp = client.create(\n    response_model=Description,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": content,\n        }\n    ],\n)\n\nprint(resp)\n```\n\n----------------------------------------\n\nTITLE: Progress Tracking with Streaming Responses\nDESCRIPTION: Shows how to implement progress tracking for streaming responses by monitoring completed fields. The example uses a Report model with three fields and calculates completion percentage.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/streaming/basics.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\nclass Report(BaseModel):\n    title: str\n    summary: str\n    conclusion: str\n\n# Track completed fields\ncompleted = set()\ntotal_fields = 3  # Number of fields in our model\n\nfor partial in client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Generate a report on climate change\"}\n    ],\n    response_model=Report,\n    stream=True\n):\n    # Check which fields are complete\n    for field in [\"title\", \"summary\", \"conclusion\"]:\n        if hasattr(partial, field) and getattr(partial, field) and field not in completed:\n            completed.add(field)\n            percent = (len(completed) / total_fields) * 100\n            print(f\"Received: {field} - {percent:.0f}% complete\")\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Model for LLM Response\nDESCRIPTION: Creates a QuestionAnswer Pydantic model that defines the structure for flashcard data including questions, options, difficulty level, and metadata.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-flashcards.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nfrom pydantic import BaseModel, Field\nfrom pydantic.json_schema import SkipJsonSchema\n\n\nclass QuestionAnswer(BaseModel):\n    question: str = Field(description=\"Question about the topic\")\n    options: list[str] = Field(\n        description=\"Potential answers to the question.\", min_items=3, max_items=5\n    )\n    answer_index: int = Field(\n        description=\"Index of the correct answer options (starting from 0).\", ge=0, lt=5\n    )\n    difficulty: int = Field(\n        description=\"Difficulty of this question from 1 to 5, 5 being the most difficult.\",\n        gt=0,\n        le=5,\n    )\n    youtube_url: SkipJsonSchema[str | None] = None\n    id: uuid.UUID = Field(description=\"Unique identifier\", default_factory=uuid.uuid4)\n```\n\n----------------------------------------\n\nTITLE: Setting up Anthropic (Claude) Client with Instructor\nDESCRIPTION: Shows how to configure an Anthropic client for Claude models using Instructor, with options for default ANTHROPIC_TOOLS mode and JSON mode.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/client_setup.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom anthropic import Anthropic\n\n# Default mode (ANTHROPIC_TOOLS)\nclient = instructor.from_anthropic(Anthropic())\n\n# With JSON mode\nclient = instructor.from_anthropic(\n    Anthropic(),\n    mode=instructor.Mode.JSON\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced Reasoning Models in Python\nDESCRIPTION: Advanced response models implementing different approaches to reasoning including assumptions, error awareness, and intermediate calculations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/bad-schemas-could-break-llms.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass AssumptionBasedAnswer(BaseModel):\n    assumptions: list[str]\n    logic_flow: str\n    answer: int\n\nclass ErrorAwareCalculation(BaseModel):\n    key_steps: list[str]\n    potential_pitfalls: list[str]\n    intermediate_results: list[str]\n    answer: int\n\n lass AnswerWithIntermediateCalculations(BaseModel):\n    assumptions: list[str]\n    intermediate_calculations: list[str]\n    chain_of_thought: str\n    final_answer: int\n\nclass AssumptionBasedAnswerWithExtraFields(BaseModel):\n    assumptions: list[str]\n    logic_flow: str\n    important_intermediate_calculations: list[str]\n    potential_answers: list[int]\n    answer: int\n\n\nclass AnswerWithReasoningAndCalculations(BaseModel):\n    chain_of_thought: str\n    key_calculations: list[str]\n    potential_answers: list[int]\n    final_choice: int\n```\n\n----------------------------------------\n\nTITLE: Generating Story Outline with GPT-4o\nDESCRIPTION: Python function that uses Instructor and OpenAI to generate an initial story outline with setting, plot summary, choices, visual style, and image description. This establishes the foundation for the story before expanding individual branches.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/consistent-stories.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass GeneratedStory(BaseModel):\n    setting: str\n    plot_summary: str\n    choices: List[str]\n    visual_style: str\n    image_description: str\n\nasync def generate_story(\n    client: instructor.AsyncInstructor,\n    story_input: RestateStoryInput\n):\n    resp = await client.chat.completions.create(\n        messages=[{\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Generate a story with:\n            - Setting: {{ story_input.setting}}\n            - Title: {{ story_input.title }}\n\n            Rules:\n            - Generate 2-4 initial choices that represent actions\n            - Choices must move story forward\n            - Include brief setting description\n            - Generate a visual description for the story\n\n            Required Elements:\n            1. Plot Summary: A vivid description of the setting and plot\n            2. Initial Choices: 2-4 distinct actions the user can take\n            3. Visual Style: Description of art style, color palette\n            4. Image Description: One-sentence scene description\n            \"\"\"\n        }],\n        model=\"gpt-4o\",\n        response_model=GeneratedStory,\n        context={\"story_input\": story_input},\n    )\n    return resp\n```\n\n----------------------------------------\n\nTITLE: Nested Model Validation\nDESCRIPTION: Demonstrates nested model validation with Pydantic using Address and User models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/validation.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass Address(BaseModel):\n    \"\"\"Model representing a physical address.\"\"\"\n\n    street: str = Field(description=\"Street address including number\")\n    city: str = Field(description=\"City name\")\n    country: str = Field(description=\"Country name\")\n\n\nclass User(BaseModel):\n    \"\"\"Model representing a user with nested address validation.\"\"\"\n\n    name: str = Field(description=\"User's full name\")\n    addresses: List[Address] = Field(description=\"List of user's addresses\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Maybe Pattern\nDESCRIPTION: This snippet implements the Maybe pattern to encapsulate either a valid result or an error, useful for handling potentially missing data in LLM calls. The MaybeCharacter class includes a result field (optional Character), an error flag, and an optional error message.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n\n\nclass MaybeCharacter(BaseModel):\n    result: Optional[Character] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n```\n\n----------------------------------------\n\nTITLE: Graph Visualization with Graphviz\nDESCRIPTION: Creates a visual representation of entities and their relationships using Graphviz. Generates HTML-style labels for entities and creates a directed graph showing dependencies between entities.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/entity_resolution.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom graphviz import Digraph\n\ndef generate_html_label(entity: Entity) -> str:\n    rows = [\n        f\"<tr><td>{prop.key}</td><td>{prop.resolved_absolute_value}</td></tr>\"\n        for prop in entity.properties\n    ]\n    table_rows = \"\".join(rows)\n    return f\"<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>{entity.entity_title}</b></td></tr>{table_rows}</table>>\"\n\n\ndef generate_graph(data: DocumentExtraction):\n    dot = Digraph(comment=\"Entity Graph\", node_attr={\"shape\": \"plaintext\"})\n\n    for entity in data.entities:\n        label = generate_html_label(entity)\n        dot.node(str(entity.id), label)\n\n    for entity in data.entities:\n        for dep_id in entity.dependencies:\n            dot.edge(str(entity.id), str(dep_id))\n\n    dot.render(\"entity.gv\", view=True)\n```\n\n----------------------------------------\n\nTITLE: Synchronous Iterable User Extraction\nDESCRIPTION: Shows how to use OpenAI client with Instructor to extract multiple User objects using an Iterable response model. Demonstrates non-streaming implementation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/lists.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[User],\n    stream=False,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\n                         Correctly segment it into entitites\\\n                        Make sure the JSON is correct\",\n        },\n    ],\n)\nfor user in users:\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pydantic Model with LLM Validator and Logfire Integration\nDESCRIPTION: Code that creates a Pydantic model with Instructor's llm_validator to check for objectionable content. Logfire is configured to monitor both Pydantic validation and OpenAI API calls.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\nfrom pydantic import BaseModel\nfrom pydantic.functional_validators import AfterValidator\nfrom instructor import llm_validator\nimport logfire\nimport instructor\nfrom openai import OpenAI\n\nopenai_client = OpenAI()\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\"))\nlogfire.instrument_openai(openai_client)\nclient = instructor.from_openai(openai_client)\n\n\nclass Statement(BaseModel):\n    message: Annotated[\n        str,\n        AfterValidator(\n            llm_validator(\"Don't allow any objectionable content\", client=client)\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Person Model and Instructor Client with OpenAI\nDESCRIPTION: Shows how to define a Person model with name and age fields, initialize an Instructor-wrapped OpenAI client, and use it to extract structured data from text. The example demonstrates converting natural language text into a strongly-typed Person object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/contributing.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define your model\nclass Person(BaseModel):\n    name: str\n    age: int\n    \n# Create the patched client\nclient = instructor.from_openai(OpenAI())\n\n# Use the model\nperson = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Person,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: John Doe is 25 years old\"}\n    ]\n)\n\nprint(person.name)  # \"John Doe\"\nprint(person.age)   # 25\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor and Pydantic Integration\nDESCRIPTION: This snippet demonstrates how to set up an Instructor-enhanced OpenAI client that uses Pydantic models to structure responses. It creates a UserDetail class with name and age fields, along with an introduce method, and uses it to extract information from text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pydantic\nimport instructor\nfrom openai import OpenAI\n\n# Enables the response_model\nclient = instructor.from_openai(OpenAI())\n\n\nclass UserDetail(pydantic.BaseModel):\n    name: str\n    age: int\n\n    def introduce(self):\n        return f\"Hello I'm {self.name} and I'm {self.age} years old\"\n\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Nested Model Extraction with OpenAI\nDESCRIPTION: Example showing how to extract nested data structures using OpenAI client with Instructor\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import List\nimport os\nfrom openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: List[Address]\n\n# Initialize with API key\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# Enable instructor patches for OpenAI client\nclient = instructor.from_openai(client)\n# Create structured output with nested objects\nuser = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n            Extract: Jason is 25 years old.\n            He lives at 123 Main St, New York, USA\n            and has a summer house at 456 Beach Rd, Miami, USA\n        \"\"\"},\n    ],\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Pairwise LLM Judge with Sample Questions and Texts\nDESCRIPTION: Demonstrates how to test the judge_relevance function with a set of predefined question-text pairs and calculate a score based on the accuracy of the judgments.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/pairwise-llm-judge.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    test_pairs = [\n        {\n            \"question\": \"What are the main causes of climate change?\",\n            \"text\": \"Global warming is primarily caused by human activities, such as burning fossil fuels, deforestation, and industrial processes. These activities release greenhouse gases into the atmosphere, trapping heat and leading to a rise in global temperatures.\",\n            \"is_similar\": True,\n        },\n        # ... (other test pairs)\n    ]\n\n    score = 0\n    for pair in test_pairs:\n        result = judge_relevance(pair[\"question\"], pair[\"text\"])\n        if result.similarity == pair[\"is_similar\"]:\n            score += 1\n\n    print(f\"Score: {score}/{len(test_pairs)}\")\n    #> Score 9/10\n```\n\n----------------------------------------\n\nTITLE: Implementing Back Translation for Prompt Performance Improvement with Instructor and OpenAI\nDESCRIPTION: A comprehensive implementation of back translation to improve prompt performance by translating prompts to different languages and back to English. This technique creates prompt variations that can help overcome the sensitivity of Large Language Models to specific phrasings. The implementation uses the Instructor library and OpenAI's API with Pydantic models for structured responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/prompt_paraphrasing.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nimport random\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass TranslatedPrompt(BaseModel):\n    translation: str\n\n\nasync def translate_prompt(prompt: str, from_language: str, to_language: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                You are an expert translation assistant.\n                You are going to be given a prompt and\n                asked to translate it from {from_language}\n                to {to_language}. Paraphrase and use\n                synonyms where possible, especially for\n                the examples.\n                \"\"\",\n            },\n            {\"role\": \"user\", \"content\": f\"Prompt: {prompt}\"},\n        ],\n        response_model=TranslatedPrompt,\n    )\n\n\nasync def generate_permutation(prompt: str, language: str) -> str:\n    tranlated_prompt = await translate_prompt(prompt, \"english\", language)\n    backtranslated_prompt = await translate_prompt(\n        tranlated_prompt.translation, language, \"english\"\n    )\n    return backtranslated_prompt.translation\n\n\nasync def generate_prompts(\n    prompt: str, languages: list[str], permutations: int\n) -> list[str]:\n    coros = [\n        generate_permutation(prompt, random.choice(languages))\n        for _ in range(permutations)\n    ]\n    return await asyncio.gather(*coros)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    prompt = \"\"\"\n    You are an expert system that excels at Sentiment\n    Analysis of User Reviews.\n\n    Here are a few examples to refer to:\n\n    1. That was a fantastic experience I had! I'm\n    definitely recommending this to all my friends\n    // Positive\n    2. I think it was a passable evening. I don't think\n    there was anything remarkable or off-putting for me.\n    // Negative\n    3. I'm horrified at the state of affairs in this new\n    restaurant // Negative\n\n    Sentence: This was a fantastic experience!\n    \"\"\"\n    languages = [\"french\", \"spanish\", \"chinese\"]\n    permutations = 2\n\n    generated_prompts = asyncio.run(generate_prompts(prompt, languages, permutations))\n    for prompt in generated_prompts:\n        print(prompt)\n        \"\"\"\n        You are an expert system specializing in user review sentiment analysis. Here are a few examples to guide you: 1. It was an exceptional experience! I will definitely recommend it to all my friends // Positive 2. I think it was a mediocre evening. There wasn't anything outstanding or particularly bad for me // Negative 3. I am horrified by the condition of things in this new restaurant // Negative Sentence: It was an amazing experience!\n        \"\"\"\n        \"\"\"\n        You are an expert system that excels in User Review Sentiment Analysis.\n\n        Here are some reference examples:\n\n        1. I had an amazing experience! I will definitely recommend it to all my friends.\n        // Positive\n        2. I think it was an average evening. I don't believe there was anything remarkable or unpleasant about it for me.\n        // Negative\n        3. I am horrified by the situation at this new restaurant.\n        // Negative\n\n        Sentence: This was a fantastic experience!\n        \"\"\"\n        \"\"\"\n        You are an expert system skilled in conducting user\n        review sentiment analysis.\n\n        Here are some examples for reference:\n\n        1. That was an awesome experience! I'll definitely\n        recommend it to all my friends // Positive\n        2. I think it was an okay evening. I don't find\n        anything particularly outstanding or unpleasant.\n        // Neutral\n        3. I am very shocked by the condition of this new\n        restaurant // Negative\n\n        Sentence: This was a wonderful experience!\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading Images with Gemini AI for Structured Analysis\nDESCRIPTION: Demonstrates multiple ways to load images (URL, local file, base64) for analysis with the Gemini model. The code extracts structured image descriptions using a Pydantic model to capture objects, scene, and colors.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash\",\n    response_model=ImageDescription,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"What is in this image?\",\n                # Option 1: Direct URL with autodetection\n                Image.from_url(url),\n                # Option 2: Local file\n                # Image.from_path(\"path/to/local/image.jpg\")\n                # Option 3: Base64 string\n                # Image.from_base64(\"base64_encoded_string_here\")\n                # Option 4: Autodetect\n                # Image.autodetect(<url|path|base64>)\n            ],\n        },\n    ],\n)\n\nprint(response)\n# Example output:\n# ImageDescription(\n#     objects=['blueberries', 'leaves'],\n#     scene='A blueberry bush with clusters of ripe blueberries and some unripe ones against a cloudy sky',\n#     colors=['green', 'blue', 'purple', 'white']\n# )\n```\n\n----------------------------------------\n\nTITLE: Returning Original Completion with Instructor\nDESCRIPTION: Shows how to use the `create_with_completion` method to return both the structured User object and the original LLM completion\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Optional Nested Fields with Instructor\nDESCRIPTION: This code snippet shows how to handle optional nested fields in Instructor. It defines models for SocialMedia, ContactInfo, and Person, where some fields are marked as Optional to handle missing data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/nested_structure.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\nfrom typing import Optional\n\nclient = instructor.from_openai(OpenAI())\n\nclass SocialMedia(BaseModel):\n    twitter: Optional[str] = None\n    linkedin: Optional[str] = None\n    instagram: Optional[str] = None\n\nclass ContactInfo(BaseModel):\n    email: str\n    phone: Optional[str] = None\n    social: Optional[SocialMedia] = None  # Optional nested structure\n\nclass Person(BaseModel):\n    name: str\n    contact: ContactInfo\n```\n\n----------------------------------------\n\nTITLE: Nested Models with Azure OpenAI and Instructor\nDESCRIPTION: This example shows how to handle complex nested structures using Azure OpenAI and instructor for structured output generation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/azure.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport instructor\nfrom openai import AzureOpenAI\nfrom pydantic import BaseModel\n\nclient = AzureOpenAI(\n    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n    api_version=\"2024-02-01\",\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n)\nclient = instructor.from_openai(client)\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass UserWithAddress(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-mini\",  # Your deployment name\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n        John is 30 years old and has two addresses:\n        1. 123 Main St, New York, USA\n        2. 456 High St, London, UK\n        \"\"\",\n        }\n    ],\n    response_model=UserWithAddress,\n)\n\nprint(resp)\n# {\n#     'name': 'John',\n#     'age': 30,\n#     'addresses': [\n#         {\n#             'street': '123 Main St',\n#             'city': 'New York',\n#             'country': 'USA'\n#         },\n#         {\n#             'street': '456 High St',\n#             'city': 'London',\n#             'country': 'UK'\n#         }\n#     ]\n# }\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Query Expansion\nDESCRIPTION: Creates a function to expand user queries with temporal context using OpenAI's GPT-3.5 model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef expand_query(q) -> Query:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Query,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n            },\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n        ],\n    )\n\nquery = expand_query(\"What are some recent developments in AI?\")\nquery\n```\n\n----------------------------------------\n\nTITLE: Synchronous Groq AI Integration\nDESCRIPTION: Example of using Groq AI with instructor for synchronous structured output generation using a simple User model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/groq.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom groq import Groq\nimport instructor\nfrom pydantic import BaseModel\n\n# Initialize with API key\nclient = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n\n# Enable instructor patches for Groq client\nclient = instructor.from_groq(client)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Create structured output\nuser = client.chat.completions.create(\n    model=\"llama3-groq-70b-8192-tool-use-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n    ],\n    response_model=User,\n)\n\nprint(user)\n# > User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Models for Tag Classification\nDESCRIPTION: Implements the core data models using Pydantic for tag validation and request/response handling. Includes validation logic to prevent hallucination of invalid tags.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, ValidationInfo, model_validator\n\n\nclass Tag(BaseModel):\n    id: int\n    name: str\n\n    @model_validator(mode=\"after\")\n    def validate_ids(self, info: ValidationInfo):\n        context = info.context\n        if context:\n            tags: List[Tag] = context.get(\"tags\")\n            assert self.id in {\n                tag.id for tag in tags\n            }, f\"Tag ID {self.id} not found in context\"\n            assert self.name in {\n                tag.name for tag in tags\n            }, f\"Tag name {self.name} not found in context\"\n        return self\n\n\nclass TagWithInstructions(Tag):\n    instructions: str\n\n\nclass TagRequest(BaseModel):\n    texts: List[str]\n    tags: List[TagWithInstructions]\n\n\nclass TagResponse(BaseModel):\n    texts: List[str]\n    predictions: List[Tag]\n```\n\n----------------------------------------\n\nTITLE: Basic AWS Bedrock Integration with Pydantic\nDESCRIPTION: Demonstrates basic setup and usage of AWS Bedrock with Instructor for structured output generation using a simple User model\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/bedrock.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\nimport instructor\nfrom pydantic import BaseModel\n\n# Initialize the Bedrock client\nbedrock_client = boto3.client('bedrock-runtime')\n\n# Enable instructor patches for Bedrock client\nclient = instructor.from_bedrock(bedrock_client)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Create structured output\nuser = client.converse(\n    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n    ],\n    response_model=User,\n)\n\nprint(user)\n# > User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: PDF Analysis with Google GenAI and Pre-uploaded File\nDESCRIPTION: Demonstrates PDF analysis using Google's Gemini model with a pre-uploaded file. Shows how to handle file uploads separately and use existing files for analysis.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/multimodal.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai import Client\nimport instructor\nfrom pydantic import BaseModel\nfrom instructor.multimodal import PDFWithGenaiFile\nimport requests\n\n# Set up the client\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\nclient = instructor.from_genai(Client())\n\nwith requests.get(url) as response:\n    pdf_data = response.content\n    with open(\"./invoice.pdf\", \"wb\") as f:\n        f.write(pdf_data)\n\nfile = client.files.upload(\n    file=\"invoice.pdf\",\n)\n\n\n# Create a model for analyzing PDFs\nclass Invoice(BaseModel):\n    total: float\n    items: list[str]\n\n\n# Load and analyze a PDF\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash\",\n    response_model=Invoice,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Analyze this document\",\n                PDFWithGenaiFile.from_existing_genai_file(file_name=file.name),\n            ],\n        }\n    ],\n)\n\nprint(response)\n# > Total = 220, items = ['English Tea', 'Tofu']\n```\n\n----------------------------------------\n\nTITLE: Adding Chain of Thought to Reusable Components in Pydantic Models\nDESCRIPTION: This code snippet shows how to add a 'chain of thought' field within a reusable component in Pydantic models. It helps in understanding or optimizing time range allocations by providing step-by-step reasoning.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass TimeRange(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Step by step reasoning to get the correct time range\"\n    )\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n```\n\n----------------------------------------\n\nTITLE: UserProfile Model with Pre-validation Hooks\nDESCRIPTION: Shows pre-validation data transformation using Pydantic validators.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/validation.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, validator\n\n\nclass UserProfile(BaseModel):\n    \"\"\"Model representing a user profile with pre-validation transformation.\"\"\"\n\n    username: str = Field(description=\"User's unique username\")\n\n    @validator('username', pre=True)\n    def lowercase_username(cls, v):\n        \"\"\"Transform username to lowercase before validation.\"\"\"\n        return v.lower()\n```\n\n----------------------------------------\n\nTITLE: Personal Style Response Model Definition\nDESCRIPTION: Defines a Pydantic model for validating extracted style metadata against the taxonomy with custom validation logic.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extracting-model-metadata.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass PersonalStyle(BaseModel):\n    \"\"\"\n    Ideally you map this to a specific taxonomy\n    \"\"\"\n\n    categories: list[str]\n    subcategories: list[str]\n    product_types: list[str]\n    colors: list[str]\n\n    @model_validator(mode=\"after\")\n    def validate_options(self, info: ValidationInfo):\n        context = info.context\n        colors = context[\"colors\"]\n        categories = context[\"categories\"]\n        subcategories = context[\"subcategories\"]\n        product_types = context[\"product_types\"]\n\n        # Validate colors\n        for color in self.colors:\n            if color not in colors:\n                raise ValueError(\n                    f\"Color {color} is not in the taxonomy. Valid colors are {colors}\"\n                )\n        for category in self.categories:\n            if category not in categories:\n                raise ValueError(\n                    f\"Category {category} is not in the taxonomy. Valid categories are {categories}\"\n                )\n\n        for subcategory in self.subcategories:\n            if subcategory not in subcategories:\n                raise ValueError(\n                    f\"Subcategory {subcategory} is not in the taxonomy. Valid subcategories are {subcategories}\"\n                )\n\n        for product_type in self.product_types:\n            if product_type not in product_types:\n                raise ValueError(\n                    f\"Product type {product_type} is not in the taxonomy. Valid product types are {product_types}\"\n                )\n\n        return self\n```\n\n----------------------------------------\n\nTITLE: Patching LLM Providers with Instructor\nDESCRIPTION: Examples of connecting Instructor to different LLM providers like OpenAI and Anthropic\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/start-here.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# For OpenAI\nclient = instructor.from_openai(OpenAI())\n\n# For Anthropic\nclient = instructor.from_anthropic(Anthropic())\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced Query Expansion with Chain of Thought\nDESCRIPTION: Enhanced query expansion with chain-of-thought reasoning for more precise date range selection and query specificity.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\ndef expand_query(q) -> Query:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=Query,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n            },\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n        ],\n    )\n\nexpand_query(\"What are some recent developments in AI?\")\n```\n\n----------------------------------------\n\nTITLE: Structured Output with llama-cpp-python\nDESCRIPTION: Integration of llama-cpp-python with Instructor for local model inference using JSON schema and speculative decoding\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/open_source.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport llama_cpp\nimport instructor\nfrom llama_cpp.llama_speculative import LlamaPromptLookupDecoding\nfrom pydantic import BaseModel\n\n\nllama = llama_cpp.Llama(\n    model_path=\"../../models/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q4_K_M.gguf\",\n    n_gpu_layers=-1,\n    chat_format=\"chatml\",\n    n_ctx=2048,\n    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=2),\n    logits_all=True,\n    verbose=False,\n)\n\n\ncreate = instructor.patch(\n    create=llama.create_chat_completion_openai_v1,\n    mode=instructor.Mode.JSON_SCHEMA,\n)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract `Jason is 30 years old`\",\n        }\n    ],\n    response_model=UserDetail,\n)\n\nprint(user)\n#> name='Jason' age=30\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Models for Question Decomposition in Python\nDESCRIPTION: This snippet defines two Pydantic models: Question and QueryPlan. The Question model represents individual queries with unique IDs and subquestions, while QueryPlan structures the overall decomposition plan including the root question and its subquestions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass Question(BaseModel):\n    id: int = Field(..., description=\"A unique identifier for the question\")\n    query: str = Field(..., description=\"The question decomposed as much as possible\")\n    subquestions: list[int] = Field(\n        default_factory=list,\n        description=\"The subquestions that this question is composed of\",\n    )\n\n\nclass QueryPlan(BaseModel):\n    root_question: str = Field(..., description=\"The root question that the user asked\")\n    plan: list[Question] = Field(\n        ..., description=\"The plan to answer the root question and its subquestions\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Chain of Thought Validation Implementation\nDESCRIPTION: Complex validation example that checks if an answer follows the chain of thought reasoning using model validators.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\n            },\n        ],\n        # this comes from client = instructor.from_openai(OpenAI())\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n```\n\n----------------------------------------\n\nTITLE: Defining Relationships Between Entities in Pydantic Models\nDESCRIPTION: This code demonstrates how to define relationships between entities using Pydantic. It creates a model for users with a list of friend IDs, and a container model that holds multiple user details.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    friends: List[int]\n\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail]\n```\n\n----------------------------------------\n\nTITLE: Optional Fields in Pydantic Models\nDESCRIPTION: Demonstrates how to handle optional fields in data extraction using Optional type hints and default values. This allows for flexible data extraction where some fields might be missing.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/simple_object.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass MovieReview(BaseModel):\n    title: str\n    director: Optional[str] = None  # Optional field\n    rating: float\n```\n\n----------------------------------------\n\nTITLE: Basic Instructor Integration with OpenAI\nDESCRIPTION: Demonstrates the minimal setup required to use Instructor with OpenAI client for extracting structured data from completions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/why.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Patch the OpenAI client with Instructor\nclient = instructor.from_openai(OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n# Function to extract user details\ndef extract_user() -> UserDetail:\n    user = client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n        ]\n    )\n    return user\n```\n\n----------------------------------------\n\nTITLE: Streaming Iterable Collections with Vertex AI\nDESCRIPTION: Implementation of streaming iterable collections using Instructor with Vertex AI. Shows how to handle continuous streaming of multiple model instances.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/vertex.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nimport instructor\nfrom pydantic import BaseModel\n\nvertexai.init()\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-2.0-flash\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n)\n\n# Stream iterable responses\nresponse_stream = client.chat.completions.create_iterable(\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Make up two people\"},\n    ],\n)\n\nfor user in response_stream:\n    print(f\"Generated user: {user}\")\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Objects with Instructor and OpenAI\nDESCRIPTION: Illustrates how to stream partial objects using the create_partial method. This approach allows for handling incomplete data as it's being generated, useful for real-time processing or displaying incremental results.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser_stream = client.chat.completions.create_partial(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n\nfor user in user_stream:\n    print(user)\n    #> name=None age=None\n    #> name=None age=None\n    #> name=None age=None\n    #> name=None age=None\n    #> name=None age=None\n    #> name=None age=None\n    #> name='John Doe' age=None\n    #> name='John Doe' age=None\n    #> name='John Doe' age=None\n    #> name='John Doe' age=30\n    #> name='John Doe' age=30\n    # name=None age=None\n    # name='' age=None\n    # name='John' age=None\n    # name='John Doe' age=None\n    # name='John Doe' age=30\n```\n\n----------------------------------------\n\nTITLE: OpenAI Function Calling Implementation\nDESCRIPTION: Example of using OpenAI's function calling feature with Pydantic models for structured output.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/1-introduction.ipynb#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nclass PersonBirthday(BaseModel):\n    name: str\n    age: int\n    birthday: datetime.date\n\nschema = {\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"age\": {\"type\": \"integer\"},\n        \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"},\n    },\n    \"required\": [\"name\", \"age\"],\n    \"type\": \"object\",\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Knowledge Graphs with Graphviz\nDESCRIPTION: Creates a function to visualize knowledge graphs using the Graphviz library. The function takes a KnowledgeGraph object and generates a visual representation saved as a file.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/knowledge_graph.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom graphviz import Digraph\n\ndef visualize_knowledge_graph(kg: KnowledgeGraph):\n    dot = Digraph(comment=\"Knowledge Graph\")\n\n    # Add nodes\n    for node in kg.nodes:\n        dot.node(str(node.id), node.label, color=node.color)\n\n    # Add edges\n    for edge in kg.edges:\n        dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n    # Render the graph\n    dot.render(\"knowledge_graph.gv\", view=True)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Token Usage in Pydantic\nDESCRIPTION: Demonstrates how to optimize token usage by disabling Pydantic's error URLs in validation messages using a helper function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/reask_validation.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.utils import disable_pydantic_error_url\nfrom pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndisable_pydantic_error_url()\n\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Expanding Queries Asynchronously in Python\nDESCRIPTION: The async function expand_query utilizes an OpenAI client to create a chat completion based on a user query. It takes a query and model parameters to interact with a language model, returning a structured Query object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# We'll use a different client for async calls\n# To highlight the difference and how we can use both\naclient = instructor.patch(AsyncOpenAI())\n\n\nasync def expand_query(\n    q, *, model: str = \"gpt-4-1106-preview\", temp: float = 0\n) -> Query:\n    return await aclient.chat.completions.create(\n        model=model,\n        temperature=temp,\n        response_model=Query,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n            },\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n        ],\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Nested Data Structure Extraction with Gemini\nDESCRIPTION: Shows how to extract complex nested data structures using Instructor with Gemini model. Includes nested Address model within User model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/google.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n)\n\nuser = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract: Jason is 25 years old.\n            He lives at 123 Main St, New York, USA\n            and has a summer house at 456 Beach Rd, Miami, USA\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n\nprint(user)\n```\n\n----------------------------------------\n\nTITLE: Setting Default Values in Pydantic Models\nDESCRIPTION: Demonstrates how to set default values for fields in a Pydantic model using the Field function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/fields.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(default='John Doe')\n\n\nuser = User()\nprint(user)\n#> name='John Doe'\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Schema Parser with Instructor\nDESCRIPTION: Shows how to use the recursive schema with Instructor and OpenAI to parse text into hierarchical structures. Includes example usage with a company organization structure.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/recursive.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef parse_hierarchy(text: str) -> RecursiveNode:\n    \"\"\"Parse text into a hierarchical structure.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at parsing text into hierarchical structures.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Parse this text into a hierarchical structure: {text}\",\n            },\n        ],\n        response_model=RecursiveNode,\n    )\n\n\n# Example usage\nhierarchy = parse_hierarchy(\n    \"\"\"\nCompany: Acme Corp\n- Department: Engineering\n  - Team: Frontend\n    - Project: Website Redesign\n    - Project: Mobile App\n  - Team: Backend\n    - Project: API v2\n    - Project: Database Migration\n- Department: Marketing\n  - Team: Digital\n    - Project: Social Media Campaign\n  - Team: Brand\n    - Project: Logo Refresh\n\"\"\"\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Taxonomy from YAML\nDESCRIPTION: Code to read and parse the taxonomy from a YAML file and create sets of categories, subcategories, and product types.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extracting-model-metadata.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\n\nwith open(\"taxonomy.yml\", \"r\") as file:\n    taxonomy = yaml.safe_load(file)\n\ncolors = taxonomy[\"colors\"]\ncategories = set(taxonomy.keys())\ncategories.remove(\"colors\")\n\nsubcategories = set()\nproduct_types = set()\nfor category in categories:\n    for subcategory in taxonomy[category].keys():\n        subcategories.add(subcategory)\n        for product_type in taxonomy[category][subcategory]:\n            product_types.add(product_type)\n```\n\n----------------------------------------\n\nTITLE: Defining Time Filter Model with Pydantic\nDESCRIPTION: Creates a Pydantic model to represent time ranges with optional start and end dates for filtering data in RAG systems.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-timelines.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom typing import Optional\nfrom pydantic import BaseModel\n\n\nclass TimeFilter(BaseModel):\n    start_date: Optional[datetime] = None\n    end_date: Optional[datetime] = None\n```\n\n----------------------------------------\n\nTITLE: OpenAI Integration Example\nDESCRIPTION: Demonstrates basic OpenAI integration with Instructor library for user information extraction using a Pydantic model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the user from the string belo - Chris is a 27 year old engineer in San Francisco\",\n        }\n    ],\n    max_tokens=100,\n)\n\nprint(resp)\n#> name='Chris' age=27\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Answer Models in Python\nDESCRIPTION: Basic response models showing the difference between a model with chain of thought reasoning and one without. Shows the structure for capturing both the reasoning process and final answer.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/bad-schemas-could-break-llms.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Answer(BaseModel):\n    chain_of_thought: str\n    answer: int\n\n\nclass OnlyAnswer(BaseModel):\n    answer: int\n```\n\n----------------------------------------\n\nTITLE: Calculating Entity Density in Python\nDESCRIPTION: This Python code defines a function `calculate_entity_density` that calculates the entity density of a given sentence. It utilizes `nltk.word_tokenize` to tokenize the sentence and `spaCy` to identify named entities, then calculates the density as the ratio of entities to tokens.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ndef calculate_entity_density(sentence: str):\n    tokens = nltk.word_tokenize(sentence)\n    entities = nlp(sentence).ents\n    entity_density = round(len(entities) / len(tokens), 3)\n\n    return len(tokens), len(entities), entity_density\n```\n\n----------------------------------------\n\nTITLE: Defining Transcript Elements with Pydantic Models in Python\nDESCRIPTION: This code defines Pydantic models for representing tasks, participants, and assignments in a transcript. It includes classes for Task, Participant, Assignment, and Transcript, which can be used to structure extracted information from a transcript.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass Task(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Participant(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Assignment(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Transcript(BaseModel):\n    tasks: List[Task]\n    participants: List[Participant]\n    assignments: List[Assignment]\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Field Constraints in Python with Instructor\nDESCRIPTION: Demonstrates how to use Pydantic's Field function to add basic constraints to fields when extracting structured data. It includes examples of min/max length, age range, and email pattern validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/field_validation.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass User(BaseModel):\n    name: str = Field(..., min_length=2, max_length=50)\n    age: int = Field(..., ge=0, le=120)  # greater than or equal to 0, less than or equal to 120\n    email: str = Field(..., pattern=r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n\n# Extract with validation\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"I'm John Smith, 35 years old, with email john@example.com\"}\n    ],\n    response_model=User\n)\n```\n\n----------------------------------------\n\nTITLE: OpenAI Mode Usage Examples\nDESCRIPTION: Demonstrates different OpenAI mode configurations including TOOLS for general use, JSON for simple extractions, and PARALLEL_TOOLS for complex processes.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Best for most OpenAI use cases\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n\n# For very simple extractions\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.JSON)\n\n# For complex, multi-step processes\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous User Data Extraction with Fireworks\nDESCRIPTION: Shows how to use Instructor with Fireworks AI asynchronously for structured data extraction\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/fireworks.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom fireworks.client import AsyncFireworks\nimport instructor\nfrom pydantic import BaseModel\nimport asyncio\n\n# Initialize async client\nclient = AsyncFireworks()\n\n# Enable instructor patches\nclient = instructor.from_fireworks(client)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_user():\n    user = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract: Jason is 25 years old\",\n            }\n        ],\n        model=\"accounts/fireworks/models/llama-v3-8b-instruct\",\n        response_model=User,\n    )\n    return user\n\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)  # User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Segmenting Search Queries using OpenAI Function Call in Python\nDESCRIPTION: This function uses the OpenAI client to segment a given search query into multiple Search objects. It demonstrates how to use the response_model parameter with Iterable[Search] to generate structured output from the language model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/search.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef segment(data: str) -> Search:\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=Iterable[Search],\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Consider the data below: '\\n{data}' and segment it into multiple search queries\",\n            },\n        ],\n        max_tokens=1000,\n    )\n```\n\n----------------------------------------\n\nTITLE: Non-Streaming Product Recommendations with Instructor AI in Python\nDESCRIPTION: This snippet shows how to get product recommendations without streaming using Instructor AI. It uses the same prompt and model as the streaming version but sets stream to False.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generator.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstart_perf = time.perf_counter()\nrecommendations_list = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=False,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nprint(recommendations_list[0])\nend_perf = time.perf_counter()\nprint(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n```\n\n----------------------------------------\n\nTITLE: Defining Response Model with Custom Validators\nDESCRIPTION: Demonstrates how to create a Pydantic model with field validators that enforce specific formatting rules like uppercase names.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/reask_validation.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n```\n\n----------------------------------------\n\nTITLE: Defining Relationships Between Entities in Pydantic Models\nDESCRIPTION: This code snippet shows how to define relationships between entities in Pydantic models. It demonstrates how to incorporate an id and a friends field to represent relationships between users.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    friends: List[int] = Field(\n        ...,\n        description=\"Correct and complete list of friend IDs, representing relationships between users.\",\n    )\n\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail] = Field(\n        ...,\n        description=\"Collection of users, correctly capturing the relationships among them.\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Chunk ID and Retrieving Question Details in Python\nDESCRIPTION: This snippet demonstrates how to generate a unique chunk ID using MD5 hash and retrieve question details. It uses the hashlib library to create a hash of the chunk text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport hashlib\n\nsample_question, chunk = questions[0]\n\nchunk_id = hashlib.md5(chunk.encode()).hexdigest()\nchunk_id, sample_question.question, chunk\n```\n\n----------------------------------------\n\nTITLE: Structured Output with Groq\nDESCRIPTION: Implementation of structured output extraction using Groq's platform with Instructor integration\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/open_source.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pydantic import BaseModel\n\nimport groq\nimport instructor\n\n\nclient = groq.Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\n# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.create methods\n# to support the response_model parameter\nclient = instructor.from_openai(client, mode=instructor.Mode.MD_JSON)\n\n\n# Now, we can use the response_model parameter using only a base model\n# rather than having to use the OpenAISchema class\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"mixtral-8x7b-32768\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserExtract), \"Should be instance of UserExtract\"\n\nprint(user)\n#> name='jason' age=25\n```\n\n----------------------------------------\n\nTITLE: Extracting Receipt Data Using GPT-4\nDESCRIPTION: Implements the receipt data extraction function using OpenAI's GPT-4 model. Processes image URLs and returns structured receipt data using the defined Receipt model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extracting_receipts.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef extract(url: str) -> Receipt:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        max_tokens=4000,\n        response_model=Receipt,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": url},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Analyze the image and return the items in the receipt and the total amount.\",\n                    },\n                ],\n            }\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Streaming Iterables of Structured Objects with Instructor\nDESCRIPTION: Shows how to use the create_iterable method to stream multiple structured objects. This is useful when extracting a collection of items from the language model's output, allowing for processing each item as it becomes available.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create_iterable(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create 2 users\"},\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n    #> name='John Doe' age=30\n    #> name='Jane Doe' age=28\n    # User(name='John Doe', age=30)\n    # User(name='Jane Smith', age=25)\n```\n\n----------------------------------------\n\nTITLE: Defining Query Planning Structures with Pydantic\nDESCRIPTION: Defines the core data structures for query planning using Pydantic models. Includes Query class for individual questions and QueryPlan class for managing the query graph with dependencies.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/planning-tasks.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Literal\nfrom pydantic import Field, BaseModel\n\n\nclass Query(BaseModel):\n    \"\"\"Class representing a single question in a query plan.\"\"\"\n\n    id: int = Field(..., description=\"Unique id of the query\")\n    question: str = Field(\n        ...,\n        description=\"Question asked using a question answering system\",\n    )\n    dependencies: List[int] = Field(\n        default_factory=list,\n        description=\"List of sub questions that need to be answered before asking this question\",\n    )\n    node_type: Literal[\"SINGLE\", \"MERGE_MULTIPLE_RESPONSES\"] = Field(\n        default=\"SINGLE\",\n        description=\"Type of question, either a single question or a multi-question merge\",\n    )\n\n\nclass QueryPlan(BaseModel):\n    \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\"\n\n    query_graph: List[Query] = Field(\n        ..., description=\"The query graph representing the plan\"\n    )\n\n    def _dependencies(self, ids: List[int]) -> List[Query]:\n        \"\"\"Returns the dependencies of a query given their ids.\"\"\"\n        return [q for q in self.query_graph if q.id in ids]\n```\n\n----------------------------------------\n\nTITLE: Defining Multi-Label Classification Structure with Pydantic\nDESCRIPTION: This snippet defines a Pydantic model for multi-label classification of support tickets. It uses a List of Literals for class labels and includes few-shot examples in the model's docstring to guide the classification process.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/classification.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\n\nclass MultiClassPrediction(BaseModel):\n    \"\"\"\n    Class for a multi-class label prediction.\n\n    Examples:\n    - \"My account is locked\": [\"TECH_ISSUE\"]\n    - \"I can't access my billing info\": [\"TECH_ISSUE\", \"BILLING\"]\n    - \"When do you close for holidays?\": [\"GENERAL_QUERY\"]\n    - \"My payment didn't go through and now I can't log in\": [\"BILLING\", \"TECH_ISSUE\"]\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        ...,\n        description=\"The chain of thought that led to the prediction.\",\n    )\n\n    class_labels: List[Literal[\"TECH_ISSUE\", \"BILLING\", \"GENERAL_QUERY\"]] = Field(\n        ...,\n        description=\"The predicted class labels for the support ticket.\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Demonstrating AIResponse Usage with Chain-of-Thought Validation in Python\nDESCRIPTION: This code snippet shows how to use the AIResponse model, which would trigger the chain-of-thought validation when instantiated.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nAIResponse(\n    chain_of_thought=\"The user suffers from diabetes.\",\n    answer=\"The user has a broken leg.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data from LLM Using Instructor and Pydantic\nDESCRIPTION: This code demonstrates how to use Instructor to extract structured data from an LLM response. It defines a Pydantic model for user data, patches the OpenAI client with Instructor, and extracts name and age information from text into a typed User object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/best_framework.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport instructor\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,  # (1)!\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the user's name and age from this: John is 25 years old\",\n        }\n    ],\n)\n\nprint(user)  # (2)!\n#> User(name='John', age=25)\n```\n\n----------------------------------------\n\nTITLE: Implementing Optional Nested Structures\nDESCRIPTION: Shows how to create and work with optional nested data structures in Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/optional_fields.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n\nclass Contact(BaseModel):\n    email: str\n    phone: Optional[str] = None\n    address: Optional[Address] = None  # Optional nested structure\n\nclass Person(BaseModel):\n    name: str\n    contact: Contact\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Type for Markdown Table Conversion in Python\nDESCRIPTION: Creates a custom Pydantic type (MarkdownDataFrame) that can parse markdown tables into pandas DataFrames. It uses Annotated with validators to handle the conversion from markdown string to DataFrame and serialization back to markdown.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/tidy-data-from-messy-tables.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nimport pandas as pd\n\n\ndef md_to_df(data: Any) -> Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],\n    BeforeValidator(md_to_df),\n    PlainSerializer(lambda df: df.to_markdown()),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be separate\",\n        }\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Iterables and List Processing\nDESCRIPTION: Shows how to process streamed data as iterables using Instructor's list handling capabilities.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/why.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterable\n\nUsers = Iterable[User]\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Users,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Consider the data below:\\n{input}\"\n                \"Correctly segment it into entitites\"\n                \"Make sure the JSON is correct\"\n            ),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    assert isinstance(user, User)\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Using Enumerations for Validation in Python with Instructor\nDESCRIPTION: Shows how to use Enums to validate fields against a predefined set of values. This example includes enums for task status and priority.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/field_validation.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Status(str, Enum):\n    PENDING = \"pending\"\n    APPROVED = \"approved\"\n    REJECTED = \"rejected\"\n\nclass Priority(str, Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n\nclass Task(BaseModel):\n    title: str\n    description: str\n    status: Status  # Must be one of the enum values\n    priority: Priority  # Must be one of the enum values\n\n# Extract with enum validation\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Task: Update website, Description: Refresh content on homepage, Status: pending, Priority: high\"}\n    ],\n    response_model=Task\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI client with Instructor\nDESCRIPTION: This snippet initializes the OpenAI client and patches it with Instructor to enable response model usage. It also defines a House enum and a Character class with the house attribute as the enum. The Character data is extracted using the `chat.completions.create` method and the response is formatted to JSON.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import Literal\n\n\nclient = instructor.patch(OpenAI())\n\n\n# Tip: Do not use auto() as they cast to 1,2,3,4\nclass House(Enum):\n    Gryffindor = \"gryffindor\"\n    Hufflepuff = \"hufflepuff\"\n    Ravenclaw = \"ravenclaw\"\n    Slytherin = \"slytherin\"\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: House\n\n    def say_hello(self):\n        print(\n            f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\"\n        )\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n```\n\n----------------------------------------\n\nTITLE: Implementing Execute Subcommand for Instructor-AI CLI in Python\nDESCRIPTION: This snippet defines the 'execute' subcommand for the Instructor-AI CLI. It takes a task description as an argument and executes the specified task.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/jobs.md#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n@cli.command()\n@click.argument('task')\ndef execute(task):\n    \"\"\"Execute a task.\"\"\"\n    click.echo(f\"Executing task: {task}\")\n    # Add task execution logic here\n```\n\n----------------------------------------\n\nTITLE: Streaming Iterables with Instructor\nDESCRIPTION: Shows how to use the `create_iterable` method to generate and stream multiple structured objects\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create_iterable(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create 2 users\"},\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Calculating response cost using Instructor with LiteLLM\nDESCRIPTION: Python code demonstrating how to calculate the cost of a response using Instructor with LiteLLM. It creates a structured output and accesses the response cost from the raw completion object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/litellm.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom litellm import completion\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_litellm(completion)\ninstructor_resp, raw_completion = client.chat.completions.create_with_completion(\n    model=\"claude-3-5-sonnet-20240620\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(raw_completion._hidden_params[\"response_cost\"])\n#> 0.00189\n```\n\n----------------------------------------\n\nTITLE: Multiple Response Types with Discriminated Unions\nDESCRIPTION: Implements success and error response types using discriminated unions with Literal types.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/unions.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union, Literal\nfrom pydantic import BaseModel\n\n\nclass SuccessResponse(BaseModel):\n    status: Literal[\"success\"]\n    data: dict\n\n\nclass ErrorResponse(BaseModel):\n    status: Literal[\"error\"]\n    message: str\n\n\nResponse = Union[SuccessResponse, ErrorResponse]\n```\n\n----------------------------------------\n\nTITLE: Calculating entity density for a startup acquisition sentence in Python\nDESCRIPTION: This snippet applies the `calculate_entity_density` function to a sentence about Apple acquiring a UK startup. It showcases how the function computes the number of tokens, the number of entities and the entity density providing an insight into the sentence's information density.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsentence_2 = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ncalculate_entity_density(sentence_2)\n```\n\n----------------------------------------\n\nTITLE: Processing GSM8k Dataset for LLM Benchmarking in Python\nDESCRIPTION: This code loads the GSM8k dataset, processes it by separating reasoning steps from answers, and creates a clean version for benchmarking LLM performance. It extracts the question, numerical answer, and reasoning components from each problem and pushes the processed dataset to HuggingFace Hub.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/bad-schemas-could-break-llms.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset, Dataset, DatasetDict\n\nsplits = [\"test\", \"train\"]\n\n\ndef generate_gsm8k(split):\n    ds = load_dataset(\"gsm8k\", \"main\", split=split, streaming=True)\n    for row in ds:\n        reasoning, answer = row[\"answer\"].split(\"####\")\n        answer = int(answer.strip().replace(\",\", \"\"))\n        yield {\n            \"question\": row[\"question\"],\n            \"answer\": answer,\n            \"reasoning\": reasoning,\n        }\n\n\n# Create the dataset for train and test splits\ntrain_dataset = Dataset.from_generator(lambda: generate_gsm8k(\"train\"))\ntest_dataset = Dataset.from_generator(lambda: generate_gsm8k(\"test\"))\n\n# Combine them into a DatasetDict\ndataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n\ndataset.push_to_hub(\"567-labs/gsm8k\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Batch Job for Synthetic Data Generation in Python\nDESCRIPTION: This snippet shows how to use the BatchJob object from the 'instructor' library to create a .jsonl file compatible with OpenAI's Batch API. It processes a dataset of text passages and generates messages for each passage.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/batch_job_oai.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom instructor.batch import BatchJob\nfrom pydantic import BaseModel, Field\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\", streaming=True).take(200)\n\n\ndef get_messages(dataset):\n    for row in dataset:\n        for passage in row['passages']['passage_text']:\n            yield [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the text chunk: {passage}\"},\n            ]\n\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(description=\"The generated question from the text chunk.\")\n    answer: str = Field(description=\"The answer to the generated question.\")\n\n\nBatchJob.create_from_messages(\n    messages_batch=get_messages(dataset),\n    model=\"gpt-4o\",\n    file_path=\"./test.jsonl\",\n    response_model=QuestionAnswerPair,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Search Query Structure with Pydantic\nDESCRIPTION: Example of a Pydantic model for structured search queries, demonstrating type validation, optional fields, and literal type constraints.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/pydantic-is-still-all-you-need.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Search(BaseModel):\n    query: str\n    start_date: Optional[datetime]\n    end_date: Optional[datetime]\n    limit: Optional[int]\n    source: Literal[\"news\", \"social\", \"blog\"]\n```\n\n----------------------------------------\n\nTITLE: Defining RewrittenSummary Pydantic Model\nDESCRIPTION: Defines a Pydantic model for handling rewritten summaries with fields for the summary text, absent entities, and missing entities. Includes detailed documentation about guidelines for summary generation and entity handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chain-of-density.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: List[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: List[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Custom Field Validators for Product Data in Python\nDESCRIPTION: Implements custom field validators using Pydantic's field_validator decorator for product data validation, including SKU format and name formatting rules.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/field_level_validation.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, field_validator\nimport instructor\nfrom openai import OpenAI\nimport re\n\nclient = instructor.from_openai(OpenAI())\n\nclass Product(BaseModel):\n    name: str\n    sku: str\n    price: float\n    \n    @field_validator('name')\n    @classmethod\n    def validate_name(cls, v):\n        if len(v.strip()) < 3:\n            raise ValueError(\"Product name must be at least 3 characters long\")\n        return v.strip().title()  # Clean up and format\n    \n    @field_validator('sku')\n    @classmethod\n    def validate_sku(cls, v):\n        pattern = r'^[A-Z]{3}-\\d{4}$'\n        if not re.match(pattern, v):\n            raise ValueError(\"SKU must be in format XXX-0000 (3 uppercase letters, dash, 4 digits)\")\n        return v\n```\n\n----------------------------------------\n\nTITLE: Creating Discriminated Unions for Query Types\nDESCRIPTION: Shows how to implement discriminated unions using Literal types to handle different query types with OpenAI integration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/unions.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal, Union\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\n\nclass UserQuery(BaseModel):\n    type: Literal[\"user\"]\n    username: str\n\n\nclass SystemQuery(BaseModel):\n    type: Literal[\"system\"]\n    command: str\n\n\nQuery = Union[UserQuery, SystemQuery]\n\n# Usage with Instructor\nclient = instructor.from_openai(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Query,\n    messages=[{\"role\": \"user\", \"content\": \"Parse: user lookup jsmith\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Partial Streaming with Instructor and OpenAI\nDESCRIPTION: Demonstrates how to use partial streaming to process a single user profile object as it's being generated. The example shows real-time updates of user data fields as they become available.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import from_openai\nimport openai\nfrom pydantic import BaseModel\n\nclient = from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    bio: str\n\n\nuser = client.chat.completions.create_partial(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user profile for Jason, age 25\"},\n    ],\n    response_model=User,\n)\n\nfor user_partial in user:\n    print(user_partial)\n\n# > name='Jason' age=None bio='None'\n# > name='Jason' age=25 bio='A tech'\n# > name='Jason' age=25 bio='A tech enthusiast'\n# > name='Jason' age=25 bio='A tech enthusiast who loves coding, gaming, and exploring new'\n# > name='Jason' age=25 bio='A tech enthusiast who loves coding, gaming, and exploring new technologies'\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Responses with Instructor\nDESCRIPTION: Example of using Instructor's streaming capabilities for larger responses or improved user experience.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/getting-started.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import Partial\n\n# Stream the response as it's being generated\nstream = client.chat.completions.create_partial(\n    model=\"gpt-3.5-turbo\",\n    response_model=Person,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract a detailed person profile for John Smith, 35, who lives in Chicago and Springfield.\"}\n    ],\n)\n\nfor partial in stream:\n    # This will incrementally show the response being built\n    print(partial)\n```\n\n----------------------------------------\n\nTITLE: Task Generation with Progress Tracking\nDESCRIPTION: A practical example showing how to stream a list of tasks with real-time progress tracking. Includes time measurement, progress percentage calculation, and detailed task information display.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/streaming/lists.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport time\n\nclient = instructor.from_openai(OpenAI())\n\nclass Task(BaseModel):\n    title: str = Field(..., description=\"Task title\")\n    description: str = Field(..., description=\"Detailed task description\")\n    priority: str = Field(..., description=\"Task priority (High/Medium/Low)\")\n    estimated_hours: float = Field(..., description=\"Estimated hours to complete\")\n\nprint(\"Generating project tasks...\")\nstart_time = time.time()\nreceived_tasks = 0\n\nfor task in client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Generate a list of 5 tasks for building a personal website\"}\n    ],\n    response_model=List[Task],\n    stream=True\n):\n    received_tasks += 1\n    print(f\"\\nTask {received_tasks}: {task.title} (Priority: {task.priority})\")\n    print(f\"Description: {task.description[:100]}...\")\n    print(f\"Estimated time: {task.estimated_hours} hours\")\n    \n    # Calculate progress percentage based on expected items\n    progress = (received_tasks / 5) * 100\n    print(f\"Progress: {progress:.0f}%\")\n\nelapsed_time = time.time() - start_time\nprint(f\"\\nAll {received_tasks} tasks generated in {elapsed_time:.2f} seconds\")\n```\n\n----------------------------------------\n\nTITLE: Complex Example-Based Generation with Model Config\nDESCRIPTION: Advanced synthetic data generation using model-level examples and GPT-4 Turbo, specifically configured to generate wizard-themed user data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/fake-data.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\n\nfrom typing import Iterable\nfrom pydantic import BaseModel, ConfigDict\nfrom openai import OpenAI\n\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    \"\"\"Old Wizards\"\"\"\n\n    name: str\n    age: int\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"name\": \"Gandalf the Grey\", \"age\": 1000},\n                {\"name\": \"Albus Dumbledore\", \"age\": 150},\n            ]\n        }\n    )\n\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=Iterable[UserDetail],\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Generate `{count}` synthetic examples\"},\n        ],\n    )\n\n\nfor user in generate_fake_users(5):\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Implementing Redis Cache Decorator for Pydantic Models\nDESCRIPTION: A decorator implementation that caches function results returning Pydantic models using Redis. It handles serialization and deserialization of Pydantic models automatically, with key generation based on function name and arguments.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/caching.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n```\n\n----------------------------------------\n\nTITLE: FastAPI Endpoint for Text Tagging in Python\nDESCRIPTION: This snippet defines a FastAPI endpoint for text tagging. It uses Pydantic models for request and response validation, demonstrating how to integrate the tagging system into a web API.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.post(\"/tag\", response_model=TagResponse)\nasync def tag(request: TagRequest) -> TagResponse:\n    return await tag_request(request)\n```\n\n----------------------------------------\n\nTITLE: Generating GPT-3.5 Response with Context\nDESCRIPTION: Demonstrates how to generate a response from GPT-3.5 Turbo using a specific context and question, structured using the QuestionAnswer model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/validators/readme.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil is to live a life of sin and debauchery.\"\n\nqa: QuestionAnswer = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Function Calling with Vertex AI in Python\nDESCRIPTION: This snippet shows how to use parallel function calling with Vertex AI to fetch weather information for multiple locations and perform a Google search simultaneously. It uses the instructor library and Pydantic models to structure the function calls.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/parallel.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport vertexai\nfrom vertexai.generative_models import GenerativeModel\nfrom typing import Iterable, Literal\nfrom pydantic import BaseModel\n\nvertexai.init()\n\n\nclass Weather(BaseModel):\n    location: str\n    units: Literal[\"imperial\", \"metric\"]\n\n\nclass GoogleSearch(BaseModel):\n    query: str\n\n\nclient = instructor.from_vertexai(\n    GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n    mode=instructor.Mode.VERTEXAI_PARALLEL_TOOLS,\n)\n\nfunction_calls = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather in toronto and dallas and who won the super bowl?\",\n        },\n    ],\n    response_model=Iterable[Weather | GoogleSearch],\n)\n\nfor fc in function_calls:\n    print(fc)\n    #> location='Toronto' units='metric'\n    #> location='Dallas' units='imperial'\n    #> query='who won the super bowl'\n```\n\n----------------------------------------\n\nTITLE: Testing Timestamp Format Normalization\nDESCRIPTION: Test cases for the SegmentWithTimestamp class that demonstrate how the solution correctly handles different input formats (MM:SS and HH:MM:SS) and normalizes them to a consistent output format.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/timestamp.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    # Test cases for SegmentWithTimestamp\n    test_cases = [\n        (\n            SegmentWithTimestamp(\n                title=\"Introduction\", time_format=\"MM:SS\", timestamp=\"00:30\"\n            ),\n            \"00:00:30\",\n        ),\n        (\n            SegmentWithTimestamp(\n                title=\"Main Topic\", time_format=\"HH:MM:SS\", timestamp=\"00:15:45\"\n            ),\n            \"00:15:45\",\n        ),\n        (\n            SegmentWithTimestamp(\n                title=\"Conclusion\", time_format=\"MM:SS\", timestamp=\"65:00\"\n            ),\n            \"01:05:00\",\n        ),\n    ]\n\n    for input_data, expected_output in test_cases:\n        try:\n            assert input_data.timestamp == expected_output\n            print(f\"Test passed: {input_data.timestamp} == {expected_output}\")\n        except AssertionError:\n            print(f\"Test failed: {input_data.timestamp} != {expected_output}\")\n\n    # Output:\n    # Test passed: 00:00:30 == 00:00:30\n    # Test passed: 00:15:45 == 00:15:45\n    # Test passed: 01:05:00 == 01:05:00\n```\n\n----------------------------------------\n\nTITLE: Implementing Validation with Pydantic and Context Variables\nDESCRIPTION: Shows how to use ValidationInfo to access context and implement validators for handling sensitive information, including banning specific words and redacting patterns using regular expressions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/jinja-proposal.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationInfo, field_validator\n\nclass Response(BaseModel):\n    text: str\n\n    @field_validator('text')\n    @classmethod\n    def no_banned_words(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            banned_words = context.get('banned_words', set())\n            banned_words_found = [word for word in banned_words if word.lower() in v.lower()]\n            if banned_words_found:\n                raise ValueError(f\"Banned words found in text: {', '.join(banned_words_found)}, rewrite it but just without the banned words\")\n        return v\n\n    @field_validator('text')\n    @classmethod\n    def redact_regex(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            redact_patterns = context.get('redact_patterns', [])\n            for pattern in redact_patterns:\n                v = re.sub(pattern, '****', v)\n        return v\n\nresponse = client.create(\n    model=\"gpt-4o\",\n    response_model=Response,\n    messages=[\n        {\n            \"role\": \"user\", \n            \"content\": \"\"\"\n                Write about a {{ topic }}\n\n                {% if banned_words %}\n                You must not use the following banned words:\n\n                <banned_words>\n                {% for word in banned_words %}\n                * {{ word }}\n                {% endfor %}\n                </banned_words>\n                {% endif %}\n              \"\"\"\n        },\n    ],\n    context={\n        \"topic\": \"jason and now his phone number is 123-456-7890\"\n        \"banned_words\": [\"jason\"],\n        \"redact_patterns\": [\n            r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # Phone number pattern\n            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",          # SSN pattern\n        ],\n    },\n    max_retries=3,\n)\n\nprint(response.text)\n# > While i can't say his name anymore, his phone number is ****\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Data Extraction with Provider\nDESCRIPTION: Implementation of asynchronous structured data extraction using the provider's async client with Instructor\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/templates/provider_template.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport os\nimport asyncio\nfrom typing import Optional\n\n# Third-party imports\nimport instructor\nfrom provider_sdk import AsyncClientClass\nfrom pydantic import BaseModel, Field\n\n# Set up environment (typically handled before script execution)\n# os.environ[\"PROVIDER_API_KEY\"] = \"your-api-key\"  # Uncomment and replace with your API key if not set\n\n# Define your data structure with proper annotations\nclass UserExtract(BaseModel):\n    \"\"\"Model for extracting user information from text.\"\"\"\n    name: str = Field(description=\"The user's full name\")\n    age: int = Field(description=\"The user's age in years\")\n\n# Initialize the async client with explicit mode\nclient = instructor.from_provider(\n    AsyncClientClass(\n        api_key=os.environ.get(\"PROVIDER_API_KEY\", \"your_api_key_here\"),\n    ),\n    mode=instructor.Mode.PROVIDER_SPECIFIC_MODE,\n)\n\nasync def extract_data(text: str) -> UserExtract:\n    \"\"\"\n    Asynchronously extract structured data from text.\n    \n    Args:\n        text: The input text to extract from\n        \n    Returns:\n        A structured UserExtract object\n    \"\"\"\n    try:\n        user = await client.chat.completions.create(\n            model=\"provider-model-name\",  # Use latest stable model version\n            response_model=UserExtract,\n            messages=[\n                {\"role\": \"system\", \"content\": \"Extract structured user information from the text.\"},\n                {\"role\": \"user\", \"content\": text},\n            ],\n        )\n        return user\n    except Exception as e:\n        print(f\"Error during extraction: {e}\")\n        raise\n\n# Example usage\nasync def main():\n    result = await extract_data(\"Extract jason is 25 years old\")\n    print(result.model_dump_json(indent=2))\n\n# Run the async function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# Expected output:\n# {\n#   \"name\": \"Jason\",\n#   \"age\": 25\n# }\n```\n\n----------------------------------------\n\nTITLE: Using Tuples for Simple Types in Pydantic Models\nDESCRIPTION: This code snippet shows how to use tuples as a more compact alternative to custom classes for simple types in Pydantic models. It defines a UserDetail class with a properties field using a list of tuples.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Tuple\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Tuple[int, str]] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating instructor clients with different providers\nDESCRIPTION: Shows how to create instructor clients for different LLM providers (OpenAI, Anthropic, LiteLLM) and the unified API for using them. Demonstrates type variable usage for generic typing.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/version-1.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport anthropic\nimport litellm\nimport instructor\nfrom typing import TypeVar\n\nT = TypeVar(\"T\")\n\n# These are all ways to create a client\nclient = instructor.from_openai(openai.OpenAI())\nclient = instructor.from_anthropic(anthropic.Anthropic())\nclient = instructor.from_litellm(litellm.completion)\n\n# all of these will route to the same underlying create function\n# allow you to add instructor to try it out, while easily removing it\nclient.create(model=\"gpt-4\", response_model=type[T]) -> T\nclient.chat.completions.create(model=\"gpt-4\", response_model=type[T]) -> T\nclient.messages.create(model=\"gpt-4\", response_model=type[T]) -> T\n```\n\n----------------------------------------\n\nTITLE: Reasoning Model Integration\nDESCRIPTION: Example demonstrating how to use the DeepSeek reasoning model with Instructor for detailed reasoning traces.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/deepseek.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\nfrom rich import print\n\nclient = instructor.from_openai(\n    OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\"),\n    mode=instructor.Mode.MD_JSON,\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Create structured output\ncompletion, raw_completion = client.chat.completions.create_with_completion(\n    model=\"deepseek-reasoner\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n    ],\n    response_model=User,\n)\n\nprint(completion)\nprint(raw_completion.choices[0].message.reasoning_content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Handling within Function Calls using Pydantic\nDESCRIPTION: This snippet demonstrates how to create a wrapper class (MaybeUser) to handle errors within function calls. It allows for better error handling without breaking the code flow by holding either the result of an operation or an error message.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Optional Fields with Pydantic\nDESCRIPTION: Demonstrates how to create a basic data model with required and optional fields using Pydantic and Instructor AI.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/optional_fields.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Person(BaseModel):\n    name: str  # Required field\n    age: Optional[int] = None  # Optional field with None default\n    occupation: Optional[str] = None  # Optional field with None default\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Entities\nDESCRIPTION: This snippet shows how to extract multiple entities from the API using a list. The Character class is defined, and the response model is set to Iterable[Character]. Then the response is iterated over to print each character.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Iterable\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n```\n\n----------------------------------------\n\nTITLE: Complex Validation with Multiple Fields in Python\nDESCRIPTION: Shows implementation of complex validators that handle multiple fields and conditional logic. Includes validation for employee data with date comparisons and skills requirements.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/custom_validators.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, field_validator, model_validator\nimport instructor\nfrom openai import OpenAI\nfrom typing import List, Optional\nfrom datetime import date\n\nclient = instructor.from_openai(OpenAI())\n\nclass Employee(BaseModel):\n    name: str\n    hire_date: date\n    termination_date: Optional[date] = None\n    skills: List[str]\n    \n    @field_validator('skills')\n    @classmethod\n    def validate_skills(cls, skills):\n        if len(skills) < 1:\n            raise ValueError(\"Employee must have at least one skill\")\n        return skills\n    \n    @model_validator(mode='after')\n    def validate_dates(self):\n        if self.termination_date and self.termination_date < self.hire_date:\n            raise ValueError(\"Termination date cannot be before hire date\")\n        return self\n```\n\n----------------------------------------\n\nTITLE: Initializing Instructor with OpenAI Client\nDESCRIPTION: Shows how to initialize the Instructor library by patching the OpenAI client for JSON extraction capabilities.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/using_json.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n```\n\n----------------------------------------\n\nTITLE: OpenAI Moderation Integration\nDESCRIPTION: Example of integrating OpenAI's moderation endpoint using the instructor library for content moderation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n```\n\n----------------------------------------\n\nTITLE: Async Question Generation Implementation\nDESCRIPTION: Implements async question generation with rate limiting and retries\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom asyncio import Semaphore\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom openai import AsyncOpenAI\nimport asyncio\n\nclient = from_openai(AsyncOpenAI())\n\nasync def generate_questions(chunks: list[str], max_queries: int):\n    @retry(\n        stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10)\n    )\n    async def generate_question(\n        chunk: str, sem: Semaphore\n    ) -> tuple[QuestionAnswerPair, str]:\n        async with sem:\n            return (\n                await client.chat.completions.create(\n                    model=\"gpt-3.5-turbo\",\n                    messages=[\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"You are a world class AI that excels at generating hypothetical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\n                        },\n                        {\"role\": \"user\", \"content\": f\"Here is the text chunk: {chunk}\"},\n                    ],\n                    response_model=QuestionAnswerPair,\n                ),\n                chunk,\n            )\n\n    sem = Semaphore(max_queries)\n    coros = [generate_question(chunk, sem) for chunk in chunks]\n    return await asyncio.gather(*coros)\n```\n\n----------------------------------------\n\nTITLE: Generating Question-Answer Pairs with OpenAI in Python\nDESCRIPTION: This snippet demonstrates how to use the 'instructor' library with OpenAI to generate a question-answer pair from a given text chunk. It defines a Pydantic model for the response and uses the OpenAI client to make the API call.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/batch_job_oai.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\nfrom instructor import from_openai\n\nclient = from_openai(OpenAI())\n\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(description=\"The generated question from the text chunk.\")\n    answer: str = Field(description=\"The answer to the generated question.\")\n\n\ndef generate_question(chunk: str) -> QuestionAnswerPair:\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the text chunk: {chunk}\"},\n        ],\n        response_model=QuestionAnswerPair,\n    )\n\n\ntext_chunk = \"\"\"\nThe Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\n\"\"\"\nprint(generate_question(text_chunk).model_dump_json(indent=2))\n```\n\n----------------------------------------\n\nTITLE: Customizing JSON Schema Properties\nDESCRIPTION: Shows how to customize JSON schema with additional metadata like title, description, and examples.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/fields.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, SecretStr\n\n\nclass User(BaseModel):\n    age: int = Field(description='Age of the user')\n    name: str = Field(title='Username')\n    password: SecretStr = Field(\n        json_schema_extra={\n            'title': 'Password',\n            'description': 'Password of the user',\n            'examples': ['123456'],\n        }\n    )\n\n\nprint(User.model_json_schema())\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain-of-Thought Validation Function in Python\nDESCRIPTION: This function uses an LLM to validate whether the provided answer follows logically from the chain of thought. It raises a ValueError if the validation fails.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value follows from the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\n            },\n        ],\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n```\n\n----------------------------------------\n\nTITLE: Defining Recursive Pydantic Model\nDESCRIPTION: Demonstrates how to create a recursive Pydantic model for representing hierarchical data structures. The model includes a name, optional value, and a list of child nodes of the same type.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/recursive.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\n\n\nclass RecursiveNode(BaseModel):\n    \"\"\"A node that can contain child nodes of the same type.\"\"\"\n\n    name: str = Field(..., description=\"Name of the node\")\n    value: Optional[str] = Field(\n        None, description=\"Optional value associated with the node\"\n    )\n    children: List[\"RecursiveNode\"] = Field(\n        default_factory=list, description=\"List of child nodes\"\n    )\n\n\n# Required for recursive Pydantic models\nRecursiveNode.model_rebuild()\n```\n\n----------------------------------------\n\nTITLE: Using async client with instructor\nDESCRIPTION: Example of using instructor with an async OpenAI client. Shows how the type inference works correctly with async functions returning the properly typed response.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/version-1.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.AsyncOpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract():\n    return await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Create a user\"},\n        ],\n        response_model=User,\n    )\n```\n\n----------------------------------------\n\nTITLE: Extracting Character data using Literals\nDESCRIPTION: This snippet defines a Character class with the house attribute as a Literal type, limiting the possible values. The Character data is extracted using the `chat.completions.create` method, then the response is formatted to JSON.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n```\n\n----------------------------------------\n\nTITLE: Configuring Burr Application with OpenTelemetry Instrumentation in Python\nDESCRIPTION: This code snippet demonstrates how to instrument the OpenAI library and configure a Burr ApplicationBuilder with telemetry tracking. It enables OpenTelemetry tracing for the YouTube Q&A application.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-flashcards.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom burr.core import ApplicationBuilder\nfrom opentelemetry.instrumentation.openai import OpenAIApiInstrumentor\n\n# instrument before importing instructor or creating the OpenAI client\nOpenAIApiInstrumentor().instrument()\n\napp = (\n    ApplicationBuilder()\n    .with_actions(\n        process_user_input,\n        get_youtube_transcript,\n        generate_question_and_answers,\n    )\n    .with_transitions(\n        (\"process_user_input\", \"get_youtube_transcript\"),\n        (\"get_youtube_transcript\", \"generate_question_and_answers\"),\n        (\"generate_question_and_answers\", \"process_user_input\"),\n    )\n    .with_tracker(project=\"youtube-qna\", use_otel_tracing=True)\n    .with_entrypoint(\"process_user_input\")\n    .build()\n)\n```\n\n----------------------------------------\n\nTITLE: Citation Validation Implementation\nDESCRIPTION: Creates a validation system to ensure citations exist within provided context to prevent hallucination.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-1-validation-rag.ipynb#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import ValidationInfo\n\ndef citation_exists(v: str, info: ValidationInfo):\n    context = info.context\n    if context:\n        context = context.get(\"text_chunk\")\n        if v not in context:\n            raise ValueError(\n                f\"Citation `{v}` not found in text, only use citations from the text.\"\n            )\n    return v\n\nCitation = Annotated[str, AfterValidator(citation_exists)]\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: Citation\n```\n\n----------------------------------------\n\nTITLE: LanceDB Schema Definition with OpenAI Embeddings\nDESCRIPTION: Defines data schema using Pydantic model with OpenAI embeddings integration\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\nfunc = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n\nclass Chunk(LanceModel):\n    passage: str = func.SourceField()\n    chunk_id: str\n    embedding: Vector(func.ndims()) = func.VectorField()\n\ntable = db.create_table(DB_TABLE, schema=Chunk, exist_ok=True, mode=\"overwrite\")\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with Anthropic's Claude Model\nDESCRIPTION: Example of using Instructor with Anthropic's Claude model for structured data extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/getting-started.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n# Create an instructor-patched Anthropic client\nclient = instructor.from_anthropic(Anthropic())\n\nuser_info = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    response_model=UserInfo,\n    messages=[\n        {\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}\n    ],\n)\n\nprint(f\"Name: {user_info.name}, Age: {user_info.age}\")\n```\n\n----------------------------------------\n\nTITLE: Practical COSP Usage Example with OpenAI in Python\nDESCRIPTION: This code demonstrates how to use the COSPSelector to identify and use the best examples for few-shot learning in a prompt. It selects optimal examples from a candidate pool and formats them for inclusion in a prompt for improved model performance.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/few_shot/cosp.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize COSP selector\nclient = OpenAI()\nselector = COSPSelector(client)\n\n# Candidate examples\ncandidates = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Machine learning is a subset of artificial intelligence\",\n    \"Python is a high-level programming language\",\n    # ... more examples\n]\n\n# Select best examples\nbest_examples = selector.select_examples(candidates, k=3)\n\n# Use selected examples in your prompt\nselected_texts = [ex.text for ex in best_examples]\nprompt = f\"\"\"Use these examples to guide your response:\n\nExamples:\n{chr(10).join(f'- {text}' for text in selected_texts)}\n\nNow, please respond to: [your query here]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Nested Object Extraction Example\nDESCRIPTION: Example showing how to extract nested data structures using DeepSeek and Instructor with multiple address objects.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/deepseek.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport os\nfrom openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\n# Initialize with API key\nclient = instructor.from_openai(\n    OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n)\n\n\n# Create structured output with nested objects\nuser = client.chat.completions.create(\n    model=\"deepseek-chat\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract: Jason is 25 years old.\n            He lives at 123 Main St, New York, USA\n            and has a summer house at 456 Beach Rd, Miami, USA\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n\nprint(user)\n```\n\n----------------------------------------\n\nTITLE: Calling Compute Method on Structured Data in Python\nDESCRIPTION: This code snippet shows how to call a compute method directly on a StructuredData object, demonstrating the object-oriented approach to working with extracted data in the Instructor library.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/philosophy.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata.compute()\n```\n\n----------------------------------------\n\nTITLE: Initializing Vertex AI Gemini Client with Instructor Patching for Tool Calling\nDESCRIPTION: This snippet demonstrates how to initialize a Vertex AI Gemini client with Instructor patching using the VERTEXAI_TOOLS mode. It requires initializing the Vertex AI project and has some limitations as it's in preview.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nimport vertexai\n\nvertexai.init(project=\"vertexai-generative-models\")\n\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Lists of Unions for Multiple Response Types\nDESCRIPTION: Shows how to handle multiple response types in a list using Union types.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/types.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union, List\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Weather(BaseModel, frozen=True):\n    location: str\n\n\nclass Add(BaseModel, frozen=True):\n    a: int\n    b: int\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[Union[Add, Weather]],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Add 5 and 5, and also whats the weather in Toronto?\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Pydantic Model Creation\nDESCRIPTION: Demonstrates how to create Pydantic models dynamically using create_model function. Creates a new model inheriting from a base model with additional fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/models.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, create_model\n\n\nclass FooModel(BaseModel):\n    foo: str\n    bar: int = 123\n\n\nBarModel = create_model(\n    'BarModel',\n    apple=(str, 'russet'),\n    banana=(str, 'yellow'),\n    __base__=FooModel,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Iterable Streaming for Multiple Users with Instructor\nDESCRIPTION: Shows how to use iterable streaming to extract multiple user profiles from text. The example demonstrates processing a list of users with their names and ages using structured output.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n# Extract multiple users from text\nusers = client.chat.completions.create_iterable(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n            Extract users:\n            1. Jason is 25 years old\n            2. Sarah is 30 years old\n            3. Mike is 28 years old\n        \"\"\"},\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n    #> name='Jason' age=25\n    #> name='Sarah' age=30\n    #> name='Mike' age=28\n```\n\n----------------------------------------\n\nTITLE: Simple Synthetic User Data Generation with Pydantic and OpenAI\nDESCRIPTION: Basic implementation of synthetic data generation using Pydantic model and OpenAI. Creates simple user details with name and age fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/fake-data.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterable\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Generate a {count} synthetic users\"},\n        ],\n    )\n\n\nfor user in generate_fake_users(5):\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Complete Implementation Example\nDESCRIPTION: Shows the full implementation combining all components including imports, client setup, model definition, and execution logic.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/anthropic-prompt-caching.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import Instructor, Mode, patch\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\nclient = Instructor(\n    client=Anthropic(),\n    create=patch(\n        create=Anthropic().beta.prompt_caching.messages.create,\n        mode=Mode.ANTHROPIC_TOOLS,\n    ),\n    mode=Mode.ANTHROPIC_TOOLS,\n)\n\n\nclass Character(BaseModel):\n    name: str\n    description: str\n\n\nwith open(\"./book.txt\") as f:\n    book = f.read()\n\nfor _ in range(2):\n    resp, completion = client.chat.completions.create_with_completion(\n        model=\"claude-3-haiku-20240307\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"<book>\" + book + \"</book>\",\n                        \"cache_control\": {\"type\": \"ephemeral\"},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Extract a character from the text given above\",\n                    },\n                ],\n            },\n        ],\n        response_model=Character,\n        max_tokens=1000,\n    )\n    assert isinstance(resp, Character)\n    print(completion.usage)\n    print(resp)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Streaming with Mistral and Instructor\nDESCRIPTION: Python code demonstrating asynchronous streaming using Mistral and Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydantic import BaseModel\nimport instructor\nfrom mistralai import Mistral\nfrom instructor.dsl.partial import Partial\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n# Initialize client with async support\nclient = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\ninstructor_client = instructor.from_mistral(client, use_async=True)\n\nasync def stream_partial():\n    model = await instructor_client.chat.completions.create(\n        model=\"mistral-large-latest\",\n        response_model=Partial[UserExtract],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Jason Liu is 25 years old\"},\n        ],\n    )\n\n    async for partial_user in model:\n        print(f\"Received update: {partial_user}\")\n\nasync def stream_iterable():\n    users = instructor_client.chat.completions.create_iterable(\n        model=\"mistral-large-latest\",\n        response_model=UserExtract,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make up two people\"},\n        ],\n    )\n\n    async for user in users:\n        print(f\"Generated user: {user}\")\n\n# Run async functions\nasyncio.run(stream_partial())\nasyncio.run(stream_iterable())\n```\n\n----------------------------------------\n\nTITLE: Integrating Instructor with FastAPI in Python\nDESCRIPTION: Example of how to use Instructor with FastAPI to create an endpoint for extracting structured user information from text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/faq.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\napp = FastAPI()\nclient = instructor.from_openai(OpenAI())\n\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n@app.post(\"/extract\")\nasync def extract_user_info(text: str) -> UserInfo:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserInfo,\n        messages=[{\"role\": \"user\", \"content\": text}]\n    )\n```\n\n----------------------------------------\n\nTITLE: Customizing Retry Behavior Configuration in Python\nDESCRIPTION: Shows how to customize retry behavior by configuring various options when initializing the Instructor client, including max retries, parsing failure handling, and error throwing behavior.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/retry_mechanisms.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\n# Customize retry behavior\nclient = instructor.from_openai(\n    OpenAI(),\n    max_retries=3,                   # Maximum number of retries\n    retry_if_parsing_fails=True,     # Retry on JSON parsing failures\n    throw_error=True                 # Throw an error if all retries fail\n)\n```\n\n----------------------------------------\n\nTITLE: Main Execution Function with Language Detection and Verification\nDESCRIPTION: Runs the summarization on multiple documents asynchronously, then compares the language of the original text and its summary to verify language preservation across different languages.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/matching-language.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    import asyncio\n\n    async def main():\n        results = await asyncio.gather(*[summarize_text(doc) for doc in docs])\n        for summary, doc in results:\n            source_lang = detect(doc)\n            target_lang = detect(summary)\n            print(\n                f\"Source: {source_lang}, Summary: {target_lang}, Match: {source_lang == target_lang}\"\n            )\n\n    asyncio.run(main())\n    \"\"\"\n    Source: et, Summary: et, Match: True\n    Source: tl, Summary: tl, Match: True\n    Source: sw, Summary: sw, Match: True\n    Source: tr, Summary: tr, Match: True\n    Source: vi, Summary: vi, Match: True\n    Source: fr, Summary: fr, Match: True\n    Source: zh-cn, Summary: zh-cn, Match: True\n    Source: de, Summary: de, Match: True\n    Source: hi, Summary: hi, Match: True\n    Source: ja, Summary: ja, Match: True\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining IdentifiedProduct Model in Python with Pydantic\nDESCRIPTION: Creates a Pydantic model representing a list of products identified in images. It includes fields for products, error flag, and error message.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/image_to_ad_copy.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\n\n\nclass IdentifiedProduct(BaseModel):\n    \"\"\"\n    Represents a list of products identified in the images.\n    \"\"\"\n\n    products: Optional[List[Product]] = Field(\n        description=\"A list of products identified by the AI.\",\n        example=[\n            Product(\n                name=\"Headphones\",\n                description=\"Wireless headphones with noise cancellation.\",\n                key_features=[\"Wireless\", \"Noise Cancellation\"],\n            )\n        ],\n        default=None,\n    )\n\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n```\n\n----------------------------------------\n\nTITLE: Instructor Client Setup\nDESCRIPTION: Initialization of the Instructor client by wrapping the OpenAI client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/first_extraction.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_openai(OpenAI())\n```\n\n----------------------------------------\n\nTITLE: Implementing TypedDict with OpenAI API in Python\nDESCRIPTION: Demonstrates how to define and use a TypedDict class for structured OpenAI API responses. The example creates a User TypedDict with name and age fields, and uses it with the instructor-enhanced OpenAI client to process natural language into structured data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/typeddicts.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import TypedDict\nfrom openai import OpenAI\nimport instructor\n\n\nclass User(TypedDict):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(OpenAI())\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Timothy is a man from New York who is turning 32 this year\",\n        }\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Aligning Citations and Answers using LLM in Python\nDESCRIPTION: This snippet demonstrates how to ensure that provided answers are aligned with given citations and context. It uses a model validator in the AnswerWithCitaton class to verify the alignment using an LLM.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/citations.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n\n    @model_validator(mode=\"after\")\n    def validate_answer(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following answers match the question and the context?\\n\\nQuestion: {self.question}\\n\\nAnswer: {self.answer}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n```\n\n----------------------------------------\n\nTITLE: Implementing Uncertainty Estimation for LLMs using Disagreement Metric in Python\nDESCRIPTION: This code demonstrates how to calculate the uncertainty of an LLM's responses using the disagreement metric. It queries a LLM multiple times with the same question about the Empire State Building's height, then calculates the ratio of unique responses to total responses to quantify uncertainty. The implementation uses Instructor and Pydantic for structured outputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/thought_generation/chain_of_thought_few_shot/active_prompt.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\nclass Response(BaseModel):\n    height: int\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef query_llm():\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"How tall is the Empire State Building in meters?\",\n            }\n        ],\n    )\n\n\ndef calculate_disagreement(responses):\n    unique_responses = set(responses)\n    h = len(unique_responses)\n    return h / k\n\n\nif __name__ == \"__main__\":\n    k = 5  # (1)!\n    responses = [query_llm() for _ in range(k)]  # Query the LLM k times\n    for response in responses:\n        print(response)\n        #> height=443\n        #> height=443\n        #> height=443\n        #> height=443\n        #> height=381\n\n    print(\n        calculate_disagreement([response.height for response in responses])\n    )  # Calculate the uncertainty metric\n    #> 0.4\n```\n\n----------------------------------------\n\nTITLE: Optimizing Message Extraction in Python\nDESCRIPTION: This snippet shows the optimization of the extract_messages function, replacing nested get() calls with direct key lookups to reduce function call overhead and improve performance.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/dictionary_operations.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\n\n\ndef extract_messages(kwargs: dict[str, Any]) -> Any:\n    return kwargs.get(\n        \"messages\", kwargs.get(\"contents\", kwargs.get(\"chat_history\", []))\n    )\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\n\n\ndef extract_messages(kwargs: dict[str, Any]) -> Any:\n    if \"messages\" in kwargs:\n        return kwargs[\"messages\"]\n    if \"contents\" in kwargs:\n        return kwargs[\"contents\"]\n    if \"chat_history\" in kwargs:\n        return kwargs[\"chat_history\"]\n    return []\n```\n\n----------------------------------------\n\nTITLE: Tracking Experiment Performance Using Weights and Biases in Python\nDESCRIPTION: The code initiates a Weights and Biases run, processes multiple queries asynchronously, and logs performance metrics along with schema and results. It saves the experiment data in various formats for later analysis.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport time\nimport pandas as pd\nimport wandb\n\nmodel = \"gpt-4-1106-preview\"\ntemp = 0\n\nrun = wandb.init(\n    project=\"query\",\n    config={\"model\": model, \"temp\": temp},\n)\n\ntest_queries = [\n    \"latest developments in artificial intelligence last 3 weeks\",\n    \"renewable energy trends past month\",\n    \"quantum computing advancements last 2 months\",\n    \"biotechnology updates last 10 days\",\n]\nstart = time.perf_counter()\nqueries = await asyncio.gather(\n    *[expand_query(q, model=model, temp=temp) for q in test_queries]\n)\nduration = time.perf_counter() - start\n\nwith open(\"schema.json\", \"w+\") as f:\n    schema = Query.model_json_schema()\n    json.dump(schema, f, indent=2)\n\nwith open(\"results.jsonlines\", \"w+\") as f:\n    for query in queries:\n        f.write(query.model_dump_json() + \"\\n\")\n\n\ndf = dicts_to_df([q.report() for q in queries])\ndf[\"input\"] = test_queries\ndf.to_csv(\"results.csv\")\n\n\nrun.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))})\nrun.log(\n    {\n        \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),\n        \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),\n        \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),\n        \"duration (s)\": duration,\n        \"average duration (s)\": duration / len(queries),\n        \"n_queries\": len(queries),\n    }\n)\n\nrun.log(\n    {\n        \"results\": wandb.Table(dataframe=df),\n    }\n)\n\nfiles = wandb.Artifact(\"data\", type=\"dataset\")\nfiles.add_file(\"schema.json\")\nfiles.add_file(\"results.jsonlines\")\nfiles.add_file(\"results.csv\")\n\nrun.log_artifact(files)\nrun.finish()\n```\n\n----------------------------------------\n\nTITLE: Validation with Union Types\nDESCRIPTION: Shows how to implement validation for union types using Instructor's validation hook.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/unions.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import patch\nfrom openai import OpenAI\n\nclient = patch(OpenAI())\n\n\ndef validate_response(response: Response) -> bool:\n    if isinstance(response, ErrorResponse):\n        return len(response.message) > 0\n    return True\n\n\nresult = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Response,\n    validation_hook=validate_response,\n    messages=[{\"role\": \"user\", \"content\": \"Process this request\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Cerebras and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with Cerebras to extract structured data. It uses a Pydantic model for data extraction and Cerebras' client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom cerebras.cloud.sdk import Cerebras\nimport instructor\nfrom pydantic import BaseModel\nimport os\n\nclient = Cerebras(\n    api_key=os.environ.get(\"CEREBRAS_API_KEY\"),\n)\nclient = instructor.from_cerebras(client)\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nresp = client.chat.completions.create(\n    model=\"llama3.1-70b\",\n    response_model=ExtractUser,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n)\n\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Running Chain of Density Summarization and Timing Execution in Python\nDESCRIPTION: This code runs the summarize_article function on the loaded article and measures the execution time. It uses the Jupyter magic command %%time for timing.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nsummaries = summarize_article(article)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Chain of Density Implementation\nDESCRIPTION: This snippet shows the pip command to install the required Python packages: instructor for OpenAI function calling, aiohttp for asynchronous HTTP requests, and rich for enhanced console output.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chain-of-density.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor aiohttp rich\n```\n\n----------------------------------------\n\nTITLE: Creating Optional Response Models with Error Handling\nDESCRIPTION: This code demonstrates how to design Pydantic models that can represent either successful responses or errors. It defines a MaybeUser class that can contain either user details or error information.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail]\n    error: bool\n    message: Optional[str]\n```\n\n----------------------------------------\n\nTITLE: Defining Typed Computations on Structured Data in Python\nDESCRIPTION: This snippet illustrates how to define a function that performs typed computations on structured data extracted from an LLM. It uses type hinting to specify the input data type.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/philosophy.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef compute(data: StructuredData):\n```\n\n----------------------------------------\n\nTITLE: Updated GitHub Actions Workflow Using uv\nDESCRIPTION: This YAML configuration shows the updated GitHub Actions workflow that uses uv instead of Poetry. It includes steps for installing uv, setting up Python, installing project dependencies, and running tests using uv commands. The workflow maintains similar functionality but leverages uv for improved performance.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/migrating-to-uv.md#2025-04-14_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nname: Test\non:\n  pull_request:\n  push:\n    branches:\n      - main\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\"]\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install uv\n        uses: astral-sh/setup-uv@v4\n        with:\n          enable-cache: true\n\n      - name: Set up Python\n        run: uv python install ${{ matrix.python-version }}\n\n      - name: Install the project\n        run: uv sync --all-extras\n      - name: Run tests\n        if: matrix.python-version != '3.11'\n        run: uv run pytest tests/ -k 'not llm and not openai and not gemini and not anthropic and not cohere and not vertexai'\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}\n\n      - name: Run Gemini Tests\n        if: matrix.python-version == '3.11'\n        run: uv run pytest tests/llm/test_gemini\n        env:\n          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}\n\n      - name: Generate coverage report\n        if: matrix.python-version == '3.11'\n        run: |\n          uv run coverage run -m pytest tests/ -k \"not docs and not anthropic and not gemini and not cohere and not vertexai and not fireworks\"\n          uv run coverage report\n          uv run coverage html\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n```\n\n----------------------------------------\n\nTITLE: AWS Bedrock with Custom Mode Configuration\nDESCRIPTION: Shows how to configure AWS Bedrock with specific modes (BEDROCK_TOOLS) for function calling support\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/bedrock.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\nimport instructor\nfrom instructor import Mode\nfrom pydantic import BaseModel\n\n# Initialize the Bedrock client\nbedrock_client = boto3.client('bedrock-runtime')\n\n# Enable instructor patches for Bedrock client with specific mode\nclient = instructor.from_bedrock(\n    bedrock_client, \n    mode=Mode.BEDROCK_TOOLS\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Create structured output\nuser = client.converse(\n    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n    ],\n    response_model=User,\n)\n\nprint(user)\n# > User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Configuring Validation Retry Behavior in Python with Instructor\nDESCRIPTION: Shows how to control retry behavior for validation failures in Instructor. This example demonstrates setting the number of retries and whether to raise an exception on validation failure.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/field_validation.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_openai(\n    OpenAI(),\n    max_retries=2,  # Number of retries after the initial attempt\n    throw_error=True  # Whether to raise an exception on validation failure\n)\n```\n\n----------------------------------------\n\nTITLE: Limiting List Length in Pydantic Models\nDESCRIPTION: This snippet demonstrates how to manage the length of lists in Pydantic models, particularly for arbitrary properties. It uses prompting and enumeration to limit the list length, ensuring a manageable set of properties.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Viewing Batch Job List Command Help\nDESCRIPTION: Shows the help menu for the list command, which displays existing batch jobs with options for limiting results, polling, and choosing between OpenAI and Anthropic.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ instructor batch list --help\n\n Usage: instructor batch list [OPTIONS]\n\n See all existing batch jobs\n\n╭─ Options ───────────────────────────────────────────────────────────────────────────╮\n│ --limit                    INTEGER  Total number of batch jobs to show              │\n│                                     [default: 10]                                   │\n│ --poll                     INTEGER  Time in seconds to wait for the batch job to    │\n│                                     complete                                        │\n│                                     [default: 10]                                   │\n│ --screen    --no-screen             Enable or disable screen output                 │\n│                                     [default: no-screen]                            │\n│ --use-anthropic                     Use Anthropic API instead of OpenAI             │\n│                                     [default: False]                                │\n│ --help                              Show this message and exit.                     │\n╰─────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini Client with Instructor Patching for JSON Mode\nDESCRIPTION: This snippet shows how to initialize a Gemini client with Instructor patching using the GEMINI_JSON mode. This mode uses Gemini's response mimetype field to generate a response in JSON format using the provided schema.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\n\nclient = instructor.from_gemini(\n    genai.GenerativeModel(), mode=instructor.Mode.GEMINI_JSON\n)\n```\n\n----------------------------------------\n\nTITLE: Defining RewrittenSummary Data Class in Python\nDESCRIPTION: Creates a Pydantic model for a rewritten, denser summary with additional entities. It includes fields for the summary, absent entities, and missing entities, along with custom validators for each field.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: list[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: list[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n\n    @field_validator(\"summary\")\n    def min_length(cls, v: str):\n        tokens = nltk.word_tokenize(v)\n        num_tokens = len(tokens)\n        if num_tokens < 60:\n            raise ValueError(\n                \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n            )\n        return v\n\n    @field_validator(\"missing\")\n    def has_missing_entities(cls, missing_entities: list[str]):\n        if len(missing_entities) == 0:\n            raise ValueError(\n                \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n            )\n        return missing_entities\n\n    @field_validator(\"absent\")\n    def has_no_absent_entities(cls, absent_entities: list[str]):\n        absent_entity_string = \",\".join(absent_entities)\n        if len(absent_entities) > 0:\n            print(f\"Detected absent entities of {absent_entity_string}\")\n            raise ValueError(\n                f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n            )\n        return absent_entities\n\n    @field_validator(\"summary\")\n    def min_entity_density(cls, v: str):\n        tokens = nltk.word_tokenize(v)\n        num_tokens = len(tokens)\n\n        # Extract Entities\n        doc = nlp(v)\n        num_entities = len(doc.ents)\n\n        density = num_entities / num_tokens\n        if density < 0.08:\n            raise ValueError(\n                f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n            )\n\n        return v\n```\n\n----------------------------------------\n\nTITLE: Instructor Integration Example\nDESCRIPTION: Demonstration of using the Instructor library with OpenAI for structured outputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/1-introduction.ipynb#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport datetime\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"\n            Today is {datetime.date.today()}\n\n            Extract `Jason Liu is thirty years old his birthday is yesterday`\n            he lives at 123 Main St, San Francisco, CA\"\"\",\n        },\n    ],\n    response_model=PersonAddress,\n)\nresp\n```\n\n----------------------------------------\n\nTITLE: Implementing Image Caching with Anthropic Client\nDESCRIPTION: Shows how to implement caching for images with the Anthropic client. This feature helps reduce costs and improve performance when analyzing images repeatedly by caching the processed image data for subsequent API calls.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport os\n\n# Third-party imports\nimport instructor\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel, Field\n\n# Set up environment (typically handled before script execution)\n# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"  # Uncomment and replace with your API key if not set\n\n# Define your model for image analysis\nclass ImageAnalyzer(BaseModel):\n    \"\"\"Model for analyzing image content.\"\"\"\n    content_description: str = Field(description=\"Description of what appears in the images\")\n    objects: list[str] = Field(description=\"List of objects visible in the images\")\n    scene_type: str = Field(description=\"Type of scene shown in the images (indoor, outdoor, etc.)\")\n\n# Initialize client with explicit mode and image caching enabled\nclient = instructor.from_anthropic(\n    Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\")),\n    mode=instructor.Mode.ANTHROPIC_TOOLS,\n    enable_prompt_caching=True  # Enable prompt caching\n)\n\ntry:\n    # Configure cache control for images\n    cache_control = {\"type\": \"ephemeral\"}\n\n    # Make a request with cached images\n    response = client.chat.completions.create(\n        model=\"claude-3-haiku-20240307\",  # Use latest stable model\n        response_model=ImageAnalyzer,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Analyze the content of the provided images in detail.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    \"What is in these two images?\",\n                    # Remote image with caching\n                    {\n                        \"type\": \"image\",\n                        \"source\": \"https://example.com/image.jpg\",\n                        \"cache_control\": cache_control\n                    },\n                    # Local image with caching\n                    {\n                        \"type\": \"image\",\n                        \"source\": \"path/to/image.jpg\",\n                        \"cache_control\": cache_control\n                    },\n                ]\n            }\n        ],\n        autodetect_images=True  # Automatically handle image content\n    )\n\n    # Process the results\n    print(f\"Description: {response.content_description}\")\n    print(f\"Objects: {', '.join(response.objects)}\")\n    print(f\"Scene type: {response.scene_type}\")\n\n    # Subsequent identical requests will use cached images\n\nexcept Exception as e:\n    print(f\"Error during image analysis: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Text Summarization Function\nDESCRIPTION: Defines an asynchronous function that sends a text to OpenAI for summarization using the structured response model, returning both the summary and original text for later comparison.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/matching-language.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def summarize_text(text: str):\n    response = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=GeneratedSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Generate a concise summary in the language of the article. \",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Summarize the following text in a concise way:\\n{text}\",\n            },\n        ],\n    )  # type: ignore\n    return response.summary, text\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple Image URLs for Table Extraction in Python\nDESCRIPTION: This code snippet demonstrates how to use the extract function to process multiple image URLs. It iterates through a list of URLs, extracts tables from each image, and prints the caption and dataframe for each extracted table using the rich library for formatted console output.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/tables_from_vision.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nurls = [\n    \"https://a.storyblok.com/f/47007/2400x1260/f816b031cb/uk-ireland-in-three-charts_chart_a.png/m/2880x0\",\n    \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png/m/2880x0\",\n]\n\nfor url in urls:\n    for table in extract(url).tables:\n        console.print(table.caption, \"\\n\", table.dataframe)\n```\n\n----------------------------------------\n\nTITLE: AI Response Model with Chain of Thought Validation\nDESCRIPTION: Pydantic model implementation for AI responses that includes chain of thought validation using model validators.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, model_validator\n\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -> Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Azure OpenAI with Instructor\nDESCRIPTION: This example shows how to use Azure OpenAI with instructor for basic structured output generation using a Pydantic model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/azure.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport instructor\nfrom openai import AzureOpenAI\nfrom pydantic import BaseModel\n\nclient = AzureOpenAI(\n    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n    api_version=\"2024-02-01\",\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n)\nclient = instructor.from_openai(client)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Synchronous usage\nuser = client.chat.completions.create(\n    model=\"gpt-4o-mini\",  # Your deployment name\n    messages=[{\"role\": \"user\", \"content\": \"John is 30 years old\"}],\n    response_model=User,\n)\n\nprint(user)\n# > name='John' age=30\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic User Model in Python\nDESCRIPTION: This snippet defines a simple Pydantic model for a User with name and age fields. It's used to demonstrate the concept of partial responses in streaming.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/partial.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n```\n\n----------------------------------------\n\nTITLE: Audio Analysis with OpenAI\nDESCRIPTION: Example showing how to analyze audio files using OpenAI with Instructor\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import Audio\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\n\nclass AudioDescription(BaseModel):\n    transcript: str\n    summary: str\n    speakers: list[str]\n    key_points: list[str]\n\n\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/gettysburg.wav\"\n\nclient = instructor.from_openai(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-audio-preview\",\n    response_model=AudioDescription,\n    modalities=[\"text\"],\n    audio={\"voice\": \"alloy\", \"format\": \"wav\"},\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Please transcribe and analyze this audio:\",\n                Audio.from_url(url),\n            ],\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Client with Gemini Models in Python\nDESCRIPTION: Demonstrates how to use the OpenAI client to interact with Gemini models for chat completions. This snippet shows the basic setup and a simple query extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/google-openai-client.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/\", api_key=\"YOUR_API_KEY\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gemini-1.5-flash\",\n    messages=[{\"role\": \"user\", \"content\": \"Extract name and age from: John is 30\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Practical Example of Table Extraction from Image in Python\nDESCRIPTION: This code snippet demonstrates the practical application of the table extraction method. It imports necessary modules and functions, sets up the OpenAI client, and defines the extract_table function for processing image URLs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extracting_tables.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef extract_table(url: str) -> Iterable[Table]:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Table],\n        max_tokens=1800,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Extract table from image.\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": url}},\n                ],\n            }\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing Writer Client and Extracting Structured Data with Instructor in Python\nDESCRIPTION: This snippet demonstrates how to initialize a Writer client using Instructor, define a Pydantic model, and extract structured data from a text prompt. It showcases the basic usage of the Writer integration with Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/writer-support.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom writerai import Writer\nfrom pydantic import BaseModel\n\n# Initialize Writer client\nclient = instructor.from_writer(Writer(api_key=\"your API key\"))\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Extract structured data\nuser = client.chat.completions.create(\n    model=\"palmyra-x-004\",\n    messages=[{\"role\": \"user\", \"content\": \"Extract: John is 30 years old\"}],\n    response_model=User,\n)\n\nprint(user)\n#> name='John' age=30\n```\n\n----------------------------------------\n\nTITLE: LLM Response Validation with Reasking\nDESCRIPTION: Implements validation with automatic reasking mechanism for handling objectionable content.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-1-validation-rag.ipynb#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import llm_validator\n\nNotEvilAnswer = Annotated[\n    str,\n    AfterValidator(llm_validator(\"don't say objectionable things\", client=client)),\n]\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: NotEvilAnswer\n```\n\n----------------------------------------\n\nTITLE: Description-Based Synthetic Data Generation\nDESCRIPTION: Synthetic data generation using field descriptions to influence the output, specifically generating French-sounding names with corresponding ages.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/fake-data.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\n\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str = Field(description=\"Fancy French sounding names\")\n    age: int\n\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_fake_users(count: int) -> Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Generate `{count}` synthetic users\"},\n        ],\n    )\n\n\nfor user in generate_fake_users(5):\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced Validation with Validators\nDESCRIPTION: Shows how to implement complex validation rules using Pydantic's validator decorators.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, field_validator\nfrom datetime import date\n\nclass Reservation(BaseModel):\n    check_in: date\n    check_out: date\n    guests: int = Field(ge=1)\n\n    @field_validator(\"check_out\")\n    def check_dates(cls, v, values):\n        if \"check_in\" in values.data and v <= values.data[\"check_in\"]:\n            raise ValueError(\"check_out must be after check_in\")\n        return v\n```\n\n----------------------------------------\n\nTITLE: Template Variables Usage with Gemini\nDESCRIPTION: Shows how to use template variables for dynamic content with Google GenAI SDK\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom google import genai\nimport instructor\nfrom pydantic import BaseModel\nfrom google.genai import types\n\n\n# Define your Pydantic model\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Initialize and patch the client\nclient = genai.Client()\nclient = instructor.from_genai(client, mode=instructor.Mode.GENAI_TOOLS)\n\n# Single string (converted to user message)\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    messages=[\"{{name}} is {{ age }} years old\"],\n    response_model=User,\n    context={\n        \"name\": \"Jason\",\n        \"age\": 25,\n    },\n)\n\nprint(response)\n# > name='Jason' age=25\n\n# Standard format\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"{{ name }} is {{ age }} years old\"}],\n    response_model=User,\n    context={\n        \"name\": \"Jason\",\n        \"age\": 25,\n    },\n)\n\nprint(response)\n# > name='Jason' age=25\n\n# Using genai's Content type\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    messages=[\n        genai.types.Content(\n            role=\"user\",\n            parts=[genai.types.Part.from_text(text=\"{{name}} is {{age}} years old\")],\n        )\n    ],\n    response_model=User,\n    context={\n        \"name\": \"Jason\",\n        \"age\": 25,\n    },\n)\n\nprint(response)\n# > name='Jason' age=25\n```\n\n----------------------------------------\n\nTITLE: Partial Streaming with Gemini\nDESCRIPTION: Demonstrates how to stream partial responses when extracting user data with Gemini model. Useful for processing responses as they arrive.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/google.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    bio: str\n\n\nuser = client.chat.completions.create_partial(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Create a user profile for Jason and 1 sentence bio, age 25\",\n        },\n    ],\n    response_model=User,\n)\n\nfor user_partial in user:\n    print(user_partial)\n```\n\n----------------------------------------\n\nTITLE: Defining Extraction Model for RAG Enhancement\nDESCRIPTION: Creates a Pydantic model for structured extraction of topics, summaries, hypothetical questions, and keywords from text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Extraction(BaseModel):\n    topic: str\n    summary: str\n    hypothetical_questions: list[str] = Field(\n        default_factory=list,\n        description=\"Hypothetical questions that this document could answer\",\n    )\n    keywords: list[str] = Field(\n        default_factory=list, description=\"Keywords that this document is about\"\n    )\n```\n\n----------------------------------------\n\nTITLE: OpenAI Streaming Implementation\nDESCRIPTION: Demonstrates how to implement streaming responses from OpenAI's API using generators for real-time token processing.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generator.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Set your OpenAI API key\nclient = OpenAI(\n    api_key=\"My API Key\",\n)\n\nresponse_generator = client.chat.completions.create(\n    model='gpt-3.5-turbo',\n    messages=[{'role': 'user', 'content': \"What are some good reasons to smile?\"}],\n    temperature=0,\n    stream=True,\n)\n\nfor chunk in response_generator:\n    print(chunk.choices[0].delta.content, end=\"\")\n```\n\n----------------------------------------\n\nTITLE: Product Taxonomy Definition in YAML\nDESCRIPTION: Defines a hierarchical product taxonomy for fashion items including tops, bottoms and colors categories.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extracting-model-metadata.md#2025-04-14_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntops:\n  t-shirts:\n    - crew_neck\n    - v_neck\n    - graphic_tees\n  sweaters:\n    - crewneck\n    - cardigan\n    - pullover\n  jackets:\n    - bomber_jackets\n    - denim_jackets\n    - leather_jackets\n\nbottoms:\n  pants:\n    - chinos\n    - dress_pants\n    - cargo_pants\n  shorts:\n    - athletic_shorts\n    - cargo_shorts\n\ncolors:\n  - black\n  - navy\n  - white\n  - beige\n  - brown\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Validation with LLM Validator\nDESCRIPTION: Demonstrates how to add custom validation using llm_validator to catch and prevent objectionable content in responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/self_critique.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instructor import llm_validator\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\n                \"don't say objectionable things\", client=client, allow_override=True\n            )\n        ),\n    ]\n\ntry:\n    qa: QuestionAnswerNoEvil = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=QuestionAnswerNoEvil,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n            },\n        ],\n    )\nexcept Exception as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Basic Pydantic Model Definition for LLM Response\nDESCRIPTION: Demonstrates how to create a basic Pydantic model with field descriptions that will be used to generate LLM prompts. The model includes name and age fields with descriptions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/models.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    \"\"\"\n    This is the prompt that will be used to generate the response.\n    Any instructions here will be passed to the language model.\n    \"\"\"\n\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Groq AI Integration\nDESCRIPTION: Implementation of asynchronous structured output generation using Groq AI with AsyncGroq client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/groq.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom groq import AsyncGroq\nimport instructor\nfrom pydantic import BaseModel\nimport asyncio\n\n# Initialize with API key\nclient = AsyncGroq(api_key=os.getenv(\"GROQ_API_KEY\"))\n\n# Enable instructor patches for Groq client\nclient = instructor.from_groq(client)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_user():\n    user = await client.chat.completions.create(\n        model=\"llama3-groq-70b-8192-tool-use-preview\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n        ],\n        response_model=User,\n    )\n    return user\n\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)\n# > User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Character Extraction with Instructor and Anthropic Claude\nDESCRIPTION: Demonstrates setting up Instructor with Anthropic's Claude model to extract character information from literary text. Creates a Pydantic model for character data and uses prompt caching for efficient processing. Shows how to handle both inline text and file-based input with proper cache control parameters.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompt_caching.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\nclient = instructor.from_anthropic(Anthropic())\n\n\nclass Character(BaseModel):\n    name: str\n    description: str\n\n\n# Note: For testing this example locally, create a book.txt file with content like:\n# Sample book.txt content:\n# \"Pride and Prejudice by Jane Austen\n#\n# It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n# However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is\n# so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or\n# other of their daughters...\"\nbook = \"\"\"\nPride and Prejudice by Jane Austen\n\nIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\nHowever little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is\nso well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or\nother of their daughters...\n\"\"\"\n\n# Uncomment to read from an actual file instead of using the sample text above\n# with open(\"./book.txt\") as f:\n#     book = f.read()\n\nresp, completion = client.chat.completions.create_with_completion(\n    model=\"claude-3-5-sonnet-20240620\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"<book>\" + book + \"</book>\",\n                    \"cache_control\": {\"type\": \"ephemeral\"},  # (1)!\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Extract a character from the text given above\",\n                },\n            ],\n        },\n    ],\n    response_model=Character,\n    max_tokens=1000,\n)\n```\n\n----------------------------------------\n\nTITLE: Nested Model Extraction with Mistral and Instructor\nDESCRIPTION: Python code demonstrating extraction of nested models using Mistral and Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import List\nimport os\nfrom mistralai import Mistral\nfrom instructor import from_mistral, Mode\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: List[Address]\n\n# Initialize with API key\nclient = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n\n# Enable instructor patches for Mistral client\ninstructor_client = from_mistral(\n    client=client,\n    mode=Mode.MISTRAL_TOOLS,\n)\n\n# Create structured output with nested objects\nuser = instructor_client.chat.completions.create(\n    response_model=User,\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n            Extract: Jason is 25 years old.\n            He lives at 123 Main St, New York, USA\n            and has a summer house at 456 Beach Rd, Miami, USA\n        \"\"\"}\n    ],\n    model=\"mistral-large-latest\",\n    temperature=0,\n)\n\nprint(user)\n# Output:\n# User(\n#     name='Jason',\n#     age=25,\n#     addresses=[\n#         Address(street='123 Main St', city='New York', country='USA'),\n#         Address(street='456 Beach Rd', city='Miami', country='USA')\n#     ]\n# )\n```\n\n----------------------------------------\n\nTITLE: Name Validation with Pydantic Model\nDESCRIPTION: Example of a Pydantic model using a validator to ensure names contain spaces and converting them to lowercase.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import Annotated\nfrom pydantic import AfterValidator\n\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\n\nperson = UserDetail(age=29, name=\"Jason\")\n```\n\n----------------------------------------\n\nTITLE: Defining Hero Model with SQLModel and Instructor in Python\nDESCRIPTION: This snippet defines a Hero model that inherits from both SQLModel and instructor.OpenAISchema. It includes fields for id, name, secret_name, and age. The id field uses SkipJsonSchema to prevent the LLM from generating a UUID.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/sqlmodel.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom uuid import UUID, uuid4\nfrom pydantic.json_schema import SkipJsonSchema\nfrom sqlmodel import Field, SQLModel\nimport instructor\n\n\nclass Hero(SQLModel, instructor.OpenAISchema, table=True):\n    id: SkipJsonSchema[UUID] = Field(default_factory=lambda: uuid4(), primary_key=True)\n    name: str\n    secret_name: str\n    age: Optional[int] = None\n```\n\n----------------------------------------\n\nTITLE: Enhanced Data Model for Future Implementations\nDESCRIPTION: Defines extended Pydantic models for future features including timestamp tracking and enhanced recommendation details.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/multimodal-gemini.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TimestampedRecommendation(BaseModel):\n    timestamp: str\n    timestamp_format: Literal[\"HH:MM\", \"HH:MM:SS\"]  # Helps with parsing\n    recommendation: str\n\n\nclass EnhancedRecommendations(BaseModel):\n    destinations: list[TouristDestination]\n    timestamped_mentions: list[TimestampedRecommendation]\n```\n\n----------------------------------------\n\nTITLE: Processing PDFs with Gemini AI's Files API\nDESCRIPTION: Shows how to use PDFs with Gemini's Files API using the PDFWithGenaiFile class to handle existing uploaded files or uploading new ones. Includes options for retrying uploads and handling file processing timeouts.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nPDFWithGenaiFile.from_new_genai_file(\n    \"./invoice.pdf\",\n    retry_delay=1,  # Time to wait before checking if file is ready to use\n    max_retries=20 # Number of times to check before throwing an error\n),\n```\n\n----------------------------------------\n\nTITLE: Creating Flexible Schemas with Key-Value Pairs in Pydantic\nDESCRIPTION: This code shows how to implement flexible property storage using nested Pydantic models. It defines a Property class for key-value pairs and incorporates a list of these properties in the UserDetail model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property]\n```\n\n----------------------------------------\n\nTITLE: PDF Analysis with OpenAI\nDESCRIPTION: Example demonstrating PDF analysis and data extraction using OpenAI with Instructor\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import PDF\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\n\nclass Receipt(BaseModel):\n    total: int\n    items: list[str]\n\n\nclient = instructor.from_openai(OpenAI())\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\n# Multiple ways to load an PDF:\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=Receipt,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Extract out the total and line items from the invoice\",\n                PDF.from_url(url),\n            ],\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing MarkdownDataFrame conversion and extraction in Python\nDESCRIPTION: This code snippet defines a custom MarkdownDataFrame type and functions to extract data from text into Pandas DataFrames. It uses OpenAI's API with the instructor library to process natural language into structured data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/pandas_df.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import (\n    BaseModel,\n    BeforeValidator,\n    PlainSerializer,\n    InstanceOf,\n    WithJsonSchema,\n)\nimport pandas as pd\nimport instructor\nimport openai\n\n\ndef md_to_df(data: Any) -> Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    # Validates final type\n    InstanceOf[pd.DataFrame],\n    # Converts markdown to DataFrame\n    BeforeValidator(md_to_df),\n    # Converts DataFrame to markdown on model_dump_json\n    PlainSerializer(lambda df: df.to_markdown()),\n    # Adds a description to the type\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n            The markdown representation of the table,\n            each one should be tidy, do not try to join\n            tables that should be seperate\"\"\",\n        }\n    ),\n]\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef extract_df(data: str) -> pd.DataFrame:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MarkdownDataFrame,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data extraction system, table of writing perfectly formatted markdown tables.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Extract the data into a table: {data}\",\n            },\n        ],\n    )\n\n\nclass Table(BaseModel):\n    title: str\n    data: MarkdownDataFrame\n\n\ndef extract_table(data: str) -> Table:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Table,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data extraction system, table of writing perfectly formatted markdown tables.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Extract the data into a table: {data}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    df = extract_df(\n        \"\"\"Create a table of the last 5 presidents of the United States,\n        including their party and the years they served.\"\"\"\n    )\n    assert isinstance(df, pd.DataFrame)\n    print(df)\n    \"\"\"\n                         Party          Years Served\n     President\n    Joe Biden                  Democrat  2021 - Present\n    Donald Trump             Republican     2017 - 2021\n    Barack Obama               Democrat     2009 - 2017\n    George W. Bush           Republican     2001 - 2009\n    Bill Clinton               Democrat     1993 - 2001\n    \"\"\"\n\n    table = extract_table(\n        \"\"\"Create a table of the last 5 presidents of the United States,\n        including their party and the years they served.\"\"\"\n    )\n    assert isinstance(table, Table)\n    assert isinstance(table.data, pd.DataFrame)\n    print(table.title)\n    #> Last 5 Presidents of the United States\n    print(table.data)\n    \"\"\"\n                         Party  Years Served\n     President\n    Joe Biden        Democratic     2021-2025\n    Donald Trump     Republican     2017-2021\n    Barack Obama     Democratic     2009-2017\n    George W. Bush   Republican     2001-2009\n    Bill Clinton     Democratic     1993-2001\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generating Styled Emails with Instructor and OpenAI\nDESCRIPTION: This code demonstrates how to generate emails with a specific tone using Instructor and Pydantic. It creates a structured Email model and uses OpenAI's GPT-4o to generate formal or informal emails based on subject, recipient, sender, and specified tone.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/zero_shot/style_prompting.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nimport openai\n\n\nclass Email(BaseModel):\n    subject: str\n    message: str\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef generate_email(subject, to, sender, tone):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Write an email about {subject} to {to} from {sender}.\n                The email should be {tone}.\n                \"\"\",\n            }\n        ],\n        response_model=Email,\n    )\n\n\nif __name__ == \"__main__\":\n    email = generate_email(\n        subject=\"invitation to all-hands on Monday at 6pm\",\n        to=\"John Smith\",\n        sender=\"Jane Doe\",\n        tone=\"formal\",\n    )\n\n    print(email.subject)\n    #> Invitation to All-Hands Meeting\n    print(email.message)\n    \"\"\"\n    Dear Mr. Smith,\n\n    I hope this message finds you well. I am writing to formally invite you to our upcoming all-hands meeting scheduled for Monday at 6:00 PM. This meeting is an important opportunity for us to come together, discuss key updates, and align on our strategic goals.\n\n    Please confirm your availability at your earliest convenience. Your presence and contributions to the discussion would be greatly valued.\n\n    Thank you and I look forward to your confirmation.\n\n    Warm regards,\n\n    Jane Doe\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing a Choose Your Own Adventure Story Structure in Mermaid\nDESCRIPTION: A mermaid diagram showing how a Choose Your Own Adventure story can be represented as a DAG. The diagram displays a story root with three initial choices, each leading to two sub-choices, demonstrating the branching narrative structure.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extract-model-looks.md#2025-04-14_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A[Story Root] --> B[Choice 1]\n    A --> C[Choice 2]\n    A --> D[Choice 3]\n    B --> E[Choice 1.1]\n    B --> F[Choice 1.2]\n    C --> G[Choice 2.1]\n    C --> H[Choice 2.2]\n    D --> I[Choice 3.1]\n    D --> J[Choice 3.2]\n```\n\n----------------------------------------\n\nTITLE: Validation and Retries Implementation\nDESCRIPTION: Shows how to implement validation rules and automatic retries for data extraction\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\nfrom pydantic import AfterValidator, BaseModel\nimport instructor\nfrom google import genai\n\n\ndef uppercase_validator(v: str) -> str:\n    if v.islower():\n        raise ValueError(\"Name must be ALL CAPS\")\n    return v\n\n\nclass UserDetail(BaseModel):\n    name: Annotated[str, AfterValidator(uppercase_validator)]\n    age: int\n\n\nclient = instructor.from_genai(genai.Client())\n\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Extract: jason is 25 years old\"}],\n    response_model=UserDetail,\n    max_retries=3,\n)\n\nprint(response)  # UserDetail(name='JASON', age=25)\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Substring Check for Citation Verification in Python\nDESCRIPTION: This snippet defines a Pydantic model to verify if a given substring quote exists within a text chunk. It uses a field validator to check for the substring and raises an error if not found.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/citations.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom openai import OpenAI\nfrom pydantic import BaseModel, ValidationInfo, field_validator\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @field_validator(\"substring_quote\")\n    @classmethod\n    def substring_quote_exists(cls, v: str, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        for text_chunk in context.values():\n            if v in text_chunk:  # (1)\n                return v\n        raise ValueError(\"Could not find substring_quote `{v}` in contexts\")\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n```\n\n----------------------------------------\n\nTITLE: Streaming with Union Types\nDESCRIPTION: Demonstrates how to handle streaming responses with union types for different content types.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/unions.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef stream_content():\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Message,\n        stream=True,\n        messages=[{\"role\": \"user\", \"content\": \"Generate mixed content\"}],\n    )\n    for partial in response:\n        if partial.content:\n            for item in partial.content:\n                if isinstance(item, TextContent):\n                    print(f\"Text: {item.text}\")\n                elif isinstance(item, ImageContent):\n                    print(f\"Image: {item.url}\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Repetitiveness Score in COSP - LaTeX Formula\nDESCRIPTION: Mathematical formula for computing repetitiveness score by measuring cosine similarity between phrases in the reasoning chain. Higher scores indicate more redundancy in the generated text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/cosp.md#2025-04-14_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\nR_r(r_j^{(i)}) = \\frac{2}{Q(Q-1)} \\sum_{a=1}^{Q} \\sum_{b=a+1}^{Q} W_{ab}\n```\n\n----------------------------------------\n\nTITLE: Defining Validation Function Structure in Python\nDESCRIPTION: Demonstrates the basic structure of a validation function that checks a condition and raises an error or returns a mutated value.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n```\n\n----------------------------------------\n\nTITLE: Direct List Extraction without Wrapper Model\nDESCRIPTION: Shows how to extract a list of books directly without using a wrapper model. Demonstrates simplified list extraction using List[Book] as the response model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/list_extraction.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Book(BaseModel):\n    title: str\n    author: str\n    publication_year: int\n\n# Extract a list directly\nbooks = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\"\"\n        Classic novels:\n        1. To Kill a Mockingbird by Harper Lee (1960)\n        2. 1984 by George Orwell (1949)\n        3. The Great Gatsby by F. Scott Fitzgerald (1925)\n        \"\"\"}\n    ],\n    response_model=List[Book]  # Direct list extraction\n)\n\n# Access the extracted data\nfor book in books:\n    print(f\"{book.title} by {book.author} ({book.publication_year})\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Parea with OpenAI Client\nDESCRIPTION: Setup code to integrate Parea with OpenAI client and Instructor. Includes environment loading and client initialization with proper wrapping for monitoring.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/parea.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport instructor\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom parea import Parea\n\nload_dotenv()\n\nclient = OpenAI()\n\np = Parea(api_key=os.getenv(\"PAREA_API_KEY\"))\np.wrap_openai_client(client, \"instructor\")\n\nclient = instructor.from_openai(client)\n```\n\n----------------------------------------\n\nTITLE: Viewing Download-File Command Help\nDESCRIPTION: Shows the help menu for the download-file command, which downloads results from a completed batch job to a specified file path.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ instructor batch download-file --help\n\n Usage: instructor batch download-file [OPTIONS]\n\n Download the file associated with a batch job\n\n╭─ Options ───────────────────────────────────────────────────────────────────────────╮\n│ *  --batch-id           TEXT  Batch job ID to download [default: None] [required]   │\n│ *  --download-file-path TEXT  Path to download file to [default: None] [required]   │\n│    --use-anthropic           Use Anthropic API instead of OpenAI                    │\n│                              [default: False]                                       │\n│    --help                    Show this message and exit.                            │\n╰─────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n----------------------------------------\n\nTITLE: Defining Burr Actions for Application Flow\nDESCRIPTION: Creates action functions decorated with Burr's @action to handle different steps of the flashcard generation process.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-flashcards.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom burr.core import action, State\n\n\n@action(reads=[], writes=[\"youtube_url\"])\ndef process_user_input(state: State, user_input: str) -> State:\n    \"\"\"Process user input and update the YouTube URL.\"\"\"\n    youtube_url = (\n        user_input  # In practice, we would have more complex validation logic.\n    )\n    return state.update(youtube_url=youtube_url)\n\n\n@action(reads=[\"youtube_url\"], writes=[\"transcript\"])\ndef get_youtube_transcript(state: State) -> State:\n    \"\"\"Get the official YouTube transcript for a video given it's URL\"\"\"\n    youtube_url = state[\"youtube_url\"]\n\n    _, _, video_id = youtube_url.partition(\"?v=\")\n    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n    full_transcript = \" \".join([entry[\"text\"] for entry in transcript])\n\n    # store the transcript in state\n    return state.update(transcript=full_transcript, youtube_url=youtube_url)\n\n\n@action(reads=[\"transcript\", \"youtube_url\"], writes=[\"question_answers\"])\ndef generate_question_and_answers(state: State) -> State:\n    \"\"\"Generate `QuestionAnswer` from a YouTube transcript using an LLM.\"\"\"\n    # read the transcript from state\n    transcript = state[\"transcript\"]\n    youtube_url = state[\"youtube_url\"]\n\n    # create the instructor client\n    instructor_client = instructor.from_openai(openai.OpenAI())\n    system_prompt = (\n        \"Analyze the given YouTube transcript and generate question-answer pairs\"\n        \" to help study and understand the topic better. Please rate all questions from 1 to 5\"\n        \" based on their difficulty.\"\n    )\n    response = instructor_client.chat.completions.create_iterable(\n        model=\"gpt-4o-mini\",\n        response_model=QuestionAnswer,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": transcript},\n        ],\n    )\n\n    # iterate over QuestionAnswer, add the `youtube_url`, and append to state\n    for qna in response:\n        qna.youtube_url = youtube_url\n        # `State` is immutable, so `.append()` returns a new object with the appended value\n        state = state.append(question_answers=qna)\n\n    return state\n```\n\n----------------------------------------\n\nTITLE: Iterable Streaming Implementation\nDESCRIPTION: Example showing how to stream multiple user objects using the create_iterable method.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/deepseek.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport os\nfrom openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\n\n\n# Initialize with API key\nclient = instructor.from_openai(\n    OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Extract multiple users from text\nusers = client.chat.completions.create_iterable(\n    model=\"deepseek-chat\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract users:\n            1. Jason is 25 years old\n            2. Sarah is 30 years old\n            3. Mike is 28 years old\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Defining Response Model for Language-Aware Summarization\nDESCRIPTION: Creates a Pydantic model that defines the structure for a generated summary, including a field to track the detected language of the original text to ensure summaries are generated in the same language.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/matching-language.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass GeneratedSummary(BaseModel):\n    detected_language: str = Field(\n        description=\"The language code of the original article. The summary must be generated in this same language.\",\n    )\n    summary: str\n```\n\n----------------------------------------\n\nTITLE: Partial Streaming with Writer\nDESCRIPTION: Demonstrates how to use partial streaming to process structured data as it arrives from the API.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/writer.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom writerai import Writer\nfrom pydantic import BaseModel\n\nclient = instructor.from_writer(Writer())\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create_partial(\n    model=\"palmyra-x-004\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Ivan is 27 and lives in Singapore\",\n        }\n    ],\n    response_model=Person,\n)\n\nfor person in resp:\n    print(person)\n    # > name=None age=None\n    # > name='Ivan' age=None\n    # > name='Ivan' age=27\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Structure Validation in Python with Instructor\nDESCRIPTION: Shows how to apply validation at different levels in nested structures. This example includes validation for address fields within a person object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/field_validation.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, field_validator\nimport instructor\nfrom openai import OpenAI\nfrom typing import List\n\nclient = instructor.from_openai(OpenAI())\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n    \n    @field_validator('state')\n    @classmethod\n    def validate_state(cls, v):\n        valid_states = {\"CA\", \"NY\", \"TX\", \"FL\"}  # Example: just a few states\n        if v not in valid_states:\n            raise ValueError(f\"State must be one of: {', '.join(valid_states)}\")\n        return v\n    \n    @field_validator('zip_code')\n    @classmethod\n    def validate_zip(cls, v):\n        if not v.isdigit() or len(v) != 5:\n            raise ValueError(\"ZIP code must be 5 digits\")\n        return v\n\nclass Person(BaseModel):\n    name: str\n    addresses: List[Address]  # Nested structure with validation\n```\n\n----------------------------------------\n\nTITLE: Extracting Entities and Generating Graph from Legal Contract using Python\nDESCRIPTION: This code uses an AI model to analyze a sample legal contract, extract relevant entities and their relationships, and then generate a graphical representation of these entities. The result is saved as an image file named 'entity.gv'.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/entity_resolution.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncontent = \"\"\"\nSample Legal Contract\nAgreement Contract\n\nThis Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\").\n\nArticle 1: Scope of Work\n\nThe Service Provider will deliver the software product to the Client 30 days after the agreement date.\n\nArticle 2: Payment Terms\n\nThe total payment for the service is $50,000.\nAn initial payment of $10,000 will be made within 7 days of the the signed date.\nThe final payment will be due 45 days after [SignDate].\n\nArticle 3: Confidentiality\n\nThe parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.\n\nArticle 4: Termination\n\nThe contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].\n\"\"\"  # Your legal contract here\nmodel = ask_ai(content)\ngenerate_graph(model)\n```\n\n----------------------------------------\n\nTITLE: Executing Competitor Extraction on Sample Slides in Python\nDESCRIPTION: This code demonstrates the execution of the competitor extraction function on a sample slide URL. It prints the extracted data in JSON format, showing the structured information about competitors in the accommodation services industry.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extract_slides.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nurl = [\n    'https://miro.medium.com/v2/resize:fit:1276/0*h1Rsv-fZWzQUyOkt',\n]\nmodel = read_images(url)\nprint(model.model_dump_json(indent=2))\n\"\"\"\n{\n  \"industry_list\": [\n    {\n      \"name\": \"Accommodation Services\",\n      \"competitor_list\": [\n        {\n          \"name\": \"CouchSurfing\",\n          \"features\": [\n            \"Free accommodation\",\n            \"Cultural exchange\",\n            \"Community-driven\",\n            \"User profiles and reviews\"\n          ]\n        },\n        {\n          \"name\": \"Craigslist\",\n          \"features\": [\n            \"Local listings\",\n            \"Variety of accommodation types\",\n            \"Direct communication with hosts\",\n            \"No booking fees\"\n          ]\n        },\n        {\n          \"name\": \"BedandBreakfast.com\",\n          \"features\": [\n            \"Specialized in B&Bs\",\n            \"User reviews\",\n            \"Booking options\",\n            \"Local experiences\"\n          ]\n        },\n        {\n          \"name\": \"AirBed & Breakfast (Airbnb)\",\n          \"features\": [\n            \"Wide range of accommodations\",\n            \"User reviews\",\n            \"Instant booking\",\n            \"Host profiles\"\n          ]\n        },\n        {\n          \"name\": \"Hostels.com\",\n          \"features\": [\n            \"Budget-friendly hostels\",\n            \"User reviews\",\n            \"Booking options\",\n            \"Global reach\"\n          ]\n        },\n        {\n          \"name\": \"RentDigs.com\",\n          \"features\": [\n            \"Rental listings\",\n            \"User-friendly interface\",\n            \"Local listings\",\n            \"Direct communication with landlords\"\n          ]\n        },\n        {\n          \"name\": \"VRBO\",\n          \"features\": [\n            \"Vacation rentals\",\n            \"Family-friendly options\",\n            \"User reviews\",\n            \"Booking protection\"\n          ]\n        },\n        {\n          \"name\": \"Hotels.com\",\n          \"features\": [\n            \"Wide range of hotels\",\n            \"Rewards program\",\n            \"User reviews\",\n            \"Price match guarantee\"\n          ]\n        }\n      ]\n    }\n  ]\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Anthropic Mode Usage Examples\nDESCRIPTION: Shows Anthropic mode configurations for both complex structures using ANTHROPIC_TOOLS with Claude 3+ and simpler extractions using ANTHROPIC_JSON.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# For Claude 3+ with complex structures\nclient = instructor.from_anthropic(Anthropic(), mode=instructor.Mode.ANTHROPIC_TOOLS)\n\n# For simpler extractions or older Claude models\nclient = instructor.from_anthropic(Anthropic(), mode=instructor.Mode.ANTHROPIC_JSON)\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with Gemini SDK\nDESCRIPTION: Python code demonstrating how to use the instructor library with Gemini SDK to extract structured data. It defines a User model and uses it to process a chat completion request.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/announcing-gemini-tool-calling-support.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",  # (1)!\n    )\n)\n\nresp = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#> name='Jason' age=25\n```\n\n----------------------------------------\n\nTITLE: Nested Object Handling with Perplexity\nDESCRIPTION: Demonstrates handling nested objects using Pydantic models with Perplexity AI, including complex data structures with lists and nested models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/perplexity.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\n\n# Initialize with API key\nclient = OpenAI(\n    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n    base_url=\"https://api.perplexity.ai\"\n)\n\n# Enable instructor patches for Perplexity client\nclient = instructor.from_perplexity(client)\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\n# Create structured output with nested objects\nuser = client.chat.completions.create(\n    model=\"sonar-medium-online\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract: Jason is 25 years old.\n            He lives at 123 Main St, New York, USA\n            and has a summer house at 456 Beach Rd, Miami, USA\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Response for Text Classification\nDESCRIPTION: This JSON snippet shows the structure of the response from the text classification system. It includes the input texts and their corresponding predictions with tag IDs and names.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"texts\": [\n    \"What is your phone number?\",\n    \"What is your email address?\",\n    \"What is your address?\",\n    \"What is your privacy policy?\"\n  ],\n  \"predictions\": [\n    {\n      \"id\": 1,\n      \"name\": \"phone\"\n    },\n    {\n      \"id\": 2,\n      \"name\": \"email\"\n    },\n    {\n      \"id\": 3,\n      \"name\": \"address\"\n    },\n    {\n      \"id\": 4,\n      \"name\": \"Other\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Improved Timestamp Handling with Pydantic and Custom Validation\nDESCRIPTION: An enhanced implementation that combines Pydantic data validation with a custom validator to handle different timestamp formats (HH:MM:SS and MM:SS) and normalize them to a consistent output format.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/timestamp.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, model_validator\nfrom typing import Literal\n\n\nclass SegmentWithTimestamp(BaseModel):\n    title: str = Field(..., description=\"The title of the segment\")\n    time_format: Literal[\"HH:MM:SS\", \"MM:SS\"] = Field(\n        ..., description=\"The format of the timestamp\"\n    )\n    timestamp: str = Field(\n        ..., description=\"The timestamp of the event as either HH:MM:SS or MM:SS\"\n    )\n\n    @model_validator(mode=\"after\")\n    def parse_timestamp(self):\n        if self.time_format == \"HH:MM:SS\":\n            hours, minutes, seconds = map(int, self.timestamp.split(\":\"))\n        elif self.time_format == \"MM:SS\":\n            hours, minutes, seconds = 0, *map(int, self.timestamp.split(\":\"))\n        else:\n            raise ValueError(\"Invalid time format, must be HH:MM:SS or MM:SS\")\n\n        # Normalize seconds and minutes\n        total_seconds = hours * 3600 + minutes * 60 + seconds\n        hours, remainder = divmod(total_seconds, 3600)\n        minutes, seconds = divmod(remainder, 60)\n\n        if hours > 0:\n            self.timestamp = f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n        else:\n            self.timestamp = f\"00:{minutes:02d}:{seconds:02d}\"\n\n        return self\n```\n\n----------------------------------------\n\nTITLE: Basic Person Model Definition\nDESCRIPTION: Definition of a simple Person model using Pydantic BaseModel with name and age fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/first_extraction.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Person(BaseModel):\n    name: str\n    age: int\n```\n\n----------------------------------------\n\nTITLE: Generator vs List Performance Comparison\nDESCRIPTION: Compares performance between generator and list comprehension approaches for processing data, including timing measurements for first result retrieval.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generator.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\n\ndef expensive_func(x):\n    \"\"\"Simulate an expensive operation.\"\"\"\n    time.sleep(1)\n    return x**2\n\n\ndef calculate_time_for_first_result_with_list(func_input, func):\n    \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = [func(x) for x in func_input][0]\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n    #> Time for first result (list): 5.02 seconds\n    return result\n\n\ndef calculate_time_for_first_result_with_generator(func_input, func):\n    \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = next(func(x) for x in func_input)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    #> Time for first result (generator): 1.00 seconds\n    return result\n\n\n# Prepare inputs for the function\nnumbers = [1, 2, 3, 4, 5]\n\n# Benchmarking\nfirst_result_list = calculate_time_for_first_result_with_list(numbers, expensive_func)\nfirst_result_gen = calculate_time_for_first_result_with_generator(\n    numbers, expensive_func\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Models for Tourist Data\nDESCRIPTION: Creates Pydantic models to structure the output data for tourist destinations and recommendations. Includes models for individual destinations and a container for recommendations with chain of thought reasoning.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/multimodal-gemini.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass TouristDestination(BaseModel):\n    name: str\n    description: str\n    location: str\n\n\nclass Recommendations(BaseModel):\n    chain_of_thought: str\n    description: str\n    destinations: list[TouristDestination]\n```\n\n----------------------------------------\n\nTITLE: Calculating tokens in a sentence using NLTK in Python\nDESCRIPTION: This snippet calculates the number of tokens in a sentence using NLTK. First, the sentence is tokenized using `nltk.word_tokenize`, and then the length of the resulting token list is calculated using `len()` function to get the total number of tokens.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsentence = \"I'm fascinated by machine learning!\"\ntokens = nltk.word_tokenize(sentence)\nprint(tokens)\nprint(len(tokens))\n```\n\n----------------------------------------\n\nTITLE: Uploading Video File for Analysis\nDESCRIPTION: Uploads a video file to be processed by the Gemini model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/multimodal-gemini.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfile = genai.upload_file(\"./takayama.mp4\")\n```\n\n----------------------------------------\n\nTITLE: Using ImageWithCacheControl for Anthropic Prompt Caching in Python\nDESCRIPTION: This snippet demonstrates how to use the ImageWithCacheControl class to leverage Anthropic's prompt caching feature. It shows how to create an image instance with cache control parameters for efficient image analysis.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/multimodal.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom instructor.multimodal import ImageWithCacheControl\nimport anthropic\nfrom pydantic import BaseModel\n\n\nclass ImageDescription(BaseModel):\n    description: str\n    items: list[str]\n\n\n# Download a sample image for demonstration\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/image.jpg\"\n\nclient = instructor.from_anthropic(anthropic.Anthropic())\n\nresponse, completion = client.chat.completions.create_with_completion(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=ImageDescription,\n    autodetect_images=True,  # Set this to True\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"What is in this image?\",\n                ImageWithCacheControl.from_image_params(\n                    {\n                        \"source\": url,\n                        \"cache_control\": {\n                            \"type\": \"ephemeral\",\n                        },\n                    }\n                ),\n            ],\n        }\n    ],\n    max_tokens=1000,\n)\n\nprint(response)\n# > description='A bush with numerous clusters of blueberries surrounded by green leaves, under a cloudy sky.' items=['blueberries', 'green leaves', 'cloudy sky']\n\nprint(completion.usage.cache_creation_input_tokens)\n# > 1820\n```\n\n----------------------------------------\n\nTITLE: GPT-4 Vision API Integration\nDESCRIPTION: Shows how to make an API call to GPT-4 vision using Instructor to process images and extract structured metadata.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extracting-model-metadata.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"\nYou are a helpful assistant. You are given a list of images and you need to map the person style of the person in the image to a given taxonomy.\n\nHere is the taxonomy that you should use\n\nColors:\n{% for color in colors %}\n* {{ color }}\n{% endfor %}\n\nCategories:\n{% for category in categories %}\n* {{ category }}\n{% endfor %}\n\nSubcategories:\n{% for subcategory in subcategories %}\n* {{ subcategory }}\n{% endfor %}\n\nProduct types:\n{% for product_type in product_types %}\n* {{ product_type }}\n{% endfor %}\n\"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Here are the images of the person, describe the personal style of the person in the image from a first-person perspective( Eg. You are ... )\",\n                *images,\n            ],\n        },\n    ],\n    response_model=PersonalStyle,\n    context={\n        \"colors\": colors,\n        \"categories\": list(categories),\n        \"subcategories\": list(subcategories),\n        \"product_types\": list(product_types),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Synchronous Structured Output Generation with Perplexity\nDESCRIPTION: Demonstrates basic synchronous implementation of structured output generation using Perplexity AI and Pydantic models. Creates a simple User model with name and age fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/perplexity.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\n\n# Initialize with API key\nclient = OpenAI(\n    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n    base_url=\"https://api.perplexity.ai\"\n)\n\n# Enable instructor patches for Perplexity client\nclient = instructor.from_perplexity(client)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Create structured output\nuser = client.chat.completions.create(\n    model=\"sonar-medium-online\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n    ],\n    response_model=User,\n)\n\nprint(user)\n# > User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Package via Pip\nDESCRIPTION: Simple pip command to install the Instructor package. Requires Python 3.9+ and pip package manager. Key dependencies include openai, typer, docstring-parser, and pydantic packages.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/installation.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\n```\n\n----------------------------------------\n\nTITLE: Implementing Field Validation Rules\nDESCRIPTION: Demonstrates how to add validation rules to fields using Pydantic's Field class with various constraints.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass Product(BaseModel):\n    name: str = Field(min_length=3)\n    price: float = Field(gt=0)  # greater than 0\n    quantity: int = Field(ge=0)  # greater than or equal to 0\n    description: str = Field(max_length=500)\n```\n\n----------------------------------------\n\nTITLE: Implementing User Profile Extraction with Pydantic and Instructor\nDESCRIPTION: Demonstrates how to create a Pydantic model for user data and use it with Instructor to extract structured information from text. The example shows extracting name, age, and email information using a defined User model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/using_json.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    email: str\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the user's name, age, and email from this: John Doe is 25 years old. His email is john@example.com\",\n        }\n    ],\n)\n\nprint(user.model_dump())\n#> {\n#     \"name\": \"John Doe\",\n#     \"age\": 25,\n#     \"email\": \"john@example.com\"\n#   }\n```\n\n----------------------------------------\n\nTITLE: Switching Providers with Instructor in Python\nDESCRIPTION: Demonstrates the ease of switching between providers using Instructor. This snippet shows how to switch from OpenAI to Anthropic by changing just a few lines of code.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/google-openai-client.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom instructor import from_openai\n\nclient = from_openai(\n    OpenAI()\n)\n\n# rest of code\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom anthropic import Anthropic\nfrom instructor import from_anthropic\n\nclient = from_anthropic(Anthropic())\n\n# rest of code\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Model for Quiz Generator in Python\nDESCRIPTION: This Python code defines Pydantic models for generating quizzes from YouTube video transcripts. It includes models for quiz questions and the overall video quiz with fields for questions, options, correct answers, and explanations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-transcripts.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass QuizQuestion(BaseModel):\n    question: str = Field(..., description=\"The quiz question\")\n    options: List[str] = Field(\n        ..., min_items=2, max_items=4, description=\"Possible answers to the question\"\n    )\n    correct_answer: int = Field(\n        ...,\n        ge=0,\n        lt=4,\n        description=\"The index of the correct answer in the options list\",\n    )\n    explanation: str = Field(\n        ..., description=\"An explanation of why the correct answer is correct\"\n    )\n\n\nclass VideoQuiz(BaseModel):\n    title: str = Field(\n        ..., description=\"The title of the quiz, based on the video content\"\n    )\n    questions: List[QuizQuestion] = Field(\n        ...,\n        min_items=5,\n        max_items=20,\n        description=\"A list of quiz questions based on the video content\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Synchronous SambaNova Integration\nDESCRIPTION: Example of using Instructor with SambaNova's LLM API synchronously. Demonstrates setting up the client with custom base URL and extracting structured data using a Pydantic model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/sambanova.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\nimport os\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"https://api.sambanova.ai/v1\",\n        api_key=os.environ[\"SAMBANOVA_API_KEY\"]\n    )\n)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nuser = client.chat.completions.create(\n    model=\"Meta-Llama-3.1-405B-Instruct\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Ivan is 28\"},\n    ],\n    response_model=User,\n)\n\nprint(user)\n# > User(name='Ivan', age=28)\n```\n\n----------------------------------------\n\nTITLE: Initializing Anthropic Client with Prompt Caching\nDESCRIPTION: Sets up the Instructor client with Anthropic's beta prompt caching functionality using the ANTHROPIC_TOOLS mode.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/anthropic-prompt-caching.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import Instructor, Mode, patch\nfrom anthropic import Anthropic\n\n\nclient = Instructor(\n    client=Anthropic(),\n    create=patch(\n        create=Anthropic().beta.prompt_caching.messages.create,\n        mode=Mode.ANTHROPIC_TOOLS,\n    ),\n    mode=Mode.ANTHROPIC_TOOLS,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Segmented Search Queries in Python\nDESCRIPTION: This code snippet demonstrates how to use the segment function to split a complex search query into multiple Search objects. It then iterates over the results and prints each segmented query as JSON.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/search.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfor search in segment(\"Search for a picture of a cat and a video of a dog\"):\n    print(search.model_dump_json())\n    #> {\"query\":\"picture of a cat\",\"type\":\"image\"}\n    #> {\"query\":\"video of a dog\",\"type\":\"video\"}\n```\n\n----------------------------------------\n\nTITLE: Metric Implementation - Reciprocal Rank\nDESCRIPTION: Implements Reciprocal Rank metric for evaluating search result rankings\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef rr(results, labels):\n    return max(\n        [\n            round(1 / (results.index(label) + 1), 2) if label in results else 0\n            for label in labels\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Decomposed Prompting System with Python Pydantic Models\nDESCRIPTION: A complete implementation of a decomposed prompting system using Pydantic models and OpenAI. The code defines models for splitting strings, extracting characters at specific positions, and merging strings, along with action planning functionality using LLM.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/decomposition/decomp.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import Union\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Split(BaseModel):\n    split_char: str = Field(\n        description=\"\"\"This is the character to split\n        the string with\"\"\"\n    )\n\n    def split_chars(self, s: str, c: str):\n        return s.split(c)\n\n\nclass StrPos(BaseModel):\n    index: int = Field(\n        description=\"\"\"This is the index of the character\n        we wish to return\"\"\"\n    )\n\n    def get_char(self, s: list[str], i: int):\n        return [c[i] for c in s]\n\n\nclass Merge(BaseModel):\n    merge_char: str = Field(\n        description=\"\"\"This is the character to merge the\n        inputs we plan to pass to this function with\"\"\"\n    )\n\n    def merge_string(self, s: list[str]):\n        return self.merge_char.join(s)\n\n\nclass Action(BaseModel):\n    id: int = Field(\n        description=\"\"\"Unique Incremental id to identify\n        this action with\"\"\"\n    )\n    action: Union[Split, StrPos, Merge]\n\n\nclass ActionPlan(BaseModel):\n    initial_data: str\n    plan: list[Action]\n\n\ndef derive_action_plan(task_description: str) -> ActionPlan:\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"Generate an action plan to help you complete\n                the task outlined by the user\"\"\",\n            },\n            {\"role\": \"user\", \"content\": task_description},\n        ],\n        response_model=ActionPlan,\n        max_retries=3,\n        model=\"gpt-4o\",\n    )\n\n\nif __name__ == \"__main__\":\n    task = \"\"\"Concatenate the second letter of every word in Jack\n    Ryan together\"\"\"\n    plan = derive_action_plan(task)\n    print(plan.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"initial_data\": \"Jack Ryan\",\n      \"plan\": [\n        {\n          \"id\": 1,\n          \"action\": {\n            \"split_char\": \" \"\n          }\n        },\n        {\n          \"id\": 2,\n          \"action\": {\n            \"index\": 1\n          }\n        },\n        {\n          \"id\": 3,\n          \"action\": {\n            \"merge_char\": \"\"\n          }\n        }\n      ]\n    }\n    \"\"\"\n\n    curr = plan.initial_data\n    cache = {}\n\n    for action in plan.plan:\n        if isinstance(action.action, Split) and isinstance(curr, str):\n            curr = action.action.split_chars(curr, action.action.split_char)\n        elif isinstance(action.action, StrPos) and isinstance(curr, list):\n            curr = action.action.get_char(curr, action.action.index)\n        elif isinstance(action.action, Merge) and isinstance(curr, list):\n            curr = action.action.merge_string(curr)\n        else:\n            raise ValueError(\"Unsupported Operation\")\n\n        print(action, curr)\n        #> id=1 action=Split(split_char=' ') ['Jack', 'Ryan']\n        #> id=2 action=StrPos(index=1) ['a', 'y']\n        #> id=3 action=Merge(merge_char='') ay\n\n    print(curr)\n    #> ay\n```\n\n----------------------------------------\n\nTITLE: Nested Object Extraction with Fireworks\nDESCRIPTION: Demonstrates handling complex nested data structures using Pydantic models with Fireworks AI\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/fireworks.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fireworks.client import Fireworks\nimport instructor\nfrom pydantic import BaseModel\n\n\n# Enable instructor patches\nclient = instructor.from_fireworks(Fireworks())\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\n# Create structured output with nested objects\nuser = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n                Extract: Jason is 25 years old.\n                He lives at 123 Main St, New York, USA\n                and has a summer house at 456 Beach Rd, Miami, USA\n            \"\"\",\n        }\n    ],\n    model=\"accounts/fireworks/models/llama-v3-8b-instruct\",\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Configurable User Data Extraction with Gemini\nDESCRIPTION: Demonstrates how to use configuration options like temperature and token limits when extracting user data with Gemini model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/google.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n    mode=instructor.Mode.GEMINI_JSON,\n)\n\n# note that client.chat.completions.create will also work\nresp = client.messages.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        },\n    ],\n    response_model=User,\n    generation_config={\n        \"temperature\": 0.5,\n        \"max_tokens\": 1000,\n        \"top_p\": 1,\n        \"top_k\": 32,\n    },\n)\n\nprint(resp)\n```\n\n----------------------------------------\n\nTITLE: Calling the say_hello method\nDESCRIPTION: This snippet shows how to call the `say_hello` method on the Character object extracted from the OpenAI API. This is used to print a formatted string containing the character's name, age, and house.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresp.say_hello()\n```\n\n----------------------------------------\n\nTITLE: Citation Validation Model\nDESCRIPTION: Pydantic model that validates if citations exist in the original text source using field validators.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import ValidationInfo, BaseModel, field_validator\n\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo):\n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text chunks\")\n        return v\n```\n\n----------------------------------------\n\nTITLE: Implementing Receipt Total Validation\nDESCRIPTION: Adds a model validator to the Receipt class that ensures the sum of individual item prices matches the total amount on the receipt. Raises ValueError if there's a mismatch.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extracting_receipts.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import model_validator\n\n\n@model_validator(mode=\"after\")\ndef check_total(self):\n    items = self.items\n    total = self.total\n    calculated_total = sum(item.price * item.quantity for item in items)\n    if calculated_total != total:\n        raise ValueError(\n            f\"Total {total} does not match the sum of item prices {calculated_total}\"\n        )\n    return self\n```\n\n----------------------------------------\n\nTITLE: Generating Initial Story Outline with GPT-4o in Python\nDESCRIPTION: Python function using instructor and Pydantic to generate a structured story outline with GPT-4o. The function creates a story setting, plot summary, initial choices, visual style, and image description based on user input parameters.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extract-model-looks.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass GeneratedStory(BaseModel):\n    setting: str\n    plot_summary: str\n    choices: List[str]\n    visual_style: str\n    image_description: str\n\nasync def generate_story(\n    client: instructor.AsyncInstructor,\n    story_input: RestateStoryInput\n):\n    resp = await client.chat.completions.create(\n        messages=[{\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Generate a story with:\n            - Setting: {{ story_input.setting}}\n            - Title: {{ story_input.title }}\n\n            Rules:\n            - Generate 2-4 initial choices that represent actions\n            - Choices must move story forward\n            - Include brief setting description\n            - Generate a visual description for the story\n\n            Required Elements:\n            1. Plot Summary: A vivid description of the setting and plot\n            2. Initial Choices: 2-4 distinct actions the user can take\n            3. Visual Style: Description of art style, color palette\n            4. Image Description: One-sentence scene description\n            \"\"\"\n        }],\n        model=\"gpt-4o\",\n        response_model=GeneratedStory,\n        context={\"story_input\": story_input},\n    )\n    return resp\n```\n\n----------------------------------------\n\nTITLE: Model Fine-Tuning Command\nDESCRIPTION: Command to launch the interactive fine-tuning interface for creating and managing custom models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/index.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Start the fine-tuning interface\ninstructor finetune\n```\n\n----------------------------------------\n\nTITLE: Using Jinja Templating with Instructor for Dynamic Prompts\nDESCRIPTION: Demonstrates Instructor's Jinja templating support for dynamic message content. The example shows how to use variables in system and user messages via the context parameter, allowing for reusable prompt templates that can be populated with different data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclass Analysis(BaseModel):\n    key_points: list[str]\n    summary: str\n\nclient = instructor.from_openai(OpenAI())\n\n# Context will be used to render templates in messages\nanalysis = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Analysis,\n    messages=[\n        {\n            \"role\": \"system\", \n            \"content\": \"You are an expert {{ analyst_type }} analyst.\"\n        },\n        {\n            \"role\": \"user\", \n            \"content\": \"\"\"\n            Please analyze the following {{ document_type }}:\n            \n            {{ content }}\n            \n            Provide a detailed analysis.\n            \"\"\"\n        }\n    ],\n    context={\n        \"analyst_type\": \"financial\",\n        \"document_type\": \"news article\",\n        \"content\": \"Renewable energy investments reached record levels in 2023...\"\n    }\n)\n\nprint(f\"Key points: {analysis.key_points}\")\nprint(f\"Summary: {analysis.summary}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Graph Generation Function with OpenAI\nDESCRIPTION: Implements function to generate knowledge graphs using OpenAI's API.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generate_graph(input) -> KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as small knowledge graph: {input}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Hero Record with OpenAI in Python\nDESCRIPTION: This function uses the OpenAI client to generate a new Hero record. It sends a prompt to create a new superhero and uses the Hero model as the response model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/sqlmodel.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef create_hero() -> Hero:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Hero,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make a new superhero\"},\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Defining Pydantic Model for User Details\nDESCRIPTION: Sets up an OpenAI client with instructor and defines a Pydantic model for user details. This snippet demonstrates the basic structure for extracting user information using a language model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/caching.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Enables `response_model`\nclient = instructor.from_openai(OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\ndef extract(data) -> UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt Caching with Anthropic and Instructor\nDESCRIPTION: Example showing how to use Anthropic's prompt caching feature with Instructor to improve response times and reduce costs. This approach is useful when making multiple calls with similar large contexts, as it caches the context after the first request.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/structured-output-anthropic.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\n# Set up the client with prompt caching\nclient = instructor.from_anthropic(Anthropic())\n\n\n# Define your Pydantic model\nclass Character(BaseModel):\n    name: str\n    description: str\n\n\n# Load your large context\nwith open(\"./book.txt\") as f:\n    book = f.read()\n\n# Make multiple calls using the cached context\nfor _ in range(2):\n    resp, completion = client.chat.completions.create_with_completion(\n        model=\"claude-3-7-sonnet-latest\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"<book>\" + book + \"</book>\",\n                        \"cache_control\": {\"type\": \"ephemeral\"},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Extract a character from the text given above\",\n                    },\n                ],\n            },\n        ],\n        response_model=Character,\n        max_tokens=1000,\n    )\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Fireworks and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with Fireworks to extract structured data. It uses a Pydantic model for data extraction and Fireworks' client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom fireworks.client import Fireworks\nimport instructor\nfrom pydantic import BaseModel\nimport os\n\nclient = Fireworks(\n    api_key=os.environ.get(\"FIREWORKS_API_KEY\"),\n)\nclient = instructor.from_fireworks(client)\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nresp = client.chat.completions.create(\n    model=\"accounts/fireworks/models/llama-v3p2-1b-instruct\",\n    response_model=ExtractUser,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n)\n\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Litellm and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with Litellm to extract structured data. It uses a Pydantic model for data extraction and Litellm's completion function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom litellm import completion\nfrom pydantic import BaseModel\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_litellm(completion)\n\nresp = client.chat.completions.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=ExtractUser,\n)\n\nassert isinstance(resp, ExtractUser)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Reusing Components with Different Contexts in Pydantic Models\nDESCRIPTION: This snippet demonstrates how to reuse the same component for different contexts within a Pydantic model. It defines a TimeRange component that is used for both work_time and leisure_time in the UserDetail class.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass TimeRange(BaseModel):\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    work_time: TimeRange = Field(\n        ..., description=\"Time range during which the user is working.\"\n    )\n    leisure_time: TimeRange = Field(\n        ..., description=\"Time range reserved for leisure activities.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Model for Study Notes Generator in Python\nDESCRIPTION: This Python code defines Pydantic models for generating study notes from YouTube video transcripts. It includes models for concepts and study notes with fields for terms, definitions, timestamps, topics, and key points.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-transcripts.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Concept(BaseModel):\n    term: str = Field(..., description=\"A key term or concept mentioned in the video\")\n    definition: str = Field(\n        ..., description=\"A brief definition or explanation of the term\"\n    )\n\n\nclass StudyNote(BaseModel):\n    timestamp: float = Field(\n        ..., description=\"The timestamp where this note starts in the video\"\n    )\n    topic: str = Field(..., description=\"The main topic being discussed at this point\")\n    key_points: List[str] = Field(..., description=\"A list of key points discussed\")\n    concepts: List[Concept] = Field(\n        ..., description=\"Important concepts mentioned in this section\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Registering Hooks Example\nDESCRIPTION: Example showing how to register and use hooks with the Instructor client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nimport pprint\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef log_completion_kwargs(*args, **kwargs):\n    pprint.pprint({\"args\": args, \"kwargs\": kwargs})\n\n\nclient.on(\"completion:kwargs\", log_completion_kwargs)\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    response_model=str,\n)\nprint(resp)\n#> Hello, user! How can I assist you today?\n```\n\n----------------------------------------\n\nTITLE: Implementing DiVeRSe Prompting with Instructor and OpenAI\nDESCRIPTION: Full implementation of the DiVeRSe prompting technique using Instructor and OpenAI. Includes model definitions for Response and Grading, functions for generating and scoring responses, and batch processing capabilities to handle multiple example sets.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/diverse.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom typing import Literal\nfrom textwrap import dedent\nimport asyncio\nfrom collections import defaultdict\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass Response(BaseModel):\n    chain_of_thought: str\n    answer: int\n\n\nclass Grading(BaseModel):\n    grade: Literal[\"Poor\", \"Average\", \"Good\", \"Excellent\"]\n\n    def get_score(self):\n        mapping = {\n            \"Poor\": 0.25,\n            \"Average\": 0.5,\n            \"Good\": 0.75,\n            \"Excellent\": 1,\n        }\n        return mapping[self.grade]\n\n\nasync def generate_response(query: str, examples: list[str]):\n    formatted_examples = \"\\n\".join(examples)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are a world class AI that excels at answering\n                user queries in a succint and accurate manner.\n\n                <query>\n                {query}\n                </query>\n\n                <examples>\n                {formatted_examples}\n                </examples>\n                \"\"\"\n                ),\n            }\n        ],\n        response_model=Response,\n    )\n\n\nasync def score_response(query: str, response: Response) -> tuple[Response, Grading]:\n    return (\n        response,\n        await client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": dedent(\n                        f\"\"\"\n                You are a world class AI that excels at grading\n                responses to a user query in a succint and clear\n                manner.\n\n                <query>\n                {query}\n                </query>\n\n                <response>\n                {response}\n                </response>\n                \"\"\"\n                    ),\n                }\n            ],\n            response_model=Grading,\n        ),\n    )\n\n\nasync def generate_response_batch(\n    query: str, examples: list[str], n_examples_per_batch: int\n):\n    batches: list[list[str]] = []\n    for i in range(0, len(examples), n_examples_per_batch):\n        batches.append(examples[i : i + n_examples_per_batch])\n\n    coros = [generate_response(query, example_batch) for example_batch in batches]\n    return await asyncio.gather(*coros)\n\n\nasync def score_responses(\n    query: str, responses: list[Response]\n) -> list[tuple[Response, Grading]]:\n    coros = [score_response(query, response) for response in responses]\n    return await asyncio.gather(*coros)\n\n\nif __name__ == \"__main__\":\n    examples = [\n        \"\"\"\n        Q: James decides to run 3 sprints 3 times a week.\n        He runs 60 meters each sprint. How many total\n        meters does he run a week?\n        A: James decides to run 3 sprints 3 times a week.\n        He runs 60 meters each sprint. So he runs 60 meters\n        x 3 sprints x 3 times a week. That is 60 meters x 9.\n        The answer is 540.\n        \"\"\",\n        \"\"\"\n        Q: Brandon's iPhone is four times as old as Ben's\n        iPhone. Ben's iPhone is two times older than Suzy's\n        iPhone. If Suzy's iPhone is 1 year old, how old is\n        Brandon's iPhone?\n        A: Brandon's iPhone is 4 times as old as Ben's\n        iPhone. Ben's iPhone is 2 times older than Suzy's\n        iPhone. So Brandon's iPhone is 4 x 2 = 8 times older\n        than Suzy's iPhone. Suzy's iPhone is 1 year old. So\n        Brandon's iPhone is 8 x 1 = 8 years old. The answer\n        is 8.\n        \"\"\",\n        \"\"\"\n        Q: Jean has 30 lollipops. Jean eats 2 of the\n        lollipops. With the remaining lollipops, Jean wants\n        to package 2 lollipops in one bag. How many bags can\n        Jean fill?\n        A: Jean started with 30 lollipops. She ate 2 of\n        them. So she has 28 lollipops left. She wants to\n        package 2 lollipops in one bag. So she can package\n        28 / 2 = 14 bags. The answer is 14.\n        \"\"\",\n        \"\"\"\n        Q: Weng earns $12 an hour for babysitting.\n        Yesterday, she just did 50 minutes of babysitting.\n        How much did she earn?\n        A: Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n        Working 50 minutes, she earned 0.2 x 50 =\n        $<<0.2*50=10>>10. The answer is 10\n        \"\"\",\n    ]\n\n    query = \"\"\"Betty is saving money for a new wallet which\n    costs $100. Betty has only half of the money she needs.\n    Her parents decided to give her $15 for that purpose,\n    and her grandparents twice as much as her parents. How\n    much more money does Betty need to buy the wallet?\"\"\"\n\n    generated_responses = asyncio.run(generate_response_batch(query, examples, 1))\n\n    scored_responses = asyncio.run(score_responses(query, generated_responses))\n\n    scores: dict[int, float] = defaultdict(int)\n\n    for response, grade in scored_responses:\n        scores[response.answer] += grade.get_score()\n\n    print(scores)\n    #> defaultdict(<class 'int'>, {5: 3.5})\n\n    answer = max(scores, key=scores.get)\n    print(answer)\n    #> 5\n```\n\n----------------------------------------\n\nTITLE: Sequential Processing with For Loop in Python\nDESCRIPTION: Demonstrates sequential processing of a dataset using a for loop and await keyword. This method processes each item one at a time, waiting for each to complete before moving to the next.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/learn-async.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npersons = []\nfor text in dataset:\n    person = await extract_person(text)\n    persons.append(person)\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Responses with Vertex AI\nDESCRIPTION: Example of streaming partial model responses using Instructor with Vertex AI. Demonstrates incremental model building with real-time updates.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/vertex.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nimport instructor\nfrom pydantic import BaseModel\nfrom instructor.dsl.partial import Partial\n\nvertexai.init()\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-2.0-flash\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n)\n\n# Stream partial responses\nresponse_stream = client.chat.completions.create(\n    response_model=Partial[UserExtract],\n    stream=True,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Anibal is 23 years old\"},\n    ],\n)\n\nfor partial_user in response_stream:\n    print(f\"Received update: {partial_user}\")\n```\n\n----------------------------------------\n\nTITLE: Generator Expression Syntax\nDESCRIPTION: Shows the concise syntax for creating a generator expression to generate squares of numbers.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generator.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsquares = (x * x for x in range(10))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor\nDESCRIPTION: Sets up the OpenAI client with instructor patch and defines a basic QuestionAnswer model for handling Q&A responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/self_critique.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of live is to live a life of sin and debauchery.\"\n\nqa: QuestionAnswer = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Question-Answering with Context Using OpenAI and Instructor in Python\nDESCRIPTION: This snippet demonstrates how to use the OpenAI API with Instructor to generate answers based on provided context, utilizing a QuestionAnswer Pydantic model for response validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\n\nquestion = \"What is the meaning of life?\"\ncontext = (\n    \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n)\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n\nresp.answer\n```\n\n----------------------------------------\n\nTITLE: Viewing Instructor Batch Command Help\nDESCRIPTION: Displays the help menu for the instructor batch command, showing available subcommands for managing batch jobs with OpenAI and Anthropic.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ instructor batch --help\n\n Usage: instructor batch [OPTIONS] COMMAND [ARGS]...\n\n Manage OpenAI and Anthropic Batch jobs\n\n╭─ Options ───────────────────────────────────────────────────────────────────────────╮\n│ --help          Show this message and exit.                                         │\n╰─────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Commands ──────────────────────────────────────────────────────────────────────────╮\n│ cancel             Cancel a batch job                                               │\n│ create-from-file   Create a batch job from a file                                   │\n│ download-file      Download the file associated with a batch job                    │\n│ list               See all existing batch jobs                                      │\n╰─────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n----------------------------------------\n\nTITLE: Initializing Anthropic JSON Mode Client\nDESCRIPTION: Sets up an Instructor client for direct JSON output with Anthropic models, suitable for all Claude models and simple structures.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_anthropic(\n    Anthropic(),\n    mode=instructor.Mode.ANTHROPIC_JSON\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Sensitive Information in Instructor AI Prompts\nDESCRIPTION: This example shows how to work with sensitive information in Instructor AI prompts using Pydantic's SecretStr type. It demonstrates how to include sensitive data in prompts while preserving security and preventing accidental exposure in logs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/templating.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, SecretStr\nimport instructor\nimport openai\n\n\nclass UserContext(BaseModel):\n    name: str\n    address: SecretStr\n\n\nclass Address(BaseModel):\n    street: SecretStr\n    city: str\n    state: str\n    zipcode: str\n\n\nclient = instructor.from_openai(openai.OpenAI())\ncontext = UserContext(name=\"scolvin\", address=\"secret address\")\n\naddress = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"{{ user.name }} is `{{ user.address.get_secret_value() }}`, normalize it to an address object\",\n        },\n    ],\n    context={\"user\": context},\n    response_model=Address,\n)\nprint(context)\n#> name='scolvin' address=SecretStr('**********')\nprint(address)\n#> street=SecretStr('**********') city='scolvin' state='NA' zipcode='00000'\n```\n\n----------------------------------------\n\nTITLE: Nested Object Extraction with Writer\nDESCRIPTION: Example of extracting complex nested data structures using Address and User models with multiple addresses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/writer.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom writerai import Writer\nfrom pydantic import BaseModel\n\n# Initialize Writer client\nclient = instructor.from_writer(Writer())\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\n# Create structured output with nested objects\nuser = client.chat.completions.create(\n    model=\"palmyra-x-004\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract: Jason is 25 years old.\n            He lives at 123 Main St, New York, USA\n            and has a summer house at 456 Beach Rd, Miami, USA\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\nprint(user)\n```\n\n----------------------------------------\n\nTITLE: Running a Fine-Tune with Instructor CLI\nDESCRIPTION: A bash command demonstrating how to create a fine-tuning job using the Instructor CLI with the JSONL file generated from the distillation process.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/distillation.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninstructor jobs create-from-file math_finetunes.jsonl\n```\n\n----------------------------------------\n\nTITLE: Patching OpenAI Client with Instructor\nDESCRIPTION: Initializes and patches an AsyncOpenAI client using the Instructor library to enable response_model functionality for structured outputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/matching-language.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Patch the OpenAI client to enable response_model\nclient = patch(AsyncOpenAI())\n```\n\n----------------------------------------\n\nTITLE: Processing Tag Request and Printing JSON Response in Python\nDESCRIPTION: This snippet shows how to process a tag request asynchronously and print the JSON response. It uses the asyncio library to run the tag_request function and then prints the result as formatted JSON.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresponse = asyncio.run(tag_request(request))\nprint(response.model_dump_json(indent=2))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor\nDESCRIPTION: Sets up the OpenAI client with Instructor library for async operations. This enables asynchronous processing of classification requests.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\n\nclient = instructor.from_openai(\n    openai.AsyncOpenAI(),\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing a Simple DAG Structure in Mermaid\nDESCRIPTION: A basic mermaid diagram showing a directed acyclic graph with nodes A, B, C, and D, where A connects to B and C, while both B and C connect to D. This illustrates the concept of a DAG where connections are directed and there are no cycles.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extract-model-looks.md#2025-04-14_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A --> B\n    A --> C\n    B --> D\n    C --> D\n```\n\n----------------------------------------\n\nTITLE: Streaming Multiple Entities\nDESCRIPTION: This snippet modifies the previous one to enable streaming of the extracted data. This allows the results to be processed as they are generated, rather than waiting for the entire response.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Iterable\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n```\n\n----------------------------------------\n\nTITLE: Creating Pydantic Model with Annotated Types in Python\nDESCRIPTION: Demonstrates how Instructor wraps response models with Pydantic BaseModel using type annotations and fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/types.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\nfrom pydantic import create_model, Field, BaseModel\n\ntypehint = Annotated[bool, Field(description=\"Sample Description\")]\n\nmodel = create_model(\"Response\", content=(typehint, ...), __base__=BaseModel)\n\nprint(model.model_json_schema())\n```\n\n----------------------------------------\n\nTITLE: Initializing Anthropic Client with Instructor Integration\nDESCRIPTION: Sets up the Anthropic client with Instructor integration using the ANTHROPIC_TOOLS mode for structured output generation. This initialization connects to Anthropic's API using the API key from environment variables.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclient = from_anthropic(\n    anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\")),\n    mode=instructor.Mode.ANTHROPIC_TOOLS\n)\n```\n\n----------------------------------------\n\nTITLE: Using PDF Class with OpenAI for PDF Analysis in Python\nDESCRIPTION: This snippet shows how to use the PDF class from Instructor to analyze a PDF file using OpenAI's GPT-4 model. It loads a PDF from a URL and extracts specific information like total amount and items from an invoice.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/multimodal.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\nfrom instructor.multimodal import PDF\n\n# Set up the client\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\nclient = instructor.from_openai(OpenAI())\n\n\n# Create a model for analyzing PDFs\nclass Invoice(BaseModel):\n    total: float\n    items: list[str]\n\n\n# Load and analyze a PDF\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=Invoice,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Analyze this document\",\n                PDF.from_url(url),\n            ],\n        }\n    ],\n)\n\nprint(response)\n# > Total = 220, items = ['English Tea', 'Tofu']\n```\n\n----------------------------------------\n\nTITLE: Consuming Streaming API with Python Requests\nDESCRIPTION: This snippet demonstrates how to use the requests library to consume the streaming API endpoint. It sends a POST request to the /extract endpoint and iterates over the response content, printing each chunk.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/full-fastapi-visibility.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresponse = requests.post(\n    \"http://127.0.0.1:3000/extract\",\n    json={\n        \"query\": \"Alice and Bob are best friends. They are currently 32 and 43 respectively. \"\n    },\n    stream=True,\n)\n\nfor chunk in response.iter_content(chunk_size=1024):\n    if chunk:\n        print(str(chunk, encoding=\"utf-8\"), end=\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Partial Extraction with Streaming\nDESCRIPTION: Example of using Partial extraction with streaming to process incomplete data in real-time.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/why.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom instructor import Partial\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.from_openai(OpenAI())\n\ntext_block = \"...\"\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Partial[MeetingInfo],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Data Extraction with Writer\nDESCRIPTION: Shows how to perform asynchronous structured data extraction using AsyncWriter with the same User model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/writer.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom writerai import AsyncWriter\nfrom pydantic import BaseModel\n\n# Initialize Writer client\nclient = instructor.from_writer(AsyncWriter())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_user():\n    # Extract structured data\n    user = await client.chat.completions.create(\n        model=\"palmyra-x-004\",\n        messages=[{\"role\": \"user\", \"content\": \"Extract: John is 30 years old\"}],\n        response_model=User,\n    )\n\n    print(user)\n    # > name='John' age=30\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(extract_user())\n```\n\n----------------------------------------\n\nTITLE: System Messages Implementation with Gemini\nDESCRIPTION: Demonstrates different ways to use system messages with Google GenAI SDK\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom google import genai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = genai.Client()\nclient = instructor.from_genai(client, mode=instructor.Mode.GENAI_TOOLS)\n\n# As a parameter\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    system=\"Jason is 25 years old\",\n    messages=[{\"role\": \"user\", \"content\": \"You are a data extraction assistant\"}],\n    response_model=User,\n)\n\nprint(response)\n# > name='Jason' age=25\n\n# Or as a message with role \"system\"\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Jason is 25 years old\"},\n        {\"role\": \"user\", \"content\": \"You are a data extraction assistant\"},\n    ],\n    response_model=User,\n)\n\nprint(response)\n# > name='Jason' age=25\n```\n\n----------------------------------------\n\nTITLE: Defining Table Class for Organizing Extracted Data in Python\nDESCRIPTION: This code defines a Table class using Pydantic's BaseModel. It includes a caption field and a dataframe field of type MarkdownDataFrame, which handles the complexity of processing markdown tables.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extracting_tables.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nclass Table(BaseModel):\n    caption: str\n    dataframe: MarkdownDataFrame\n```\n\n----------------------------------------\n\nTITLE: Synchronous User Extraction with Mistral and Instructor\nDESCRIPTION: Python code demonstrating a simple synchronous user extraction using Mistral and Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pydantic import BaseModel\nfrom mistralai import Mistral\nfrom instructor import from_mistral, Mode\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n# Initialize with API key\nclient = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n\n# Enable instructor patches for Mistral client\ninstructor_client = from_mistral(\n    client=client,\n    mode=Mode.MISTRAL_TOOLS,\n)\n\n# Extract a single user\nuser = instructor_client.chat.completions.create(\n    response_model=UserDetails,\n    model=\"mistral-large-latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Jason is 25 years old\"}],\n    temperature=0,\n)\n\nprint(user)\n# Output: UserDetails(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Responses with Gemini Structured Outputs\nDESCRIPTION: Demonstrates how to stream responses incrementally when using Gemini models with structured outputs. This enables processing responses as they're generated rather than waiting for the complete result.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport instructor\nfrom google import genai\n\n\nclient = instructor.from_genai(\n    genai.Client(), mode=instructor.Mode.GENAI_STRUCTURED_OUTPUTS\n)\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nclass PersonList(BaseModel):\n    people: list[Person]\n\n\nstream = client.chat.completions.create_partial(\n    model=\"gemini-2.0-flash-001\",\n    system=\"You are a helpful assistant. You must return a function call with the schema provided.\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Ivan is 20 years old, Jason is 25 years old, and John is 30 years old\",\n        }\n    ],\n    response_model=PersonList,\n)\n\nfor extraction in stream:\n    print(extraction)\n    # > people=[PartialPerson(name='Ivan', age=None)]\n    # > people=[PartialPerson(name='Ivan', age=20), PartialPerson(name='Jason', age=25), PartialPerson(name='John', age=None)]\n    # > people=[PartialPerson(name='Ivan', age=20), PartialPerson(name='Jason', age=25), PartialPerson(name='John', age=30)]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for MistralAI and Instructor\nDESCRIPTION: This command installs the necessary Python packages to use MistralAI with Instructor. It includes instructor for the Instructor library, mistralai for the MistralAI client, and pydantic for data validation and settings management.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/mistral.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor mistralai pydantic\n```\n\n----------------------------------------\n\nTITLE: Implementing Self-Consistency with Instructor and AsyncOpenAI\nDESCRIPTION: This code implements the Self-Consistency technique which improves LLM performance by generating multiple responses and taking a majority vote. It uses the Instructor library with AsyncOpenAI to create parallel requests, defining a Pydantic model for structured outputs, and combines responses using Counter to find the most common answer.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/self_consistency.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel, Field\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom collections import Counter\nfrom textwrap import dedent\n\n\nclass SelfConsistencyResponse(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"reasoning behind the final correct answer\"\n    )\n    correct_answer: int\n\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nasync def generate_self_consistent_response(prompt: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an intelligent question\n                answering AI system that excels at answering\n                user queries. Make sure to generate a\n                comprehensive explanation of your thought\n                process before providing the final answer\"\"\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_model=SelfConsistencyResponse,\n        temperature=0.5,\n    )\n\n\nasync def generate_self_consistent_responses(prompt: str, num_responses: int):\n    coros = [generate_self_consistent_response(prompt) for _ in range(num_responses)]\n    responses = await asyncio.gather(*coros)\n    return responses\n\n\nif __name__ == \"__main__\":\n    prompt = dedent(\n        \"\"\"\n        Janet's ducks lay 16 eggs per day.\n        She eats three for breakfast every\n        morning and bakes muffins for her\n        friends every day with four. She sells\n        the remainder for $2 per egg. How\n        much does she make every day?\n        \"\"\"\n    )\n    responses = asyncio.run(generate_self_consistent_responses(prompt, 5))\n    answer_counts = Counter([response.correct_answer for response in responses])\n    most_common_answer, _ = answer_counts.most_common(1)[0]\n\n    print(most_common_answer)\n    #> 18\n```\n\n----------------------------------------\n\nTITLE: Handling Errors with Maybe Pattern\nDESCRIPTION: This snippet shows how to handle potential errors using the MaybeCharacter returned by the extract function. If the error flag is set, it raises a ValueError with the error message.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nuser = extract(\"404 Error\")\n\nif user.error:\n    raise ValueError(user.message)\n```\n\n----------------------------------------\n\nTITLE: Profile Model with Optional Fields\nDESCRIPTION: Shows how to implement optional fields in a Pydantic model using Optional type hints.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/validation.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom pydantic import BaseModel, Field\n\n\nclass Profile(BaseModel):\n    \"\"\"Model representing a user profile with optional fields.\"\"\"\n\n    name: str = Field(description=\"User's full name\")\n    bio: Optional[str] = Field(None, description=\"Optional user biography\")\n```\n\n----------------------------------------\n\nTITLE: Defining Judgment Model with Pydantic for LLM Output\nDESCRIPTION: Creates a Pydantic model 'Judgment' to structure the output of the LLM, including thought process, justification, and similarity judgment.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/pairwise-llm-judge.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Judgment(BaseModel):\n    thought: str = Field(\n        description=\"The step-by-step reasoning process used to analyze the question and text\"\n    )\n    justification: str = Field(\n        description=\"Explanation for the similarity judgment, detailing key factors that led to the conclusion\"\n    )\n    similarity: bool = Field(\n        description=\"Boolean judgment indicating whether the question and text are similar or relevant (True) or not (False)\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing instructor with Cohere support\nDESCRIPTION: Install the instructor library with Cohere support using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cohere.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[cohere]\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Fine-Tuning Jobs in Instructor CLI\nDESCRIPTION: This snippet shows how to list and view the status of fine-tuning jobs using the Instructor CLI. It displays detailed information about each job, including ID, status, creation time, and model details.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/finetune.md#2025-04-14_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n$ instructor jobs list\n\nOpenAI Fine Tuning Job Monitoring\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃                ┃              ┃                ┃     Completion ┃                 ┃                ┃        ┃                 ┃\n┃ Job ID         ┃ Status       ┃  Creation Time ┃           Time ┃ Model Name      ┃ File ID        ┃ Epochs ┃ Base Model      ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ ftjob-PWo6uwk… │ 🚫 cancelled │     2023-08-23 │            N/A │                 │ file-F7lJg6Z4… │ 3      │ gpt-3.5-turbo-… │\n│                │              │       23:10:54 │                │                 │                │        │                 │\n│ ftjob-1whjva8… │ 🚫 cancelled │     2023-08-23 │            N/A │                 │ file-F7lJg6Z4… │ 3      │ gpt-3.5-turbo-… │\n│                │              │       22:47:05 │                │                 │                │        │                 │\n│ ftjob-wGoBDld… │ 🚫 cancelled │     2023-08-23 │            N/A │                 │ file-F7lJg6Z4… │ 3      │ gpt-3.5-turbo-… │\n│                │              │       22:44:12 │                │                 │                │        │                 │\n│ ftjob-yd5aRTc… │ ✅ succeeded │     2023-08-23 │     2023-08-23 │ ft:gpt-3.5-tur… │ file-IQxAUDqX… │ 3      │ gpt-3.5-turbo-… │\n│                │              │       14:26:03 │       15:02:29 │                 │                │        │                 │\n└────────────────┴──────────────┴────────────────┴────────────────┴─────────────────┴────────────────┴────────┴─────────────────┘\n                                    Automatically refreshes every 5 seconds, press Ctrl+C to exit\n```\n\n----------------------------------------\n\nTITLE: Installing Anthropic Integration\nDESCRIPTION: Installation command for Instructor with Anthropic (Claude) support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install \"instructor[anthropic]\"\n```\n\n----------------------------------------\n\nTITLE: Claude Prompt Template for Context Generation\nDESCRIPTION: The prompt template used by Anthropic's Claude to generate contextual information for document chunks\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/situate-context.md#2025-04-14_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n<document>\n{{WHOLE_DOCUMENT}}\n</document>\nHere is the chunk we want to situate within the whole document\n<chunk>\n{{CHUNK_CONTENT}}\n</chunk>\nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n```\n\n----------------------------------------\n\nTITLE: Testing Multi-Label Classification for Support Tickets in Python\nDESCRIPTION: This code snippet tests a multi-label classification function on a support ticket. It asserts that the correct labels ('TECH_ISSUE' and 'BILLING') are present in the classification results and prints the input ticket and predicted labels.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/classification.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Test multi-label classification\nticket = \"My account is locked and I can't access my billing info.\"\nprediction = multi_classify(ticket)\nassert \"TECH_ISSUE\" in prediction.class_labels\nassert \"BILLING\" in prediction.class_labels\nprint(f\"Ticket: {ticket}\")\n#> Ticket: My account is locked and I can't access my billing info.\nprint(f\"Predicted Labels: {prediction.class_labels}\")\n#> Predicted Labels: ['TECH_ISSUE', 'BILLING']\n```\n\n----------------------------------------\n\nTITLE: Creating Fine-Tuning Job from ID Example in Instructor CLI\nDESCRIPTION: This snippet demonstrates how to create a fine-tuning job from an existing ID using the Instructor CLI. It includes steps for uploading files, listing files, and creating the job with specific parameters.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/finetune.md#2025-04-14_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n$ instructor files upload transformed_data.jsonl\n$ instructor files upload validation_data.jsonl\n$ instructor files list\n...\n$ instructor jobs create_from_id <file_id> --validation_file <validation_file_id> --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n```\n\n----------------------------------------\n\nTITLE: Opening Instructor Documentation using CLI\nDESCRIPTION: Shows how to use the Instructor CLI to open the documentation in a browser. The command supports searching for specific topics within the documentation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninstructor docs [QUERY]\n```\n\n----------------------------------------\n\nTITLE: Simple User Extraction - Asynchronous Implementation\nDESCRIPTION: Example demonstrating asynchronous user information extraction using DeepSeek and Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/deepseek.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nimport instructor\n\nclient = instructor.from_openai(\n    AsyncOpenAI(\n        api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\"\n    )\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_user():\n    user = await client.chat.completions.create(\n        model=\"deepseek-chat\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n        ],\n        response_model=User,\n    )\n    return user\n\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)\n# > name='Jason' age=25\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Anthropic Support\nDESCRIPTION: Command to install the Instructor library with Anthropic support using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/structured-output-anthropic.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor[anthropic]\n```\n\n----------------------------------------\n\nTITLE: Computing Recall and Reciprocal Rank for Retrieved Results in Python\nDESCRIPTION: This snippet calculates the recall and reciprocal rank (RR) for the retrieved chunk IDs compared to the ground truth chunk ID.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrecall(retrieved_chunk_ids, [chunk_id]), rr(retrieved_chunk_ids, [chunk_id])\n```\n\n----------------------------------------\n\nTITLE: Calculating Entity Density for Generated Summaries in Python\nDESCRIPTION: This code calculates and prints the entity density for each summary in the chain. It uses a previously defined function calculate_entity_density to compute tokens, entity count, and density.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfor index, summary in enumerate(summaries):\n    tokens, entity, density = calculate_entity_density(summary)\n    print(\n        f\"Article {index + 1} -> Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Real-time Streaming Validation with Instructor\nDESCRIPTION: Demonstrates how to use the create_iterable method to extract multiple instances of a defined schema from OpenAI responses. The example shows user extraction with name and age fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS_STRICT)\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nusers = client.chat.completions.create_iterable(\n    model=\"gpt-4o-mini\",\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\n        },\n    ],\n)\n\nfor user in users:\n    print(user)\n    #> name='Jason' age=10\n    #> name='John' age=10\n```\n\n----------------------------------------\n\nTITLE: Setting Instructor Mode for Mistral Client\nDESCRIPTION: Python code to set the mode for the Mistral client using Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pydantic import BaseModel\nfrom mistralai import Mistral\nfrom instructor import from_mistral\n\n# Initialize with API key\nclient = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n\n# Enable instructor patches for Mistral client\ninstructor_client = from_mistral(\n    client=client,\n    # Set the mode here\n    mode=Mode.MISTRAL_TOOLS,\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting PII Data with OpenAI API\nDESCRIPTION: Uses OpenAI's ChatCompletion model to extract PII information from documents. Demonstrates API configuration and request formatting for PII extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/pii.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\nEXAMPLE_DOCUMENT = \"\"\"\n# Fake Document with PII for Testing PII Scrubbing Model\n# (The content here)\n\"\"\"\n\npii_data = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=PIIDataExtraction,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class PII scrubbing model, Extract the PII data from the following document\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": EXAMPLE_DOCUMENT,\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Instructor with OpenAI Client in Python\nDESCRIPTION: Sets up the environment by importing the necessary libraries and initializing the Instructor-enhanced OpenAI client for structured outputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/llm-as-reranker.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n```\n\n----------------------------------------\n\nTITLE: Implementing Partial Literal Mixin for Streaming in Python\nDESCRIPTION: This snippet shows how to use the PartialLiteralMixin when working with literal values in streaming responses. It's necessary to import this mixin to handle incomplete Literal values during streaming.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/partial.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\nfrom pydantic import BaseModel\nfrom instructor.dsl.partial import PartialLiteralMixin\n\n\nclass User(BaseModel, PartialLiteralMixin):\n    name: str\n    age: int\n    category: Literal[\"admin\", \"user\", \"guest\"]\n\n\n# The rest of your code below\n```\n\n----------------------------------------\n\nTITLE: Custom Behavior in Pydantic LLM Models\nDESCRIPTION: Demonstrates how to add custom methods to Pydantic models for additional functionality. Implements a search query model with an execute method.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/models.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import Literal\n\nfrom openai import OpenAI\n\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass SearchQuery(BaseModel):\n    query: str\n    query_type: Literal[\"web\", \"image\", \"video\"]\n\n    def execute(self):\n        print(f\"Searching for {self.query} of type {self.query_type}\")\n        #> Searching for cat of type image\n        return \"Results for cat\"\n\n\nquery = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for a picture of a cat\"}],\n    response_model=SearchQuery,\n)\n\nresults = query.execute()\n```\n\n----------------------------------------\n\nTITLE: Using Fine-Tuned GPT-3.5 Model for Summarization\nDESCRIPTION: This code snippet shows how to update the @instructions.distil decorator to use the fine-tuned GPT-3.5 model for summarization.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chain-of-density.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef distil_summarization(text: str) -> GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1])\n```\n\n----------------------------------------\n\nTITLE: Simple User Extraction - Synchronous Implementation\nDESCRIPTION: Example showing how to extract user information synchronously using DeepSeek and Instructor with Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/deepseek.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\n\nclient = instructor.from_openai(\n    OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Create structured output\nuser = client.chat.completions.create(\n    model=\"deepseek-chat\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n    ],\n    response_model=User,\n)\n\nprint(user)\n# > name='Jason' age=25\n```\n\n----------------------------------------\n\nTITLE: List Constraints with Field Parameters\nDESCRIPTION: Shows how to add constraints to lists using Pydantic's Field parameters. Demonstrates setting minimum and maximum items for lists.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/list_extraction.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Ingredient(BaseModel):\n    name: str\n    amount: str\n\nclass Recipe(BaseModel):\n    title: str\n    ingredients: List[Ingredient] = Field(\n        ...,\n        min_items=2,         # Minimum 2 ingredients\n        max_items=10,        # Maximum 10 ingredients\n        description=\"List of ingredients needed for the recipe\"\n    )\n    steps: List[str] = Field(\n        ...,\n        min_items=1,\n        description=\"Step-by-step instructions to prepare the recipe\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Client with Retries\nDESCRIPTION: Shows how to use the instructor-enhanced OpenAI client with retry logic for handling validation failures and JSON decoding errors.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/reask_validation.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(model.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running Cortex LLM Model\nDESCRIPTION: Command to initialize and run a quantized llama3.2 model in Cortex\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cortex.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncortex run llama3.2:3b-gguf-q4-km\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to OpenAI API Calls with Instructor\nDESCRIPTION: Example showing how to add metadata to API calls when using Instructor with API Model Distillation. This metadata can help track and organize API calls for distillation purposes.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/openai-distilation-store.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n    store=True,\n    metadata={\"task\": \"user_extraction\", \"source\": \"customer_support_chat\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Iterable Streaming with Gemini\nDESCRIPTION: Shows how to stream multiple user objects using iterable responses with Gemini model. Useful for processing lists of structured data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/google.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Extract multiple users from text\nusers = client.chat.completions.create_iterable(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract users:\n            1. Jason is 25 years old\n            2. Sarah is 30 years old\n            3. Mike is 28 years old\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Field Validation with Pydantic in Python\nDESCRIPTION: Demonstrates basic field validation using Pydantic's Field constraints for user data validation. Includes validation for name length, age range, and email format.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/field_level_validation.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass User(BaseModel):\n    name: str = Field(..., min_length=2, description=\"User's full name\")\n    age: int = Field(..., ge=18, le=120, description=\"User's age in years\")\n    email: str = Field(\n        ..., \n        pattern=r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\",\n        description=\"Valid email address\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Poetry Installation and Setup\nDESCRIPTION: Commands for installing Poetry and setting up the development environment.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/contributing.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Clone the repository\ngit clone https://github.com/YOUR-USERNAME/instructor.git\ncd instructor\n\n# Install with development dependencies\npoetry install --with dev,docs\n```\n\n----------------------------------------\n\nTITLE: Defining CLI Command Group with Click in Python\nDESCRIPTION: This snippet creates the main command group for the Instructor-AI CLI using the Click library. It sets up the basic structure for subcommands and includes a version option.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/jobs.md#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@click.group()\n@click.version_option(__version__)\ndef cli():\n    \"\"\"Instructor AI command line tool.\"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: F-string Based Template Function for Person Extraction\nDESCRIPTION: Shows how to create a simple template function using f-strings for person information extraction. Includes a reusable function that takes content and document type as parameters.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/prompt_templates.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_person(content, document_type=\"text\"):\n    prompt = f\"\"\"\n    Extract information about the person mentioned in the following {document_type}:\n    \n    {content}\n    \n    Please provide their name, age, and occupation.\n    \"\"\"\n    \n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        response_model=Person\n    )\n\n# Use the function\nperson = extract_person(\n    \"According to his resume, John Smith (42) works as a software developer.\",\n    document_type=\"resume\"\n)\n```\n\n----------------------------------------\n\nTITLE: Order Model with Custom Validators\nDESCRIPTION: Demonstrates custom validation logic using Pydantic validators for an order model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/validation.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field, field_validator\n\n\nclass Order(BaseModel):\n    \"\"\"Model representing an order with custom validation logic.\"\"\"\n\n    items: List[str] = Field(description=\"List of item names in the order\")\n    total: float = Field(description=\"Total order amount\")\n\n    @field_validator('total')\n    @classmethod\n    def validate_total(cls, v):\n        \"\"\"Validate that the total amount is not negative.\"\"\"\n        if v < 0:\n            raise ValueError('Total cannot be negative')\n        return v\n```\n\n----------------------------------------\n\nTITLE: Database-Driven Dynamic Model Creation\nDESCRIPTION: Shows how to create Pydantic models dynamically based on database schema information. Includes SQL query and type mapping implementation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/models.md#2025-04-14_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT property_name, property_type, description\nFROM prompt\nWHERE model_name = {model_name}\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, create_model\nfrom typing import List\n\ntypes = {\n    'string': str,\n    'integer': int,\n    'boolean': bool,\n    'number': float,\n    'List[str]': List[str],\n}\n\n# Mocked cursor.fetchall()\ncursor = [\n    ('name', 'string', 'The name of the user.'),\n    ('age', 'integer', 'The age of the user.'),\n    ('email', 'string', 'The email of the user.'),\n]\n\nBarModel = create_model(\n    'User',\n    **{\n        property_name: (types[property_type], description)\n        for property_name, property_type, description in cursor\n    },\n    __base__=BaseModel,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with LiteLLM Support\nDESCRIPTION: This command installs the Instructor library with LiteLLM support using Poetry package manager.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/watsonx.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install instructor --with litellm\n```\n\n----------------------------------------\n\nTITLE: Synchronous Data Extraction with Writer\nDESCRIPTION: Demonstrates basic synchronous structured data extraction using Writer's Palmyra-X-004 model with a simple User model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/writer.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom writerai import Writer\nfrom pydantic import BaseModel\n\n# Initialize Writer client\nclient = instructor.from_writer(Writer(api_key=\"your API key\"))\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Extract structured data\nuser = client.chat.completions.create(\n    model=\"palmyra-x-004\",\n    messages=[{\"role\": \"user\", \"content\": \"Extract: John is 30 years old\"}],\n    response_model=User,\n)\n\nprint(user)\n#> name='John' age=30\n```\n\n----------------------------------------\n\nTITLE: External Service Validation in Python\nDESCRIPTION: Shows how to implement validation using external services, demonstrated with a ZIP code validation example. Includes basic validation logic that could be extended to use external APIs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/validation/custom_validators.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, field_validator\nimport instructor\nfrom openai import OpenAI\nimport requests\n\nclient = instructor.from_openai(OpenAI())\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n    \n    @field_validator('zip_code')\n    @classmethod\n    def validate_zip_code(cls, value):\n        # Example of validation using an external service (simplified)\n        # In a real app, you might use a postal code validation API\n        if not (value.isdigit() and len(value) == 5):\n            raise ValueError(\"Zip code must be 5 digits\")\n        return value\n```\n\n----------------------------------------\n\nTITLE: Using Literals in Pydantic Models as an Alternative to Enums\nDESCRIPTION: This code snippet shows how to use Literal types in Pydantic models as an alternative to Enums. It defines a UserDetail class with a role field that can only take specific string values.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Product Model in Python with Pydantic\nDESCRIPTION: Creates a Pydantic model representing a product extracted from an image using AI. The model includes fields for name, key features, and description, with a method to generate a prompt.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/image_to_ad_copy.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\n\nclass Product(BaseModel):\n    \"\"\"\n    Represents a product extracted from an image using AI.\n\n    The product attributes are dynamically determined based on the content\n    of the image and the AI's interpretation. This class serves as a structured\n    representation of the identified product characteristics.\n    \"\"\"\n\n    name: str = Field(\n        description=\"A generic name for the product.\", example=\"Headphones\"\n    )\n    key_features: Optional[List[str]] = Field(\n        description=\"A list of key features of the product that stand out.\",\n        default=None,\n    )\n\n    description: Optional[str] = Field(\n        description=\"A description of the product.\",\n        default=None,\n    )\n\n    # Can be customized and automatically generated\n    def generate_prompt(self):\n        prompt = f\"Product: {self.name}\\n\"\n        if self.description:\n            prompt += f\"Description: {self.description}\\n\"\n        if self.key_features:\n            prompt += f\"Key Features: {', '.join(self.key_features)}\\n\"\n        return prompt\n```\n\n----------------------------------------\n\nTITLE: Setting up Cohere Client with Instructor\nDESCRIPTION: Demonstrates the configuration of a Cohere client using Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/client_setup.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport cohere\n\ncohere_client = cohere.Client(\"YOUR_API_KEY\")\nclient = instructor.from_cohere(cohere_client)\n```\n\n----------------------------------------\n\nTITLE: Using Primitive Types with Instructor OpenAI Client\nDESCRIPTION: Shows how to use basic boolean type responses with the Instructor-wrapped OpenAI client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/types.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=bool,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Is it true that Paris is the capital of France?\",\n        },\n    ],\n)\nassert resp is True, \"Paris is the capital of France\"\nprint(resp)\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Cerebras Support\nDESCRIPTION: Command to install Instructor with Cerebras support using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cerebras.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[cerebras_cloud_sdk]\"\n```\n\n----------------------------------------\n\nTITLE: Generating and Visualizing Knowledge Graph from Text Chunks in Python\nDESCRIPTION: This code snippet demonstrates how to use the generate_graph function with a list of text chunks to create and visualize a knowledge graph.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/knowledge_graph.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntext_chunks = [\n    \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",\n    \"Professors are smart.\",\n    \"Sarah knows Jason and is a student of his.\",\n    \"Sarah is a student at the University of Toronto. and UofT is in Canada\",\n]\ngraph: KnowledgeGraph = generate_graph(text_chunks)\ngraph.draw(prefix=\"final\")\n```\n\n----------------------------------------\n\nTITLE: Running Email Classification with Logfire Monitoring\nDESCRIPTION: Code that processes multiple example emails through the classification function. Each email is classified as spam or not spam, with Logfire capturing the entire process for monitoring.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nemails = [\n    \"Hello there I'm a Nigerian prince and I want to give you money\",\n    \"Meeting with Thomas has been set at Friday next week\",\n    \"Here are some weekly product updates from our marketing team\",\n]\n\nfor email in emails:\n    classify(email)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Groq Inference\nDESCRIPTION: This command installs the necessary Python packages to use Groq with Instructor AI, including instructor, groq, pydantic, openai, and anthropic.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/groq.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor groq pydantic openai anthropic\n```\n\n----------------------------------------\n\nTITLE: Streaming Iterable Collections with Mistral and Instructor\nDESCRIPTION: Python code demonstrating streaming of iterable collections using Mistral and Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport instructor\nfrom mistralai import Mistral\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n# Initialize with API key\nclient = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n\n# Enable instructor patches for Mistral client\ninstructor_client = instructor.from_mistral(client)\n\n# Stream iterable responses\nusers = instructor_client.chat.completions.create_iterable(\n    model=\"mistral-large-latest\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Make up two people\"},\n    ],\n)\n\nfor user in users:\n    print(f\"Generated user: {user}\")\n# Output:\n# Generated user: UserExtract(name='Emily Johnson', age=32)\n# Generated user: UserExtract(name='Michael Chen', age=28)\n```\n\n----------------------------------------\n\nTITLE: Handling Arbitrary Properties in Pydantic Models\nDESCRIPTION: This code snippet shows how to handle arbitrary properties in Pydantic models using a list of key-value pairs. It defines a Property class and incorporates it into the UserDetail class to extract undefined attributes.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ..., description=\"Extract any other properties that might be relevant.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Flattening Dictionaries and Converting to DataFrame in Python\nDESCRIPTION: Two functions are defined: flatten_dict and dicts_to_df. flatten_dict converts a nested dictionary structure into a flat dictionary format, while dicts_to_df takes a list of dictionaries and creates a pandas DataFrame, useful for managing data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# % pip install pandas wandb\nimport pandas as pd\nfrom typing import Any\n\n\ndef flatten_dict(\n    d: dict[str, Any], parent_key: str = \"\", sep: str = \"_\"\n) -> dict[str, Any]:\n    \"\"\"\n    Flatten a nested dictionary.\n\n    :param d: The nested dictionary to flatten.\n    :param parent_key: The base key to use for the flattened keys.\n    :param sep: Separator to use between keys.\n    :return: A flattened dictionary.\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\ndef dicts_to_df(list_of_dicts: list[dict[str, Any]]) -> pd.DataFrame:\n    \"\"\"\n    Convert a list of dictionaries to a pandas DataFrame.\n\n    :param list_of_dicts: List of dictionaries, potentially nested.\n    :return: A pandas DataFrame representing the flattened data.\n    \"\"\"\n    # Flatten each dictionary and create a DataFrame\n    flattened_data = [flatten_dict(d) for d in list_of_dicts]\n    return pd.DataFrame(flattened_data)\n\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variable\nDESCRIPTION: Exports the LangSmith API key as an environment variable for authentication.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/langsmith.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport LANGCHAIN_API_KEY=<your-api-key>\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Results with Rich Console Output\nDESCRIPTION: Shows how to use create_partial method to stream information dynamically for frontend components. Extracts meeting information including users, dates, and budget details with real-time console updates.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom rich.console import Console\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS_STRICT)\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meeting is scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\nclass MeetingInfo(BaseModel):\n    users: list[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\nextraction_stream = client.chat.completions.create_partial(\n    model=\"gpt-4o-mini\",\n    response_model=MeetingInfo,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n```\n\n----------------------------------------\n\nTITLE: Optional Fields with Union Types\nDESCRIPTION: Demonstrates how to use Optional types with Union for nullable fields in Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/unions.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    email: Optional[str] = None  # Same as Union[str, None]\n```\n\n----------------------------------------\n\nTITLE: Defining Data Models with Pydantic\nDESCRIPTION: Example of defining character and property data models using Pydantic BaseModel with type hints and field descriptions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/why.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Literal\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    name: str = Field(description=\"name of property in snake case\")\n    value: str\n\nclass Character(BaseModel):\n    \"\"\"\n    Any character in a fictional story\n    \"\"\"\n    name: str\n    age: int\n    properties: List[Property]\n    role: Literal['protagonist', 'antagonist', 'supporting']\n\nclass AllCharacters(BaseModel):\n    characters: List[Character] = Field(description=\"A list of all characters in the story\")\n```\n\n----------------------------------------\n\nTITLE: Complex Transaction Validation\nDESCRIPTION: Shows complex validation rules using multiple validators and datetime handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/validation.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field, validator\n\n\nclass Transaction(BaseModel):\n    \"\"\"Model representing a financial transaction with complex validation.\"\"\"\n\n    amount: float = Field(description=\"Transaction amount\")\n    currency: str = Field(description=\"Three-letter currency code (USD, EUR, GBP)\")\n    timestamp: datetime = Field(description=\"Transaction timestamp\")\n\n    @validator('currency')\n    def validate_currency(cls, v):\n        \"\"\"Validate that currency is one of the supported codes.\"\"\"\n        valid_currencies = ['USD', 'EUR', 'GBP']\n        if v not in valid_currencies:\n            raise ValueError(f'Currency must be one of {valid_currencies}')\n        return v\n\n    @validator('timestamp')\n    def validate_timestamp(cls, v):\n        \"\"\"Validate that timestamp is not in the future.\"\"\"\n        if v > datetime.now():\n            raise ValueError(\"Transaction timestamp cannot be in the future\")\n        return v\n```\n\n----------------------------------------\n\nTITLE: Usage Monitoring Commands\nDESCRIPTION: Commands to monitor OpenAI API usage including total usage, daily breakdown, and model-specific costs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/index.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# View total usage for the current month\ninstructor usage\n\n# View usage breakdown by day\ninstructor usage --by-day\n\n# Calculate cost for a specific model\ninstructor usage --model gpt-4\n```\n\n----------------------------------------\n\nTITLE: Creating Completions with Jinja Templates in Python\nDESCRIPTION: This snippet demonstrates how to create a completion using a Jinja template in the message content. It uses the OpenAI client to generate a response based on a User model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"Extract the information from the\n            following text: {{ data }}`\"\"\",\n        },\n    ],\n    response_model=User,\n    context={\"data\": \"John Doe is thirty years old\"},\n)\n\nprint(response)\n#> User(name='John Doe', age=30)\n```\n\n----------------------------------------\n\nTITLE: Implementing Role Management with Literal in Pydantic\nDESCRIPTION: This snippet shows an alternative approach using Python's Literal type from the typing module instead of Enum. The code defines a UserDetail model with a role field constrained to specific string values, providing a simpler implementation for standardized roles.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/enums.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n```\n\n----------------------------------------\n\nTITLE: Extracting Table Data from Image URL in Python\nDESCRIPTION: Script that takes an image URL containing table data, extracts the information using an extract_table function, and prints the resulting pandas DataFrame. The code processes a table showing top 10 grossing apps with their rankings and categories.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extracting_tables.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png\"\ntables = extract_table(url)\nfor table in tables:\n    print(table.dataframe)\n    \"\"\"\n                                      Android App   ... Category\n     Android Rank                                   ...\n    1                                   Google One  ...    Social networking\n    2                                      Disney+  ...        Entertainment\n    3                TikTok - Videos, Music & LIVE  ...        Entertainment\n    4                             Candy Crush Saga  ...        Entertainment\n    5               Tinder: Dating, Chat & Friends  ...                Games\n    6                                  Coin Master  ...        Entertainment\n    7                                       Roblox  ...               Dating\n    8               Bumble - Dating & Make Friends  ...                Games\n    9                                  Royal Match  ...             Business\n    10                 Spotify: Music and Podcasts  ...            Education\n\n    [10 rows x 5 columns]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Sequence Diagram of Instructor Request Flow\nDESCRIPTION: This sequence diagram shows the step-by-step flow of an Instructor request, from initial user input through model conversion, request formatting, API communication, response parsing, validation, and potential retry cycles when validation fails.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/architecture.md#2025-04-14_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant User\n    participant Instructor\n    participant ProviderClient\n    participant LLM\n\n    User->>Instructor: create(response_model=Model, messages=[...])\n    Instructor->>Instructor: Convert Pydantic model to schema\n    Instructor->>Instructor: Format request for provider\n    Instructor->>ProviderClient: Forward request with tools/functions\n    ProviderClient->>LLM: Make API request\n    LLM->>ProviderClient: Return response\n    ProviderClient->>Instructor: Return response\n    Instructor->>Instructor: Parse structured data\n    Instructor->>Instructor: Validate against model\n    \n    alt Validation succeeds\n        Instructor->>User: Return validated Pydantic object\n    else Validation fails\n        Instructor->>Instructor: Generate error feedback\n        Instructor->>ProviderClient: Retry with feedback\n        ProviderClient->>LLM: Make new request\n        LLM->>ProviderClient: Return new response\n        ProviderClient->>Instructor: Return new response\n        Instructor->>Instructor: Parse and validate again\n        Instructor->>User: Return validated Pydantic object\n    end\n```\n\n----------------------------------------\n\nTITLE: Multimodal PDF Analysis with Mistral and Instructor\nDESCRIPTION: Python code demonstrating multimodal PDF analysis using Mistral and Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import PDF\nfrom pydantic import BaseModel\nimport instructor\nfrom mistralai import Mistral\nimport os\n\nclass Receipt(BaseModel):\n    total: int\n    items: list[str]\n\nclient = instructor.from_mistral(Mistral(os.environ[\"MISTRAL_API_KEY\"]))\n\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\n\nresponse = client.chat.completions.create(\n    model=\"ministral-8b-latest\",\n    response_model=Receipt,\n    max_tokens=1000,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Extract out the total and line items from the invoice\",\n                PDF.from_url(\n                    url\n                ),  # Also supports PDF.from_path() and PDF.from_base64()\n            ],\n        },\n    ],\n)\n\nprint(response)\n# > Receipt(total=220, items=['English Tea', 'Tofu'])\n```\n\n----------------------------------------\n\nTITLE: Initializing Instructor with Provider Client\nDESCRIPTION: Python code showing how to import and patch a provider client with Instructor to enable structured output capabilities.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/index.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom provider_package import Client\n\nclient = instructor.from_provider(Client())\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor for Citation Validation\nDESCRIPTION: Sets up the OpenAI client with Instructor to enable response model validation and context validation for citations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/exact_citations.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\ndef ask_ai(question: str, context: str) -> QuestionAnswer:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0,\n        response_model=QuestionAnswer,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\",\n            },\n            {\"role\": \"user\", \"content\": f\"{context}\"},\n            {\"role\": \"user\", \"content\": f\"Question: {question}\"},\n        ],\n        validation_context={\"text_chunk\": context},\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Universal Self Prompting Classification with OpenAI\nDESCRIPTION: Python implementation of Universal Self Prompting for emotion classification using OpenAI's API. The code demonstrates a two-stage process: generating balanced example samples across different emotion classes and using these examples to make final predictions. It uses Pydantic for type validation and async operations for efficient API calls.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/usp.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import Literal\nfrom instructor import from_openai\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom collections import defaultdict\n\n\nclass Classification(BaseModel):\n    chain_of_thought: str\n    label: Literal[\"Happy\", \"Angry\", \"Sadness\"]\n    confidence: Literal[\n        \"Uncertain\", \"Somewhat Confident\", \"Confident\", \"Highly Confident\"\n    ]\n\n    def confidence_score(self) -> int:\n        confidence_order = {\n            \"Highly Confident\": 4,\n            \"Confident\": 3,\n            \"Somewhat Confident\": 2,\n            \"Uncertain\": 1,\n        }\n        return confidence_order[self.confidence]\n\n\nclient = from_openai(AsyncOpenAI())\n\n\nasync def generate_prediction(query: str):\n    return (\n        await client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Classify the following query {query} into\n                    one of the following categories: Happy, Angry, Sadness\"\"\",\n                }\n            ],\n            response_model=Classification,\n        ),\n        query,\n    )\n\n\nasync def generate_predictions(queries: list[str]) -> list[tuple[Classification, str]]:\n    return await asyncio.gather(*[generate_prediction(query) for query in queries])\n\n\ndef get_balanced_sample(predictions: list[tuple[Classification, str]], k: int):\n    label_to_queries: dict[str, list[tuple[Classification, str]]] = defaultdict(list)\n\n    for prediction in predictions:\n        label_to_queries[prediction[0].label].append(prediction)\n\n    num_classes = len(label_to_queries)\n    num_samples_per_class = k // num_classes\n\n    res: list[str] = []\n    for label, label_queries in label_to_queries.items():\n        label_queries = sorted(\n            label_queries, key=lambda x: x[0].confidence_score(), reverse=True\n        )\n        label_queries = [\n            label_queries[1] for label_queries in label_queries[:num_samples_per_class]\n        ]\n        res.extend([f\"{query} ({label})\" for query in label_queries])\n\n    return res\n\n\nasync def generate_response_with_examples(query: str, examples: list[str]):\n    formatted_examples = \"\\n\".join(examples)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Classification,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                You are a helpful assistant that classifies queries into one of the following categories: Happy, Angry, Sadness.\n\n                Here are some samples of queries and their categories:\n\n                <examples>\n                {formatted_examples}\n                </examples>\n\n                Here is a user query to classify\n\n                <query>\n                {query}\n                </query>\n                \"\"\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    examples = [\n        \"\"\"\n        i do feel that running is a divine experience and\n        that i can expect to have some type of spiritual\n        encounter\n        \"\"\",\n        \"\"\"\n        i get giddy over feeling elegant in a perfectly\n        fitted pencil skirt\n        \"\"\",\n        \"\"\"\n        i plan to share my everyday life stories traveling\n        adventures inspirations and handmade creations with\n        you and hope you will also feel inspired\n        \"\"\",\n        \"\"\"\n        i need to feel the dough to make sure its just\n        perfect\n        \"\"\",\n        \"\"\"\n        i found myself feeling a little discouraged that\n        morning\n        \"\"\",\n        \"i didnt really feel that embarrassed\",\n        \"i feel like a miserable piece of garbage\",\n        \"\"\"\n        i feel like throwing away the shitty piece of shit\n        paper\n        \"\"\",\n        \"\"\"\n        i feel irritated and rejected without anyone doing\n        anything or saying anything\n        \"\"\",\n        \"i feel angered and firey\",\n        \"\"\"\n        im feeling bitter today my mood has been strange the\n        entire day so i guess its that\n        \"\"\",\n        \"i just feel really violent right now\",\n        \"i know there are days in which you feel distracted\",\n    ]\n\n    labels = asyncio.run(generate_predictions(examples))\n    balanced_sample = get_balanced_sample(labels, 3)\n    for sample in balanced_sample:\n        print(sample)\n\n    response = asyncio.run(\n        generate_response_with_examples(\n            \"\"\"\n            i feel furious that right to life advocates can\n            and do tell me how to live and die through\n            lobbying and supporting those politicians\n            sympathic to their views\n            \"\"\",\n            balanced_sample,\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Accessing Raw LLM Response with Instructor in Python\nDESCRIPTION: Demonstrates how to obtain both the structured Pydantic model and the raw LLM response using the create_with_completion method.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/faq.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresult, completion = client.chat.completions.create_with_completion(\n    response_model=MyModel,\n    messages=[...]\n)\n```\n\n----------------------------------------\n\nTITLE: Using Maybe Type for Uncertain Fields\nDESCRIPTION: Shows how to use Instructor's Maybe type for handling uncertain or ambiguous field values.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/optional_fields.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\nfrom instructor.types import Maybe\n\nclient = instructor.from_openai(OpenAI())\n\nclass PersonInfo(BaseModel):\n    name: str\n    age: Maybe[int] = None  # Maybe type for uncertain fields\n```\n\n----------------------------------------\n\nTITLE: Automatic Validation and Retries with Instructor in Python\nDESCRIPTION: Illustrates Instructor's automatic validation and retry capabilities. This example uses tenacity for custom retry strategies, demonstrating how to handle validation failures and implement exponential backoff.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/google-openai-client.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import Retrying, stop_after_attempt, wait_fixed\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    # Stop after the second attempt and wait a fixed 1 second between attempts\n    max_retries=Retrying(\n        stop=stop_after_attempt(2),\n        wait=wait_fixed(1),\n    ),\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Basic Context Generation Example\nDESCRIPTION: A simple example showing how contextual information is added to document chunks to preserve context\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/situate-context.md#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\noriginal_chunk = \"The company's revenue grew by 3% over the previous quarter.\"\n\ncontextualized_chunk = \"This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter.\"\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Model for Structured Output in Python\nDESCRIPTION: Creates a Pydantic model 'UserExtract' to define the structure of the expected output. This model specifies two fields: name (string) and age (integer).\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/together.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n```\n\n----------------------------------------\n\nTITLE: Using Instructor Library with Automatic Validation and Retries in Python\nDESCRIPTION: This example demonstrates how the instructor library simplifies validation with automatic retries. It uses the same Pydantic model with an uppercase validator, but instructor handles validation failures by regenerating responses automatically.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom pydantic import BaseModel, field_validator\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    def ensure_uppercase(cls, v: str) -> str:\n        if not v.isupper():\n            raise ValueError(\"All letters must be uppercase. Got: \" + v)\n        return v\n\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS_STRICT)\n\nresp = client.chat.completions.create(\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the following user: Jason is 25 years old.\",\n        }\n    ],\n    model=\"gpt-4o-mini\",\n)\n\nprint(resp)\n#> name='JASON' age=25\n```\n\n----------------------------------------\n\nTITLE: Initializing instructor client with OpenAI\nDESCRIPTION: Basic setup for creating an instructor client from an OpenAI client. This replaces the previous instructor.patch method with the new instructor.from_openai approach.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/version-1.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\n\nclient = instructor.from_openai(openai.OpenAI())\n```\n\n----------------------------------------\n\nTITLE: Adding Field Metadata to Weather Forecast Model\nDESCRIPTION: Shows how to add descriptive metadata to model fields using Pydantic's Field class for weather data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass WeatherForecast(BaseModel):\n    \"\"\"Weather forecast for a specific location\"\"\"\n\n    temperature: float = Field(\n        description=\"Current temperature in Celsius\"\n    )\n    condition: str = Field(\n        description=\"Weather condition (sunny, cloudy, rainy, etc.)\"\n    )\n    humidity: int = Field(\n        description=\"Humidity percentage from 0-100\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Full Name Validation Implementation\nDESCRIPTION: Implements name validation using Pydantic with custom validators to ensure names contain spaces and apply uppercase transformation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-1-validation-rag.ipynb#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\nfrom pydantic import BaseModel, AfterValidator, WithJsonSchema\n\ndef name_must_contain_space(v: str) -> str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v\n\ndef uppercase_name(v: str) -> str:\n    return v.upper()\n\nFullName = Annotated[\n    str,\n    AfterValidator(name_must_contain_space),\n    AfterValidator(uppercase_name),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"The user's full name\",\n        }\n    ),\n]\n\nclass UserDetail(BaseModel):\n    age: int\n    name: FullName\n```\n\n----------------------------------------\n\nTITLE: Basic Structured Output Extraction with Cerebras Inference\nDESCRIPTION: Example showing how to use Instructor with Cerebras to extract structured data using a Pydantic model. The code initializes a client, defines a Person model, and extracts name and age from text.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs-with-cerebras-inference.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom cerebras.cloud.sdk import Cerebras\nfrom pydantic import BaseModel\n\nclient = instructor.from_cerebras(Cerebras())\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create(\n    model=\"llama3.1-70b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the name and age of the person in this sentence: John Smith is 29 years old.\",\n        }\n    ],\n    response_model=Person,\n)\n\nprint(resp)\n#> Person(name='John Smith', age=29)\n```\n\n----------------------------------------\n\nTITLE: Implementing Retry Logic with Corrections\nDESCRIPTION: Shows the implementation of retry logic to handle validation errors and generate appropriate responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/self_critique.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nqa: QuestionAnswerNoEvil = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=QuestionAnswerNoEvil,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Enhanced KnowledgeGraph Model with Update Support\nDESCRIPTION: Extended version of the KnowledgeGraph model that includes methods for graph updates and visualization. Adds functionality to combine multiple graphs and render the results using Graphviz.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/knowledge_graph.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass KnowledgeGraph(BaseModel):\n    nodes: Optional[List[Node]] = Field(..., default_factory=list)\n    edges: Optional[List[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -> \"KnowledgeGraph\":\n        \"\"\"Updates the current graph with the other graph, deduplicating nodes and edges.\"\"\"\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),\n            edges=list(set(self.edges + other.edges)),\n        )\n\n    def draw(self, prefix: str = None):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(str(node.id), node.label, color=node.color)\n\n        for edge in self.edges:\n            dot.edge(\n                str(edge.source), str(edge.target), label=edge.label, color=edge.color\n            )\n        dot.render(prefix, format=\"png\", view=True)\n```\n\n----------------------------------------\n\nTITLE: Anthropic Integration Example\nDESCRIPTION: Shows how to switch from OpenAI to Anthropic provider while maintaining the same functionality for user information extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\nclient = instructor.from_anthropic(Anthropic())\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresp = client.chat.completions.create(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the user from the string belo - Chris is a 27 year old engineer in San Francisco\",\n        }\n    ],\n    max_tokens=100,\n)\n\nprint(resp)\n#> name='Chris' age=27\n```\n\n----------------------------------------\n\nTITLE: Asynchronous User Extraction with Mistral and Instructor\nDESCRIPTION: Python code demonstrating asynchronous user extraction using Mistral and Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport asyncio\nfrom pydantic import BaseModel\nfrom mistralai import Mistral\nfrom instructor import from_mistral, Mode\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n# Initialize with API key\nclient = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n\n# Enable instructor patches for async Mistral client\ninstructor_client = from_mistral(\n    client=client,\n    mode=Mode.MISTRAL_TOOLS,\n    use_async=True,\n)\n\nasync def extract_user():\n    user = await instructor_client.chat.completions.create(\n        response_model=User,\n        messages=[{\"role\": \"user\", \"content\": \"Jack is 28 years old.\"}],\n        temperature=0,\n        model=\"mistral-large-latest\",\n    )\n    return user\n\n# Run async function\nuser = asyncio.run(extract_user())\nprint(user)\n# Output: User(name='Jack', age=28)\n```\n\n----------------------------------------\n\nTITLE: Field Validation Example\nDESCRIPTION: Shows how to use Pydantic's Field class for primitive type validation, specifically for age validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-1-validation-rag.ipynb#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import Field\n\nAge = Annotated[int, Field(gt=0)]\n\nclass UserDetail(BaseModel):\n    age: Age\n    name: FullName\n\ntry:\n    person = UserDetail(age=-10, name=\"Jason\")\nexcept Exception as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Implementing Demonstration Ensembling (DENSE) for Text Classification with Instructor\nDESCRIPTION: This code demonstrates how to implement the Demonstration Ensembling (DENSE) technique for text classification. It creates multiple prompts with different subsets of examples, uses async OpenAI calls to generate responses for each subset, and then aggregates the results using a voting mechanism to determine the final classification.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/dense.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom collections import Counter\nfrom typing import Literal\nfrom textwrap import dedent\n\n\nclass DemonstrationResponse(BaseModel):\n    correct_answer: Literal[\"Positive\", \"Negative\", \"Neutral\"]\n\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nasync def generate_self_consistent_response(prompt: str, examples: list[str]):\n    concetenated_examples = \"\\n\".join(examples)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are an intelligent AI System that excels\n                at classifying user queries into three\n                possible labels:\n                - Positive\n                - Negative\n                - Neutral\n\n                You are about to be given a user query and\n                asked to classify it into one of the three\n                categories. Make sure to refer closely to\n                the examples provided to you, examining each\n                individual example before coming up with the\n                final answer.\n\n                Here are the examples:\n                {concetenated_examples}\n                \"\"\"\n                ),\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_model=DemonstrationResponse,\n        temperature=0,\n    )\n\n\nasync def generate_self_consistent_responses(\n    prompt: str, num_responses: int, examples: list[str]\n):\n    assert (\n        len(examples) % num_responses == 0\n    ), \"The number of examples must be evenly divisible by num_responses\"\n\n    # Batch the examples into num_responses batches\n    batch_size = len(examples) // num_responses\n\n    coros = [\n        generate_self_consistent_response(prompt, examples[i : i + batch_size])\n        for i in range(0, len(examples), batch_size)\n    ]\n\n    responses = await asyncio.gather(*coros)\n    return responses\n\n\nif __name__ == \"__main__\":\n    user_query = \"What is the weather like today?\"\n    examples = [\n        \"I love this product! [Positive]\",\n        \"This is the worst service ever. [Negative]\",\n        \"The movie was okay, not great but not terrible. [Neutral]\",\n        \"I'm so happy with my new phone! [Positive]\",\n        \"The food was terrible and the service was slow. [Negative]\",\n        \"It's an average day, nothing special. [Neutral]\",\n        \"Fantastic experience, will come again! [Positive]\",\n        \"I wouldn't recommend this to anyone. [Negative]\",\n        \"The book was neither good nor bad. [Neutral]\",\n        \"Absolutely thrilled with the results! [Positive]\",\n    ]\n    responses = asyncio.run(generate_self_consistent_responses(user_query, 5, examples))\n    answer_counts = Counter([response.correct_answer for response in responses])\n    most_common_answer, _ = answer_counts.most_common(1)[0]\n    print(most_common_answer)\n    #> Neutral\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with OpenAI-Compatible Providers\nDESCRIPTION: Demonstrates how to use Instructor with providers offering an OpenAI-compatible API, such as Azure OpenAI and Groq.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/client_setup.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\n# Example for Azure OpenAI\nazure_client = instructor.from_openai(\n    OpenAI(\n        api_key=\"your-azure-api-key\",\n        base_url=\"https://your-resource-name.openai.azure.com/openai/deployments/your-deployment-name\"\n    )\n)\n\n# Example for Groq\ngroq_client = instructor.from_openai(\n    OpenAI(\n        api_key=\"your-groq-api-key\",\n        base_url=\"https://api.groq.com/openai/v1\"\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Enhanced Node and Edge Classes with Hash Implementation\nDESCRIPTION: Extends Node and Edge classes with hash functionality for deduplication.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n    def __hash__(self) -> int:\n        return hash((id, self.label))\n\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n    def __hash__(self) -> int:\n        return hash((self.source, self.target, self.label))\n```\n\n----------------------------------------\n\nTITLE: Synchronous User Data Extraction with Fireworks\nDESCRIPTION: Demonstrates basic synchronous usage of Instructor with Fireworks AI to extract structured user data using Pydantic models\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/fireworks.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fireworks.client import Fireworks\nimport instructor\nfrom pydantic import BaseModel\n\n# Initialize the client\nclient = Fireworks()\n\n# Enable instructor patches\nclient = instructor.from_fireworks(client)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Create structured output\nuser = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract: Jason is 25 years old\",\n        }\n    ],\n    model=\"accounts/fireworks/models/llama-v3-8b-instruct\",\n    response_model=User,\n)\n\nprint(user)\n# > User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor Patching for Markdown JSON Mode\nDESCRIPTION: This snippet demonstrates how to initialize an OpenAI client with Instructor patching using the MD_JSON mode. This mode is not recommended and may not be supported in the future, but is left to support vision models and models provided by Databricks.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.MD_JSON)\n```\n\n----------------------------------------\n\nTITLE: Image Processing Setup with Gemini\nDESCRIPTION: Demonstrates setup for processing images using Instructor with Google GenAI SDK\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import Image\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom google.genai import Client\n\n\nclass ImageDescription(BaseModel):\n    objects: list[str] = Field(..., description=\"The objects in the image\")\n    scene: str = Field(..., description=\"The scene of the image\")\n    colors: list[str] = Field(..., description=\"The colors in the image\")\n\n\nclient = instructor.from_genai(Client())\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/image.jpg\"\n```\n\n----------------------------------------\n\nTITLE: LLM-based Topic Validation\nDESCRIPTION: Implements topic validation using LLM to ensure responses stay within specific subject areas.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-1-validation-rag.ipynb#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import llm_validator\n\nHealthTopicStr = Annotated[\n    str,\n    AfterValidator(\n        llm_validator(\n            \"don't talk about any other topic except health best practices and topics\",\n            client=client,\n        )\n    ),\n]\n\nclass AssistantMessage(BaseModel):\n    message: HealthTopicStr\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Gemini Client with Instructor\nDESCRIPTION: Illustrates the setup of a Google Gemini client using Instructor, specifying the model and mode.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/client_setup.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\n\ngenai.configure(api_key=\"YOUR_API_KEY\")\nmodel = genai.GenerativeModel(\"gemini-1.5-flash\")\n\nclient = instructor.from_gemini(\n    model,\n    mode=instructor.Mode.GEMINI_TOOLS  # or GEMINI_JSON\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Anthropic Support\nDESCRIPTION: Command to install the instructor client with Anthropic integration\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anthropic.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[anthropic]\"\n```\n\n----------------------------------------\n\nTITLE: Basic Structured Data Extraction with Gemini\nDESCRIPTION: Demonstrates basic usage of Instructor with Google GenAI SDK to extract structured data using a Pydantic model\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom google import genai\nimport instructor\nfrom pydantic import BaseModel\n\n# Define your Pydantic model\nclass User(BaseModel):\n    name: str\n    age: int\n\n# Initialize and patch the client\nclient = genai.Client()\nclient = instructor.from_genai(client, mode=instructor.Mode.GENAI_TOOLS)\n\n# Extract structured data\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    messages=[{\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"}],\n    response_model=User,\n)\n\nprint(response)  # User(name='Jason', age=25)\n```\n\n----------------------------------------\n\nTITLE: Calculating Normalized Entropy in COSP - LaTeX Formula\nDESCRIPTION: Mathematical formula for computing normalized entropy of generated responses. Takes into account the frequency of unique answers among multiple generations to measure consistency.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/cosp.md#2025-04-14_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n\\mathcal{H}\\left(x^{(i)} \\mid \\left\\{\\hat{y}_j^{(i)}\\right\\}_{j=1}^m\\right) = \\frac{\\sum_{\\alpha=1}^u \\hat{p}\\left(\\hat{y}_{\\alpha}^{(i)}\\right) \\log \\hat{p}\\left(\\hat{y}_{\\alpha}^{(i)}\\right)}{\\log m}\n```\n\n----------------------------------------\n\nTITLE: Setting Databricks Environment Variables\nDESCRIPTION: Commands to set up required Databricks API key and workspace URL as environment variables.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/databricks.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport DATABRICKS_API_KEY=your_api_key_here\nexport DATABRICKS_HOST=your_workspace_url\n```\n\n----------------------------------------\n\nTITLE: Streaming multiple objects with create_iterable\nDESCRIPTION: Uses create_iterable to extract multiple objects from a single LLM response. This allows processing a sequence of objects as they're generated by the model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/version-1.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create_iterable(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create 2 users\"},\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n    #> name='John Doe' age=30\n    #> name='Jane Smith' age=28\n    # User(name='John Doe', age=30)\n    # User(name='Jane Smith', age=25)\n```\n\n----------------------------------------\n\nTITLE: Context-Based Content Moderation\nDESCRIPTION: Implements content moderation using context-based validation with blacklisted words.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-1-validation-rag.ipynb#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import ValidationInfo\n\ndef message_cannot_have_blacklisted_words(v: str, info: ValidationInfo) -> str:\n    blacklist = info.context.get(\"blacklist\", [])\n    for word in blacklist:\n        assert word not in v.lower(), f\"`{word}` was found in the message `{v}`\"\n    return v\n\nModeratedStr = Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\nclass Response(BaseModel):\n    message: ModeratedStr\n```\n\n----------------------------------------\n\nTITLE: Streaming partial response objects with create_partial\nDESCRIPTION: Demonstrates the create_partial method which streams partially complete objects as they're being generated. This allows tracking the LLM's progress in constructing the response object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/version-1.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser_stream = client.chat.completions.create_partial(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n\nfor user in user_stream:\n    print(user)\n    #> name=None age=None\n    #> name=None age=None\n    #> name=None age=None\n    #> name=None age=25\n    #> name=None age=25\n    #> name=None age=25\n    #> name='' age=25\n    #> name='John' age=25\n    #> name='John Smith' age=25\n    #> name='John Smith' age=25\n    # name=None age=None\n    # name='' age=None\n    # name='John' age=None\n    # name='John Doe' age=None\n    # name='John Doe' age=30\n```\n\n----------------------------------------\n\nTITLE: Defining Query Models for Temporal Context\nDESCRIPTION: Creates Pydantic models for handling date ranges and query restructuring in search operations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import date\n\nclass DateRange(BaseModel):\n    start: date\n    end: date\n\nclass Query(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with VertexAI SDK\nDESCRIPTION: Python code showing how to use the instructor library with VertexAI SDK to extract structured data. It initializes VertexAI, defines a User model, and processes a chat completion request.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/announcing-gemini-tool-calling-support.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nfrom pydantic import BaseModel\n\nvertexai.init()\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),  # (1)!\n)\n\n\nresp = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#> name='Jason' age=25\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Extraction with OpenAI\nDESCRIPTION: Processes text chunks using OpenAI's GPT-4 model to extract structured information based on the Extraction model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\nfrom collections.abc import Iterable\n\nextractions = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    stream=True,\n    response_model=Iterable[Extraction],\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",\n        },\n        {\"role\": \"user\", \"content\": text_chunk},\n    ],\n)\n\nfor extraction in extractions:\n    pprint(extraction.model_dump())\n```\n\n----------------------------------------\n\nTITLE: Loading Images with Instructor\nDESCRIPTION: Demonstrates how to load multiple images using the Instructor library's Image class from local files.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extracting-model-metadata.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\n\n# Load images using instructor.Image.from_path\nimages = []\nfor image_file in image_files:\n    image_path = os.path.join(\"./images\", image_file)\n    image = instructor.Image.from_path(image_path)\n    images.append(image)\n```\n\n----------------------------------------\n\nTITLE: Importing Instructor and OpenAI for Personal Assistant Example\nDESCRIPTION: This snippet shows the initial import statements needed to use the Instructor library with OpenAI for the personal assistant example, setting up the environment for executing structured queries.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-and-beyond.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Question-Answer Pydantic Model\nDESCRIPTION: Creates a simple Pydantic model with question and answer fields for structuring the response data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/validators/readme.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Model for Chapter Information in Python\nDESCRIPTION: This Python code defines a Pydantic model 'Chapter' to structure the chapter information extracted from YouTube transcripts. It includes fields for start and end timestamps, title, and summary.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-transcripts.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass Chapter(BaseModel):\n    start_ts: float = Field(\n        ...,\n        description=\"Starting timestamp for a chapter.\",\n    )\n    end_ts: float = Field(\n        ...,\n        description=\"Ending timestamp for a chapter\",\n    )\n    title: str = Field(\n        ..., description=\"A concise and descriptive title for the chapter.\"\n    )\n    summary: str = Field(\n        ...,\n        description=\"A brief summary of the chapter's content, don't use words like 'the speaker'\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Git Development Workflow Commands\nDESCRIPTION: Git commands for setting up and managing the development workflow.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/contributing.md#2025-04-14_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/YOUR-USERNAME/instructor.git\ncd instructor\ngit remote add upstream https://github.com/instructor-ai/instructor.git\n\ngit checkout -b feature/your-feature-name\n\n# Run tests\npytest tests/ -k 'not llm and not openai'  # Skip LLM tests for faster local dev\n\n# Commit changes\ngit add .\ngit commit -m \"Your descriptive commit message\"\n\ngit fetch upstream\ngit rebase upstream/main\ngit push origin feature/your-feature-name\n```\n\n----------------------------------------\n\nTITLE: Visualizing Core Components of Instructor Architecture in Mermaid\nDESCRIPTION: This diagram illustrates the key components of Instructor and how they interact, from user code and Pydantic models through schema conversion, provider-specific adaptation, request handling, response parsing, validation, and retry mechanisms.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/architecture.md#2025-04-14_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    User[User Code] --> PydanticModel[Pydantic Model]\n    User --> ClientInit[Initialize Provider Client]\n    ClientInit --> PatchedClient[Patched LLM Client]\n    PydanticModel --> SchemaConverter[Schema Converter]\n    SchemaConverter --> ProviderAdapter[Provider-Specific Adapter]\n    PatchedClient --> APIRequest[API Request]\n    ProviderAdapter --> APIRequest\n    APIRequest --> ResponseParser[Response Parser]\n    ResponseParser --> Validator[Pydantic Validator]\n    Validator -- Valid --> StructuredOutput[Structured Output]\n    Validator -- Invalid --> RetryMechanism[Retry Mechanism]\n    RetryMechanism --> APIRequest\n```\n\n----------------------------------------\n\nTITLE: Defining Validation Class for LLM Output Validation in Python\nDESCRIPTION: This snippet defines a Pydantic model called Validation for reusable validation of LLM outputs. It includes fields for validity status and error messages.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\n\nclass Validation(BaseModel):\n    is_valid: bool = Field(\n        ..., description=\"Whether the value is valid based on the rules\"\n    )\n    error_message: Optional[str] = Field(\n        ...,\n        description=\"The error message if the value is not valid, to be used for re-asking the model\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Updating Build System Configuration in pyproject.toml\nDESCRIPTION: This TOML configuration updates the build system to use hatchling instead of setuptools, which is necessary for handling Metadata 2.4 keys when migrating from Poetry to UV.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/migrating-to-uv.md#2025-04-14_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Evaluation Results with Pandas in Python\nDESCRIPTION: This code uses pandas to create a DataFrame from the evaluation results and calculates the mean scores for each metric.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame(fts_results)\ndf.mean()\n```\n\n----------------------------------------\n\nTITLE: Initializing Databricks Client with Instructor Patching for Markdown JSON Mode\nDESCRIPTION: This snippet shows how to initialize a Databricks client with Instructor patching using the MD_JSON mode. It requires setting Databricks environment variables for authentication and endpoint URL.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport os\nfrom openai import OpenAI\n\nDATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_TOKEN\", \"\")\nDATABRICKS_HOST = os.environ.get(\"DATABRICKS_HOST\", \"\")\n\n# Assuming Databricks environment variables are set\nclient = instructor.from_openai(\n    OpenAI(\n        api_key=DATABRICKS_TOKEN,\n        base_url=f\"{DATABRICKS_HOST}/serving-endpoints\",\n    ),\n    mode=instructor.Mode.MD_JSON,\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Parallel Story Generation in Mermaid\nDESCRIPTION: A mermaid diagram illustrating the parallel story generation approach with a hierarchical structure. The diagram shows a start node branching into decisions which lead to outcomes, with styling to distinguish different node types and tooltips for context.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extract-model-looks.md#2025-04-14_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    %% Main nodes\n    A[Find Door] --> B[Open Door]\n    A --> C[Walk Away]\n\n    B --> D[Read Book]\n    B --> E[Leave Room]\n\n    C --> F[Go Home]\n    C --> G[Wait Outside]\n\n    %% Styling for visual hierarchy\n    classDef start fill:#ff9999,stroke:#333,stroke-width:2px\n    classDef decision fill:#99ccff,stroke:#333,stroke-width:2px\n    classDef outcome fill:#99ffff,stroke:#333,stroke-width:1px\n\n    %% Apply styles\n    class A start\n    class B,C decision\n    class D,E,F,G outcome\n\n    %% Add tooltips for context\n    click B \"Door context\" \"Open Door Context\"\n    click C \"Away context\" \"Walk Away Context\"\n    click D \"Door and Book context\" \"Read Book Context\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Literal Types for Classification\nDESCRIPTION: Shows how to use Literal types for simple classification tasks with specific string options.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/types.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom typing import Literal\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Literal[\"BILLING\", \"SHIPPING\"],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Response Validation Implementation\nDESCRIPTION: Shows how to implement response validation using Pydantic Field validators with automatic reattempts on validation failures.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclass UserWithValidation(BaseModel):\n    name: str\n    age: int = Field(gt=0, lt=150)  # Age must be between 0 and 150\n    email: str = Field(pattern=r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\")\n\nclient = instructor.from_openai(OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserWithValidation,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract the user: John Doe is 30 years old, email is john@example.com\"}\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Groq API Key\nDESCRIPTION: Environment variable setup for Groq API authentication\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/open_source.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport GROQ_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Performing Full Text Search and Retrieving Chunk IDs in Python\nDESCRIPTION: This code performs a full text search on a table using a sample question, retrieves the top 25 results, and extracts the chunk IDs from the retrieved items.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nretrieved_results = (\n    table.search(sample_question.question, query_type=\"fts\").limit(25).to_list()\n)\nretrieved_chunk_ids = [item[\"chunk_id\"] for item in retrieved_results]\n\nretrieved_chunk_ids[:3]\n```\n\n----------------------------------------\n\nTITLE: Nested Unions for Content Types\nDESCRIPTION: Shows how to implement nested unions for handling different content types in messages.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/unions.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union, List\nfrom pydantic import BaseModel\n\n\nclass TextContent(BaseModel):\n    type: Literal[\"text\"]\n    text: str\n\n\nclass ImageContent(BaseModel):\n    type: Literal[\"image\"]\n    url: str\n\n\nclass Message(BaseModel):\n    content: List[Union[TextContent, ImageContent]]\n```\n\n----------------------------------------\n\nTITLE: Printing Generated Summaries in Python\nDESCRIPTION: This code prints each summary in the chain of summaries, allowing for qualitative assessment of the summarization process and the increase in entity density.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfor summary in summaries:\n    print(f\"\\n{summary}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Testing the LLM Reranker with Sample Data in Python\nDESCRIPTION: Demonstrates how to use the reranker with a sample query and text chunks. The function processes the query, evaluates and sorts the chunks based on relevance, and outputs the ranked results with reasoning.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/llm-as-reranker.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    query = \"What are the health benefits of regular exercise?\"\n    chunks = [\n        {\n            \"id\": 0,\n            \"text\": \"Regular exercise can improve cardiovascular health and reduce the risk of heart disease.\",\n        },\n        {\n            \"id\": 1,\n            \"text\": \"The price of gym memberships varies widely depending on location and facilities.\",\n        },\n        {\n            \"id\": 2,\n            \"text\": \"Exercise has been shown to boost mood and reduce symptoms of depression and anxiety.\",\n        },\n        {\n            \"id\": 3,\n            \"text\": \"Proper nutrition is essential for maintaining a healthy lifestyle.\",\n        },\n        {\n            \"id\": 4,\n            \"text\": \"Strength training can increase muscle mass and improve bone density, especially important as we age.\",\n        },\n    ]\n\n    results = rerank_results(query, chunks)\n\n    print(\"Reranked results:\")\n    for label in results.labels:\n        print(f\"Chunk {label.chunk_id} (Relevancy: {label.relevancy}):\")\n        print(f\"Text: {chunks[label.chunk_id]['text']}\")\n        print(f\"Reasoning: {label.chain_of_thought}\")\n        print()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Document Line Preprocessing Function\nDESCRIPTION: Creates a preprocessor function that adds line numbers to document text and builds a mapping between line numbers and content for later reference.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/document_segmentation.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef doc_with_lines(document):\n    document_lines = document.split(\"\\n\")\n    document_with_line_numbers = \"\"\n    line2text = {}\n    for i, line in enumerate(document_lines):\n        document_with_line_numbers += f\"[{i}] {line}\\n\"\n        line2text[i] = line\n    return document_with_line_numbers, line2text\n```\n\n----------------------------------------\n\nTITLE: Partial Streaming with Fireworks\nDESCRIPTION: Shows how to implement partial streaming of structured responses from Fireworks AI models\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/fireworks.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fireworks.client import Fireworks\nimport instructor\nfrom pydantic import BaseModel\n\n\n# Enable instructor patches\nclient = instructor.from_fireworks(Fireworks())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    bio: str\n\n\nuser = client.chat.completions.create_partial(\n    model=\"accounts/fireworks/models/llama-v3-8b-instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Create a user profile for Jason + 1 sentence bio, age 25\",\n        },\n    ],\n    response_model=User,\n)\n\nfor user_partial in user:\n    print(user_partial)\n```\n\n----------------------------------------\n\nTITLE: Nested Data Structure Extraction\nDESCRIPTION: Complex example demonstrating nested Pydantic models for structured data extraction including address information\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cortex.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import from_openai\nfrom pydantic import BaseModel\nimport openai\n\nclient = from_openai(\n    openai.OpenAI(\n        base_url=\"http://localhost:39281/v1\",\n        api_key=\"this is a fake api key that doesn't matter\",\n    )\n)\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\nuser = client.chat.completions.create(\n    model=\"llama3.2:3b-gguf-q4-km\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract: Jason is 25 years old.\n            He lives at 123 Main St, New York, USA\n            and has a summer house at 456 Beach Rd, Miami, USA\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n\nprint(user)\n```\n\n----------------------------------------\n\nTITLE: Creating a Logfire-Instrumented Classification Function with Instructor\nDESCRIPTION: Function to classify text using Instructor and OpenAI, instrumented with Logfire for observability. The logfire.instrument decorator tags and tracks the function execution, capturing arguments and performance metrics.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@logfire.instrument(\"classification\", extract_args=True)  # (1)!\ndef classify(data: str) -> SinglePrediction:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=SinglePrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: {data}\",\n            },\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Metric Implementation - Recall\nDESCRIPTION: Implements Recall metric for evaluating search result completeness\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef recall(results, relevant_chunks):\n    return sum([1 if chunk in results else 0 for chunk in relevant_chunks]) / len(\n        relevant_chunks\n    )\n```\n\n----------------------------------------\n\nTITLE: Basic Synchronous Data Extraction with Provider\nDESCRIPTION: Example of structured data extraction using the provider's synchronous client implementation with Instructor and Pydantic models\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/templates/provider_template.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Standard library imports\nimport os\nfrom typing import Optional\n\n# Third-party imports\nimport instructor\nfrom provider_sdk import ClientClass\nfrom pydantic import BaseModel, Field\n\n# Set up environment (typically handled before script execution)\n# os.environ[\"PROVIDER_API_KEY\"] = \"your-api-key\"  # Uncomment and replace with your API key if not set\n\n# Initialize the client with explicit mode\nclient = instructor.from_provider(\n    ClientClass(\n        api_key=os.environ.get(\"PROVIDER_API_KEY\", \"your_api_key_here\"),\n        # Other configuration options\n    ),\n    mode=instructor.Mode.PROVIDER_SPECIFIC_MODE,\n)\n\n# Define your data structure with proper annotations\nclass UserExtract(BaseModel):\n    \"\"\"Model for extracting user information from text.\"\"\"\n    name: str = Field(description=\"The user's full name\")\n    age: int = Field(description=\"The user's age in years\")\n\n# Extract structured data\ntry:\n    user = client.chat.completions.create(\n        model=\"provider-model-name\",  # Use latest stable model version\n        response_model=UserExtract,\n        messages=[\n            {\"role\": \"system\", \"content\": \"Extract structured user information from the text.\"},\n            {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n        ],\n    )\n    \n    print(user.model_dump_json(indent=2))\n    # Expected output:\n    # {\n    #   \"name\": \"Jason\",\n    #   \"age\": 25\n    # }\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Markdown to DataFrame Converter for Vision Models\nDESCRIPTION: Implementation of a custom converter that transforms markdown table text into a pandas DataFrame. This is used to process structured data extracted from images by GPT-4V.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BeforeValidator, InstanceOf, WithJsonSchema\n\n\ndef md_to_df(data: Any) -> Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],  # (1)!\n    BeforeValidator(md_to_df),  # (2)!\n    WithJsonSchema(  # (3)!\n        {\n            \"type\": \"string\",\n            \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be separate\",\n        }\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Viewing Cancel Command Help\nDESCRIPTION: Shows the help menu for the cancel command, which allows cancellation of a batch job by providing its ID.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ instructor batch cancel --help\n\n Usage: instructor batch cancel [OPTIONS]\n\n Cancel a batch job\n\n╭─ Options ───────────────────────────────────────────────────────────────────────────╮\n│ *  --batch-id        TEXT  Batch job ID to cancel [default: None] [required]        │\n│    --use-anthropic        Use Anthropic API instead of OpenAI                       │\n│                           [default: False]                                          │\n│    --help                 Show this message and exit.                               │\n╰─────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n----------------------------------------\n\nTITLE: Nested Object Handling with Groq AI\nDESCRIPTION: Example of handling nested object structures using Groq AI, demonstrating complex data extraction with multiple addresses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/groq.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom groq import Groq\nimport instructor\nfrom pydantic import BaseModel\n\n# Initialize with API key\nclient = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n\n# Enable instructor patches for Groq client\nclient = instructor.from_groq(client)\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\n# Create structured output with nested objects\nuser = client.chat.completions.create(\n    model=\"llama3-groq-70b-8192-tool-use-preview\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n            Extract: Jason is 25 years old.\n            He lives at 123 Main St, New York, USA\n            and has a summer house at 456 Beach Rd, Miami, USA\n        \"\"\",\n        },\n    ],\n    response_model=User,\n)\n\nprint(user)\n```\n\n----------------------------------------\n\nTITLE: Setting Instructor Modes\nDESCRIPTION: Demonstrates different modes for controlling how Instructor gets structured data from LLMs\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/start-here.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Using OpenAI's function calling\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n\n# Using JSON output directly\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.JSON)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies\nDESCRIPTION: Lists required Python packages and their versions for the project. Includes core dependencies like pydantic, openai, instructor, logfire, and FastAPI with specific version constraints.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/logfire-fastapi/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\npydantic==2.7.1\nopenai==1.24.1\ninstructor==1.0.3\nlogfire==0.28.0\nfastapi==0.110.3\nuvicorn[standard]\nlogfire[fastapi]\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Model for Email Classification with Instructor\nDESCRIPTION: Pydantic model definition for classifying emails as either spam or not spam. This model uses an enum to define the possible classification labels.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport enum\n\n\nclass Labels(str, enum.Enum):\n    \"\"\"Enumeration for single-label text classification.\"\"\"\n\n    SPAM = \"spam\"\n    NOT_SPAM = \"not_spam\"\n\n\nclass SinglePrediction(BaseModel):\n    \"\"\"\n    Class for a single class label prediction.\n    \"\"\"\n\n    class_label: Labels\n```\n\n----------------------------------------\n\nTITLE: Installing LangSmith and Instructor Packages\nDESCRIPTION: Commands to install both LangSmith SDK and Instructor packages\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/batch_classification_langsmith.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -U langsmith\npip install -U instructor\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Local AI Classification\nDESCRIPTION: Commands to install instructor and pydantic packages needed for the classification system.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/local_classification.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor pydantic\n```\n\n----------------------------------------\n\nTITLE: Optional Fields in Pydantic LLM Model\nDESCRIPTION: Shows how to define optional fields in a Pydantic model using Optional type hints and default values. Includes an optional email field.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/models.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n    email: Optional[str] = Field(description=\"The email of the user.\", default=None)\n```\n\n----------------------------------------\n\nTITLE: Synchronous User Data Extraction\nDESCRIPTION: Example of synchronous user data extraction using Pydantic models and Instructor\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cortex.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import from_openai\nfrom pydantic import BaseModel\nimport openai\n\nclient = from_openai(\n    openai.OpenAI(\n        base_url=\"http://localhost:39281/v1\",\n        api_key=\"this is a fake api key that doesn't matter\",\n    )\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create(\n    model=\"llama3.2:3b-gguf-q4-km\",\n    messages=[{\"role\": \"user\", \"content\": \"Ivan is 27 and lives in Singapore\"}],\n    response_model=User,\n)\n\nprint(resp)\n# > name='Ivan', age=27\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Gemini Support\nDESCRIPTION: Command to install the instructor library with Google GenerativeAI support for Gemini SDK integration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/announcing-gemini-tool-calling-support.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[google-generativeai]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Fine-Tuning Job from ID in Instructor CLI\nDESCRIPTION: This snippet shows the options available when creating a fine-tuning job from an existing ID using the Instructor CLI. It includes parameters for model selection, fine-tuning settings, and validation file ID.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/finetune.md#2025-04-14_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n❯ instructor jobs create-from-id --help\n\n Usage: instructor jobs create-from-id [OPTIONS] ID\n\n Create a fine-tuning job from an existing ID.\n\n╭─ Arguments ───────────────────────────────────────────────────────────────────────────╮\n│ *    id      TEXT  ID of the existing fine-tuning job [default: None] [required]      │\n╰───────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Options ─────────────────────────────────────────────────────────────────────────────╮\n│ --model                           TEXT     Model to use for fine-tuning               │\n│                                            [default: gpt-3.5-turbo]                   │\n│ --n-epochs                        INTEGER  Number of epochs for fine-tuning           │\n│ --batch-size                      TEXT     Batch size for fine-tuning                 │\n│ --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning   │\n│ --validation-file-id              TEXT     ID of the uploaded validation file         │\n│                                            [default: None]                            │\n│ --help                                     Show this message and exit.                │\n╰───────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n----------------------------------------\n\nTITLE: Implementing In-Memory Caching with functools.cache\nDESCRIPTION: Demonstrates how to use functools.cache for simple in-memory caching of function results. This is ideal for functions with immutable arguments called repeatedly in small to medium-sized applications.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/caching.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport functools\n\n\n@functools.cache\ndef extract(data):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt Subcommand for Instructor-AI CLI in Python\nDESCRIPTION: This snippet defines the 'prompt' subcommand for the Instructor-AI CLI. It takes various options such as model, temperature, and max tokens, and executes the prompt functionality.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/jobs.md#2025-04-14_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n@cli.command()\n@click.option(\"--model\", default=\"gpt-3.5-turbo\", help=\"The model to use for the prompt.\")\n@click.option(\"--temperature\", default=0.7, help=\"The temperature to use for the prompt.\")\n@click.option(\"--max-tokens\", default=256, help=\"The maximum number of tokens to generate.\")\n@click.option(\"--stop\", multiple=True, help=\"The stop sequence(s) to use for the prompt.\")\n@click.option(\"--prompt\", prompt=\"Enter your prompt\", help=\"The prompt to use.\")\ndef prompt(model, temperature, max_tokens, stop, prompt):\n    \"\"\"Generate a response to a prompt.\"\"\"\n    click.echo(f\"Generating response for prompt: {prompt}\")\n    # Add logic to generate response here\n```\n\n----------------------------------------\n\nTITLE: Initializing the Gemini Client with Instructor\nDESCRIPTION: Sets up the Gemini client using Instructor, configuring it to use the latest Gemini 1.5 Pro model for generating structured responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generating-pdf-citations.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-pro-latest\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Inserting AI-Generated Hero into SQLite Database using SQLModel in Python\nDESCRIPTION: This snippet demonstrates how to create a SQLite database engine, generate a hero using the create_hero function, and insert the hero into the database using a SQLModel session.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/sqlmodel.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nengine = create_engine(\"sqlite:///database.db\")\nSQLModel.metadata.create_all(engine)\n\nhero = create_hero()\n\n# The Raw Response from the LLM will not have an id due to the SkipJsonSchema\nprint(hero._raw_response.choices[0].message.content)\n#> {'name': 'Superman', 'secret_name': 'Clark Kent', 'age': 30}\n\n# The model_dump() method will include the generated id as it has been loaded as a Hero object\nprint(hero.model_dump())\n#> {'name': 'Superman', 'secret_name': 'Clark Kent', 'age': 30, 'id': UUID('1234-5678-...')}\n\nwith Session(engine) as session:\n    session.add(hero)\n    session.commit()\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Lines File for Batch Job with Python\nDESCRIPTION: Python script that creates a JSONL file for batch processing. The script defines a Classification model and generates messages for email spam classification using Instructor's BatchJob utility.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.batch import BatchJob\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\nclass Classification(BaseModel):\n    label: Literal[\"SPAM\", \"NOT_SPAM\"] = Field(\n        ..., description=\"Whether the email is spam or not\"\n    )\n\nemails = [\n    \"Hello there I'm a Nigerian prince and I want to give you money\",\n    \"Meeting with Thomas has been set at Friday next week\",\n    \"Here are some weekly product updates from our marketing team\",\n]\n\nmessages = [\n    [\n        {\n            \"role\": \"system\",\n            \"content\": f\"Classify the following email {email}\",\n        }\n    ]\n    for email in emails\n]\n\nimport json\n\nwith open(\"output.jsonl\", \"w\") as f:\n    for line in BatchJob.create_from_messages(\n        messages,\n        model=\"gpt-3.5-turbo\",\n        response_model=Classification,\n        max_tokens=100,\n    ):\n        f.write(json.dumps(line) + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Entity Extraction using OpenAI API\nDESCRIPTION: Implements entity extraction using OpenAI's API with the Instructor library. The function asks the AI to extract and resolve entities from input content, returning a DocumentExtraction object.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/entity_resolution.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\ndef ask_ai(content) -> DocumentExtraction:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=DocumentExtraction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Extract and resolve a list of entities from the following document:\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": content,\n            },\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Question-Answer Pair Model Definition\nDESCRIPTION: Defines Pydantic model for generating synthetic questions and answers\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(description=\"The generated question from the text chunk.\")\n    answer: str = Field(description=\"The answer to the generated question.\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing Message Handler Selection in Python\nDESCRIPTION: This snippet demonstrates the optimization of the handle_reask_kwargs function, replacing a large mapping dictionary with direct conditional checks to reduce memory overhead and improve lookup performance.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/dictionary_operations.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef handle_reask_kwargs(kwargs, mode, response, exception):\n    kwargs = kwargs.copy()\n    functions = {\n        Mode.ANTHROPIC_TOOLS: reask_anthropic_tools,\n        Mode.ANTHROPIC_JSON: reask_anthropic_json,\n        # ... many more mappings\n    }\n    reask_function = functions.get(mode, reask_default)\n    return reask_function(kwargs=kwargs, response=response, exception=exception)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef handle_reask_kwargs(kwargs, mode, response, exception):\n    kwargs_copy = kwargs.copy()\n    \n    if mode in {Mode.ANTHROPIC_TOOLS, Mode.ANTHROPIC_REASONING_TOOLS}:\n        return reask_anthropic_tools(kwargs_copy, response, exception)\n    elif mode == Mode.ANTHROPIC_JSON:\n        return reask_anthropic_json(kwargs_copy, response, exception)\n    # ... optimized conditional checks with grouped modes\n    else:\n        return reask_default(kwargs_copy, response, exception)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Choice Expansion with State Tracking in Python\nDESCRIPTION: Python function that recursively expands story choices in parallel while maintaining path-specific context. The implementation uses asyncio for concurrent processing, with a semaphore to control API call rates, and accumulates previous choices to ensure narrative consistency in each branch.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/extract-model-looks.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def rewrite_choice(\n    client: instructor.AsyncInstructor,\n    choice: str,\n    story: GeneratedStory,\n    prev_choices: list[dict],  # Accumulator for path state\n    max_depth: int,\n    sem: asyncio.Semaphore\n) -> FinalStoryChoice:\n    # Each choice knows its entire path history\n    async with sem:\n        rewritten_choice = await client.chat.completions.create(\n            model=\"gpt-4o\",\n            response_model=RewrittenChoice,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": \"\"\"\n                Given this choice: {{ choice }}\n\n                Story context:\n                Setting: {{ story.setting }}\n                Plot: {{ story.plot_summary }}\n\n                Previous choices made in this path:\n                {% for prev in prev_choices %}\n                - {{ prev.choice_description }}\n                  Result: {{ prev.choice_consequences }}\n                {% endfor %}\n\n                Generate the next story beat and 2-4 new choices.\n                The story should end in {{ max_depth - len(prev_choices) }} more turns.\n                \"\"\"\n            }],\n            context={\n                \"choice\": choice,\n                \"story\": story,\n                \"prev_choices\": prev_choices,\n            }\n        )\n\n    # For terminal nodes (at max depth)\n    if len(prev_choices) == max_depth - 1:\n        return FinalStoryChoice(\n            choice_description=rewritten_choice.choice_description,\n            choice_consequences=rewritten_choice.choice_consequences,\n            choices=[]  # Terminal node\n        )\n\n    # Recursively expand child choices\n    child_choices = await asyncio.gather(*[\n        rewrite_choice(\n            client=client,\n            choice=new_choice,\n            story=story,\n            prev_choices=prev_choices + [{\n                \"choice_description\": rewritten_choice.choice_description,\n                \"choice_consequences\": rewritten_choice.choice_consequences\n            }],\n            max_depth=max_depth,\n            sem=sem\n        )\n        for new_choice in rewritten_choice.choices\n    ])\n\n    return FinalStoryChoice(\n        choice_description=rewritten_choice.choice_description,\n        choice_consequences=rewritten_choice.choice_consequences,\n        choices=child_choices\n    )\n```\n\n----------------------------------------\n\nTITLE: Original GitHub Actions Workflow Using Poetry\nDESCRIPTION: This YAML configuration defines a GitHub Actions workflow that uses Poetry for Python dependency management and test execution. It includes steps for setting up Python, caching Poetry virtualenvs, installing dependencies, and running tests for different Python versions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/migrating-to-uv.md#2025-04-14_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: Test\non:\n  pull_request:\n  push:\n    branches:\n      - main\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\"]\n\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Cache Poetry virtualenv\n        uses: actions/cache@v2\n        with:\n          path: ~/.cache/pypoetry/virtualenvs\n          key: ${{ runner.os }}-poetry-${{ hashFiles('**/poetry.lock') }}\n          restore-keys: |\n            ${{ runner.os }}-poetry-\n\n      - name: Install Poetry\n        uses: snok/install-poetry@v1.3.1\n\n      - name: Install dependencies\n        run: poetry install --with dev,anthropic\n\n      - name: Run tests\n        if: matrix.python-version != '3.11'\n        run: poetry run pytest tests/ -k 'not llm and not openai and not gemini and not anthropic and not cohere and not vertexai' && poetry run pytest tests/llm/test_cohere\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}\n\n      - name: Run Gemini Tests\n        run: poetry run pytest tests/llm/test_gemini\n        env:\n          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}\n\n      - name: Generate coverage report\n        if: matrix.python-version == '3.11'\n        run: |\n          poetry run coverage run -m pytest tests/ -k \"not docs and not anthropic and not gemini and not cohere and not vertexai and not fireworks\"\n          poetry run coverage report\n          poetry run coverage html\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n```\n\n----------------------------------------\n\nTITLE: Implementing Mutual Information Analysis for Prompt Selection in Python\nDESCRIPTION: Complete implementation of a system that evaluates different prompt templates by measuring the mutual information between model responses. Uses Pydantic for structured output, entropy calculations to measure confidence distribution, and async OpenAI API calls to generate responses efficiently.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/max_mutual_information.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import AsyncOpenAI\nfrom instructor import from_openai\nfrom pydantic import BaseModel\nfrom typing import Callable, Literal\nfrom textwrap import dedent\nimport math\nimport asyncio\n\n\nclass Response(BaseModel):\n    chain_of_thought: str\n    response: Literal[\"A\", \"B\"]\n    confidence: Literal[\n        \"Very High Confidence\",\n        \"High Confidence\",\n        \"Moderate Confidence\",\n        \"Low Confidence\",\n        \"Very Low Confidence\",\n    ]\n\n    def generate_score(self) -> float:\n        confidence_scores = {\n            \"Very High Confidence\": 1,\n            \"High Confidence\": 0.8,\n            \"Moderate Confidence\": 0.6,\n            \"Low Confidence\": 0.4,\n            \"Very Low Confidence\": 0.2,\n        }\n        return confidence_scores[self.confidence]\n\n\nclient = from_openai(AsyncOpenAI())\n\n\ndef prompt_template_1(question: str, options: list[str]):\n    assert len(options) == 2\n    a, b = options\n\n    return dedent(\n        f\"\"\"\n    You are a world class AI System which excels at understanding complex user stories and generating responses. Output your prediction and also quantify your confidence in your prediction with the following scale.\n\n    - Very High Confidence: The model is highly confident in its prediction, displaying deep understanding, flawless execution, and no noticeable errors.\n    - High Confidence: The model is confident in its prediction, with strong relevance and minor errors that do not detract from overall quality.\n    - Moderate Confidence: The model has moderate confidence in its prediction, which is generally relevant with some inaccuracies, and meets minimum requirements.\n    - Low Confidence: The model has low confidence in its prediction, with limited relevance and several inaccuracies.\n    - Very Low Confidence: The model has very low confidence in its prediction, which is largely irrelevant, inaccurate, or incomplete, needing significant improvement\n\n\n    Context\n    {question}\n\n    Options\n    A. {a}\n    B. {b}\n    \"\"\"\n    )\n\n\ndef prompt_template_2(question: str, options: list[str]):\n    assert len(options) == 2\n    a, b = options\n\n    return dedent(\n        f\"\"\"\n    <prompt>\n        <Task>\n        You are about to be passed a story. You are to select the correct response from the options provided.\n\n         <confidence-levels>\n             <level>\n                 <name>Very High Confidence</name>\n                 <description>The model is highly confident in its prediction, displaying deep understanding, flawless execution, and no noticeable errors.</description>\n             </level>\n             <level>\n                 <name>High Confidence</name>\n                 <description>The model is confident in its prediction, with strong relevance and minor errors that do not detract from overall quality.</description>\n             </level>\n             <level>\n                 <name>Moderate Confidence</name>\n                 <description>The model has moderate confidence in its prediction, which is generally relevant with some inaccuracies, and meets minimum requirements.</description>\n             </level>\n             <level>\n                 <name>Low Confidence</name>\n                 <description>The model has low confidence in its prediction, with limited relevance and several inaccuracies.</description>\n             </level>\n             <level>\n                 <name>Very Low Confidence</name>\n                 <description>The model has very low confidence in its prediction, which is largely irrelevant, inaccurate, or incomplete, needing significant improvement</description>\n             </level>\n         </confidence-levels>\n        </Task>\n\n        <Question>\n        {question}\n        </Question>\n\n        <Options>\n        <option>A: {a}</option>\n        <option>B: {b}</option>\n        </Options>\n    </prompt>\n    \"\"\"\n    )\n\n\nasync def generate_response(\n    question: str, options: list[str], prompt_template: Callable[[str, list[str]], str]\n):\n    return await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": prompt_template(question, options),\n            }\n        ],\n        response_model=Response,\n    )\n\n\nasync def generate_responses(\n    questions: list[str], prompt_template: Callable[[str, list[str]], str]\n):\n    return await asyncio.gather(\n        *[\n            generate_response(\n                question=question[\"question\"],\n                options=question[\"options\"],\n                prompt_template=prompt_template,\n            )\n            for question in questions\n        ]\n    )\n\n\ndef calculate_entropy(probs: list[float]) -> float:\n    return sum([p * math.log(p) if p != 0 else 0 for p in probs])\n\n\ndef calculate_mutual_information(predictions: list[Response]) -> float:\n    probs = [\n        [prediction.generate_score(), 1 - prediction.generate_score()]\n        for prediction in predictions\n    ]\n\n    avg_probs = [0, 0]\n\n    for p1, p2 in probs:\n        avg_probs[0] += p1\n        avg_probs[1] += p2\n\n    h_marginal = calculate_entropy([i / len(probs) for i in avg_probs])\n    h_conditional = sum([calculate_entropy(prob) for prob in probs]) / len(probs)\n\n    return h_marginal - h_conditional\n\n\nif __name__ == \"__main__\":\n    queries = [\n        {\n            \"question\": \"Karen was assigned a roommate her first year of college. Her roommate asked her to go to a nearby city for a concert. Karen agreed happily. The show was absolutely exhilarating.\",\n            \"options\": [\n                \"Karen became good friends with her roommate.\",\n                \"Karen hated her roommate.\",\n            ],\n        },\n        {\n            \"question\": \"Jim got his first credit card in college. He didn't have a job so he bought everything on his card. After he graduated he amounted a $10,000 debt. Jim realized that he was foolish to spend so much money.\t\",\n            \"options\": [\n                \"Jim decided to devise a plan for repayment.\",\n                \"Jim decided to open another credit card.\",\n            ],\n        },\n        {\n            \"question\": \"Gina misplaced her phone at her grandparents. It wasn't anywhere in the living room. She realized she was in the car before. She grabbed her dad's keys and ran outside.\",\n            \"options\": [\n                \"She found her phone in the car.\",\n                \"She didn't want her phone anymore.\",\n            ],\n        },\n    ]\n\n    best_mi_score = float(\"-inf\")\n    best_template = None\n\n    for prompt_template in [prompt_template_1, prompt_template_2]:\n        responses = asyncio.run(generate_responses(queries, prompt_template))\n        mi_score = calculate_mutual_information(responses)\n        print(f\"{prompt_template.__name__}: {mi_score}\")\n        #> prompt_template_1: -0.0781292189485728\n        #> prompt_template_2: -0.05907285153542691\n        if mi_score > best_mi_score:\n            best_mi_score = mi_score\n            best_template = prompt_template.__name__\n\n    print(best_template, best_mi_score)\n    #> prompt_template_2 -0.05907285153542691\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with pip for different LLM providers\nDESCRIPTION: Commands to install Instructor using pip, including options for specific providers like Anthropic and Google/Gemini.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/faq.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\npip install \"instructor[anthropic]\"\npip install \"instructor[google-generativeai]\"\n```\n\n----------------------------------------\n\nTITLE: OpenAI Client Patching with Instructor\nDESCRIPTION: Setup code showing how to patch the OpenAI client with the instructor library for validation integration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n```\n\n----------------------------------------\n\nTITLE: Extracting Arbitrary Properties\nDESCRIPTION: This snippet defines classes Property and Character, using Property as a key-value pair to represent arbitrary character attributes. Character defines properties as a list of Property objects. The `chat.completions.create` method extracts the Character with its properties based on the user prompt.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Property(BaseModel):\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: list[Property]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n```\n\n----------------------------------------\n\nTITLE: Customizing Retry Behavior in Python with Instructor\nDESCRIPTION: Example of how to customize the retry behavior when using Instructor, utilizing the tenacity library to set a maximum number of attempts.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/faq.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tenacity import stop_after_attempt\n\nresult = client.chat.completions.create(\n    response_model=MyModel,\n    max_retries=stop_after_attempt(5),  # Retry up to 5 times\n    messages=[...]\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Mistral Integration\nDESCRIPTION: Installation command for Instructor with Mistral AI support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npip install \"instructor[mistralai]\"\n```\n\n----------------------------------------\n\nTITLE: Validation Model Structure using Pydantic\nDESCRIPTION: Defines the basic validation response structure using Pydantic BaseModel with is_valid and error_message fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Validation(BaseModel):\n    is_valid: bool = Field(\n        ..., description=\"Whether the value is valid based on the rules\"\n    )\n    error_message: Optional[str] = Field(\n        ...,\n        description=\"The error message if the value is not valid, to be used for re-asking the model\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Basic Error Handling with OpenAI Completions in Python\nDESCRIPTION: Demonstrates basic error handling when making API calls to OpenAI for text extraction. Uses try-catch block to handle potential exceptions and includes error counting.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    resp = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract the user name and age: 'John is twenty years old'\",\n            }\n        ],\n        response_model=User,\n    )\n    print(f\"Extracted: {resp}\")\nexcept Exception as e:\n    print(f\"Main exception caught: {e}\")\n\nprint(f\"Total errors recorded: {error_counter.error_count}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama and Pulling a Model\nDESCRIPTION: Command to pull the Llama 2 model using Ollama CLI.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/ollama.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama pull llama2\n```\n\n----------------------------------------\n\nTITLE: Defining Hook Protocol Types\nDESCRIPTION: Protocol classes that define type safety for different hook handler functions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Protocol\n\n\n# Handler protocol types for type safety\nclass CompletionKwargsHandler(Protocol):\n    \"\"\"Protocol for completion kwargs handlers.\"\"\"\n\n    def __call__(self, *args: Any, **kwargs: Any) -> None: ...\n\n\nclass CompletionResponseHandler(Protocol):\n    \"\"\"Protocol for completion response handlers.\"\"\"\n\n    def __call__(self, response: Any) -> None: ...\n\n\nclass CompletionErrorHandler(Protocol):\n    \"\"\"Protocol for completion error and last attempt handlers.\"\"\"\n\n    def __call__(self, error: Exception) -> None: ...\n\n\nclass ParseErrorHandler(Protocol):\n    \"\"\"Protocol for parse error handlers.\"\"\"\n\n    def __call__(self, error: Exception) -> None: ...\n```\n\n----------------------------------------\n\nTITLE: Defining User Profile Data in Python\nDESCRIPTION: This snippet defines a multi-line string containing user profile data including customer ID, recent purchases, browsing history, and other relevant information.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generator.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprofile_data = \"\"\"\nCustomer ID: 12345\nRecent Purchases: [Laptop, Wireless Headphones, Smart Watch]\nFrequently Browsed Categories: [Electronics, Books, Fitness Equipment]\nProduct Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars}\nRecent Search History: [best budget laptops 2023, latest sci-fi books, yoga mats]\nPreferred Brands: [Apple, AllBirds, Bench]\nResponses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested}\nLoyalty Program Status: Gold Member\nAverage Monthly Spend: $500\nPreferred Shopping Times: Weekend Evenings\n...\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Competitor Extraction Function in Python\nDESCRIPTION: This function uses the OpenAI API with the instructor library to extract competitor information from a list of image URLs. It processes the images and returns structured data using the defined Competition model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extract_slides.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n# Define functions\ndef read_images(image_urls: List[str]) -> Competition:\n    \"\"\"\n    Given a list of image URLs, identify the competitors in the images.\n    \"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_model=Competition,\n        max_tokens=2048,\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Identify competitors and generate key features for each competitor.\",\n                    },\n                    *[\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n                        for url in image_urls\n                    ],\n                ],\n            }\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini JSON Mode Client\nDESCRIPTION: Sets up an Instructor client for direct JSON output with Gemini models, supporting multi-modal inputs and simple structures.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_gemini(\n    genai.GenerativeModel(\"gemini-1.5-pro\"),\n    mode=instructor.Mode.GEMINI_JSON\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming Response Implementation\nDESCRIPTION: Example of implementing streaming responses for real-time updates during LLM processing.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclass Report(BaseModel):\n    summary: str\n    analysis: str\n    recommendations: list[str]\n\nclient = instructor.from_openai(OpenAI())\n\n# Enable streaming\nfor partial in client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Report,\n    stream=True,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a detailed report about renewable energy.\"}\n    ]\n):\n    # Process each update\n    print(f\"Received update: {partial.model_dump_json()}\")\n\n# The final response has the complete model\nprint(f\"Final report: {partial}\")\n```\n\n----------------------------------------\n\nTITLE: Async Type Inference with Instructor Client\nDESCRIPTION: Demonstrates the use of Instructor with an asynchronous OpenAI client, highlighting the type inference in async contexts\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.AsyncOpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract():\n    return await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Create a user\"},\n        ],\n        response_model=User,\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI TOOLS Mode Client\nDESCRIPTION: Sets up an Instructor client using OpenAI's tool calling API, which is recommended for most OpenAI use cases with GPT-3.5 and GPT-4 models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_openai(\n    OpenAI(),\n    mode=instructor.Mode.TOOLS\n)\n```\n\n----------------------------------------\n\nTITLE: Running Full Text Search Evaluation on Synthetic Questions in Python\nDESCRIPTION: This snippet performs a full evaluation of full text search on synthetic questions. It generates chunk IDs, retrieves results, and calculates various metrics for each question.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport hashlib\nfrom tqdm import tqdm\n\nfts_results = []\n\nfor sample_qn, chunk in tqdm(questions):\n    chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n    cleaned_question = \"\".join(\n        char for char in sample_qn.question if char.isalnum() or char.isspace()\n    )\n    retrieved_results = (\n        table.search(cleaned_question, query_type=\"fts\").limit(25).to_list()\n    )\n    retrieved_chunk_ids = [item[\"chunk_id\"] for item in retrieved_results]\n\n    fts_results.append(\n        {\n            metric: score_fn(retrieved_chunk_ids, [chunk_id])\n            for metric, score_fn in score_fns.items()\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Calculating entity density for a knowledge graph sentence in Python\nDESCRIPTION: This snippet applies the `calculate_entity_density` function to a sentence about knowledge graphs. It calculates and returns the number of tokens, number of entities, and the entity density of the given sentence, providing insights into the information density.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsentence_1 = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ncalculate_entity_density(sentence_1)\n```\n\n----------------------------------------\n\nTITLE: Parsing Generated Responses from Batch Job in Python\nDESCRIPTION: This snippet shows how to use the BatchJob class to parse the generated responses from a .jsonl file into Pydantic models. It demonstrates handling both successfully parsed and unparsed responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/batch_job_oai.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.batch import BatchJob\nfrom pydantic import BaseModel, Field\n\n# <%hide%>\nwith open(\"./output.jsonl\", \"w\") as f:\n    f.write('')\n# <%hide%>\n\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(description=\"The generated question from the text chunk.\")\n    answer: str = Field(description=\"The answer to the generated question.\")\n\n\nparsed, unparsed = BatchJob.parse_from_file(\n    file_path=\"./output.jsonl\", response_model=QuestionAnswerPair\n)\n\nprint(len(parsed))\n#> 0\nprint(len(unparsed))\n#> 0\n\n# <%hide%>\nimport os\n\nif os.path.exists(\"./output.jsonl\"):\n    os.remove(\"./output.jsonl\")\n# <%hide%>\n```\n\n----------------------------------------\n\nTITLE: Entropy Calculation Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating entropy of a probability distribution where P(Ti) represents the probability of the i-th token in the final output distribution.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/max_mutual_information.md#2025-04-14_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\nH(P(Y|X)) = \\sum_{i=0}^n P(T_i) log (P(T_i))\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Text for Prompt Caching Example in Markdown\nDESCRIPTION: This snippet provides instructions for downloading a sample text from 'Pride and Prejudice' by Jane Austen to use as an example of substantial context when working with language models. The text is used to demonstrate the benefits of prompt caching.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompt_caching.md#2025-04-14_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n??? note \"Source Text\"\n\n    In the following example, we'll be using a short excerpt from the novel \"Pride and Prejudice\" by Jane Austen. This text serves as an example of a substantial context that might typically lead to slow response times and high costs when working with language models. You can download it manually [here](https://www.gutenberg.org/cache/epub/1342/pg1342.txt)\n```\n\n----------------------------------------\n\nTITLE: Creating Fine-Tuning Job from Log File\nDESCRIPTION: Instructor CLI command to create a fine-tuning job using the generated log file.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/distilations/readme.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninstructor jobs create-from-file math_finetunes.jsonl\n```\n\n----------------------------------------\n\nTITLE: Viewing Usage Options in OpenAI API Usage CLI\nDESCRIPTION: This command displays the help menu for the usage command in the OpenAI API Usage CLI, showing available options and subcommands.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/usage.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ instructor usage --help\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n Usage: instructor usage [OPTIONS] COMMAND [ARGS]...\n\n Check OpenAI API usage data\n\n╭─ Options ───────────────────────────────────────────────────────╮\n│ --help          Show this message and exit.                     │\n╰─────────────────────────────────────────────────────────────────╯\n╭─ Commands ──────────────────────────────────────────────────────╮\n│ list       Displays OpenAI API usage data for the past N days.  │\n╰─────────────────────────────────────────────────────────────────╯\n```\n\n----------------------------------------\n\nTITLE: Message Length Validation Model\nDESCRIPTION: Pydantic model implementation for validating message length using Field constraints.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantMessage(BaseModel):\n    message: str = Field(..., max_length=100)\n```\n\n----------------------------------------\n\nTITLE: Implementing Validate Subcommand for Instructor-AI CLI in Python\nDESCRIPTION: This snippet defines the 'validate' subcommand for the Instructor-AI CLI. It takes a file path as an argument and performs validation on the specified file.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/jobs.md#2025-04-14_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n@cli.command()\n@click.argument('file_path', type=click.Path(exists=True))\ndef validate(file_path):\n    \"\"\"Validate a file.\"\"\"\n    click.echo(f\"Validating file: {file_path}\")\n    # Add validation logic here\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Pydantic Validator in Python\nDESCRIPTION: Demonstrates how to create a basic Pydantic validator that ensures a name field is in all uppercase letters.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/retrying.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\nfrom pydantic import AfterValidator, BaseModel\n\n\ndef uppercase_validator(v):\n    if v.islower():\n        raise ValueError(\"Name must be ALL CAPS\")\n    return v\n\n\nclass UserDetail(BaseModel):\n    name: Annotated[str, AfterValidator(uppercase_validator)]\n    age: int\n\n\ntry:\n    UserDetail(name=\"jason\", age=12)\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must be ALL CAPS [type=value_error, input_value='jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Field Validation in Pydantic Models\nDESCRIPTION: Shows how to add basic validation rules to fields using Pydantic's Field class. Includes an example of ensuring a price field is greater than zero.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/simple_object.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass Product(BaseModel):\n    name: str\n    price: float = Field(gt=0, description=\"The product price in USD\")\n    in_stock: bool\n```\n\n----------------------------------------\n\nTITLE: Pre-commit Setup Commands\nDESCRIPTION: Commands for installing and setting up pre-commit hooks.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor\nDESCRIPTION: Sets up the OpenAI client with instructor patch for function-calling API integration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.patch(OpenAI())\n```\n\n----------------------------------------\n\nTITLE: Optional Fields with Field Validation\nDESCRIPTION: Demonstrates how to add validation rules to optional fields using Pydantic's Field class.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/optional_fields.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass UserProfile(BaseModel):\n    username: str\n    email: str\n    bio: Optional[str] = Field(\n        None,  # Default value\n        max_length=200,  # Validation applies if present\n        description=\"User's biography, limited to 200 characters\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Calculating Entropy and Repetitiveness Metrics for COSP in Python\nDESCRIPTION: This code snippet calculates the key metrics used in COSP for example selection: entropy (to measure response variability) and repetitiveness (to check response consistency). These metrics help identify high-quality examples for few-shot learning.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/few_shot/cosp.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef calculate_metrics(responses: List[Response]) -> tuple[float, float]:\n    # Calculate entropy\n    confidences = [r.confidence for r in responses]\n    entropy_score = entropy(confidences)\n\n    # Calculate repetitiveness\n    unique_responses = len(set(r.content for r in responses))\n    repetitiveness = 1 - (unique_responses / len(responses))\n\n    return entropy_score, repetitiveness\n```\n\n----------------------------------------\n\nTITLE: Pydantic Model for Tag with Confidence Score in Python\nDESCRIPTION: This code defines a Pydantic model for a tag with a confidence score. It extends the base Tag model and adds a confidence field with validation to ensure the score is between 0 and 1.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass TagWithConfidence(Tag):\n    confidence: float = Field(\n        ...,\n        ge=0,\n        le=1,\n        description=\"The confidence of the prediction, 0 is low, 1 is high\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Using the create method with Pydantic response model\nDESCRIPTION: Basic example of using the client.chat.completions.create method with a Pydantic model as the response_model parameter to get structured data from an LLM.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/version-1.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Pandas DataFrame Type\nDESCRIPTION: Implements a custom type for converting markdown to Pandas DataFrame with validation and serialization.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/types.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nimport pandas as pd\nimport instructor\nimport openai\n\n\ndef md_to_df(data: Any) -> Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    # Validates final type\n    InstanceOf[pd.DataFrame],\n    # Converts markdown to DataFrame\n    BeforeValidator(md_to_df),\n    # Converts DataFrame to markdown on model_dump_json\n    PlainSerializer(lambda df: df.to_markdown()),\n    # Adds a description to the type\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n            The markdown representation of the table,\n            each one should be tidy, do not try to join\n            tables that should be seperate\"\"\",\n        }\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Installing NLTK and spaCy dependencies in Python\nDESCRIPTION: This code installs the necessary dependencies: NLTK for tokenization and spaCy with the English language model for entity recognition.  It downloads the 'punkt' resource from NLTK and the 'en_core_web_sm' model from spaCy using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\n\nnltk.download(\"punkt\")\n\n!python -m spacy download en_core_web_sm --quiet\n```\n\n----------------------------------------\n\nTITLE: Templating with Jinja in Instructor\nDESCRIPTION: Demonstrates a basic example of using Jinja templating to create dynamic prompts with Instructor\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\nclass User(BaseModel):\n    name: str\n    age: int\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor Patching for Parallel Tool Calling\nDESCRIPTION: This snippet shows how to initialize an OpenAI client with Instructor patching using the PARALLEL_TOOLS mode. This mode requires the response_model to be set to Iterable[Union[...]] types.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/patching.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)\n```\n\n----------------------------------------\n\nTITLE: Data Ingestion Function Implementation\nDESCRIPTION: Implements functions to process and ingest passages into LanceDB\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport hashlib\nfrom itertools import batched\n\ndef get_passages(dataset):\n    for row in dataset:\n        for passage in row[\"passages\"][\"passage_text\"]:\n            yield {\n                \"passage\": passage,\n                \"chunk_id\": hashlib.md5(passage.encode()).hexdigest(),\n            }\n\npassages = batched(get_passages(dataset), 10)\n\nfor passage_batch in passages:\n    # print(passage_batch)\n    table.add(list(passage_batch))\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements Specification\nDESCRIPTION: Defines the required Python packages and their versions for the Instructor AI project. Includes core dependencies like OpenAI and Pydantic, as well as development tools like Ruff and pre-commit.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/requirements-examples.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nopenai>=1.1.0\npydantic\ndocstring-parser\nrich\naiohttp\nruff==0.8.1\npre-commit==4.0.1\npyright==1.1.390\ntyper\ncohere\ndatasets\ntrafilatura\n```\n\n----------------------------------------\n\nTITLE: Testing User Data Extraction Reference - Python/pytest\nDESCRIPTION: Example reference to test_extract_users.py which demonstrates how to create parameterized tests for extracting user data across different models and modes using pytest.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/tests/llm/test_openai/evals/readme.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntest_extract_users.py\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response with Index-Based Choice\nDESCRIPTION: Example of how field naming affects model output, showing a response where the model uses an index-based choice instead of direct answer.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/bad-schemas-could-break-llms.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"chain_of_thought\": \"In the race, there are a total of 240 Asians. Given that 80 were Japanese, we can calculate the number of Chinese participants by subtracting the number of Japanese from the total number of Asians: 240 - 80 = 160. Now, it is given that there are 60 boys on the Chinese team. Therefore, to find the number of girls on the Chinese team, we subtract the number of boys from the total number of Chinese participants: 160 - 60 = 100 girls. Thus, the number of girls on the Chinese team is 100.\",\n    \"necessary_calculations\": [\n        \"Total Asians = 240\",\n        \"Japanese participants = 80\",\n        \"Chinese participants = Total Asians - Japanese participants = 240 - 80 = 160\",\n        \"Boys in Chinese team = 60\",\n        \"Girls in Chinese team = Chinese participants - Boys in Chinese team = 160 - 60 = 100\",\n    ],\n    \"potential_final_choices\": [\"60\", \"100\", \"80\", \"120\"],\n    \"final_choice\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Advanced Arbitrary Properties for Multiple Users\nDESCRIPTION: This snippet demonstrates how to handle advanced arbitrary properties for multiple users in Pydantic models. It aims to use consistent key names when extracting properties across users.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompting.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n\n\nclass UserDetails(BaseModel):\n    \"\"\"\n    Extract information for multiple users.\n    Use consistent key names for properties across users.\n    \"\"\"\n\n    users: List[UserDetail]\n```\n\n----------------------------------------\n\nTITLE: Omitting Fields from JSON Schema\nDESCRIPTION: Demonstrates how to use SkipJsonSchema to exclude fields from the model's JSON schema.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/fields.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom pydantic.json_schema import SkipJsonSchema\nfrom typing import Union\n\n\nclass Response(BaseModel):\n    question: str\n    answer: str\n    private_field: SkipJsonSchema[Union[str, None]] = None\n\n\nassert \"private_field\" not in Response.model_json_schema()[\"properties\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Hook Handler\nDESCRIPTION: Basic hook handler function signature for handling completion arguments.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef handler(*args, **kwargs) -> None: ...\n```\n\n----------------------------------------\n\nTITLE: Defining Relationships Between Entities\nDESCRIPTION: This snippet defines relationships between entities by including an array of integer IDs referencing other entities.  The `friends_array` field in the Character class establishes these relationships.  The prompt instructs the model to generate characters with corresponding friend relationships.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Character(BaseModel):\n    id: int\n    name: str\n    friends_array: list[int] = Field(\n        description=\"Relationships to their friends using the id\"\n    )\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output of Extracted Action Items\nDESCRIPTION: Example JSON output showing the structured format of extracted action items, including priorities, assignees, subtasks, and dependencies.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/action_items.md#2025-04-14_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 1,\n    \"name\": \"Improve Authentication System\",\n    \"description\": \"Revamp the front-end and optimize the back-end of the authentication system\",\n    \"priority\": \"High\",\n    \"assignees\": [\"Bob\", \"Carol\"],\n    \"subtasks\": [\n      {\n        \"id\": 2,\n        \"name\": \"Front-end Revamp\"\n      },\n      {\n        \"id\": 3,\n        \"name\": \"Back-end Optimization\"\n      }\n    ],\n    \"dependencies\": []\n  },\n  {\n    \"id\": 4,\n    \"name\": \"Integrate Authentication System with Billing System\",\n    \"description\": \"Integrate the improved authentication system with the new billing system\",\n    \"priority\": \"Medium\",\n    \"assignees\": [\"Bob\"],\n    \"subtasks\": [],\n    \"dependencies\": [1]\n  },\n  {\n    \"id\": 5,\n    \"name\": \"Update User Documentation\",\n    \"description\": \"Update the user documentation to reflect the changes in the authentication system\",\n    \"priority\": \"Low\",\n    \"assignees\": [\"Carol\"],\n    \"subtasks\": [],\n    \"dependencies\": [2]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with Instructor\nDESCRIPTION: Sets up the OpenAI client with instructor library for enhanced functionality.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n```\n\n----------------------------------------\n\nTITLE: Handling Optional Values\nDESCRIPTION: Shows various patterns for safely handling optional values in code.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/optional_fields.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Check for None before using\nif person.age is not None:\n    drinking_age = \"Legal\" if person.age >= 21 else \"Underage\"\nelse:\n    drinking_age = \"Unknown\"\n\n# Use conditional expressions\nprice_display = f\"${product.price}\" if product.price is not None else \"Price unavailable\"\n\n# Provide defaults with 'or'\ndisplay_name = user.nickname or user.username\n```\n\n----------------------------------------\n\nTITLE: Generating Prompting Technique Flowchart with Mermaid\nDESCRIPTION: A mermaid flowchart diagram displaying the relationships between different prompting techniques. It shows the decision flow from choosing a technique based on available examples, reasoning needs, problem complexity, verification requirements, and perspective variations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/index.md#2025-04-14_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    A[Choose Prompting Technique] --> B{Have Examples?}\n    \n    B -->|No| C[Zero-Shot Techniques]\n    B -->|Yes| D[Few-Shot Techniques]\n    \n    C --> C1[Role Prompting]\n    C --> C2[Emotional Language]\n    C --> C3[Style Definition]\n    C --> C4[Follow-Up Generation]\n    \n    D --> D1[Example Ordering]\n    D --> D2[Example Selection]\n    D --> D3[Example Generation]\n    \n    A --> E{Need Reasoning?}\n    \n    E -->|Yes| F[Thought Generation]\n    F --> F1[Chain of Thought]\n    F --> F2[Step-Back Prompting]\n    F --> F3[Thread of Thought]\n    \n    A --> G{Complex Problem?}\n    \n    G -->|Yes| H[Decomposition]\n    H --> H1[Least-to-Most]\n    H --> H2[Tree of Thought]\n    H --> H3[Plan and Solve]\n    \n    A --> I{Need Verification?}\n    \n    I -->|Yes| J[Self-Criticism]\n    J --> J1[Self-Verification]\n    J --> J2[Chain of Verification]\n    J --> J3[Self-Refinement]\n    \n    A --> K{Want Multiple Perspectives?}\n    \n    K -->|Yes| L[Ensembling]\n    L --> L1[Self-Consistency]\n    L --> L2[Meta-CoT]\n    L --> L3[Specialized Experts]\n    \n    classDef category fill:#e2f0fb,stroke:#b8daff,color:#004085;\n    classDef technique fill:#d4edda,stroke:#c3e6cb,color:#155724;\n    classDef decision fill:#fff3cd,stroke:#ffeeba,color:#856404;\n    \n    class A,C,D,F,H,J,L category\n    class C1,C2,C3,C4,D1,D2,D3,F1,F2,F3,H1,H2,H3,J1,J2,J3,L1,L2,L3 technique\n    class B,E,G,I,K decision\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Streaming of Partial Results with Instructor and OpenAI\nDESCRIPTION: This example demonstrates how to use asynchronous streaming with Instructor and OpenAI to process partial results as they come in. It uses the async for syntax to iterate over the streamed results.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/partial.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def print_partial_results():\n    user = client.chat.completions.create_partial(\n        model=\"gpt-4-turbo-preview\",\n        response_model=User,\n        max_retries=2,\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Jason is 12 years old\"},\n        ],\n    )\n    async for m in user:\n        print(m)\n        #> name=None age=None\n        #> name=None age=None\n        #> name='' age=None\n        #> name='Jason' age=None\n        #> name='Jason' age=None\n        #> name='Jason' age=None\n        #> name='Jason' age=None\n        #> name='Jason' age=12\n        #> name='Jason' age=12\n\n\nimport asyncio\n\nasyncio.run(print_partial_results())\n```\n\n----------------------------------------\n\nTITLE: Creating Local Experience for YouTube Q&A Application in Python\nDESCRIPTION: This code snippet shows how to create a simple local experience for the YouTube Q&A application using a while loop. It prompts the user for input, runs the application, and prints the number of generated question-answer pairs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-flashcards.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwhile True:\n    user_input = input(\"Enter a YouTube URL (q to quit): \")\n    if user_input.lower() == \"q\":\n        break\n\n    action_name, result, state = app.run(\n        halt_before=[\"process_user_input\"],\n        inputs={\"user_input\": user_input},\n    )\n    print(f\"{len(state['question_answers'])} question-answer pairs generated\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Limited Validation with OpenAI Structured Outputs in Python\nDESCRIPTION: This code example shows how OpenAI's Structured Outputs handles validation errors. It creates a Pydantic model with a validator that ensures names are uppercase, but demonstrates that validation failures result in exceptions without retry capability.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nfrom pydantic import BaseModel, field_validator\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    def ensure_uppercase(cls, v: str) -> str:\n        if not v.isupper():\n            raise ValueError(\"All letters must be uppercase. Got: \" + v)\n        return v\n\n\nclient = openai.OpenAI()\ntry:\n    resp = client.beta.chat.completions.parse(\n        response_format=User,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract the following user: Jason is 25 years old.\",\n            },\n        ],\n        model=\"gpt-4o-mini\",\n    )\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    name\n      Value error, All letters must be uppercase. Got: Jason [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.9/v/value_error\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Validation and Error Handling\nDESCRIPTION: Example of using Pydantic for data validation with Instructor, including custom validators and field constraints.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/getting-started.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, field_validator\n\nclass User(BaseModel):\n    name: str\n    age: int = Field(gt=0, lt=120)  # Age must be between 0 and 120\n    \n    @field_validator('name')\n    def name_must_have_space(cls, v):\n        if ' ' not in v:\n            raise ValueError('Name must include first and last name')\n        return v\n\n# This will make the LLM retry if validation fails\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Tom is 25 years old.\"}\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Email Validation Model\nDESCRIPTION: Pydantic model definition for email validation with custom field validator to check URLs from instructor documentation. Includes URL pattern matching and validation logic.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/parea.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Email(BaseModel):\n    subject: str\n    body: str = Field(\n        ...,\n        description=\"Email body, Should contain links to instructor documentation. \",\n    )\n\n    @field_validator(\"body\")\n    def check_urls(cls, v):\n        urls = re.findall(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", v)\n        errors = []\n        for url in urls:\n            if not url.startswith(\"https://python.useinstructor.com\"):\n                errors.append(\n                    f\"URL {url} is not from useinstructor.com, Only include URLs that include use instructor.com. \"\n                )\n            response = requests.get(url)\n            if response.status_code != 200:\n                errors.append(\n                    f\"URL {url} returned status code {response.status_code}. Only include valid URLs that exist.\"\n                )\n            elif \"404\" in response.text:\n                errors.append(\n                    f\"URL {url} contained '404' in the body. Only include valid URLs that exist.\"\n                )\n        if errors:\n            raise ValueError(\"\\n\".join(errors))\n        return\n```\n\n----------------------------------------\n\nTITLE: Clearing All Hooks Example\nDESCRIPTION: Example showing how to clear specific or all hook events from the client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\n# Define a simple handler\ndef log_completion_kwargs(*args, **kwargs):\n    print(\"Logging completion kwargs...\")\n\n# Register the hook\nclient.on(\"completion:kwargs\", log_completion_kwargs)\n\n# Make a request that triggers the hook\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    response_model=str,\n)\n\n# Clear hooks for a specific event\nclient.clear(\"completion:kwargs\")\n\n# Register another handler for a different event\ndef log_response(response):\n    print(\"Logging response...\")\n\nclient.on(\"completion:response\", log_response)\n\n# Clear all hooks\nclient.clear()\n```\n\n----------------------------------------\n\nTITLE: Creating Batch Job from File Example\nDESCRIPTION: Shows how to create a batch job using a JSONL file with the instructor CLI.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ instructor batch create-from-file --file-path output.jsonl\n```\n\n----------------------------------------\n\nTITLE: Creating Fine-Tuning Job from File Example in Instructor CLI\nDESCRIPTION: This snippet provides an example of how to create a fine-tuning job from a file using the Instructor CLI. It demonstrates setting various parameters such as validation file, number of epochs, batch size, and learning rate multiplier.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/finetune.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ instructor jobs create-from-file transformed_data.jsonl --validation_file validation_data.jsonl --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for Cerebras Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with Cerebras support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[cerebras]\"\n```\n\n----------------------------------------\n\nTITLE: Dependency Installation Commands\nDESCRIPTION: Commands for installing project dependencies using UV and Poetry, including optional provider-specific installations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Using uv (recommended)\nuv pip install -e \".[dev,docs,test-docs]\"\n\n# Using poetry\npoetry install --with dev,docs,test-docs\n\n# For specific providers, add the provider name as an extra\n# Example: uv pip install -e \".[dev,docs,test-docs,anthropic]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install the required Instructor and Pydantic libraries using pip package manager.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/distilations/readme.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor pydantic\n```\n\n----------------------------------------\n\nTITLE: Advanced Instructor Implementation with Model Dispatch\nDESCRIPTION: Example showing how to use a fine-tuned model with Instructor's dispatch mode, including patching and model specification.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/distilation-part1.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import Instructions, patch\n\npatch()\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -> Multiply:\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key\nDESCRIPTION: Environment variable setup for Anthropic API key configuration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport ANTHROPIC_API_KEY=your_anthropic_key\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: Lists required Python packages with specific version constraints for a project. Includes data validation (pydantic), OpenAI API integration (openai), instructor library, and logging functionality (logfire).\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/logfire/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npydantic==2.7.1\nopenai==1.24.1\ninstructor==1.0.3\nlogfire==0.28.0\n```\n\n----------------------------------------\n\nTITLE: Implementing Enum-based Fields\nDESCRIPTION: Demonstrates how to use Enums with Pydantic models to restrict field values to a specific set.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum\nfrom pydantic import BaseModel\n\nclass UserType(str, Enum):\n    ADMIN = \"admin\"\n    REGULAR = \"regular\"\n    GUEST = \"guest\"\n\nclass User(BaseModel):\n    name: str\n    user_type: UserType\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output from MetaphorQuery Model\nDESCRIPTION: This JSON example shows the structured output from the query understanding system, including a rewritten query, date range specifications, and a domain allowlist focused on academic sources.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-and-beyond.md#2025-04-14_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"rewritten_query\": \"novel developments advancements ai artificial intelligence machine learning\",\n  \"published_daterange\": {\n    \"start\": \"2023-09-17\",\n    \"end\": \"2021-06-17\"\n  },\n  \"domains_allow_list\": [\"arxiv.org\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Marginal Entropy Calculation in LaTeX\nDESCRIPTION: Formula for calculating the marginal entropy (H_marginal) across n examples in the training dataset.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/max_mutual_information.md#2025-04-14_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\nH_{marginal} = H(\\frac{1}{n} \\sum_{i=0}^n P(Y_i | X_i) )\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI JSON Mode Client\nDESCRIPTION: Configures an Instructor client for direct JSON output, suitable for models without tool calling support or simple extractions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_openai(\n    OpenAI(),\n    mode=instructor.Mode.JSON\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting Cohere API key\nDESCRIPTION: Set the Cohere API key as an environment variable for authentication.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cohere.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport CO_API_KEY=<YOUR_COHERE_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Extracting entities related to authors and books using spaCy in Python\nDESCRIPTION: This code snippet shows how to use spaCy to extract named entities from a sentence discussing authors and books.  It processes the sentence with the `nlp` object and accesses the `ents` attribute of the resulting `Doc` object to identify the entities present.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsentence = \"For example, a node representing an author like 'J.K. Rowling'\\\ncan be connected to another node representing one of her books, 'Harry Potter'\\\n, with the edge 'author of'\"\n\ndoc = nlp(sentence)\ndoc.ents\n```\n\n----------------------------------------\n\nTITLE: Generating Metric Functions for Different K Values in Python\nDESCRIPTION: This code uses itertools.product to generate metric functions for different combinations of metrics (MRR and recall) and k values. It creates a dictionary of scoring functions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom itertools import product\n\nSIZES = [3, 5, 10, 15, 25]\nMETRICS = [[\"mrr\", rr], [\"recall\", recall]]\n\nscore_fns = {}\n\nfor metric, size in product(METRICS, SIZES):\n    metric_name, score_fn = metric\n    score_fns[f\"{metric_name}@{size}\"] = (\n        lambda predictions, labels, fn=score_fn, k=size: fn(predictions[:k], labels)\n    )  # type: ignore\n```\n\n----------------------------------------\n\nTITLE: Setting Up Simple Max Retries with Instructor and OpenAI in Python\nDESCRIPTION: Shows how to configure a simple retry mechanism using the 'max_retries' parameter with Instructor and OpenAI client.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/retrying.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=3,  # (1)!\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n# (2)!\n```\n\n----------------------------------------\n\nTITLE: Removing Specific Hooks\nDESCRIPTION: Example demonstrating how to remove specific hooks using the off method.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nimport pprint\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef log_completion_kwargs(*args, **kwargs):\n    pprint.pprint({\"args\": args, \"kwargs\": kwargs})\n\n\n# Register the hook\nclient.on(\"completion:kwargs\", log_completion_kwargs)\n\n# Then later, remove it when no longer needed\nclient.off(\"completion:kwargs\", log_completion_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Defining Search Client and Retrieval Models in Python\nDESCRIPTION: This snippet defines the SearchClient and Retrieval models using Pydantic. The SearchClient contains parameters for search queries, including keywords and source identifiers, facilitating structured query handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\n\nclass SearchClient(BaseModel):\n    query: str = Field(description=\"The search query that will go into the search bar\")\n    keywords: list[str]\n    email: str\n    source: Literal[\"gmail\", \"calendar\"]\n    date_range: DateRange\n\n\nclass Retrieval(BaseModel):\n    queries: list[SearchClient]\n\n```\n\n----------------------------------------\n\nTITLE: Main Execution Block for Instructor-AI CLI in Python\nDESCRIPTION: This snippet contains the main execution block for the Instructor-AI CLI. It checks if the script is being run as the main program and calls the CLI function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/jobs.md#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nif __name__ == '__main__':\n    cli()\n```\n\n----------------------------------------\n\nTITLE: Implementing Performance Testing Models in Python\nDESCRIPTION: Collection of response models used for testing performance differences between JSON mode and Tool Calling implementations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/bad-schemas-could-break-llms.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Answer(BaseModel):\n    chain_of_thought: str\n    answer: int\n\n\nclass AnswerWithCalculation(BaseModel):\n    chain_of_thought: str\n    required_calculations: list[str]\n    answer: int\n\n\nclass AssumptionBasedAnswer(BaseModel):\n    assumptions: list[str]\n    logic_flow: str\n    answer: int\n\n\nclass ErrorAwareCalculation(BaseModel):\n    key_steps: list[str]\n    potential_pitfalls: list[str]\n    intermediate_results: list[str]\n    answer: int\n\n\nclass AnswerWithNecessaryCalculationAndFinalChoice(BaseModel):\n    chain_of_thought: str\n    necessary_calculations: list[str]\n    potential_final_choices: list[str]\n    final_choice: int\n```\n\n----------------------------------------\n\nTITLE: Breaking Change Commit Format\nDESCRIPTION: Example of how to format a commit message that includes breaking changes using the ! indicator.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nfeat(api)!: change parameter order in from_openai factory function\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for Anthropic Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with Anthropic support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[anthropic]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Package\nDESCRIPTION: Command to install the Instructor package using pip\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor\"\n```\n\n----------------------------------------\n\nTITLE: OpenAI Moderation Integration\nDESCRIPTION: Demonstrates integration with OpenAI's moderation endpoint for content filtering.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-1-validation-rag.ipynb#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\nfrom pydantic import AfterValidator\nfrom instructor import openai_moderation\n\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\nModeratedStr = Annotated[str, AfterValidator(openai_moderation(client=client))]\n\nclass Response(BaseModel):\n    message: ModeratedStr\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with LiteLLM support using pip\nDESCRIPTION: Command to install Instructor with LiteLLM support using pip package manager.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/litellm.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[litellm]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Mistral Client with Instructor\nDESCRIPTION: Shows how to set up a Mistral AI client using Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/client_setup.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom mistralai.client import MistralClient\n\nmistral_client = MistralClient(api_key=\"YOUR_API_KEY\")\nclient = instructor.from_mistral(mistral_client)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data from LLM in Python\nDESCRIPTION: This code snippet shows the function signature for extracting structured data from a language model using the Instructor library. It demonstrates the use of type hinting for the return value.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/philosophy.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract(a) -> StructuredData:\n```\n\n----------------------------------------\n\nTITLE: Testing Single-Label Classification\nDESCRIPTION: This snippet demonstrates how to use the classify function to predict labels for sample texts. It asserts the correctness of the predictions and prints the results.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/classification.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    for text, label in [\n        (\"Hey Jason! You're awesome\", \"NOT_SPAM\"),\n        (\"I am a nigerian prince and I need your help.\", \"SPAM\"),\n    ]:\n        prediction = classify(text)\n        assert prediction.label == label\n        print(f\"Text: {text}, Predicted Label: {prediction.label}\")\n        #> Text: Hey Jason! You're awesome, Predicted Label: NOT_SPAM\n        #> Text: I am a nigerian prince and I need your help., Predicted Label: SPAM\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Provider Dependencies\nDESCRIPTION: Command to install Instructor package with provider-specific extras\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/templates/provider_template.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[provider-specific-extras]\"\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys as Environment Variables\nDESCRIPTION: Bash commands to set API keys for various LLM providers as environment variables.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/getting-started.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# For OpenAI\nexport OPENAI_API_KEY=your_openai_api_key\n\n# For Anthropic\nexport ANTHROPIC_API_KEY=your_anthropic_api_key\n\n# For other providers, set relevant API keys\n```\n\n----------------------------------------\n\nTITLE: Async Implementation with Azure OpenAI and Instructor\nDESCRIPTION: This snippet demonstrates how to use Azure OpenAI with instructor in an asynchronous manner for structured output generation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/azure.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport instructor\nimport asyncio\nfrom openai import AsyncAzureOpenAI\nfrom pydantic import BaseModel\n\nclient = AsyncAzureOpenAI(\n    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n    api_version=\"2024-02-15-preview\",\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n)\nclient = instructor.from_openai(client)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def get_user_async():\n    return await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"John is 30 years old\"}],\n        response_model=User,\n    )\n\n\n# Run async function\nuser = asyncio.run(get_user_async())\nprint(user)\n# > name='John' age=30\n```\n\n----------------------------------------\n\nTITLE: Working with List Fields\nDESCRIPTION: Demonstrates how to handle lists of items in Pydantic models using Python's List type hint.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass BlogPost(BaseModel):\n    title: str\n    content: str\n    tags: List[str]\n```\n\n----------------------------------------\n\nTITLE: Measuring Performance Improvement with Caching\nDESCRIPTION: Illustrates how to measure the performance improvement gained by using caching. It compares the execution time of the first call (uncached) with subsequent calls that use the cached result.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/caching.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nstart = time.perf_counter()  # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\n#> Time taken: 0.92\n#> Time taken: 1.20e-06 # (3)\n```\n\n----------------------------------------\n\nTITLE: Using Annotated with Pydantic Fields\nDESCRIPTION: Demonstrates how to combine Field with Annotated type hints for field customization.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/fields.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom uuid import uuid4\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: Annotated[str, Field(default_factory=lambda: uuid4().hex)]\n```\n\n----------------------------------------\n\nTITLE: Setting DeepSeek API Key via Environment Variable\nDESCRIPTION: Command to set the DeepSeek API key as an environment variable.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/deepseek.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport DEEPSEEK_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Accessing Nested Optional Structures\nDESCRIPTION: Demonstrates safe access patterns for nested optional structures.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/optional_fields.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Access nested data safely\nif person.contact.address:\n    print(f\"Address: {person.contact.address.city}\")\nelse:\n    print(\"No address information available\")\n```\n\n----------------------------------------\n\nTITLE: Returning Original Completion with Structured Output\nDESCRIPTION: Demonstrates how to retrieve both the structured output and the original completion object from OpenAI. This is useful when additional metadata from the API response is needed alongside the parsed result.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Logging Output Format for Fine-tuning\nDESCRIPTION: Demonstrates the structure of the logging output generated by Instructor for fine-tuning, including system messages, user inputs, and function calls.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/distilation-part1.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\n            \"role\": \"assistant\",\n            \"function_call\": {\n                \"name\": \"Multiply\",\n                \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}',\n            },\n        },\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Viewing Uploaded Files in Instructor CLI\nDESCRIPTION: This snippet demonstrates how to list and view uploaded files using the Instructor CLI. It displays detailed information about each file, including ID, size, creation time, and purpose.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/finetune.md#2025-04-14_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n$ instructor files list\n\nOpenAI Files\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ File ID                       ┃ Size (bytes) ┃ Creation Time       ┃ Filename ┃ Purpose   ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩\n│ file-0lw2BSNRUlXZXRRu2beCCWjl │       369523 │ 2023-08-23 23:31:57 │ file     │ fine-tune │\n│ file-IHaUXcMEykmFUp1kt2puCDEq │       369523 │ 2023-08-23 23:09:35 │ file     │ fine-tune │\n│ file-ja9vRBf0FydEOTolaa3BMqES │       369523 │ 2023-08-23 22:42:29 │ file     │ fine-tune │\n│ file-F7lJg6Z47CREvmx4kyvyZ6Sn │       369523 │ 2023-08-23 22:42:03 │ file     │ fine-tune │\n│ file-YUxqZPyJRl5GJCUTw3cNmA46 │       369523 │ 2023-08-23 22:29:10 │ file     │ fine-tune │\n└───────────────────────────────┴──────────────┴─────────────────────┴──────────┴───────────┘\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Command to set the OpenAI API key as an environment variable for authentication.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/index.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Running Ollama Local Model\nDESCRIPTION: Command to run the llama2 model locally using Ollama\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/open_source.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run llama2\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with Perplexity Sonar Models\nDESCRIPTION: Example of using Instructor with Perplexity's Sonar models via the OpenAI client API. Shows how to extract structured data by pointing the OpenAI client to Perplexity's API endpoint.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_perplexity(OpenAI(base_url=\"https://api.perplexity.ai\"))\n\nresp = client.chat.completions.create(\n    model=\"sonar\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for YouTube Transcript Analysis\nDESCRIPTION: This bash command installs the necessary Python packages (openai, instructor, pydantic, and youtube_transcript_api) for analyzing YouTube transcripts.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-transcripts.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openai instructor pydantic youtube_transcript_api\n```\n\n----------------------------------------\n\nTITLE: Structured Output with Mistral\nDESCRIPTION: Implementation of structured output extraction using Mistral Large model with Instructor integration\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/open_source.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom mistralai.client import MistralClient\n\n\nclient = MistralClient()\n\npatched_chat = instructor.from_openai(\n    create=client.chat, mode=instructor.Mode.MISTRAL_TOOLS\n)\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\nresp = patched_chat(\n    model=\"mistral-large-latest\",\n    response_model=UserDetails,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f'Extract the following entities: \"Jason is 20\"',\n        },\n    ],\n)\n\nprint(resp)\n#> name='Jason' age=20\n```\n\n----------------------------------------\n\nTITLE: Initializing Gemini TOOLS Mode Client\nDESCRIPTION: Configures an Instructor client using Google's function calling for Gemini models, suitable for complex structured outputs but not multi-modal inputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_gemini(\n    genai.GenerativeModel(\"gemini-1.5-pro\"),\n    mode=instructor.Mode.GEMINI_TOOLS\n)\n```\n\n----------------------------------------\n\nTITLE: Setting MistralAI API Key Environment Variable\nDESCRIPTION: This command exports the MistralAI API key as an environment variable. The API key is required to authenticate and use the MistralAI service. Replace <your-api-key> with the actual API key obtained from the MistralAI website.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/mistral.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MISTRAL_API_KEY=<your-api-key>\n```\n\n----------------------------------------\n\nTITLE: Listing OpenAI Batch Jobs Example\nDESCRIPTION: Demonstrates how to list existing batch jobs with the instructor CLI, showing job IDs, creation dates, statuses, and completion statistics.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ instructor batch list --limit 5\n\n                                   OpenAI Batch Jobs\n┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ Batch ID             ┃ Created At          ┃ Status    ┃ Failed ┃ Completed ┃ Total ┃\n┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ batch_BSMSiMMy8on2D… │ 2024-06-19 15:10:21 │ cancelled │ 0      │ 298       │ 300   │\n│ batch_pD5dqHmqjWYF5… │ 2024-06-19 15:09:38 │ completed │ 0      │ 15        │ 15    │\n│ batch_zsTSsWVLgpEan… │ 2024-06-19 15:06:05 │ completed │ 0      │ 15        │ 15    │\n│ batch_igaa2j9VBVw2Z… │ 2024-06-19 15:01:59 │ completed │ 0      │ 300       │ 300   │\n│ batch_HcjI2wG46Y1LY… │ 2024-06-12 15:45:37 │ completed │ 0      │ 3         │ 3     │\n└──────────────────────┴─────────────────────┴───────────┴────────┴───────────┴───────┘\n```\n\n----------------------------------------\n\nTITLE: Creating FastAPI-style Endpoints with Pydantic Response Models\nDESCRIPTION: This code shows how FastAPI uses Pydantic for defining API response schemas. It defines a UserDetails model and sets it as the response_model for an API endpoint, demonstrating the parallel with Instructor's approach.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introduction.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport fastapi\nfrom pydantic import BaseModel\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\napp = fastapi.FastAPI()\n\n@app.get(\"/user/{user_id}\", response_model=UserDetails)\nasync def get_user(user_id: int) -> UserDetails:\n    return ...\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output for Structured Retrieval Queries\nDESCRIPTION: This JSON snippet demonstrates the expected output format from the Retrieval model. It includes a list of queries with specific parameters for searching different data sources like email and calendar.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/rag-and-beyond.md#2025-04-14_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queries\": [\n        {\n            \"query\": null,\n            \"keywords\": null,\n            \"email\": \"jason@example.com\",\n            \"source\": \"gmail\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": null\n        },\n        {\n            \"query\": null,\n            \"keywords\": [\"meeting\", \"call\", \"zoom\"],\n            \"email\": \"jason@example.com\",\n            \"source\": \"calendar\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": null\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison Table in Markdown\nDESCRIPTION: Markdown table comparing execution times of different async processing methods, showing results both with and without rate limiting via semaphore.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/learn-async.md#2025-04-14_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n| Method               | Execution Time | Rate Limited (Semaphore) |\n| -------------------- | -------------- | ------------------------ |\n| For Loop             | 6.17 seconds   |                          |\n| Asyncio.gather       | 0.85 seconds   |                          |\n| Asyncio.as_completed | 0.95 seconds   |                          |\n| Asyncio.gather       | 3.04 seconds   | 2                        |\n| Asyncio.as_completed | 3.26 seconds   | 2                        |\n```\n\n----------------------------------------\n\nTITLE: Setting Default Values for Optional Fields\nDESCRIPTION: Shows how to implement optional fields with meaningful default values using Pydantic models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/optional_fields.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    currency: str = \"USD\"  # Default value\n    in_stock: bool = True  # Default value\n    tags: List[str] = []  # Default empty list\n```\n\n----------------------------------------\n\nTITLE: Basic Hook Usage with OpenAI Client\nDESCRIPTION: Shows basic initialization and usage of hooks with the OpenAI client using the Instructor library.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport instructor\n\n\nclient = instructor.patch(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=str,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Tokenizing a sentence using NLTK in Python\nDESCRIPTION: This code snippet demonstrates how to tokenize a sentence using the `nltk.word_tokenize` function. The `nltk.word_tokenize` function splits the input sentence into individual tokens, handling edge cases and contractions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\n\nsentence = \"My favourite type of Sashimi is Toro\"\n\nnltk.word_tokenize(sentence)\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Validation Failures\nDESCRIPTION: Shows how to implement error handling for validation failures when using the custom validation model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/validators/readme.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    qa: QuestionAnswerNoEvil = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=QuestionAnswerNoEvil,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n            },\n        ],\n    )\nexcept Exception as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Building the Burr Application\nDESCRIPTION: Constructs the final Burr application by defining actions, transitions, and the entry point for the flashcard generation workflow.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-flashcards.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom burr.core import ApplicationBuilder\n\napp = (\n    ApplicationBuilder()\n    .with_actions(\n        process_user_input,\n        get_youtube_transcript,\n        generate_question_and_answers,\n    )\n    .with_transitions(\n        (\"process_user_input\", \"get_youtube_transcript\"),\n        (\"get_youtube_transcript\", \"generate_question_and_answers\"),\n        (\"generate_question_and_answers\", \"process_user_input\"),\n    )\n    .with_entrypoint(\"process_user_input\")\n    .build()\n)\napp.visualize()\n```\n\n----------------------------------------\n\nTITLE: Initializing Mistral Client with API Key\nDESCRIPTION: Python code to initialize the Mistral client with an API key.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mistralai import Mistral\nclient = Mistral(api_key='your-api-key-here')\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key via Environment Variable\nDESCRIPTION: Command to set the OpenAI API key as an environment variable\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/openai.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Installing Core Instructor Package\nDESCRIPTION: Basic installation command for the Instructor library using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install instructor\n```\n\n----------------------------------------\n\nTITLE: Launching YouTube Q&A Application with Burr in Python\nDESCRIPTION: This code snippet demonstrates how to launch the YouTube Q&A application using Burr's Application.run() method. It halts before 'process_user_input' to get the YouTube URL from the user and returns action name, result, and state.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-flashcards.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\naction_name, result, state = app.run(\n    halt_before=[\"process_user_input\"],\n    inputs={\"user_input\": \"https://www.youtube.com/watch?v=hqutVJyd3TI\"},\n)\nprint(state[\"question_answers\"][0])\n```\n\n----------------------------------------\n\nTITLE: Initializing Anthropic TOOLS Mode Client\nDESCRIPTION: Configures an Instructor client using Anthropic's Tool Calling API for Claude 3+ models, optimized for complex structured outputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_anthropic(\n    Anthropic(),\n    mode=instructor.Mode.ANTHROPIC_TOOLS\n)\n```\n\n----------------------------------------\n\nTITLE: Running Commands in UV Virtual Environment\nDESCRIPTION: This command runs a specific command using the virtual environment created by UV, ensuring the correct environment is used for project tasks.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/migrating-to-uv.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv run <command>\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for Fireworks Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with Fireworks support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[fireworks]\"\n```\n\n----------------------------------------\n\nTITLE: Executing Image Data Extraction with GPT-4V and Logfire Monitoring\nDESCRIPTION: Code that extracts tabular data from an image using the extract_table_from_image function. This demonstrates using GPT-4V to interpret visual data while monitoring the process with Logfire.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://cdn.statcdn.com/Infographic/images/normal/16330.jpeg\"\ntables = extract_table_from_image(url)\nfor table in tables:\n    print(table.caption, end=\"\\n\")\n    print(table.dataframe.to_markdown())\n```\n\n----------------------------------------\n\nTITLE: Defining Product List in Python\nDESCRIPTION: This snippet creates a list of product dictionaries, each containing a product ID and name. The products cover various categories like electronics, books, and fitness equipment.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generator.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nproducts = [\n    {\n        \"product_id\": 1,\n        \"product_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\",\n    },\n    {\n        \"product_id\": 2,\n        \"product_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\",\n    },\n    {\n        \"product_id\": 3,\n        \"product_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\",\n    },\n    {\n        \"product_id\": 4,\n        \"product_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\",\n    },\n    {\n        \"product_id\": 5,\n        \"product_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\",\n    },\n    {\n        \"product_id\": 6,\n        \"product_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\",\n    },\n    {\n        \"product_id\": 7,\n        \"product_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\",\n    },\n    {\n        \"product_id\": 8,\n        \"product_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\",\n    },\n    {\n        \"product_id\": 9,\n        \"product_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\",\n    },\n    {\n        \"product_id\": 10,\n        \"product_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\",\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Message Formatting with Gemini\nDESCRIPTION: Shows different ways to format messages when using Google GenAI SDK with Instructor\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom google import genai\nimport instructor\nfrom pydantic import BaseModel\nfrom google.genai import types\n\n# Define your Pydantic model\nclass User(BaseModel):\n    name: str\n    age: int\n\n# Initialize and patch the client\nclient = genai.Client()\nclient = instructor.from_genai(client, mode=instructor.Mode.GENAI_TOOLS)\n\n# Single string (converted to user message)\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    messages=\"Jason is 25 years old\",\n    response_model=User,\n)\n\nprint(response)\n# > name='Jason' age=25\n\n# Standard format\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Jason is 25 years old\"}\n    ],\n    response_model=User,\n)\n\nprint(response)\n# > name='Jason' age=25\n\n# Using genai's Content type\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash-001\",\n    messages=[\n        genai.types.Content(\n            role=\"user\",\n            parts=[genai.types.Part.from_text(text=\"Jason is 25 years old\")]\n        )\n    ],\n    response_model=User,\n)\n\nprint(response)\n# > name='Jason' age=25\n```\n\n----------------------------------------\n\nTITLE: Concurrent Processing with asyncio.gather in Python\nDESCRIPTION: Shows how to use asyncio.gather to run multiple tasks concurrently. This method creates a list of tasks and waits for all of them to complete before returning the results.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/learn-async.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def gather():\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    all_persons = await asyncio.gather(*tasks_get_persons)\n```\n\n----------------------------------------\n\nTITLE: Installing Groq AI Dependencies\nDESCRIPTION: Commands to set up Groq API key and install the required instructor package with Groq support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/groq.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GROQ_API_KEY=<your-api-key-here>\npip install \"instructor[groq]\"\n```\n\n----------------------------------------\n\nTITLE: Basic Timestamp Handling with Pydantic (Problematic Approach)\nDESCRIPTION: A simple but problematic implementation using Pydantic for timestamp handling, which fails to account for format variability between MM:SS and HH:MM:SS formats.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/timestamp.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Segment(BaseModel):\n    title: str = Field(..., description=\"The title of the segment\")\n    timestamp: str = Field(..., description=\"The timestamp of the event as HH:MM:SS\")\n\n\n# This might work for some cases, but fails for others:\n# \"2:00\" could be interpreted as 2 minutes or 2 hours\n# \"1:30:00\" doesn't fit the expected format\n```\n\n----------------------------------------\n\nTITLE: Example Documentation Code Block\nDESCRIPTION: Example showing proper documentation format with complete imports.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/contributing.md#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Complete example with imports\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n```\n\n----------------------------------------\n\nTITLE: LanceDB Database Connection Setup\nDESCRIPTION: Establishes connection to LanceDB database for data storage\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lancedb import connect\n\nDB_PATH = \"./db\"\nDB_TABLE = \"ms_marco\"\n\n# Create a db at the path `./db`\ndb = connect(DB_PATH)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Ollama and Instructor\nDESCRIPTION: This code demonstrates how to use Instructor with Ollama to extract structured data. It sets up a custom OpenAI client for Ollama and uses a Pydantic model for data extraction.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport instructor\n\nclass ExtractUser(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model=\"llama3\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=ExtractUser,\n)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Setting Together AI API Key\nDESCRIPTION: Environment variable setup for Together AI API authentication\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/open_source.md#2025-04-14_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport TOGETHER_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Package\nDESCRIPTION: Command to install the Instructor package via pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/deepseek.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor\"\n```\n\n----------------------------------------\n\nTITLE: Managing Sensitive Information with Pydantic Models and SecretStr\nDESCRIPTION: Demonstrates how to use Pydantic models with SecretStr to handle sensitive information in templates while maintaining security in logging and display.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/jinja-proposal.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, SecretStr\n\n\nclass UserContext(BaseModel):\n    name: str\n    address: SecretStr\n\n\nclass Address(BaseModel):\n    street: SecretStr\n    city: str\n    state: str\n    zipcode: str\n\n\ndef normalize_address(address: Address):\n    context = UserContext(username=\"scolvin\", address=address)\n    address = client.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"{{ user.name }} is `{{ user.address.get_secret_value() }}`, normalize it to an address object\",\n            },\n        ],\n        context={\"user\": context},\n    )\n    print(context)\n    #> UserContext(username='jliu', address=\"******\")\n    print(address)\n    #> Address(street='******', city=\"Toronto\", state=\"Ontario\", zipcode=\"M5A 0J3\")\n    logger.info(\n        f\"Normalized address: {address}\",\n        extra={\"user_context\": context, \"address\": address},\n    )\n    return address\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI MD_JSON Mode Client\nDESCRIPTION: Sets up an Instructor client that outputs JSON inside Markdown code blocks for cleaner outputs and more readable prompts.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = instructor.from_openai(\n    OpenAI(),\n    mode=instructor.Mode.MD_JSON\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Ollama Model using CLI\nDESCRIPTION: Command to pull and install the Llama3 model using Ollama CLI\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/ollama.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama pull llama3\n```\n\n----------------------------------------\n\nTITLE: Creating Fine-Tuning Job with Instructor CLI\nDESCRIPTION: This shell command uses the Instructor CLI to create a fine-tuning job from the generated .jsonl file.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chain-of-density.md#2025-04-14_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ninstructor jobs create-from-file generated.jsonl\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for Ollama Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with Ollama support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[ollama]\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for Pairwise LLM Judge in Python\nDESCRIPTION: Imports necessary libraries and initializes the OpenAI client with Instructor for structured outputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/pairwise-llm-judge.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n```\n\n----------------------------------------\n\nTITLE: Final Validated Output\nDESCRIPTION: Shows the final, validated response after implementing the correction mechanism.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/self_critique.md#2025-04-14_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on individual beliefs and philosophies.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Maybe Field Values\nDESCRIPTION: Demonstrates how to check and handle Maybe field values safely.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/patterns/optional_fields.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif person.age and person.age.is_uncertain:\n    print(f\"Uncertain age: approximately {person.age.value}\")\nelif person.age:\n    print(f\"Age: {person.age.value}\")\nelse:\n    print(\"Age: Unknown\")\n```\n\n----------------------------------------\n\nTITLE: Creating Structured Outputs with Anthropic and Instructor\nDESCRIPTION: A complete example demonstrating how to patch Anthropic's client with Instructor to generate structured data outputs using Pydantic models. The code shows how to define data models, create the enhanced client, and make a request that returns structured data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/anthropic.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import List\nimport anthropic\nimport instructor\n\n# Patching the Anthropics client with the instructor for enhanced capabilities\nanthropic_client = instructor.from_openai(\n    create=anthropic.Anthropic().messages.create,\n    mode=instructor.Mode.ANTHROPIC_JSON\n)\n\nclass Properties(BaseModel):\n    name: str\n    value: str\n\nclass User(BaseModel):\n    name: str\n    age: int\n    properties: List[Properties]\n\nuser_response = anthropic_client(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    max_retries=0,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Create a user for a model with a name, age, and properties.\",\n        }\n    ],\n    response_model=User,\n)  # type: ignore\n\nprint(user_response.model_dump_json(indent=2))\n\"\"\"\n{\n    \"name\": \"John\",\n    \"age\": 25,\n    \"properties\": [\n        {\n            \"key\": \"favorite_color\",\n            \"value\": \"blue\"\n        }\n    ]\n}\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up GitHub MCP Server for Cursor Integration\nDESCRIPTION: JSON configuration for integrating a GitHub MCP server with Cursor. Creates a configuration file that allows Cursor's AI to interact with GitHub repositories and issues.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/why-care-about-mcps.md#2025-04-14_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<Personal Access Token Goes Here>\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Provider Dependencies Configuration\nDESCRIPTION: TOML configuration for adding new LLM provider dependencies to the project.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/contributing.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[project.optional-dependencies]\n# Add your provider\nmy-provider = [\"my-provider-sdk>=1.0.0,<2.0.0\"]\n\n[dependency-groups]\n# Mirror in dependency groups\nmy-provider = [\"my-provider-sdk>=1.0.0,<2.0.0\"]\n```\n\n----------------------------------------\n\nTITLE: Working with Invalid JSON Data\nDESCRIPTION: Example demonstrating issues with unvalidated JSON data handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/1-introduction.ipynb#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata = [{\"first_name\": \"Jason\", \"age\": 10}, {\"firstName\": \"Jason\", \"age\": \"10\"}]\n```\n\nLANGUAGE: python\nCODE:\n```\nfor obj in data:\n    name = obj.get(\"first_name\")\n    age = obj.get(\"age\")\n    print(f\"{name} is {age}\")\n\nfor obj in data:\n    name = obj.get(\"first_name\")\n    age = obj.get(\"age\")\n    try:\n        age_next_year = age + 1\n        print(f\"Next year {name} will be {age_next_year} years old\")\n    except TypeError:\n        traceback.print_exc()\n```\n\n----------------------------------------\n\nTITLE: Basic Synchronous User Data Extraction with Gemini\nDESCRIPTION: Demonstrates how to extract user information synchronously using Instructor with Gemini model. Creates a simple User model with name and age fields.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/google.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n    mode=instructor.Mode.GEMINI_JSON,\n)\n\n# note that client.chat.completions.create will also work\nresp = client.messages.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Anthropic support\nDESCRIPTION: Command to install the Instructor library with Anthropic integration support via pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/anthropic.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor[anthropic]\n```\n\n----------------------------------------\n\nTITLE: Viewing Jobs Options in Instructor CLI\nDESCRIPTION: This snippet shows the available options and commands for managing fine-tuning jobs using the Instructor CLI. It includes commands for canceling, creating, and listing jobs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/finetune.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ instructor jobs --help\n\n Usage: instructor jobs [OPTIONS] COMMAND [ARGS]...\n\n Monitor and create fine tuning jobs\n\n╭─ Options ───────────────────────────────────────────────────────────────────────────────╮\n│ --help                            Display the help message.                             │\n╰─────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Commands ──────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ cancel                    Cancel a fine-tuning job.                                                         │\n│ create-from-file          Create a fine-tuning job from a file.                                             │\n│ create-from-id            Create a fine-tuning job from an existing ID.                                     │\n│ list                      Monitor the status of the most recent fine-tuning jobs.                           │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n----------------------------------------\n\nTITLE: Explicit Mode Configuration with Perplexity\nDESCRIPTION: Shows how to configure Perplexity AI with explicit mode settings using Mode.PERPLEXITY_JSON for direct JSON response generation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/perplexity.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nimport instructor\nfrom instructor import Mode\nfrom pydantic import BaseModel\n\n# Initialize with API key\nclient = OpenAI(\n    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n    base_url=\"https://api.perplexity.ai\"\n)\n\n# Enable instructor patches for Perplexity client with explicit mode\nclient = instructor.from_perplexity(\n    client, \n    mode=Mode.PERPLEXITY_JSON\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# Create structured output\nuser = client.chat.completions.create(\n    model=\"sonar-medium-online\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract: Jason is 25 years old\"},\n    ],\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontend Navigation Structure\nDESCRIPTION: Grid-based navigation layout using Material Design icons and markdown formatting to organize cookbook sections into major categories: text processing, multi-modal, data tools, and deployment.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/index.md#2025-04-14_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"grid cards\" markdown>\n\n- :material-text-box-multiple: **Text Processing**\n\n    Extract structured information from text documents\n\n    [:octicons-arrow-right-16: View Recipes](#text-processing)\n\n- :material-image: **Multi-Modal**\n\n    Work with images and other media types\n\n    [:octicons-arrow-right-16: View Recipes](#multi-modal-examples)\n\n- :material-database: **Data Tools**\n\n    Integrate with databases and data processing tools\n\n    [:octicons-arrow-right-16: View Recipes](#data-tools)\n\n- :material-server: **Deployment**\n\n    Options for local and cloud deployment\n\n    [:octicons-arrow-right-16: View Recipes](#deployment-options)\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Documentation Access Commands\nDESCRIPTION: Commands to access Instructor documentation directly from the terminal, including search and specific page navigation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/index.md#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Open main documentation\ninstructor docs\n\n# Search for specific topic\ninstructor docs validation\n\n# Open specific page\ninstructor docs concepts/models\n```\n\n----------------------------------------\n\nTITLE: Structured Output with Ollama and Instructor\nDESCRIPTION: Implementation of structured output extraction using Ollama local model with Instructor and Pydantic for data validation\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/open_source.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\n\nuser = client.chat.completions.create(\n    model=\"llama2\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Jason is 30 years old\",\n        }\n    ],\n    response_model=UserDetail,\n)\n\nprint(user)\n#> name='Jason' age=30\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for Vertex AI Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with Vertex AI support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[vertexai]\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing a Choose Your Own Adventure Story as a DAG\nDESCRIPTION: A Mermaid diagram representing a Choose Your Own Adventure story as a DAG, where the story root branches into different choices, and each choice leads to more choices, forming a tree-like structure.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/consistent-stories.md#2025-04-14_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A[Story Root] --> B[Choice 1]\n    A --> C[Choice 2]\n    A --> D[Choice 3]\n    B --> E[Choice 1.1]\n    B --> F[Choice 1.2]\n    C --> G[Choice 2.1]\n    C --> H[Choice 2.2]\n    D --> I[Choice 3.1]\n    D --> J[Choice 3.2]\n```\n\n----------------------------------------\n\nTITLE: Basic Validator Function Structure\nDESCRIPTION: Demonstrates the basic structure of a validation function that checks conditions and raises errors or mutates values.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-1-validation-rag.ipynb#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n```\n\n----------------------------------------\n\nTITLE: Defining Character Model and Loading Text\nDESCRIPTION: Creates a Pydantic model for character data and loads the source text from a file.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/anthropic-prompt-caching.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"./book.txt\") as f:\n    book = f.read()\n\n\nclass Character(BaseModel):\n    name: str\n    description: str\n```\n\n----------------------------------------\n\nTITLE: Configuring Perplexity Environment Variables\nDESCRIPTION: Sets up the required environment variables for Perplexity API access including the API key and base URL.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/open_source_examples/README.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport PERPLEXITY_API_KEY=your key here\nexport PERPLEXITY_BASE_URL=https://api.perplexity.ai\n```\n\n----------------------------------------\n\nTITLE: Setting Anyscale API Key\nDESCRIPTION: Command to set up the Anyscale API key as an environment variable for authentication.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anyscale.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ANYSCALE_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Installs the necessary Python packages LangSmith and Instructor using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/langsmith.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -U langsmith\npip install -U instructor\n```\n\n----------------------------------------\n\nTITLE: UV Installation Commands for Different Platforms\nDESCRIPTION: Installation commands for UV package manager on macOS/Linux and Windows systems.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows PowerShell\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n----------------------------------------\n\nTITLE: Authenticated API Request - Bash Example\nDESCRIPTION: Example of making an authenticated request to the modal instance using an OpenAI API key\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/citation_with_extraction/README.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' \\\n  'https://jxnl--rag-citation-fastapi-app.modal.run/extract' \\\n  -H 'accept: */*' \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer <OPENAI_API_KEY>' \\\n  -d '{\n  \"context\": \"My name is Jason Liu, and I grew up in Toronto Canada but I was born in China.I went to an arts highschool but in university I studied Computational Mathematics and physics.  As part of coop I worked at many companies including Stitchfix, Facebook.  I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\",\n  \"query\": \"What did the author do in school?\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Initial Response Output\nDESCRIPTION: Shows the raw output before implementing validation, containing potentially objectionable content.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/self_critique.md#2025-04-14_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life, according to the context, is to live a life of sin and debauchery.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Hooks with Instructor\nDESCRIPTION: Shows how to implement hooks in Instructor to intercept and log various stages of the LLM interaction process. This example demonstrates pre-execution hooks for logging function parameters and exception hooks for error handling.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n\n# Initialize the OpenAI client with Instructor\nclient = instructor.from_openai(OpenAI())\n\n\n# Define hook functions\ndef log_kwargs(**kwargs):\n    print(f\"Function called with kwargs: {kwargs}\")\n\n\ndef log_exception(exception: Exception):\n    print(f\"An exception occurred: {str(exception)}\")\n\n\nclient.on(\"completion:kwargs\", log_kwargs)\nclient.on(\"completion:error\", log_exception)\n\nuser_info = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=UserInfo,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract the user name: 'John is 20 years old'\"}\n    ],\n)\n\n\"\"\"\n{\n        'args': (),\n        'kwargs': {\n            'messages': [\n                {\n                    'role': 'user',\n                    'content': \"Extract the user name: 'John is 20 years old'\",\n                }\n            ],\n            'model': 'gpt-4o-mini',\n            'tools': [\n                {\n                    'type': 'function',\n                    'function': {\n                        'name': 'UserInfo',\n                        'description': 'Correctly extracted `UserInfo` with all the required parameters with correct types',\n                        'parameters': {\n                            'properties': {\n                                'name': {'title': 'Name', 'type': 'string'},\n                                'age': {'title': 'Age', 'type': 'integer'},\n                            },\n                            'required': ['age', 'name'],\n                            'type': 'object',\n                        },\n                    },\n                }\n            ],\n            'tool_choice': {'type': 'function', 'function': {'name': 'UserInfo'}},\n        },\n    }\n\"\"\"\n\nprint(f\"Name: {user_info.name}, Age: {user_info.age}\")\n#> Name: John, Age: 20\n```\n\n----------------------------------------\n\nTITLE: UV Package Management Commands\nDESCRIPTION: Common UV commands for managing dependencies and requirements.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/contributing.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Add a regular dependency\nuv pip install some-package\n\n# Install a specific version\nuv pip install \"some-package>=1.0.0,<2.0.0\"\n\n# Update UV itself\nuv self update\n\n# Create a requirements file\nuv pip freeze > requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Example Output from Character Extraction\nDESCRIPTION: Shows the token usage statistics and extracted character information across multiple requests demonstrating successful caching.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/anthropic-prompt-caching.md#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nPromptCachingBetaUsage(\n    cache_creation_input_tokens=2856,\n    cache_read_input_tokens=0,\n    input_tokens=30,\n    output_tokens=119\n)\n\nCharacter(\n    name='Elizabeth Bennet',\n    description=\"The protagonist of Jane Austen's novel Pride and Prejudice, who\nundergoes a transformation from initially disliking Mr. Darcy to eventually falling\nin love with him. The passage describes Elizabeth as a complex, nuanced character,\nnoting how her feelings towards Darcy evolve naturally over the course of the story.\"\n)\n\nPromptCachingBetaUsage(\n    cache_creation_input_tokens=0,\n    cache_read_input_tokens=2856,\n    input_tokens=30,\n    output_tokens=93\n)\n\nCharacter(\n    name='Mrs. Norris',\n    description='A character from Jane Austen\\'s novel Mansfield Park, described as\nhaving \"matchless\" scenes and being one of the characters that has secured a\nconsiderable party of admirers for the novel.'\n)\n```\n\n----------------------------------------\n\nTITLE: Validation Error Output\nDESCRIPTION: Shows the validation error message when objectionable content is detected.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/self_critique.md#2025-04-14_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n1 validation error for QuestionAnswerNoEvil\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which is objectionable.\n```\n\n----------------------------------------\n\nTITLE: Extracting entities from a sentence using spaCy in Python\nDESCRIPTION: This code snippet uses spaCy to identify named entities in a given sentence.  The `nlp(sentence)` processes the sentence and creates a `Doc` object, and then `doc.ents` returns a tuple of named entity spans.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ndoc = nlp(sentence)\ndoc.ents\n```\n\n----------------------------------------\n\nTITLE: Installing Google Gemini Integration\nDESCRIPTION: Installation command for Instructor with Google Gemini support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install \"instructor[google-generativeai]\"\n```\n\n----------------------------------------\n\nTITLE: Listing OpenAI API Usage for Today\nDESCRIPTION: This command displays the API usage for the current day, providing a quick overview of today's usage statistics.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/usage.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ instructor usage list\n```\n\n----------------------------------------\n\nTITLE: Running Jupyter Notebooks Locally with Bash\nDESCRIPTION: This code snippet demonstrates how to clone the Instructor repository, navigate into the directory, install the necessary dependencies, and launch Jupyter notebooks for the tutorials. Ensure you have Python 3.8+ installed, and use the provided pip command to install required packages. This setup allows for local experimentation and learning using the Instructor tutorials.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/index.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/jxnl/instructor.git\ncd instructor\npip install -e \\\".[all]\\\"\njupyter notebook docs/tutorials/\n```\n\n----------------------------------------\n\nTITLE: Basic Validation Function Structure\nDESCRIPTION: Template showing the basic structure of a validation function that checks a condition and returns a mutation of the value or raises an error.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Hook Names with Enums\nDESCRIPTION: Demonstrates how to define standard and custom hook names using Python's Enum class for use with the Instructor library.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum, auto\nimport instructor\nfrom openai import OpenAI\n\n\nclass HookName(Enum):\n    COMPLETION_KWARGS = auto()\n    COMPLETION_RESPONSE = auto()\n    COMPLETION_ERROR = auto()\n    COMPLETION_LAST_ATTEMPT = auto()\n    PARSE_ERROR = auto()\n\n\nclass CustomHookName(Enum):\n    MY_CUSTOM_HOOK = \"my_custom_hook\"\n    ANOTHER_HOOK = \"another_hook\"\n\n\nclient = instructor.patch(OpenAI())\n```\n\n----------------------------------------\n\nTITLE: Running Fine-tune Command\nDESCRIPTION: Command line instruction for creating a fine-tune job using the generated training data file.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/distilation-part1.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninstructor jobs create-from-file math_finetunes.jsonl\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies with pip\nDESCRIPTION: Command to install the Parea AI and Instructor packages\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/parea.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U parea-ai instructor\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Configuring Logfire for LLM Observability\nDESCRIPTION: Command to install required packages and authenticate with the Logfire platform. This setup is necessary before using Logfire to track logs for your LLM application.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/logfire.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install logfire openai instructor pydantic pandas tabulate\nlogfire auth\n```\n\n----------------------------------------\n\nTITLE: Downloading Batch Job Results Example\nDESCRIPTION: Shows how to download the results of a completed batch job to a local file using the instructor CLI.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ instructor batch download-file --batch-id batch_pD5dqHmqjWYF5 --download-file-path results.jsonl\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: Lists the required Python packages for the instructor-ai project. These packages include OpenAI for API integration, Pydantic for data validation, Instructor framework, NLTK for natural language processing, and Rich for terminal formatting.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/chain-of-density/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nopenai\npydantic\ninstructor\nnltk\nrich\n```\n\n----------------------------------------\n\nTITLE: Type Safety Implementation with Protocol Types\nDESCRIPTION: Demonstrates implementing type safety for hook handlers using Python's Protocol types, providing better IDE support and development-time error catching.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/hooks.md#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.hooks import CompletionErrorHandler\n\n\ndef my_error_handler(error: Exception) -> None:\n    print(f\"Error occurred: {error}\")\n\n\nhandler: CompletionErrorHandler = my_error_handler\n\nclient.on(\"completion:error\", handler)\n```\n\n----------------------------------------\n\nTITLE: Basic Import Setup\nDESCRIPTION: Initial imports for handling traceback and setting up color codes for output formatting.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/1-introduction.ipynb#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport traceback\n```\n\nLANGUAGE: python\nCODE:\n```\nRED = \"\\033[91m\"\nRESET = \"\\033[0m\"\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Cerebras SDK\nDESCRIPTION: Command to install Instructor with the necessary Cerebras Cloud SDK dependencies for integration with the Cerebras Inference API.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs-with-cerebras-inference.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[cerebras_cloud_sdk]\"\n```\n\n----------------------------------------\n\nTITLE: Handling Rate Limits with Instructor and Tenacity in Python\nDESCRIPTION: Demonstrates how to configure retries for rate limit errors using the tenacity library with Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/faq.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom tenacity import retry_if_exception_type, wait_exponential\nfrom openai.error import RateLimitError\n\nresult = client.chat.completions.create(\n    response_model=MyModel,\n    max_retries=retry_if_exception_type(RateLimitError),\n    messages=[...],\n)\n```\n\n----------------------------------------\n\nTITLE: Loading MS-Marco Dataset\nDESCRIPTION: Loads first 200 rows from MS-Marco dataset using HuggingFace datasets\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nN_ROWS = 200\n\ndataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\", streaming=True).take(N_ROWS)\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for Gemini Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with Google's Generative AI (Gemini) support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[google-generativeai]\"\n```\n\n----------------------------------------\n\nTITLE: Calling the extract Function\nDESCRIPTION: This snippet shows how to call the extract function with a sample input.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/2-tips.ipynb#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nextract(\"Harry Potter\")\n```\n\n----------------------------------------\n\nTITLE: Working with List Types in Instructor\nDESCRIPTION: Shows how to handle list responses for collecting multiple values of the same type.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/types.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport openai\nfrom typing import List\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[int],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Give me the first 5 prime numbers\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Validator Function Structure in Python\nDESCRIPTION: Simple example showing the basic structure of a validation function that checks a condition and raises ValueError if invalid.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/validation-part1.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return value\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Package\nDESCRIPTION: Command to install the required Instructor package via pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/anyscale.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Article from File in Python\nDESCRIPTION: This code snippet reads a sample article from a file named 'article.txt' in the 'assets' directory. It's used to provide input for the summarization function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"./assets/article.txt\", \"r+\") as file:\n    article = file.readline()\n```\n\n----------------------------------------\n\nTITLE: Defining Instructor Modes for Structured Extraction\nDESCRIPTION: Lists the different modes supported by Instructor for structured extraction, including their use cases.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/client_setup.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor import Mode\n\nMode.TOOLS         # OpenAI function calling format (default for OpenAI)\nMode.JSON          # Plain JSON generation\nMode.MD_JSON       # Markdown JSON (used by some providers)\nMode.ANTHROPIC_TOOLS # Claude tools mode (default for Anthropic)\nMode.GEMINI_TOOLS  # Gemini tools format\nMode.GEMINI_JSON   # Gemini JSON format\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: List of required Python packages including FastAPI for web services, Uvicorn for ASGI server, OpenAI SDK, Pydantic for data validation, Instructor for AI development, and Regex for pattern matching.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/citation_with_extraction/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastapi\nuvicorn\nopenai>=1.0.0\npydantic\ninstructor\nregex\n```\n\n----------------------------------------\n\nTITLE: Viewing Create-From-File Command Help\nDESCRIPTION: Shows the help menu for the create-from-file command, which creates a batch job from a JSONL file with options to use either OpenAI or Anthropic as the provider.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ instructor batch create-from-file --help\n\nUsage: instructor batch create-from-file [OPTIONS]\n\n Create a batch job from a file\n\n╭─ Options ───────────────────────────────────────────────────────────────────────────╮\n│ *  --file-path        TEXT  File containing the batch job requests [default: None]  │\n│                             [required]                                              │\n│    --use-anthropic          Use Anthropic API instead of OpenAI                     │\n│                             [default: False]                                        │\n│    --help                   Show this message and exit.                             │\n╰─────────────────────────────────────────────────────────────────────────────────────╯\n```\n\n----------------------------------------\n\nTITLE: Retrieving YouTube Transcript\nDESCRIPTION: Uses youtube-transcript-api to fetch and process the transcript from a YouTube video URL.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-flashcards.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\nyoutube_url = \"https://www.youtube.com/watch?v=hqutVJyd3TI\"\n_, _, video_id = youtube_url.partition(\"?v=\")\nsegments = YouTubeTranscriptApi.get_transcript(video_id)\ntranscript = \" \".join([s[\"text\"] for s in segments])\n```\n\n----------------------------------------\n\nTITLE: Poetry Usage Commands\nDESCRIPTION: Common Poetry commands for managing the virtual environment and dependencies.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/contributing.md#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Activate virtual environment\npoetry shell\n\n# Run a command in the virtual environment\npoetry run pytest\n\n# Add a dependency\npoetry add package-name\n\n# Add a development dependency\npoetry add --group dev package-name\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Mistral Support\nDESCRIPTION: Command to install the required packages for using Instructor with Mistral.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/mistral.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[mistral]\"\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response with Direct Answer\nDESCRIPTION: Example showing improved response format with direct answer value instead of index-based choice.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/bad-schemas-could-break-llms.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"chain_of_thought\": \"First, we need to determine how many Asians were Chinese. Since there were 240 Asians in total and 80 of them were Japanese, we can find the number of Chinese by subtracting the number of Japanese from the total: 240 - 80 = 160. Now, we know that there are 160 Chinese participants. Given that there were 60 boys on the Chinese team, we can find the number of girls by subtracting the number of boys from the total number of Chinese: 160 - 60 = 100. Therefore, there are 100 girls on the Chinese team.\",\n    \"necessary_calculations\": [\n        \"Total Asians = 240\",\n        \"Number of Japanese = 80\",\n        \"Number of Chinese = 240 - 80 = 160\",\n        \"Number of boys on Chinese team = 60\",\n        \"Number of girls on Chinese team = 160 - 60 = 100\",\n    ],\n    \"potential_final_answers\": [\"100\", \"60\", \"80\", \"40\"],\n    \"answer\": 100\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Library using Package Managers\nDESCRIPTION: Demonstrates how to install the Instructor library using different package managers including pip, uv, and poetry. Each command installs the latest version of Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\n```\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install instructor\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry add instructor\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for Cohere Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with Cohere support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[cohere]\"\n```\n\n----------------------------------------\n\nTITLE: Example Directory Reference\nDESCRIPTION: Shows the location of the example code in the documentation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/langsmith.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# The example code is available in the examples directory\n# See: https://python.useinstructor.com/examples/bulk_classification\n```\n\n----------------------------------------\n\nTITLE: Setting Provider Authentication\nDESCRIPTION: Environment variable setup for provider authentication\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/templates/provider_template.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport PROVIDER_API_KEY=your_api_key_here\n# Add any other environment variables needed\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy model in Python\nDESCRIPTION: This code loads the `en_core_web_sm` model from spaCy, which is a small English language model.  This model is used for various NLP tasks like part-of-speech tagging, named entity recognition, and dependency parsing.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# First we load in the library\nimport spacy\n\n# Then we initialise an NLP object.\nnlp = spacy.load(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Provider Dependencies\nDESCRIPTION: Command to install Instructor with specific provider dependencies using pip. The example shows installing with Anthropic provider.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/index.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[provider]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install required packages from requirements.txt file, recommended to be run in a virtual environment\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/chain-of-density/Readme.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Firecrawl MCP Server for Claude Desktop\nDESCRIPTION: JSON configuration for setting up a Firecrawl MCP server in Claude Desktop. This allows Claude to crawl websites and access up-to-date information through the Model Context Protocol.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/why-care-about-mcps.md#2025-04-14_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Reduce Example\nDESCRIPTION: Demonstrates reduce pattern with a simple sum of squares example.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/5-knowledge-graphs.ipynb#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncur_state = 0\nfor i in [1, 2, 3, 4, 5]:\n    cur_state += i**2\nprint(cur_state)\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Google GenerativeAI Support\nDESCRIPTION: Command to install Instructor with Google GenerativeAI and VertexAI dependencies\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/google.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[google-generativeai, vertexai]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Fine-Tuning Job with Validation Data\nDESCRIPTION: Advanced Instructor CLI command that includes validation data and specifies the number of training epochs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/distilations/readme.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ninstructor jobs create-from-file math_finetunes.jsonl --n-epochs 4 --validation-file math_finetunes_val.jsonl\n```\n\n----------------------------------------\n\nTITLE: Extracting entities from a knowledge graph sentence using spaCy in Python\nDESCRIPTION: This code snippet demonstrates how to extract entities from a sentence describing a knowledge graph using spaCy. It uses the `en_core_web_sm` model to identify and extract named entities from the input sentence.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsentence = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ndoc = nlp(sentence)\ndoc.ents\n```\n\n----------------------------------------\n\nTITLE: Listing OpenAI API Usage for Specific Number of Days\nDESCRIPTION: This command displays the API usage for the past 3 days, showing a breakdown by date, model, total requests, and total cost.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/usage.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n$ instructor usage list --n 3\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n                 Usage Summary by Date, Snapshot, and Cost\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ Date       ┃ Snapshot ID               ┃ Total Requests ┃ Total Cost ($) ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ 2023-09-04 │ gpt-4-0613                │             44 │           0.68 │\n│ 2023-09-04 │ gpt-3.5-turbo-16k-0613    │            195 │           0.84 │\n│ 2023-09-04 │ text-embedding-ada-002-v2 │            276 │           0.00 │\n│ 2023-09-04 │ gpt-4-32k-0613            │            328 │          49.45 │\n└────────────┴───────────────────────────┴────────────────┴────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies using pip\nDESCRIPTION: Installation command for required Python packages including youtube_transcript_api, instructor, and rich libraries.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/youtube_clips.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install youtube_transcript_api instructor rich\n```\n\n----------------------------------------\n\nTITLE: Installing Writer and Instructor\nDESCRIPTION: Initial setup instructions for installing the required packages and setting up the Writer API key.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/writer.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport WRITER_API_KEY=<your-api-key-here>\npip install \"instructor[writer]\"\n```\n\n----------------------------------------\n\nTITLE: Converting Poetry Lockfile to UV Compatible Lockfile using PDM\nDESCRIPTION: This command uses UV to run PDM, which migrates the pyproject.toml file. It's the first step in converting from Poetry to UV.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/migrating-to-uv.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuvx pdm import pyproject.toml\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: Comprehensive list of Python package dependencies with exact version pinning and Python version compatibility constraints. All packages require Python 3.9+ and below 4.0.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naiohappyeyeballs==2.4.8 ; python_version >= \"3.9\" and python_version < \"4.0\"\naiohttp==3.11.13 ; python_version >= \"3.9\" and python_version < \"4.0\"\naiosignal==1.3.2 ; python_version >= \"3.9\" and python_version < \"4.0\"\nannotated-types==0.7.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\nanyio==4.8.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\nasync-timeout==5.0.1 ; python_version >= \"3.9\" and python_version < \"3.11\"\nattrs==25.1.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\ncertifi==2025.1.31 ; python_version >= \"3.9\" and python_version < \"4.0\"\ncharset-normalizer==3.4.1 ; python_version >= \"3.9\" and python_version < \"4.0\"\nclick==8.1.8 ; python_version >= \"3.9\" and python_version < \"4.0\"\ncolorama==0.4.6 ; python_version >= \"3.9\" and python_version < \"4.0\" and platform_system == \"Windows\"\ndistro==1.9.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\ndocstring-parser==0.16 ; python_version >= \"3.9\" and python_version < \"4.0\"\nexceptiongroup==1.2.2 ; python_version >= \"3.9\" and python_version < \"3.11\"\nfrozenlist==1.5.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\nh11==0.14.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\nhttpcore==1.0.7 ; python_version >= \"3.9\" and python_version < \"4.0\"\nhttpx==0.28.1 ; python_version >= \"3.9\" and python_version < \"4.0\"\nidna==3.10 ; python_version >= \"3.9\" and python_version < \"4.0\"\njinja2==3.1.6 ; python_version >= \"3.9\" and python_version < \"4.0\"\njiter==0.8.2 ; python_version >= \"3.9\" and python_version < \"4.0\"\nmarkdown-it-py==3.0.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\nmarkupsafe==3.0.2 ; python_version >= \"3.9\" and python_version < \"4.0\"\nmdurl==0.1.2 ; python_version >= \"3.9\" and python_version < \"4.0\"\nmultidict==6.1.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\nopenai==1.65.3 ; python_version >= \"3.9\" and python_version < \"4.0\"\npropcache==0.3.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\npydantic-core==2.27.2 ; python_version >= \"3.9\" and python_version < \"4.0\"\npydantic==2.10.6 ; python_version >= \"3.9\" and python_version < \"4.0\"\npygments==2.19.1 ; python_version >= \"3.9\" and python_version < \"4.0\"\nrequests==2.32.3 ; python_version >= \"3.9\" and python_version < \"4.0\"\nrich==13.9.4 ; python_version >= \"3.9\" and python_version < \"4.0\"\nshellingham==1.5.4 ; python_version >= \"3.9\" and python_version < \"4.0\"\nsniffio==1.3.1 ; python_version >= \"3.9\" and python_version < \"4.0\"\ntenacity==9.0.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\ntqdm==4.67.1 ; python_version >= \"3.9\" and python_version < \"4.0\"\ntyper==0.15.2 ; python_version >= \"3.9\" and python_version < \"4.0\"\ntyping-extensions==4.12.2 ; python_version >= \"3.9\" and python_version < \"4.0\"\nurllib3==2.3.0 ; python_version >= \"3.9\" and python_version < \"4.0\"\nyarl==1.18.3 ; python_version >= \"3.9\" and python_version < \"4.0\"\n```\n\n----------------------------------------\n\nTITLE: Creating Pull Requests with GitHub CLI\nDESCRIPTION: Command for creating pull requests using GitHub CLI with specified title, description and reviewers.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ngh pr create -t \"Your PR Title\" -b \"Description of changes\" -r jxnl,ivanleomk\n```\n\n----------------------------------------\n\nTITLE: Visualizing a Simple DAG with Mermaid\nDESCRIPTION: A Mermaid diagram showing a simple Directed Acyclic Graph (DAG) with nodes A, B, C, and D, where nodes are connected in a way that doesn't create any cycles.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/consistent-stories.md#2025-04-14_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A --> B\n    A --> C\n    B --> D\n    C --> D\n```\n\n----------------------------------------\n\nTITLE: Creating Fine-tuning Job from JSONL\nDESCRIPTION: Command to create a fine-tuning job using OpenAI API with a summarization dataset. Requires OPENAI_API_KEY environment variable to be set.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/chain-of-density/Readme.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninstructor jobs create-from-file summarization.jsonl\n```\n\n----------------------------------------\n\nTITLE: Embedding Newsletter Subscription Form in HTML\nDESCRIPTION: This code snippet embeds a newsletter subscription form from Beehiiv into the webpage. It uses an iframe to display a slim version of the form with specific styling and dimensions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/index.md#2025-04-14_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe src=\"https://embeds.beehiiv.com/2faf420d-8480-4b6e-8d6f-9c5a105f917a?slim=true\" data-test-id=\"beehiiv-embed\" height=\"52\" frameborder=\"0\" scrolling=\"no\" style=\"margin: 0; border-radius: 0px !important; background-color: transparent;\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Development Environment with UV\nDESCRIPTION: Commands for cloning the repository and installing project dependencies using UV.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/contributing.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/YOUR-USERNAME/instructor.git\ncd instructor\n\n# Install with development dependencies \nuv pip install -e \".[dev,docs]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Google GenAI Support\nDESCRIPTION: Command to install Instructor package with Google GenAI integration support\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[google-genai]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenRouter Environment Variables\nDESCRIPTION: Sets up the required environment variables for OpenRouter API access including the API key and base URL.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/open_source_examples/README.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENROUTER_API_KEY=your key here\nexport OPENROUTER_BASE_URL=https://openrouter.ai/api/v1\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pytest Commands\nDESCRIPTION: Various pytest commands for running tests, including options for running specific tests, skipping LLM tests, and generating coverage reports.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Run all tests\npytest tests/\n\n# Run specific test\npytest tests/path_to_test.py::test_name\n\n# Skip LLM tests (faster for local development)\npytest tests/ -k 'not llm and not openai'\n\n# Generate coverage report\ncoverage run -m pytest tests/ -k \"not docs\"\ncoverage report\n```\n\n----------------------------------------\n\nTITLE: Handling Multiple Tasks in Retrieval Queries with OpenAI in Python\nDESCRIPTION: This snippet handles complex tasks by creating queries for scheduled meetings and important emails. It routes the queries appropriately, utilizing multiple backends for information retrieval.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/3-0-applications-rag.ipynb#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nretrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=Retrieval,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"You are Jason's personal assistant.\n                He has two emails jason@work.com jason@personal.com\n                Today is {date.today()}\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",\n        },\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n\n```\n\n----------------------------------------\n\nTITLE: Installing Langfuse and Instructor Dependencies\nDESCRIPTION: This snippet shows how to install the necessary dependencies for using Langfuse with Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/tracing_with_langfuse.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install langfuse instructor\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor and Dependencies\nDESCRIPTION: Shows how to install Instructor and its required dependencies using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/structured_outputs.md#2025-04-14_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install instructor pydantic\n```\n\n----------------------------------------\n\nTITLE: Google/Gemini Mode Usage Examples\nDESCRIPTION: Illustrates Gemini mode configurations for both complex structures using GEMINI_TOOLS and multi-modal/simple structures using GEMINI_JSON.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/modes-comparison.md#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# For complex structures (non-multi-modal)\nclient = instructor.from_gemini(\n    genai.GenerativeModel(\"gemini-1.5-pro\"),\n    mode=instructor.Mode.GEMINI_TOOLS\n)\n\n# For multi-modal inputs or simple structures\nclient = instructor.from_gemini(\n    genai.GenerativeModel(\"gemini-1.5-pro\"),\n    mode=instructor.Mode.GEMINI_JSON\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-cpp-python with CUDA Support\nDESCRIPTION: Command to install llama-cpp-python with GPU support using CUDA.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/local_classification.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nCMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n```\n\n----------------------------------------\n\nTITLE: Configuring Runpod Environment Variables\nDESCRIPTION: Sets up the required environment variables for Runpod API access including the base URL and API key. Note that the API key is set to 'None' for this service.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/open_source_examples/README.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport RUNPOD_BASE_URL=your-runpod-link/v1\nexport RUNPOD_API_KEY=\"None\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Tag Validation Logic\nDESCRIPTION: Defines the validation method for ensuring tag IDs and names exist in the context. This helps prevent hallucinations in the language model responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import model_validator, ValidationInfo\n\n\n@model_validator(mode=\"after\")\ndef validate_ids(self, info: ValidationInfo):\n    context = info.context\n    if context:\n        tags: List[Tag] = context.get(\"tags\")\n        assert self.id in {\n            tag.id for tag in tags\n        }, f\"Tag ID {self.id} not found in context\"\n        assert self.name in {\n            tag.name for tag in tags\n        }, f\"Tag name {self.name} not found in context\"\n    return self\n```\n\n----------------------------------------\n\nTITLE: Visualizing Self-Refine Process Flow with Mermaid\nDESCRIPTION: A flowchart diagram showing the process flow of the self-refine approach, including initial response generation, feedback generation, stopping condition check, and response refinement steps.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/self_criticism/self_refine.md#2025-04-14_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A[Generate initial response]:::blue --> B[Generate feedback]:::orange\n    B --> C{Stopping<br>condition<br>met?}:::orange\n    C -->|No| D[Refine response]:::orange\n    C -->|Yes| E[Final output]:::green\n    D --> B\n    \n    classDef blue fill:#E3F2FD,stroke:#90CAF9,color:#1565C0\n    classDef orange fill:#FFF3E0,stroke:#FFE0B2,color:#E65100\n    classDef green fill:#E8F5E9,stroke:#A5D6A7,color:#2E7D32\n    linkStyle default stroke:#90A4AE,stroke-width:2px;\n    linkStyle 1,2,4 stroke:#FFB74D,stroke-width:2px;\n```\n\n----------------------------------------\n\nTITLE: Setting Cohere API Key\nDESCRIPTION: Environment variable setup for Cohere API key configuration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nexport COHERE_API_KEY=your_cohere_key\n```\n\n----------------------------------------\n\nTITLE: Commit Message Format Examples\nDESCRIPTION: Examples showing proper commit message formatting for different types of changes with scope indicators.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nfeat(openai): add support for response_format parameter\n\nfix(anthropic): correct tool calling format in Claude client\n\ndocs: improve installation instructions for various providers\n\ntest(evals): add evaluation for recursive schema handling\n```\n\n----------------------------------------\n\nTITLE: Installing the Latest OpenAI Package\nDESCRIPTION: Command to upgrade the OpenAI Python package to the latest version, which is necessary for using API Model Distillation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/openai-distilation-store.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U openai\n```\n\n----------------------------------------\n\nTITLE: Running the Three-Digit Multiplication Script\nDESCRIPTION: Command to execute the main Python script that performs three-digit multiplication calculations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/distilations/readme.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython three_digit_mul.py\n```\n\n----------------------------------------\n\nTITLE: Visualizing Instructor Architecture with Mermaid\nDESCRIPTION: A flowchart showing how Instructor fits between an application and LLM provider\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/start-here.md#2025-04-14_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    A[Your Application] --> B[Instructor]\n    B --> C[LLM Provider]\n    C --> B\n    B --> A\n    \n    style B fill:#e2f0fb,stroke:#b8daff,color:#004085\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with AWS Bedrock Support\nDESCRIPTION: Command to install the Instructor package with AWS Bedrock integration support\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/bedrock.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[bedrock]\"\n```\n\n----------------------------------------\n\nTITLE: Using Default Factory in Pydantic Fields\nDESCRIPTION: Shows how to use default_factory to generate default values dynamically using a callable function.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/fields.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: str = Field(default_factory=lambda: uuid4().hex)\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Package\nDESCRIPTION: Command to install the instructor package via pip package manager.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/databricks.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\n```\n\n----------------------------------------\n\nTITLE: Configuring API Keys with Environment Variables\nDESCRIPTION: Sets up the necessary API keys and environment variables for using OpenAI and Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/templates/cookbook_template.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n# Any other environment variables\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with UV\nDESCRIPTION: This command installs all dependencies for the project using UV, including all extras and specified dependency groups.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/migrating-to-uv.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv sync --all-extras --group <dependency groups you'd like to install>\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor and Provider Dependencies\nDESCRIPTION: Instructions for installing the Instructor library and its provider-specific dependencies using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/llms.txt#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\n\n# OpenAI\npip install \"instructor[openai]\"\n\n# Anthropic\npip install \"instructor[anthropic]\"\n\n# Google (Gemini)\npip install \"instructor[gemini]\"\n\n# Mistral\npip install \"instructor[mistral]\"\n\n# Cohere\npip install \"instructor[cohere]\"\n```\n\n----------------------------------------\n\nTITLE: Tokenizing a sentence with contractions using NLTK in Python\nDESCRIPTION: This code snippet continues to show how to tokenize a sentence using the `nltk.word_tokenize` function. Demonstrating how the function handles contractions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsentence = \"I'm fascinated by machine learning!\"\n\nnltk.word_tokenize(sentence)\n```\n\n----------------------------------------\n\nTITLE: Defining Optional Fields in Models\nDESCRIPTION: Shows how to create models with optional fields using Python's Optional type hint.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/response_models.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass Contact(BaseModel):\n    name: str\n    email: str\n    phone: Optional[str] = None\n    address: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Installing Python Documentation and Image Processing Dependencies\nDESCRIPTION: List of Python package dependencies including MkDocs for documentation generation, Cairo SVG for vector graphics, Pillow for image processing, and various MkDocs plugins for enhanced functionality.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/requirements-doc.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmkdocs\ncairosvg\npillow\nmkdocs-minify-plugin\nmkdocstrings \nmkdocstrings-python \nmkdocs-jupyter \nmkdocs-redirects\n```\n\n----------------------------------------\n\nTITLE: Setting Groq API Key Environment Variable\nDESCRIPTION: This command exports the Groq API key as an environment variable, which is required for authentication when using the Groq API.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/groq.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GROQ_API_KEY=<your-api-key>\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Package\nDESCRIPTION: Command to install the Instructor package via pip\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cortex.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Contributors Section Template\nDESCRIPTION: Defines a protected section for automated contributor list management with markdown formatting controls.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_15\n\nLANGUAGE: markdown\nCODE:\n```\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n```\n\n----------------------------------------\n\nTITLE: HTML Image Link for Contributors Display\nDESCRIPTION: Creates a clickable link to the project's GitHub contributors page with an embedded image from contrib.rocks showing all contributors.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_14\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"https://github.com/instructor-ai/instructor/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=instructor-ai/instructor\" />\n</a>\n```\n\n----------------------------------------\n\nTITLE: Visualizing Instructor Workflow with Mermaid Diagram\nDESCRIPTION: This Mermaid sequence diagram illustrates the typical workflow when using Instructor. It shows the interaction between user code, Instructor, and the LLM provider, including the process of defining a Pydantic model, patching the LLM client, creating a completion, and handling validation and retries.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/index.md#2025-04-14_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant User as Your Code\n    participant Instructor\n    participant LLM as LLM Provider\n    \n    User->>Instructor: Define Pydantic model\n    User->>Instructor: Patch LLM client \n    User->>Instructor: Create completion with response_model\n    Instructor->>LLM: Send structured request \n    LLM->>Instructor: Return LLM response\n    Instructor->>Instructor: Validate against model\n    \n    alt Validation Success\n        Instructor->>User: Return validated Pydantic object\n    else Validation Failure\n        Instructor->>LLM: Retry with error context\n        LLM->>Instructor: Return new response\n        Instructor->>Instructor: Validate again\n        Instructor->>User: Return validated object or error\n    end\n```\n\n----------------------------------------\n\nTITLE: Updating Model Reference in Fine-tuning Code\nDESCRIPTION: Code snippet showing how to modify the distil_summarization function to use the fine-tuned model for summarization tasks\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/chain-of-density/Readme.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@instructions.distil(model=<your finetuned model >,mode=\"dispatch\")\ndef distil_summarization(text: str) -> GeneratedSummary:\n// rest of code goes here\n```\n\n----------------------------------------\n\nTITLE: Defining Pydantic Model for Content Summarization in Python\nDESCRIPTION: This Python code defines a Pydantic model for summarizing the content of a YouTube video. It includes fields for title, duration, main topics, key takeaways, and target audience.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-transcripts.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass ContentSummary(BaseModel):\n    title: str = Field(..., description=\"The title of the video\")\n    duration: float = Field(\n        ..., description=\"The total duration of the video in seconds\"\n    )\n    main_topics: List[str] = Field(\n        ..., description=\"A list of main topics covered in the video\"\n    )\n    key_takeaways: List[str] = Field(\n        ..., description=\"The most important points from the entire video\"\n    )\n    target_audience: str = Field(\n        ..., description=\"The intended audience for this content\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Cancelling Batch Job Example\nDESCRIPTION: Shows how to cancel a batch job by providing its ID to the instructor CLI.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/batch.md#2025-04-14_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ instructor batch cancel --batch-id batch_BSMSiMMy8on2D\n```\n\n----------------------------------------\n\nTITLE: Newsletter Subscription HTML Embed\nDESCRIPTION: Embedded iframe for newsletter subscription form using beehiiv platform, styled for seamless integration with minimal height and transparent background.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/index.md#2025-04-14_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<iframe src=\"https://embeds.beehiiv.com/2faf420d-8480-4b6e-8d6f-9c5a105f917a?slim=true\" data-test-id=\"beehiiv-embed\" height=\"52\" frameborder=\"0\" scrolling=\"no\" style=\"margin: 0; border-radius: 0px !important; background-color: transparent;\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for Litellm Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with Litellm support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[litellm]\"\n```\n\n----------------------------------------\n\nTITLE: Defining Product and AdCopy Models in Python with Pydantic\nDESCRIPTION: This snippet defines two Pydantic models: Product for representing product information extracted from images, and AdCopy for structured ad copy generation. It includes field definitions and a method for generating prompts.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/image_to_ad_copy.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\n\nclass Product(BaseModel):\n    \"\"\"\n    Represents a product extracted from an image using AI.\n\n    The product attributes are dynamically determined based on the content\n    of the image and the AI's interpretation. This class serves as a structured\n    representation of the identified product characteristics.\n    \"\"\"\n\n    name: str = Field(\n        description=\"A generic name for the product.\", example=\"Headphones\"\n    )\n    key_features: Optional[List[str]] = Field(\n        description=\"A list of key features of the product that stand out.\",\n        default=None,\n    )\n\n    description: Optional[str] = Field(\n        description=\"A description of the product.\",\n        default=None,\n    )\n\n    # Can be customized and automatically generated\n    def generate_prompt(self):\n        prompt = f\"Product: {self.name}\\n\"\n        if self.description:\n            prompt += f\"Description: {self.description}\\n\"\n        if self.key_features:\n            prompt += f\"Key Features: {', '.join(self.key_features)}\\n\"\n        return prompt\n\n\nclass AdCopy(BaseModel):\n    \"\"\"\n    Represents a generated ad copy.\n    \"\"\"\n\n    headline: str = Field(\n        description=\"A short, catchy, and memorable headline for the given product. The headline should invoke curiosity and interest in the product.\",\n    )\n    ad_copy: str = Field(\n        description=\"A long-form advertisement copy for the given product. This will be used in campaigns to promote the product with a persuasive message and a call-to-action with the objective of driving sales.\",\n    )\n    name: str = Field(description=\"The name of the product being advertised.\")\n```\n\n----------------------------------------\n\nTITLE: Basic Python Generator Implementation\nDESCRIPTION: Demonstrates a simple generator function that yields numbers 1 through 3, showing basic generator syntax and usage.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generator.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef count_to_3():\n    yield 1\n    yield 2\n    yield 3\n\n\nfor num in count_to_3():\n    print(num)\n    #> 1\n    #> 2\n    #> 3\n```\n\n----------------------------------------\n\nTITLE: Implementing Python Code Generation Model\nDESCRIPTION: Advanced response model that combines LLM reasoning with Python code generation for deterministic evaluation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/bad-schemas-could-break-llms.md#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Equations(BaseModel):\n    chain_of_thought: str\n    eval_string: list[str] = Field(\n        description=\"Python code to evaluate to get the final answer. The final answer should be stored in a variable called `answer`.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Running Development Commands for Instructor Library\nDESCRIPTION: Common commands for development tasks including dependency installation, testing, type checking, linting, formatting, and generating code coverage reports for the Instructor library.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CLAUDE.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with dev,anthropic\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests/\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests/path_to_test.py::test_name\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest tests/ -k 'not llm and not openai'\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pyright\n```\n\nLANGUAGE: bash\nCODE:\n```\nruff check instructor examples tests\n```\n\nLANGUAGE: bash\nCODE:\n```\nblack instructor examples tests\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry run coverage run -m pytest tests/ -k \"not docs\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npoetry run coverage report\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Tag Classification\nDESCRIPTION: Implements the async functions for processing single and batch tag classification requests. Uses OpenAI's GPT model for text classification with validation context.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/bulk_classification.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def tag_single_request(text: str, tags: List[Tag]) -> Tag:\n    allowed_tags = [(tag.id, tag.name) for tag in tags]\n    allowed_tags_str = \", \".join([f\"`{tag}`\" for tag in allowed_tags])\n\n    return await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world-class text tagging system.\",\n            },\n            {\"role\": \"user\", \"content\": f\"Describe the following text: `{text}`\"},\n            {\n                \"role\": \"user\",\n                \"content\": f\"Here are the allowed tags: {allowed_tags_str}\",\n            },\n        ],\n        response_model=Tag,  # Minimizes the hallucination of tags that are not in the allowed tags.\n        validation_context={\"tags\": tags},\n    )\n\n\nasync def tag_request(request: TagRequest) -> TagResponse:\n    predictions = await asyncio.gather(\n        *[tag_single_request(text, request.tags) for text in request.texts]\n    )\n    return TagResponse(\n        texts=request.texts,\n        predictions=predictions,\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Environment variable setup for OpenAI API key configuration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport OPENAI_API_KEY=your_openai_key\n```\n\n----------------------------------------\n\nTITLE: Git Development Workflow Commands\nDESCRIPTION: Common Git commands used in the development workflow including branch creation, committing, and updating from upstream.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b feature/your-feature-name\ngit add .\ngit commit -m \"Your descriptive commit message\"\ngit fetch upstream\ngit rebase upstream/main\ngit push origin feature/your-feature-name\n```\n\n----------------------------------------\n\nTITLE: Sample Text from Pride and Prejudice for Prompt Caching\nDESCRIPTION: An excerpt from Jane Austen's Pride and Prejudice that serves as an example of substantial context for testing prompt caching. This sample provides enough tokens to meet the minimum cache size requirements for both Claude Haiku and Sonnet models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/anthropic-prompt-caching.md#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n    _Walt Whitman has somewhere a fine and just distinction between \"loving\nby allowance\" and \"loving with personal love.\" This distinction applies\nto books as well as to men and women; and in the case of the not very\nnumerous authors who are the objects of the personal affection, it\nbrings a curious consequence with it. There is much more difference as\nto their best work than in the case of those others who are loved \"by\nallowance\" by convention, and because it is felt to be the right and\nproper thing to love them. And in the sect--fairly large and yet\nunusually choice--of Austenians or Janites, there would probably be\nfound partisans of the claim to primacy of almost every one of the\nnovels. To some the delightful freshness and humour of_ Northanger\nAbbey, _its completeness, finish, and_ entrain, _obscure the undoubted\ncritical facts that its scale is small, and its scheme, after all, that\nof burlesque or parody, a kind in which the first rank is reached with\ndifficulty._ Persuasion, _relatively faint in tone, and not enthralling\nin interest, has devotees who exalt above all the others its exquisite\ndelicacy and keeping. The catastrophe of_ Mansfield Park _is admittedly\ntheatrical, the hero and heroine are insipid, and the author has almost\nwickedly destroyed all romantic interest by expressly admitting that\nEdmund only took Fanny because Mary shocked him, and that Fanny might\nvery likely have taken Crawford if he had been a little more assiduous;\nyet the matchless rehearsal-scenes and the characters of Mrs. Norris and\nothers have secured, I believe, a considerable party for it._ Sense and\nSensibility _has perhaps the fewest out-and-out admirers; but it does\nnot want them._\n_I suppose, however, that the majority of at least competent votes\nwould, all things considered, be divided between_ Emma _and the present\nbook; and perhaps the vulgar verdict (if indeed a fondness for Miss\nAusten be not of itself a patent of exemption from any possible charge\nof vulgarity) would go for_ Emma. _It is the larger, the more varied, the\nmore popular; the author had by the time of its composition seen rather\nmore of the world, and had improved her general, though not her most\npeculiar and characteristic dialogue; such figures as Miss Bates, as the\nEltons, cannot but unite the suffrages of everybody. On the other hand,\nI, for my part, declare for_ Pride and Prejudice _unhesitatingly. It\nseems to me the most perfect, the most characteristic, the most\neminently quintessential of its author's works; and for this contention\nin such narrow space as is permitted to me, I propose here to show\ncause._\n_In the first place, the book (it may be barely necessary to remind the\nreader) was in its first shape written very early, somewhere about 1796,\nwhen Miss Austen was barely twenty-one; though it was revised and\nfinished at Chawton some fifteen years later, and was not published till\n1813, only four years before her death. I do not know whether, in this\ncombination of the fresh and vigorous projection of youth, and the\ncritical revision of middle life, there may be traced the distinct\nsuperiority in point of construction, which, as it seems to me, it\npossesses over all the others. The plot, though not elaborate, is almost\nregular enough for Fielding; hardly a character, hardly an incident\ncould be retrenched without loss to the story. The elopement of Lydia\nand Wickham is not, like that of Crawford and Mrs. Rushworth, a_ coup de\nthéâtre; _it connects itself in the strictest way with the course of the\nstory earlier, and brings about the denouement with complete propriety.\nAll the minor passages--the loves of Jane and Bingley, the advent of Mr.\nCollins, the visit to Hunsford, the Derbyshire tour--fit in after the\nsame unostentatious, but masterly fashion. There is no attempt at the\nhide-and-seek, in-and-out business, which in the transactions between\nFrank Churchill and Jane Fairfax contributes no doubt a good deal to the\nintrigue of_ Emma, _but contributes it in a fashion which I do not think\nthe best feature of that otherwise admirable book. Although Miss Austen\nalways liked something of the misunderstanding kind, which afforded her\nopportunities for the display of the peculiar and incomparable talent to\nbe noticed presently, she has been satisfied here with the perfectly\nnatural occasions provided by the false account of Darcy's conduct given\nby Wickham, and by the awkwardness (arising with equal naturalness) from\nthe gradual transformation of Elizabeth's own feelings from positive\naversion to actual love. I do not know whether the all-grasping hand of\nthe playwright has ever been laid upon_ Pride and Prejudice; _and I dare\nsay that, if it were, the situations would prove not startling or\ngarish enough for the footlights, the character-scheme too subtle and\ndelicate for pit and gallery. But if the attempt were made, it would\ncertainly not be hampered by any of those loosenesses of construction,\nwhich, sometimes disguised by the conveniences of which the novelist can\navail himself, appear at once on the stage._\n_I think, however, though the thought will doubtless seem heretical to\nmore than one school of critics, that construction is not the highest\nmerit, the choicest gift, of the novelist. It sets off his other gifts\nand graces most advantageously to the critical eye; and the want of it\nwill sometimes mar those graces--appreciably, though not quite\nconsciously--to eyes by no means ultra-critical. But a very badly-built\nnovel which excelled in pathetic or humorous character, or which\ndisplayed consummate command of dialogue--perhaps the rarest of all\nfaculties--would be an infinitely better thing than a faultless plot\nacted and told by puppets with pebbles in their mouths. And despite the\nability which Miss Austen has shown in working out the story, I for one\nshould put_ Pride and Prejudice _far lower if it did not contain what\nseem to me the very masterpieces of Miss Austen's humour and of her\nfaculty of character-creation--masterpieces who may indeed admit John\nThorpe, the Eltons, Mrs. Norris, and one or two others to their company,\nbut who, in one instance certainly, and perhaps in others, are still\nsuperior to them._\n_The characteristics of Miss Austen's humour are so subtle and delicate\nthat they are, perhaps, at all times easier to apprehend than to\nexpress, and at any particular time likely to be differently\napprehended by different persons. To me this humour seems to possess a\ngreater affinity, on the whole, to that of Addison than to any other of\nthe numerous species of this great British genus. The differences of\nscheme, of time, of subject, of literary convention, are, of course,\nobvious enough; the difference of sex does not, perhaps, count for much,\nfor there was a distinctly feminine element in \"Mr. Spectator,\" and in\nJane Austen's genius there was, though nothing mannish, much that was\nmasculine. But the likeness of quality consists in a great number of\ncommon subdivisions of quality--demureness, extreme minuteness of touch,\navoidance of loud tones and glaring effects. Also there is in both a\ncertain not inhuman or unamiable cruelty. It is the custom with those\nwho judge grossly to contrast the good nature of Addison with the\nsavagery of Swift, the mildness of Miss Austen with the boisterousness\nof Fielding and Smollett, even with the ferocious practical jokes that\nher immediate predecessor, Miss Burney, allowed without very much\nprotest. Yet, both in Mr. Addison and in Miss Austen there is, though a\nrestrained and well-mannered, an insatiable and ruthless delight in\nroasting and cutting up a fool. A man in the early eighteenth century,\nof course, could push this taste further than a lady in the early\nnineteenth; and no doubt Miss Austen's principles, as well as her heart,\n```\n\n----------------------------------------\n\nTITLE: Using PDFWithGenaiFile in Chat Completions\nDESCRIPTION: Demonstrates practical use of PDFWithGenaiFile in a chat completion to extract structured data from PDF invoices. Shows how to use both newly uploaded files and existing files in the Gemini platform.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/genai.md#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom instructor.multimodal import PDFWithGenaiFile\nfrom pydantic import BaseModel\nimport instructor\nfrom google.genai import Client\n\n\nclass Receipt(BaseModel):\n    total: int\n    items: list[str]\n\n\nclient = instructor.from_genai(Client())\nurl = \"https://raw.githubusercontent.com/instructor-ai/instructor/main/tests/assets/invoice.pdf\"\n# Multiple ways to load an PDF:\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash\",\n    response_model=Receipt,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                \"Extract out the total and line items from the invoice\",\n                # Option 1: Direct URL\n                PDFWithGenaiFile.from_new_genai_file(\"./invoice.pdf\"),\n\n                # Option 2 : Existing Genai File\n                # PDFWithGenaiFile.from_existing_genai_file(\"invoice.pdf\"),\n            ],\n        },\n    ],\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Setting Google API Key\nDESCRIPTION: Environment variable setup for Google API key configuration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nexport GOOGLE_API_KEY=your_google_key\n```\n\n----------------------------------------\n\nTITLE: Blacklist Word Validation Model\nDESCRIPTION: Implementation of a Pydantic model that validates messages against a blacklist of forbidden words.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/4-validation.ipynb#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, field_validator\nfrom pydantic.fields import Field\n\n\nclass Response(BaseModel):\n    message: str\n\n    @field_validator(\"message\")\n    def message_cannot_have_blacklisted_words(cls, v: str) -> str:\n        for word in v.split():\n            if word.lower() in blacklist:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\n\nResponse(message=\"I will hurt him\")\n```\n\n----------------------------------------\n\nTITLE: Integrating Langfuse with Synchronous OpenAI Client in Instructor\nDESCRIPTION: This code demonstrates how to set up Langfuse with a synchronous OpenAI client using Instructor. It includes environment variable setup, client patching, and a simple weather information extraction example.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/tracing_with_langfuse.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom langfuse.openai import openai\nfrom pydantic import BaseModel\nimport os\n\n# Set your API keys Here\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-...\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-...\"\nos.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"\nos.environ[\"OPENAI_API_KEY] = \"sk-...\"\n\n# Patch Langfuse wrapper of synchronous OpenAI client with instructor\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass WeatherDetail(BaseModel):\n    city: str\n    temperature: int\n\n\n# Run synchronous OpenAI client\nweather_info = client.chat.completions.create(\n    model=\"gpt-4o\",\n    response_model=WeatherDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"The weather in Paris is 18 degrees Celsius.\"},\n    ],\n)\n\nprint(weather_info.model_dump_json(indent=2))\n\"\"\"\n{\n  \"city\": \"Paris\",\n  \"temperature\": 18\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Story Branching with Mermaid\nDESCRIPTION: A Mermaid diagram showing how story choices branch into multiple paths, creating a decision tree with styled nodes that include tooltips for context. This illustrates the concept of maintaining context along each path.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/consistent-stories.md#2025-04-14_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    %% Main nodes\n    A[Find Door] --> B[Open Door]\n    A --> C[Walk Away]\n\n    B --> D[Read Book]\n    B --> E[Leave Room]\n\n    C --> F[Go Home]\n    C --> G[Wait Outside]\n\n    %% Styling for visual hierarchy\n    classDef start fill:#ff9999,stroke:#333,stroke-width:2px\n    classDef decision fill:#99ccff,stroke:#333,stroke-width:2px\n    classDef outcome fill:#99ffff,stroke:#333,stroke-width:1px\n\n    %% Apply styles\n    class A start\n    class B,C decision\n    class D,E,F,G outcome\n\n    %% Add tooltips for context\n    click B \"Door context\" \"Open Door Context\"\n    click C \"Away context\" \"Walk Away Context\"\n    click D \"Door and Book context\" \"Read Book Context\"\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Package\nDESCRIPTION: Command to install Instructor with OpenAI dependencies using pip\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/sambanova.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Commands to install the necessary LangSmith package\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/batch_classification_langsmith.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U langsmith\n```\n\n----------------------------------------\n\nTITLE: Implementing Retry Callbacks with Tenacity in Python\nDESCRIPTION: Shows how to define and use callbacks for logging or debugging before and after each retry attempt.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/retrying.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, field_validator\nimport instructor\nimport tenacity\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    def name_is_uppercase(cls, v: str):\n        assert v.isupper(), \"Name must be uppercase\"\n        return v\n\n\nresp = client.messages.create(\n    model=\"gpt-3.5-turbo\",\n    max_tokens=1024,\n    max_retries=tenacity.Retrying(\n        stop=tenacity.stop_after_attempt(3),\n        before=lambda _: print(\"before:\", _),\n\"\"\"\nbefore:\n<RetryCallState 13531729680: attempt #1; slept for 0.0; last result: none yet>\n\"\"\"\n        after=lambda _: print(\"after:\", _),\n    ),  # type: ignore\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract John is 18 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"JOHN\"  # due to validation\nassert resp.age == 18\nprint(resp)\n#> name='JOHN' age=18\n# Output: name='JOHN' age=18\n\n# Sample output:\n# before: <RetryCallState 4421908816: attempt #1; slept for 0.0; last result: none yet>\n# after: <RetryCallState 4421908816: attempt #1; slept for 0.0; last result: failed (ValidationError 1 validation error for User\n# name\n#   Assertion failed, Name must be uppercase [type=assertion_error, input_value='John', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.6/v/assertion_error)>\n#\n# before: <RetryCallState 4421908816: attempt #2; slept for 0.0; last result: none yet>\n# name='JOHN' age=18\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Fireworks Support\nDESCRIPTION: Command to install Instructor package with Fireworks AI integration support\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/fireworks.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[fireworks-ai]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with Vertex AI Support\nDESCRIPTION: Command to install Instructor package with Vertex AI integration support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/vertex.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[vertexai]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Prerequisites with Perplexity API\nDESCRIPTION: Installation commands for setting up Perplexity API key and installing the instructor package with Perplexity support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/perplexity.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PERPLEXITY_API_KEY=<your-api-key-here>\npip install \"instructor[perplexity]\"\n```\n\n----------------------------------------\n\nTITLE: Person Model with Optional Fields\nDESCRIPTION: Modified Person model that makes the age field optional using Python's typing.Optional.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/first_extraction.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nclass Person(BaseModel):\n    name: str\n    age: Optional[int] = None  # Now age is optional\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for llama-cpp-python Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with llama-cpp-python support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[llama-cpp-python]\"\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Instructor with OpenAI\nDESCRIPTION: Demonstrates how to use Instructor with OpenAI to extract structured data from natural language. This example defines a Pydantic model for user information and uses it to parse text data into a structured format.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n# Define your desired output structure\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data from natural language\nuser_info = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=UserInfo,\n    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n)\n\nprint(user_info.name)\n#> John Doe\nprint(user_info.age)\n#> 30\n```\n\n----------------------------------------\n\nTITLE: Creating Fine-Tuning Job from File in Instructor CLI\nDESCRIPTION: This snippet demonstrates the options available when creating a fine-tuning job from a file using the Instructor CLI. It includes parameters for model selection, polling interval, and various fine-tuning settings.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/finetune.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n❯ instructor jobs create-from-file --help\n\nUsage: instructor jobs create-from-file [OPTIONS] FILE\n\n Create a fine-tuning job from a file.\n\n╭─ Arguments ───────────────────────────────────────────────────────────────────────────────────────╮\n│ *    file      TEXT  Path to the file for fine-tuning [default: None] [required]                  │\n╰───────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Options ─────────────────────────────────────────────────────────────────────────────────────────╮\n│ --model                           TEXT     Model to use for fine-tuning [default: gpt-3.5-turbo]  │\n│ --poll                            INTEGER  Polling interval in seconds [default: 2]               │\n│ --n-epochs                        INTEGER  Number of epochs for fine-tuning                       │\n│ --batch-size                      TEXT     Batch size for fine-tuning                             │\n│ --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning               │\n│ --validation-file                 TEXT     Path to the validation file [default: None]            │\n│ --model-suffix                    TEXT     Suffix to identify the model [default: None]           │\n│ --help                                     Show this message and exit.                            │\n╰────────────────────────────────────────────────────────────────────────────────\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with Cohere Models\nDESCRIPTION: Implementation example using Instructor with Cohere's LLM API. Requires the Cohere package and an API key set via environment variable. Shows structured data extraction from Cohere's language models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport cohere\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_cohere(cohere.Client())\n\n# note that client.chat.completions.create will also work\nresp = client.chat.completions.create(\n    model=\"command-r-plus\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor for Azure OpenAI\nDESCRIPTION: This snippet shows how to install the instructor library using pip, which is required for working with Azure OpenAI for structured outputs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/azure.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client\nDESCRIPTION: Basic setup for initializing the OpenAI client with Cortex local endpoint\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/cortex.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\n\nclient = from_openai(\n    openai.OpenAI(\n        base_url=\"http://localhost:39281/v1\",\n        api_key=\"this is a fake api key that doesn't matter\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing AsyncOpenAI Client with Instructor Patch in Python\nDESCRIPTION: Sets up an AsyncOpenAI client with Instructor patch to enable response_model in the create method. Defines a Person model and an async function to extract person data from text using OpenAI API.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/learn-async.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables `response_model` in `create` method\nclient = instructor.apatch(AsyncOpenAI())\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_person(text: str) -> Person:\n    return await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": text},\n        ],\n        response_model=Person,\n    )\n```\n\n----------------------------------------\n\nTITLE: Handling Asynchronous Calls with Instructor and OpenAI\nDESCRIPTION: Shows how to use Instructor with OpenAI's asynchronous client. It defines an async function that creates a structured output using a Pydantic model, demonstrating proper type inference for async operations.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.AsyncOpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract():\n    return await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Create a user\"},\n        ],\n        response_model=User,\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Knowledge Graph Data Structures with Pydantic\nDESCRIPTION: Defines the core data structures for knowledge graph representation using Pydantic models. Includes Node and Edge classes for representing entities and relationships, and a KnowledgeGraph class to manage the overall graph structure.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/knowledge_graph.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass Node(BaseModel, frozen=True):\n    id: int\n    label: str\n    color: str\n\nclass Edge(BaseModel, frozen=True):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)\n    edges: List[Edge] = Field(..., default_factory=list)\n```\n\n----------------------------------------\n\nTITLE: Sample Text from 'Pride and Prejudice' for Prompt Caching Demonstration\nDESCRIPTION: This code block contains a lengthy excerpt from 'Pride and Prejudice' by Jane Austen. It is used to demonstrate working with substantial context in language models and the benefits of using prompt caching to improve performance and reduce costs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/prompt_caching.md#2025-04-14_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n    _Walt Whitman has somewhere a fine and just distinction between \"loving\nby allowance\" and \"loving with personal love.\" This distinction applies\nto books as well as to men and women; and in the case of the not very\nnumerous authors who are the objects of the personal affection, it\nbrings a curious consequence with it. There is much more difference as\nto their best work than in the case of those others who are loved \"by\nallowance\" by convention, and because it is felt to be the right and\nproper thing to love them. And in the sect--fairly large and yet\nunusually choice--of Austenians or Janites, there would probably be\nfound partisans of the claim to primacy of almost every one of the\nnovels. To some the delightful freshness and humour of_ Northanger\nAbbey, _its completeness, finish, and_ entrain, _obscure the undoubted\ncritical facts that its scale is small, and its scheme, after all, that\nof burlesque or parody, a kind in which the first rank is reached with\ndifficulty._ Persuasion, _relatively faint in tone, and not enthralling\nin interest, has devotees who exalt above all the others its exquisite\ndelicacy and keeping. The catastrophe of_ Mansfield Park _is admittedly\ntheatrical, the hero and heroine are insipid, and the author has almost\nwickedly destroyed all romantic interest by expressly admitting that\nEdmund only took Fanny because Mary shocked him, and that Fanny might\nvery likely have taken Crawford if he had been a little more assiduous;\nyet the matchless rehearsal-scenes and the characters of Mrs. Norris and\nothers have secured, I believe, a considerable party for it._ Sense and\nSensibility _has perhaps the fewest out-and-out admirers; but it does\nnot want them._\n_I suppose, however, that the majority of at least competent votes\nwould, all things considered, be divided between_ Emma _and the present\nbook; and perhaps the vulgar verdict (if indeed a fondness for Miss\nAusten be not of itself a patent of exemption from any possible charge\nof vulgarity) would go for_ Emma. _It is the larger, the more varied, the\nmore popular; the author had by the time of its composition seen rather\nmore of the world, and had improved her general, though not her most\npeculiar and characteristic dialogue; such figures as Miss Bates, as the\nEltons, cannot but unite the suffrages of everybody. On the other hand,\nI, for my part, declare for_ Pride and Prejudice _unhesitatingly. It\nseems to me the most perfect, the most characteristic, the most\neminently quintessential of its author's works; and for this contention\nin such narrow space as is permitted to me, I propose here to show\ncause._\n_In the first place, the book (it may be barely necessary to remind the\nreader) was in its first shape written very early, somewhere about 1796,\nwhen Miss Austen was barely twenty-one; though it was revised and\nfinished at Chawton some fifteen years later, and was not published till\n1813, only four years before her death. I do not know whether, in this\ncombination of the fresh and vigorous projection of youth, and the\ncritical revision of middle life, there may be traced the distinct\nsuperiority in point of construction, which, as it seems to me, it\npossesses over all the others. The plot, though not elaborate, is almost\nregular enough for Fielding; hardly a character, hardly an incident\ncould be retrenched without loss to the story. The elopement of Lydia\nand Wickham is not, like that of Crawford and Mrs. Rushworth, a_ coup de\nthéâtre; _it connects itself in the strictest way with the course of the\nstory earlier, and brings about the denouement with complete propriety.\nAll the minor passages--the loves of Jane and Bingley, the advent of Mr.\nCollins, the visit to Hunsford, the Derbyshire tour--fit in after the\nsame unostentatious, but masterly fashion. There is no attempt at the\nhide-and-seek, in-and-out business, which in the transactions between\nFrank Churchill and Jane Fairfax contributes no doubt a good deal to the\nintrigue of_ Emma, _but contributes it in a fashion which I do not think\nthe best feature of that otherwise admirable book. Although Miss Austen\nalways liked something of the misunderstanding kind, which afforded her\nopportunities for the display of the peculiar and incomparable talent to\nbe noticed presently, she has been satisfied here with the perfectly\nnatural occasions provided by the false account of Darcy's conduct given\nby Wickham, and by the awkwardness (arising with equal naturalness) from\nthe gradual transformation of Elizabeth's own feelings from positive\naversion to actual love. I do not know whether the all-grasping hand of\nthe playwright has ever been laid upon_ Pride and Prejudice; _and I dare\nsay that, if it were, the situations would prove not startling or\ngarish enough for the footlights, the character-scheme too subtle and\ndelicate for pit and gallery. But if the attempt were made, it would\ncertainly not be hampered by any of those loosenesses of construction,\nwhich, sometimes disguised by the conveniences of which the novelist can\navail himself, appear at once on the stage._\n_I think, however, though the thought will doubtless seem heretical to\nmore than one school of critics, that construction is not the highest\nmerit, the choicest gift, of the novelist. It sets off his other gifts\nand graces most advantageously to the critical eye; and the want of it\nwill sometimes mar those graces--appreciably, though not quite\nconsciously--to eyes by no means ultra-critical. But a very badly-built\nnovel which excelled in pathetic or humorous character, or which\ndisplayed consummate command of dialogue--perhaps the rarest of all\nfaculties--would be an infinitely better thing than a faultless plot\nacted and told by puppets with pebbles in their mouths. And despite the\nability which Miss Austen has shown in working out the story, I for one\nshould put_ Pride and Prejudice _far lower if it did not contain what\nseem to me the very masterpieces of Miss Austen's humour and of her\nfaculty of character-creation--masterpieces who may indeed admit John\nThorpe, the Eltons, Mrs. Norris, and one or two others to their company,\nbut who, in one instance certainly, and perhaps in others, are still\nsuperior to them._\n_The characteristics of Miss Austen's humour are so subtle and delicate\nthat they are, perhaps, at all times easier to apprehend than to\nexpress, and at any particular time likely to be differently\napprehended by different persons. To me this humour seems to possess a\ngreater affinity, on the whole, to that of Addison than to any other of\nthe numerous species of this great British genus. The differences of\nscheme, of time, of subject, of literary convention, are, of course,\nobvious enough; the difference of sex does not, perhaps, count for much,\nfor there was a distinctly feminine element in \"Mr. Spectator,\" and in\nJane Austen's genius there was, though nothing mannish, much that was\nmasculine. But the likeness of quality consists in a great number of\ncommon subdivisions of quality--demureness, extreme minuteness of touch,\navoidance of loud tones and glaring effects. Also there is in both a\ncertain not inhuman or unamiable cruelty. It is the custom with those\nwho judge grossly to contrast the good nature of Addison with the\nsavagery of Swift, the mildness of Miss Austen with the boisterousness\nof Fielding and Smollett, even with the ferocious practical jokes that\nher immediate predecessor, Miss Burney, allowed without very much\nprotest. Yet, both in Mr. Addison and in Miss Austen there is, though a\nrestrained and well-mannered, an insatiable and ruthless delight in\nroasting and cutting up a fool. A man in the early eighteenth century,\nof course, could push this taste further than a lady in the early\nnineteenth; and no doubt Miss Austen's principles, as well as her heart,\nwould have shrunk from such things as the letter from the unfortunate\nhusband in the_ Spectator, _who describes, with all the gusto and all the\ninnocence in the world, how his wife and his friend induce him to play\nat blind-man's-buff. But another_ Spectator _letter--that of the damsel\nof fourteen who wishes to marry Mr. Shapely, and assures her selected\nMentor that \"he admires your_ Spectators _mightily\"--might have been\nwritten by a rather more ladylike and intelligent Lydia Bennet in the\ndays of Lydia's great-grandmother; while, on the other hand, some (I\nthink unreasonably) have found \"cynicism\" in touches of Miss Austen's\nown, such as her satire of Mrs. Musgrove's self-deceiving regrets over\nher son. But this word \"cynical\" is one of the most misused in the\nEnglish language, especially when, by a glaring and gratuitous\nfalsification of its original sense, it is applied, not to rough and\nsnarling invective, but to gentle and oblique satire. If cynicism means\nthe perception of \"the other side,\" the sense of \"the accepted hells\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installation command for required dependencies including OpenAI, Instructor, YouTube transcript API, and Burr.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-flashcards.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openai instructor pydantic youtube_transcript_api \"burr[start]\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Retry Logic for Validation\nDESCRIPTION: Demonstrates how to implement retry logic when validation fails, using the max_retries parameter to allow multiple attempts.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/validators/readme.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nqa: QuestionAnswerNoEvil = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswerNoEvil,\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Using Instructor with Google Gemini Models\nDESCRIPTION: Example showing how to use Instructor with Google's Gemini models. Requires the Google AI Python SDK and jsonref library. Demonstrates structured data extraction from Gemini's language models.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\n# genai.configure(api_key=os.environ[\"API_KEY\"]) # alternative API key configuration\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",  # model defaults to \"gemini-pro\"\n    ),\n    mode=instructor.Mode.GEMINI_JSON,\n)\n```\n\n----------------------------------------\n\nTITLE: Partial Streaming Implementation\nDESCRIPTION: Example demonstrating how to stream partial responses when creating a user profile.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/deepseek.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nimport os\nfrom openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\n\n\n# Initialize with API key\nclient = instructor.from_openai(\n    OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    bio: str\n\n\nuser = client.chat.completions.create_partial(\n    model=\"deepseek-chat\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Create a user profile for Jason and a one sentence bio, age 25\",\n        },\n    ],\n    response_model=User,\n)\n\nfor user_partial in user:\n    print(user_partial)\n```\n\n----------------------------------------\n\nTITLE: Rate-Limited Concurrent Processing with Semaphore and gather in Python\nDESCRIPTION: Implements rate-limited concurrent processing using asyncio.Semaphore and asyncio.gather. This approach limits the number of concurrent requests to be considerate to the server while still processing tasks concurrently.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/learn-async.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsem = asyncio.Semaphore(2)\n\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -> Person:\n    async with sem:\n        return await extract_person(text)\n\n\nasync def rate_limited_gather(sem: Semaphore):\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    resp = await asyncio.gather(*tasks_get_persons)\n```\n\n----------------------------------------\n\nTITLE: Importing Pydantic for Data Modeling in Python\nDESCRIPTION: Imports necessary components from Pydantic for creating data models with validation.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/6-chain-of-density.ipynb#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field, field_validator\n```\n\n----------------------------------------\n\nTITLE: Querying Extract Endpoint with Context - Bash Example\nDESCRIPTION: Example of how to query the /extract endpoint using curl with a JSON payload containing context and query parameters\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/citation_with_extraction/README.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H \"Content-Type: application/json\" -d '{\n  \"context\": \"My name is Jason Liu, and I grew up in Toronto Canada but I was born in China.I went to an arts highschool but in university I studied Computational Mathematics and physics.  As part of coop I worked at many companies including Stitchfix, Facebook.  I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\",\n  \"query\": \"What did the author do in school?\"\n}' -N http://localhost:8000/extract\n```\n\n----------------------------------------\n\nTITLE: Installing Pydantic Dependency\nDESCRIPTION: Installation command for Pydantic, which is required for defining data models in Instructor.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install pydantic\n```\n\n----------------------------------------\n\nTITLE: Getting the model response with original completion object\nDESCRIPTION: Uses create_with_completion to get both the parsed response model and the original completion object, which can be useful for accessing raw response data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/version-1.md#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Gemini and Instructor\nDESCRIPTION: This command installs the necessary Python packages to use Instructor with Google's Generative AI.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/chat-with-your-pdf-with-gemini.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[google-generativeai]\"\n```\n\n----------------------------------------\n\nTITLE: Extracting YouTube Transcript Using youtube-transcript-api in Python\nDESCRIPTION: This Python function uses the youtube-transcript-api to extract the transcript of a YouTube video given its video ID. It returns the transcript as a formatted string with timestamps.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/youtube-transcripts.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\n\ndef get_youtube_transcript(video_id: str) -> str:\n    try:\n        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n        return \" \".join(\n            [f\"ts={entry['start']} - {entry['text']}\" for entry in transcript]\n        )\n    except Exception as e:\n        print(f\"Error fetching transcript: {e}\")\n        return \"\"\n```\n\n----------------------------------------\n\nTITLE: Installing LiteLLM Integration\nDESCRIPTION: Installation command for Instructor with LiteLLM support for multiple providers.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npip install \"instructor[litellm]\"\n```\n\n----------------------------------------\n\nTITLE: Installing UV Package Manager\nDESCRIPTION: Commands for installing UV, a fast Python package installer and resolver, on different operating systems.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/contributing.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows PowerShell\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n----------------------------------------\n\nTITLE: Defining Receipt Data Models with Pydantic\nDESCRIPTION: Defines two Pydantic models - Item for individual receipt items and Receipt for the complete receipt structure. These models provide type validation and structure for the extracted data.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/extracting_receipts.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    name: str\n    price: float\n    quantity: int\n\n\nclass Receipt(BaseModel):\n    items: list[Item]\n    total: float\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor AI for Groq Integration\nDESCRIPTION: This snippet shows how to install Instructor AI with Groq support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[groq]\"\n```\n\n----------------------------------------\n\nTITLE: Running Instructor Hooks Example\nDESCRIPTION: Commands to navigate to the hooks example directory and execute the demonstration script. This will show hook interactions including request details, token counts, and various event handlers in action.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/hooks/README.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Navigate to the hooks example directory\ncd examples/hooks\n\n# Run the example\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Setting Mistral API Key\nDESCRIPTION: Environment variable setup for Mistral API key configuration.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nexport MISTRAL_API_KEY=your_mistral_key\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules for LLM Validation\nDESCRIPTION: Sets up the necessary imports for working with Pydantic models and OpenAI integration, including the llm_validator and required patches.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/validators/readme.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import Annotated\nfrom pydantic import (\n    BaseModel,\n    BeforeValidator,\n)\nfrom instructor import llm_validator, patch\nimport openai\n\npatch()\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor with VertexAI Support\nDESCRIPTION: Command to install the instructor library with VertexAI support for integration with the VertexAI SDK.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/announcing-gemini-tool-calling-support.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[vertexai]\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Validation Model\nDESCRIPTION: Defines a new model with custom validation using llm_validator to filter objectionable content from responses.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/validators/readme.md#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", allow_override=True)\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Setting Cerebras API Key Environment Variable\nDESCRIPTION: Command to set the Cerebras API key as an environment variable for use with the SDK. This is required before using any Cerebras Inference functionality.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/introducing-structured-outputs-with-cerebras-inference.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport CEREBRAS_API_KEY=<your-api-key>\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Refresh for Page Redirection\nDESCRIPTION: This HTML code snippet implements a meta refresh tag to automatically redirect visitors to the consolidated Union Types guide page after 0 seconds.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/concepts/union.md#2025-04-14_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<meta http-equiv=\"refresh\" content=\"0; url=./unions.md\">\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing Batch Jobs using Instructor CLI in Bash\nDESCRIPTION: This snippet demonstrates how to use the 'instructor' CLI to create a batch job from a .jsonl file, check its status, and download the results. It also shows how to cancel a batch job if needed.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/batch_job_oai.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n> % ls -a | grep test.jsonl\ntest.jsonl\n\n> % instructor batch create-from-file --file-path test.jsonl\n\ninstructor batch cancel --batch-id <batch id here>\n\ninstructor batch download-file --download-file-path output.jsonl --batch-id batch_Z8XUudoweH43R9c4sr4wRYub\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for Instructor Library\nDESCRIPTION: A BibTeX citation entry for referencing the Instructor library in academic research or projects\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_24\n\nLANGUAGE: bibtex\nCODE:\n```\n@software{liu2024instructor,\n  author = {Jason Liu and Contributors},\n  title = {Instructor: A library for structured outputs from large language models},\n  url = {https://github.com/instructor-ai/instructor},\n  year = {2024},\n  month = {3}\n}\n```\n\n----------------------------------------\n\nTITLE: Server Response Format - Shell Example\nDESCRIPTION: Example of the SSE response format showing extracted facts with citations\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/citation_with_extraction/README.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndata: {'body': 'In school, the author went to an arts high school.', 'spans': [(91, 106)], 'citation': ['arts highschool']}\ndata: {'body': 'In university, the author studied Computational Mathematics and physics.', 'spans': [(135, 172)], 'citation': ['Computational Mathematics and physics']}\n```\n\n----------------------------------------\n\nTITLE: Optional Dependencies Configuration in TOML\nDESCRIPTION: Configuration example for adding optional dependencies in pyproject.toml file for provider support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[project.optional-dependencies]\n# Add your provider here\nmy-provider = [\"my-provider-sdk>=1.0.0,<2.0.0\"]\n\n[dependency-groups]\n# Also add to dependency groups\nmy-provider = [\"my-provider-sdk>=1.0.0,<2.0.0\"]\n```\n\n----------------------------------------\n\nTITLE: Conditional Entropy Calculation in LaTeX\nDESCRIPTION: Formula for calculating the conditional entropy (H_conditional) as the average entropy across all distributions in the dataset.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/prompting/ensembling/max_mutual_information.md#2025-04-14_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\nH_{conditional} = \\frac{1}{n} \\sum_{i=0}^n H(P(Y_i|X_i))\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies - Bash Command\nDESCRIPTION: Command to install required Python packages from requirements.txt\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/citation_with_extraction/README.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Installation of necessary Python packages using uv package manager\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/tutorials/7-synthetic-data-generation.ipynb#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!uv pip install instructor openai datasets lancedb tantivy tenacity tqdm\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor CLI Package\nDESCRIPTION: Command to install the Instructor package using pip package manager.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/cli/index.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install instructor\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor via pip\nDESCRIPTION: Command to install or upgrade the Instructor library using pip.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/README.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U instructor\n```\n\n----------------------------------------\n\nTITLE: Embedding Newsletter Subscription Form with Beehiiv\nDESCRIPTION: An iframe element that embeds a Beehiiv newsletter subscription form with specific styling and dimensions.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/newsletter.md#2025-04-14_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe src=\"https://embeds.beehiiv.com/2faf420d-8480-4b6e-8d6f-9c5a105f917a?slim=true\" data-test-id=\"beehiiv-embed\" height=\"52\" width=\"80%\" frameborder=\"0\" scrolling=\"no\" style=\"margin: 0; border-radius: 0px !important; background-color: transparent;\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Installing Cohere Integration\nDESCRIPTION: Installation command for Instructor with Cohere support.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/learning/getting_started/installation.md#2025-04-14_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npip install \"instructor[cohere]\"\n```\n\n----------------------------------------\n\nTITLE: Launching FastAPI Server with Uvicorn\nDESCRIPTION: Command to start the FastAPI server in development mode with auto-reload enabled. The server provides API documentation at http://127.0.0.1:8000/docs.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/logfire-fastapi/Readme.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn server:app --reload\n```\n\n----------------------------------------\n\nTITLE: Starting FastAPI Server - Bash Command\nDESCRIPTION: Command to start the FastAPI server with hot reload enabled\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/examples/citation_with_extraction/README.md#2025-04-14_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn main:app --reload\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-cpp-python with CPU Support\nDESCRIPTION: Alternative installation command for CPU-only systems using OpenBLAS.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/examples/local_classification.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nCMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n```\n\n----------------------------------------\n\nTITLE: Highlighting Citations in the PDF for Verification\nDESCRIPTION: Shows how to visually highlight the cited text in the original PDF. This code searches for each citation text in the appropriate page and adds highlight annotations for easy verification.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generating-pdf-citations.md#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor citation in resp.citations:\n    page = doc.load_page(citation.page_number - 1)\n    for text in citation.text:\n        text_instances = page.search_for(text)\n        for instance in text_instances:\n            page.add_highlight_annot(instance)\n\ndoc.save(\"./highlighted.pdf\")\ndoc.close()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for PDF Processing with Gemini\nDESCRIPTION: Command to install the instructor library with Google Generative AI support and PyMuPDF for PDF parsing.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/blog/posts/generating-pdf-citations.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"instructor[google-generativeai]\" pymupdf\n```\n\n----------------------------------------\n\nTITLE: Type Inference with Instructor Client - Basic Usage\nDESCRIPTION: Shows how to use Instructor's `create` method with a Pydantic model to generate a structured User object, demonstrating type inference capabilities\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/index.md#2025-04-14_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n```\n\n----------------------------------------\n\nTITLE: Repository Setup Commands in Bash\nDESCRIPTION: Commands for cloning the repository and setting up the remote upstream connection.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/CONTRIBUTING.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/YOUR-USERNAME/instructor.git\ncd instructor\ngit remote add upstream https://github.com/instructor-ai/instructor.git\n```\n\n----------------------------------------\n\nTITLE: Using Patched Client with Pydantic Model\nDESCRIPTION: Example of using the Instructor-patched client to create chat completions with a custom Pydantic response model.\nSOURCE: https://github.com/instructor-ai/instructor/blob/main/docs/integrations/index.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    response_model=YourModel,\n    messages=[{\"role\": \"user\", \"content\": \"Your prompt\"}]\n)\n```"
  }
]
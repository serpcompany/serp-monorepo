[
  {
    "owner": "adbar",
    "repo": "trafilatura",
    "content": "TITLE: Fetching and Extracting Web Content with Trafilatura in Python\nDESCRIPTION: Demonstrates how to use Trafilatura's Python API for downloading and extracting main text and comments from a given web page. Requires the \"trafilatura\" Python package (install with \"pip install trafilatura\"). The snippet imports the library, fetches a web page by URL, and then extracts its main content. Input is a target web URL; output is the extracted plain text. Dependencies: Python >=3.6 and the Trafilatura package.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/index.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport trafilatura\ndownloaded = trafilatura.fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\ntrafilatura.extract(downloaded)\n# outputs main content and comments as plain text ...\n```\n\n----------------------------------------\n\nTITLE: Extracting Web Page Content using Trafilatura CLI in Bash\nDESCRIPTION: Illustrates how to use Trafilatura from the command line to download and extract main text and comments from a URL. Requires the Trafilatura CLI to be installed (via \"pip install trafilatura\"). The \"-u\" option specifies the target URL to process; output is sent to standard output as plain text. No additional configuration is necessary for basic usage.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/index.rst#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ntrafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n# outputs main content and comments as plain text ...\n```\n\n----------------------------------------\n\nTITLE: Extracting Main Text Content Using Trafilatura in Python\nDESCRIPTION: Demonstrates how to fetch an HTML page from a URL and extract its main textual content using Trafilatura's fetch_url and extract functions. Requires the 'trafilatura' package installed. The fetch_url function downloads the HTML, and extract processes it to obtain readable text which is printed to the console. The code assumes internet access and valid URLs. This snippet is the foundational example showing basic usage without modifications.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/quickstart.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# import the necessary functions\n>>> from trafilatura import fetch_url, extract\n\n# grab a HTML file to extract data from\n>>> downloaded = fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n\n# output main content and comments as plain text\n>>> result = extract(downloaded)\n>>> print(result)\n```\n\n----------------------------------------\n\nTITLE: Extracting Metadata from Web Pages Using Trafilatura in Python\nDESCRIPTION: Demonstrates fetching a web page and extracting metadata such as title, author, and publication date using extract_metadata function. This snippet requires fetching the URL content in advance. The output provides structured metadata information, assisting in detailed document analysis and indexing.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/quickstart.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from trafilatura import fetch_url, extract_metadata\n>>> downloaded = fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n>>> extract_metadata(downloaded)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content Using Trafilatura's extract() in Python\nDESCRIPTION: This snippet illustrates extracting readable text from a downloaded web document using the `extract` function. It accepts a raw HTML string (`document`) and returns the extracted plain text as output by default, or XML if specified via the `output_format` parameter. This requires the Trafilatura package and a valid HTML input. The example also demonstrates printing the extracted content.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura import extract\n\ntext = extract(document)\nprint(text)\n```\n\nLANGUAGE: python\nCODE:\n```\nxml_text = extract(document, output_format='xml')\nprint(xml_text)\n```\n\n----------------------------------------\n\nTITLE: Fetching URL and Extracting Content with Metadata Using fetch_url() and bare_extraction()\nDESCRIPTION: This code illustrates fetching a webpage via its URL and performing metadata extraction, highlighting the utility of passing URLs directly for date extraction and metadata retrieval. The fetch_url() function downloads the content, and bare_extraction() produces Python object outputs such as dictionaries and lxml elements.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# define a URL and download the example\n>>> url = \"https://web.archive.org/web/20210613232513/https://www.thecanary.co/feature/2021/05/19/another-by-election-headache-is-incoming-for-keir-starmer/\"\n>>> downloaded = fetch_url(url)\n\n# content discarded since necessary metadata couldn't be extracted\n>>> bare_extraction(downloaded, only_with_metadata=True)\n\n# date found in URL, extraction successful\n>>> bare_extraction(downloaded, only_with_metadata=True, url=url)\n```\n\n----------------------------------------\n\nTITLE: Extracting Main Content Using Trafilatura Command-Line Interface\nDESCRIPTION: Demonstrates using the Trafilatura command-line tool to extract main textual content including comments from a given URL with the -u or --URL option. This example assumes the trafilatura CLI is installed and available in the system PATH. It enables quick web content extraction directly from shell environment without Python scripting.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/quickstart.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# outputs main content and comments as plain text\n$ trafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n```\n\n----------------------------------------\n\nTITLE: Working with Raw Python Objects Using Trafilatura's bare_extraction()\nDESCRIPTION: This snippet uses `bare_extraction` to obtain a dictionary representation of a document's structure including metadata. It allows programmatic access to components like 'sitename'. Requires Trafilatura and a raw HTML input string. Displaying keys or specific values aids in further customized processing or filtering of extracted data.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura import bare_extraction\n\ndoc_dict = bare_extraction(document)\nprint(doc_dict.keys())\nprint(doc_dict['sitename'])\n```\n\n----------------------------------------\n\nTITLE: Customizing HTML Element Extraction\nDESCRIPTION: Examples showing how to include or exclude specific HTML elements during extraction.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# exclude comments from the output\n>>> result = extract(downloaded, include_comments=False)\n\n# skip tables and include links in the output\n>>> result = extract(downloaded, include_tables=False, include_links=True)\n\n# convert relative links to absolute links where possible\n>>> extract(downloaded, output_format='xml', include_links=True, url=url)\n```\n\n----------------------------------------\n\nTITLE: Loading a Custom Settings File for Trafilatura Extraction (Python)\nDESCRIPTION: This Python code showcases how to load and use a custom settings file for Trafilatura extraction, providing flexibility to override all default parameters at once. It demonstrates the use of 'use_config' from 'trafilatura.settings' to read a configuration file, then applies these settings by either passing them as the 'config' parameter or specifying the settings file path with 'settingsfile'. Inputs include a path to a configuration file and the already downloaded content to extract. Dependencies: 'trafilatura' package (importing 'extract' and 'use_config'). Limitations: the configuration file must cover all required settings; using 'settingsfile' may be less performant.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/settings.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# load the required functions\n>>> from trafilatura import extract\n>>> from trafilatura.settings import use_config\n\n# load the new settings by providing a file name\n>>> newconfig = use_config(\"myfile.cfg\")\n\n# use with a previously downloaded document\n>>> extract(downloaded, config=newconfig)\n\n# provide a file name directly (can be slower)\n>>> extract(downloaded, settingsfile=\"myfile.cfg\")\n```\n\n----------------------------------------\n\nTITLE: Processing URLs with Trafilatura CLI (Bash)\nDESCRIPTION: Demonstrates how to extract content directly from a single URL using the `trafilatura` command-line tool. It shows the default behavior (outputting main content and comments as plain text), how to request XML output using the `--xml` option, and how to display the command's help message. This requires `trafilatura` to be installed and accessible in the system's PATH. Input is a URL string; output is the extracted text or data printed to standard output.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# outputs main content and comments as plain text ...\n$ trafilatura -u \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# outputs main text with basic XML structure ...\n$ trafilatura --xml --URL \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# displays help message\n$ trafilatura -h\n```\n\n----------------------------------------\n\nTITLE: Processing URL List File with Trafilatura CLI (Bash)\nDESCRIPTION: Shows how to process multiple URLs listed in a text file, one URL per line, using the `-i` or `--input-file` option. The extracted results are saved to a specified output directory using the `-o` or `--output-dir` option. Examples demonstrate outputting as raw text and in XML format. Requires an input file (`list.txt`) containing URLs and a writable output directory; output files are saved in the specified directory.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ trafilatura -i list.txt -o txtfiles/\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ trafilatura --xml -i list.txt -o xmlfiles/\n```\n\n----------------------------------------\n\nTITLE: JSON Output with Metadata\nDESCRIPTION: Example demonstrating how to extract content with metadata in JSON format.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# output in JSON format with metadata extracted\n>>> extract(downloaded, output_format=\"json\", with_metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Combining Extraction Options in Trafilatura CLI with File Input\nDESCRIPTION: Shows usage of Trafilatura CLI with file input and option flags to customize extraction behavior. The example demonstrates outputting JSON format and skipping tables during extraction. This provides flexible extraction settings from the command-line by combining multiple flags. Requires a local HTML file as input and functional trafilatura CLI installation.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/quickstart.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ < myfile.html trafilatura --json --no-tables\n```\n\n----------------------------------------\n\nTITLE: Checking if Text Can Be Extracted\nDESCRIPTION: Example of using is_probably_readerable() function to guess if a page has a main text to extract.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from trafilatura.readability_lxml import is_probably_readerable\n>>> is_probably_readerable(html)  # HTML string or already parsed tree\n```\n\n----------------------------------------\n\nTITLE: Processing HTML from Stdin with Trafilatura CLI (Bash)\nDESCRIPTION: Illustrates how to provide HTML content to the `trafilatura` command through standard input (stdin), rather than fetching it from a URL. This method is useful for processing local HTML files or piping the output of other commands like `cat`, `wget`, or `curl` directly into `trafilatura`. Requires HTML content to be available on stdin; output is the extracted text or data printed to standard output.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# use the contents of an already existing file\n$ cat myfile.html | trafilatura\n```\n\nLANGUAGE: bash\nCODE:\n```\n# alternative syntax\n$ < myfile.html trafilatura\n```\n\nLANGUAGE: bash\nCODE:\n```\n# use a custom download utility and pipe it to trafilatura\n$ wget -qO- \"https://de.creativecommons.org/index.php/was-ist-cc/\" | trafilatura\n```\n\n----------------------------------------\n\nTITLE: Customizing Output Formats in Trafilatura Extraction Using Python\nDESCRIPTION: Illustrates how to alter the format of extracted content using the extract function's output_format parameter. Supported formats demonstrated include XML for structured document preservation, JSON for metadata extraction with comment exclusion, and Markdown with metadata inclusion. These options allow tailoring of the extraction output to specific use cases. Depends on prior downloading of the HTML content. Optional boolean flags control comment inclusion and metadata extraction.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/quickstart.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# change the output format to XML (allowing for preservation of document structure)\n>>> result = extract(downloaded, output_format=\"xml\")\n\n# discard potential comments, extract metadata and change the output to JSON\n>>> extract(downloaded, output_format=\"json\", include_comments=False)\n\n# set the output to Markdown and extract metadata\n>>> extract(downloaded, output_format=\"markdown\", with_metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Searching Sitemaps for URLs with Trafilatura in Python\nDESCRIPTION: This snippet performs a sitemap search on a given URL, filtering results by target language (e.g., German). It uses `sitemap_search` from Trafilatura's sitemaps module. The function returns a list of discovered links from the sitemap, which can be used for crawling or content collection. Requires Trafilatura package.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura.sitemaps import sitemap_search\n\nlinks = sitemap_search('https://www.sitemaps.org', target_lang='de')\n# 5 first links found in the sitemap\nprint('\\n'.join(links[:5]))\n```\n\n----------------------------------------\n\nTITLE: Extracting URLs from XML Sitemaps with sitemap_search() Function in Python\nDESCRIPTION: This code utilizes the sitemaps module to locate all URLs listed in a website's XML sitemap, optionally filtering by target language. The process involves providing the homepage URL, with the function returning a list of filtered links.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n# load sitemaps module\n>>> from trafilatura import sitemaps\n\n# automatically find sitemaps via homepage\n>>> mylinks = sitemaps.sitemap_search('https://www.theguardian.com/')\n\n# filter links by target language (e.g., English)\n>>> mylinks = sitemaps.sitemap_search('https://www.un.org/', target_lang='en')\n```\n\n----------------------------------------\n\nTITLE: Converting BeautifulSoup Object to lxml Tree for Content Extraction in Python\nDESCRIPTION: This code converts a BeautifulSoup object into an lxml HTML tree using convert_tree(), enabling its use with Trafilatura for content extraction. It illustrates interoperability between BeautifulSoup and Trafilatura.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n# from bs4 import BeautifulSoup\n>>> from lxml.html.soupparser import convert_tree\n>>> from trafilatura import extract\n\n>>> soup = BeautifulSoup(\"<html><body><time>The date is Feb 2, 2024</time></body></html>\", \"lxml\")\n>>> lxml_tree = convert_tree(soup)[0]\n>>> extract(lxml_tree)\n```\n\n----------------------------------------\n\nTITLE: Comparing Baseline and Full Text Extraction in Trafilatura with Python\nDESCRIPTION: This snippet compares two extraction approaches: a baseline extraction returning an LXML object, extracted text, and length, and the full extraction returning only the text. This helps users evaluate different levels of content extraction detail. The functions require a raw HTML string input and the Trafilatura package.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura import baseline\n\nlxml_object, text, length = baseline(document)\nprint(text)\n```\n\nLANGUAGE: python\nCODE:\n```\ntext = extract(document)\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Throttled Downloads with Trafilatura Python\nDESCRIPTION: Illustrates how to perform efficient multi-threaded downloads from different domains using Trafilatura's built-in management. This approach utilizes helper functions like `add_to_compressed_dict`, `load_download_buffer`, and `buffered_downloads` to manage URLs, apply domain-aware throttling, and process downloads concurrently with a specified number of threads.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/downloads.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura.downloads import add_to_compressed_dict, buffered_downloads, load_download_buffer\n\n# list of URLs\nmylist = ['https://www.example.org', 'https://www.httpbin.org/html']\n# number of threads to use\nthreads = 4\n\n# converted the input list to an internal format\nurl_store = add_to_compressed_dict(mylist)\n# processing loop\nwhile url_store.done is False:\n    bufferlist, url_store = load_download_buffer(url_store, sleep_time=5)\n    # process downloads\n    for url, result in buffered_downloads(bufferlist, threads):\n        # do something here\n        print(url)\n        print(result)\n```\n\n----------------------------------------\n\nTITLE: Formatting Preserved in XML Output\nDESCRIPTION: Example showing how to extract content while preserving formatting in XML structure.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# some formatting preserved in basic XML structure\n>>> extract(downloaded, output_format=\"xml\")\n```\n\n----------------------------------------\n\nTITLE: Performing iterative crawling with focused_crawler() in Python\nDESCRIPTION: Illustrates executing subsequent crawl iterations by passing previous crawl states through 'todo' and 'known_links'. Adjusts parameters like max_seen_urls and max_known_urls to control crawl scope. This setup allows for stepwise crawling and data collection over time.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/crawls.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n>>> to_visit, known_links = focused_crawler(\"https://example.org\", max_seen_urls=10, max_known_urls=100000, todo=to_visit, known_links=known_links)\n```\n\n----------------------------------------\n\nTITLE: Focused Web Crawling with Trafilatura's focused_crawler in Python\nDESCRIPTION: This snippet uses Trafilatura's focused crawler to explore a website, returning sets of URLs to visit and known URLs seen during the crawl. It limits the crawl to a maximum number of URLs to avoid overloading. The example outputs counts and samples of URLs found. Requires Trafilatura and network access. Useful for exploratory crawling to gather relevant content efficiently.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura.spider import focused_crawler\n\nto_visit, known_urls = focused_crawler('https://www.telebasel.ch/', max_seen_urls=3)\nprint(len(to_visit), len(known_urls))\n# has to be converted to a list in order to select such slices\nprint('\\n'.join(list(to_visit)[:5]))\nprint('---')\nprint('\\n'.join(list(known_urls)[:5]))\n```\n\n----------------------------------------\n\nTITLE: Extracting Metadata with Custom Date Parameters Using extract() Function in Python\nDESCRIPTION: This snippet demonstrates how to extract content and metadata from a downloaded document with custom date handling parameters, including extensive search, max date cutoff, and custom output format. Dependencies include the 'trafilatura' library and a previously downloaded document object.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n# import the extract() function, use a previously downloaded document\n>>> extract(downloaded, output_format=\"xml\", date_extraction_params={\n        \"extensive_search\": True, \"max_date\": \"2018-07-01\"\n    })\n```\n\n----------------------------------------\n\nTITLE: Setting Target Language for Extraction\nDESCRIPTION: Example showing how to set a target language using 2-letter ISO codes to filter extraction results.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> result = extract(downloaded, url, target_language=\"de\")\n```\n\n----------------------------------------\n\nTITLE: Checking Duplicate Text Content Using LRU Cache in Python\nDESCRIPTION: This snippet demonstrates how to use the duplicate_test() function from Trafilatura's deduplication module to identify duplicate text segments within an LXML element. It requires the lxml.etree module and Trafilatura's Extractor settings object to configure minimum text length and maximum allowed repetitions. The test returns False if the element is new and True if it repeats more than allowed. This helps filter boilerplate or redundant text during extraction.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/deduplication.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from lxml.etree import fromstring\n>>> from trafilatura.deduplication import duplicate_test\n>>> from trafilatura.settings import Extractor\n\n>>> options = Extractor()\n>>> options.min_duplcheck_size = 0  # even short segments are considered\n>>> options.max_repetitions = 0  # no repetition allowed\n\n>>> elem = fromstring(\"<p>Here is text.</p>\")\n>>> duplicate_test(elem, options)\nFalse\n>>> duplicate_test(elem, options)\nTrue\n```\n\n----------------------------------------\n\nTITLE: Processing Lists of Links with Trafilatura on the Command Line\nDESCRIPTION: These examples showcase the use of Trafilatura's CLI for batch downloading and processing URLs into text or XML formats. The '-i' or '--input-file' option specifies the list of URLs, while '-o' or '--output-dir' designates the output folder (which must be writable and is created if absent). The optional '--backup-dir' flag archives downloaded HTML files. Requires a valid installation of Trafilatura and appropriate file system permissions.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial0.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrafilatura -i list.txt -o txtfiles/\n```\n\nLANGUAGE: bash\nCODE:\n```\ntrafilatura --xml -i list.txt -o xmlfiles/\n```\n\nLANGUAGE: bash\nCODE:\n```\ntrafilatura --xml -i list.txt -o xmlfiles/ --backup-dir htmlfiles/\n```\n\n----------------------------------------\n\nTITLE: Discovering Web Feeds URLs using Trafilatura in Python\nDESCRIPTION: This example finds web feed URLs (RSS, Atom) on a given site using the `find_feed_urls` function from Trafilatura's feeds module. Input is the homepage URL as a string, and output is a list of feed URLs. The snippet prints the first five feed links. Useful for feed aggregation or monitoring news updates. Requires Trafilatura package.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura.feeds import find_feed_urls\n\nlinks = find_feed_urls('https://www.nzz.ch')\n# 5 first links in the feed\nprint('\\n'.join(links[:5]))\n```\n\n----------------------------------------\n\nTITLE: Using focused_crawler() function in Python to start a web crawl\nDESCRIPTION: Demonstrates how to import and invoke the focused_crawler() function from the trafilatura.spider module to initiate a crawl starting from a specified URL. It shows setting parameters like max_seen_urls and how the function returns the current state of URLs to visit and known links, facilitating iterative crawling tasks.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/crawls.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> from trafilatura.spider import focused_crawler\n\n# perform the first iteration (will not work with this website, there are no internal links)\n>>> to_visit, known_links = focused_crawler(\"https://example.org\", max_seen_urls=1)\n```\n\n----------------------------------------\n\nTITLE: Discovering Links from Feeds with Trafilatura CLI (Bash)\nDESCRIPTION: Demonstrates how to use `trafilatura` to discover links from web feeds (RSS/Atom) using the `--feed` option. It can automatically detect feeds from a homepage URL or use a direct feed URL. The `--list` option displays the discovered URLs to standard output instead of performing extraction. The last example shows processing a list of *source* URLs (homepages or feeds) from a file in parallel for discovery. Requires feed URLs or homepages with discoverable feeds; output is a list of discovered URLs.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# automatically detecting feeds starting from the homepage\n$ trafilatura --feed \"https://www.dwds.de/\" --list\n```\n\nLANGUAGE: bash\nCODE:\n```\n# already known feed\n$ trafilatura --feed \"https://www.dwds.de/api/feed/themenglossar/Corona\" --list\n```\n\nLANGUAGE: bash\nCODE:\n```\n# processing a list in parallel\n$ trafilatura -i mylist.txt --feed --list\n```\n\n----------------------------------------\n\nTITLE: Processing URL List with Backup using Trafilatura CLI (Bash)\nDESCRIPTION: Extends processing a list of URLs from an input file by adding the `--backup-dir` option to save the original HTML source files. The extracted content (in XML format in this example) is saved to the specified output directory. Requires an input file (`links.txt`) and writable output (`converted/`) and backup (`html-sources/`) directories. Output includes saved original HTML files and extracted XML files.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ trafilatura --input-file links.txt --output-dir converted/ --backup-dir html-sources/ --xml\n```\n\n----------------------------------------\n\nTITLE: Running Trafilatura for Sitemap Filtering via Command-Line (Bash)\nDESCRIPTION: Executes the Trafilatura tool with various command-line arguments to process a website's sitemap, list candidate links, and filter URLs via the --url-filter option. The first command restricts results to links with a given subpath; the second command uses a protocol-based filter. Requires Trafilatura to be installed and accessible in PATH. The --sitemap argument specifies the target sitemap, while --list outputs just the URLs matching the filter. Inputs are a sitemap URL and a filter string or pattern; outputs are filtered URL lists displayed in the terminal. Ensure proper quoting of URLs and filters to avoid shell parsing errors.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntrafilatura --sitemap \"https://www.sitemaps.org/\" --list --url-filter \"https://www.sitemaps.org/de\"\n```\n\nLANGUAGE: bash\nCODE:\n```\ntrafilatura --sitemap \"https://www.sitemaps.org/\" --list --url-filter \"protocol\"\n```\n\n----------------------------------------\n\nTITLE: Cleaning URLs with clean_url (Python)\nDESCRIPTION: Illustrates the usage of the `clean_url` function from `courlan` to normalize URL formats. This function removes unnecessary characters like default ports or converts scheme/hostnames to lowercase, returning a standardized URL string. This helps in preventing inconsistencies and identifying duplicate URLs.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/url-management.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from courlan import clean_url\n\n>>> clean_url('HTTPS://WWW.DWDS.DE:80/')\n'https://www.dwds.de'\n```\n\n----------------------------------------\n\nTITLE: Selecting Output Format for Trafilatura CLI Extraction (Bash)\nDESCRIPTION: Shows the available arguments in Trafilatura's CLI for controlling output format, including shorthands and explicit --output-format. Allows users to generate results in various serializations: csv, json, html, markdown, txt, xml, or xmltei. The --validate-tei option can be used to check for XML TEI schema conformance. Inputs are format flags; output depends on subsequent extraction task. Requires Trafilatura to be installed.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n--output-format {csv,json,html,markdown,txt,xml,xmltei}\n                determine output format\n--csv                 shorthand for CSV output\n--html                shorthand for HTML output\n--json                shorthand for JSON output\n--markdown            shorthand for MD output\n--xml                 shorthand for XML output\n--xmltei              shorthand for XML TEI output\n--validate-tei        validate XML TEI output\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from an lxml.html Tree Object in Python\nDESCRIPTION: This snippet demonstrates how to parse an HTML string into an lxml tree and extract the main textual content using Trafilatura's extract() function. It requires lxml and the relevant HTML content as input.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# define document and load it with LXML\n>>> from lxml import html\n>>> my_doc = \"\"\"<html><body><article><p>\n                Here is the main text.\n                </p></article></body></html>\"\"\"\n>>> mytree = html.fromstring(my_doc)\n\n# extract from the already loaded LXML tree\n>>> extract(mytree)\n'Here is the main text.'\n```\n\n----------------------------------------\n\nTITLE: Large Multimodal Corpus Construction with Python\nDESCRIPTION: This Python code snippet details the creation of a massive multimodal dataset (images interleaved with text) for training large AI models. It manages data collection from multiple sources, synchronization of images with textual descriptions, and data storage. Dependencies include data handling libraries like pandas and image processing modules.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/used-by.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n<python code snippet here>\n```\n\n----------------------------------------\n\nTITLE: Using Baseline Function for Better Performance\nDESCRIPTION: Example of using the baseline function which provides a better balance between precision and recall while improving performance.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from trafilatura import baseline\n>>> postbody, text, len_text = baseline(downloaded)\n```\n\n----------------------------------------\n\nTITLE: Extracting and Managing Web Crawl Rules Using Trafilatura Utilities in Python\nDESCRIPTION: This snippet shows how to use the Trafilatura-provided 'courlan' module to extract the domain from a base URL and store associated crawling rules in a dictionary. It demonstrates retrieving the crawl delay for a specific domain using the 'get_crawl_delay' function. The snippet requires the Trafilatura framework environment, including the 'courlan' module and auxiliary functions like 'get_crawl_delay'. Key variables include 'base_url' (the webpage URL to crawl), 'rules_dict' (a dictionary keyed by domain containing crawl rules), and 'seconds' (the crawl delay in seconds). The output helps manage polite web crawling by respecting domain-specific rules like crawl delays. Limitations include dependency on the correct structure of rules in 'rules_dict' and that 'base_url' must be defined prior to usage.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/downloads.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# this module comes with trafilatura\nfrom courlan import extract_domain\n\nrules_dict = dict()\n# storing information\ndomain = extract_domain(base_url)\nrules_dict[domain] = rules\n# retrieving rules info\nseconds = get_crawl_delay(rules_dict[domain])\n```\n\n----------------------------------------\n\nTITLE: Configuring Domain-Specific Sleep Delay for Downloads with Trafilatura Python\nDESCRIPTION: Shows how to introduce a delay between consecutive download requests to the same domain using the `sleep_time` parameter in the `load_download_buffer()` function. Setting a `sleep_time` helps prevent overwhelming web servers, adhere to politeness standards, and reduce the risk of being blocked. A value of 30 seconds is suggested as a safe default.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/downloads.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura.downloads import load_download_buffer\n\n# 30 seconds is a safe choice\nmybuffer, threads, domain_dict, backoff_dict = load_download_buffer(url_store, sleep_time=30)\n# then proceed as instructed above...\n```\n\n----------------------------------------\n\nTITLE: Mathematical Web Text Dataset Construction in Python\nDESCRIPTION: This code creates an open dataset of high-quality mathematical texts from web sources for NLP research. It involves crawling mathematical content, extracting equations and explanations, and formatting for downstream tasks. Relies on web scraping libraries and mathematical expression parsers.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/used-by.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n<python code snippet here>\n```\n\n----------------------------------------\n\nTITLE: Adjusting Precision in Text Extraction\nDESCRIPTION: Example showing how to favor precision over recall during extraction to focus on the most relevant content.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> result = extract(downloaded, url, favor_precision=True)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Crawl Delay from Robots.txt with Trafilatura Python\nDESCRIPTION: Illustrates how to use Trafilatura's `get_crawl_delay()` function to retrieve the `Crawl-Delay` value specified in a parsed `robots.txt` file. This function helps determine the recommended time to wait between requests for a specific domain. It also allows specifying a default value to use if no `Crawl-Delay` rule is found.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/downloads.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# get the desired information\nseconds = get_crawl_delay(rules)\n# provide a backup value in case no rule exists (happens quite often)\nseconds = get_crawl_delay(rules, default=30)\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with courlan CLI (Bash)\nDESCRIPTION: Provides examples of using the `courlan` command-line utility to filter and normalize URLs from a text file. It demonstrates basic filtering, strict filtering using the `--strict` flag, and filtering by language using the `--language` flag. The `--inputfile` and `--outputfile` options specify the source and destination files.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/url-management.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ courlan --inputfile url-list.txt --outputfile cleaned-urls.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ courlan --language de --strict --inputfile mylist.txt --outputfile mylist-filtered.txt\n```\n\n----------------------------------------\n\nTITLE: Validating URLs with validate_url (Python)\nDESCRIPTION: Shows how to use the `validate_url` function to check if a URL string conforms to a valid format. It returns a tuple where the first element is a boolean indicating validity and the second element is a `ParseResult` object from `urllib.parse` if valid, otherwise `None`. This is useful for verifying URL structure before further processing.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/url-management.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from courlan import validate_url\n\n>>> validate_url('http://1234')\n(False, None)\n>>> validate_url('http://www.example.org/')\n(True, ParseResult(scheme='http', netloc='www.example.org', path='/', params='', query='', fragment=''))\n```\n\n----------------------------------------\n\nTITLE: Filtering URL Lists with Grep on the Command Line\nDESCRIPTION: This set of examples uses 'grep', a text-search command-line utility, to manually filter a URL list based on inclusion or exclusion patterns. The first command retains only URLs containing '/article/'. The second excludes URLs with '/video/'. Both assume 'grep' is installed and the source URL list is line-based. Outputs are written to filtered files for downstream usage.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial0.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngrep \"/article/\" mylist.txt > filtered-list.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\ngrep -v \"/video/\" mylist.txt > filtered-list.txt\n```\n\n----------------------------------------\n\nTITLE: Using html2txt for Maximum Recall\nDESCRIPTION: Example of using the html2txt function to extract all possible text from HTML content, maximizing recall.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from trafilatura import html2txt\n>>> html2txt(downloaded)\n```\n\n----------------------------------------\n\nTITLE: Enabling Fast Mode Extraction with Trafilatura in Python\nDESCRIPTION: Shows how to enable the no_fallback flag in the extract function to speed up extraction by bypassing fallback algorithms. This may improve performance but potentially reduce extraction accuracy. This option is suitable when faster processing is more critical than complete data coverage. The example presumes previously downloaded HTML content stored in the 'downloaded' variable.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/quickstart.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# faster mode without backup extraction\n>>> result = extract(downloaded, no_fallback=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Trafilatura with All Optional Features (Bash)\nDESCRIPTION: This command installs the core Trafilatura library along with all recommended optional dependencies using pip's extra syntax. This includes packages for faster encoding detection, date extraction, language detection, and alternative download methods.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/installation.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install trafilatura[all]  # all additional functionality\n```\n\n----------------------------------------\n\nTITLE: Connecting to Epsilla Vector Database with Python Client\nDESCRIPTION: This code snippet demonstrates how to connect to a local Epsilla vector database server using the pyepsilla Python client, load a specific database, and create a table with fields for ID, document text, and embedding vectors. It sets up the environment for storing text embeddings derived from crawled web pages.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial-epsilla.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom pyepsilla import vectordb\nclient = vectordb.Client(\n    host='localhost',\n    port='8888'\n)\n\nstatus_code, response = client.load_db(\n    db_name=\"TrafilaturaDB\",\n    db_path=\"/tmp/trafilatura_store\"\n)\nprint(response)\n\nclient.use_db(db_name=\"TrafilaturaDB\")\n\nclient.drop_table('Trafilatura')\nclient.create_table(\n  table_name=\"Trafilatura\",\n  table_fields=[\n    {\"name\": \"ID\", \"dataType\": \"INT\"},\n    {\"name\": \"Doc\", \"dataType\": \"STRING\"},\n    {\"name\": \"Embedding\", \"dataType\": \"VECTOR_FLOAT\", \"dimensions\": 384}\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Modifying Trafilatura Extraction Settings at Runtime (Python)\nDESCRIPTION: This Python snippet demonstrates how to override the default extraction minimum output length using a deep copy of Trafilatura’s DEFAULT_CONFIG. It shows how to prepare a minimal HTML string for extraction, set a higher threshold for minimal output size (\"MIN_OUTPUT_SIZE\"), and run extraction with custom settings, which will fail if content is too short. The final call without config shows default extraction succeeding. Dependencies: the 'copy' module, and the 'trafilatura' package with the 'extract' function and 'DEFAULT_CONFIG'. Inputs are HTML content (as string) and a modifiable configuration dictionary. Returns the extracted text or None if constraints are not met. Limitation: Custom settings must be valid, and string values are expected for config options.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/settings.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# load necessary functions and data\n>>> from copy import deepcopy\n>>> from trafilatura import extract\n>>> from trafilatura.settings import DEFAULT_CONFIG\n\n# a very short HTML file\n>>> my_html = \"<html><body><p>Text.</p></body></html>\"\n\n# load the configuration and change the minimum output length\n>>> my_config = deepcopy(DEFAULT_CONFIG)\n>>> my_config['DEFAULT']['MIN_OUTPUT_SIZE'] = '1000'\n\n# apply new settings, extraction will fail\n>>> extract(my_html, config=my_config)\n>>>\n# default extraction works\n>>> extract(my_html)\n'Text.'\n```\n\n----------------------------------------\n\nTITLE: Discovering RSS/Atom Feeds from a Web Page Using find_feed_urls() in Python\nDESCRIPTION: This snippet leverages the feeds module to locate RSS or Atom feed URLs from a homepage URL, with options to filter by target language. It demonstrates both automatic discovery and direct use of a known feed URL, returning a sorted list of links.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n# import the feeds module\n>>> from trafilatura import feeds\n\n# automatically retrieve feeds from a website\n>>> mylist = feeds.find_feed_urls('https://www.theguardian.com/')\n>>> mylist\n['https://www.theguardian.com/international/rss', '...'] # and so on\n\n# use a predetermined feed URL directly\n>>> mylist = feeds.find_feed_urls('https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml')\n>>> mylist is not []\nTrue # it's not empty\n\n# search for feeds in a specific language (e.g., English)\n>>> mylist = feeds.find_feed_urls('https://www.un.org/en/rss.xml', target_lang='en')\n>>> mylist is not []\nTrue\n```\n\n----------------------------------------\n\nTITLE: Using Simhash Class for Near-Duplicate Detection in Python\nDESCRIPTION: This snippet explains how to create Simhash objects to compute similarity between two text strings. The Simhash class from Trafilatura implements locality-sensitive hashing to identify near-duplicate content efficiently. The similarity() method returns a float from 0 to 1 indicating similarity degree. Additionally, it demonstrates reconstructing a Simhash object from an existing hash value to reuse comparisons.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/deduplication.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from trafilatura.deduplication import Simhash\n\n>>> first = Simhash(\"This is a text.\")\n>>> second = Simhash(\"This is a test.\")\n>>> second.similarity(first)\n0.84375\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> first_copy = Simhash(existing_hash=first.hash)\n>>> first_copy.similarity(first)\n1.0\n```\n\n----------------------------------------\n\nTITLE: Fallback Download from Internet Archive Using Python\nDESCRIPTION: This Python snippet demonstrates how to attempt a fallback download from the Internet Archive when an initial download fails. It shows the construction of a new URL pointing to the archived version of the target page and fetching it using a function like fetch_url(). This snippet requires an existing function capable of retrieving the content from a given URL. It is intended to help recover content when live downloads are unavailable due to link rot or other failures.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/troubleshooting.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# url is the target\n# downloaded is the result of the download\n# also needs a function fetch_url() or equivalent\nif downloaded is None:\n    new_url = \"https://web.archive.org/web/20/\" + url\n    downloaded = fetch_url(new_url)\n```\n\n----------------------------------------\n\nTITLE: Using Web Crawling and Link Navigation Arguments with Trafilatura (Bash)\nDESCRIPTION: Lists optional navigation arguments for link discovery, web crawling, feed and sitemap processing in Trafilatura's CLI. Highlights usage of --feed, --sitemap, --crawl, --explore, --probe, --archived, and --url-filter to direct how URLs are gathered and filtered during extraction. Requires Trafilatura to be installed. Most options accept a URL or pattern as input; some work as toggles; output varies based on extraction context.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n--feed [FEED]         look for feeds and/or pass a feed URL as input\n--sitemap [SITEMAP]   look for sitemaps for the given website and/or enter a sitemap URL\n--crawl [CRAWL]       crawl a fixed number of pages within a website starting from the given URL\n--explore [EXPLORE]   explore the given websites (combination of sitemap and crawl)\n--probe [PROBE]       probe for extractable content (works best with target language)\n--archived            try to fetch URLs from the Internet Archive if downloads fail\n--url-filter URL_FILTER [URL_FILTER ...] only process/output URLs containing these patterns (space-separated strings)\n```\n\n----------------------------------------\n\nTITLE: Extracting Metadata from Web Documents using Trafilatura's extract_metadata() in Python\nDESCRIPTION: This snippet fetches metadata (author, date, title, etc.) from a web document using the `extract_metadata` function. Requires Trafilatura installed and a raw HTML input string. The method outputs metadata information useful for indexing or summarization without performing full text extraction.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura import extract_metadata\n\nextract_metadata(document)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Extraction Speed\nDESCRIPTION: Examples demonstrating how to optimize extraction speed by disabling fallbacks and reducing feature usage.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# skip algorithms used as fallback\n>>> result = extract(downloaded, no_fallback=True)\n\n# combination for shorter processing times\n>>> result = extract(downloaded, include_comments=False, include_tables=False, no_fallback=True)\n```\n\n----------------------------------------\n\nTITLE: Performing Simple Sequential Downloads with Trafilatura Python\nDESCRIPTION: Demonstrates how to download a single URL or a list of URLs sequentially using the `fetch_url()` function. This method is suitable for simple, single-threaded tasks and utilizes a connection pool for efficiency. The function returns the decoded HTML content as a Unicode string.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/downloads.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura.downloads import fetch_url\n\n# single download\ndownloaded = fetch_url('https://www.example.org')\n\n# sequential downloads using a list\nmylist = [\"https://www.example.org\", \"https://httpbin.org\"]\nfor url in mylist:\n    downloaded = fetch_url(url)\n    # do something with it\n```\n\n----------------------------------------\n\nTITLE: Extracting Content from Unicode Strings or Parsed LXML Trees with Trafilatura in Python\nDESCRIPTION: These snippets show two input format options for Trafilatura extraction functions. One reads the HTML document as a Unicode string from a UTF-8 file, while the second parses the string into an LXML HTML tree object. Both formats are supported by Trafilatura's extraction methods, enabling flexible input handling depending on the use case and preprocessing needs.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# filename = 'myfile.html'\n# with open(filename, encoding='utf-8') as f:\n#    document = f.read()\n# extract(document)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom lxml import html\n\ntree = html.fromstring(document)\n#extract(tree)\n```\n\n----------------------------------------\n\nTITLE: Performing Web Crawling and Generating Embeddings with Trafilatura and HuggingFace BGE\nDESCRIPTION: This code snippet shows how to fetch web page content using Trafilatura, generate vector embeddings with HuggingFace BGE embeddings model, and prepare records for insertion into the Epsilla database. It involves importing necessary libraries, initializing the embedding model, crawling multiple URLs, extracting content, creating embeddings, and structuring data records for storage.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial-epsilla.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom trafilatura import fetch_url, extract\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\n\nmodel_name = \"BAAI/bge-small-en\"\nmodel_kwargs = {'device': 'cpu'}\nencode_kwargs = {'normalize_embeddings': False}\n\nhf = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)\n\nurls = [\n    'https://www.tensorflow.org/',\n    'https://pytorch.org/',\n    'https://react.dev/',\n]\nresults = [extract(fetch_url(url)) for url in urls]\n\nembeddings = [hf.embed_query(result) for result in results]\nrecords = [\n    {\"ID\": idx, \"Doc\": results[idx], \"Embedding\": embeddings[idx]} \n    for idx in range(len(results))\n]\nclient.insert(\n   table_name=\"Trafilatura\",\n   records=records\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Reticulate Package in R\nDESCRIPTION: Installs the 'reticulate' R package from CRAN and loads it into the current R session. Required dependency: the user must have R installed and an internet connection. No parameters are required; expected output is the successful installation and loading of the package with no return value on success.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-r.rst#_snippet_0\n\nLANGUAGE: R\nCODE:\n```\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n```\n\n----------------------------------------\n\nTITLE: Running Command-Line Downloads from File with Trafilatura Bash\nDESCRIPTION: Provides a simple command-line example using the `trafilatura` tool to read a list of URLs from a specified input file (`-i`). It saves the processed text output to a directory (`-o`) and optionally backs up the original HTML in another directory (`--backup-dir`). This method automatically uses threading and domain-aware throttling.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/downloads.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ trafilatura -i list.txt -o txtfiles/ --backup-dir htmlbackup/\n```\n\n----------------------------------------\n\nTITLE: Piping HTML Content into Trafilatura for Extraction on Command-Line\nDESCRIPTION: Illustrates how to feed HTML content from local files to the Trafilatura command-line tool using shell pipe and input redirection. This method supports processing existing HTML documents for extraction without network access. It depends on having the trafilatura executable properly installed. The snippet includes alternate approaches to pass file contents.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/quickstart.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ cat myfile.html | trafilatura # use the contents of an already existing file\n$ < myfile.html trafilatura # same here\n```\n\n----------------------------------------\n\nTITLE: Making a POST request to Trafilatura API using Python with requests library\nDESCRIPTION: This Python code snippet shows how to send a POST request to the Trafilatura API for webpage data extraction. It constructs a JSON payload with the target URL and output format, sets the appropriate header, and sends the request using the requests library. The response, expected in JSON, is printed out. Dependencies include Python and the requests library.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-api.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"https://trafilatura.mooo.com/extract-demo\"\n\npayload = {\n    \"url\": \"https://example.org\",\n    \"args\": { \"output_format\": \"xml\" }\n}\nheaders = {\n    \"content-type\": \"application/json\",\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\n\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Filtering Tokens and Counting Top 20 Frequencies (Bash)\nDESCRIPTION: Filters tokens from `tokens.txt` using `sed` to remove punctuation (`s/[[:punct:]]//g`), delete empty lines (`/^$/d`), and convert text to lowercase (`s/.*/\\L\\0/`), saving the result to `tokens-filtered.txt`. It then displays the top 20 most frequent filtered tokens using `sort`, `uniq -c`, `sort -nrk1`, and `head -20`. Requires `tokens.txt` as input.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial1.rst#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n$ < tokens.txt sed -e \"s/[[:punct:]]//g\" -e \"/^$/d\" -e \"s/.*/\\L\\0/\" > tokens-filtered.txt\n# display most frequent tokens\n$ < tokens-filtered.txt sort | uniq -c | sort -nrk1 | head -20\n```\n\n----------------------------------------\n\nTITLE: Concatenating and Tokenizing Text Files using Cat and SoMaJo (Bash)\nDESCRIPTION: Concatenates all `.txt` files within the `txtfiles` directory into a single file named `all.txt` using `cat`. Then, it tokenizes the content of `all.txt` using `somajo-tokenizer` and saves the resulting tokens (one per line) into `tokens.txt`. Requires `SoMaJo` installed and text files present in the `txtfiles` directory.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial1.rst#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n$ cat txtfiles/*.txt > txtfiles/all.txt\n$ somajo-tokenizer txtfiles/all.txt > tokens.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Encoding Detector (Bash)\nDESCRIPTION: This command installs the `cchardet` package, an optional dependency that provides faster and potentially more accurate encoding detection for Trafilatura. It is highly recommended but may not work on all systems.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/installation.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install cchardet  # single package only\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from URLs using Trafilatura (Bash)\nDESCRIPTION: Uses the `trafilatura` command-line tool to process a list of URLs from `list.txt`. The first command saves extracted text as raw text files in the `txtfiles` directory. The second command saves the output in XML format in the `xmlfiles` directory. Requires `trafilatura` installed and a readable input file (`list.txt`) containing one URL per line.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial1.rst#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n$ trafilatura -i list.txt -o txtfiles\t# output as raw text\n$ trafilatura --xml -i list.txt -o xmlfiles\t# output in XML format\n```\n\n----------------------------------------\n\nTITLE: Updating Trafilatura Package (Bash)\nDESCRIPTION: Use this command to upgrade an existing installation of the Trafilatura library to the latest version available on PyPI. It ensures you benefit from recent improvements and bug fixes.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/installation.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# to make sure you have the latest version\n$ pip install --upgrade trafilatura\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version (Bash)\nDESCRIPTION: This command is used in a terminal or command prompt to verify the installed Python version. Trafilatura requires Python 3.6 or higher. The output shows the installed version.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/installation.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 --version  # python can also work\nPython 3.10.12       # version 3.6 or higher is fine\n```\n\n----------------------------------------\n\nTITLE: Sorting and Random Sampling of URL Lists with Bash Utilities\nDESCRIPTION: These commands enable unique sorting and random sampling of URL lists using GNU core utilities. The 'sort -u' command removes duplicate lines, 'sort -R' (or 'shuf') randomizes order, and 'head' selects the top N lines from the randomized list to create a representative sample. These steps are helpful before processing large datasets. No dependencies beyond standard Unix utilities required.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial0.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsort -u myfile.txt > myfile-sorted.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\nsort -R myfile.txt > myfile-random.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\nshuf myfile.txt > myfile-random.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\nshuf myfile.txt | head -100 > myfile-random-sample.txt\n```\n\n----------------------------------------\n\nTITLE: Calculating Top 20 Bigram and Trigram Frequencies (Bash)\nDESCRIPTION: Calculates the frequencies of word bigrams and trigrams from a filtered token file (`tokens-filtered.txt`). `tr \"\\n\" \" \"` converts the token list into a single line. `awk` generates the n-grams. `sort | uniq -c | sort -nrk1` counts and orders the frequencies. `head -20` displays the top 20 results for both bigrams and trigrams. Requires `tokens-filtered.txt` as input.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial1.rst#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\n# word bigrams\n$ < tokens-filtered.txt tr \"\\n\" \" \" | awk '{for (i=1; i<NF; i++) print $i, $(i+1)}' | sort | uniq -c | sort -nrk1 | head -20\n# word trigrams\n$ < tokens-filtered.txt tr \"\\n\" \" \" | awk '{for (i=1; i<NF; i++) print $i, $(i+1), $(i+2)}' | sort | uniq -c | sort -nrk1 | head -20\n```\n\n----------------------------------------\n\nTITLE: Tokenizing and Cleaning XML Files using SoMaJo and Sed (Bash)\nDESCRIPTION: Demonstrates processing XML files extracted by `trafilatura`. The first command tokenizes a specific XML file (`xmlfiles/filename.xml`) using `somajo-tokenizer --xml`. The second command chains `somajo-tokenizer` with `sed` to first tokenize the XML and then remove all XML tags (`s|</*.*>||g`) and empty lines (`/^$/d`) from the output, preparing the content for frequency analysis similar to plain text. Requires `SoMaJo` and an input XML file.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial1.rst#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\n# tokenize a file\n$ somajo-tokenizer --xml xmlfiles/filename.xml\n# remove tags\n$ somajo-tokenizer --xml xmlfiles/filename.xml | sed -e \"s|</*.*>||g\" -e \"/^$/d\"\n```\n\n----------------------------------------\n\nTITLE: Using Extractor Class for Parameter Management\nDESCRIPTION: Example showing how to use the Extractor class to define and manage extraction parameters in a convenient way.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# import the Extractor class from the settings module\n>>> from trafilatura.settings import Extractor\n\n# set multiple options at once\n>>> options = Extractor(output_format=\"json\", with_metadata=True)\n\n# add or adjust settings as needed\n>>> options.formatting = True  # same as include_formatting\n>>> options.source = \"My Source\"  # useful for debugging\n\n# use the options in an extraction function\n>>> extract(my_doc, options=options)\n```\n\n----------------------------------------\n\nTITLE: Sending a POST request to Trafilatura API via Bash using curl\nDESCRIPTION: This Bash code snippet demonstrates how to perform a POST request to the Trafilatura API endpoint using curl. It specifies the URL to extract and sets the output format to XML by including a JSON payload in the request body. Required dependencies include curl installed on the system.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-api.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ curl -X POST \"https://trafilatura.mooo.com/extract-demo\" \\\n       -H \"content-type: application/json\" \\\n       --data '{\n                \"url\": \"https://example.org\",\n                \"args\": {\n                  \"output_format\": \"xml\"\n                 }\n              }'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Full Response Objects with Trafilatura Python\nDESCRIPTION: Shows how to use the `fetch_response()` function to obtain a `Response` object instead of just the HTML string. This object provides access to detailed information like headers, status code, original URL, and raw binary data. Optional parameters like `decode` and `with_headers` control which attributes are populated.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/downloads.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Response object instead of Unicode string\n>>> response = fetch_response('https://www.example.org')\n>>> response.status\n200\n>>> response.url\n'https://www.example.org'\n>>> response.data\n# raw HTML in binary format\n>>> response = fetch_response('https://www.example.org', decode=True, with_headers=True)\n# headers and html attributes used\n```\n\n----------------------------------------\n\nTITLE: Fetching Web Pages with Trafilatura in Python\nDESCRIPTION: This snippet demonstrates how to download the raw HTML content of a webpage using Trafilatura's `fetch_url` function. The main dependency is the Trafilatura package. The input is a URL string, and the output is the raw HTML document as a Unicode string suitable for further processing with extraction functions.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura import fetch_url\n\ndocument = fetch_url('https://www.example.org')\n```\n\n----------------------------------------\n\nTITLE: Producing and Validating TEI XML Documents via Trafilatura CLI in Bash\nDESCRIPTION: This Bash command uses Trafilatura's command-line interface to extract a web page and output its content as a TEI-compliant XML file with validation. The necessary dependency is the Trafilatura tool installed and available in the system path. The flags --xmltei and --validate activate TEI formatting and validation, respectively, while --URL specifies the target resource. Input is a URL, and the expected output is a validated TEI XML written to STDOUT or an error message if validation fails.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial2.rst#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ntrafilatura --xmltei --validate --URL \"https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SOCKS Proxy via Environment Variable in Bash\nDESCRIPTION: Shows how to configure Trafilatura to route downloads through a SOCKS proxy by setting the `http_proxy` environment variable in a Bash shell. Provides examples for setting up a basic proxy and one that requires user authentication. This affects subsequent Trafilatura executions in that shell.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/downloads.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# set socks proxy\nexport http_proxy=socks5://PROXYHOST:PROXYPORT\n\n# with user and password\nexport http_proxy=socks5://USER:PASSWORD@PROXYHOST:PROXYPORT\n```\n\n----------------------------------------\n\nTITLE: Web Crawl Processing and Annotated Corpora Building Using Python\nDESCRIPTION: This suite of Python scripts processes web crawl data to generate clean, register-annotated corpora for linguistic research. It includes crawling, cleaning, and annotation steps, facilitating corpus linguistics and NLP model training. Libraries utilized include Scrapy, regex, and annotation toolkits.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/used-by.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n<python code snippet here>\n```\n\n----------------------------------------\n\nTITLE: Language Model Training Data Preparation in Python\nDESCRIPTION: This Python code prepares web data for training language models by cleaning, tokenizing, and structuring web text datasets. It aims to create high-quality training corpora with minimal noise for language modeling tasks. Uses NLP libraries such as Tokenizers, NLTK, or spaCy.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/used-by.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n<python code snippet here>\n```\n\n----------------------------------------\n\nTITLE: Resetting Caches in Trafilatura to Manage Memory Usage in Python\nDESCRIPTION: This snippet shows how to clear internal caches in Trafilatura to prevent memory leaks during large-scale processing. It involves importing reset_caches() and invoking it to release RAM in long-running or memory-intensive applications.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-python.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# import the function\n>>> from trafilatura.meta import reset_caches\n\n# use it at any given point\n>>> reset_caches()\n```\n\n----------------------------------------\n\nTITLE: Filtering Links with coURLan using Bash\nDESCRIPTION: This snippet demonstrates the use of the coURLan command-line tool to filter a list of raw URLs for content adequacy. It requires both 'courlan' and 'trafilatura' to be installed. The '--inputfile' parameter specifies the path to the unfiltered text file containing URLs, while '--outputfile' specifies where to save the filtered list. The output file will contain only those URLs considered suitable by coURLan's heuristics for text extraction, improving efficiency and quality in subsequent crawling steps.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial0.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ courlan --inputfile raw-linklist.txt --outputfile filtered-linklist.txt\n```\n\n----------------------------------------\n\nTITLE: Extracting All Text Content from HTML Using html2txt in Python\nDESCRIPTION: Utilizes the html2txt function from Trafilatura to extract all text content from an HTML document in a plain text format simulating html2txt style extraction. This differs from main content extraction by including full text rather than only relevant paragraphs. Requires fetching or loading the HTML document first.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/quickstart.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from trafilatura import html2txt\n>>> html2txt(downloaded)\n```\n\n----------------------------------------\n\nTITLE: Sampling URLs with courlan CLI (Bash)\nDESCRIPTION: Shows how to use the `courlan` command-line utility for sampling URLs from a file. The `--sample` flag activates the sampling mode, and `--samplesize` specifies the target number of URLs per domain. This allows for efficient batch processing of URL lists for sampling purposes.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/url-management.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ courlan --inputfile urls.txt --outputfile samples-urls.txt --sample --samplesize 50\n```\n\n----------------------------------------\n\nTITLE: Selecting Random Word Samples in Python\nDESCRIPTION: This Python snippet demonstrates selecting a specified number (`k=3`) of unique random words from a predefined list (`wordlist`) using the `random.sample()` function. This technique is shown as part of implementing the BootCaT method for generating random search engine query terms to discover seed URLs for web corpora. It requires the `random` module (standard library) and assumes `wordlist` is populated.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/sources.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import random\n# use the list gathered in (1)\n>>> wordlist = ['word1', 'word2', 'word3', 'word4']  # and so on\n# draw 3 random words from the list\n>>> selection = random.sample(wordlist, k=3)\n```\n\n----------------------------------------\n\nTITLE: Filtering HTML for Feed Links using Bash Regex\nDESCRIPTION: This Bash code block provides a regular expression pattern designed to identify `<link>` tags within HTML documents that specify web syndication feeds (RSS, Atom, RDF). It can be used with tools like `grep` or `sed` to extract feed URLs from web page source code as part of a corpus building or web crawling process.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/sources.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"(<link[^>]*(?:\\s(?:type=[\\\"']?(application\\/rss\\+xml|application\\/atom\\+xml|application\\/rss|application\\/atom|application\\/rdf\\+xml|application\\/rdf|text\\/rss\\+xml|text\\/atom\\+xml|text\\/rss|text\\/atom|text\\/rdf\\+xml|text\\/rdf|text\\/xml|application\\/xml)[\\\"']?|rel=[\\\"']?(?:alternate)[\\\"']?))[^>]*>)\"\n```\n\n----------------------------------------\n\nTITLE: Sampling URLs with sample_urls (Python)\nDESCRIPTION: Demonstrates how to use the `sample_urls` function in Python to select a subset of URLs from a list, controlling the number of URLs per domain. This helps in managing dataset size and preventing domain overrepresentation. The example shows passing a list of URLs and the desired sample size.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/url-management.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from courlan import sample_urls\n>>> my_urls = ['…', '…', '…', ]  # etc.\n>>> my_sample = sample_urls(my_urls, 50)\n# optional: exclude_min=None, exclude_max=None, strict=False, verbose=False\n```\n\n----------------------------------------\n\nTITLE: Filtering URLs with check_url (Python)\nDESCRIPTION: Demonstrates how to use the `check_url` function from the `courlan` package to filter and validate URLs. It shows basic usage, removing noisy query parameters with `strict=True`, and filtering based on language using the `language` parameter. The function returns a tuple containing the cleaned URL and hostname, or `None` if the URL is rejected.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/url-management.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# load the function from the included courlan package\n>>> from courlan import check_url\n\n# checking a URL returns None or a tuple (cleaned url, hostname)\n>>> check_url('https://github.com/adbar/courlan')\n('https://github.com/adbar/courlan', 'github.com')\n\n# noisy query parameters can be removed\n>>> check_url('https://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.org', strict=True)\n('https://httpbin.org/redirect-to', 'httpbin.org')\n\n# optional argument targeting webpages in English or German\n>>> my_url = 'https://www.un.org/en/about-us'\n>>> url, domain_name = check_url(my_url, language='en')\n>>> url, domain_name = check_url(my_url, language='de')\n```\n\n----------------------------------------\n\nTITLE: Checking Robot Exclusion Standard Rules with Python\nDESCRIPTION: Demonstrates using Python's standard library `urllib.robotparser` to fetch and parse a website's `robots.txt` file. The example shows how to set the URL, read the rules, and check if a specific URL is allowed to be fetched for a given user agent using the `can_fetch()` method. This helps ensure compliance with website crawling policies.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/downloads.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport urllib.robotparser\nfrom trafilatura import get_crawl_delay\n    \n# define a website to look for rules\nbase_url = 'https://www.example.org'\n    \n# load the necessary components, fetch and parse the file\nrules = urllib.robotparser.RobotFileParser()\nrules.set_url(base_url + '/robots.txt')\nrules.read()\n\n# determine if a page can be fetched by all crawlers\nrules.can_fetch(\"*\", \"https://www.example.org/page1234.html\")\n# returns True or False\n```\n\n----------------------------------------\n\nTITLE: Generating Content Fingerprint Using Simhash and Hash-Based Filenames in Python\nDESCRIPTION: These examples show standalone functions in Trafilatura's deduplication module for hashing text content. The content_fingerprint() function returns the simhash of input text as a hexadecimal string for consistent content identification. The generate_hash_filename() function creates a filename-safe string based on the hash of the content, useful for naming files according to their textual contents. Both support managing duplicates without full object usage.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/deduplication.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from trafilatura.deduplication import content_fingerprint\n>>> content_fingerprint(\"Here is text.\")\n'd2ff47ba297cc254'\n```\n\nLANGUAGE: python\nCODE:\n```\n# create a filename-safe string by hashing the given content\n>>> from trafilatura.deduplication import generate_hash_filename\n>>> generate_hash_filename(\"This is a text.\")\n'qAgzZnskrcRgeftk'\n```\n\n----------------------------------------\n\nTITLE: Checking for remaining navigation pages with is_still_navigation in Python\nDESCRIPTION: Shows how to determine if there are still navigation pages to visit by applying the is_still_navigation() function to the 'to_visit' list. Returns a boolean indicating whether the crawl should continue, aiding crawl control and management.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/crawls.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n>>> from trafilatura.spider import is_still_navigation\n\n>>> is_still_navigation(to_visit)\n# returns True or False\n```\n\n----------------------------------------\n\nTITLE: HTML Main Content Extraction Using Visual Features in Java\nDESCRIPTION: This Java code leverages visual features such as first impression areas to identify main content sections within HTML pages. It aims to improve accuracy over traditional DOM-based methods by analyzing layout cues, useful for summarization, and content extraction tools. Libraries may include HTML parsers and image processing modules.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/used-by.rst#_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n<java code snippet here>\n```\n\n----------------------------------------\n\nTITLE: Citing Trafilatura in Academic Publications (BibTeX)\nDESCRIPTION: Provides a BibTeX citation entry for referencing the Trafilatura library in academic works. This snippet is used for including in bibliography management tools for LaTeX documents when citing the Trafilatura system demonstration paper. No dependencies; use as static reference.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/index.rst#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n@inproceedings{barbaresi-2021-trafilatura,\n  title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},\n  author = \"Barbaresi, Adrien\",\n  booktitle = \"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations\",\n  pages = \"122--131\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2021.acl-demo.15\",\n  year = 2021,\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading Web Pages with Wget and Processing with Trafilatura (Bash)\nDESCRIPTION: Demonstrates using `wget` to download web pages specified in `mylist.txt` into a `download/` directory with a 5-second delay between requests. Subsequently, it uses `trafilatura` to process the downloaded HTML files from `download/`, outputting TEI XML files (without comments) into the `corpus/` directory. Requires `wget` and `trafilatura` to be installed.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial0.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# download if necessary\n$ wget --directory-prefix=download/ --wait 5 --input-file=mylist.txt\n# process a directory with archived HTML files\n$ trafilatura --input-dir download/ --output-dir corpus/ --xmltei --no-comments\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version Using Bash\nDESCRIPTION: Executes a shell command to display the currently installed Python version. Requires Python to be installed and accessible on the system PATH. Returns the Python version string; version 3.6 or higher is required. No arguments needed.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-r.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 --version\n```\n\n----------------------------------------\n\nTITLE: Discovering Links from Sitemaps with Trafilatura CLI (Bash)\nDESCRIPTION: Shows how to use `trafilatura` to discover links listed in sitemaps using the `--sitemap` option. It can take a homepage URL (attempting auto-detection) or a direct sitemap URL. The `--list` option outputs the discovered URLs, which can be redirected to a file. The `--target-language` option demonstrates filtering discovered URLs based on language. Requires sitemap URLs or homepages with discoverable sitemaps; output is a list of discovered URLs.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# run link discovery through a sitemap for sitemaps.org and store the resulting links in a file\n$ trafilatura --sitemap \"https://www.sitemaps.org/\" --list > mylinks.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\n# using an already known sitemap URL\n$ trafilatura --sitemap \"https://www.sitemaps.org/sitemap.xml\" --list\n```\n\nLANGUAGE: bash\nCODE:\n```\n# targeting webpages in German\n$ trafilatura --sitemap \"https://www.sitemaps.org/\" --list --target-language \"de\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Full Argument List for Trafilatura CLI (Bash)\nDESCRIPTION: Demonstrates use of the Trafilatura CLI help command to display a comprehensive list of all arguments and usage patterns. Useful for discovering available options, inputs, and outputs, this snippet is suitable for new users configuring their first runs. Key flags include input source selection, output directory, crawling options, and extraction customizations. Requires Trafilatura to be installed and available in the PATH. Input is the command with no required arguments; output is a printed help message that summarizes all operational modes.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-cli.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntrafilatura [-h] [-i INPUTFILE | --input-dir INPUTDIR | -u URL]\n               [--parallel PARALLEL] [-b BLACKLIST] [--list]\n               [-o OUTPUTDIR] [--backup-dir BACKUP_DIR] [--keep-dirs]\n               [--feed [FEED] | --sitemap [SITEMAP] | --crawl [CRAWL] |\n               --explore [EXPLORE] | --probe [PROBE]] [--archived]\n               [--url-filter URL_FILTER [URL_FILTER ...]] [-f]\n               [--formatting] [--links] [--images] [--no-comments]\n               [--no-tables] [--only-with-metadata] [--with-metadata]\n               [--target-language TARGET_LANGUAGE] [--deduplicate]\n               [--config-file CONFIG_FILE] [--precision] [--recall]\n               [--output-format {csv,json,html,markdown,txt,xml,xmltei} | \n               --csv | --html | --json | --markdown | --xml | --xmltei]\n               [--validate-tei] [-v] [--version]\n```\n\n----------------------------------------\n\nTITLE: Fetching and Extracting Web Content with Trafilatura from R\nDESCRIPTION: Illustrates workflow for downloading and extracting text from a web page using Trafilatura in R. Requires Reticulate and Trafilatura installed. Steps include importing Trafilatura, fetching a URL, extracting content, and optionally specifying output format and URL for extraction (e.g., XML). Inputs: a URL string. Outputs: extracted text or XML content. Limitations: the targeted web page should be accessible and compatible with Trafilatura parsing.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-r.rst#_snippet_4\n\nLANGUAGE: R\nCODE:\n```\n# getting started\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# loading the Trafilatura module\ntrafilatura <- import(\"trafilatura\")\n\n# fetching a web page\nurl <- \"https://example.org/\"\ndownloaded <- trafilatura$fetch_url(url)\n\n# extracting the text content\ntext <- trafilatura$extract(downloaded)\ncat(text)\n\n# extraction with arguments\ntrafilatura$extract(downloaded, output_format=\"xml\", url=url)\n```\n\n----------------------------------------\n\nTITLE: Calculating Top 10 Token Frequencies using Sort and Uniq (Bash)\nDESCRIPTION: Calculates word frequencies from a tokenized file (`tokens.txt`). It sorts the tokens alphabetically (`sort`), counts unique occurrences (`uniq -c`), sorts the results numerically in reverse order (`sort -nrk1`), and displays the top 10 most frequent tokens using `head -10`. Assumes `tokens.txt` contains one token per line.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial1.rst#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\n$ sort tokens.txt | uniq -c | sort -nrk1 | head -10\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Limitations of Web Crawling with Trafilatura's focused_crawler in Python\nDESCRIPTION: This example highlights web crawling limitations by showing that some dynamic websites may yield no URLs during crawling. Using the same focused_crawler approach on a dynamic content site, it prints zero results. Useful for understanding the constraints of crawling JavaScript-heavy or dynamically generated pages.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nto_visit, known_urls = focused_crawler('https://www.zhdk.ch/', max_seen_urls=3)\nprint(len(to_visit), len(known_urls))\n# nothing is found as the content is dynamic in nature\nprint('\\n'.join(list(to_visit)[:5]))\nprint('---')\nprint('\\n'.join(list(known_urls)[:5]))\n```\n\n----------------------------------------\n\nTITLE: Extracting and Validating TEI XML Documents with Trafilatura in Python\nDESCRIPTION: This Python snippet demonstrates how to use the Trafilatura library to fetch a web page, extract its content in TEI-compliant XML format, and validate the result against the TEI schema. The key dependencies are Trafilatura (for fetch_url and extract functions) and a working internet connection to access the target URL. The extract function's tei_validation parameter controls schema validation; setting output_format to 'xmltei' ensures TEI output. Input is a web URL as a string, and the output is a validated TEI XML string or validation error.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial2.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# load the necessary components\nfrom trafilatura import fetch_url, extract\n\n# download a file\ndownloaded = fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n\n# extract information as XML TEI and validate the result\nresult = extract(downloaded, output_format='xmltei', tei_validation=True)\n```\n\n----------------------------------------\n\nTITLE: Validating XML-TEI Documents Using Trafilatura in Python\nDESCRIPTION: This snippet validates XML-TEI documents extracted using Trafilatura. First, it obtains an XML-TEI representation via `bare_extraction` with `output_format='xmltei'`, then retrieves the document 'body' tree and validates it using `validate_tei`. The methods check the structure's correctness. Dependencies include Trafilatura and lxml. The validation returns boolean validity and error messages if invalid.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom trafilatura import bare_extraction\nfrom trafilatura.xml import validate_tei\n\ndoc_dict = bare_extraction(document, output_format='xmltei')\ntei_tree = doc_dict['body']\n# not valid (happens frequently)\nvalidate_tei(tei_tree)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using Specific Trafilatura Submodules via py_run_string in R\nDESCRIPTION: Demonstrates how to import and use specific functions from Trafilatura submodules directly in R via py_run_string. Shows examples for discovering links in sitemaps and extracting metadata. Requires reticulate and Trafilatura. Inputs: Python code strings and downloaded document object. Outputs: sitemaps URLs or metadata entries as R objects. Limitations: Some functions may expect specific input types (e.g., correct object format returned by fetch_url).\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-r.rst#_snippet_7\n\nLANGUAGE: R\nCODE:\n```\n# using the code for link discovery in sitemaps\nsitemapsfunc <- py_run_string(\"from trafilatura.sitemaps import sitemap_search\")\nsitemapsfunc$sitemap_search(\"https://www.sitemaps.org/\")\n# and so on...\n\n# import the metadata part of the package as a function\nmetadatafunc <- py_run_string(\"from trafilatura.metadata import extract_metadata\")\ndownloaded <- trafilatura$fetch_url(\"https://github.com/rstudio/reticulate\")\nmetadatafunc$extract_metadata(downloaded)\n# and so on...\n```\n\n----------------------------------------\n\nTITLE: HTML Web Page Content Extraction Using JavaScript Browser Extension\nDESCRIPTION: This JavaScript snippet demonstrates the development of a browser extension to extract main content from web pages. It involves injecting scripts into web pages to identify and isolate primary content areas, facilitating effective data scraping and web archiving applications. Dependencies include DOM manipulation APIs and browser extension APIs.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/used-by.rst#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n<js code snippet here>\n```\n\n----------------------------------------\n\nTITLE: Using the Stored Embeddings to Find Relevant Projects via Vector Search\nDESCRIPTION: This snippet illustrates how to perform a vector search in Epsilla to find the most relevant crawled web page based on a query string. It involves generating a query embedding with the same model, executing a search query in the database with a limit of one result, and displaying the most relevant document, which in this case is React.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial-epsilla.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nquery = \"A modern frontend library\"\nquery_embedding = hf.embed_query(query)\nstatus_code, response = client.query(\n    table_name=\"Trafilatura\",\n    query_field=\"Embedding\",\n    query_vector=query_embedding,\n    limit=1\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Installing Trafilatura Development Dependencies (shell)\nDESCRIPTION: Explains how to install the necessary Python packages, including development-specific dependencies, required for contributing or running tests for the Trafilatura project. This command uses pip to install the package with an extra set of dependencies specified by `[dev]` to ensure all development tools are available.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install trafilatura[dev]\n```\n\n----------------------------------------\n\nTITLE: Parsing and Validating XML Files Using lxml and Trafilatura in Python\nDESCRIPTION: This example shows how to parse an XML file using lxml's etree and validate it against TEI standards with Trafilatura's `validate_tei` function. The snippet requires the filename to be parsed and returns True/False along with error messages depending on validity. This is suitable for validating saved XML-TEI documents externally from extraction workflows.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom lxml import etree\nfrom trafilatura.xml import validate_tei\n\n# Example: file named \"document.xml\"\n# mytree = etree.parse('document.xml')\n# validate_tei(mytree)\n# Output: True or False & error message\n```\n\n----------------------------------------\n\nTITLE: Command-line web crawling with --crawl option using Trafilatura\nDESCRIPTION: Executes a fixed-page crawl of a specified website from the command line, using the --crawl option to limit the number of pages visited. Results are redirected to a text file for analysis of collected URLs.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/crawls.rst#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n$ trafilatura --crawl \"https://www.example.org\" > links.txt\n```\n\n----------------------------------------\n\nTITLE: Validating Existing TEI XML Files with lxml and Trafilatura in Python\nDESCRIPTION: This Python snippet shows how to validate an existing TEI XML document using lxml for parsing and Trafilatura's validate_tei function for schema validation. Required dependencies are lxml (for etree) and Trafilatura with the xml module. The input is the file path to a TEI XML document; the function returns True if valid, or outputs an error message describing the first validation issue. Only TEI XML files are supported; malformed XML or unsupported schemas may cause errors.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial2.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# load the necessary components\nfrom lxml import etree\nfrom trafilatura.xml import validate_tei\n\n# open a file and parse it\nmytree = etree.parse('document-name.xml')\n\n# validate it\nvalidate_tei(mytree)\n# returns True or an error message\n```\n\n----------------------------------------\n\nTITLE: Running Specific Pytest Test File (shell)\nDESCRIPTION: Shows how to run tests only from a specific test file (e.g., `realworld_tests.py`) using the pytest command. This is useful for focusing on particular test suites during development cycles or when debugging specific features.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npytest realworld_tests.py\n```\n\n----------------------------------------\n\nTITLE: Saving Token Frequencies to a CSV File (Bash)\nDESCRIPTION: Processes a token file (`tokens.txt`) by sorting, counting unique tokens (`uniq -c`), sorting by frequency (`sort -nrk1`), and formatting the output using `sed` to remove leading spaces and replace the first space between the count and the token with a tab character. The results (count\ttoken) are saved to `txtfiles/frequencies.csv`.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/tutorial1.rst#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n$ < tokens.txt sort | uniq -c | sort -nrk1 | sed -e \"s|^ *||g\" -e  \"s| |\\t|\" > txtfiles/frequencies.csv\n```\n\n----------------------------------------\n\nTITLE: Running All Pytest Tests (shell)\nDESCRIPTION: Describes the command to execute all tests within the Trafilatura project's directory using the pytest framework. Running this command from the project's root directory will discover and execute all tests configured with pytest.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Extracting All Text from an HTML Document Using html2txt in R\nDESCRIPTION: Applies Trafilatura's html2txt function to extract all possible text from a previously downloaded web page. Requires Trafilatura module loaded and a valid downloaded document object. Input: the downloaded object from fetch_url. Output: the full extracted text content. No additional arguments required.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-r.rst#_snippet_5\n\nLANGUAGE: R\nCODE:\n```\ntrafilatura$html2txt(downloaded)\n```\n\n----------------------------------------\n\nTITLE: Running Python Code Directly from R with py_run_string\nDESCRIPTION: Runs arbitrary Python code from within an R session using py_run_string. This example imports Trafilatura, sets a URL, performs extraction in Python, and converts the result to an R object. Requires reticulate and Trafilatura installed. Inputs: Python command strings; expects access to valid variables and objects. Outputs: R objects with content processed on the Python side.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-r.rst#_snippet_6\n\nLANGUAGE: R\nCODE:\n```\npy_run_string(\"import trafilatura\")\nurl <- \"https://www.example.com\"\npy_df <- py_run_string(\"trafilatura.extract(url)\")\ndf <- py_to_r(py_df)\n```\n\n----------------------------------------\n\nTITLE: Installing Trafilatura Python Package via Reticulate in R\nDESCRIPTION: Uses the reticulate::py_install() function to install the 'trafilatura' Python package from within R. Requires reticulate to be installed and a working Python environment. Takes the package name as a parameter; expected output is the installation of Trafilatura in the specified Python environment.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-r.rst#_snippet_2\n\nLANGUAGE: R\nCODE:\n```\nlibrary(reticulate)\npy_install(\"trafilatura\")\n```\n\n----------------------------------------\n\nTITLE: Installing/Updating Trafilatura from Git (Bash)\nDESCRIPTION: This command forces a reinstallation and update of Trafilatura directly from the latest codebase on its GitHub repository. Use this to access features or fixes not yet available in the latest PyPI release.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/installation.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# latest available code base\n$ pip install --force-reinstall -U git+https://github.com/adbar/trafilatura\n```\n\n----------------------------------------\n\nTITLE: Web Dataset Collection for Japanese using Python\nDESCRIPTION: This Python script automates the collection of Japanese web content for training large-scale Japanese language models. It involves crawling, filtering, and storing Japanese web pages, with focus on creating comprehensive datasets. Dependencies include requests, beautifulsoup4, and language detection tools.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/used-by.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n<python code snippet here>\n```\n\n----------------------------------------\n\nTITLE: Metadata and N-gram Trend Viewer for Personal Web Archives in Python\nDESCRIPTION: This Python script provides tools for searching metadata and visualizing N-gram trends in personal web archive collections. It involves indexing metadata, executing search queries, and generating trend graphs, supporting digital humanities research and digital preservation workflows. Dependencies include NLP libraries like NLTK or spaCy and visualization tools.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/used-by.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n<python code snippet here>\n```\n\n----------------------------------------\n\nTITLE: Installing Trafilatura with pip via Bash\nDESCRIPTION: Demonstrates installing the Trafilatura package using pip from the shell. Requires a Python environment where pip is available. Outputs the installation progress in the terminal.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/docs/usage-r.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install trafilatura\n```\n\n----------------------------------------\n\nTITLE: Running Mypy Type Checking (shell)\nDESCRIPTION: Explains how to perform static type checking on the Trafilatura project's source code using the Mypy tool. This command targets the main project directory `trafilatura/` to check all Python files within it for type consistency.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmypy trafilatura/\n```\n\n----------------------------------------\n\nTITLE: Running Specific Test File with Python Interpreter (shell)\nDESCRIPTION: Provides an alternative method to run a specific test file (e.g., `realworld_tests.py`) directly using the Python 3 interpreter. This approach might be used for simpler test scripts or as an alternative to running via pytest.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython3 realworld_tests.py\n```\n\n----------------------------------------\n\nTITLE: Citing Trafilatura in Academic Publications - Shell/BibTeX\nDESCRIPTION: This snippet is a BibTeX citation entry for referencing Trafilatura in scientific publications. It captures the essential details including title, author, booktitle, pages, publisher, URL, and publication year. Intended for users needing to formally cite the Trafilatura package, especially in NLP or web data research. It is placed within a shell code block for formatting purposes but represents BibTeX content and requires no dependencies beyond standard BibTeX usage.\nSOURCE: https://github.com/adbar/trafilatura/blob/master/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n@inproceedings{barbaresi-2021-trafilatura,\n  title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},\n  author = \"Barbaresi, Adrien\",\n  booktitle = \"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations\",\n  pages = \"122--131\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2021.acl-demo.15\",\n  year = 2021,\n}\n```"
  }
]
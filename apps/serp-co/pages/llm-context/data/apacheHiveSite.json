[
  {
    "owner": "apache",
    "repo": "hive-site",
    "content": "TITLE: Basic Hive SELECT Syntax\nDESCRIPTION: Defines the complete syntax for Hive SELECT statements, including all optional clauses. Shows the structure with CommonTableExpression, WHERE, GROUP BY, ORDER BY, CLUSTER BY, DISTRIBUTE BY, SORT BY, and LIMIT clauses.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n[WITH CommonTableExpression (, CommonTableExpression)*]    (Note: Only available starting with Hive 0.13.0)\nSELECT [ALL | DISTINCT] select_expr, select_expr, ...\n  FROM table_reference\n  [WHERE where_condition]\n  [GROUP BY col_list]\n  [ORDER BY col_list]\n  [CLUSTER BY col_list\n    | [DISTRIBUTE BY col_list] [SORT BY col_list]\n  ]\n [LIMIT [offset,] rows]\n```\n\n----------------------------------------\n\nTITLE: Creating Tables in Hive SQL\nDESCRIPTION: Complete CREATE TABLE syntax for Apache Hive including options for temporary/external tables, partitioning, clustering, skewed tables, row formats, and storage options. Includes detailed specifications for data types, constraints, and table properties.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nCREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    \n  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])]\n  [COMMENT table_comment]\n  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]\n  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]\n  [SKEWED BY (col_name, col_name, ...)]                  \n     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)\n     [STORED AS DIRECTORIES]\n  [\n   [ROW FORMAT row_format] \n   [STORED AS file_format]\n     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]\n  ]\n  [LOCATION hdfs_path]\n  [TBLPROPERTIES (property_name=property_value, ...)]\n  [AS select_statement];\n\nCREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name\n  LIKE existing_table_or_view_name\n  [LOCATION hdfs_path];\n```\n\n----------------------------------------\n\nTITLE: Dropping Views in Hive\nDESCRIPTION: Syntax for removing a view from Hive. Can include IF EXISTS clause to prevent errors if the view doesn't exist.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_48\n\nLANGUAGE: sql\nCODE:\n```\nDROP VIEW [IF EXISTS] [db_name.]view_name;\n```\n\n----------------------------------------\n\nTITLE: Creating a Database in Hive SQL\nDESCRIPTION: Syntax for creating a new database or schema in Hive. Includes options for specifying a comment, location, managed location, and database properties.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE [REMOTE] (DATABASE|SCHEMA) [IF NOT EXISTS] database_name\n  [COMMENT database_comment]\n  [LOCATION hdfs_path]\n  [MANAGEDLOCATION hdfs_path]\n  [WITH DBPROPERTIES (property_name=property_value, ...)];\n```\n\n----------------------------------------\n\nTITLE: Using the isnotnull() function in Hive SQL\nDESCRIPTION: Checks if a value is not NULL. Returns true if the argument is not NULL and false otherwise.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_71\n\nLANGUAGE: SQL\nCODE:\n```\nisnotnull(a)\n```\n\n----------------------------------------\n\nTITLE: Creating Simple Temporary Macro Examples in HiveQL\nDESCRIPTION: Examples of temporary macro creation with different parameter patterns: a macro with no parameters that returns a fixed value, a macro with a string parameter, and a macro with two integer parameters for addition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_59\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TEMPORARY MACRO fixed_number() 42;\nCREATE TEMPORARY MACRO string_len_plus_two(x string) length(x) + 2;\nCREATE TEMPORARY MACRO simple_add (x int, y int) x + y;\n```\n\n----------------------------------------\n\nTITLE: Example of Adding Multiple Partitions in Hive 0.8+\nDESCRIPTION: Example showing how to add multiple partitions in a single ALTER TABLE statement in Hive 0.8 and later. This adds two partitions with different date values to the page_view table, specifying storage locations for each.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_34\n\nLANGUAGE: hql\nCODE:\n```\nALTER TABLE page_view ADD PARTITION (dt='2008-08-08', country='us') location '/path/to/us/part080808'\n                          PARTITION (dt='2008-08-09', country='us') location '/path/to/us/part080809';\n\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table in Hive\nDESCRIPTION: Creates a partitioned table 'page_view' with specified columns, comments, and storage format. The table is partitioned by date and country.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_4\n\nLANGUAGE: HiveQL\nCODE:\n```\nCREATE TABLE page_view(viewTime INT, userid BIGINT,\n                    page_url STRING, referrer_url STRING,\n                    ip STRING COMMENT 'IP Address of the User')\n    COMMENT 'This is the page view table'\n    PARTITIONED BY(dt STRING, country STRING)\n    STORED AS SEQUENCEFILE;\n```\n\n----------------------------------------\n\nTITLE: Using isnotnull Function in Hive SQL\nDESCRIPTION: Returns true if the input value is not NULL and false otherwise. Provides complementary functionality to isnull().\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nisnotnull ( a )\n```\n\n----------------------------------------\n\nTITLE: Efficient JSON Parsing with json_tuple UDTF\nDESCRIPTION: Optimized approach using json_tuple() UDTF with LATERAL VIEW to extract multiple values from a JSON string in a single parse operation, improving query efficiency for complex JSON structures.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_58\n\nLANGUAGE: SQL\nCODE:\n```\nselect a.timestamp, b.*\nfrom log a lateral view json_tuple(a.appevent, 'eventid', 'eventname') b as f1, f2;\n```\n\n----------------------------------------\n\nTITLE: Basic Hive CLI Commands Usage Examples\nDESCRIPTION: Examples demonstrating common Hive CLI commands such as setting configuration parameters, listing files, and executing shell commands or queries. These commands can be executed directly in the Hive CLI interface.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-commands_34838882.md#2025-04-09_snippet_0\n\nLANGUAGE: hiveql\nCODE:\n```\n  hive> set mapred.reduce.tasks=32;\n  hive> set;\n  hive> select a.* from tab1;\n  hive> !ls;\n  hive> dfs -ls;\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Partitioned Table in Hive\nDESCRIPTION: This example demonstrates how to create a partitioned table named 'page_view' with multiple columns and partitioned by date and country. The table is stored as a SequenceFile.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE page_view(viewTime INT, userid BIGINT,\n     page_url STRING, referrer_url STRING,\n     ip STRING COMMENT 'IP Address of the User')\n COMMENT 'This is the page view table'\n PARTITIONED BY(dt STRING, country STRING)\n STORED AS SEQUENCEFILE;\n```\n\n----------------------------------------\n\nTITLE: Using the nvl() function in Hive SQL\nDESCRIPTION: Returns a default value if the first argument is null, otherwise returns the first argument. Useful for handling NULL values in calculations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_72\n\nLANGUAGE: SQL\nCODE:\n```\nnvl(T value, T default_value)\n```\n\n----------------------------------------\n\nTITLE: Partition Pruning in JOIN Queries\nDESCRIPTION: Demonstrates how to specify partition filters in the ON clause of a JOIN to enable partition pruning when joining tables. This optimizes query performance by limiting the data scanned.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n    SELECT page_views.*\n    FROM page_views JOIN dim_users\n      ON (page_views.user_id = dim_users.id AND page_views.date >= '2008-03-01' AND page_views.date <= '2008-03-31')\n```\n\n----------------------------------------\n\nTITLE: Query Rewriting Example Using Materialized View in Hive\nDESCRIPTION: Example of a query that can be rewritten using the previously created materialized view.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT empid, deptname\nFROM emps\nJOIN depts\n  ON (emps.deptno = depts.deptno)\nWHERE hire_date >= '2018-01-01'\n    AND hire_date <= '2018-03-31';\n```\n\n----------------------------------------\n\nTITLE: Creating CSV/TSV Table with OpenCSVSerde in Hive SQL\nDESCRIPTION: This snippet illustrates how to create a table in Hive for CSV or TSV data using OpenCSVSerde. It shows an example of creating a TSV (Tab-separated) table with custom separator, quote, and escape characters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE my_table(a string, b string, ...)\nROW FORMAT SERDE\n'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n\"separatorChar\" = \"\\t\",\n\"quoteChar\"     = \"'\",\n\"escapeChar\"    = \"\\\\\"\n)\nSTORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: Creating Star Schema Tables in Hive\nDESCRIPTION: DDL statements to create a star schema based on the SSB benchmark, including customer, dates, part, supplier, and lineorder tables with integrity constraints\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views-in-hive_283118346.md#2025-04-09_snippet_5\n\nLANGUAGE: HiveQL\nCODE:\n```\nCREATE TABLE customer(\nc_custkey BIGINT,\nc_name STRING,\nc_address STRING,\nc_city STRING,\nc_nation STRING,\nc_region STRING,\nc_phone STRING,\nc_mktsegment STRING,\nPRIMARY KEY (c_custkey) DISABLE RELY)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n```\n\n----------------------------------------\n\nTITLE: Creating Table with Custom Field Delimiter in Hive\nDESCRIPTION: Creates a partitioned table 'page_view' with a custom field delimiter '1'. This example demonstrates how to specify a non-default row format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_5\n\nLANGUAGE: HiveQL\nCODE:\n```\nCREATE TABLE page_view(viewTime INT, userid BIGINT,\n                    page_url STRING, referrer_url STRING,\n                    ip STRING COMMENT 'IP Address of the User')\n    COMMENT 'This is the page view table'\n    PARTITIONED BY(dt STRING, country STRING)\n    ROW FORMAT DELIMITED\n            FIELDS TERMINATED BY '1'\n    STORED AS SEQUENCEFILE;\n```\n\n----------------------------------------\n\nTITLE: Querying Data in Hive\nDESCRIPTION: Demonstrates various HiveQL queries including SELECT, INSERT OVERWRITE, filtering, and writing results to HDFS or local directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.foo FROM invites a WHERE a.ds='2008-08-15';\n```\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT OVERWRITE DIRECTORY '/tmp/hdfs_out' SELECT a.* FROM invites a WHERE a.ds='2008-08-15';\n```\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/local_out' SELECT a.* FROM pokes a;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT OVERWRITE TABLE events SELECT a.* FROM profiles a;\nINSERT OVERWRITE TABLE events SELECT a.* FROM profiles a WHERE a.key < 100;\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/reg_3' SELECT a.* FROM events a;\nINSERT OVERWRITE DIRECTORY '/tmp/reg_4' select a.invites, a.pokes FROM profiles a;\nINSERT OVERWRITE DIRECTORY '/tmp/reg_5' SELECT COUNT(*) FROM invites a WHERE a.ds='2008-08-15';\nINSERT OVERWRITE DIRECTORY '/tmp/reg_5' SELECT a.foo, a.bar FROM invites a;\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/sum' SELECT SUM(a.pc) FROM pc1 a;\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Table with JsonSerDe in Hive SQL\nDESCRIPTION: This example shows how to create a table in Hive for storing JSON data using JsonSerDe. It includes two variations: one for Hive versions 0.12 to 2.x, and another for Hive 3.0.0 and later.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nADD JAR /usr/lib/hive-hcatalog/lib/hive-hcatalog-core.jar;\nCREATE TABLE my_table(a string, b bigint, ...)\nROW FORMAT SERDE\n'org.apache.hive.hcatalog.data.JsonSerDe'\nSTORED AS TEXTFILE;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE my_table(a string, b bigint, ...)\nROW FORMAT SERDE\n'org.apache.hadoop.hive.serde2.JsonSerDe'\nSTORED AS TEXTFILE;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE my_table(a string, b bigint, ...) STORED AS JSONFILE;\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Hive Tables Syntax\nDESCRIPTION: SQL syntax for the LOAD DATA command in Hive, used to load data files into Hive tables. The command supports loading from local or remote file systems with options for overwriting existing data and specifying partitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nLOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]\n\nLOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] [INPUTFORMAT 'inputformat' SERDE 'serde'] (3.0 or later)\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Query via JDBC to HiveServer2\nDESCRIPTION: Java code demonstrating how to create a Statement object and execute a SQL query against HiveServer2, returning results in a ResultSet object.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_39\n\nLANGUAGE: java\nCODE:\n```\nStatement stmt = cnct.createStatement();\nResultSet rset = stmt.executeQuery(\"SELECT foo FROM bar\");\n```\n\n----------------------------------------\n\nTITLE: Metastore JDBC Connection Configuration\nDESCRIPTION: Default JDBC connection configuration for Hive metastore using Derby embedded database. Includes connection URL, driver class, and basic authentication settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_37\n\nLANGUAGE: properties\nCODE:\n```\njavax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=metastore_db;create=true\njavax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver\njavax.jdo.option.ConnectionUserName=APP\njavax.jdo.option.ConnectionPassword=mine\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Index Usage in Hive\nDESCRIPTION: Controls whether to automatically use indexes for query optimization. Default is false.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_74\n\nLANGUAGE: properties\nCODE:\n```\nhive.optimize.index.filter=false\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Statistics Gathering in Hive\nDESCRIPTION: When true, enables automatic gathering and updating of statistics during Hive DML operations, except for LOAD DATA statements.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_77\n\nLANGUAGE: properties\nCODE:\n```\nhive.stats.autogather=true\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Hive Tables Syntax\nDESCRIPTION: Comprehensive syntax examples for inserting data into Hive tables from queries. Includes standard syntax, Hive extensions for multiple inserts, and dynamic partition inserts with both OVERWRITE and INTO options.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nStandard syntax:\nINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement;\n\nHive extension (multiple inserts):\nFROM from_statement\nINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1\n[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] \n[INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...;\nFROM from_statement\nINSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1\n[INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] \n[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] ...;\n\nHive extension (dynamic partition inserts):\nINSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement;\nINSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement;\n```\n\n----------------------------------------\n\nTITLE: Using stack() UDTF in Hive SQL\nDESCRIPTION: The stack() function breaks up n values into r rows, with each row having n/r columns. The number of rows (r) must be a constant value.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nstack(int r,T1  V1,...,Tn/r Vn)\n```\n\n----------------------------------------\n\nTITLE: Creating MovieLens User Ratings Table in Hive SQL\nDESCRIPTION: This snippet demonstrates how to create a table in Hive for storing MovieLens user ratings data. It defines the table structure with fields for user ID, movie ID, rating, and timestamp, using tab-delimited text file storage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/userguide_27362066.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE u_data (\n  userid INT,\n  movieid INT,\n  rating INT,\n  unixtime STRING)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: Creating Denormalized Materialized View\nDESCRIPTION: Creates a materialized view that denormalizes the star schema database contents for optimized querying\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views-in-hive_283118346.md#2025-04-09_snippet_6\n\nLANGUAGE: HiveQL\nCODE:\n```\nCREATE MATERIALIZED VIEW mv2\nAS\nSELECT <dims>,\nlo_revenue,\nlo_extendedprice * lo_discount AS d_price,\nlo_revenue - lo_supplycost\nFROM customer, dates, lineorder, part, supplier\nWHERE lo_orderdate = d_datekey\nAND lo_partkey = p_partkey\nAND lo_suppkey = s_suppkey\nAND lo_custkey = c_custkey;\n```\n\n----------------------------------------\n\nTITLE: Creating a Bucketed Sorted Table in Hive\nDESCRIPTION: This example shows how to create a bucketed and sorted table. The table is partitioned, clustered by userid, sorted by viewTime, and includes custom row format and storage specifications.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE page_view(viewTime INT, userid BIGINT,\n     page_url STRING, referrer_url STRING,\n     ip STRING COMMENT 'IP Address of the User')\n COMMENT 'This is the page view table'\n PARTITIONED BY(dt STRING, country STRING)\n CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS\n ROW FORMAT DELIMITED\n   FIELDS TERMINATED BY '\\001'\n   COLLECTION ITEMS TERMINATED BY '\\002'\n   MAP KEYS TERMINATED BY '\\003'\n STORED AS SEQUENCEFILE;\n```\n\n----------------------------------------\n\nTITLE: ANALYZE TABLE Syntax in Hive\nDESCRIPTION: Complete syntax for the ANALYZE TABLE command used to gather statistics for existing tables and partitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nANALYZE TABLE [db_name.]tablename [PARTITION(partcol1[=val1], partcol2[=val2], ...)]\n  COMPUTE STATISTICS \n  [FOR COLUMNS]          -- (Note: Hive 0.10.0 and later.)\n  [CACHE METADATA]       -- (Note: Hive 2.1.0 and later.)\n  [NOSCAN];\n```\n\n----------------------------------------\n\nTITLE: Abort Transactions Command Syntax in Hive\nDESCRIPTION: Syntax for the ABORT TRANSACTIONS command introduced in Hive 1.3.0 and 2.1.0. This command cleans up specified transaction IDs from the Hive metastore to remove dangling or failed transactions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_102\n\nLANGUAGE: sql\nCODE:\n```\nABORT TRANSACTIONS transactionID [ transactionID ...];\n```\n\n----------------------------------------\n\nTITLE: Creating Skewed Table with Multiple Columns in HiveQL\nDESCRIPTION: Creates a table 'list_bucket_multiple' with two skewed columns 'col1' and 'col2', specifying multiple skew value combinations. Optionally stores as directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE list_bucket_multiple (col1 STRING, col2 int, col3 STRING)\n  SKEWED BY (col1, col2) ON (('s1',1), ('s3',3), ('s13',13), ('s78',78)) [STORED AS DIRECTORIES];\n```\n\n----------------------------------------\n\nTITLE: Connecting to HiveServer2 using Beeline CLI\nDESCRIPTION: Demonstrates how to connect to HiveServer2 using the Beeline command-line interface, including showing tables after connection.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n% bin/beeline \nHive version 0.11.0-SNAPSHOT by Apache\nbeeline> !connect jdbc:hive2://localhost:10000 scott tiger\n!connect jdbc:hive2://localhost:10000 scott tiger \nConnecting to jdbc:hive2://localhost:10000\nConnected to: Hive (version 0.10.0)\nDriver: Hive (version 0.10.0-SNAPSHOT)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\n0: jdbc:hive2://localhost:10000> show tables;\nshow tables;\n+-------------------+\n|     tab_name      |\n+-------------------+\n| primitives        |\n| src               |\n| src1              |\n| src_json          |\n| src_sequencefile  |\n| src_thrift        |\n| srcbucket         |\n| srcbucket2        |\n| srcpart           |\n+-------------------+\n9 rows selected (1.079 seconds)\n```\n\n----------------------------------------\n\nTITLE: Creating Materialized Views\nDESCRIPTION: Syntax for creating materialized views in Hive 3.0+ with options for partitioning, clustering, and storage format specifications.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_52\n\nLANGUAGE: sql\nCODE:\n```\nCREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name\n  [DISABLE REWRITE]\n  [COMMENT materialized_view_comment]\n  [PARTITIONED ON (col_name, ...)]\n  [CLUSTERED ON (col_name, ...) | DISTRIBUTED ON (col_name, ...) SORTED ON (col_name, ...)]\n  [\n    [ROW FORMAT row_format]\n    [STORED AS file_format]\n      | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]\n  ]\n  [LOCATION hdfs_path]\n  [TBLPROPERTIES (property_name=property_value, ...)]\n  AS SELECT ...;\n```\n\n----------------------------------------\n\nTITLE: Direct Select Query\nDESCRIPTION: Shows a simple SELECT query for displaying results directly to the client.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_23\n\nLANGUAGE: hql\nCODE:\n```\nSELECT user.*\nFROM user\nWHERE user.active = 1;\n```\n\n----------------------------------------\n\nTITLE: Mixed Static and Dynamic Partition Insert in Hive\nDESCRIPTION: Example demonstrating INSERT with both static (ds) and dynamic (hr) partition columns. Static partition value is explicitly specified while dynamic partition value is derived from the data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/dynamicpartitions_27823715.md#2025-04-09_snippet_1\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE T PARTITION (ds='2010-03-03', hr) \nSELECT key, value, /*ds,*/ hr FROM srcpart WHERE ds is not null and hr>10;\n```\n\n----------------------------------------\n\nTITLE: Using posexplode() UDTF with Arrays\nDESCRIPTION: Example of the posexplode() UDTF which returns both the position/index and the element from an array. This function is useful when the original position of elements is important to preserve.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_56\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT posexplode(myCol) AS pos, myNewCol FROM myTable;\n```\n\n----------------------------------------\n\nTITLE: Creating Hive Tables\nDESCRIPTION: Demonstrates how to create simple and partitioned Hive tables using HiveQL CREATE TABLE statements.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE pokes (foo INT, bar STRING);\n```\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);\n```\n\n----------------------------------------\n\nTITLE: Creating Table with Complex Data Types in Hive\nDESCRIPTION: Creates a partitioned table 'page_view' with array and map columns, demonstrating how to define complex data types in Hive tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_7\n\nLANGUAGE: HiveQL\nCODE:\n```\nCREATE TABLE page_view(viewTime INT, userid BIGINT,\n                    page_url STRING, referrer_url STRING,\n                    friends ARRAY<BIGINT>, properties MAP<STRING, STRING>\n                    ip STRING COMMENT 'IP Address of the User')\n    COMMENT 'This is the page view table'\n    PARTITIONED BY(dt STRING, country STRING)\n    CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS\n    ROW FORMAT DELIMITED\n            FIELDS TERMINATED BY '1'\n            COLLECTION ITEMS TERMINATED BY '2'\n            MAP KEYS TERMINATED BY '3'\n    STORED AS SEQUENCEFILE;\n```\n\n----------------------------------------\n\nTITLE: Describing Extended Table Properties in Hive\nDESCRIPTION: Shows all properties of a table, including columns and additional metadata.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_12\n\nLANGUAGE: HiveQL\nCODE:\n```\nDESCRIBE EXTENDED page_view;\n```\n\n----------------------------------------\n\nTITLE: String Replace Function\nDESCRIPTION: Replaces all non-overlapping occurrences of old substring with new substring.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\nreplace(string A, string OLD, string NEW)\n```\n\n----------------------------------------\n\nTITLE: Setting Default File Format for Hive Tables\nDESCRIPTION: Configuration property to set the default file format for CREATE TABLE statements in Hive. Options include TextFile, SequenceFile, RCfile, ORC, and Parquet.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_24\n\nLANGUAGE: properties\nCODE:\n```\nhive.default.fileformat=TextFile\n```\n\n----------------------------------------\n\nTITLE: Character Translation in Hive\nDESCRIPTION: Translates input string by replacing characters present in the 'from' string with corresponding characters in the 'to' string, similar to PostgreSQL's translate function.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\ntranslate(string|char|varchar input, string|char|varchar from, string|char|varchar to)\n```\n\n----------------------------------------\n\nTITLE: Materialized View Maintenance with Scheduled Queries in Hive\nDESCRIPTION: Demonstrates setting up automated rebuilding of materialized views using scheduled queries. Includes table creation, data loading, view creation and scheduled maintenance.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/scheduled-queries_145724128.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n-- some settings...they might be there already\nset hive.support.concurrency=true;\nset hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\nset hive.strict.checks.cartesian.product=false;\nset hive.stats.fetch.column.stats=true;\nset hive.materializedview.rewriting=true;\n\n-- create some tables\nCREATE TABLE emps (\n  empid INT,\n  deptno INT,\n  name VARCHAR(256),\n  salary FLOAT,\n  hire_date TIMESTAMP)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n \nCREATE TABLE depts (\n  deptno INT,\n  deptname VARCHAR(256),\n  locationid INT)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n\n-- load data\ninsert into emps values (100, 10, 'Bill', 10000, 1000), (200, 20, 'Eric', 8000, 500),\n  (150, 10, 'Sebastian', 7000, null), (110, 10, 'Theodore', 10000, 250), (120, 10, 'Bill', 10000, 250),\n  (1330, 10, 'Bill', 10000, '2020-01-02');\ninsert into depts values (10, 'Sales', 10), (30, 'Marketing', null), (20, 'HR', 20);\n\ninsert into emps values (1330, 10, 'Bill', 10000, '2020-01-02');\n\n-- create mv\nCREATE MATERIALIZED VIEW mv1 AS\n  SELECT empid, deptname, hire_date FROM emps\n    JOIN depts ON (emps.deptno = depts.deptno)\n    WHERE hire_date >= '2016-01-01 00:00:00';\n\nEXPLAIN\nSELECT empid, deptname FROM emps\nJOIN depts ON (emps.deptno = depts.deptno)\nWHERE hire_date >= '2018-01-01';\n\n-- create a schedule to rebuild mv\ncreate scheduled query mv_rebuild cron '0 */10 * * * ? *' defined as \n  alter materialized view mv1 rebuild;\n\n-- from this expalin it will be seen that the mv1 is being used\nEXPLAIN\nSELECT empid, deptname FROM emps\nJOIN depts ON (emps.deptno = depts.deptno)\nWHERE hire_date >= '2018-01-01';\n\n-- insert a new record\ninsert into emps values (1330, 10, 'Bill', 10000, '2020-01-02');\n\n-- the source tables are scanned\nEXPLAIN\nSELECT empid, deptname FROM emps\nJOIN depts ON (emps.deptno = depts.deptno)\nWHERE hire_date >= '2018-01-01';\n\n-- wait 10 minutes or execute\nalter scheduled query mv_rebuild execute;\n\n-- run it again...the view should be rebuilt\nEXPLAIN\nSELECT empid, deptname FROM emps\nJOIN depts ON (emps.deptno = depts.deptno)\nWHERE hire_date >= '2018-01-01';\n```\n\n----------------------------------------\n\nTITLE: Writing Data to Hive using Secure Streaming Connection\nDESCRIPTION: This Java code shows how to write data to Hive using a secure streaming connection. It demonstrates beginning a transaction, writing data, and committing the transaction over a secure connection.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest_40509746.md#2025-04-09_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n///// Batch 1 - First TXN â€“ over secure connection\ntxnBatch3.beginNextTransaction();\ntxnBatch3.write(\"28,Eric Baldeschwieler\".getBytes());\ntxnBatch3.write(\"29,Ari Zilka\".getBytes());\ntxnBatch3.commit();\n\ntxnBatch3.close();\nsecureConn.close();\n```\n\n----------------------------------------\n\nTITLE: Adding and Deleting Resources using Ivy URLs in Hive\nDESCRIPTION: Syntax for managing resources using Ivy URLs which allows Hive to retrieve artifacts from Maven repositories. Introduced in Hive 1.2.0, this supports adding resources by specifying group, module, and version information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-cli_27362033.md#2025-04-09_snippet_6\n\nLANGUAGE: hive\nCODE:\n```\nADD { FILE[S] | JAR[S] | ARCHIVE[S] } <ivy://org:module:version?key=value&key=value&...> <ivy://org:module:version?key=value&key1=value1&...>*\nDELETE { FILE[S] | JAR[S] | ARCHIVE[S] } <ivy://org:module:version> <ivy://org:module:version>*\n```\n\n----------------------------------------\n\nTITLE: Recovering Partitions in Amazon EMR Hive\nDESCRIPTION: Amazon Elastic MapReduce (EMR)'s version of Hive uses an alternative syntax for the MSCK REPAIR TABLE command. This command updates the metastore with partition information from HDFS.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_39\n\nLANGUAGE: hql\nCODE:\n```\nALTER TABLE table_name RECOVER PARTITIONS;\n\n```\n\n----------------------------------------\n\nTITLE: Adding Custom HTTP Headers in Hive JDBC Connection\nDESCRIPTION: This snippet demonstrates how to include custom HTTP headers in a Hive JDBC connection URL. It allows passing additional information like authentication details to intermediate servers.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_54\n\nLANGUAGE: java\nCODE:\n```\njdbc:hive2://<host>:<port>/<db>;transportMode=http;httpPath=<http_endpoint>;http.header.<name1>=<value1>;http.header.<name2>=<value2>\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Union Types in Hive\nDESCRIPTION: Shows how to create a table with UNIONTYPE column and query it. The UNIONTYPE can hold different data types identified by a tag value indicating which part of the union is being used.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE union_test(foo UNIONTYPE<int, double, array<string>, struct<a:int,b:string>>);\nSELECT foo FROM union_test;\n\n{0:1}\n{1:2.0}\n{2:[\"three\",\"four\"]}\n{3:{\"a\":5,\"b\":\"five\"}}\n{2:[\"six\",\"seven\"]}\n{3:{\"a\":8,\"b\":\"eight\"}}\n{0:9}\n{1:10.0}\n```\n\n----------------------------------------\n\nTITLE: Creating a Denormalized Materialized View from Star Schema in Hive SQL\nDESCRIPTION: Example of creating a materialized view (mv2) that denormalizes the star schema database by joining the dimension tables with the fact table. The materialized view includes calculated columns for price discounts and revenue minus cost.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nCREATE MATERIALIZED VIEW mv2\nAS\nSELECT <dims>,\n    lo_revenue,\n    lo_extendedprice * lo_discount AS d_price,\n    lo_revenue - lo_supplycost\nFROM customer, dates, lineorder, part, supplier\nWHERE lo_orderdate = d_datekey\n    AND lo_partkey = p_partkey\n    AND lo_suppkey = s_suppkey\n    AND lo_custkey = c_custkey;\n```\n\n----------------------------------------\n\nTITLE: Aggregation Queries in Hive\nDESCRIPTION: Demonstrates different types of aggregation queries including distinct counts and multiple aggregations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_26\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE pv_gender_sum\nSELECT pv_users.gender, count (DISTINCT pv_users.userid)\nFROM pv_users\nGROUP BY pv_users.gender;\n```\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE pv_gender_agg\nSELECT pv_users.gender, count(DISTINCT pv_users.userid), count(*), sum(DISTINCT pv_users.userid)\nFROM pv_users\nGROUP BY pv_users.gender;\n```\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE pv_gender_agg\nSELECT pv_users.gender, count(DISTINCT pv_users.userid), count(DISTINCT pv_users.ip)\nFROM pv_users\nGROUP BY pv_users.gender;\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Decimal Type in Hive\nDESCRIPTION: Shows how to create a table in Hive with a Decimal data type column. Decimal types provide precise values and greater range than DOUBLE type for financial and precision-critical applications.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\ncreate table decimal_1 (t decimal);\n```\n\n----------------------------------------\n\nTITLE: Showing Views in Hive\nDESCRIPTION: Syntax for the SHOW VIEWS command which lists all views in a specified database that match an optional pattern. Introduced in Hive 2.2.0, this command supports wildcards for pattern matching and can use either IN or FROM clauses to specify the database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_69\n\nLANGUAGE: sql\nCODE:\n```\nSHOW VIEWS [IN/FROM database_name] [LIKE 'pattern_with_wildcards'];\n```\n\n----------------------------------------\n\nTITLE: CTE Usage in Views, CTAS, and Insert Statements\nDESCRIPTION: Shows how to use CTEs in different contexts including INSERT operations, CREATE TABLE AS SELECT (CTAS), and view creation with name collision examples.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/common-table-expression_38572242.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- insert example\ncreate table s1 like src;\nwith q1 as ( select key, value from src where key = '5')\nfrom q1\ninsert overwrite table s1\nselect *;\n\n-- ctas example\ncreate table s2 as\nwith q1 as ( select key from src where key = '4')\nselect * from q1;\n\n-- view example\ncreate view v1 as\nwith q1 as ( select key from src where key = '5')\nselect * from q1;\nselect * from v1;\n \n-- view example, name collision\ncreate view v1 as\nwith q1 as ( select key from src where key = '5')\nselect * from q1;\nwith q1 as ( select key from src where key = '4')\nselect * from v1;\n```\n\n----------------------------------------\n\nTITLE: Display Column Statistics\nDESCRIPTION: Commands to view column statistics for tables and partitions in Hive 0.14.0 and later versions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_96\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE FORMATTED [db_name.]table_name column_name;\nDESCRIBE FORMATTED [db_name.]table_name column_name PARTITION (partition_spec);\n```\n\n----------------------------------------\n\nTITLE: MERGE Syntax in HiveQL\nDESCRIPTION: Standard syntax for merge operations in ACID-compliant Hive tables. Available starting in Hive 2.2. Allows conditional UPDATE, DELETE, and INSERT operations based on join conditions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nStandard Syntax:\nMERGE INTO <target table> AS T USING <source expression/table> AS S\nON <boolean expression1>\nWHEN MATCHED [AND <boolean expression2>] THEN UPDATE SET <set clause list>\nWHEN MATCHED [AND <boolean expression3>] THEN DELETE\nWHEN NOT MATCHED [AND <boolean expression4>] THEN INSERT VALUES<value list>\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Replication Parameters in Source Cluster\nDESCRIPTION: This snippet shows the essential configuration parameters that need to be set up in the source cluster for Hive replication. It includes enabling the DbNotificationListener, DML events, and the ChangeManager.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivereplicationv2development_66850051.md#2025-04-09_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nhive.metastore.transactional.event.listeners = org.apache.hive.hcatalog.listener.DbNotificationListener\n\nhive.metastore.dml.events = true\n\n//\"Turn on ChangeManager, so delete files will go to cmrootdir.\"\nhive.repl.cm.enabled = true\n```\n\n----------------------------------------\n\nTITLE: JDBC Client Implementation for Apache Hive\nDESCRIPTION: Java JDBC client implementation for connecting to Hive server. Demonstrates basic operations like creating tables, loading data, and executing queries. Supports both embedded and standalone server modes via JDBC connection URL.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveclient_27362101.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport java.sql.SQLException;\nimport java.sql.Connection;\nimport java.sql.ResultSet;\nimport java.sql.Statement;\nimport java.sql.DriverManager;\n\npublic class HiveJdbcClient {\n  private static String driverName = \"org.apache.hadoop.hive.jdbc.HiveDriver\";\n\n  public static void main(String[] args) throws SQLException {\n    try {\n      Class.forName(driverName);\n    } catch (ClassNotFoundException e) {\n      // TODO Auto-generated catch block\n      e.printStackTrace();\n      System.exit(1);\n    }\n    Connection con = DriverManager.getConnection(\"jdbc:hive://localhost:10000/default\", \"\", \"\");\n    Statement stmt = con.createStatement();\n    String tableName = \"testHiveDriverTable\";\n    stmt.executeQuery(\"drop table \" + tableName);\n    ResultSet res = stmt.executeQuery(\"create table \" + tableName + \" (key int, value string)\");\n    // show tables\n    String sql = \"show tables '\" + tableName + \"'\";\n    System.out.println(\"Running: \" + sql);\n    res = stmt.executeQuery(sql);\n    if (res.next()) {\n      System.out.println(res.getString(1));\n    }\n    // describe table\n    sql = \"describe \" + tableName;\n    System.out.println(\"Running: \" + sql);\n    res = stmt.executeQuery(sql);\n    while (res.next()) {\n      System.out.println(res.getString(1) + \"\\t\" + res.getString(2));\n    }\n\n    // load data into table\n    // NOTE: filepath has to be local to the hive server\n    // NOTE: /tmp/a.txt is a ctrl-A separated file with two fields per line\n    String filepath = \"/tmp/a.txt\";\n    sql = \"load data local inpath '\" + filepath + \"' into table \" + tableName;\n    System.out.println(\"Running: \" + sql);\n    res = stmt.executeQuery(sql);\n\n    // select * query\n    sql = \"select * from \" + tableName;\n    System.out.println(\"Running: \" + sql);\n    res = stmt.executeQuery(sql);\n    while (res.next()) {\n      System.out.println(String.valueOf(res.getInt(1)) + \"\\t\" + res.getString(2));\n    }\n\n    // regular hive query\n    sql = \"select count(1) from \" + tableName;\n    System.out.println(\"Running: \" + sql);\n    res = stmt.executeQuery(sql);\n    while (res.next()) {\n      System.out.println(res.getString(1));\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: TRANSFORM with Custom SerDe and RecordReader in HiveQL\nDESCRIPTION: This example shows how to use TRANSFORM with a custom SerDe and RecordReader, demonstrating advanced configuration options for data serialization and deserialization.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-transform_27362047.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n  FROM (\n    FROM src\n    SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'\n    USING '/bin/cat'\n    AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'\n    RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'\n  ) tmap\n  INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue\n```\n\n----------------------------------------\n\nTITLE: Describe Partition Example Usage\nDESCRIPTION: Complete example showing how to use DESCRIBE commands with partitioned tables, including both extended and formatted output.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_99\n\nLANGUAGE: sql\nCODE:\n```\nhive> show partitions part_table;\nOK\nd=abc\n\nhive> DESCRIBE extended part_table partition (d='abc');\nOK\ni                       int                                         \nd                       string                                      \n                 \n# Partition Information          \n# col_name              data_type               comment             \n                 \nd                       string                                      \n                 \nDetailed Partition Information  Partition(values:[abc], dbName:default, tableName:part_table, createTime:1459382234, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:i, type:int, comment:null), FieldSchema(name:d, type:string, comment:null)], location:file:/tmp/warehouse/part_table/d=abc, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:{numFiles=1, COLUMN_STATS_ACCURATE=true, transient_lastDdlTime=1459382234, numRows=1, totalSize=2, rawDataSize=1})   \nTime taken: 0.325 seconds, Fetched: 9 row(s)\n```\n\n----------------------------------------\n\nTITLE: WHERE Clause Example in Hive\nDESCRIPTION: Shows a SELECT query with a WHERE clause that filters sales records based on amount and region conditions. The WHERE clause uses boolean expressions with comparison and logical operators.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sales WHERE amount > 10 AND region = \"US\"\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Environment Variables\nDESCRIPTION: Commands to set up HIVE_HOME environment variable and add Hive binaries to system PATH.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd hive-x.y.z\n$ export HIVE_HOME={{pwd}}\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ export PATH=$HIVE_HOME/bin:$PATH\n```\n\n----------------------------------------\n\nTITLE: Initializing gRPC Client for Hive Metastore in Java\nDESCRIPTION: This code snippet demonstrates how to instantiate and use a gRPC client for Hive Metastore. It creates a ManagedChannel, builds a blocking stub, and shows an example call to the getTable method.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/158869886.md#2025-04-09_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nManagedChannel channel = ManagedChannelBuilder.forTarget(target)\n    // Channels are secure by default (via SSL/TLS). For the example we disable TLS to avoid\n    // needing certificates.\n    .usePlaintext()\n    .build();\nHiveMetaStoreGrpc.HiveMetastore BlockingStub blockingStub = HiveMetaStoreGrpc.newBlockingStub(channel);\nblockingStub.getTable(getTableRequest);\n```\n\n----------------------------------------\n\nTITLE: Defining Avro Schema for Data Conversion\nDESCRIPTION: JSON schema definition that maps Hive data types to their corresponding Avro representations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"com.linkedin.haivvreo\",\n  \"name\": \"test_serializer\",\n  \"type\": \"record\",\n  \"fields\": [\n    { \"name\":\"string1\", \"type\":\"string\" },\n    { \"name\":\"int1\", \"type\":\"int\" },\n    { \"name\":\"tinyint1\", \"type\":\"int\" },\n    { \"name\":\"smallint1\", \"type\":\"int\" },\n    { \"name\":\"bigint1\", \"type\":\"long\" },\n    { \"name\":\"boolean1\", \"type\":\"boolean\" },\n    { \"name\":\"float1\", \"type\":\"float\" },\n    { \"name\":\"double1\", \"type\":\"double\" },\n    { \"name\":\"list1\", \"type\":{\"type\":\"array\", \"items\":\"string\"} },\n    { \"name\":\"map1\", \"type\":{\"type\":\"map\", \"values\":\"int\"} },\n    { \"name\":\"struct1\", \"type\":{\"type\":\"record\", \"name\":\"struct1_name\", \"fields\": [\n          { \"name\":\"sInt\", \"type\":\"int\" }, { \"name\":\"sBoolean\", \"type\":\"boolean\" }, { \"name\":\"sString\", \"type\":\"string\" } ] } },\n    { \"name\":\"union1\", \"type\":[\"float\", \"boolean\", \"string\"] },\n    { \"name\":\"enum1\", \"type\":{\"type\":\"enum\", \"name\":\"enum1_values\", \"symbols\":[\"BLUE\",\"RED\", \"GREEN\"]} },\n    { \"name\":\"nullableint\", \"type\":[\"int\", \"null\"] },\n    { \"name\":\"bytes1\", \"type\":\"bytes\" },\n    { \"name\":\"fixed1\", \"type\":{\"type\":\"fixed\", \"name\":\"threebytes\", \"size\":3} }\n  ] }\n```\n\n----------------------------------------\n\nTITLE: Creating Star Schema Tables with Integrity Constraints in Hive SQL\nDESCRIPTION: DDL statements for creating a star schema based on the SSB benchmark, including customer, dates, part, supplier, and lineorder tables. The schema includes primary key and foreign key constraints with DISABLE RELY to make them visible to the optimizer.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE `customer`(\n  `c_custkey` BIGINT, \n  `c_name` STRING, \n  `c_address` STRING, \n  `c_city` STRING, \n  `c_nation` STRING, \n  `c_region` STRING, \n  `c_phone` STRING, \n  `c_mktsegment` STRING,\n  PRIMARY KEY (`c_custkey`) DISABLE RELY)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n\nCREATE TABLE `dates`(\n  `d_datekey` BIGINT, \n  `d_date` STRING, \n  `d_dayofweek` STRING, \n  `d_month` STRING, \n  `d_year` INT, \n  `d_yearmonthnum` INT, \n  `d_yearmonth` STRING, \n  `d_daynuminweek` INT,\n  `d_daynuminmonth` INT,\n  `d_daynuminyear` INT,\n  `d_monthnuminyear` INT,\n  `d_weeknuminyear` INT,\n  `d_sellingseason` STRING,\n  `d_lastdayinweekfl` INT,\n  `d_lastdayinmonthfl` INT,\n  `d_holidayfl` INT,\n  `d_weekdayfl`INT,\n  PRIMARY KEY (`d_datekey`) DISABLE RELY)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n\nCREATE TABLE `part`(\n  `p_partkey` BIGINT, \n  `p_name` STRING, \n  `p_mfgr` STRING, \n  `p_category` STRING, \n  `p_brand1` STRING, \n  `p_color` STRING, \n  `p_type` STRING, \n  `p_size` INT, \n  `p_container` STRING,\n  PRIMARY KEY (`p_partkey`) DISABLE RELY)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n\nCREATE TABLE `supplier`(\n  `s_suppkey` BIGINT, \n  `s_name` STRING, \n  `s_address` STRING, \n  `s_city` STRING, \n  `s_nation` STRING, \n  `s_region` STRING, \n  `s_phone` STRING,\n  PRIMARY KEY (`s_suppkey`) DISABLE RELY)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n\nCREATE TABLE `lineorder`(\n  `lo_orderkey` BIGINT, \n  `lo_linenumber` int, \n  `lo_custkey` BIGINT not null DISABLE RELY,\n  `lo_partkey` BIGINT not null DISABLE RELY,\n  `lo_suppkey` BIGINT not null DISABLE RELY,\n  `lo_orderdate` BIGINT not null DISABLE RELY,\n  `lo_ordpriority` STRING, \n  `lo_shippriority` STRING, \n  `lo_quantity` DOUBLE, \n  `lo_extendedprice` DOUBLE, \n  `lo_ordtotalprice` DOUBLE, \n  `lo_discount` DOUBLE, \n  `lo_revenue` DOUBLE, \n  `lo_supplycost` DOUBLE, \n  `lo_tax` DOUBLE, \n  `lo_commitdate` BIGINT, \n  `lo_shipmode` STRING,\n  PRIMARY KEY (`lo_orderkey`) DISABLE RELY,\n  CONSTRAINT fk1 FOREIGN KEY (`lo_custkey`) REFERENCES `customer_n1`(`c_custkey`) DISABLE RELY,\n  CONSTRAINT fk2 FOREIGN KEY (`lo_orderdate`) REFERENCES `dates_n0`(`d_datekey`) DISABLE RELY,\n  CONSTRAINT fk3 FOREIGN KEY (`lo_partkey`) REFERENCES `ssb_part_n0`(`p_partkey`) DISABLE RELY,\n  CONSTRAINT fk4 FOREIGN KEY (`lo_suppkey`) REFERENCES `supplier_n0`(`s_suppkey`) DISABLE RELY)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n```\n\n----------------------------------------\n\nTITLE: Defining SQL Service Interface in Thrift\nDESCRIPTION: Defines the main service interface exposing all SQL operations including session management, query execution, metadata retrieval and result fetching.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-thrift-api_27843687.md#2025-04-09_snippet_4\n\nLANGUAGE: thrift\nCODE:\n```\nservice TSQLService {\n  TOpenSessionResp OpenSession(1:TOpenSessionReq req);\n  TCloseSessionResp CloseSession(1:TCloseSessionReq req);\n  TGetInfoResp GetInfo(1:TGetInfoReq req);\n  TExecuteStatementResp ExecuteStatement(1:TExecuteStatementReq req);\n  TGetTypeInfoResp GetTypeInfo(1:TGetTypeInfoReq req);\n  TGetTablesResp GetTables(1:TGetTablesReq req);\n  TGetColumnsResp GetColumns(1:TGetColumnsReq req);\n  TGetFunctionsResp GetFunctions(1:TGetFunctionsReq req);\n  TGetOperationStatusResp GetOperationStatus(1:TGetOperationStatusReq req);\n  TCancelOperationResp CancelOperation(1:TCancelOperationReq req);\n  TGetQueryPlanResp GetQueryPlan(1:TGetQueryPlanReq req);\n  TGetResultSetMetadataResp GetResultSetMetadata(1:TGetResultSetMetadataReq req);\n  TFetchResultsResp FetchResults(1:TFetchResultsReq req);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Tables with Primary and Foreign Key Constraints in HiveQL\nDESCRIPTION: Creates two tables 'pk' and 'fk' with non-validated primary and foreign key constraints. These constraints are not enforced but can be used by SQL tools for query optimization.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table pk(id1 integer, id2 integer,\n  primary key(id1, id2) disable novalidate);\n\ncreate table fk(id1 integer, id2 integer,\n  constraint c1 foreign key(id1, id2) references pk(id2, id1) disable novalidate);\n```\n\n----------------------------------------\n\nTITLE: Creating ACID Table with Custom Compaction Properties in Hive\nDESCRIPTION: Example showing how to create a transactional table with custom compaction settings including memory allocation, delta thresholds, and compaction triggers.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-transactions_40509723.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE table_name (\n  id                int,\n  name              string\n)\nCLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC\nTBLPROPERTIES (\"transactional\"=\"true\",\n  \"compactor.mapreduce.map.memory.mb\"=\"2048\",     -- specify compaction map job properties\n  \"compactorthreshold.hive.compactor.delta.num.threshold\"=\"4\",  -- trigger minor compaction if there are more than 4 delta directories\n  \"compactorthreshold.hive.compactor.delta.pct.threshold\"=\"0.5\" -- trigger major compaction if the ratio of size of delta files to\n                                                                   -- size of base files is greater than 50%\n);\n```\n\n----------------------------------------\n\nTITLE: Materialized View Management Operations in Hive\nDESCRIPTION: SQL commands for dropping materialized views, showing existing views, and describing view details.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n-- Drops a materialized view\nDROP MATERIALIZED VIEW [db_name.]materialized_view_name;\n-- Shows materialized views (with optional filters)\nSHOW MATERIALIZED VIEWS [IN database_name] ['identifier_with_wildcards'];\n-- Shows information about a specific materialized view\nDESCRIBE [EXTENDED | FORMATTED] [db_name.]materialized_view_name;\n```\n\n----------------------------------------\n\nTITLE: Optimized Multi-Table Join Query in Hive SQL\nDESCRIPTION: This query demonstrates how Hive can optimize joins over multiple tables into a single map/reduce job when the same column is used in join clauses.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)\n```\n\n----------------------------------------\n\nTITLE: Supported IN Subquery in SELECT\nDESCRIPTION: Example of a supported IN subquery that checks if the part size is equal to the maximum part size. Both correlated and uncorrelated IN subqueries will be supported in the implementation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/subqueries-in-select_68717850.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nSELECT p_size IN (\n\t\tSELECT MAX(p_size) FROM part)\nFROM part\n```\n\n----------------------------------------\n\nTITLE: Configuring GroupBy Shuffle Method in Hive on Spark\nDESCRIPTION: Property to choose between Spark's groupByKey or repartitionAndSortWithinPartitions for group by operations. GroupByKey has better performance but may use more memory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_53\n\nLANGUAGE: properties\nCODE:\n```\nhive.spark.use.groupby.shuffle = true\n```\n\n----------------------------------------\n\nTITLE: Creating a Remote Table with SSH Tunneling in Hive SQL\nDESCRIPTION: This SQL statement creates a remote table in Hive using SSH tunneling to access a remote Hive metastore. It specifies the SSH tunnel configuration, including route, private keys, and known hosts.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/80452092.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE REMOTE TABLE tbl_name\nVIA 'org.apache.hadoop.hive.metastore.SSHThriftHiveMetastoreClientFactory'\nWITH TBLPROPERTIES (\n  'hive.metastore.uris' = 'thrift://metastore.domain:9083'\n  'ssh.tunnel.route' = 'bastionuser@bastion-host.domain -> user@cluster-node.domain'\n  'ssh.tunnel.private.keys' = '/home/user/.ssh/bastionuser-key-pair.pem,/home/user/.ssh/user-key-pair.pem'\n  'ssh.tunnel.known.hosts' = '/home/user/.ssh/known_hosts'\n);\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Materialized View for Employee Data\nDESCRIPTION: Example of creating a materialized view for employee-department join data and subsequent query optimization\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views-in-hive_283118346.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE MATERIALIZED VIEW mv1\nAS\nSELECT empid, deptname, hire_date\nFROM emps JOIN depts\nON (emps.deptno = depts.deptno)\nWHERE hire_date >= '2016-01-01';\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT empid, deptname\nFROM emps\nJOIN depts\nON (emps.deptno = depts.deptno)\nWHERE hire_date >= '2018-01-01'\nAND hire_date <= '2018-03-31';\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT empid, deptname\nFROM mv1\nWHERE hire_date >= '2018-01-01'\nAND hire_date <= '2018-03-31';\n```\n\n----------------------------------------\n\nTITLE: EXPLAIN Syntax in Apache Hive\nDESCRIPTION: The basic syntax for the EXPLAIN command in Hive, showing various optional clauses that can be used to get different types of information about query execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nEXPLAIN [EXTENDED|CBO|AST|DEPENDENCY|AUTHORIZATION|LOCKS|VECTORIZATION|ANALYZE] query\n```\n\n----------------------------------------\n\nTITLE: Configuring a Hive Table with Complex Composite Row Keys using HBaseKeyFactory\nDESCRIPTION: SQL statement demonstrating how to create a Hive table with complex composite row keys using a custom HBaseKeyFactory implementation. The example uses a factory that handles fixed width fields in the row key.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\n-- Parse a row key with 3 fixed width fields each of width 10\n-- Example taken from: https://svn.apache.org/repos/asf/hive/trunk/hbase-handler/src/test/queries/positive/hbase_custom_key2.q\nCREATE TABLE hbase_ck_4(key struct<col1:string,col2:string,col3:string>, value string)\nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n    \"hbase.table.name\" = \"hbase_custom2\",\n    \"hbase.mapred.output.outputtable\" = \"hbase_custom2\",\n    \"hbase.columns.mapping\" = \":key,cf:string\",\n    \"hbase.composite.key.factory\"=\"org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory2\");\n```\n\n----------------------------------------\n\nTITLE: Basic Materialized View Management Operations in Hive\nDESCRIPTION: Common DDL operations for managing materialized views including dropping views, showing views, and describing view details\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views-in-hive_283118346.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- Drops a materialized view\nDROP MATERIALIZED VIEW [db_name.]materialized_view_name;\n-- Shows materialized views (with optional filters)\nSHOW MATERIALIZED VIEWS [IN database_name] ['identifier_with_wildcards'];\n-- Shows information about a specific materialized view\nDESCRIBE [EXTENDED | FORMATTED] [db_name.]materialized_view_name;\n```\n\n----------------------------------------\n\nTITLE: Creating a Table Using CTAS in Hive\nDESCRIPTION: This example shows how to create a new table 'new_key_value_store' using Create Table As Select (CTAS). It demonstrates custom SerDe, storage format, and data transformation in the SELECT statement.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE new_key_value_store\n   ROW FORMAT SERDE \"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe\"\n   STORED AS RCFile\n   AS\nSELECT (key % 1024) new_key, concat(key, value) key_value_pair\nFROM key_value_store\nSORT BY new_key, key_value_pair;\n```\n\n----------------------------------------\n\nTITLE: Setting Hive and Spark Configuration Properties\nDESCRIPTION: This code snippet provides recommended configuration settings for Hive and Spark to optimize performance. It includes settings for vectorized execution, cost-based optimization, join optimizations, and various other performance tuning parameters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/44302539.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\nmapreduce.input.fileinputformat.split.maxsize=750000000\nhive.vectorized.execution.enabled=true\n\nhive.cbo.enable=true\nhive.optimize.reducededuplication.min.reducer=4\nhive.optimize.reducededuplication=true\nhive.orc.splits.include.file.footer=false\nhive.merge.mapfiles=true\nhive.merge.sparkfiles=false\nhive.merge.smallfiles.avgsize=16000000\nhive.merge.size.per.task=256000000\nhive.merge.orcfile.stripe.level=true\nhive.auto.convert.join=true\nhive.auto.convert.join.noconditionaltask=true\nhive.auto.convert.join.noconditionaltask.size=894435328\nhive.optimize.bucketmapjoin.sortedmerge=false\nhive.map.aggr.hash.percentmemory=0.5\nhive.map.aggr=true\nhive.optimize.sort.dynamic.partition=false\nhive.stats.autogather=true\nhive.stats.fetch.column.stats=true\nhive.vectorized.execution.reduce.enabled=false\nhive.vectorized.groupby.checkinterval=4096\nhive.vectorized.groupby.flush.percent=0.1\nhive.compute.query.using.stats=true\nhive.limit.pushdown.memory.usage=0.4\nhive.optimize.index.filter=true\nhive.exec.reducers.bytes.per.reducer=67108864\nhive.smbjoin.cache.rows=10000\nhive.exec.orc.default.stripe.size=67108864\nhive.fetch.task.conversion=more\nhive.fetch.task.conversion.threshold=1073741824\nhive.fetch.task.aggr=false\nmapreduce.input.fileinputformat.list-status.num-threads=5\nspark.kryo.referenceTracking=false\nspark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch\n```\n\n----------------------------------------\n\nTITLE: Example Schema for Load Data Operation\nDESCRIPTION: Example of a table schema that can be used with LOAD DATA command in Hive 3.0+. This demonstrates a partitioned ORC table where the partition information can be inferred from the data file structure during loading.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE tab1 (col1 int, col2 int) PARTITIONED BY (col3 int) STORED AS ORC;\nLOAD DATA LOCAL INPATH 'filepath' INTO TABLE tab1;\n```\n\n----------------------------------------\n\nTITLE: Order By Syntax in HiveQL\nDESCRIPTION: Defines the syntax for ORDER BY clause in Hive SQL, including column ordering and null handling options. Supports both ascending and descending order with null positioning control.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sortby_27362045.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncolOrder: ( ASC | DESC )\ncolNullOrder: (NULLS FIRST | NULLS LAST)           -- (Note: Available in Hive 2.1.0 and later)\norderBy: ORDER BY colName colOrder? colNullOrder? (',' colName colOrder? colNullOrder?)*\nquery: SELECT expression (',' expression)* FROM src orderBy\n```\n\n----------------------------------------\n\nTITLE: Hive CLI Basic Usage Options\nDESCRIPTION: Command line options available in the Hive CLI including variable substitution, query execution, and connection parameters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-cli_27362033.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nusage: hive\n -d,--define <key=value>          Variable substitution to apply to Hive\n                                  commands. e.g. -d A=B or --define A=B\n -e <quoted-query-string>         SQL from command line\n -f <filename>                    SQL from files\n -H,--help                        Print help information\n -h <hostname>                    Connecting to Hive Server on remote host\n    --hiveconf <property=value>   Use value for given property\n    --hivevar <key=value>         Variable substitution to apply to hive\n                                  commands. e.g. --hivevar A=B\n -i <filename>                    Initialization SQL file\n -p <port>                        Connecting to Hive Server on port number\n -S,--silent                      Silent mode in interactive shell\n -v,--verbose                     Verbose mode (echo executed SQL to the\n                                  console)\n```\n\n----------------------------------------\n\nTITLE: Altering Database Properties and Settings in Hive SQL\nDESCRIPTION: SQL commands for modifying database properties, owner, and location settings in Hive. Includes options for setting database properties, changing ownership, and modifying location paths for both regular and managed locations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);\n\nALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;\n\nALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path;\n\nALTER (DATABASE|SCHEMA) database_name SET MANAGEDLOCATION hdfs_path;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive SAML Authentication Properties\nDESCRIPTION: XML configuration properties for enabling and configuring SAML 2.0 authentication in hive-site.xml. Includes settings for IDP metadata, entity IDs, group attributes, and callback URLs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/170266662.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>  \n  <name>hive.server2.authentication</name>  \n  <value>SAML</value>  \n</property>\n```\n\n----------------------------------------\n\nTITLE: Revoking Object Privileges in Hive\nDESCRIPTION: Revokes specific privileges on tables or views from users or roles, with option to revoke just grant option.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nREVOKE [GRANT OPTION FOR]\n    priv_type [, priv_type ] ...\n    ON table_or_view_name\n    FROM principal_specification [, principal_specification] ... ;\n\nprincipal_specification\n  : USER user\n  | ROLE role\n \npriv_type\n  : INSERT | SELECT | UPDATE | DELETE | ALL\n```\n\n----------------------------------------\n\nTITLE: Valid GROUP BY Query with Aggregation in Hive\nDESCRIPTION: Shows a correct GROUP BY query that includes the grouping column and an aggregation function in the SELECT clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT\n   a,\n   sum(b)\nFROM\n   t1\nGROUP BY\n   a;\n```\n\n----------------------------------------\n\nTITLE: Creating a Druid-Stored Materialized View in Hive\nDESCRIPTION: Example of creating a materialized view stored in Druid using a custom storage handler.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE MATERIALIZED VIEW druid_wiki_mv\nSTORED AS 'org.apache.hadoop.hive.druid.DruidStorageHandler'\nAS\nSELECT __time, page, user, c_added, c_removed\nFROM src;\n```\n\n----------------------------------------\n\nTITLE: Creating Materialized View with Storage Handler in Hive\nDESCRIPTION: Example of creating a materialized view in Hive using Druid storage handler to store data externally\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views-in-hive_283118346.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE MATERIALIZED VIEW druid_wiki_mv\nSTORED AS 'org.apache.hadoop.hive.druid.DruidStorageHandler'\nAS\nSELECT __time, page, user, c_added, c_removed\nFROM src;\n```\n\n----------------------------------------\n\nTITLE: Multiple MapJoins on Different Keys in Hive SQL\nDESCRIPTION: This complex SQL query demonstrates an attempt to perform multiple mapjoins on different keys. However, this specific query is not supported in Hive. It joins a big table with two small tables using MAPJOIN hints, but such nested mapjoins are not allowed. The query is provided as an example of what not to do.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_20\n\nLANGUAGE: HQL\nCODE:\n```\nselect /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM\n  ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM\n    bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)                                                   \n  ) firstjoin                                                             \n  JOIN                                                                  \n  smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo)\n```\n\n----------------------------------------\n\nTITLE: Adding JAR Resources with Exclusions in Hive\nDESCRIPTION: Examples of adding JAR resources using Ivy URLs with exclusion parameters to control which dependencies are included. The first example excludes Avro from Hadoop, and the second additionally disables transitive dependencies.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-cli_27362033.md#2025-04-09_snippet_8\n\nLANGUAGE: hive\nCODE:\n```\nhive>ADD JAR ivy://org.apache.pig:pig:0.10.0?exclude=org.apache.hadoop:avro;\nhive>ADD JAR ivy://org.apache.pig:pig:0.10.0?exclude=org.apache.hadoop:avro&transitive=false;\n```\n\n----------------------------------------\n\nTITLE: INSERT...VALUES Syntax in HiveQL\nDESCRIPTION: Standard syntax for inserting values directly into Hive tables. Supports both direct table insertion and partition specification. Available starting in Hive 0.14.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nStandard Syntax:\nINSERT INTO TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...]\n \nWhere values_row is:\n( value [, value ...] )\nwhere a value is either null or any valid SQL literal\n```\n\n----------------------------------------\n\nTITLE: Querying an Accumulo-Backed Hive Table\nDESCRIPTION: Demonstrates a simple SELECT query on an Accumulo-backed Hive table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/accumulointegration_46633569.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from accumulo_table;\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Stripe Size\nDESCRIPTION: Property to set the default ORC stripe size in bytes. This affects the size of data chunks written to ORC files.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_30\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.orc.default.stripe.size=67108864\n```\n\n----------------------------------------\n\nTITLE: Creating a Materialized View for Employee Data in Hive\nDESCRIPTION: Example of creating a materialized view joining employee and department data with a date filter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE MATERIALIZED VIEW mv1\nAS\nSELECT empid, deptname, hire_date\nFROM emps JOIN depts\n  ON (emps.deptno = depts.deptno)\nWHERE hire_date >= '2016-01-01';\n```\n\n----------------------------------------\n\nTITLE: AES Encryption and Decryption in Hive SQL\nDESCRIPTION: Shows how to use aes_encrypt() and aes_decrypt() functions in Hive SQL for encrypting and decrypting data using AES algorithm with various key lengths.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_81\n\nLANGUAGE: SQL\nCODE:\n```\nbase64(aes_encrypt('ABC', '1234567890123456'))\n```\n\nLANGUAGE: SQL\nCODE:\n```\naes_decrypt(unbase64('y6Ss+zCYObpCbgfWfyNWTw=='), '1234567890123456')\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom UDF in Java for Apache Hive\nDESCRIPTION: This snippet demonstrates how to create a custom UDF class in Java that extends the UDF interface. The example shows a 'Lower' function that converts text to lowercase.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveplugins_27362098.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npackage com.example.hive.udf;\n\nimport org.apache.hadoop.hive.ql.exec.UDF;\nimport org.apache.hadoop.io.Text;\n\npublic final class Lower extends UDF {\n  public Text evaluate(final Text s) {\n    if (s == null) { return null; }\n    return new Text(s.toString().toLowerCase());\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Local Scratch Directory\nDESCRIPTION: Configures the scratch space for Hive jobs when running in local mode.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.local.scratchdir=/tmp/${user.name}\n```\n\n----------------------------------------\n\nTITLE: Creating Consistent Hash for Masked Data in Hive\nDESCRIPTION: Returns a hashed value based on the input string. The hash is consistent and can be used to join masked values together across tables. Returns null for non-string types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_46\n\nLANGUAGE: sql\nCODE:\n```\nmask_hash(string|char|varchar str)\n```\n\n----------------------------------------\n\nTITLE: Using json_tuple() UDTF in Hive SQL\nDESCRIPTION: The json_tuple() function takes a JSON string and a set of n keys, and returns a tuple of n values. This is more efficient than using get_json_object() UDF for multiple keys as it can extract multiple values in a single call.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\njson_tuple(string jsonStr, string k1,...,string kn)\n```\n\n----------------------------------------\n\nTITLE: Connecting to HiveServer2 in NoSASL Mode\nDESCRIPTION: Demonstrates how to connect to HiveServer2 using Beeline in NoSASL authentication mode.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n% bin/beeline  \nbeeline> !connect jdbc:hive2://<host>:<port>/<db>;auth=noSasl hiveuser pass \n```\n\n----------------------------------------\n\nTITLE: Creating an ACID Table in Hive SQL\nDESCRIPTION: SQL command to create a transactional table in Hive. This example demonstrates the syntax for creating a bucketed ORC table with ACID properties enabled.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118453.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE acid_table (\n  id INT,\n  name STRING\n)\nCLUSTERED BY (id) INTO 2 BUCKETS\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n```\n\n----------------------------------------\n\nTITLE: Creating Tables for Materialized View Example in Hive\nDESCRIPTION: DDL statements to create example tables for demonstrating materialized view usage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE emps (\n  empid INT,\n  deptno INT,\n  name VARCHAR(256),\n  salary FLOAT,\n  hire_date TIMESTAMP)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n\nCREATE TABLE depts (\n  deptno INT,\n  deptname VARCHAR(256),\n  locationid INT)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n```\n\n----------------------------------------\n\nTITLE: Creating External HBase-Backed Hive Table\nDESCRIPTION: SQL command to create an external Hive table that connects to an existing HBase table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE hbase_table_2(key int, value string) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \"cf1:val\")\nTBLPROPERTIES(\"hbase.table.name\" = \"some_existing_table\", \"hbase.mapred.output.outputtable\" = \"some_existing_table\");\n```\n\n----------------------------------------\n\nTITLE: Creating an External Table in Hive\nDESCRIPTION: This example demonstrates how to create an external table that points to an HDFS location for storage. It includes custom field terminators and is stored as a text file.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE EXTERNAL TABLE page_view(viewTime INT, userid BIGINT,\n     page_url STRING, referrer_url STRING,\n     ip STRING COMMENT 'IP Address of the User',\n     country STRING COMMENT 'country of origination')\n COMMENT 'This is the staging page view table'\n ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\054'\n STORED AS TEXTFILE\n LOCATION '<hdfs_location>';\n```\n\n----------------------------------------\n\nTITLE: Creating JDBC Connection to HiveServer2\nDESCRIPTION: Java code that creates a JDBC connection to HiveServer2 with hostname, port, username and password. The default port is 10000 and in non-secure mode the password is ignored.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_37\n\nLANGUAGE: java\nCODE:\n```\nConnection cnct = DriverManager.getConnection(\"jdbc:hive2://<host>:<port>\", \"<user>\", \"<password>\");\n```\n\n----------------------------------------\n\nTITLE: Creating a Postgres Connector in Hive SQL\nDESCRIPTION: Example of creating a connector for PostgreSQL in Hive with username and password properties. This connector can be used to map remote PostgreSQL databases to Hive databases.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/data-connectors-in-hive_177049669.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE CONNECTOR pg_local TYPE 'postgres' URL 'jdbc:<postgresql://localhost:5432>' WITH DCPROPERTIES (\"hive.sql.dbcp.username\"=\"postgres\", \"hive.sql.dbcp.password\"=\"postgres\");\n```\n\n----------------------------------------\n\nTITLE: Querying Hive Function Documentation\nDESCRIPTION: Commands to display available functions and their documentation in Hive CLI or Beeline. These commands help users get information about specific functions and their usage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSHOW FUNCTIONS;\nDESCRIBE FUNCTION <function_name>;\nDESCRIBE FUNCTION EXTENDED <function_name>;\n```\n\n----------------------------------------\n\nTITLE: CTAS with Mixed Static and Dynamic Partitions\nDESCRIPTION: Example demonstrating CTAS with a static value for one partition column (ds) while keeping another column (hr) dynamic.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/dynamicpartitions_27823715.md#2025-04-09_snippet_5\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE T (key int, value string) PARTITIONED BY (ds string, hr int) AS \nSELECT key, value, \"2010-03-03\", hr+1 hr1 FROM srcpart WHERE ds is not null and hr>10;\n```\n\n----------------------------------------\n\nTITLE: Creating JDBC Connection to HiveServer2 in Non-Secure Mode\nDESCRIPTION: Java code showing how to connect to HiveServer2 in non-secure mode where the password field is ignored but must be provided as an empty string.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_38\n\nLANGUAGE: java\nCODE:\n```\nConnection cnct = DriverManager.getConnection(\"jdbc:hive2://<host>:<port>\", \"<user>\", \"\");\n```\n\n----------------------------------------\n\nTITLE: Implicit Join Notation Example in Hive 0.13.0+\nDESCRIPTION: Example of implicit join notation introduced in Hive 0.13.0, which allows joining tables using a comma-separated list in the FROM clause without the JOIN keyword.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *   \n FROM table1 t1, table2 t2, table3 t3   \n WHERE t1.id = t2.id AND t2.id = t3.id AND t1.zipcode = '02535';\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Threads for Input Listing in Hive XML\nDESCRIPTION: Configures the maximum number of threads that Hive will use to list file information from file systems, such as file size and number of files per table. Recommended to be greater than 1 for blobstore systems.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_116\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.exec.input.listing.max.threads</name>\n  <value>0</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Creating External Hive Table with Accumulo Backend\nDESCRIPTION: Creates an external table that references an existing Accumulo table, allowing independent lifecycle management.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/accumulointegration_46633569.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE countries(key string, name string, country string, country_id int)\nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES (\"accumulo.columns.mapping\" = \":rowID,info:name,info:country,info:country_id\");\n```\n\n----------------------------------------\n\nTITLE: Starting HiveServer in Hive 0.8 and Later\nDESCRIPTION: Command examples for starting the Thrift HiveServer in Hive 0.8 and later versions, including available options like port configuration and thread settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver_27362111.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ build/dist/bin/hive --service hiveserver --help\nusage: hiveserver\n -h,--help                        Print help information\n    --hiveconf <property=value>   Use value for given property\n    --maxWorkerThreads <arg>      maximum number of worker threads,\n                                  default:2147483647\n    --minWorkerThreads <arg>      minimum number of worker threads,\n                                  default:100\n -p <port>                        Hive Server port number, default:10000\n -v,--verbose                     Verbose mode\n\n$ bin/hive --service hiveserver\n```\n\n----------------------------------------\n\nTITLE: System Commands in Beeline\nDESCRIPTION: Utility commands for reloading JARs, executing DFS commands, and running queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_8\n\nLANGUAGE: hql\nCODE:\n```\nreload;\ndfs <dfs command>;\n<query string>;\n```\n\n----------------------------------------\n\nTITLE: Altering Connectors in Hive SQL\nDESCRIPTION: Commands for modifying existing connector properties, URL, and ownership. Allows updating connection properties, changing datasource URL, and transferring connector ownership.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nALTER CONNECTOR connector_name SET DCPROPERTIES (property_name=property_value, ...);\n\nALTER CONNECTOR connector_name SET URL new_url;\n\nALTER CONNECTOR connector_name SET OWNER [USER|ROLE] user_or_role;\n```\n\n----------------------------------------\n\nTITLE: Percentile Calculations in Hive SQL\nDESCRIPTION: Functions to calculate exact and approximate percentiles for columns in a group, with options for multiple percentiles and accuracy control.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\npercentile(bigint col, p)\npercentile(bigint col, array(p1 [, p2]...))\npercentile_approx(double col, p [, B])\npercentile_approx(double col, array(p1 [, p2]...) [, B])\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Fetch Size for Hive Connection\nDESCRIPTION: This snippet shows how to set the JDBC fetch size in the connection string for a Hive JDBC connection. The fetch size determines the number of rows that should be fetched from the database when more rows are needed by the client.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_46\n\nLANGUAGE: sql\nCODE:\n```\njdbc:hive2://<host>:<port>/<db>;fetchsize=<value>\n```\n\n----------------------------------------\n\nTITLE: Using TRANSFORM with Map and Reduce in HiveQL\nDESCRIPTION: This example demonstrates how to use the TRANSFORM clause with custom map and reduce scripts in a Hive query, including clustering and output specification.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-transform_27362047.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\n  FROM (\n    FROM pv_users\n    MAP pv_users.userid, pv_users.date\n    USING 'map_script'\n    AS dt, uid\n    CLUSTER BY dt) map_output\n  INSERT OVERWRITE TABLE pv_users_reduced\n    REDUCE map_output.dt, map_output.uid\n    USING 'reduce_script'\n    AS date, count;\n  FROM (\n    FROM pv_users\n    SELECT TRANSFORM(pv_users.userid, pv_users.date)\n    USING 'map_script'\n    AS dt, uid\n    CLUSTER BY dt) map_output\n  INSERT OVERWRITE TABLE pv_users_reduced\n    SELECT TRANSFORM(map_output.dt, map_output.uid)\n    USING 'reduce_script'\n    AS date, count;\n```\n\n----------------------------------------\n\nTITLE: Lateral View Syntax in Hive HQL\nDESCRIPTION: Defines the syntax for lateral view statements in Hive. It shows how to construct a lateral view using a user-defined table generating function (udtf) with expressions and how to incorporate it into a FROM clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lateralview_27362040.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nlateralView: LATERAL VIEW udtf(expression) tableAlias AS columnAlias (',' columnAlias)*\nfromClause: FROM baseTable (lateralView)*\n\n```\n\n----------------------------------------\n\nTITLE: Creating HBase-Backed Hive Table\nDESCRIPTION: SQL command to create a new Hive table that stores data in HBase, including column mapping and table properties configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE hbase_table_1(key int, value string) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf1:val\")\nTBLPROPERTIES (\"hbase.table.name\" = \"xyz\", \"hbase.mapred.output.outputtable\" = \"xyz\");\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Test Instances Simultaneously\nDESCRIPTION: Example of running multiple test instances as the same user by setting unique HIVE_PTEST_SUFFIX environment variables for each run.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nHIVE_PTEST_SUFFIX=first_run hive_repo/testutils/ptest/hivetest.py --test &\nHIVE_PTEST_SUFFIX=second_run hive_repo/testutils/ptest/hivetest.py --test &\n\n```\n\n----------------------------------------\n\nTITLE: Join Operations in Hive\nDESCRIPTION: Shows various types of joins including inner join, full outer join, and left semi join.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_25\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE pv_users\nSELECT pv.*, u.gender, u.age\nFROM user u JOIN page_view pv ON (pv.userid = u.id)\nWHERE pv.date = '2008-03-03';\n```\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE pv_users\nSELECT pv.*, u.gender, u.age\nFROM user u FULL OUTER JOIN page_view pv ON (pv.userid = u.id)\nWHERE pv.date = '2008-03-03';\n```\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE pv_users\nSELECT u.*\nFROM user u LEFT SEMI JOIN page_view pv ON (pv.userid = u.id)\nWHERE pv.date = '2008-03-03';\n```\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE pv_friends\nSELECT pv.*, u.gender, u.age, f.friends\nFROM page_view pv JOIN user u ON (pv.userid = u.id) JOIN friend_list f ON (u.id = f.uid)\nWHERE pv.date = '2008-03-03';\n```\n\n----------------------------------------\n\nTITLE: Python Mapper Script for Weekday Extraction\nDESCRIPTION: Python script (weekday_mapper.py) that transforms timestamps in the data to weekday numbers. This mapper script reads tab-delimited data, converts Unix timestamps to weekday integers (1-7), and outputs the transformed data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport datetime\n\nfor line in sys.stdin:\n  line = line.strip()\n  userid, movieid, rating, unixtime = line.split('\\t')\n  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()\n  print '\\t'.join([userid, movieid, rating, str(weekday)])\n```\n\n----------------------------------------\n\nTITLE: Basic CAST Format Syntax\nDESCRIPTION: Shows the basic syntax for casting between datetime/string types using format templates.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/122917025.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCAST(<timestamp/date> AS <varchar/char/string> [FORMAT <template>])  \nCAST(<varchar/char/string> AS <timestamp/date> [FORMAT <template>])\n```\n\n----------------------------------------\n\nTITLE: Creating Tables with Various Constraints in HiveQL\nDESCRIPTION: Creates tables with UNIQUE, NOT NULL, DEFAULT, and CHECK constraints. All constraints except UNIQUE are enforced. DEFAULT is not supported for complex data types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table constraints1(id1 integer UNIQUE disable novalidate, id2 integer NOT NULL, \n  usr string DEFAULT current_user(), price double CHECK (price > 0 AND price <= 1000));\n\ncreate table constraints2(id1 integer, id2 integer,\n  constraint c1_unique UNIQUE(id1) disable novalidate);\n\ncreate table constraints3(id1 integer, id2 integer,\n  constraint c1_check CHECK(id1 + id2 > 0));\n```\n\n----------------------------------------\n\nTITLE: Creating Druid Table from Select Query (CTAS)\nDESCRIPTION: Creates a Druid table using a Create Table As Select (CTAS) statement, allowing data preprocessing and column mapping.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE druid_table_1\nSTORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'\nAS\n<select `timecolumn` as `__time`, `dimension1`, `dimension2`, `metric1`, `metric2`....>;\n```\n\n----------------------------------------\n\nTITLE: Setting ZooKeeper ACL for Hive Token Store in XML\nDESCRIPTION: Configures the Access Control List (ACL) for Hive delegation token store entries in ZooKeeper. Lists all server principals for the cluster, separated by commas.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_101\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.cluster.delegation.token.store.zookeeper.acl</name>\n  <value>sasl:hive/host1@EXAMPLE.COM:cdrwa,sasl:hive/host2@EXAMPLE.COM:cdrwa</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: LIMIT with Offset Parameter\nDESCRIPTION: Shows how to use LIMIT with both offset and row count parameters to implement pagination. The example retrieves customers 3 through 7 when ordered by creation date.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM customers ORDER BY create_date LIMIT 2,5\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequence File Storage with Compression in Hive\nDESCRIPTION: Creates two tables - one for initial data load and another using SequenceFile format for better compression and distribution. Includes compression configuration settings and data transfer between tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/compressedstorage_27362073.md#2025-04-09_snippet_1\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE raw (line STRING)\n   ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n';\n\nCREATE TABLE raw_sequence (line STRING)\n   STORED AS SEQUENCEFILE;\n\nLOAD DATA LOCAL INPATH '/tmp/weblogs/20090603-access.log.gz' INTO TABLE raw;\n\nSET hive.exec.compress.output=true;\nSET io.seqfile.compression.type=BLOCK; -- NONE/RECORD/BLOCK (see below)\nINSERT OVERWRITE TABLE raw_sequence SELECT * FROM raw;\n```\n\n----------------------------------------\n\nTITLE: Creating a Transactional Hive Table for Streaming\nDESCRIPTION: This SQL snippet shows how to create a Hive table that supports streaming. The table is partitioned, clustered, and uses ORC format which is required for streaming.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest_40509746.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table alerts ( id int , msg string )\n     partitioned by (continent string, country string)\n     clustered by (id) into 5 buckets\n     stored as orc tblproperties(\"transactional\"=\"true\");\n```\n\n----------------------------------------\n\nTITLE: Setting Compaction Worker Pool for Database/Table in Apache Hive\nDESCRIPTION: This snippet shows how to assign databases, tables, and partitions to compaction pools using a table property. This property is used by the Initiator when creating compaction requests.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/compaction-pooling_240884493.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nhive.compactor.worker.pool={pool_name}\n```\n\n----------------------------------------\n\nTITLE: Creating a Table Like Another Table in Hive\nDESCRIPTION: This example demonstrates how to create an empty table with the same schema as an existing table using the CREATE TABLE LIKE syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE empty_key_value_store\nLIKE key_value_store [TBLPROPERTIES (property_name=property_value, ...)];\n```\n\n----------------------------------------\n\nTITLE: Running Hive Components\nDESCRIPTION: Commands to run various Hive components including CLI, HiveServer2, Beeline, HCatalog, and WebHCat.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ $HIVE_HOME/bin/schematool -dbType <db type> -initSchema\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ $HIVE_HOME/bin/hiveserver2\n\n$ $HIVE_HOME/bin/beeline -u jdbc:hive2://$HS2_HOST:$HS2_PORT\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ $HIVE_HOME/bin/beeline -u jdbc:hive2://\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ $HIVE_HOME/hcatalog/sbin/hcat_server.sh\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ $HIVE_HOME/hcatalog/bin/hcat\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ $HIVE_HOME/hcatalog/sbin/webhcat_server.sh\n```\n\n----------------------------------------\n\nTITLE: LIMIT with ORDER BY\nDESCRIPTION: Demonstrates combining LIMIT with ORDER BY to retrieve a specific number of rows in a particular order. The example returns the first 5 customers by creation date.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM customers ORDER BY create_date LIMIT 5\n```\n\n----------------------------------------\n\nTITLE: Statistical Functions in Hive SQL\nDESCRIPTION: Various statistical functions including variance, standard deviation, covariance, and correlation calculations for numeric columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nvariance(col), var_pop(col)\nvar_samp(col)\nstddev_pop(col)\nstddev_samp(col)\ncovar_pop(col1, col2)\ncovar_samp(col1, col2)\ncorr(col1, col2)\n```\n\n----------------------------------------\n\nTITLE: Connecting to a Database with Beeline\nDESCRIPTION: Basic command line options for connecting to a database using Beeline, including database URL, username, and password parameters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nbeeline -u db_URL\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline -r\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline -n valid_user\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline -p valid_password\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline -p\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline -d driver_class\n```\n\n----------------------------------------\n\nTITLE: Including ACID Tables in Hive Replication Dumps in XML\nDESCRIPTION: Determines whether replication dump should include information about ACID tables. Used in conjunction with hive.repl.dump.metadata.only for copying ACID table metadata without transaction semantics.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_111\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.repl.dump.include.acid.tables</name>\n  <value>false</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Parameters for Direct LZO File Creation\nDESCRIPTION: Hive query parameters for Option 1, which directly creates LZO files as the output of a Hive query. This sets the compression codec and enables output compression.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lzo_33298193.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nSET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzoCodec\nSET hive.exec.compress.output=true\nSET mapreduce.output.fileoutputformat.compress=true\n```\n\n----------------------------------------\n\nTITLE: Hive Example Showing Key Uniqueness Differences with Regular Tables\nDESCRIPTION: SQL statements demonstrating how Hive preserves duplicate keys when copying data between regular Hive tables. This example shows that duplicates are maintained when working with standard Hive tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE pokes2(foo INT, bar STRING);\nINSERT OVERWRITE TABLE pokes2 SELECT * FROM pokes;\n-- this will return 3\nSELECT COUNT(1) FROM POKES WHERE foo=498;\n-- this will also return 3\nSELECT COUNT(1) FROM pokes2 WHERE foo=498;\n```\n\n----------------------------------------\n\nTITLE: Automated Table Analysis with Scheduled Queries in Hive\nDESCRIPTION: Shows how to set up periodic analysis of an external table using scheduled queries. Includes table creation, data loading, and automated statistics computation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/scheduled-queries_145724128.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n-- create external table\ncreate external table t (a integer);\n\n-- see where the table lives:\ndesc formatted t;\n\n-- in a terminal; load some data into the table directory:\nseq 1 10 > /data/hive/warehouse/t/f1\n\n-- back in hive you will see that \nselect count(1) from t;\n\ncreate scheduled query t_analyze cron '0 */1 * * * ? *' as analyze table t compute statistics for columns;\n\n-- wait some time or execute by issuing:\nalter scheduled query t_analyze execute;\n\nselect * from information_schema.scheduled_executions s where schedule_name='ex_analyze' order by scheduled_execution_id desc limit 3;\n\n-- and the numrows have been updated\ndesc formatted t;\n\n-- we don't want this running every minute anymore...\nalter scheduled query t_analyze disable;\n```\n\n----------------------------------------\n\nTITLE: Querying with Auto Join Conversion in Hive SQL\nDESCRIPTION: This SQL query demonstrates how auto join conversion simplifies the query syntax by removing the need for explicit map-join hints. The joins will be automatically converted to map-joins if the tables fit within the configured size limit.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joinoptimization_33293167.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect count(*) from\nstore_sales \njoin time_dim on (ss_sold_time_sk = t_time_sk)\njoin date_dim on (ss_sold_date_sk = d_date_sk)\nwhere t_hour = 8 and d_year = 2002\n```\n\n----------------------------------------\n\nTITLE: Altering Decimal Columns in Specific Partitions in Hive SQL\nDESCRIPTION: Demonstrates how to alter decimal columns in specific partitions of a table. This approach alters one partition at a time.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE foo PARTITION (ds='2008-04-08', hr=11) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);\nALTER TABLE foo PARTITION (ds='2008-04-08', hr=12) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);\n...\n```\n\n----------------------------------------\n\nTITLE: Creating Remote Databases with Postgres Connectors in Hive SQL\nDESCRIPTION: Examples of creating remote databases in Hive using PostgreSQL connectors. Shows both cleartext password approach and using a keystore for securing credentials when connecting to remote PostgreSQL databases.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/data-connectors-in-hive_177049669.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE REMOTE DATABASE pg_hive_testing USING pg_local WITH DBPROPERTIES (\"connector.remoteDbName\"=\"hive_hms_testing\");\n\n// USING keystore instead of cleartext passwords in DCPROPERTIES\nCREATE CONNECTOR pg_local_ks TYPE 'postgres' URL 'jdbc:<postgresql://localhost:5432/hive_hms_testing>' WITH DCPROPERTIES(\"hive.sql.dbcp.username\"=\"postgres\",\"hive.sql.dbcp.password.keystore\"=\"jceks://app/local/hive/secrets.jceks\" \"hive.sql.dbcp.password.key\"=\"postgres.credential\");\nCREATE REMOTE DATABASE pg_ks_local USING pg_local_ks(\"connector.remoteDbName\"=\"hive_hms_testing\");\n\n3. Use the tables in REMOTE database much like the JDBC-storagehandler based tables in hive. One big difference is that the metadata for these tables are never persisted in hive. Currently, create/alter/drop table DDLs are not supported in REMOTE databases.\n\nUSE pg_hive_testing;\nSHOW TABLES;\nDESCRIBE [formatted] <tablename>;\nSELECT <col1> from <tablename> where <filter1> and <filter2>;\n```\n\n----------------------------------------\n\nTITLE: Hive CLI Interactive Shell Commands\nDESCRIPTION: Examples of various interactive shell commands supported by the Hive CLI, including resource management, configuration, and query execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/replacing-the-implementation-of-hive-cli-using-beeline_61311909.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nhive> source /root/test.sql;\nhive> show tables;\ntest1\ntest2\nhive> exit;\nhive> quit;\nhive> set;\nhive> set hive.cli.print.header=true;\nhive> set -v;\nhive> reset;\nhive> add file /opt/a.txt;\nAdded resources: [/opt/a.txt]\nhive> list files;\n/opt/a.txt\nhive> delete file /opt/a.txt;\nhive> add jar /usr/share/vnc/classes/vncviewer.jar;\nAdded [/usr/share/vnc/classes/vncviewer.jar]to class path\nAdded resources:[/usr/share/vnc/classes/vncviewer.jar]\nhive> list jars;\n/usr/share/vnc/classes/vncviewer.jar\nhive> delete jar /usr/share/vnc/classes/vncviewer.jar;\nhive> !ls;\nbin\nconf\nhive> dfs -ls / ;\nFound 2 items\ndrwx-wx-wx  - root supergroup  0   2015-08-12 19:06 /tmp\ndrwxr-xr-x  - root supergroup  0   2015-08-12 19:43 /user\nhive> select * from pokes; \nOK\npokes.foo   pokes.bar\n238         val_238\n86          val_86\n311         val_311\nhive>source /opt/s.sql;\n```\n\n----------------------------------------\n\nTITLE: Executing a Complex Join Query Between Druid and Hive\nDESCRIPTION: This query joins data from a Druid table 'druid_table_1' with a Hive table 'hive_table_1'. It performs aggregations and ordering on the Druid side before joining with Hive data. The query showcases the ability to combine data from different storage systems in a single HiveQL query.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.channel, b.col1\nFROM\n(\n  SELECT `channel`, max(delta) as m, sum(added)\n  FROM druid_table_1\n  GROUP BY `channel`, `floor_year`(`__time`)\n  ORDER BY m DESC\n  LIMIT 1000\n) a\nJOIN\n(\n  SELECT col1, col2\n  FROM hive_table_1\n) b\nON a.channel = b.col2;\n```\n\n----------------------------------------\n\nTITLE: Creating Hive Table with Multiple Data Types\nDESCRIPTION: SQL command to create a test table that demonstrates all possible Hive data types with custom row format delimiters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE test_serializer(string1 STRING,\n                             int1 INT,\n                             tinyint1 TINYINT,\n                             smallint1 SMALLINT,\n                             bigint1 BIGINT,\n                             boolean1 BOOLEAN,\n                             float1 FLOAT,\n                             double1 DOUBLE,\n                             list1 ARRAY<STRING>,\n                             map1 MAP<STRING,INT>,\n                             struct1 STRUCT<sint:INT,sboolean:BOOLEAN,sstring:STRING>,\n                             union1 uniontype<FLOAT, BOOLEAN, STRING>,\n                             enum1 STRING,\n                             nullableint INT,\n                             bytes1 BINARY,\n                             fixed1 BINARY)\n ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY ':' MAP KEYS TERMINATED BY '#' LINES TERMINATED BY '\\n'\n STORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: Base64 Conversion Function\nDESCRIPTION: Converts binary input to base64 encoded string\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nbase64(binary bin)\n```\n\n----------------------------------------\n\nTITLE: Creating a Materialized View in Hive\nDESCRIPTION: Syntax for creating a materialized view in Hive, similar to CTAS statement. Includes options for partitioning, clustering, and storage format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name\n  [DISABLE REWRITE]\n  [COMMENT materialized_view_comment]\n  [PARTITIONED ON (col_name, ...)]\n  [CLUSTERED ON (col_name, ...) | DISTRIBUTED ON (col_name, ...) SORTED ON (col_name, ...)]\n  [\n    [ROW FORMAT row_format]\n    [STORED AS file_format]\n      | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]\n  ]\n  [LOCATION hdfs_path]\n  [TBLPROPERTIES (property_name=property_value, ...)]\nAS\n<query>;\n```\n\n----------------------------------------\n\nTITLE: EVERY Based Schedule Examples in Hive SQL\nDESCRIPTION: Alternative syntax examples for scheduling queries using the EVERY keyword\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/scheduled-queries_145724128.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nEVERY 2 MINUTES\nEVERY HOUR AT '0:07:30'\nEVERY DAY AT '11:35:30'\n```\n\n----------------------------------------\n\nTITLE: Example of Dropping a View\nDESCRIPTION: Simple example showing how to drop a specific view.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_49\n\nLANGUAGE: sql\nCODE:\n```\nDROP VIEW onion_referrers;\n```\n\n----------------------------------------\n\nTITLE: Creating Index in Hive SQL\nDESCRIPTION: SQL syntax for creating an index in Hive with various options including deferred rebuild, index properties, table specifications, partitioning, and storage formats.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/indexdev_27362104.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX index_name\nON TABLE base_table_name (col_name, ...)\nAS 'index.handler.class.name'\n[WITH DEFERRED REBUILD]\n[IDXPROPERTIES (property_name=property_value, ...)]\n[IN TABLE index_table_name]\n[PARTITIONED BY (col_name, ...)]\n[\n   [ ROW FORMAT ...] STORED AS ...\n   | STORED BY ...\n]\n[LOCATION hdfs_path]\n[TBLPROPERTIES (...)]\n[COMMENT \"index comment\"]\n```\n\n----------------------------------------\n\nTITLE: Basic Hive Partition Filter Example with AND/OR Logic\nDESCRIPTION: An example of a partition filter for a table with 'country' and 'state' partition keys, demonstrating the use of comparison operators and logical operators with nested parentheses.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/partition-filter-syntax_103092177.md#2025-04-09_snippet_0\n\nLANGUAGE: hql\nCODE:\n```\ncountry = \"USA\" AND (state = \"CA\" OR state = \"AZ\")\n```\n\n----------------------------------------\n\nTITLE: Creating Table Structure for Partition Column Statistics in Metastore\nDESCRIPTION: SQL definition for the PART_COL_STATS table which stores column statistics for partitioned tables. Includes fields for various statistics metrics specific to table partitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/column-statistics-in-hive_29131019.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE PART_COL_STATS  \n (\n CS_ID NUMBER NOT NULL,  \n PART_ID NUMBER NOT NULL,\n\nDB_NAME VARCHAR(128) NOT NULL,  \n COLUMN_NAME VARCHAR(128) NOT NULL,  \n COLUMN_TYPE VARCHAR(128) NOT NULL,  \n TABLE_NAME VARCHAR(128) NOT NULL,  \n PART_NAME VARCHAR(128) NOT NULL,\n\nLOW_VALUE RAW,  \n HIGH_VALUE RAW,  \n NUM_NULLS BIGINT,  \n NUM_DISTINCTS BIGINT,\n\nBIT_VECTOR, BLOB,  /* introduced in [HIVE-16997](https://issues.apache.org/jira/browse/HIVE-16997) in Hive 3.0.0 */\n\nAVG_COL_LEN DOUBLE,  \n MAX_COL_LEN BIGINT,  \n NUM_TRUES BIGINT,  \n NUM_FALSES BIGINT,  \n LAST_ANALYZED BIGINT NOT NULL)\n```\n\n----------------------------------------\n\nTITLE: Querying a Sample from a Bucketized Table in HiveQL\nDESCRIPTION: This example demonstrates how to select all columns from a sample of the 'source' table, specifically the 3rd bucket out of 32 buckets, using a random sampling method.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT *\nFROM source TABLESAMPLE(BUCKET 3 OUT OF 32 ON rand()) s;\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Mathematical Functions in HiveQL\nDESCRIPTION: Example of mathematical function usage in Hive queries, including rounding, logarithmic, and trigonometric operations\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_4\n\nLANGUAGE: HiveQL\nCODE:\n```\nSELECT round(2.5),                 -- returns 3.0\n       bround(2.5),                -- returns 2.0\n       floor(2.8),                -- returns 2\n       ceil(2.2),                 -- returns 3\n       exp(1),                    -- returns 2.718281828459045\n       ln(10),                    -- returns 2.302585092994046\n       log2(8),                   -- returns 3.0\n       sqrt(16),                  -- returns 4.0\n       abs(-5),                   -- returns 5.0\n       sin(pi()/2)                -- returns 1.0\n```\n\n----------------------------------------\n\nTITLE: Creating Bucketed and Sorted Table in Hive\nDESCRIPTION: Creates a partitioned table 'page_view' that is bucketed by userid and sorted by viewTime. This structure allows for efficient sampling and query processing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_6\n\nLANGUAGE: HiveQL\nCODE:\n```\nCREATE TABLE page_view(viewTime INT, userid BIGINT,\n                    page_url STRING, referrer_url STRING,\n                    ip STRING COMMENT 'IP Address of the User')\n    COMMENT 'This is the page view table'\n    PARTITIONED BY(dt STRING, country STRING)\n    CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS\n    ROW FORMAT DELIMITED\n            FIELDS TERMINATED BY '1'\n            COLLECTION ITEMS TERMINATED BY '2'\n            MAP KEYS TERMINATED BY '3'\n    STORED AS SEQUENCEFILE;\n```\n\n----------------------------------------\n\nTITLE: Simple Join Query in Hive SQL\nDESCRIPTION: This snippet demonstrates a basic join between two tables using the ON clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.* FROM a JOIN b ON (a.id = b.id)\n```\n\n----------------------------------------\n\nTITLE: Creating External Kudu Table in Hive\nDESCRIPTION: SQL command to create an external Hive table that references an existing Kudu table. Demonstrates the required STORED BY clause and TBLPROPERTIES for Kudu integration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/kudu-integration_133631955.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE kudu_table (foo INT, bar STRING, baz DOUBLE)\nSTORED BY 'org.apache.hadoop.hive.kudu.KuduStorageHandler'\nTBLPROPERTIES (\n  \"kudu.table_name\"=\"default.kudu_table\", \n  \"kudu.master_addresses\"=\"localhost:7051\"\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Execution Engine in Java\nDESCRIPTION: Sets the execution engine for Hive queries. Options are 'mr' for MapReduce (default but deprecated), 'tez' for Tez execution, or 'spark' for Spark execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nhiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, \"tez\");\n```\n\n----------------------------------------\n\nTITLE: Display and Formatting Options in Beeline\nDESCRIPTION: Command options for controlling output display, formatting, and visual appearance in Beeline.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --color=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --showHeader=false\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --headerInterval=50\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --fastConnect=false\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --autoCommit=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --verbose=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --showWarnings=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --showDbInPrompt=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --showNestedErrs=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --numberFormat=\"#,###,##0.00\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline--force=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --maxWidth=150\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --maxColumnWidth=25\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --silent=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --autosave=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --outputformat=tsv\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --truncateTable=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --delimiterForDSV=DELIMITER\n```\n\n----------------------------------------\n\nTITLE: Recovering Partitions Using MSCK REPAIR TABLE\nDESCRIPTION: Syntax for updating partition metadata in the Hive metastore for partitions that exist in HDFS but not in the metastore. Options include ADD, DROP, and SYNC PARTITIONS to add missing partitions, remove deleted partitions, or do both respectively.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_38\n\nLANGUAGE: hql\nCODE:\n```\nMSCK [REPAIR] TABLE table_name [ADD/DROP/SYNC PARTITIONS];\n\n```\n\n----------------------------------------\n\nTITLE: Managing Table Constraints in Hive\nDESCRIPTION: SQL statements for adding and removing table constraints including primary keys, foreign keys, unique constraints, and column constraints.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name ADD CONSTRAINT constraint_name PRIMARY KEY (column, ...) DISABLE NOVALIDATE;\nALTER TABLE table_name ADD CONSTRAINT constraint_name FOREIGN KEY (column, ...) REFERENCES table_name(column, ...) DISABLE NOVALIDATE RELY;\nALTER TABLE table_name ADD CONSTRAINT constraint_name UNIQUE (column, ...) DISABLE NOVALIDATE;\nALTER TABLE table_name CHANGE COLUMN column_name column_name data_type CONSTRAINT constraint_name NOT NULL ENABLE;\nALTER TABLE table_name CHANGE COLUMN column_name column_name data_type CONSTRAINT constraint_name DEFAULT default_value ENABLE;\nALTER TABLE table_name CHANGE COLUMN column_name column_name data_type CONSTRAINT constraint_name CHECK check_expression ENABLE;\n\nALTER TABLE table_name DROP CONSTRAINT constraint_name;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Table for Streaming (SQL)\nDESCRIPTION: SQL commands to create a Hive table compatible with streaming. The table must use ORC format, be bucketed, and have transactions enabled.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-streaming-mutation-api_61337025.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE stream_table (\n  col1 TYPE1,\n  col2 TYPE2\n)\nCLUSTERED BY (col1) INTO 10 BUCKETS\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n```\n\n----------------------------------------\n\nTITLE: UNION with Column Aliases\nDESCRIPTION: Shows how to use column aliases to ensure schema matching between UNION components.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-union_27362049.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nINSERT OVERWRITE TABLE target_table\n  SELECT name, id, category FROM source_table_1\n  UNION ALL\n  SELECT name, id, \"Category159\" as category FROM source_table_2\n```\n\n----------------------------------------\n\nTITLE: Scanning HBase Table with Map Columns\nDESCRIPTION: HBase shell command showing the structure of data when using the MAP type to store different columns in each row. Demonstrates how column names are stored as part of the HBase column qualifiers.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nhbase(main):012:0> scan \"hbase_table_1\"\nROW                          COLUMN+CELL                                                                      \n 100                         column=cf:val_100, timestamp=1267739509194, value=100                            \n 98                          column=cf:val_98, timestamp=1267739509194, value=98                              \n2 row(s) in 0.0080 seconds\n\n```\n\n----------------------------------------\n\nTITLE: UNION with Schema Mismatch Error Example\nDESCRIPTION: Shows an example that would fail due to schema mismatch between UNION components.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-union_27362049.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nINSERT OVERWRITE TABLE target_table\n  SELECT name, id, category FROM source_table_1\n  UNION ALL\n  SELECT name, id, \"Category159\" FROM source_table_2\n```\n\n----------------------------------------\n\nTITLE: Setting Spark as Hive Execution Engine\nDESCRIPTION: Configuration command to set Spark as the execution engine in Hive. By default, the execution engine is set to 'mr' (MapReduce).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nset hive.execution.engine=spark;\n```\n\n----------------------------------------\n\nTITLE: Creating Transactional Table in HiveQL\nDESCRIPTION: Creates a transactional table 'transactional_table_test' that supports ACID operations. The table is partitioned and stored in ORC format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TRANSACTIONAL TABLE transactional_table_test(key string, value string) PARTITIONED BY(ds string) STORED AS ORC;\n```\n\n----------------------------------------\n\nTITLE: Using histogram_numeric() to analyze user age distribution by gender\nDESCRIPTION: Query that uses histogram_numeric() to estimate the frequency distribution of user ages, grouped by gender.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statisticsanddatamining_27362058.md#2025-04-09_snippet_5\n\nLANGUAGE: hql\nCODE:\n```\nSELECT histogram_numeric(age) FROM users GROUP BY gender;\n```\n\n----------------------------------------\n\nTITLE: Creating External Hive Table Linked to Druid Datasource\nDESCRIPTION: SQL command to create an external Hive table that connects to an existing Druid datasource. The table uses DruidStorageHandler and requires specifying the Druid datasource name in table properties.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE druid_table_1\nSTORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'\nTBLPROPERTIES (\"druid.datasource\" = \"wikiticker\");\n```\n\n----------------------------------------\n\nTITLE: Using Python HiveServer2 Client\nDESCRIPTION: Example demonstrating how to connect to HiveServer2, execute queries, and fetch results using the pyhs2 driver. Shows connection setup with authentication, database selection, and basic query operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hiveserver2_30758712.md#2025-04-09_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pyhs2\n\nwith pyhs2.connect(host='localhost',\n                   port=10000,\n                   authMechanism=\"PLAIN\",\n                   user='root',\n                   password='test',\n                   database='default') as conn:\n    with conn.cursor() as cur:\n    \t#Show databases\n    \tprint cur.getDatabases()\n\n    \t#Execute query\n        cur.execute(\"select * from table\")\n \n        #Return column info from query\n        print cur.getSchema()\n\n        #Fetch table results\n        for i in cur.fetch():\n            print i\n```\n\n----------------------------------------\n\nTITLE: Reading ACID Data in Java\nDESCRIPTION: Steps to read ACID data consistently, including obtaining a transaction list, acquiring a lock, configuring OrcInputFormat, and releasing the lock.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-streaming-mutation-api_61337025.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// 1. Obtain ValidTxnList from metastore\nValidTxnList txnList = metastore.getValidTxns();\n\n// 2. Acquire lock and issue heartbeats\nLockImpl lock = new LockImpl(metastore, tableId, partitionId);\nlock.acquire();\n\n// 3. Configure OrcInputFormat and read data\nOrcInputFormat.setValidTxnList(jobConf, txnList);\n// ... configure other OrcInputFormat settings\nRecordReader reader = inputFormat.getRecordReader(split, jobConf, reporter);\n// Read records and get ROW__ID values\nAcidRecordReader acidReader = (AcidRecordReader) reader;\nRecordIdentifier recordId = acidReader.getRecordIdentifier();\n\n// 4. Release lock\nlock.release();\n```\n\n----------------------------------------\n\nTITLE: Custom MapReduce Integration in Hive\nDESCRIPTION: Demonstrates how to integrate custom mapper and reducer scripts with Hive queries using TRANSFORM clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_37\n\nLANGUAGE: sql\nCODE:\n```\nFROM (\n     FROM pv_users\n     MAP pv_users.userid, pv_users.date\n     USING 'map_script'\n     AS dt, uid\n     CLUSTER BY dt) map_output\n\nINSERT OVERWRITE TABLE pv_users_reduced\n    REDUCE map_output.dt, map_output.uid\n    USING 'reduce_script'\n    AS date, count;\n```\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport datetime\n\nfor line in sys.stdin:\n  line = line.strip()\n  userid, unixtime = line.split('\\t')\n  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()\n  print ','.join([userid, str(weekday)])\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Table for Sketch Operations in Hive\nDESCRIPTION: Creates a transactional ORC table named 'sketch_input' with integer and character columns, and inserts sample data for demonstrating sketch operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/datasketches-integration_177050456.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table sketch_input (id int, category char(1))\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n\ninsert into table sketch_input values\n  (1,'a'),(1, 'a'), (2, 'a'), (3, 'a'), (4, 'a'), (5, 'a'), (6, 'a'), (7, 'a'), (8, 'a'), (9, 'a'), (10, 'a'),\n  (6,'b'),(6, 'b'), (7, 'b'), (8, 'b'), (9, 'b'), (10, 'b'), (11, 'b'), (12, 'b'), (13, 'b'), (14, 'b'), (15, 'b')\n; \n```\n\n----------------------------------------\n\nTITLE: Required Improvements from Spark Community\nDESCRIPTION: Identifies specific improvements needed from the Spark community to support Hive integration, including Java API enhancements, thread safety improvements, and shuffle functionality.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_10\n\nLANGUAGE: markdown\nCODE:\n```\n## 3.4 Potentially Required Work from Spark\n\nDuring the course of prototyping and design, a few issues on Spark have been identified, as shown throughout the document. Potentially more, but the following is a summary of improvement that's needed from Spark community for the project:\n\n1. Job monitoring API in Java.\n2. SparkContext thread safety issue.\n3. Improve shuffle functionality and API.\n4. Potentially, Java API for extending RDD.\n```\n\n----------------------------------------\n\nTITLE: Configuring HiveServer2 Dynamic Service Discovery\nDESCRIPTION: This property enables dynamic service discovery for HiveServer2 clients using ZooKeeper. When enabled, each HiveServer2 instance registers itself with ZooKeeper upon startup.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_40\n\nLANGUAGE: properties\nCODE:\n```\nhive.server2.support.dynamic.service.discovery=false\n```\n\n----------------------------------------\n\nTITLE: CTE Select Statement Examples in Hive\nDESCRIPTION: Demonstrates various ways to use CTEs in SELECT statements, including basic queries, from-style syntax, chaining CTEs, and union operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/common-table-expression_38572242.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith q1 as ( select key from src where key = '5')\nselect *\nfrom q1;\n\n-- from style\nwith q1 as (select * from src where key= '5')\nfrom q1\nselect *;\n \n-- chaining CTEs\nwith q1 as ( select key from q2 where key = '5'),\nq2 as ( select key from src where key = '5')\nselect * from (select key from q1) a;\n \n-- union example\nwith q1 as (select * from src where key= '5'),\nq2 as (select * from src s2 where key = '4')\nselect * from q1 union all select * from q2;\n```\n\n----------------------------------------\n\nTITLE: Configuring RPC Server Address for Hive-Spark Communication\nDESCRIPTION: Sets the server address of the HiveServer2 host to be used for communication between the Hive client and remote Spark driver.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_59\n\nLANGUAGE: properties\nCODE:\n```\nhive.spark.client.rpc.server.address = hive.spark.client.rpc.server.address\n```\n\n----------------------------------------\n\nTITLE: Creating Hive Table with Multiple Columns and Families in HBase\nDESCRIPTION: Example of creating a Hive table that maps to multiple HBase column families with specific column mappings. This shows how to map three Hive columns to two different HBase column families.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE hbase_table_1(key int, value1 string, value2 int, value3 int) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n\"hbase.columns.mapping\" = \":key,a:b,a:c,d:e\"\n);\nINSERT OVERWRITE TABLE hbase_table_1 SELECT foo, bar, foo+1, foo+2 \nFROM pokes WHERE foo=98 OR foo=100;\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Data Ingestion in Hive SQL\nDESCRIPTION: This code demonstrates a pattern for incremental data ingestion in Hive. It creates source and target tables, maintains an offset table to track progress, and sets up a scheduled query to periodically load only new data that hasn't been ingested yet.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/scheduled-queries_145724128.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\ndrop table if exists t;\ndrop table if exists s;\n\n-- suppose that this table is an external table or something\n-- which supports the pushdown of filter condition on the id column\ncreate table s(id integer, cnt integer);\n\n-- create an internal table and an offset table\ncreate table t(id integer, cnt integer);\ncreate table t_offset(offset integer);\ninsert into t_offset values(0);\n\n-- pretend that data is added to s\ninsert into s values(1,1);\n\n-- run an ingestion...\nfrom (select id==offset as first,* from s\njoin t_offset on id>=offset) s1\ninsert into t select id,cnt where first = false\ninsert overwrite table t_offset select max(s1.id);\n\n-- configure to run ingestion every 10 minutes\ncreate scheduled query ingest every 10 minutes defined as\nfrom (select id==offset as first,* from s\njoin t_offset on id>=offset) s1\ninsert into t select id,cnt where first = false\ninsert overwrite table t_offset select max(s1.id);\n\n-- add some new values\ninsert into s values(2,2),(3,3);\n\n-- pretend that a timeout have happened\nalter scheduled query ingest execute;\n```\n\n----------------------------------------\n\nTITLE: Querying Iceberg Metadata Tables\nDESCRIPTION: Demonstrates how to access Iceberg's metadata tables using the specialized syntax to examine table history, files, snapshots, and other metadata.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nENTRIES                        SELECT * from db_name.tbl_name.entries;\nFILES                          SELECT * from db_name.tbl_name.files;\nHISTORY                        SELECT * from db_name.tbl_name.history;\nSNAPSHOTS                      SELECT * from db_name.tbl_name.snapshots;\nMANIFESTS                      SELECT * from db_name.tbl_name.manifests;\nPARTITIONS                     SELECT * from db_name.tbl_name.partitions;\nALL_DATA_FILES                 SELECT * from db_name.tbl_name.all_data_files;\nALL_MANIFESTS                  SELECT * from db_name.tbl_name.all_manifests;\nALL_ENTRIES                    SELECT * from db_name.tbl_name.all_entries;\nDATA_FILES                     SELECT * from db_name.tbl_name.data_files;\nDELETE_FILES                   SELECT * from db_name.tbl_name.delete_files;\nMETADATA_LOG_ENTRIES           SELECT * from db_name.tbl_name.metadata_log_entries;\nREFS                           SELECT * from db_name.tbl_name.refs;\nALL_DELETE_FILES               SELECT * from db_name.tbl_name.all_delete_files;\nALL_FILES                      SELECT * from db_name.tbl_name.all_files;\n```\n\n----------------------------------------\n\nTITLE: Using MAPJOIN Hint in Hive SQL\nDESCRIPTION: Example demonstrating how to explicitly use a MAPJOIN hint to optimize a join between store_sales and time_dim tables. This approach was used before automatic join optimization.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joinoptimization_33293167.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect /*+ MAPJOIN(time_dim) */ count(*) from\nstore_sales join time_dim on (ss_sold_time_sk = t_time_sk)\n```\n\n----------------------------------------\n\nTITLE: Left Outer Join Query in Hive SQL\nDESCRIPTION: This example shows a left outer join query that returns all rows from the left table, even if there's no match in the right table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key)\n```\n\n----------------------------------------\n\nTITLE: Defining Join Syntax in Apache Hive SQL\nDESCRIPTION: Formal syntax definition for join operations in Hive SQL, showing the supported join types (INNER, LEFT/RIGHT/FULL OUTER, LEFT SEMI, CROSS), table reference syntax, and join condition format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\njoin_table:\n    table_reference [INNER] JOIN table_factor [join_condition]\n  | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition\n  | table_reference LEFT SEMI JOIN table_reference join_condition\n  | table_reference CROSS JOIN table_reference [join_condition] (as of Hive 0.10)\n\ntable_reference:\n    table_factor\n  | join_table\n\ntable_factor:\n    tbl_name [alias]\n  | table_subquery alias\n  | ( table_references )\n\njoin_condition:\n    ON expression\n```\n\n----------------------------------------\n\nTITLE: Using explode() UDTF with Maps in Hive SQL\nDESCRIPTION: This variant of explode() takes a map as input and outputs one row for each key-value pair. It returns a row-set with two columns named 'key' and 'value'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nexplode(MAP<Tkey,Tvalue> m)\n```\n\n----------------------------------------\n\nTITLE: Displaying Hive Function Documentation in CLI\nDESCRIPTION: Commands to list all available functions and get detailed documentation for specific functions in Hive Beeline or CLI interfaces.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSHOW FUNCTIONS;\nDESCRIBE FUNCTION <function_name>;\nDESCRIBE FUNCTION EXTENDED <function_name>;\n\n```\n\n----------------------------------------\n\nTITLE: WebHCat Job Information JSON Response\nDESCRIPTION: Standard JSON response format when retrieving job information from WebHCat. The response includes job status details, profile information, completion percentage, exit value, and other metadata about the requested job.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-jobinfo_34017194.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"status\": {\n            \"startTime\": 1324529476131,\n            \"username\": \"ctdean\",\n            \"jobID\": {\n                      \"jtIdentifier\": \"201112212038\",\n                      \"id\": 4\n                     },\n            \"jobACLs\": {\n                       },\n            \"schedulingInfo\": \"NA\",\n            \"failureInfo\": \"NA\",\n            \"jobId\": \"job_201112212038_0004\",\n            \"jobPriority\": \"NORMAL\",\n            \"runState\": 2,\n            \"jobComplete\": true\n           },\n \"profile\": {\n             \"url\": \"http://localhost:50030/jobdetails.jsp?jobid=job_201112212038_0004\",\n             \"jobID\": {\n                       \"jtIdentifier\": \"201112212038\",\n                        \"id\": 4\n                      },\n             \"user\": \"ctdean\",\n             \"queueName\": \"default\",\n             \"jobFile\": \"hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201112212038_0004/job.xml\",\n             \"jobName\": \"PigLatin:DefaultJobName\",\n             \"jobId\": \"job_201112212038_0004\"\n            },\n \"id\": \"job_201112212038_0004\",\n \"parentId\": \"job_201112212038_0003\",\n \"percentComplete\": \"100% complete\",\n \"exitValue\": 0,\n \"user\": \"ctdean\",\n \"callback\": null,\n \"completed\": \"done\"\n}\n```\n\n----------------------------------------\n\nTITLE: Writing Query Results to Filesystem in Hive\nDESCRIPTION: Syntax for inserting query results into filesystem directories using Hive. Includes standard syntax and Hive extension for multiple inserts.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nStandard syntax:\nINSERT OVERWRITE [LOCAL] DIRECTORY directory1\n  [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0)\n  SELECT ... FROM ...\n\nHive extension (multiple inserts):\nFROM from_statement\nINSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1\n[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ...\n\n \nrow_format\n  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]\n        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]\n        [NULL DEFINED AS char] (Note: Only available starting with Hive 0.13)\n```\n\n----------------------------------------\n\nTITLE: Showing Materialized Views in Hive\nDESCRIPTION: Syntax for displaying materialized views with the SHOW MATERIALIZED VIEWS command. Lists all materialized views in a database that match a pattern and shows additional information like rewriting status and refresh mode.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_71\n\nLANGUAGE: sql\nCODE:\n```\nSHOW MATERIALIZED VIEWS [IN/FROM database_name] [LIKE 'pattern_with_wildcards'];\n```\n\n----------------------------------------\n\nTITLE: Date Arithmetic in Hive SQL\nDESCRIPTION: Functions for performing date arithmetic, including calculating the difference between dates and adding or subtracting days from a date.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_33\n\nLANGUAGE: SQL\nCODE:\n```\ndatediff(string enddate,string startdate)\ndate_add(date/timestamp/string startdate, tinyint/smallint/int days)\ndate_sub(date/timestamp/string startdate, tinyint/smallint/int days)\n```\n\n----------------------------------------\n\nTITLE: HiveServer2 Path Configuration Example\nDESCRIPTION: Example showing configuration of file system path with variable substitution\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_48\n\nLANGUAGE: markdown\nCODE:\n```\n* Default Value: `${java.io.tmpdir}/${user.name}/operation_logs`\n```\n\n----------------------------------------\n\nTITLE: Exploding Maps in Hive\nDESCRIPTION: Examples of using the explode function to transform map key-value pairs into multiple rows. Demonstrates various syntax options including column aliasing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_50\n\nLANGUAGE: HiveQL\nCODE:\n```\nselect explode(map('A',10,'B',20,'C',30));\nselect explode(map('A',10,'B',20,'C',30)) as (key,value);\nselect tf.* from (select 0) t lateral view explode(map('A',10,'B',20,'C',30)) tf;\nselect tf.* from (select 0) t lateral view explode(map('A',10,'B',20,'C',30)) tf as key,value;\n```\n\n----------------------------------------\n\nTITLE: Table Sampling in Hive\nDESCRIPTION: Demonstrates how to sample data from a table using the TABLESAMPLE clause based on bucket specifications.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nINSERT OVERWRITE TABLE pv_gender_sum_sample\nSELECT pv_gender_sum.*\nFROM pv_gender_sum TABLESAMPLE(BUCKET 3 OUT OF 32);\n```\n\n----------------------------------------\n\nTITLE: Creating a Time-Aggregated Materialized View with Druid in Hive SQL\nDESCRIPTION: Example of creating a materialized view (mv3) using Druid storage handler that rolls up wiki edit events by minute. The view aggregates character additions and removals by time and page.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nCREATE MATERIALIZED VIEW mv3\nSTORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'\nAS\nSELECT floor(time to minute) as `__time`, page,\n    SUM(characters_added) AS c_added,\n    SUM(characters_removed) AS c_removed\nFROM wiki\nGROUP BY floor(time to minute), page;\n```\n\n----------------------------------------\n\nTITLE: Column Change Examples in Hive\nDESCRIPTION: Examples demonstrating how to use the CHANGE COLUMN command to modify column names, data types, positions, and comments in a Hive table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_42\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE test_change (a int, b int, c int);\n\n// First change column a's name to a1.\nALTER TABLE test_change CHANGE a a1 INT;\n\n// Next change column a1's name to a2, its data type to string, and put it after column b.\nALTER TABLE test_change CHANGE a1 a2 STRING AFTER b;\n// The new table's structure is:  b int, a2 string, c int.\n \n// Then change column c's name to c1, and put it as the first column.\nALTER TABLE test_change CHANGE c c1 INT FIRST;\n// The new table's structure is:  c1 int, b int, a2 string.\n \n// Add a comment to column a1\nALTER TABLE test_change CHANGE a1 a1 INT COMMENT 'this is column a1';\n```\n\n----------------------------------------\n\nTITLE: Configuring Named JDBC URLs in beeline-site.xml\nDESCRIPTION: A beeline-site.xml configuration that defines multiple named JDBC connection URLs with a default selection. This allows connecting to different HiveServer2 instances using simple connection aliases.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_33\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n<property>\n  <name>beeline.hs2.jdbc.url.tcpUrl</name>\n  <value>jdbc:hive2://localhost:10000/default;user=hive;password=hive</value>\n</property>\n\n<property>\n  <name>beeline.hs2.jdbc.url.httpUrl</name>\n  <value>jdbc:hive2://localhost:10000/default;user=hive;password=hive;transportMode=http;httpPath=cliservice</value>\n</property>\n\n<property>\n  <name>beeline.hs2.jdbc.url.default</name>\n  <value>tcpUrl</value>\n</property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Casting Large Integral Literals to Decimal in Hive SQL\nDESCRIPTION: Shows how to cast a large integral literal (larger than BIGINT) to a Decimal type with precision 38 and scale 0. The 'BD' postfix is required for such large literals.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nselect CAST(18446744073709001000BD AS DECIMAL(38,0)) from my_table limit 1;\n```\n\n----------------------------------------\n\nTITLE: Setting Map Join Local Task Memory Usage Threshold in Hive SQL\nDESCRIPTION: This SQL command sets the threshold for memory usage in Map Join local tasks. If memory usage exceeds this threshold, the local task will abort.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoinoptimization_27362029.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.mapjoin.localtask.max.memory.usage = 0.999;\n```\n\n----------------------------------------\n\nTITLE: Configuring Join Emit Interval\nDESCRIPTION: Sets how many rows in the right-most join operand Hive should buffer before emitting the join result.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_10\n\nLANGUAGE: properties\nCODE:\n```\nhive.join.emit.interval=1000\n```\n\n----------------------------------------\n\nTITLE: Changing Column Properties in Hive Tables\nDESCRIPTION: Command to change a column's name, data type, comment, or position in a table or partition. The CASCADE option allows changes to propagate to all partition metadata. Available in Hive 0.14.0 and later for partitions, and CASCADE/RESTRICT in Hive 1.1.0 and later.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_41\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name [PARTITION partition_spec] CHANGE [COLUMN] col_old_name col_new_name column_type\n  [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT];\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partition Insert in Hive\nDESCRIPTION: Example of a dynamic partition insert in Hive, where the 'country' partition is dynamically created based on the last column of the SELECT clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nFROM page_view_stg pvs\nINSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country)\n       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.cnt\n```\n\n----------------------------------------\n\nTITLE: Analyzing Specific Partition Statistics in Hive\nDESCRIPTION: Example of gathering statistics for a specific partition using ANALYZE TABLE command.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr=11) COMPUTE STATISTICS;\n```\n\n----------------------------------------\n\nTITLE: Using the array_max() function in Hive SQL\nDESCRIPTION: Returns the maximum value in an array with elements that support ordering. Example: array_max(array(1, 3, 0, NULL)) returns 3.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_60\n\nLANGUAGE: SQL\nCODE:\n```\narray_max((array(obj1, obj2, obj3...))\n```\n\n----------------------------------------\n\nTITLE: Spatial Join Query Example with Output\nDESCRIPTION: Complete example of spatial join query between two tables using st_intersects with sample output.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/spatial-queries_34022710.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT ta.rec_id, tb.rec_id FROM ta JOIN tb ON (st_intersects(ta.outline, tb.outline) = TRUE);\n```\n\n----------------------------------------\n\nTITLE: Basic Table and Index Creation Example\nDESCRIPTION: Example showing creation of a table and an index using the CompactIndexHandler implementation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/indexdev_27362104.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE t(i int, j int);\nCREATE INDEX x ON TABLE t(j)\nAS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler';\n```\n\n----------------------------------------\n\nTITLE: Creating External Table with JDBC Storage Handler in Hive SQL\nDESCRIPTION: Demonstrates how to create an external table in Hive using the JDBC Storage Handler to connect to a MySQL database. Includes required table properties for connection and configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE EXTERNAL TABLE student_jdbc\n(\n  name string,\n  age int,\n  gpa double\n)\nSTORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler'\nTBLPROPERTIES (\n    \"hive.sql.database.type\" = \"MYSQL\",\n    \"hive.sql.jdbc.driver\" = \"com.mysql.jdbc.Driver\",\n    \"hive.sql.jdbc.url\" = \"jdbc:mysql://localhost/sample\",\n    \"hive.sql.dbcp.username\" = \"hive\",\n    \"hive.sql.dbcp.password\" = \"hive\",\n    \"hive.sql.table\" = \"STUDENT\",\n    \"hive.sql.dbcp.maxActive\" = \"1\"\n);\n```\n\n----------------------------------------\n\nTITLE: Sort-Merge Join Configuration in Hive\nDESCRIPTION: These configuration settings enable sort-merge joins for sorted and bucketized tables in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin = true;\nset hive.optimize.bucketmapjoin.sortedmerge = true;\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Connection for Kerberos Authentication\nDESCRIPTION: Example JDBC URL format for connecting to a secure HiveServer2 instance with Kerberos authentication. Shows how to properly format the connection URL with the server principal.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_43\n\nLANGUAGE: java\nCODE:\n```\njdbc:hive2://<host>:<port>/<db>;principal=<Server_Principal_of_HiveServer2>\n```\n\n----------------------------------------\n\nTITLE: Alternative to HAVING Using Subquery\nDESCRIPTION: Demonstrates how to achieve the same effect as a HAVING clause by using a subquery for older Hive versions that don't support HAVING. The subquery performs the aggregation which is then filtered in the outer query.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nSELECT col1 FROM (SELECT col1, SUM(col2) AS col2sum FROM t1 GROUP BY col1) t2 WHERE t2.col2sum > 10\n```\n\n----------------------------------------\n\nTITLE: INSERT...VALUES Examples in HiveQL\nDESCRIPTION: Examples demonstrating insertion of data into regular and partitioned tables in Hive. Shows both fixed and dynamic partitioning approaches.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE students (name VARCHAR(64), age INT, gpa DECIMAL(3, 2))\n  CLUSTERED BY (age) INTO 2 BUCKETS STORED AS ORC;\n\nINSERT INTO TABLE students\n  VALUES ('fred flintstone', 35, 1.28), ('barney rubble', 32, 2.32);\n\nCREATE TABLE pageviews (userid VARCHAR(64), link STRING, came_from STRING)\n  PARTITIONED BY (datestamp STRING) CLUSTERED BY (userid) INTO 256 BUCKETS STORED AS ORC;\n\nINSERT INTO TABLE pageviews PARTITION (datestamp = '2014-09-23')\n  VALUES ('jsmith', 'mail.com', 'sports.com'), ('jdoe', 'mail.com', null);\n\nINSERT INTO TABLE pageviews PARTITION (datestamp)\n  VALUES ('tjohnson', 'sports.com', 'finance.com', '2014-09-23'), ('tlee', 'finance.com', null, '2014-09-21');\n \nINSERT INTO TABLE pageviews\n  VALUES ('tjohnson', 'sports.com', 'finance.com', '2014-09-23'), ('tlee', 'finance.com', null, '2014-09-21');\n```\n\n----------------------------------------\n\nTITLE: Enabling Raw Reserved Namespace for TDE in Hive Replication XML\nDESCRIPTION: Allows Distcp super user to access raw bytes from the filesystem without decrypting on source and re-encrypting on target for TDE with same encryption keys on source and target.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_112\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.repl.add.raw.reserved.namespace</name>\n  <value>false</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Basic explode() UDTF Example in SELECT Statement\nDESCRIPTION: Example of using the explode() UDTF in a SELECT statement. The function takes an array as input and outputs each element as a separate row, allowing array elements to be transformed into table rows.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_54\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT explode(myCol) AS myNewCol FROM myTable;\n```\n\n----------------------------------------\n\nTITLE: SHOW TABLE EXTENDED Command in Hive\nDESCRIPTION: Syntax for the SHOW TABLE EXTENDED command which displays detailed information about tables matching a pattern, including file system metadata like file counts, sizes, and timestamps. Optionally can show information for a specific partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_78\n\nLANGUAGE: sql\nCODE:\n```\nSHOW TABLE EXTENDED [IN|FROM database_name] LIKE 'identifier_with_wildcards' [PARTITION(partition_spec)];\n```\n\n----------------------------------------\n\nTITLE: Altering and Dropping Hive Tables\nDESCRIPTION: Demonstrates how to rename tables, add columns, replace columns, and drop tables using HiveQL ALTER TABLE and DROP TABLE commands.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE events RENAME TO 3koobecaf;\nALTER TABLE pokes ADD COLUMNS (new_col INT);\nALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment');\nALTER TABLE invites REPLACE COLUMNS (foo INT, bar STRING, baz INT COMMENT 'baz replaces new_col2');\n```\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE invites REPLACE COLUMNS (foo INT COMMENT 'only keep the first column');\n```\n\nLANGUAGE: SQL\nCODE:\n```\nDROP TABLE pokes;\n```\n\n----------------------------------------\n\nTITLE: Basic Arithmetic Functions in Hive SQL\nDESCRIPTION: Core arithmetic operations including rounding, floor, ceiling, and random number generation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_42\n\nLANGUAGE: SQL\nCODE:\n```\nround(double a)\nround(double a,int d)\nbround(double a)\nbround(double a,int d)\nfloor(double a)\nceil(double a)\nrand(), rand(INT seed)\n```\n\n----------------------------------------\n\nTITLE: Creating ORC Table for Vectorized Execution in Hive SQL\nDESCRIPTION: SQL commands to create a table in ORC format, which is required for vectorized execution, and insert a sample row.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/vectorized-query-execution_34838326.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table vectorizedtable(state string,id int) stored as orc;\n\ninsert into vectorizedtable values('haryana',1);\n```\n\n----------------------------------------\n\nTITLE: Correcting Avro Schema Properties with ALTER TABLE\nDESCRIPTION: HiveQL command referenced in the FAQ section to fix issues with Avro schema configuration. This command allows users to update table properties when encountering errors with avro.schema.literal or avro.schema.url.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nalter table T set TBLPROPERTIES\n```\n\n----------------------------------------\n\nTITLE: Hive Variable and Parameter Settings\nDESCRIPTION: Configuration for variable substitution and table parameter handling.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_20\n\nLANGUAGE: properties\nCODE:\n```\nhive.table.parameters.default=\nhive.variable.substitute=true\nhive.error.on.empty.partition=false\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Configuration List in Hive\nDESCRIPTION: Comma-separated list of configuration options that should not be read by normal users, such as passwords. This setting helps protect sensitive information in Hive configurations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_81\n\nLANGUAGE: markdown\nCODE:\n```\n##### hive.conf.hidden.list\n\n* Default Value:\n\t+ `Hive 1.2.2: j` `avax.jdo.option.ConnectionPassword,\n\t\nhive.server2.keystore.password (` [HIVE-9013](https://issues.apache.org/jira/browse/HIVE-9013))\n\t+ ```\n\tHive 2.3.0: fs.s3.awsAccessKeyId,fs.s3.awsSecretAccessKey,fs.s3n.awsAccessKeyId,fs.s3n.awsSecretAccessKey,fs.s3a.access.key,fs.s3a.secret.key,fs.s3a.proxy.password ([HIVE-14588](https://issues.apache.org/jira/browse/HIVE-14588))\n\t```\n\t+ ``Hive 3.0.0: dfs.adls.oauth2.credential,fs.adl.oauth2.credential``  ``(``  [HIVE-18228](https://issues.apache.org/jira/browse/HIVE-18228))\n```\n\n----------------------------------------\n\nTITLE: Submitting a MapReduce job through WebHCat API using curl\nDESCRIPTION: Curl command to POST a MapReduce job with specified JAR, class, library JARs, and arguments to the WebHCat API endpoint.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-mapreducejar_34017030.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -d jar=wordcount.jar \\\n       -d class=org.myorg.WordCount \\\n       -d libjars=transform.jar \\\n       -d arg=wordcount/input \\\n       -d arg=wordcount/output \\\n       'http://localhost:50111/templeton/v1/mapreduce/jar?user.name=ekoifman'\n```\n\n----------------------------------------\n\nTITLE: Distinct Aggregation with Partitioning in HiveQL\nDESCRIPTION: Shows the syntax for using DISTINCT with aggregation functions like COUNT, SUM, and AVG over partitions in Hive 2.1.0 and later.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCOUNT(DISTINCT a) OVER (PARTITION BY c)\n```\n\n----------------------------------------\n\nTITLE: Counting Occurrences with Lateral View in Hive\nDESCRIPTION: Shows how to count occurrences of values using a lateral view with the explode() function. The query explodes an array column and then uses GROUP BY and COUNT to calculate how many times each ad appears across all pages.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lateralview_27362040.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT adid, count(1)\nFROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid\nGROUP BY adid;\n\n```\n\n----------------------------------------\n\nTITLE: Creating Druid-Based Materialized View\nDESCRIPTION: Creates a materialized view using Druid storage handler to aggregate wiki events by minute\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views-in-hive_283118346.md#2025-04-09_snippet_8\n\nLANGUAGE: HiveQL\nCODE:\n```\nCREATE MATERIALIZED VIEW mv3\nSTORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'\nAS\nSELECT floor(time to minute) as `__time`, page,\nSUM(characters_added) AS c_added,\nSUM(characters_removed) AS c_removed\nFROM wiki\nGROUP BY floor(time to minute), page;\n```\n\n----------------------------------------\n\nTITLE: Running Beeline Query in Background with nohup\nDESCRIPTION: This shell command demonstrates how to run a Beeline query in the background using nohup, allowing the terminal to disconnect while keeping the process running. It redirects output and error logs to separate files.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\nnohup beeline --silent=true --showHeader=true --outputformat=dsv -f query.hql </dev/null > /tmp/output.log 2> /tmp/error.log &\n```\n\n----------------------------------------\n\nTITLE: Describing Druid-Linked Hive Table Structure\nDESCRIPTION: SQL command and its output showing the detailed structure of a Hive table linked to a Druid datasource, including columns, data types, and table properties.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nhive> DESCRIBE FORMATTED druid_table_1;\nOK\n# col_name            \tdata_type           \tcomment\n__time              \ttimestamp           \tfrom deserializer\nadded               \tbigint              \tfrom deserializer\nchannel             \tstring              \tfrom deserializer\ncityname            \tstring              \tfrom deserializer\ncomment             \tstring              \tfrom deserializer\ncount               \tbigint              \tfrom deserializer\ncountryisocode      \tstring              \tfrom deserializer\ncountryname         \tstring              \tfrom deserializer\ndeleted             \tbigint              \tfrom deserializer\ndelta               \tbigint              \tfrom deserializer\nisanonymous         \tstring              \tfrom deserializer\nisminor             \tstring              \tfrom deserializer\nisnew               \tstring              \tfrom deserializer\nisrobot             \tstring              \tfrom deserializer\nisunpatrolled       \tstring              \tfrom deserializer\nmetrocode           \tstring              \tfrom deserializer\nnamespace           \tstring              \tfrom deserializer\npage                \tstring              \tfrom deserializer\nregionisocode       \tstring              \tfrom deserializer\nregionname          \tstring              \tfrom deserializer\nuser                \tstring              \tfrom deserializer\nuser_unique         \tstring              \tfrom deserializer\n```\n\n----------------------------------------\n\nTITLE: Displaying HDFS Directory Structure for ACID Table\nDESCRIPTION: Shows the filesystem layout for an ACID-enabled table 't' with base and delta directories. The structure demonstrates how base files and delta files are organized, with each transaction creating new delta directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-transactions_40509723.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhive> dfs -ls -R /user/hive/warehouse/t;\ndrwxr-xr-x   - ekoifman staff          0 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022\n-rw-r--r--   1 ekoifman staff        602 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022/bucket_00000\ndrwxr-xr-x   - ekoifman staff          0 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000\n-rw-r--r--   1 ekoifman staff        611 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000/bucket_00000\ndrwxr-xr-x   - ekoifman staff          0 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000\n-rw-r--r--   1 ekoifman staff        610 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000/bucket_00000\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Column Statistics Table\nDESCRIPTION: SQL statement to add a primary key constraint to the COLUMN_STATISTICS table, ensuring uniqueness of the CS_ID field.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/column-statistics-in-hive_29131019.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE COLUMN_STATISTICS ADD CONSTRAINT COLUMN_STATISTICS_PK PRIMARY KEY (CS_ID);\n```\n\n----------------------------------------\n\nTITLE: Simple query execution with result limiting\nDESCRIPTION: This snippet demonstrates a simple Hive query that selects data from a table with a limit clause. This type of query can be executed locally on the Hive CLI without requiring a full Hadoop cluster.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nhive> select * from kv limit 10;\n```\n\n----------------------------------------\n\nTITLE: Using the cast() function in Hive SQL\nDESCRIPTION: Converts the results of an expression to the specified type. For example, cast('1' as bigint) converts the string '1' to its integral representation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_68\n\nLANGUAGE: SQL\nCODE:\n```\ncast(expr as <type>)\n```\n\n----------------------------------------\n\nTITLE: Basic Lateral View with Explode Function in Hive\nDESCRIPTION: Demonstrates how to use a lateral view with the explode() function to convert an array column (adid_list) into separate rows. This transforms the array elements into individual records that can be queried.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lateralview_27362040.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pageid, adid\nFROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid;\n\n```\n\n----------------------------------------\n\nTITLE: Multi-Table MAPJOIN with Hints in Hive SQL\nDESCRIPTION: Example query that uses MAPJOIN hints to optimize joins between a fact table (store_sales) and multiple dimension tables (time_dim, date_dim) with filtering conditions. Prior to optimization improvements, this would execute as two separate map-only jobs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joinoptimization_33293167.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect /*+ MAPJOIN(time_dim, date_dim) */ count(*) from\nstore_sales \njoin time_dim on (ss_sold_time_sk = t_time_sk) \njoin date_dim on (ss_sold_date_sk = d_date_sk)\nwhere t_hour = 8 and d_year = 2002\n```\n\n----------------------------------------\n\nTITLE: CAST Format Examples\nDESCRIPTION: Demonstrates practical examples of using CAST with format patterns for date conversion.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/122917025.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect cast(dt as string format 'DD-MM-YYYY')  \nselect cast('01-05-2017' as date format 'DD-MM-YYYY')\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Table Partitioning in Hive\nDESCRIPTION: Demonstrates how to configure partitioning for a JDBC table in Hive, specifying the number of partitions, partition column, and bounds for splitting the data source.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nTBLPROPERTIES (\n    . . . . . .\n    \"hive.sql.table\" = \"DEMO\",\n    \"hive.sql.partitionColumn\" = \"num\",\n    \"hive.sql.numPartitions\" = \"3\",\n    \"hive.sql.lowerBound\" = \"1\",\n    \"hive.sql.upperBound\" = \"10\",\n    . . . . . .\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Avro-backed Hive Table with External Schema URL\nDESCRIPTION: Example showing how to create a Hive table backed by Avro data format by specifying the SerDe, input/output formats, and providing a schema URL. This approach works in all Hive versions that support AvroSerDe.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE kst\n  PARTITIONED BY (ds string)\n  ROW FORMAT SERDE\n  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\n  STORED AS INPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n  OUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n  TBLPROPERTIES (\n    'avro.schema.url'='http://schema_provider/kst.avsc');\n```\n\n----------------------------------------\n\nTITLE: Basic Dynamic Partition Insert in Hive\nDESCRIPTION: Example showing an INSERT statement using all dynamic partition columns (ds, hr). The dynamic partition columns must be specified last in the SELECT statement and in the same order as the PARTITION clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/dynamicpartitions_27823715.md#2025-04-09_snippet_0\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE T PARTITION (ds, hr) \nSELECT key, value, ds, hr FROM srcpart WHERE ds is not null and hr>10;\n```\n\n----------------------------------------\n\nTITLE: Distinct Counting per Partition in HiveQL\nDESCRIPTION: Illustrates how to perform distinct counting for each partition using the COUNT DISTINCT function with OVER clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a, COUNT(distinct a) OVER (PARTITION BY b)\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: Hive GROUP BY Queries\nDESCRIPTION: Shows examples of using GROUP BY in Hive queries for aggregation operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nFROM invites a INSERT OVERWRITE TABLE events SELECT a.bar, count(*) WHERE a.foo > 0 GROUP BY a.bar;\nINSERT OVERWRITE TABLE events SELECT a.bar, count(*) FROM invites a WHERE a.foo > 0 GROUP BY a.bar;\n```\n\n----------------------------------------\n\nTITLE: Extracting JSON Object in Hive SQL\nDESCRIPTION: Example of using the get_json_object function in Hive SQL to extract a JSON object from a JSON string based on a specified path. This function is useful for parsing and querying JSON data stored as strings in Hive tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nget_json_object(string json_string, string path)\n```\n\n----------------------------------------\n\nTITLE: Optimized Dynamic Partition Insert with DISTRIBUTE BY\nDESCRIPTION: Improved version of dynamic partition insert that uses DISTRIBUTE BY clause to handle large numbers of partitions efficiently by grouping rows by partition columns at the reducer level.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nbeeline> set hive.exec.dynamic.partition.mode=nonstrict;\nbeeline> FROM page_view_stg pvs\n      INSERT OVERWRITE TABLE page_view PARTITION(dt, country)\n             SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip,\n                    from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country\n             DISTRIBUTE BY ds, country;\n```\n\n----------------------------------------\n\nTITLE: Simple FROM Clause Subquery Example in Hive\nDESCRIPTION: Shows a simple example of using a subquery in the FROM clause, where the subquery performs a calculation on columns from table t1 and the outer query selects from this result.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-subqueries_27362044.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT col \nFROM (\n  SELECT a+b AS col\n  FROM t1\n) t2\n\n```\n\n----------------------------------------\n\nTITLE: Array Operations in Hive\nDESCRIPTION: Demonstrates array column creation and operations including element access and size calculation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_35\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE array_table (int_array_column ARRAY<INT>);\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pv.friends[2]\nFROM page_views pv;\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pv.userid, size(pv.friends)\nFROM page_view pv;\n```\n\n----------------------------------------\n\nTITLE: Using HAVING Clause in Hive\nDESCRIPTION: Shows how to filter grouped results using a HAVING clause, which was added in Hive 0.7.0. The example filters groups based on an aggregate function result.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nSELECT col1 FROM t1 GROUP BY col1 HAVING SUM(col2) > 10\n```\n\n----------------------------------------\n\nTITLE: Creating a Hive Table with Delimited Composite Row Keys for HBase\nDESCRIPTION: SQL statement to create an external Hive table with a composite row key of two string fields delimited by '~'. The table is stored in HBase using the HBaseStorageHandler with mapping for the key and a column.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\n-- Create a table with a composite row key consisting of two string fields, delimited by '~'\nCREATE EXTERNAL TABLE delimited_example(key struct<f1:string, f2:string>, value string) \nROW FORMAT DELIMITED \nCOLLECTION ITEMS TERMINATED BY '~' \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' \nWITH SERDEPROPERTIES ( \n  'hbase.columns.mapping'=':key,f:c1');\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Binary Column in Apache Hive SQL\nDESCRIPTION: SQL syntax example for creating a table in Hive with a binary data type column. This demonstrates how to define a table with mixed data types including the binary type.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/binary-datatype-proposal_27826614.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate table binary_table (a string, b binary);\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto Join Conversion in Hive\nDESCRIPTION: These configuration settings enable auto join conversion in Hive. The first parameter enables the feature, while the second sets the size limit for tables that can be converted to hashmaps in memory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joinoptimization_33293167.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nset hive.auto.convert.join.noconditionaltask = true;\nset hive.auto.convert.join.noconditionaltask.size = 10000000;\n```\n\n----------------------------------------\n\nTITLE: JDB Debug Connection Message\nDESCRIPTION: Expected output message when JVM is waiting for debugger connection.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n     [junit] Listening for transport dt_socket at address: 8000\n```\n\n----------------------------------------\n\nTITLE: Simple Query with Table Insert\nDESCRIPTION: Demonstrates a simple query with filtering and inserting results into another table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_22\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE user_active\nSELECT user.*\nFROM user\nWHERE user.active = 1;\n```\n\n----------------------------------------\n\nTITLE: Minimal ZooKeeper Configuration for HiveServer2\nDESCRIPTION: This XML configuration enables ZooKeeper-based service discovery for HiveServer2, which provides high availability and rolling upgrade capabilities. It sets the necessary properties to enable dynamic service discovery and define the ZooKeeper quorum.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_29\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n    <property>\n        <name>hive.server2.support.dynamic.service.discovery</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>hive.zookeeper.quorum</name>\n        <value>127.0.0.1:2181</value>\n    </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Querying JSON Data Using get_json_object in Hive\nDESCRIPTION: Examples of using get_json_object function to extract specific fields from JSON data using JSONPath-like syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_48\n\nLANGUAGE: sql\nCODE:\n```\nSELECT get_json_object(src_json.json, '$.owner') FROM src_json;\nSELECT get_json_object(src_json.json, '$.store.fruit[0]') FROM src_json;\nSELECT get_json_object(src_json.json, '$.non_exist_key') FROM src_json;\n```\n\n----------------------------------------\n\nTITLE: Example Hive SQL Queries\nDESCRIPTION: Basic SQL commands demonstrating table creation, partitioning, data insertion, and aggregation\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hive-with-docker_282102281.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nshow tables;\ncreate table hive_example(a string, b int) partitioned by(c int);\nalter table hive_example add partition(c=1);\ninsert into hive_example partition(c=1) values('a', 1), ('a', 2),('b',3);\nselect count(distinct a) from hive_example;\nselect sum(b) from hive_example;\n```\n\n----------------------------------------\n\nTITLE: Managing Druid Kafka Ingestion State\nDESCRIPTION: SQL commands to start, stop, and reset Druid Kafka ingestion tasks.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE druid_kafka_test SET TBLPROPERTIES('druid.kafka.ingestion' = 'START');\nALTER TABLE druid_kafka_test SET TBLPROPERTIES('druid.kafka.ingestion' = 'STOP');\nALTER TABLE druid_kafka_test SET TBLPROPERTIES('druid.kafka.ingestion' = 'RESET');\n```\n\n----------------------------------------\n\nTITLE: Counting Rows in a Hive Table\nDESCRIPTION: Demonstrates how to count the total number of rows in a Hive table using the COUNT(*) aggregate function.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT COUNT(*) FROM table2;\n```\n\n----------------------------------------\n\nTITLE: Legacy Grouping__ID Query (Pre-Hive 2.3.0)\nDESCRIPTION: Demonstrates the behavior of GROUPING__ID function before Hive 2.3.0, showing different bitvector calculation logic for aggregated columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/30151323.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT key, value, GROUPING__ID, count(*)\nFROM T1\nGROUP BY key, value WITH ROLLUP;\n```\n\n----------------------------------------\n\nTITLE: Backtick Column Name Example\nDESCRIPTION: Example showing how to use backticks for column names when hive.support.quoted.identifiers is set to 'column', allowing Unicode characters in column names.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\n```column_name```\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Hive Table\nDESCRIPTION: HQL command to load the extracted 'u.data' file into the previously created 'u_data' Hive table. The path to u.data needs to be specified.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_25\n\nLANGUAGE: hql\nCODE:\n```\nLOAD DATA LOCAL INPATH '<path>/u.data'\nOVERWRITE INTO TABLE u_data;\n```\n\n----------------------------------------\n\nTITLE: Creating Index with Table Properties in HiveQL\nDESCRIPTION: Creates a compact index on column9 of table08, setting custom table properties using the TBLPROPERTIES clause for the index storage table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_7\n\nLANGUAGE: hiveql\nCODE:\n```\nCREATE INDEX table08_index ON TABLE table08 (column9) AS 'COMPACT' TBLPROPERTIES (\"prop3\"=\"value3\", \"prop4\"=\"value4\");\n```\n\n----------------------------------------\n\nTITLE: Creating a Bucketed Table in Hive\nDESCRIPTION: This snippet demonstrates how to create a bucketed table in Hive. The table is partitioned by date and clustered into 256 buckets based on the user_id column.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl-bucketedtables_27362035.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE user_info_bucketed(user_id BIGINT, firstname STRING, lastname STRING)\nCOMMENT 'A bucketed copy of user_info'\nPARTITIONED BY(ds STRING)\nCLUSTERED BY(user_id) INTO 256 BUCKETS;\n```\n\n----------------------------------------\n\nTITLE: Additional Hive Replication Configuration Parameters\nDESCRIPTION: This snippet lists additional replication-related parameters with their default values. These are relevant only to the cluster that acts as the source cluster. The defaults should work in most cases.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivereplicationv2development_66850051.md#2025-04-09_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nREPLDIR(\"hive.repl.rootdir\",\"/user/hive/repl/\", \"HDFS root dir for all replication dumps.\"),\nREPLCMDIR(\"hive.repl.cmrootdir\",\"/user/hive/cmroot/\", \"Root dir for ChangeManager, used for deleted files.\"),\nREPLCMRETIAN(\"hive.repl.cm.retain\",\"24h\", new TimeValidator(TimeUnit.HOURS),\"Time to retain removed files in cmrootdir.\"),\nREPLCMINTERVAL(\"hive.repl.cm.interval\",\"3600s\",new TimeValidator(TimeUnit.SECONDS),\"Inteval for cmroot cleanup thread.\"),\n```\n\n----------------------------------------\n\nTITLE: Managing SerDe Properties in Hive\nDESCRIPTION: SQL statements to modify a table's SerDe class and properties. Allows setting custom serialization/deserialization properties for data handling.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name [PARTITION partition_spec] SET SERDE serde_class_name [WITH SERDEPROPERTIES serde_properties];\n\nALTER TABLE table_name [PARTITION partition_spec] SET SERDEPROPERTIES serde_properties;\n\nserde_properties:\n  : (property_name = property_value, property_name = property_value, ... )\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name SET SERDEPROPERTIES ('field.delim' = ',');\n```\n\n----------------------------------------\n\nTITLE: Multi-Table Join with Different Join Columns in Hive SQL\nDESCRIPTION: This example shows a join query that Hive converts into two map/reduce jobs due to different join columns being used.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)\n```\n\n----------------------------------------\n\nTITLE: Custom Directory Hive Build\nDESCRIPTION: Ant command to build Hive package with a custom installation directory specification\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nant -Dtarget.dir=<my-install-dir> package\n```\n\n----------------------------------------\n\nTITLE: GenericUDTF Abstract Class Definition in Apache Hive (Java)\nDESCRIPTION: This code snippet shows the abstract class definition for GenericUDTF in Apache Hive. It defines the structure and methods that need to be implemented by custom UDTFs, including initialize, process, and close. It also provides methods for setting the collector and forwarding output rows.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide-udtf_27362086.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npackage org.apache.hadoop.hive.ql.udf.generic;\n\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n\n/**\n * A Generic User-defined Table Generating Function (UDTF)\n * \n * Generates a variable number of output rows for a single input row. Useful for\n * explode(array)...\n */\n\npublic abstract class GenericUDTF {\n  Collector collector = null;\n\n  /**\n * Initialize this GenericUDTF. This will be called only once per instance.\n * \n * @param args\n *          An array of ObjectInspectors for the arguments\n * @return A StructObjectInspector for output. The output struct represents a\n *         row of the table where the fields of the stuct are the columns. The\n *         field names are unimportant as they will be overridden by user\n *         supplied column aliases.\n   */\n  public abstract StructObjectInspector initialize(ObjectInspector[] argOIs)\n      throws UDFArgumentException;\n\n  /**\n * Give a set of arguments for the UDTF to process.\n * \n * @param o\n *          object array of arguments\n   */\n  public abstract void process(Object[] args) throws HiveException;\n\n  /**\n * Called to notify the UDTF that there are no more rows to process.\n * Clean up code or additional forward() calls can be made here.\n   */\n  public abstract void close() throws HiveException;\n\n  /**\n * Associates a collector with this UDTF. Can't be specified in the\n * constructor as the UDTF may be initialized before the collector has been\n * constructed.\n * \n * @param collector\n   */\n  public final void setCollector(Collector collector) {\n    this.collector = collector;\n  }\n\n  /**\n * Passes an output row to the collector.\n * \n * @param o\n * @throws HiveException\n   */\n  protected final void forward(Object o) throws HiveException {\n    collector.collect(o);\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Querying a Block Sample in HiveQL\nDESCRIPTION: This example demonstrates how to select all columns from a 0.1% sample of the 'source' table using block sampling.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT *\nFROM source TABLESAMPLE(0.1 PERCENT) s;\n```\n\n----------------------------------------\n\nTITLE: Showing Role Grants in Hive\nDESCRIPTION: Shows roles granted to a specific user or role. Currently accessible by all users.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nSHOW ROLE GRANT (USER|ROLE) principal_name;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive to use HBase Metastore in hive-site.xml\nDESCRIPTION: This XML configuration snippet sets up Hive to use HBase as its metastore. It specifies the HBaseStore implementation for the rawstore and enables fastpath for improved performance.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasemetastoredevelopmentguide_55151960.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n    <name>hive.metastore.rawstore.impl</name>\n    <value>org.apache.hadoop.hive.metastore.hbase.HBaseStore</value>\n  </property>\n  <property>\n    <name>hive.metastore.fastpath</name>\n    <value>true</value>\n  </property>\n```\n\n----------------------------------------\n\nTITLE: Multiple OVER Clauses in HiveQL\nDESCRIPTION: Demonstrates using multiple OVER clauses in a single query, applying different window functions to different columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT \n a,\n COUNT(b) OVER (PARTITION BY c),\n SUM(b) OVER (PARTITION BY c)\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Storage Handler in Hive SQL\nDESCRIPTION: This SQL snippet demonstrates how to create a table in Hive using a storage handler. It specifies the table structure and uses the STORED BY clause to indicate the storage handler class.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/storagehandlers_27362063.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE hbase_table_1(key int, value string) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n\"hbase.columns.mapping\" = \"cf:string\",\n\"hbase.table.name\" = \"hbase_table_0\"\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive for ACID Transactions\nDESCRIPTION: This snippet shows the minimum required configuration parameters to enable ACID transactions in Hive. It includes both client-side and server-side (Metastore) settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118453.md#2025-04-09_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\n# Client Side\nhive.support.concurrency=true\nhive.exec.dynamic.partition.mode=nonstrict\nhive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager\n\n# Server Side (Metastore)\nhive.compactor.initiator.on=true\nhive.compactor.cleaner.on=true\nhive.compactor.worker.threads=<positive number>\n```\n\n----------------------------------------\n\nTITLE: Left Outer Join with Inequality Condition in Hive SQL\nDESCRIPTION: This snippet demonstrates a left outer join using an inequality condition in the ON clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.* FROM a LEFT OUTER JOIN b ON (a.id <> b.id)\n```\n\n----------------------------------------\n\nTITLE: Boolean XPath Evaluation in Hive SQL\nDESCRIPTION: Shows how to use xpath_boolean UDF to evaluate XPath expressions and return true/false based on matches.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-xpathudf_27362051.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT xpath_boolean ('<a><b>b</b></a>', 'a/b') FROM src LIMIT 1 ;\n```\n\n----------------------------------------\n\nTITLE: Creating ACID Table with Compaction Options in Hive SQL\nDESCRIPTION: This SQL snippet demonstrates how to create a transactional Hive table with custom compaction options. It sets the table as transactional, specifies compaction job properties, and defines thresholds for minor and major compactions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118453.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE table_name (\n  id                int,\n  name              string\n)\nCLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC\nTBLPROPERTIES (\"transactional\"=\"true\",\n  \"compactor.mapreduce.map.memory.mb\"=\"2048\",     -- specify compaction map job properties\n  \"compactorthreshold.hive.compactor.delta.num.threshold\"=\"4\",  -- trigger minor compaction if there are more than 4 delta directories\n  \"compactorthreshold.hive.compactor.delta.pct.threshold\"=\"0.5\" -- trigger major compaction if the ratio of size of delta files to\n                                                                   -- size of base files is greater than 50%\n);\n```\n\n----------------------------------------\n\nTITLE: Viewing Column Statistics\nDESCRIPTION: Command to view detailed column statistics for a specific partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\ndesc formatted concurrent_delete_different partition(ds='tomorrow') name;\n```\n\n----------------------------------------\n\nTITLE: Using DISTINCT in Hive SELECT Queries\nDESCRIPTION: Demonstrates the DISTINCT clause to remove duplicate rows from query results. The example shows querying with and without DISTINCT to illustrate the difference in output.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nhive> SELECT col1, col2 FROM t1\n    1 3\n    1 3\n    1 4\n    2 5\nhive> SELECT DISTINCT col1, col2 FROM t1\n    1 3\n    1 4\n    2 5\nhive> SELECT DISTINCT col1 FROM t1\n    1\n    2\n```\n\n----------------------------------------\n\nTITLE: Creating Schema and Tables in MariaDB\nDESCRIPTION: This snippet shows how to create schemas and tables in MariaDB. It creates two schemas, 'bob' and 'alice', each with a 'country' table, and inserts sample data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SCHEMA bob;\nCREATE TABLE bob.country\n(\n    id   int,\n    name varchar(20)\n);\n\ninsert into bob.country\nvalues (1, 'India');\ninsert into bob.country\nvalues (2, 'Russia');\ninsert into bob.country\nvalues (3, 'USA');\n\nCREATE SCHEMA alice;\nCREATE TABLE alice.country\n(\n    id   int,\n    name varchar(20)\n);\n\ninsert into alice.country\nvalues (4, 'Italy');\ninsert into alice.country\nvalues (5, 'Greece');\ninsert into alice.country\nvalues (6, 'China');\ninsert into alice.country\nvalues (7, 'Japan');\n```\n\n----------------------------------------\n\nTITLE: Error Response JSON Structure\nDESCRIPTION: Example JSON error response when an invalid version is specified in the WebHCat API request.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-responsetypes_34015937.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": \"null for uri: http://localhost:50111/templeton/v2\"\n}\n```\n\n----------------------------------------\n\nTITLE: Altering Table Compaction Options in Hive SQL\nDESCRIPTION: These SQL snippets show how to alter an existing table to perform minor and major compactions with custom options. The first example sets a compaction map job property, while the second changes a table property for ORC compression.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118453.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE table_name COMPACT 'minor' \n   WITH OVERWRITE TBLPROPERTIES (\"compactor.mapreduce.map.memory.mb\"=\"3072\");  -- specify compaction map job properties\nALTER TABLE table_name COMPACT 'major'\n   WITH OVERWRITE TBLPROPERTIES (\"tblprops.orc.compress.size\"=\"8192\");         -- change any other Hive table properties\n```\n\n----------------------------------------\n\nTITLE: Rendering Hive Committers Table in Markdown\nDESCRIPTION: A markdown table listing Apache Hive committers, including their Apache usernames, full names, and affiliated organizations with hyperlinks.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/community/people.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Apache username | name                              | organization                                                                 |\n|-----------------|-----------------------------------|------------------------------------------------------------------------------|\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Join Conversion in Hive SQL\nDESCRIPTION: This SQL command enables the automatic conversion of common joins to map joins in Hive queries when possible.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoinoptimization_27362029.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.auto.convert.join = true;\n```\n\n----------------------------------------\n\nTITLE: Markdown Test Status Table\nDESCRIPTION: A markdown table tracking Hive unit test statuses across different test suites including TestCliDriver, TestHBaseCliDriver, and TestMiniMrCliDriver with their corresponding JIRA tickets and resolution status.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/fix-hive-unit-tests-on-hadoop-2---hive-3949_31824506.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Test | JIRA | Status |\n| --- | --- | --- |\n| TestCliDriver.archive_excludeHadoop20.q  |  HIVE-4910? |  (star) Resolved  |\n| TestCliDriver.archive_multi.q  |  HIVE-3025? |   |\n| TestCliDriver.auto_join14.q  |  HIVE-4715  |  (star) Resolved  |\n| TestCliDriver.combine2.q  |  HIVE-4708  |  (star) Resolved  |\n| TestCliDriver.ctas_colname.q  |  HIVE-4717  |  (star) Resolved  |\n| TestCliDriver.groupby_grouping_sets4.q  |  HIVE-4717  |  (star) Resolved  |\n| TestCliDriver.infer_bucket_sort_list_bucket.q  |  HIVE-4645  |  (star) Resolved  |\n| TestCliDriver.input12.q  |  HIVE-4715  |  (star) Resolved  |\n| TestCliDriver.input39.q  |  HIVE-4715  |  (star) Resolved  |\n| TestCliDriver.join32_lessSize.q  |  HIVE-4717  |  (star) Resolved  |\n| TestCliDriver.join_1to1.q  |  HIVE-4689  |  (star) Resolved  |\n| TestCliDriver.join_vc.q  |  HIVE-4626  |  (star) Resolved  |\n| TestCliDriver.list_bucket_query_oneskew_1.q  |  HIVE-4711  |  (star) Resolved  |\n| TestCliDriver.list_bucket_query_oneskew_2.q  |  HIVE-4711  |  (star) Resolved  |\n| TestCliDriver.list_bucket_query_oneskew_3.q  |  HIVE-4711  |  (star) Resolved  |\n| TestCliDriver.multi_insert_lateral_view.q  |  HIVE-4717  |  (star) Resolved  |\n| TestCliDriver.orc_diff_part_cols.q  |  HIVE-4717  |  (star) Resolved  |\n| TestCliDriver.ptf_npath.q  |  HIVE-4721  |  (star) Resolved  |\n| TestCliDriver.recursive_dir.q  |  HIVE-4715  |  (star) Resolved  |\n| TestCliDriver.sample_islocalmode_hook.q  |  HIVE-4715  |  (star) Resolved  |\n| TestCliDriver.skewjoin.q  |  HIVE-4646  |  (star) Resolved  |\n| TestCliDriver.skewjoin_union_remove_1.q  |  HIVE-4713  |  (star) Resolved  |\n| TestCliDriver.skewjoin_union_remove_2.q  |  HIVE-4713  |  (star) Resolved  |\n| TestCliDriver.stats_partscan_1.q  |  HIVE-4690  |  (star) Resolved  |\n| TestCliDriver.truncate_column.q  |  HIVE-4712  |  (star) Resolved  |\n| TestCliDriver.truncate_column_merge.q  |  HIVE-4712  |  (star) Resolved  |\n| TestHBaseCliDriver.*  |  HIVE-4388  |  Needs investigation  |\n| TestCliDriver.list_bucket_dml_{2,4,5,9,12,13}.q  |  HIVE-4746  |  (star) Resolved  |\n| TestCliDriver.list_bucket_dml_{6,7,8}.q  |  HIVE-4750  |  (star) Resolved  |\n| TestMiniMrCliDriver.*  |  HIVE-4756  |  (star) Resolved  |\n```\n\n----------------------------------------\n\nTITLE: Querying Lock Status for Tables in Apache Hive\nDESCRIPTION: SQL commands to show locks on tables and partitions in Apache Hive. These commands are useful for debugging concurrency issues and understanding the current lock state of the system.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/locking_27362050.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSHOW LOCKS <TABLE_NAME>;\nSHOW LOCKS <TABLE_NAME> EXTENDED;\nSHOW LOCKS <TABLE_NAME> PARTITION (<PARTITION_DESC>);\nSHOW LOCKS <TABLE_NAME> PARTITION (<PARTITION_DESC>) EXTENDED;\n```\n\n----------------------------------------\n\nTITLE: Logarithmic and Exponential Functions in Hive SQL\nDESCRIPTION: Functions for logarithmic and exponential calculations including natural log, base-2 log, base-10 log, and custom base logarithms.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_43\n\nLANGUAGE: SQL\nCODE:\n```\nexp(double a)\nln(double a)\nlog10(double a)\nlog2(double a)\nlog(double base, double a)\n```\n\n----------------------------------------\n\nTITLE: Rewritten Query Using Materialized View in Hive\nDESCRIPTION: The equivalent rewritten query using the materialized view to answer the original query.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT empid, deptname\nFROM mv1\nWHERE hire_date >= '2018-01-01'\n    AND hire_date <= '2018-03-31';\n```\n\n----------------------------------------\n\nTITLE: Loading Data with HCatLoader in Pig\nDESCRIPTION: Example of using HCatLoader to read data from HCatalog-managed tables in Pig 0.14+\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_1\n\nLANGUAGE: pig\nCODE:\n```\nA = LOAD 'tablename' USING org.apache.hive.hcatalog.pig.HCatLoader();\n```\n\n----------------------------------------\n\nTITLE: Analyzing Column Statistics for Non-Partitioned Table\nDESCRIPTION: Example of gathering column statistics for a non-partitioned table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nANALYZE TABLE Table1 COMPUTE STATISTICS FOR COLUMNS;\n```\n\n----------------------------------------\n\nTITLE: Creating Database, Schemas, and Tables in MS SQL\nDESCRIPTION: This snippet demonstrates how to create a database, schemas, and tables in MS SQL. It creates a 'world' database with 'bob' and 'alice' schemas, each containing a 'country' table with sample data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE DATABASE world;\nUSE world;\n\nCREATE SCHEMA bob;\nCREATE TABLE bob.country\n(\n    id   int,\n    name varchar(20)\n);\n\ninsert into bob.country\nvalues (1, 'India');\ninsert into bob.country\nvalues (2, 'Russia');\ninsert into bob.country\nvalues (3, 'USA');\n\nCREATE SCHEMA alice;\nCREATE TABLE alice.country\n(\n    id   int,\n    name varchar(20)\n);\n\ninsert into alice.country\nvalues (4, 'Italy');\ninsert into alice.country\nvalues (5, 'Greece');\ninsert into alice.country\nvalues (6, 'China');\ninsert into alice.country\nvalues (7, 'Japan');\n```\n\n----------------------------------------\n\nTITLE: Implementing Index Handler Interface in Java\nDESCRIPTION: Defines the HiveIndexHandler interface and AbstractIndexHandler base class for creating custom index handlers in Hive. The interface specifies methods for index table management, validation, and build plan generation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/indexdev_27362104.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npackage org.apache.hadoop.hive.ql.metadata;\n\nimport java.util.List;\n\nimport org.apache.hadoop.conf.Configurable;\n\nimport org.apache.hadoop.hive.ql.plan.api.Task;\n\n/**\n * HiveIndexHandler defines a pluggable interface for adding new\n * index handlers to Hive.\n */\npublic interface HiveIndexHandler extends Configurable\n{\n  /**\n * Determines whether this handler implements indexes by creating\n * an index table.\n   *\n * @return true if index creation implies creation of an index table in Hive;\n * false if the index representation is not stored in a Hive table\n   */\n  boolean usesIndexTable();\n\n  /**\n * Requests that the handler validate an index definition and\n * fill in additional information about its stored representation.\n *\n * @param baseTable the definition of the table being indexed\n   *\n * @param index the definition of the index being created\n   *\n * @param indexTable a partial definition of the index table to be used for\n * storing the index representation, or null if usesIndexTable() returns\n * false; the handler can augment the index's storage descriptor\n * (e.g. with information about input/output format)\n * and/or the index table's definition (typically with additional\n * columns containing the index representation, e.g. pointers into HDFS)\n   *\n * @throw HiveException if the index definition is invalid with\n * respect to either the base table or the supplied index table definition\n   */\n  void analyzeIndexDefinition(\n    org.apache.hadoop.hive.metastore.api.Table baseTable,\n    org.apache.hadoop.hive.metastore.api.Index index,\n    org.apache.hadoop.hive.metastore.api.Table indexTable)\n      throws HiveException;\n\n  /**\n * Requests that the handler generate a plan for building the index;\n * the plan should read the base table and write out the index representation.\n   *\n * @param baseTable the definition of the table being indexed\n   *\n * @param index the definition of the index\n   *\n * @param partitions a list of specific partitions of the base\n * table for which the index should be built, or null if\n * an index for the entire table should be rebuilt\n   *\n * @param indexTable the definition of the index table, or\n * null if usesIndexTable() returns null\n   *\n * @return list of tasks to be executed in parallel for building\n * the index\n   *\n * @throw HiveException if plan generation fails\n   */\n  List<Task<?>> generateIndexBuildTaskList(\n    org.apache.hadoop.hive.metastore.api.Table baseTable,\n    org.apache.hadoop.hive.metastore.api.Index index,\n    List<org.apache.hadoop.hive.metastore.api.Partition> partitions,\n    org.apache.hadoop.hive.metastore.api.Table indexTable)\n      throws HiveException;\n}\n\n/**\n * Abstract base class for index handlers.  This is provided as insulation\n * so that as HiveIndexHandler evolves, default implementations of new\n * methods can be added here in order to avoid breaking existing\n * plugin implementations.\n */\npublic abstract class AbstractIndexHandler implements HiveIndexHandler\n{\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Structure for Apache Hive MapJoin Documentation\nDESCRIPTION: This snippet shows the overall structure of the documentation using Markdown syntax. It includes headers for overview, problem statement, proposed solution, and optimization details.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoin-and-partition-pruning_34015666.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Apache Hive : MapJoin and Partition Pruning\n\n* [Overview]({{< ref \"#overview\" >}})\n\t+ [Problem]({{< ref \"#problem\" >}})\n\t+ [Proposed Solution]({{< ref \"#proposed-solution\" >}})\n\t+ [Possible Extensions]({{< ref \"#possible-extensions\" >}})\n* [Optimization Details]({{< ref \"#optimization-details\" >}})\n\t+ [Compile Time]({{< ref \"#compile-time\" >}})\n\t+ [Runtime]({{< ref \"#runtime\" >}})\n\t+ [Pseudo Code]({{< ref \"#pseudo-code\" >}})\n```\n\n----------------------------------------\n\nTITLE: Defining Transform/Map-Reduce Syntax in HiveQL\nDESCRIPTION: This snippet outlines the syntax for using the TRANSFORM clause in Hive queries, including options for row formatting, clustering, distribution, and sorting.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-transform_27362047.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nclusterBy: CLUSTER BY colName (',' colName)*\ndistributeBy: DISTRIBUTE BY colName (',' colName)*\nsortBy: SORT BY colName (ASC | DESC)? (',' colName (ASC | DESC)?)*\n\nrowFormat\n  : ROW FORMAT\n    (DELIMITED [FIELDS TERMINATED BY char] \n               [COLLECTION ITEMS TERMINATED BY char]\n               [MAP KEYS TERMINATED BY char]\n               [ESCAPED BY char]\n               [LINES SEPARATED BY char]\n     | \n     SERDE serde_name [WITH SERDEPROPERTIES \n                            property_name=property_value, \n                            property_name=property_value, ...])\n\noutRowFormat : rowFormat\ninRowFormat : rowFormat\noutRecordReader : RECORDREADER className\n\nquery:\n  FROM (\n    FROM src\n    MAP expression (',' expression)*\n    (inRowFormat)?\n    USING 'my_map_script'\n    ( AS colName (',' colName)* )?\n    (outRowFormat)? (outRecordReader)?\n    ( clusterBy? | distributeBy? sortBy? ) src_alias\n  )\n  REDUCE expression (',' expression)*\n    (inRowFormat)?\n    USING 'my_reduce_script'\n    ( AS colName (',' colName)* )?\n    (outRowFormat)? (outRecordReader)?\n\n  FROM (\n    FROM src\n    SELECT TRANSFORM '(' expression (',' expression)* ')'\n    (inRowFormat)?\n    USING 'my_map_script'\n    ( AS colName (',' colName)* )?\n    (outRowFormat)? (outRecordReader)?\n    ( clusterBy? | distributeBy? sortBy? ) src_alias\n  )\n  SELECT TRANSFORM '(' expression (',' expression)* ')'\n    (inRowFormat)? \n    USING 'my_reduce_script'\n    ( AS colName (',' colName)* )?\n    (outRowFormat)? (outRecordReader)?\n```\n\n----------------------------------------\n\nTITLE: Creating an Iceberg Table Using CTAS (Create Table As Select)\nDESCRIPTION: Creates an Iceberg table by copying both schema and data from an existing source table using the CTAS (Create Table As Select) syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE TABLE_CTAS AS SELECT * FROM SRCTABLE STORED BY ICEBERG;\n```\n\n----------------------------------------\n\nTITLE: Using date_format Function in Hive SQL\nDESCRIPTION: Converts a date/timestamp/string to a formatted string using the specified pattern. The function can be used to implement other date functions like dayname and dayofyear. As of Hive 4.0.0, the formatter implementation can be configured.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\ndate_format('2015-04-08', 'y')\n```\n\n----------------------------------------\n\nTITLE: Marking Partition Events\nDESCRIPTION: Example of marking partitions as complete for batch notification purposes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-notification_34014558.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nHiveMetaStoreClient msc = new HiveMetaStoreClient(conf);\n\n// Create a map, specifying partition key names and values\nMap<String,String> partMap = new HashMap<String, String>();\npartMap.put(\"date\",\"20110711\");\npartMap.put(\"country\",\"*\");\n\n// Mark the partition as \"done\"\nmsc.markPartitionForEvent(\"mydb\", \"mytbl\", partMap, PartitionEventType.LOAD_DONE);\n```\n\n----------------------------------------\n\nTITLE: Describing Database Metadata in HQL\nDESCRIPTION: Shows the name, comment, and root location of a database. The EXTENDED option also displays database properties. Available since Hive 0.7, with SCHEMA keyword added in Hive 1.1.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_92\n\nLANGUAGE: hql\nCODE:\n```\nDESCRIBE DATABASE [EXTENDED] db_name;\nDESCRIBE SCHEMA [EXTENDED] db_name;     -- (Note: Hive 1.1.0 and later)\n```\n\n----------------------------------------\n\nTITLE: Rewritten Query Using Materialized View in Hive SQL\nDESCRIPTION: Rewritten version of the query using the materialized view instead of the original tables. This version directly uses the pre-calculated d_price column and applies filters, which should execute faster.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nSELECT SUM(d_price)\nFROM mv2\nWHERE d_year = 2013\n  AND lo_discount between 1 and 3;\n```\n\n----------------------------------------\n\nTITLE: Example of Grant and Revoke in Hive\nDESCRIPTION: Examples showing how to grant and revoke specific privileges on a table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\n0: jdbc:hive2://localhost:10000/default> grant select on table secured_table to role my_role;\nNo rows affected (0.046 seconds)\n\n0: jdbc:hive2://localhost:10000/default> revoke update, select on table secured_table from role my_role;\nNo rows affected (0.028 seconds)\n```\n\n----------------------------------------\n\nTITLE: Using explode() UDTF with Arrays in Hive SQL\nDESCRIPTION: The explode() function takes an array as input and outputs one row for each element in the array. It returns a row-set with a single column named 'col'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nexplode(ARRAY<T> a)\n```\n\n----------------------------------------\n\nTITLE: Current Date and Timestamp in Hive SQL\nDESCRIPTION: Demonstrates functions for retrieving the current date and timestamp at the start of query evaluation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\ncurrent_date\n```\n\nLANGUAGE: SQL\nCODE:\n```\ncurrent_timestamp\n```\n\n----------------------------------------\n\nTITLE: Using isnull Function in Hive SQL\nDESCRIPTION: Returns true if the input value is NULL and false otherwise. Used for NULL checking in queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\nisnull( a )\n```\n\n----------------------------------------\n\nTITLE: CSV2 Format with Quoting Disabled in Beeline\nDESCRIPTION: Example of csv2 output format with quoting disabled (set by disable.quoting.for.sv=true). Special characters are not escaped, which can cause parsing issues if values contain the delimiter character.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_24\n\nLANGUAGE: text\nCODE:\n```\nid,value,comment\n1,Value,1,Value contains comma\n2,Value\"2,Value contains double quote\n3,Value'3,Value contains single quote\n```\n\n----------------------------------------\n\nTITLE: Examples of Enhanced SHOW PARTITIONS in Hive 4.0.0+\nDESCRIPTION: Various examples of using the enhanced SHOW PARTITIONS command with WHERE, ORDER BY, and LIMIT clauses introduced in Hive 4.0.0. These examples demonstrate different ways to filter and order partition listings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_77\n\nLANGUAGE: sql\nCODE:\n```\nSHOW PARTITIONS databaseFoo.tableBar LIMIT 10;                                                               -- (Note: Hive 4.0.0 and later)\nSHOW PARTITIONS databaseFoo.tableBar PARTITION(ds='2010-03-03') LIMIT 10;                                    -- (Note: Hive 4.0.0 and later)\nSHOW PARTITIONS databaseFoo.tableBar PARTITION(ds='2010-03-03') ORDER BY hr DESC LIMIT 10;                   -- (Note: Hive 4.0.0 and later)\nSHOW PARTITIONS databaseFoo.tableBar PARTITION(ds='2010-03-03') WHERE hr >= 10 ORDER BY hr DESC LIMIT 10;    -- (Note: Hive 4.0.0 and later)\nSHOW PARTITIONS databaseFoo.tableBar WHERE hr >= 10 AND ds='2010-03-03' ORDER BY hr DESC LIMIT 10;           -- (Note: Hive 4.0.0 and later)\n```\n\n----------------------------------------\n\nTITLE: UNION with Type Conversion\nDESCRIPTION: Demonstrates explicit type casting when combining columns of different types in a UNION operation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-union_27362049.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n  SELECT name, id, cast('2001-01-01' as date) d FROM source_table_1\n  UNION ALL\n  SELECT name, id, hiredate as d FROM source_table_2\n```\n\n----------------------------------------\n\nTITLE: Using posexplode() UDTF in Hive SQL\nDESCRIPTION: The posexplode() function explodes an array into multiple rows and includes the position (index) of each element. It returns a row-set with two columns: 'pos' (position, starting from 0) and 'val' (element value).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nposexplode(ARRAY<T> a)\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Assembly JAR Location (Pre-2.2.0)\nDESCRIPTION: XML configuration for specifying Spark assembly JAR location in HDFS for versions prior to Hive 2.2.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/44302539.md#2025-04-09_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>spark.yarn.jar</name>\n  <value>hdfs://xxxx:8020/spark-assembly.jar</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Implementing gRPC to Thrift Translation Layer in Java\nDESCRIPTION: Sample implementation of a getTable method that translates between gRPC and Thrift requests/responses in the Hive Metastore server.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/158869886.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// Returns gRPC Response, and takes in gRPC GetTableRequest\npublic GetTableResponse getTable(GetTableRequest grpcTableRequest) {\n  // convertToThriftRequest is implemented by a different library, not in Hive code\n  Table thriftGetTableRequest = convertToThriftRequest(grpcTableRequest);\n  // Result is a Thrift API object    \n  GetTableResult result = HiveMetaStoreThriftServer.get_table(thriftTable);\n  return convertToThriftResponse(result);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive JDBC Connection with Browser Authentication\nDESCRIPTION: This code snippet demonstrates how to configure a Hive JDBC connection URL with browser-based authentication. It specifies the HiveServer2 host, port, authentication method, SAML response port, and timeout.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/170266662.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\njdbc:hive2://HiveServer2-host:10001/default;transportMode=http;httpPath=cliservice;auth=browser;samlResponsePort=12345;samlResponseTimeout=120\n```\n\n----------------------------------------\n\nTITLE: Extracting Substring Before/After Delimiter Occurrences in Hive\nDESCRIPTION: Returns the substring from string A before or after count occurrences of the delimiter. If count is positive, returns everything to the left of the final delimiter; if negative, returns everything to the right.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nsubstring_index(string A, string delim, int count)\n```\n\n----------------------------------------\n\nTITLE: HLL Distinct Count with Intermediate Table in Hive\nDESCRIPTION: Demonstrates using HyperLogLog (HLL) sketch to compute distinct values using an intermediate table in Hive. It builds sketches per category, estimates counts, and unions sketches across categories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/datasketches-integration_177050456.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n-- build sketches per category\ncreate temporary table sketch_intermediate (category char(1), sketch binary);\ninsert into sketch_intermediate select category, ds_hll_sketch(id) from sketch_input group by category;\n\n-- get unique count estimates per category\nselect category, ds_hll_estimate(sketch) from sketch_intermediate;\n\n-- union sketches across categories and get overall unique count estimate\nselect ds_hll_estimate(ds_hll_union(sketch)) from sketch_intermediate;\n```\n\n----------------------------------------\n\nTITLE: Updating Data in an Iceberg Table\nDESCRIPTION: Demonstrates updating a specific record in an Iceberg table by identifying it with a WHERE clause and setting new values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nUPDATE TBL_ICE WHERE ID=8 SET ID=2;\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partitioning in Pig with Partial Specification\nDESCRIPTION: Examples showing how to use HCatStorer with partial partition specifications, where only some partition keys are provided and others are determined from the data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-dynamicpartitions_34014006.md#2025-04-09_snippet_4\n\nLANGUAGE: pig\nCODE:\n```\nstore A into 'mytable' using HCatStorer(\"a=1\");\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Streaming API Settings\nDESCRIPTION: Core configuration settings that are internally overridden by the Hive Streaming API to ensure correct streaming behavior, regardless of values in hive-site.xml or custom HiveConf.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest-v2_85477610.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\nhive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager\nhive.support.concurrency = true\nhive.metastore.execute.setugi = true\nhive.exec.dynamic.partition.mode = nonstrict\nhive.exec.orc.delta.streaming.optimizations.enabled = true\nhive.metastore.client.cache.enabled = false\n```\n\n----------------------------------------\n\nTITLE: File Movement Commands in SVN\nDESCRIPTION: Shell commands for moving files in SVN and updating class references using perl\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ svn mv MyCLass.java MyClass.java\n$ perl -i -pe 's<at:var at:name=\"MyCLass\" />MyClass@g' MyClass.java\n```\n\n----------------------------------------\n\nTITLE: Setting SerDe Encoding Properties in Hive\nDESCRIPTION: Example showing how to set character encoding properties for LazySimpleSerDe using ALTER TABLE command. Demonstrates setting GBK encoding for a table named 'person'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE person SET SERDEPROPERTIES ('serialization.encoding'='GBK');\n```\n\n----------------------------------------\n\nTITLE: Clean Build with Debug Support\nDESCRIPTION: Commands to clean existing build and recompile Hive with debug support enabled.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n    > ant clean  # not necessary if the first time to compile\n    > ant -Djavac.debug=on package\n```\n\n----------------------------------------\n\nTITLE: Disabling Hive Variable Substitution\nDESCRIPTION: Command to disable Hive variable substitution functionality.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-variablesubstitution_30754722.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nset hive.variable.substitute=false;\n```\n\n----------------------------------------\n\nTITLE: Extracting Date Components in Hive SQL\nDESCRIPTION: A collection of functions to extract specific components (year, quarter, month, day, hour, minute, second) from a date or timestamp string.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_31\n\nLANGUAGE: SQL\nCODE:\n```\nyear(string date)\nquarter(date/timestamp/string)\nmonth(string date)\nday(string date)\ndayofmonth(date)\nhour(string date)\nminute(string date)\nsecond(string date)\nweekofyear(string date)\n```\n\n----------------------------------------\n\nTITLE: Granting Object Privileges in Hive\nDESCRIPTION: Grants specific privileges on tables or views to users or roles with optional grant option.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nGRANT\n    priv_type [, priv_type ] ...\n    ON table_or_view_name\n    TO principal_specification [, principal_specification] ...\n    [WITH GRANT OPTION];\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Example\nDESCRIPTION: Query showing grouping by multiple dimensions with aggregations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSELECT max(delta), sum(added)\nFROM druid_table_1\nGROUP BY `channel`, `user`;\n```\n\n----------------------------------------\n\nTITLE: Getting Current Unix Timestamp in Hive SQL\nDESCRIPTION: Retrieves the current Unix timestamp in seconds using the unix_timestamp function. This function is not deterministic and has been deprecated since Hive 2.0 in favor of the CURRENT_TIMESTAMP constant.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_28\n\nLANGUAGE: SQL\nCODE:\n```\nunix_timestamp()\n```\n\n----------------------------------------\n\nTITLE: Creating User and Granting Permissions in MS SQL\nDESCRIPTION: This snippet shows how to create a user in MS SQL, associate them with a default schema, and grant necessary permissions for connecting and querying.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE LOGIN greg WITH PASSWORD = 'GregPass123!$';\nCREATE USER greg FOR LOGIN greg WITH DEFAULT_SCHEMA=bob;\nGRANT CONNECT, SELECT TO greg;\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto Local Mode Execution\nDESCRIPTION: Setting to control automatic local mode execution for small map-reduce jobs. Disabled by default.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n  hive> SET hive.exec.mode.local.auto=false;\n```\n\n----------------------------------------\n\nTITLE: Converting Datetime String to Unix Timestamp in Hive SQL\nDESCRIPTION: Shows how to use the unix_timestamp function to convert a datetime string to Unix time (seconds since epoch). It can be used with or without a specified pattern.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nunix_timestamp('2009-03-20 11:30:01')\n```\n\nLANGUAGE: SQL\nCODE:\n```\nunix_timestamp('2009-03-20', 'uuuu-MM-dd')\n```\n\n----------------------------------------\n\nTITLE: Analyzing Complex View Dependencies with EXPLAIN DEPENDENCY\nDESCRIPTION: This example demonstrates creating nested views and using EXPLAIN DEPENDENCY to analyze the complex dependencies between multiple views and their underlying tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW V2 AS SELECT ds, key, value FROM srcpart WHERE ds IS NOT NULL;\nCREATE VIEW V4 AS\n  SELECT src1.key, src2.value as value1, src3.value as value2\n  FROM V1 src1 JOIN V2 src2 on src1.key = src2.key JOIN src src3 ON src2.key = src3.key;\nEXPLAIN DEPENDENCY SELECT * FROM V4;\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Decimal Columns in Hive SQL\nDESCRIPTION: Demonstrates how to create a table with decimal columns in Hive, specifying precision and scale. The example shows default behavior and explicit precision/scale definition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE foo (\n  a DECIMAL, -- Defaults to decimal(10,0)\n  b DECIMAL(9, 7)\n)\n```\n\n----------------------------------------\n\nTITLE: Hive CLI Command Examples\nDESCRIPTION: Examples demonstrating various ways to execute queries, set configuration variables, and handle data output using the Hive CLI.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-cli_27362033.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$HIVE_HOME/bin/hive -e 'select a.col from tab1 a'\n$HIVE_HOME/bin/hive -e 'select a.col from tab1 a' --hiveconf hive.exec.scratchdir=/home/my/hive_scratch  --hiveconf mapred.reduce.tasks=32\n$HIVE_HOME/bin/hive -S -e 'select a.col from tab1 a' > a.txt\n$HIVE_HOME/bin/hive -f /home/my/hive-script.sql\n$HIVE_HOME/bin/hive -f hdfs://<namenode>:<port>/hive-script.sql\n$HIVE_HOME/bin/hive -f s3://mys3bucket/s3-script.sql\n$HIVE_HOME/bin/hive -i /home/my/hive-init.sql\n```\n\n----------------------------------------\n\nTITLE: Using months_between Function in Hive SQL\nDESCRIPTION: Calculates the number of months between two dates. Returns positive values if date1 is later than date2, and negative if earlier. The result is rounded to 8 decimal places and handles fractional months based on a 31-day month.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nmonths_between('1997-02-28 10:30:00', '1996-10-30')\n```\n\n----------------------------------------\n\nTITLE: Deleting a Table using WebHCat API with cURL\nDESCRIPTION: This curl command demonstrates how to delete a table named 'test_table' from the 'default' database using the WebHCat API. It includes the necessary user authentication parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletetable_34016561.md#2025-04-09_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\n% curl -s -X DELETE 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Adding Partitions to Hive Tables Using ALTER TABLE\nDESCRIPTION: Syntax for adding partitions to a Hive table. Allows specifying partition column values and optional storage location. The IF NOT EXISTS clause can be used to avoid errors if the partition already exists.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_33\n\nLANGUAGE: hql\nCODE:\n```\nALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION 'location'][, PARTITION partition_spec [LOCATION 'location'], ...];\n\npartition_spec:\n  : (partition_column = partition_col_value, partition_column = partition_col_value, ...)\n\n```\n\n----------------------------------------\n\nTITLE: Cost-Based CBO Explain Query Example\nDESCRIPTION: Example of EXPLAIN CBO COST command that includes cost estimations for each operator in the execution plan.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN CBO COST\nWITH customer_total_return AS\n(SELECT sr_customer_sk AS ctr_customer_sk,\n  sr_store_sk AS ctr_store_sk,\n  SUM(SR_FEE) AS ctr_total_return\n  FROM store_returns, date_dim\n  WHERE sr_returned_date_sk = d_date_sk\n    AND d_year =2000\n  GROUP BY sr_customer_sk, sr_store_sk)\nSELECT c_customer_id\nFROM customer_total_return ctr1, store, customer\nWHERE ctr1.ctr_total_return > (SELECT AVG(ctr_total_return)*1.2\nFROM customer_total_return ctr2\nWHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk)\n  AND s_store_sk = ctr1.ctr_store_sk\n  AND s_state = 'NM'\n  AND ctr1.ctr_customer_sk = c_customer_sk\nORDER BY c_customer_id\nLIMIT 100\n```\n\n----------------------------------------\n\nTITLE: Defining HiveServer2 Thrift API Schema\nDESCRIPTION: Complete Thrift definition for the HiveServer2 API, including namespace declarations, protocol versioning, type definitions, and session operations. This specification defines the structures for type representation, result sets, session management, and operation handling.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-thrift-api_27843687.md#2025-04-09_snippet_1\n\nLANGUAGE: thrift\nCODE:\n```\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// required for GetQueryPlan()\ninclude \"ql/if/queryplan.thrift\"\n\nnamespace java org.apache.hive.service\nnamespace cpp Apache.Hive\n\n// List of protocol versions. A new token should be\n// added to the end of this list every time a change is made.\nenum ProtocolVersion {\n  HIVE_SERVER2_PROTOCOL_V1\n}\n\nenum TType {\n  BOOLEAN_TYPE,\n  TINYINT_TYPE,\n  SMALLINT_TYPE,\n  INT_TYPE,\n  BIGINT_TYPE,\n  FLOAT_TYPE,\n  DOUBLE_TYPE,\n  STRING_TYPE,\n  TIMESTAMP_TYPE,\n  BINARY_TYPE,\n  ARRAY_TYPE,\n  MAP_TYPE,\n  STRUCT_TYPE,\n  UNION_TYPE,\n  USER_DEFINED_TYPE\n}\n  \nconst set<TType> PRIMITIVE_TYPES = [\n  TType.BOOLEAN_TYPE\n  TType.TINYINT_TYPE\n  TType.SMALLINT_TYPE\n  TType.INT_TYPE\n  TType.BIGINT_TYPE\n  TType.FLOAT_TYPE\n  TType.DOUBLE_TYPE\n  TType.STRING_TYPE\n  TType.TIMESTAMP_TYPE\n  TType.BINARY_TYPE\n]\n\nconst set<TType> COMPLEX_TYPES = [\n  TType.ARRAY_TYPE\n  TType.MAP_TYPE\n  TType.STRUCT_TYPE\n  TType.UNION_TYPE\n  TType.USER_DEFINED_TYPE\n]\n\nconst set<TType> COLLECTION_TYPES = [\n  TType.ARRAY_TYPE\n  TType.MAP_TYPE\n]\n\nconst map<TType,string> TYPE_NAMES = {\n  TType.BOOLEAN_TYPE: \"BOOLEAN\",\n  TType.TINYINT_TYPE: \"TINYINT\",\n  TType.SMALLINT_TYPE: \"SMALLINT\",\n  TType.INT_TYPE: \"INT\",\n  TType.BIGINT_TYPE: \"BIGINT\",\n  TType.FLOAT_TYPE: \"FLOAT\",\n  TType.DOUBLE_TYPE: \"DOUBLE\",\n  TType.STRING_TYPE: \"STRING\",\n  TType.TIMESTAMP_TYPE: \"TIMESTAMP\",\n  TType.BINARY_TYPE: \"BINARY\",\n  TType.ARRAY_TYPE: \"ARRAY\",\n  TType.MAP_TYPE: \"MAP\",\n  TType.STRUCT_TYPE: \"STRUCT\",\n  TType.UNION_TYPE: \"UNIONTYPE\"\n}\n\n// Thrift does not support recursively defined types or forward declarations,\n// which makes it difficult to represent Hive's nested types.\n// To get around these limitations TTypeDesc employs a type list that maps\n// integer \"pointers\" to TTypeEntry objects. The following examples show\n// how different types are represented using this scheme:\n//\n// \"INT\":\n// TTypeDesc {\n//   types = [\n//     TTypeEntry.primitive_entry {\n//       type = INT_TYPE\n//     }\n//   ]\n// }\n//\n// \"ARRAY<INT>\":\n// TTypeDesc {\n//   types = [\n//     TTypeEntry.array_entry {\n//       object_type_ptr = 1\n//     },\n//     TTypeEntry.primitive_entry {\n//       type = INT_TYPE\n//     }\n//   ]\n// }\n//\n// \"MAP<INT,STRING>\":\n// TTypeDesc {\n//   types = [\n//     TTypeEntry.map_entry {\n//       key_type_ptr = 1\n//       value_type_ptr = 2\n//     },\n//     TTypeEntry.primitive_entry {\n//       type = INT_TYPE\n//     },\n//     TTypeEntry.primitive_entry {\n//       type = STRING_TYPE\n//     }\n//   ]\n// }\n\ntypedef i32 TTypeEntryPtr\n\n// Type entry for a primitive type.\nstruct TPrimitiveTypeEntry {\n  // The primitive type token. This must satisfy the condition\n  // that type is in the PRIMITIVE_TYPES set.\n  1: required TType type\n}\n\n// Type entry for an ARRAY type.\nstruct TArrayTypeEntry {\n  1: required TTypeEntryPtr object_type_ptr\n}\n\n// Type entry for a MAP type.\nstruct TMapTypeEntry {\n  1: required TTypeEntryPtr key_type_ptr\n  2: required TTypeEntryPtr value_type_ptr\n}\n\n// Type entry for a STRUCT type.\nstruct TStructTypeEntry {\n  1: required map<string, TTypeEntryPtr> name_to_type_ptr\n}\n\n// Type entry for a UNIONTYPE type.\nstruct TUnionTypeEntry {\n  1: required map<string, TTypeEntryPtr> name_to_type_ptr\n}\n\nstruct TUserDefinedTypeEntry {\n  // The fully qualified name of the class implementing this type.\n  1: required string typeClassName\n}\n\n// We use a union here since Thrift does not support inheritance.\nunion TTypeEntry {\n  1: TPrimitiveTypeEntry primitive_entry\n  2: TArrayTypeEntry array_entry\n  3: TMapTypeEntry map_entry\n  4: TStructTypeEntry struct_entry\n  5: TUnionTypeEntry union_entry\n  6: TUserDefinedTypeEntry user_defined_type_entry\n}\n\n// Type descriptor for columns.\nstruct TTypeDesc {\n  // The \"top\" type is always the first element of the list.\n  // If the top type is an ARRAY, MAP, STRUCT, or UNIONTYPE\n  // type, then subsequent elements represent nested types.\n  1: required list<TTypeEntry> types\n}\n\n// A result set column descriptor.\nstruct TColumnDesc {\n  // The name of the column\n  1: required string col_name\n\n  // The type descriptor for this column\n  2: required TTypeDesc type_desc\n  \n  // The ordinal position of this column in the schema\n  3: required i32 position\n\n  4: optional string comment\n}\n\n// Metadata used to describe the schema (column names, types, comments)\n// of result sets.\nstruct TTableSchema {\n  1: required list<TColumnDesc> columns\n}\n\n// A single column value in a result set.\n// Note that Hive's type system is richer than Thrift's,\n// so in some cases we have to map multiple Hive types\n// to the same Thrift type. On the client-side this is\n// disambiguated by looking at the Schema of the\n// result set.\nunion TColumnValue {\n  1: bool   bool_val      // BOOLEAN\n  2: byte   byte_val      // TINYINT\n  3: i16    i16_val       // SMALLINT\n  4: i32    i32_val       // INT\n  5: i64    i64_val       // BIGINT, TIMESTAMP\n  6: double double_val    // FLOAT, DOUBLE\n  7: string string_val    // STRING, LIST, MAP, STRUCT, UNIONTYPE, BINARY\n}\n\n// Represents a row in a rowset.\nstruct TRow {\n  1: list<TColumnValue> colVals\n}\n\n// Represents a rowset\nstruct TRowSet {\n  // The starting row offset of this rowset.\n  1: i64 start_row_offset\n  2: list<TRow> rows\n}\n\n// The return status code contained in each response.\nenum TStatusCode {\n  SUCCESS,\n  SUCCESS_WITH_INFO,\n  SQL_STILL_EXECUTING,\n  ERROR,\n  INVALID_HANDLE\n}\n\n// The return status of a remote request\nstruct TStatus {\n  1: required TStatusCode status_code\n\n  // If status is SUCCESS_WITH_INFO, info_msgs may be populated with\n  // additional diagnostic information.\n  2: optional list<string> info_msgs\n\n  // If status is ERROR, then the following fields may be set\n  3: optional string sql_state  // as defined in the ISO/IEF CLI specification\n  4: optional i32 error_code    // internal error code\n  5: optional string error_message\n}\n\n// The state of an operation (i.e. a query or other\n// asynchronous operation that generates a result set)\n// on the server.\nenum TOperationState {\n  // The operation is running. In this state the result\n  // set is not available.\n  RUNNING,\n\n  // The operation has completed. When an operation is in\n  // this state it's result set may be fetched.\n  FINISHED,\n\n  // The operation was canceled by a client\n  CANCELED,\n\n  // The operation failed due to an error\n  ERROR\n}\n\n// A string identifier. This is interpreted literally.\ntypedef string TIdentifier\n\n// A search pattern.\n//\n// Valid search pattern characters:\n// '_': Any single character.\n// '%': Any sequence of zero or more characters.\n// '\\': Escape character used to include special characters,\n//      e.g. '_', '%', '\\'. If a '\\' precedes a non-special\n//      character it has no special meaning and is interpreted\n//      literally.\ntypedef string TPattern\n\n// A search pattern or identifier. Used as input\n// parameter for many of the catalog functions.\nunion TPatternOrIdentifier {\n  1: TIdentifier identifier\n  2: TPattern pattern\n}\n\nstruct THandleIdentifier {\n  // 16 byte globally unique identifier\n  // This is the public ID of the handle and\n  // can be used for reporting.\n  1: binary guid,\n\n  // 16 byte secret generated by the server\n  // and used to verify that the handle is not\n  // being hijacked by another user.\n  2: binary secret,\n}\n\n// Client-side handle to persistent\n// session information on the server-side.\nstruct TSessionHandle {\n  1: required THandleIdentifier sess_id\n}\n\n// The subtype of an OperationHandle.\nenum TOperationType {\n  EXECUTE_STATEMENT,\n  GET_TYPE_INFO,\n  GET_TABLES,\n  GET_COLUMNS,\n  GET_FUNCTIONS,\n}\n\n// Client-side reference to a task running\n// asynchronously on the server.\nstruct TOperationHandle {\n  1: required THandleIdentifier op_id\n  2: required TOperationType op_type\n}\n\n// OpenSession()\n//\n// Open a session (connection) on the server against\n// which operations may be executed. \nstruct TOpenSessionReq {\n  // The version of the HiveServer2 protocol that the client is using.\n  1: required ProtocolVersion client_protocol = ProtocolVersion.HIVE_SERVER2_PROTOCOL_V1\n  \n  // Username and password for authentication.\n  // Depending on the authentication scheme being used,\n  // this information may instead be provided by a lower\n  // protocol layer, in which case these fields may be\n  // left unset.\n  2: optional string username\n  3: optional string password\n\n  // Configuration overlay which is applied when the session is\n  // first created.\n  4: optional map<string, string> configuration\n}\n\nstruct TOpenSessionResp {\n  1: required TStatus status\n\n  // The protocol version that the server is using.\n  2: ProtocolVersion server_protocol = ProtocolVersion.HIVE_SERVER2_PROTOCOL_V1\n\n  // Session Handle\n  3: TSessionHandle session_handle\n\n  // The configuration settings for this session.\n  4: map<string, string> configuration\n}\n\n// CloseSession()\n//\n// Closes the specified session and frees any resources\n// currently allocated to that session. Any open\n// operations in that session will be canceled.\nstruct TCloseSessionReq {\n  1: required TSessionHandle session_handle\n}\n\nstruct TCloseSessionResp {\n  1: required TStatus status\n}\n\n// GetInfo()\n//\n// This function is based on ODBC's SQLGetInfo() function.\n```\n\n----------------------------------------\n\nTITLE: Basic SHOW PARTITIONS Syntax in Hive\nDESCRIPTION: The basic syntax for the SHOW PARTITIONS command which lists all existing partitions for a given table in alphabetical order. This command helps in exploring the partition structure of partitioned tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_72\n\nLANGUAGE: sql\nCODE:\n```\nSHOW PARTITIONS table_name;\n```\n\n----------------------------------------\n\nTITLE: Manually Assigning Compaction Pool in Apache Hive SQL\nDESCRIPTION: This SQL command demonstrates how to manually assign a compaction request to a specific pool using the ALTER TABLE COMPACT statement. This overrides any pool assignment set by the hive.compactor.worker.pool property.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/compaction-pooling_240884493.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE COMPACT table_name POOL 'pool_name';\n```\n\n----------------------------------------\n\nTITLE: Running Hive JDBC Client with Required JAR Dependencies\nDESCRIPTION: Command-line instructions for compiling and running the JDBC client with the necessary JAR files for different modes (non-kerberos, kerberos secure, and embedded modes). Lists all required dependencies from Hive and Hadoop.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_41\n\nLANGUAGE: bash\nCODE:\n```\n# Then on the command-line\n$ javac HiveJdbcClient.java\n\n# To run the program using remote hiveserver in non-kerberos mode, we need the following jars in the classpath\n# from hive/build/dist/lib\n#     hive-jdbc*.jar\n#     hive-service*.jar\n#     libfb303-0.9.0.jar\n#  \tlibthrift-0.9.0.jar\n# \tlog4j-1.2.16.jar\n# \tslf4j-api-1.6.1.jar\n# \tslf4j-log4j12-1.6.1.jar\n# \tcommons-logging-1.0.4.jar\n#\n#\n# To run the program using kerberos secure mode, we need the following jars in the classpath \n#     hive-exec*.jar\n#     commons-configuration-1.6.jar (This is not needed with Hadoop 2.6.x and later).\n#  and from hadoop\n#     hadoop-core*.jar (use hadoop-common*.jar for Hadoop 2.x)\n#\n# To run the program in embedded mode, we need the following additional jars in the classpath\n# from hive/build/dist/lib\n#     hive-exec*.jar\n#     hive-metastore*.jar\n#     antlr-runtime-3.0.1.jar\n#     derby.jar\n#     jdo2-api-2.1.jar\n#     jpox-core-1.2.2.jar\n#     jpox-rdbms-1.2.2.jar\n# and from hadoop/build\n#     hadoop-core*.jar\n# as well as hive/build/dist/conf, any HIVE_AUX_JARS_PATH set, \n# and hadoop jars necessary to run MR jobs (eg lzo codec)\n\n$ java -cp $CLASSPATH HiveJdbcClient\n```\n\n----------------------------------------\n\nTITLE: Accessing Record Identifiers in Java using AcidInputFormat\nDESCRIPTION: This snippet demonstrates how to obtain the ROW__ID (RecordIdentifier) for records in an ACID table using the AcidInputFormat and AcidRecordReader classes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118454.md#2025-04-09_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nAcidInputFormat.getRecordIdentifier()\n```\n\n----------------------------------------\n\nTITLE: Running Hive Test Suite with Ant\nDESCRIPTION: Command to run the Hive test suite, including Hive client tests, using Ant build system.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ ant test -Dthrift.home=<THRIFT_HOME>\n```\n\n----------------------------------------\n\nTITLE: Connecting to HiveServer2 with Command Line Parameters\nDESCRIPTION: Shows how to connect to HiveServer2 using Beeline with connection parameters specified on the command line.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n% beeline -u jdbc:hive2://localhost:10000/default -n scott -w password_file\nHive version 0.11.0-SNAPSHOT by Apache\n\nConnecting to jdbc:hive2://localhost:10000/default\n```\n\n----------------------------------------\n\nTITLE: Case J1: Hive SQL Outer Join with Preserved Row Table Predicate\nDESCRIPTION: Example showing how Hive handles a join predicate on the preserved row table in a left outer join. The execution plan demonstrates that the predicate is not pushed down.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/outerjoinbehavior_35749927.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nexplain \nselect s1.key, s2.key \nfrom src s1 left join src s2 on s1.key > '2';\n```\n\n----------------------------------------\n\nTITLE: Implementing GenericUDTF for Custom Table Generation in Apache Hive (Java)\nDESCRIPTION: This code snippet demonstrates how to create a custom UDTF by extending the GenericUDTF abstract class. It implements the required methods: initialize, process, and close. The example UDTF counts the number of rows processed and outputs the count twice.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide-udtf_27362086.md#2025-04-09_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npackage org.apache.hadoop.hive.contrib.udtf.example;\n\nimport java.util.ArrayList;\n\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\nimport org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n\n/**\n * GenericUDTFCount2 outputs the number of rows seen, twice. It's output twice\n * to test outputting of rows on close with lateral view.\n *\n */\npublic class GenericUDTFCount2 extends GenericUDTF {\n\n  Integer count = Integer.valueOf(0);\n  Object forwardObj[] = new Object[1];\n\n  @Override\n  public void close() throws HiveException {\n    forwardObj[0] = count;\n    forward(forwardObj);\n    forward(forwardObj);\n  }\n\n  @Override\n  public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {\n    ArrayList<String> fieldNames = new ArrayList<String>();\n    ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();\n    fieldNames.add(\"col1\");\n    fieldOIs.add(PrimitiveObjectInspectorFactory.javaIntObjectInspector);\n    return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,\n        fieldOIs);\n  }\n\n  @Override\n  public void process(Object[] args) throws HiveException {\n    count = Integer.valueOf(count.intValue() + 1);\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Summary of Hive on Spark Integration Project\nDESCRIPTION: Summarizes the Hive on Spark integration project, acknowledging its design simplicity but implementation complexity, outlining a phased approach and the need for collaboration between communities.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_11\n\nLANGUAGE: markdown\nCODE:\n```\n# 4. Summary\n\nIt can be seen from above analysis that the project of Spark on Hive is simple and clean in terms of functionality and design, while complicated and involved in implementation, which may take significant time and resources. Therefore, we are going to take a phased approach and expect that the work on optimization and improvement will be on-going in a relatively long period of time while all basic functionality will be there in the first phase.\n\nSecondly, we expect the integration between Hive and Spark will not be always smooth. Functional gaps may be identified and problems may arise. We anticipate that Hive community and Spark community will work closely to resolve any obstacles that might come on the way.\n\nNevertheless, we believe that the impact on existing code path is minimal. While Spark execution engine may take some time to stabilize, MapReduce and Tez should continue working as it is.\n```\n\n----------------------------------------\n\nTITLE: Submitting MapReduce streaming job using WebHCat API\nDESCRIPTION: This curl command demonstrates how to submit a MapReduce streaming job through the WebHCat API. It specifies input and output locations, mapper and reducer programs, and the user name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-mapreducestream_34017023.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -d input=mydata \\\n       -d output=mycounts \\\n       -d mapper=/bin/cat \\\n       -d reducer=\"/usr/bin/wc -w\" \\\n       'http://localhost:50111/templeton/v1/mapreduce/streaming?user.name=ekoifman'\n```\n\n----------------------------------------\n\nTITLE: Filtering SHOW PARTITIONS in Hive 0.6+\nDESCRIPTION: Examples of using SHOW PARTITIONS with filters introduced in Hive 0.6. These examples demonstrate how to filter partitions by specifying parts of a partition specification to narrow down the results.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_73\n\nLANGUAGE: sql\nCODE:\n```\nSHOW PARTITIONS table_name PARTITION(ds='2010-03-03');            -- (Note: Hive 0.6 and later)\nSHOW PARTITIONS table_name PARTITION(hr='12');                    -- (Note: Hive 0.6 and later)\nSHOW PARTITIONS table_name PARTITION(ds='2010-03-03', hr='12');   -- (Note: Hive 0.6 and later)\n```\n\n----------------------------------------\n\nTITLE: Distinct Aggregation with Ordering and Window Specification in HiveQL\nDESCRIPTION: Demonstrates the use of DISTINCT with aggregation functions, including ORDER BY and window specification, supported in Hive 2.2.0 and later.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCOUNT(DISTINCT a) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)\n```\n\n----------------------------------------\n\nTITLE: Viewing Top K Statistics with DESCRIBE FORMATTED for Partition\nDESCRIPTION: HQL command to view the formatted description of a partition, which includes the top K statistics in the skewed information section if available.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_8\n\nLANGUAGE: hql\nCODE:\n```\nDESCRIBE FORMATTED table1 partition (ds='2012-09-07');\n```\n\n----------------------------------------\n\nTITLE: Executing Union Query in Hive SQL\nDESCRIPTION: Example query structure showing a UNION ALL operation between two subqueries that triggers the optimization when followed by SELECT *.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/union-optimization_29688910.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect * from \n (subq1 \n UNION ALL \n sub2) u;\n```\n\n----------------------------------------\n\nTITLE: Setting Map Join Small Table File Size Threshold\nDESCRIPTION: Defines the threshold (in bytes) for the input file size of small tables for map join conversion.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_11\n\nLANGUAGE: properties\nCODE:\n```\nhive.mapjoin.smalltable.filesize=25000000\n```\n\n----------------------------------------\n\nTITLE: Archiving a Hive Table Partition\nDESCRIPTION: Demonstrates the SQL syntax for archiving a specific partition in a Hive table. This command will initiate a MapReduce job to perform the archiving process.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-archiving_27362031.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name ARCHIVE PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)\n```\n\n----------------------------------------\n\nTITLE: Importing Hive UDF Base Class\nDESCRIPTION: The base import statement required for implementing standard Hive UDFs that process single input rows and return single output rows.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_83\n\nLANGUAGE: java\nCODE:\n```\norg.apache.hadoop.hive.ql.exec.UDF;\n```\n\n----------------------------------------\n\nTITLE: Integrating Spark with Hive for Join Operations\nDESCRIPTION: Describes the approach for implementing join operations in Hive using Spark, leveraging Spark's shuffle functionality while preserving Hive's existing join implementations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n### Join\n\nIt's rather complicated in implementing join in MapReduce world, as manifested in Hive. Hive has reduce-side join as well as map-side join (including map-side hash lookup and map-side sorted merge). We will keep Hive's join implementations. However, extra attention needs to be paid on the shuffle behavior (key generation, partitioning, sorting, etc), since Hive extensively uses MapReduce's shuffling in implementing reduce-side join. It's expected that Spark is, or will be, able to provide flexible control over the shuffling, as pointed out in the previous section([Shuffle, Group, and Sort](https://docs.google.com/a/cloudera.com/document/d/11xXVgma6UPa32cW_W64BwBssnA0d6RX4jUsATOzgkcw/edit#heading=h.rnmzzu57lmuh)).\n```\n\n----------------------------------------\n\nTITLE: Pseudocode for Common Join Cost Calculation in Hive\nDESCRIPTION: This pseudocode details the cost calculation for a common join operation in Hive. It includes formulas for CPU usage (considering sorting and merging costs) and I/O usage (accounting for intermediate result set handling and data transfer).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/cost-based-optimization-in-hive_42566775.md#2025-04-09_snippet_3\n\nLANGUAGE: pseudocode\nCODE:\n```\nT(R) = Join Cardinality estimation\nTsz = Consult Metadata to get average tuple size based on join schema;\nCPU Usage = Sorting Cost for each of the relation + Merge Cost for sorted stream\n                      = (T(R1) * log T(R1) * CPUc + T(R2) * log T(R2) * CPUc + â€¦ + T(Rm) * log T(Rm) * CPUc) + (T(R1) + T(R2) + â€¦+ T(Rm)) * CPUc nano seconds;\n\nIO Usage = Cost of writing intermediate result set in to local FS for shuffling + Cost of reading from local FS for transferring to Join operator node + Cost of transferring mapped output to Join operator node\n                  = Lw * (T(R1) * Tsz1 + T(R2) * Tsz2 + â€¦+ T(Rm) * Tszm)  + Lr * (T(R1) * Tsz1 + T(R2) * Tsz2 + â€¦+ T(Rm) * Tszm) + NEt *  (T(R1) * Tsz1 + T(R2) * Tsz2 + â€¦ + T(Rm) * Tszm)\n```\n\n----------------------------------------\n\nTITLE: Using parse_url_tuple() UDTF in Hive SQL\nDESCRIPTION: The parse_url_tuple() function takes a URL string and a set of n URL parts, returning a tuple of n values. It can extract multiple parts of a URL at once, with valid parts including HOST, PATH, QUERY, REF, PROTOCOL, and others.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nparse_url_tuple(string urlStr, string p1,...,string pn)\n```\n\n----------------------------------------\n\nTITLE: Current Date and Timestamp in Hive SQL\nDESCRIPTION: Functions to retrieve the current date and timestamp at the start of query evaluation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_35\n\nLANGUAGE: SQL\nCODE:\n```\ncurrent_date\ncurrent_timestamp\n```\n\n----------------------------------------\n\nTITLE: Formatting Dates in Hive\nDESCRIPTION: Converts date/timestamp/string to formatted string using specified pattern. Pattern behavior depends on formatter implementation controlled by hive.datetime.formatter property.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_41\n\nLANGUAGE: sql\nCODE:\n```\ndate_format(date/timestamp/string ts,string pattern)\n```\n\n----------------------------------------\n\nTITLE: Enabling Map-side Aggregation in Hive\nDESCRIPTION: Shows how to enable map-side aggregation for potentially improved efficiency in GROUP BY operations, along with a simple count query.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.map.aggr=true;\nSELECT COUNT(*) FROM table2;\n```\n\n----------------------------------------\n\nTITLE: Setting HCatalog Input Format without Filter\nDESCRIPTION: Example of setting up HCatalog input format without partition filtering.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-inputoutput_34013776.md#2025-04-09_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nHCatInputFormat.setInput(job, InputJobInfo.create(dbName, inputTableName, null));\n```\n\n----------------------------------------\n\nTITLE: Creating a Remote Database Using Connector in Hive SQL\nDESCRIPTION: Example of creating a remote database in Hive that maps to a database in a remote datasource using a connector. This allows Hive to access all tables within the remote database without persisting their metadata locally.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/data-connectors-in-hive_177049669.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE REMOTE DATABASE postgres_db1 USING <connectorName> WITH DBPROPERTIES ('connector.remoteDbName'='db1');\n```\n\n----------------------------------------\n\nTITLE: Correct Left Outer Join with Filtered ON Clause in Hive SQL\nDESCRIPTION: This example shows the correct way to filter a left outer join by including the conditions in the ON clause instead of the WHERE clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.val, b.val FROM a LEFT OUTER JOIN b\nON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07')\n```\n\n----------------------------------------\n\nTITLE: Using the if() conditional function in Hive SQL\nDESCRIPTION: Returns the second argument when the condition is true, otherwise returns the third argument. Provides simple conditional logic in SQL queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_69\n\nLANGUAGE: SQL\nCODE:\n```\nif(boolean testCondition, T valueTrue, T valueFalseOrNull)\n```\n\n----------------------------------------\n\nTITLE: Cache Configuration Properties\nDESCRIPTION: Properties that control HCatalog's HiveClient cache behavior, including expiry time and cache enablement settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-configuration-properties_39622369.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nhcatalog.hive.client.cache.expiry.time=120\nhcatalog.hive.client.cache.disabled=false\n```\n\n----------------------------------------\n\nTITLE: Testing HiveServer Connectivity with Ant\nDESCRIPTION: Commands for testing the HiveServer installation by running the HiveServer and JDBC driver tests in standalone mode, targeting the default port 10000 on localhost.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver_27362111.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ ant test -Dtestcase=TestJdbcDriver -Dstandalone=true\n$ ant test -Dtestcase=TestHiveServer -Dstandalone=true\n```\n\n----------------------------------------\n\nTITLE: CTE Basic Syntax Definition in Hive\nDESCRIPTION: Defines the basic syntax structure for Common Table Expressions in Hive, showing the grammar rules for withClause and cteClause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/common-table-expression_38572242.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwithClause: cteClause (, cteClause)*\ncteClause: cte_name AS (select statment)\n```\n\n----------------------------------------\n\nTITLE: HTTP Request Example using cURL\nDESCRIPTION: cURL command demonstrating how to submit a Pig job to WebHCat with file parameter and verbose argument.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-pig_34017169.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -d file=id.pig \\\n       -d arg=-v \\\n       'http://localhost:50111/templeton/v1/pig?user.name=ekoifman'\n```\n\n----------------------------------------\n\nTITLE: Create Table As Select (CTAS) with Dynamic Partitions\nDESCRIPTION: Example showing table creation with dynamic partitioning using CTAS syntax. The schema including partition columns must be specified in the create clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/dynamicpartitions_27823715.md#2025-04-09_snippet_4\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE T (key int, value string) PARTITIONED BY (ds string, hr int) AS \nSELECT key, value, ds, hr+1 hr1 FROM srcpart WHERE ds is not null and hr>10;\n```\n\n----------------------------------------\n\nTITLE: Altering Materialized View Rewrite Settings\nDESCRIPTION: Syntax for enabling or disabling query rewrite optimization for materialized views.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_54\n\nLANGUAGE: sql\nCODE:\n```\nALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE|DISABLE REWRITE;\n```\n\n----------------------------------------\n\nTITLE: Monthly Character Additions Query in Hive SQL\nDESCRIPTION: Query that calculates the total number of characters added per month from the wiki edit events table. This query can be optimized by using the time-aggregated materialized view.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nSELECT floor(time to month),\n    SUM(characters_added) AS c_added\nFROM wiki\nGROUP BY floor(time to month);\n```\n\n----------------------------------------\n\nTITLE: Using CASE-WHEN-THEN Expression in Hive SQL (Searched Form)\nDESCRIPTION: A conditional expression that evaluates multiple boolean conditions and returns the corresponding result for the first true condition. Similar to if-elseif-else chains in programming languages.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nCASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Auxiliary JARs Path in hive-site.xml\nDESCRIPTION: XML configuration snippet for hive-site.xml that adds the HBase JARs to Hive's auxiliary JAR path, making them available for the bulk load operation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.aux.jars.path</name>\n  <value>/user/hive/hbase-VERSION.jar,/user/hive/hive-hbase-handler-VERSION.jar</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: PARTITION BY with Single Column in HiveQL\nDESCRIPTION: Demonstrates using PARTITION BY with one partitioning column and no ORDER BY or window specification in a SELECT statement.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a, COUNT(b) OVER (PARTITION BY c)\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: Converting Unix Timestamp to String in Hive SQL\nDESCRIPTION: Demonstrates the usage of the from_unixtime function to convert a Unix timestamp to a formatted date string. The function accepts an optional pattern parameter for custom formatting.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nfrom_unixtime(0)\n```\n\n----------------------------------------\n\nTITLE: Updating Columns in Tables/Partitions in Hive\nDESCRIPTION: Command to sync serde stored schema information to the Hive Metastore. This is used for tables whose schema is stored in the serde rather than in HMS, such as Avro tables. Available in Hive 3.0.0 and later.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name [PARTITION (partition_key = 'partition_value' [, ...])] UPDATE COLUMNS;\n```\n\n----------------------------------------\n\nTITLE: Typing TRANSFORM Output in HiveQL\nDESCRIPTION: These snippets demonstrate how to specify data types for the output of a TRANSFORM clause, both with default string typing and explicit type casting.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-transform_27362047.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\n  SELECT TRANSFORM(stuff)\n  USING 'script'\n  AS thing1, thing2\n```\n\nLANGUAGE: SQL\nCODE:\n```\n  SELECT TRANSFORM(stuff)\n  USING 'script'\n  AS (thing1 INT, thing2 INT)\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Collection Functions in HiveQL\nDESCRIPTION: Example of collection function usage in Hive queries, demonstrating array and map operations\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_5\n\nLANGUAGE: HiveQL\nCODE:\n```\nSELECT size(array(1,2,3)),        -- returns 3\n       size(map('a',1,'b',2)),     -- returns 2\n       map_keys(map('a',1,'b',2)),  -- returns ['a','b']\n       array_contains(array(1,2,3), 2), -- returns true\n       sort_array(array(3,2,1))    -- returns [1,2,3]\n```\n\n----------------------------------------\n\nTITLE: Querying Scheduled Executions\nDESCRIPTION: SQL query to retrieve information about scheduled query executions\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/scheduled-queries_145724128.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from information_schema.scheduled_executions;\n```\n\n----------------------------------------\n\nTITLE: Setting up Apache Hive Repository with Multiple Remotes\nDESCRIPTION: This snippet demonstrates how to clone the Apache Hive repository and set up multiple remotes for contribution. It includes cloning the main repository and adding a personal fork as a remote.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocontribute_27362107.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# clone the apache/hive repo from github\ngit clone --origin apache https://github.com/apache/hive\ncd hive\n# add your own fork as a remote\ngit remote add GITHUB_USER git@github.com:GITHUB_USER/hive\n```\n\n----------------------------------------\n\nTITLE: Complex Join Condition Syntax in Hive 2.2.0+\nDESCRIPTION: Syntax definition for complex expressions in join ON clauses introduced in Hive 2.2.0, showing the restricted format for equality expressions that was enforced in earlier versions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\njoin_condition:  \n    ON equality_expression ( AND equality_expression )*\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nequality_expression:  \n    expression = expression\n```\n\n----------------------------------------\n\nTITLE: Initializing GenericUDF in Hive\nDESCRIPTION: This code snippet shows how the Hive engine initializes a GenericUDF by calling its initialize() method, providing ObjectInspectors for the UDF arguments.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/serde_27362059.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nGenericUDF.initialize()\n```\n\n----------------------------------------\n\nTITLE: JSON Response for Hadoop Version in WebHCat API\nDESCRIPTION: This JSON output shows the structure of the response from the GET version/hadoop endpoint. It includes the module name and version of Hadoop being used.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-versionhadoop_44303410.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n{\"module\":\"hadoop\",\"version\":\"2.4.1-SNAPSHOT}\n]\n```\n\n----------------------------------------\n\nTITLE: Multiple Aggregations with Same DISTINCT Column in Hive\nDESCRIPTION: Demonstrates performing multiple aggregations in a single query, including COUNT(DISTINCT) and SUM(DISTINCT) on the same column.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT OVERWRITE TABLE pv_gender_agg\nSELECT pv_users.gender, count(DISTINCT pv_users.userid), count(*), sum(DISTINCT pv_users.userid)\nFROM pv_users\nGROUP BY pv_users.gender;\n```\n\n----------------------------------------\n\nTITLE: Browsing Hive Tables\nDESCRIPTION: Shows how to list tables and describe table structures using HiveQL commands.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nSHOW TABLES;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nSHOW TABLES '.*s';\n```\n\nLANGUAGE: SQL\nCODE:\n```\nDESCRIBE invites;\n```\n\n----------------------------------------\n\nTITLE: Using COALESCE Function in Hive SQL\nDESCRIPTION: Returns the first non-NULL value from a list of expressions, or NULL if all expressions are NULL. This function is useful for providing fallback values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\nCOALESCE(T v1, T v2, ...)\n```\n\n----------------------------------------\n\nTITLE: Executing Star Schema Join Query in Hive SQL\nDESCRIPTION: Example of a star schema join query that joins the store_sales fact table with multiple dimension tables (household_demographics, time_dim, and store) with filtering conditions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joinoptimization_33293167.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSelect count(*) cnt\nFrom store_sales ss\n     join household_demographics hd on (ss.ss_hdemo_sk = hd.hd_demo_sk)\n     join time_dim t on (ss.ss_sold_time_sk = t.t_time_sk)\n     join store s on (s.s_store_sk = ss.ss_store_sk)\nWhere\n     t.t_hour = 8\n     t.t_minute >= 30\n     hd.hd_dep_count = 2\norder by cnt;\n```\n\n----------------------------------------\n\nTITLE: Position-based Array Explosion in Hive\nDESCRIPTION: Examples of using the posexplode function to transform array elements into multiple rows while preserving position information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_51\n\nLANGUAGE: HiveQL\nCODE:\n```\nselect posexplode(array('A','B','C'));\nselect posexplode(array('A','B','C')) as (pos,val);\nselect tf.* from (select 0) t lateral view posexplode(array('A','B','C')) tf;\nselect tf.* from (select 0) t lateral view posexplode(array('A','B','C')) tf as pos,val;\n```\n\n----------------------------------------\n\nTITLE: Using Vertical Format for Query Results in Beeline\nDESCRIPTION: The vertical output format in Beeline displays each row of the query result as a block of key-value pairs. The keys are the column names and the values are the data from that row, allowing for better readability with wide tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_15\n\nLANGUAGE: text\nCODE:\n```\nid       1\nvalue    Value1\ncomment  Test comment 1\n\nid       2\nvalue    Value2\ncomment  Test comment 2\n\nid       3\nvalue    Value3\ncomment  Test comment 3\n```\n\n----------------------------------------\n\nTITLE: Using Dynamic Partitioning with ALTER COLUMN in Hive\nDESCRIPTION: Examples of using dynamic partitioning to alter multiple partitions at once. This feature allows changing column properties across all matching partitions with a single command, improving efficiency for partition management.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_45\n\nLANGUAGE: sql\nCODE:\n```\n// hive.exec.dynamic.partition needs to be set to true to enable dynamic partitioning with ALTER PARTITION\nSET hive.exec.dynamic.partition = true;\n \n// This will alter all existing partitions in the table with ds='2008-04-08' -- be sure you know what you are doing!\nALTER TABLE foo PARTITION (ds='2008-04-08', hr) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);\n\n// This will alter all existing partitions in the table -- be sure you know what you are doing!\nALTER TABLE foo PARTITION (ds, hr) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);\n```\n\n----------------------------------------\n\nTITLE: Ntile Function in Hive SQL\nDESCRIPTION: A function to divide an ordered partition into a specified number of buckets, useful for calculating percentiles and other summary statistics.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nntile(integer x)\n```\n\n----------------------------------------\n\nTITLE: Hive Query Optimization Settings\nDESCRIPTION: Parameters controlling query optimization and execution behavior.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_21\n\nLANGUAGE: properties\nCODE:\n```\nhive.limit.optimize.enable=false\nhive.limit.optimize.fetch.max=50000\nhive.limit.optimize.limit.file=10\nhive.limit.row.max.size=100000\n```\n\n----------------------------------------\n\nTITLE: SHOW FUNCTIONS Command in Hive\nDESCRIPTION: Syntax for the SHOW FUNCTIONS command which lists all built-in and user-defined functions. The optional LIKE clause allows filtering functions by pattern matching, helping to find specific function types or names.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_86\n\nLANGUAGE: sql\nCODE:\n```\nSHOW FUNCTIONS [LIKE \"<pattern>\"];\n```\n\n----------------------------------------\n\nTITLE: Configuring Metastore Server URL in Markdown\nDESCRIPTION: This snippet shows how to configure the Metastore server URL using a markdown table. It includes parameters for both client and server configurations, specifying the format and default values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-metastore-3-0-administration_75978150.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Configured On | Parameter | Hive 2 Parameter | Format | Default Value | Comment |\n| --- | --- | --- | --- | --- | --- |\n| Client | metastore.thrift.uris | hive.metastore.uris | thrift://<HOST>:<PORT>[, thrift://<HOST>:<PORT>...] | none | HOST = hostname, PORT = should be set to match metastore.thrift.port on the server (which defaults to 9083. You can provide multiple servers in a comma separate list. |\n| Server | metastore.thrift.port | hive.metastore.port | integer | 9083 | Port Thrift will listen on. |\n```\n\n----------------------------------------\n\nTITLE: Running Pig with HCatalog Flag\nDESCRIPTION: Command to run Pig with HCatalog integration enabled using the -useHCatalog flag\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npig -useHCatalog\n```\n\n----------------------------------------\n\nTITLE: Defining gRPC Service for Hive Metastore in Protobuf\nDESCRIPTION: Example of defining a gRPC service for Hive Metastore using Protocol Buffers. It shows a simple RPC method for getting a table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/158869886.md#2025-04-09_snippet_0\n\nLANGUAGE: proto\nCODE:\n```\nservice HiveMetaStoreGrpc {\n    rpc getTable(Table) returns (GetTableResponse);\n}\n```\n\n----------------------------------------\n\nTITLE: Spatial Join Query using ST_INTERSECTS in HQL\nDESCRIPTION: Example of using spatial join operation between two tables using the ST_INTERSECTS predicate.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/spatial-queries_34022710.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM a JOIN b on ST_INTERSECTS(a.spatialcolumn, b.spatialcolumn) = TRUE;\n```\n\n----------------------------------------\n\nTITLE: Starting Beeline CLI\nDESCRIPTION: Command to start the Beeline command line interface, which is the new CLI for HiveServer2 introduced in Hive 0.11.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-installation_27362077.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/beeline\n```\n\n----------------------------------------\n\nTITLE: Storing query results directly in S3\nDESCRIPTION: This snippet demonstrates how to store query results directly in S3 by using an S3 path as the output directory. This allows for persistent storage of results outside the Hadoop cluster.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nhive> insert overwrite directory 's3n://target-bucket/tpcresults-1.sql';\n```\n\n----------------------------------------\n\nTITLE: Setting Small Table File Size Threshold for Map Join in Hive SQL\nDESCRIPTION: This SQL command sets the maximum total file size for small tables in map joins. If exceeded, Hive will use a common join instead.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoinoptimization_27362029.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.mapjoin.smalltable.filesize = 30000000;\n```\n\n----------------------------------------\n\nTITLE: Creating Avro Table with Embedded Schema\nDESCRIPTION: SQL command demonstrating how to create an Avro table with the schema directly embedded in the CREATE TABLE statement.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE embedded\n  COMMENT \"just drop the schema right into the HQL\"\n  ROW FORMAT SERDE\n  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\n  STORED AS INPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n  OUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n  TBLPROPERTIES (\n    'avro.schema.literal'='{\n      \"namespace\": \"com.howdy\",\n      \"name\": \"some_schema\",\n      \"type\": \"record\",\n      \"fields\": [ { \"name\":\"string1\",\"type\":\"string\"}]\n    }');\n```\n\n----------------------------------------\n\nTITLE: Dropping Materialized Views\nDESCRIPTION: Syntax for removing a materialized view and its associated data from Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_53\n\nLANGUAGE: sql\nCODE:\n```\nDROP MATERIALIZED VIEW [db_name.]materialized_view_name;\n```\n\n----------------------------------------\n\nTITLE: Hive Regular Expression Examples\nDESCRIPTION: Examples demonstrating the usage of regular expression functions regexp_extract() and regexp_replace() for pattern matching and string manipulation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nSELECT regexp_extract('foothebar', 'foo(.*?)(bar)', 2); -- Returns 'bar'\nSELECT regexp_replace('foobar', 'oo|ar', ''); -- Returns 'fb'\n```\n\n----------------------------------------\n\nTITLE: Hive Regular Expression Examples\nDESCRIPTION: Examples demonstrating the usage of regular expression functions regexp_extract() and regexp_replace() for pattern matching and string manipulation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nSELECT regexp_extract('foothebar', 'foo(.*?)(bar)', 2); -- Returns 'bar'\nSELECT regexp_replace('foobar', 'oo|ar', ''); -- Returns 'fb'\n```\n\n----------------------------------------\n\nTITLE: Insert with Implicit Default Values in Hive SQL\nDESCRIPTION: Shows how DEFAULT values are used when columns are omitted from an INSERT statement. The system will automatically apply the default value for any unspecified columns that have defaults defined.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/75969407.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT INTO <tableName>(co1, col3) values(<val1> , <val2>)\n```\n\n----------------------------------------\n\nTITLE: Querying User Privileges on Specific Table in Hive\nDESCRIPTION: Shows how to query privileges that a specific user (ashutosh) has on a particular table (hivejiratable) using SHOW GRANT command\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\nshow grant user ashutosh on table hivejiratable;\n```\n\n----------------------------------------\n\nTITLE: Setting ORC Encoding Strategy\nDESCRIPTION: Configuration to define the encoding strategy for ORC files. Options are SPEED and COMPRESSION, affecting integer encoding.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_33\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.orc.encoding.strategy=SPEED\n```\n\n----------------------------------------\n\nTITLE: Basic Partition Exchange Example in Hive\nDESCRIPTION: Demonstrates creating two tables partitioned by 'ds' and exchanging a partition between them. Shows the complete workflow including table creation, partition addition, and exchange.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/exchange-partition_30755801.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n--Create two tables, partitioned by ds\nCREATE TABLE T1(a string, b string) PARTITIONED BY (ds string);\nCREATE TABLE T2(a string, b string) PARTITIONED BY (ds string);\nALTER TABLE T1 ADD PARTITION (ds='1');\n\n--Move partition from T1 to T2\nALTER TABLE T2 EXCHANGE PARTITION (ds='1') WITH TABLE T1;\n```\n\n----------------------------------------\n\nTITLE: Single Lateral View Query Example in Hive\nDESCRIPTION: Shows a query with a single lateral view that explodes the col1 array into separate rows. The result includes both the exploded column (myCol1) and the original col2 column from the base table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lateralview_27362040.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT myCol1, col2 FROM baseTable\nLATERAL VIEW explode(col1) myTable1 AS myCol1;\n\n```\n\n----------------------------------------\n\nTITLE: Converting Unix Timestamp to String in Hive SQL\nDESCRIPTION: Converts a Unix timestamp (seconds since epoch) to a formatted date string using the from_unixtime function. The function accepts an optional pattern parameter for custom formatting.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_27\n\nLANGUAGE: SQL\nCODE:\n```\nfrom_unixtime(bigint unixtime[,string pattern])\n```\n\n----------------------------------------\n\nTITLE: Example Hive SQL Query with Map Join Hint\nDESCRIPTION: This SQL query demonstrates the use of a map join hint to specify the small table in a join operation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoinoptimization_27362029.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nselect /*+mapjoin(a)*/ * from src1 x join src2 y on x.key=y.key;\n```\n\n----------------------------------------\n\nTITLE: REGEX Column Specification in Hive\nDESCRIPTION: Demonstrates how to use regular expression patterns to select columns in Hive. The example selects all columns except 'ds' and 'hr' using Java regex syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSELECT `(ds|hr)?+.+` FROM sales\n```\n\n----------------------------------------\n\nTITLE: Using create_union UDF in Hive\nDESCRIPTION: Demonstrates how to create a union type using the create_union UDF which requires specifying the tag (index) to indicate which data type from the union definition is being used.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nSELECT create_union(0, key), create_union(if(key<100, 0, 1), 2.0, value), create_union(1, \"a\", struct(2, \"b\")) FROM src LIMIT 2;\n\n{0:\"238\"}\t{1:\"val_238\"}\t{1:{\"col1\":2,\"col2\":\"b\"}}\n{0:\"86\"}\t{0:2.0}\t{1:{\"col1\":2,\"col2\":\"b\"}}\n```\n\n----------------------------------------\n\nTITLE: Setting Database Context with USE Statement\nDESCRIPTION: Demonstrates how to change the current database context using the USE statement and reset to the default database. This allows running queries against tables in specific databases.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nUSE database_name;\nSELECT query_specifications;\nUSE default;\n```\n\n----------------------------------------\n\nTITLE: Basic CBO Explain Query Example\nDESCRIPTION: Example of EXPLAIN CBO command with a complex query involving CTEs, joins, and aggregations. The query analyzes customer returns data with store information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN CBO\nWITH customer_total_return AS\n(SELECT sr_customer_sk AS ctr_customer_sk,\n  sr_store_sk AS ctr_store_sk,\n  SUM(SR_FEE) AS ctr_total_return\n  FROM store_returns, date_dim\n  WHERE sr_returned_date_sk = d_date_sk\n    AND d_year =2000\n  GROUP BY sr_customer_sk, sr_store_sk)\nSELECT c_customer_id\nFROM customer_total_return ctr1, store, customer\nWHERE ctr1.ctr_total_return > (SELECT AVG(ctr_total_return)*1.2\nFROM customer_total_return ctr2\nWHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk)\n  AND s_store_sk = ctr1.ctr_store_sk\n  AND s_state = 'NM'\n  AND ctr1.ctr_customer_sk = c_customer_sk\nORDER BY c_customer_id\nLIMIT 100\n```\n\n----------------------------------------\n\nTITLE: Enabling Predicate Pushdown in Hive\nDESCRIPTION: Controls whether to enable predicate pushdown (PPD) optimization in Hive queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_9\n\nLANGUAGE: properties\nCODE:\n```\nhive.optimize.ppd=true\n```\n\n----------------------------------------\n\nTITLE: Implementing IStatsAggregator Interface in Java\nDESCRIPTION: Interface definition for aggregating statistics in Hive. This interface defines methods for initializing, aggregating statistics from storage, and terminating the aggregation process.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npackage org.apache.hadoop.hive.ql.stats;\n\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * An interface for any possible implementation for gathering statistics.\n */\n\npublic interface IStatsAggregator {\n\n  /**\n * This method does the necessary initializations according to the implementation requirements.\n   */\n  public boolean init(Configuration hconf);\n\n  /**\n * This method aggregates a given statistic from a disk storage.\n * After aggregation, this method does cleaning by removing all records from the disk storage that have the same given rowID.\n   *\n * rowID : a string identification the statistic to be gathered, possibly the table name + the partition specs.\n   *\n * key : a string noting the key to be gathered. Ex: \"numRows\".\n   *\n * */\n  public String aggregateStats(String rowID, String key);\n\n  /**\n * This method executes the necessary termination procedures, possibly closing all database connections.\n   */\n  public boolean terminate();\n\n}\n```\n\n----------------------------------------\n\nTITLE: Partition-Based Query in Hive\nDESCRIPTION: Shows how to query a partitioned table and leverage partition pruning by specifying partition predicates in the WHERE clause. The example filters page_views by date partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n    SELECT page_views.*\n    FROM page_views\n    WHERE page_views.date >= '2008-03-01' AND page_views.date <= '2008-03-31'\n```\n\n----------------------------------------\n\nTITLE: Downloading MovieLens Dataset with Curl\nDESCRIPTION: Alternative command to download the MovieLens 100k dataset zip file using curl.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ncurl --remote-name http://files.grouplens.org/datasets/movielens/ml-100k.zip\n```\n\n----------------------------------------\n\nTITLE: Dropping Temporary Functions in HiveQL\nDESCRIPTION: Syntax for unregistering a temporary function, with an optional IF EXISTS clause to prevent errors if the function doesn't exist.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_62\n\nLANGUAGE: hql\nCODE:\n```\nDROP TEMPORARY FUNCTION [IF EXISTS] function_name;\n```\n\n----------------------------------------\n\nTITLE: Transaction and Result Handling Options in Beeline\nDESCRIPTION: Command options for controlling transaction isolation levels and result set handling in Beeline.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --isolation=TRANSACTION_SERIALIZABLE\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --nullemptystring=false\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --incremental=true\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --incrementalBufferRows=1000\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --maxHistoryRows=NUMROWS\n```\n\n----------------------------------------\n\nTITLE: Defining Byte Length Sampling Syntax in HiveQL\nDESCRIPTION: This snippet shows the syntax for sampling based on byte length in Hive, allowing users to specify the amount of data to sample using size units.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nblock_sample: TABLESAMPLE (ByteLengthLiteral)\n\nByteLengthLiteral : (Digit)+ ('b' | 'B' | 'k' | 'K' | 'm' | 'M' | 'g' | 'G')\n```\n\n----------------------------------------\n\nTITLE: Setting Big Table Selection Policy for SMB Joins in Hive\nDESCRIPTION: This configuration setting allows customization of the big table selection policy for Sort-Merge-Bucket (SMB) joins in Hive. It determines which table to choose for streaming versus hashing and streaming.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joinoptimization_33293167.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\nset hive.auto.convert.sortmerge.join.bigtable.selection.policy \n    = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;\n```\n\n----------------------------------------\n\nTITLE: Running Hive Client Tests with Maven\nDESCRIPTION: Command to specifically run Hive client tests using Maven build system.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ cd odbc\n$ mvn test -Podbc,hadoop-1 -Dthrift.home=/usr/local -Dboost.home=/usr/local\n```\n\n----------------------------------------\n\nTITLE: Altering Decimal Column Definition in Hive SQL\nDESCRIPTION: Demonstrates how to alter a table to change the precision and scale of a decimal column. This is useful when upgrading pre-Hive 0.13.0 decimal columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE foo CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);\n```\n\n----------------------------------------\n\nTITLE: Inserting Values into an Iceberg Table\nDESCRIPTION: Shows how to insert literal values into an Iceberg table using the standard INSERT INTO VALUES syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO TBL_ICE VALUES (1),(2),(3),(4);\n```\n\n----------------------------------------\n\nTITLE: Case J2: Hive SQL Outer Join with Null Supplying Table Predicate\nDESCRIPTION: Example showing how Hive handles a join predicate on the null supplying table in a left outer join. The execution plan shows the predicate being pushed down to the table scan.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/outerjoinbehavior_35749927.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nexplain \nselect s1.key, s2.key \nfrom src s1 left join src s2 on s2.key > '2';\n```\n\n----------------------------------------\n\nTITLE: Creating a Hive Database with WebHCat using cURL\nDESCRIPTION: This curl command demonstrates how to create a new Hive database named 'newdb' with a comment, custom location, and properties using the WebHCat REST API. It sends a PUT request to the ddl/database endpoint with JSON data containing database configuration details.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-putdb_34016273.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X PUT -HContent-type:application/json \\\n       -d '{ \"comment\":\"Hello there\",\n             \"location\":\"hdfs://localhost:9000/user/hive/my_warehouse\",\n             \"properties\":{\"a\":\"b\"}}' \\\n       'http://localhost:50111/templeton/v1/ddl/database/newdb?user.name=rachel'\n```\n\n----------------------------------------\n\nTITLE: Truncating Table in HiveQL\nDESCRIPTION: Removes all rows from a table or specific partition(s). The TABLE keyword is optional as of Hive 4.0. Supports truncating multiple partitions by specifying a partial partition_spec.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nTRUNCATE [TABLE] table_name [PARTITION partition_spec];\n\npartition_spec:\n  : (partition_column = partition_col_value, partition_column = partition_col_value, ...)\n```\n\n----------------------------------------\n\nTITLE: Truncating Table in HiveQL\nDESCRIPTION: Removes all rows from a table or specific partition(s). The TABLE keyword is optional as of Hive 4.0. Supports truncating multiple partitions by specifying a partial partition_spec.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nTRUNCATE [TABLE] table_name [PARTITION partition_spec];\n\npartition_spec:\n  : (partition_column = partition_col_value, partition_column = partition_col_value, ...)\n```\n\n----------------------------------------\n\nTITLE: LDAP Query Result Format\nDESCRIPTION: Example showing the returned format of group entries from an LDAP query. These are the distinguished names of the groups that matched the query criteria.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_10\n\nLANGUAGE: ldap\nCODE:\n```\nuid=group1,ou=Groups,dc=example,dc=com  \nuid=group2,ou=Groups,dc=example,dc=com\n```\n\n----------------------------------------\n\nTITLE: Counting Rows in Hive Table\nDESCRIPTION: HQL query to count the total number of rows in the 'u_data' table. For older Hive versions (pre-HIVE-287), COUNT(1) may need to be used instead of COUNT(*).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_26\n\nLANGUAGE: hql\nCODE:\n```\nSELECT COUNT(*) FROM u_data;\n```\n\n----------------------------------------\n\nTITLE: Checking Out Hive Release Branch\nDESCRIPTION: Git commands to clone the Hive repository and checkout the release branch for future maintenance.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://git-wip-us.apache.org/repos/asf/hive.git/ <hive_src_dir>\ncd <hive_src_dir>\ngit checkout branch-X.Y\n```\n\n----------------------------------------\n\nTITLE: Using CASE-WHEN-THEN Expression in Hive SQL (Simple Form)\nDESCRIPTION: A conditional expression that compares a value against multiple conditions and returns the corresponding result for the first match. Operates similar to a switch statement in programming languages.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nCASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END\n```\n\n----------------------------------------\n\nTITLE: Sort By Syntax in HiveQL\nDESCRIPTION: Defines the syntax for SORT BY clause which sorts data per reducer. Unlike ORDER BY, this provides ordering within each reducer rather than total ordering.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sortby_27362045.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncolOrder: ( ASC | DESC )\nsortBy: SORT BY colName colOrder? (',' colName colOrder?)*\nquery: SELECT expression (',' expression)* FROM src sortBy\n```\n\n----------------------------------------\n\nTITLE: Implementing a Hive JDBC Client in Java\nDESCRIPTION: A complete Java example demonstrating how to connect to Hive via JDBC, create tables, execute queries, and fetch results. The code shows connection establishment, table creation, data loading, and various query execution patterns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_40\n\nLANGUAGE: java\nCODE:\n```\nimport java.sql.SQLException;\nimport java.sql.Connection;\nimport java.sql.ResultSet;\nimport java.sql.Statement;\nimport java.sql.DriverManager;\n\npublic class HiveJdbcClient {\n  private static String driverName = \"org.apache.hive.jdbc.HiveDriver\";\n\n  /**\n   * @param args\n   * @throws SQLException\n   */\n  public static void main(String[] args) throws SQLException {\n      try {\n      Class.forName(driverName);\n    } catch (ClassNotFoundException e) {\n      // TODO Auto-generated catch block\n      e.printStackTrace();\n      System.exit(1);\n    }\n    //replace \"hive\" here with the name of the user the queries should run as\n    Connection con = DriverManager.getConnection(\"jdbc:hive2://localhost:10000/default\", \"hive\", \"\");\n    Statement stmt = con.createStatement();\n    String tableName = \"testHiveDriverTable\";\n    stmt.execute(\"drop table if exists \" + tableName);\n    stmt.execute(\"create table \" + tableName + \" (key int, value string)\");\n    // show tables\n    String sql = \"show tables '\" + tableName + \"'\";\n    System.out.println(\"Running: \" + sql);\n    ResultSet res = stmt.executeQuery(sql);\n    if (res.next()) {\n      System.out.println(res.getString(1));\n    }\n       // describe table\n    sql = \"describe \" + tableName;\n    System.out.println(\"Running: \" + sql);\n    res = stmt.executeQuery(sql);\n    while (res.next()) {\n      System.out.println(res.getString(1) + \"\\t\" + res.getString(2));\n    }\n\n    // load data into table\n    // NOTE: filepath has to be local to the hive server\n    // NOTE: /tmp/a.txt is a ctrl-A separated file with two fields per line\n    String filepath = \"/tmp/a.txt\";\n    sql = \"load data local inpath '\" + filepath + \"' into table \" + tableName;\n    System.out.println(\"Running: \" + sql);\n    stmt.execute(sql);\n\n    // select * query\n    sql = \"select * from \" + tableName;\n    System.out.println(\"Running: \" + sql);\n    res = stmt.executeQuery(sql);\n    while (res.next()) {\n      System.out.println(String.valueOf(res.getInt(1)) + \"\\t\" + res.getString(2));\n    }\n\n    // regular hive query\n    sql = \"select count(1) from \" + tableName;\n    System.out.println(\"Running: \" + sql);\n    res = stmt.executeQuery(sql);\n    while (res.next()) {\n      System.out.println(res.getString(1));\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sort-Merge-Bucket Join Conversion in Hive\nDESCRIPTION: These configuration settings enable the conversion of Sort-Merge-Bucket (SMB) joins to SMB map joins in Hive. This optimization is useful for sorted and bucketed tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joinoptimization_33293167.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\nset hive.auto.convert.sortmerge.join=true;\nset hive.optimize.bucketmapjoin = true;\nset hive.optimize.bucketmapjoin.sortedmerge = true;\n```\n\n----------------------------------------\n\nTITLE: Setting Active Role in Hive\nDESCRIPTION: Sets the active role(s) for the current user. Can set a specific role, ALL roles, or NONE.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nSET ROLE (role_name|ALL|NONE);\n```\n\n----------------------------------------\n\nTITLE: Copying Derby JAR Files\nDESCRIPTION: Commands for copying required Derby client and tools JAR files to Hive and Hadoop lib directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivederbyservermode_27362068.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncp /opt/hadoop/db-derby-10.4.1.3-bin/lib/derbyclient.jar /opt/hadoop/hive/lib\ncp /opt/hadoop/db-derby-10.4.1.3-bin/lib/derbytools.jar /opt/hadoop/hive/lib\n```\n\n----------------------------------------\n\nTITLE: Creating Table with MultiDelimitSerDe in Hive\nDESCRIPTION: Example of creating a Hive table using MultiDelimitSerDe with custom field, collection, and map key delimiters. The field delimiter is set to '[,]', collection delimiter to ':', and map key delimiter to '@'. Demonstrates usage with string, array, and map data types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/multidelimitserde_46631999.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE test (\n id string,\n hivearray array<binary>,\n hivemap map<string,int>) \nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.MultiDelimitSerDe'                  \nWITH SERDEPROPERTIES (\"field.delim\"=\"[,]\",\"collection.delim\"=\":\",\"mapkey.delim\"=\"@\");\n```\n\n----------------------------------------\n\nTITLE: Creating a Wiki Edit Events Table in Hive SQL\nDESCRIPTION: DDL statement for creating a wiki edit events table that stores timestamps, page information, user data, and character changes. The table is defined as transactional and stored in ORC format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE `wiki` (\n  `time` TIMESTAMP, \n  `page` STRING, \n  `user` STRING, \n  `characters_added` BIGINT,\n  `characters_removed` BIGINT)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n```\n\n----------------------------------------\n\nTITLE: Performing MapJoin in Hive SQL\nDESCRIPTION: This SQL query demonstrates how to use the MAPJOIN hint to perform a join as a map-only job when one of the tables is small. The query joins table 'a' with table 'b' on the 'key' column, using the MAPJOIN hint to specify that table 'b' is small enough to fit in memory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_19\n\nLANGUAGE: HQL\nCODE:\n```\nSELECT /*+ MAPJOIN(b) */ a.key, a.value\nFROM a JOIN b ON a.key = b.key\n```\n\n----------------------------------------\n\nTITLE: Extracting Date Components in Hive SQL\nDESCRIPTION: Demonstrates various functions for extracting specific components (year, month, day, hour, etc.) from date or timestamp strings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nyear(\"1970-01-01 00:00:00\")\n```\n\nLANGUAGE: SQL\nCODE:\n```\nquarter('2015-04-08')\n```\n\nLANGUAGE: SQL\nCODE:\n```\nmonth(\"1970-11-01 00:00:00\")\n```\n\nLANGUAGE: SQL\nCODE:\n```\nday(\"1970-11-01 00:00:00\")\n```\n\nLANGUAGE: SQL\nCODE:\n```\nhour('2009-07-30 12:58:59')\n```\n\n----------------------------------------\n\nTITLE: Parallel Job Execution Configuration\nDESCRIPTION: Enables parallel execution of MapReduce jobs and move tasks that can run independently.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_16\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.parallel=false\n```\n\n----------------------------------------\n\nTITLE: Implementing IStatsPublisher Interface in Java\nDESCRIPTION: Interface definition for publishing statistics in Hive. This interface defines methods for initializing statistics collection, publishing individual statistics, and terminating the statistics collection process.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npackage org.apache.hadoop.hive.ql.stats;\n\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * An interface for any possible implementation for publishing statics.\n */\n\npublic interface IStatsPublisher {\n\n  /**\n * This method does the necessary initializations according to the implementation requirements.\n   */\n  public boolean init(Configuration hconf);\n\n  /**\n * This method publishes a given statistic into a disk storage, possibly HBase or MySQL.\n   *\n * rowID : a string identification the statistics to be published then gathered, possibly the table name + the partition specs.\n   *\n * key : a string noting the key to be published. Ex: \"numRows\".\n   *\n * value : an integer noting the value of the published key.\n * */\n  public boolean publishStat(String rowID, String key, String value);\n\n  /**\n * This method executes the necessary termination procedures, possibly closing all database connections.\n   */\n  public boolean terminate();\n\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Partition Details using WebHCat API with Curl\nDESCRIPTION: This curl command demonstrates how to make a GET request to the WebHCat API endpoint to retrieve details about a specific partition in a Hive table. It includes the necessary URL parameters for the database, table, and partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getpartition_34016592.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s \\\n   'http://localhost:50111/templeton/v1/ddl/database/default/table/mytest/partition/country=%27US%27?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Extracting MovieLens Dataset\nDESCRIPTION: Command to extract the downloaded MovieLens 100k zip file.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nunzip ml-100k.zip\n```\n\n----------------------------------------\n\nTITLE: Getting Current Database in Hive\nDESCRIPTION: Shows how to retrieve the current database name using the current_database() function, available since Hive 0.13.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT current_database()\n```\n\n----------------------------------------\n\nTITLE: Reloading Functions in HiveQL\nDESCRIPTION: Syntax for refreshing the function registry to detect functions that were created in other Hive sessions. Useful for HiveServer2 or other Hive CLI sessions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_65\n\nLANGUAGE: hql\nCODE:\n```\nRELOAD (FUNCTIONS|FUNCTION);\n```\n\n----------------------------------------\n\nTITLE: Monitoring Compactions in Hive SQL\nDESCRIPTION: SQL commands for monitoring and managing compactions in Hive. These include showing current compactions and manually initiating a compaction on a table or partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118453.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\n-- Show current compactions\nSHOW COMPACTIONS;\n\n-- Manually initiate a compaction\nALTER TABLE acid_table COMPACT 'major';\n\n-- Show locks\nSHOW LOCKS;\n```\n\n----------------------------------------\n\nTITLE: Monitoring Compactions in Hive SQL\nDESCRIPTION: SQL commands for monitoring and managing compactions in Hive. These include showing current compactions and manually initiating a compaction on a table or partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118453.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\n-- Show current compactions\nSHOW COMPACTIONS;\n\n-- Manually initiate a compaction\nALTER TABLE acid_table COMPACT 'major';\n\n-- Show locks\nSHOW LOCKS;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Job Information Using Curl with WebHCat API\nDESCRIPTION: Example curl command that shows how to retrieve job status information from WebHCat API by job ID. The command makes a GET request to the jobs endpoint with a specific job ID.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-job_34835065.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/jobs/job_201112212038_0004?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Creating a Feature Branch for Apache Hive Development\nDESCRIPTION: This snippet shows how to create a new feature branch for making changes to Apache Hive. It includes fetching the latest changes, creating a branch, and pushing it to a personal fork.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocontribute_27362107.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# fetch all changes - so you will create your feature branch on top of the current master\ngit fetch --all\n# create a feature branch This branch name can be anything - including the ticket id may help later on identifying the branch.\ngit branch HIVE-9999-something apache/master\ngit checkout HIVE-9999-something\n# push your feature branch to your github fork - and set that branch as upstream to this branch\ngit push GITHUB_USER -u HEAD\n```\n\n----------------------------------------\n\nTITLE: Launching Pentaho Report Designer with Hive JDBC Configuration\nDESCRIPTION: This command launches the Pentaho Report Designer using the modified configuration script that includes Hive JDBC support.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivejdbcinterface_27362100.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ sh reporter-designer.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Properties in Hive\nDESCRIPTION: Essential Spark configuration properties for Hive integration including master URL, event logging, executor memory, and serializer settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/44302539.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nset spark.master=<Spark Master URL>\nset spark.eventLog.enabled=true;\nset spark.eventLog.dir=<Spark event log folder (must exist)>\nset spark.executor.memory=512m;              \nset spark.serializer=org.apache.spark.serializer.KryoSerializer;\n```\n\n----------------------------------------\n\nTITLE: Setting HCatalog Output Format without Partition\nDESCRIPTION: Example of setting up HCatalog output format without partition specification.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-inputoutput_34013776.md#2025-04-09_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nHCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName, outputTableName, null));\n```\n\n----------------------------------------\n\nTITLE: Using Table Format for Query Results in Beeline\nDESCRIPTION: The default format mode in Beeline that displays query results in a tabular format with rows and columns properly aligned. Each row of the result corresponds to a row in the table with values displayed in separate columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n+-----+---------+-----------------+\n| id  |  value  |     comment     |\n+-----+---------+-----------------+\n| 1   | Value1  | Test comment 1  |\n| 2   | Value2  | Test comment 2  |\n| 3   | Value3  | Test comment 3  |\n+-----+---------+-----------------+\n```\n\n----------------------------------------\n\nTITLE: Showing Current Roles in Hive\nDESCRIPTION: Displays the list of current roles for the user. Any user can execute this command to see their roles.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nSHOW CURRENT ROLES;\n```\n\n----------------------------------------\n\nTITLE: Bitwise Shift Operations in Hive SQL\nDESCRIPTION: Bit manipulation functions including left shift, right shift, and unsigned right shift operations for various integer types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_46\n\nLANGUAGE: SQL\nCODE:\n```\nshiftleft(TINYINT|SMALLINT|INT a,int b)\nshiftright(TINYINT|SMALLINT|INT a,int b)\nshiftrightunsigned(TINYINT|SMALLINT|INT a,int b)\n```\n\n----------------------------------------\n\nTITLE: Partition Based Query\nDESCRIPTION: Demonstrates querying with partition filtering based on date ranges and additional conditions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_24\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE xyz_com_page_views\nSELECT page_views.*\nFROM page_views\nWHERE page_views.date >= '2008-03-01' AND page_views.date <= '2008-03-31' AND\n      page_views.referrer_url like '%xyz.com';\n```\n\n----------------------------------------\n\nTITLE: Partition Based Query\nDESCRIPTION: Demonstrates querying with partition filtering based on date ranges and additional conditions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_24\n\nLANGUAGE: hql\nCODE:\n```\nINSERT OVERWRITE TABLE xyz_com_page_views\nSELECT page_views.*\nFROM page_views\nWHERE page_views.date >= '2008-03-01' AND page_views.date <= '2008-03-31' AND\n      page_views.referrer_url like '%xyz.com';\n```\n\n----------------------------------------\n\nTITLE: Updating Hive Version for Next Development Cycle\nDESCRIPTION: Maven command to update the version in all pom.xml files for the next development cycle, adding the SNAPSHOT suffix.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nmvn versions:set -DnewVersion=0.7.1-SNAPSHOT -DgenerateBackupPoms=false\n```\n\n----------------------------------------\n\nTITLE: Sampling Query for Range Partitioning in Hive\nDESCRIPTION: Hive query that samples a table to generate range partitioning keys. This technique selects evenly distributed keys from a small sample to create boundaries for parallel sorting of data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nadd jar lib/hive-contrib-0.7.0.jar;\nset mapred.reduce.tasks=1;\ncreate temporary function row_sequence as \n'org.apache.hadoop.hive.contrib.udf.UDFRowSequence';\nselect transaction_id from\n(select transaction_id\nfrom transactions\ntablesample(bucket 1 out of 10000 on transaction_id) s \norder by transaction_id \nlimit 10000000) x\nwhere (row_sequence() % 910000)=0\norder by transaction_id\nlimit 11;\n```\n\n----------------------------------------\n\nTITLE: Rebuilding a Materialized View in Hive SQL\nDESCRIPTION: SQL statement to manually trigger the rebuild of a materialized view to refresh its contents. Rebuilding is necessary when the source tables' data changes to keep the materialized view up-to-date.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive ODBC Driver in odbc.ini\nDESCRIPTION: Configuration settings for the Hive ODBC driver in the odbc.ini file, including driver path, database, host, port and framing settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_10\n\nLANGUAGE: ini\nCODE:\n```\n[Hive]\nDriver = <path_to_libodbchive.so>\nDescription = Hive Driver v1\nDATABASE = default\nHOST = <Hive_server_address>\nPORT = <Hive_server_port>\nFRAMED = 0\n```\n\n----------------------------------------\n\nTITLE: Describe Partition Without Database\nDESCRIPTION: SQL syntax for describing a partition without database specification. Column information follows a dot.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_97\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE [EXTENDED|FORMATTED] table_name[.column_name] PARTITION partition_spec;\n```\n\n----------------------------------------\n\nTITLE: Defining Hive Grant/Revoke SQL Syntax\nDESCRIPTION: SQL syntax for grant, revoke, and deny privilege statements in Hive. Includes the structure for specifying privilege types, object types, privilege levels, and user targets with administrative options.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/authdev_27362078.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nGRANT\n    priv_type [(column_list)]\n      [, priv_type [(column_list)]] ...\n    ON [object_type] priv_level\n    TO user [, user] ...\nWITH ADMIN OPTION\n\nobject_type:\n    TABLE\n\npriv_level:\n    *\n  | *.*\n  | db_name.*\n  | db_name.tbl_name\n  | tbl_name\n\nREVOKE\n    priv_type [(column_list)]\n      [, priv_type [(column_list)]] ...\n    ON [object_type] priv_level\n    FROM user [, user] ...\n\nREVOKE ALL PRIVILEGES, GRANT OPTION\n    FROM user [, user] ...\n\nDENY  \n\tpriv_type [(column_list)]\n      [, priv_type [(column_list)]] ...\n    ON [object_type] priv_level\n    FROM user [, user] ...\n```\n\n----------------------------------------\n\nTITLE: Dropping Roles in Hive\nDESCRIPTION: Removes an existing role from Hive. Only users with admin role can execute this command.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nDROP ROLE role_name;\n```\n\n----------------------------------------\n\nTITLE: Querying and exploding ngrams results\nDESCRIPTION: Example showing how to use the explode function with ngrams to display the top 10 bigrams from text data, with their estimated frequencies.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statisticsanddatamining_27362058.md#2025-04-09_snippet_3\n\nLANGUAGE: hql\nCODE:\n```\nSELECT explode(ngrams(sentences(lower(val)), 2, 10)) AS x FROM kafka;\n{\"ngram\":[of\",\"the],\"estfrequency\":23.0}\n{\"ngram\":[on\",\"the],\"estfrequency\":20.0}\n{\"ngram\":[in\",\"the],\"estfrequency\":18.0}\n{\"ngram\":[he\",\"was],\"estfrequency\":17.0}\n{\"ngram\":[at\",\"the],\"estfrequency\":17.0}\n{\"ngram\":[that\",\"he],\"estfrequency\":16.0}\n{\"ngram\":[to\",\"the],\"estfrequency\":16.0}\n{\"ngram\":[out\",\"of],\"estfrequency\":16.0}\n{\"ngram\":[he\",\"had],\"estfrequency\":16.0}\n{\"ngram\":[it\",\"was],\"estfrequency\":15.0}\n```\n\n----------------------------------------\n\nTITLE: JDBC Client Setup Script\nDESCRIPTION: Bash script for setting up and running the JDBC client example. Creates test data and configures the classpath with required JAR dependencies.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveclient_27362101.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nHADOOP_HOME=/your/path/to/hadoop\nHIVE_HOME=/your/path/to/hive\n\necho -e '1\\x01foo' > /tmp/a.txt\necho -e '2\\x01bar' >> /tmp/a.txt\n\nHADOOP_CORE={{ls $HADOOP_HOME/hadoop-*-core.jar}}\nCLASSPATH=.:$HADOOP_CORE:$HIVE_HOME/conf\n\nfor i in ${HIVE_HOME}/lib/*.jar ; do\n    CLASSPATH=$CLASSPATH:$i\ndone\n\njava -cp $CLASSPATH HiveJdbcClient\n```\n\n----------------------------------------\n\nTITLE: Trimming Spaces from String in Hive\nDESCRIPTION: Returns the string resulting from trimming spaces from both ends of input string A.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_35\n\nLANGUAGE: sql\nCODE:\n```\ntrim(string A)\n```\n\n----------------------------------------\n\nTITLE: Creating External JDBC Table in Hive\nDESCRIPTION: This snippet demonstrates how to create an external table in Hive that connects to a MySQL database using the JDBC storage handler. It specifies the table structure and connection properties.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_6\n\nLANGUAGE: HiveQL\nCODE:\n```\nCREATE EXTERNAL TABLE voter_jdbc\n(\n  name string,\n  age int,\n  registration string,\n  contribution decimal(10,2)\n)\nSTORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler'\nTBLPROPERTIES (\n    \"hive.sql.database.type\" = \"MYSQL\",\n    \"hive.sql.jdbc.driver\" = \"com.mysql.jdbc.Driver\",\n    \"hive.sql.jdbc.url\" = \"jdbc:mysql://localhost/sample\",\n    \"hive.sql.dbcp.username\" = \"hive\",\n    \"hive.sql.dbcp.password\" = \"hive\",\n    \"hive.sql.table\" = \"VOTER\"\n);\n```\n\n----------------------------------------\n\nTITLE: Enhanced SHOW PARTITIONS with WHERE/ORDER BY/LIMIT in Hive 4.0.0+\nDESCRIPTION: Advanced syntax for SHOW PARTITIONS introduced in Hive 4.0.0 that supports SQL-like WHERE, ORDER BY, and LIMIT clauses to filter, sort, and limit the results. This provides more flexible partition discovery.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_76\n\nLANGUAGE: sql\nCODE:\n```\nSHOW PARTITIONS [db_name.]table_name [PARTITION(partition_spec)] [WHERE where_condition] [ORDER BY col_list] [LIMIT rows];   -- (Note: Hive 4.0.0 and later)\n```\n\n----------------------------------------\n\nTITLE: Listing Tables with Pattern Matching in Hive\nDESCRIPTION: Demonstrates how to list tables matching a specific pattern using Java regular expression syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_9\n\nLANGUAGE: HiveQL\nCODE:\n```\nSHOW TABLES 'page.*';\n```\n\n----------------------------------------\n\nTITLE: Creating Apache Weblog Table with RegexSerDe\nDESCRIPTION: HQL command to create a table for Apache weblog data using RegexSerDe, which parses log entries using a regular expression pattern. The table includes columns for host, identity, user, time, request, status, size, referer, and agent. This is based on HIVE-662 and HIVE-1719.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_29\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE apachelog (\n  host STRING,\n  identity STRING,\n  user STRING,\n  time STRING,\n  request STRING,\n  status STRING,\n  size STRING,\n  referer STRING,\n  agent STRING)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\nWITH SERDEPROPERTIES (\n  \"input.regex\" = \"([^]*) ([^]*) ([^]*) (-|\\\\[^\\\\]*\\\\]) ([^ \\\"]*|\\\"[^\\\"]*\\\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\"]*|\\\".*\\\") ([^ \\\"]*|\\\".*\\\"))?\"\n)\nSTORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Hive Variable Usage Examples\nDESCRIPTION: Multiple examples showing various ways to use Hive variables, including system properties, nested variables, and query usage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-variablesubstitution_30754722.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nset zzz=5;\n--  sets zzz=5\nset zzz;\n\nset system:xxx=5;\nset system:xxx;\n-- sets a system property xxx to 5\n\nset system:yyy=${system:xxx};\nset system:yyy;\n-- sets yyy with value of xxx\n\nset go=${hiveconf:zzz};\nset go;\n-- sets go base on value on zzz\n\nset hive.variable.substitute=false;\nset raw=${hiveconf:zzz};\nset raw;\n-- disable substitution set a value to the literal\n\nset hive.variable.substitute=true;\n\nEXPLAIN SELECT * FROM src where key=${hiveconf:zzz};\nSELECT * FROM src where key=${hiveconf:zzz};\n--use a variable in a query\n\nset a=1;\nset b=a;\nset c=${hiveconf:${hiveconf:b}};\nset c;\n--uses nested variables. \n\nset jar=../lib/derby.jar;\n\nadd file ${hiveconf:jar};\nlist file;\ndelete file ${hiveconf:jar};\nlist file;\n```\n\n----------------------------------------\n\nTITLE: Resource Management Commands in Beeline\nDESCRIPTION: Commands for managing distributed cache resources including files, JARs, and archives. Supports both filepath and Ivy URL formats.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_5\n\nLANGUAGE: hql\nCODE:\n```\nadd FILE[S] <filepath> <filepath>*\nadd JAR[S] <filepath> <filepath>*\nadd ARCHIVE[S] <filepath> <filepath>*\nadd FILE[S] <ivyurl> <ivyurl>*\nadd JAR[S] <ivyurl> <ivyurl>*\nadd ARCHIVE[S] <ivyurl> <ivyurl>*\n```\n\n----------------------------------------\n\nTITLE: Hive LDAP Authentication Custom Query Configuration\nDESCRIPTION: XML configuration example for Hive's customLDAPQuery property that combines group and individual user authentication. This allows all users of group1 plus the specific user4 to authenticate.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\n<property>  \n  <name>hive.server2.authentication.ldap.customLDAPQuery</name>\n  <value><![CDATA[(|(&(objectClass=groupOfNames)(cn=group1))(&(objectClass=person)(sn=user4)))]]>\n  </value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: CSV2 Format with Quoting Enabled in Beeline\nDESCRIPTION: Example of csv2 output format with quoting enabled, which adds double quotes around values containing special characters like commas or quotes. Embedded double quotes are escaped with another double quote character.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_23\n\nLANGUAGE: text\nCODE:\n```\nid,value,comment\n1,\"Value,1\",Value contains comma\n2,\"Value\"\"2\",Value contains double quote\n3,Value'3,Value contains single quote\n```\n\n----------------------------------------\n\nTITLE: Using the map_values() function in Hive SQL\nDESCRIPTION: Returns an unordered array containing all the values from the input map.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_54\n\nLANGUAGE: SQL\nCODE:\n```\nmap_values(Map<K.V>)\n```\n\n----------------------------------------\n\nTITLE: Initializing Hive with HBase Handler - Single Node\nDESCRIPTION: Command to start Hive CLI with HBase handler configuration for a single-node HBase server. Requires specific jar files and HBase master configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$HIVE_SRC/build/dist/bin/hive --auxpath $HIVE_SRC/build/dist/lib/hive-hbase-handler-0.9.0.jar,$HIVE_SRC/build/dist/lib/hbase-0.92.0.jar,$HIVE_SRC/build/dist/lib/zookeeper-3.3.4.jar,$HIVE_SRC/build/dist/lib/guava-r09.jar --hiveconf hbase.master=hbase.yoyodyne.com:60000\n```\n\n----------------------------------------\n\nTITLE: Using HDFS tables for temporary data staging\nDESCRIPTION: This snippet demonstrates creating a temporary table in HDFS and populating it with a join of customer and order data. This approach helps improve performance for repeated access patterns by staging joined data in HDFS.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nhive> create table cust_order (nationkey string, acctbal double, mktsegment string, orderstatus string, totalprice double);\nhive> from customer c left outer join orders o on (c.c_custkey = o.o_custkey)\n  insert overwrite table cust_order\n  select c.c_nationkey, c.c_acctbal, c.c_mktsegment, o.o_orderstatus, o.o_totalprice;\n```\n\n----------------------------------------\n\nTITLE: Creating Roles in Hive\nDESCRIPTION: Creates a new role in Hive. Only users with admin role can execute this command. Role names ALL, DEFAULT and NONE are reserved.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE ROLE role_name;\n```\n\n----------------------------------------\n\nTITLE: Configuring HBase Write-Ahead Log in Hive XML\nDESCRIPTION: Controls whether writes to HBase should be forced to the write-ahead log when using HBase StorageHandler in Hive. Disabling improves write performance but risks data loss in case of a crash.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_104\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.hbase.wal.enabled</name>\n  <value>true</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing a Bitmap Index in HiveQL\nDESCRIPTION: Creates a bitmap index with deferred rebuild, builds the index, shows formatted index information, and then drops the index. Bitmap indexing was added in Hive 0.8.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_2\n\nLANGUAGE: hiveql\nCODE:\n```\nCREATE INDEX table03_index ON TABLE table03 (column4) AS 'BITMAP' WITH DEFERRED REBUILD;\nALTER INDEX table03_index ON table03 REBUILD;\nSHOW FORMATTED INDEX ON table03;\nDROP INDEX table03_index ON table03;\n```\n\n----------------------------------------\n\nTITLE: Setting up Hive Environment Variables\nDESCRIPTION: Commands to set up the HIVE_HOME environment variable and update the system PATH to include Hive binaries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-installation_27362077.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd hive-x.y.z\n$ export HIVE_HOME={{pwd}}\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ export PATH=$HIVE_HOME/bin:$PATH\n```\n\n----------------------------------------\n\nTITLE: Setting Debug Environment Variables\nDESCRIPTION: Bash environment variable configuration for setting up debug port and Java debug options for Hive debugging.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n    > export HIVE_DEBUG_PORT=8000\n    > export HIVE_DEBUG=\"-Xdebug -Xrunjdwp:transport=dt_socket,address=${HIVE_DEBUG_PORT},server=y,suspend=y\"\n```\n\n----------------------------------------\n\nTITLE: Starting Remote Metastore Server using Hive CLI\nDESCRIPTION: Command to start a Thrift metastore server using the Hive CLI. This approach is available in Hive 0.5.0 and later versions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-metastore-administration_27362076.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhive --service metastore\n```\n\n----------------------------------------\n\nTITLE: Configuring Storage-Based Authorization in Hive XML\nDESCRIPTION: XML configuration entries required in hive-site.xml to enable storage-based authorization. Sets the authorization manager to StorageBasedAuthorizationProvider and enables authorization functionality.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-authorization_34014782.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n  <property>\n    <name>hive.security.authorization.enabled</name>\n    <value>true</value>\n    <description>enable or disable the hive client authorization</description>\n  </property>\n\n  <property>\n    <name>hive.security.authorization.manager</name>\n    <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider</value>\n    <description>the hive client authorization manager class name.\n    The user defined authorization class should implement interface\n    org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.\n    </description>\n  </property>\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive ACID Tables in XML\nDESCRIPTION: XML configuration snippet for enabling ACID transactions in Hive. This includes setting the transaction manager and enabling ACID operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118453.md#2025-04-09_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.support.concurrency</name>\n  <value>true</value>\n</property>\n<property>\n  <name>hive.enforce.bucketing</name>\n  <value>true</value>\n</property>\n<property>\n  <name>hive.exec.dynamic.partition.mode</name>\n  <value>nonstrict</value>\n</property>\n<property>\n  <name>hive.txn.manager</name>\n  <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>\n</property>\n<property>\n  <name>hive.compactor.initiator.on</name>\n  <value>true</value>\n</property>\n<property>\n  <name>hive.compactor.worker.threads</name>\n  <value>1</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: JSON Response for Successful Table Deletion\nDESCRIPTION: This JSON output shows the response received after successfully deleting a table using the WebHCat API. It confirms the name of the deleted table and the database it was in.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletetable_34016561.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"table\": \"test_table\",\n \"database\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Select Query in Hive SQL\nDESCRIPTION: Simple SELECT query that retrieves all columns from a Druid table with a limit of 10 rows.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM druid_table_1 LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Converting Date String to Unix Timestamp in Hive SQL\nDESCRIPTION: Converts a date string to Unix timestamp (seconds since epoch) using the unix_timestamp function. It accepts an optional pattern parameter for custom date formats.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_29\n\nLANGUAGE: SQL\nCODE:\n```\nunix_timestamp(string date)\n```\n\nLANGUAGE: SQL\nCODE:\n```\nunix_timestamp(string date,string pattern)\n```\n\n----------------------------------------\n\nTITLE: Stage Plans Output from EXPLAIN in Apache Hive\nDESCRIPTION: Detailed stage plans output from the EXPLAIN command, showing the map-reduce operations, operator trees, and file operations for each stage of query execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nSTAGE PLANS:\n  Stage: Stage-1\n    Map Reduce\n      Alias -> Map Operator Tree:\n        src\n            Reduce Output Operator\n              key expressions:\n                    expr: key\n                    type: string\n              sort order: +\n              Map-reduce partition columns:\n                    expr: rand()\n                    type: double\n              tag: -1\n              value expressions:\n                    expr: substr(value, 4)\n                    type: string\n      Reduce Operator Tree:\n        Group By Operator\n          aggregations:\n                expr: sum(UDFToDouble(VALUE.0))\n          keys:\n                expr: KEY.0\n                type: string\n          mode: partial1\n          File Output Operator\n            compressed: false\n            table:\n                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                output format: org.apache.hadoop.mapred.SequenceFileOutputFormat\n                name: binary_table\n\n  Stage: Stage-2\n    Map Reduce\n      Alias -> Map Operator Tree:\n        /tmp/hive-zshao/67494501/106593589.10001\n          Reduce Output Operator\n            key expressions:\n                  expr: 0\n                  type: string\n            sort order: +\n            Map-reduce partition columns:\n                  expr: 0\n                  type: string\n            tag: -1\n            value expressions:\n                  expr: 1\n                  type: double\n      Reduce Operator Tree:\n        Group By Operator\n          aggregations:\n                expr: sum(VALUE.0)\n          keys:\n                expr: KEY.0\n                type: string\n          mode: final\n          Select Operator\n            expressions:\n                  expr: 0\n                  type: string\n                  expr: 1\n                  type: double\n            Select Operator\n              expressions:\n                    expr: UDFToInteger(0)\n                    type: int\n                    expr: 1\n                    type: double\n              File Output Operator\n                compressed: false\n                table:\n                    input format: org.apache.hadoop.mapred.TextInputFormat\n                    output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\n                    serde: org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe\n                    name: dest_g1\n\n  Stage: Stage-0\n    Move Operator\n      tables:\n            replace: true\n            table:\n                input format: org.apache.hadoop.mapred.TextInputFormat\n                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\n                serde: org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe\n                name: dest_g1\n```\n\n----------------------------------------\n\nTITLE: Extracting Date Fields in Hive SQL\nDESCRIPTION: The extract function allows retrieving specific fields such as days or hours from a date, timestamp, interval, or convertible string source.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_32\n\nLANGUAGE: SQL\nCODE:\n```\nextract(field FROM source)\n```\n\n----------------------------------------\n\nTITLE: Counting Distinct Users by Gender in Hive\nDESCRIPTION: Shows how to count distinct users grouped by gender and insert the results into another table using GROUP BY and COUNT(DISTINCT).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT OVERWRITE TABLE pv_gender_sum\nSELECT pv_users.gender, count (DISTINCT pv_users.userid)\nFROM pv_users\nGROUP BY pv_users.gender;\n```\n\n----------------------------------------\n\nTITLE: Describe Table Command Examples in Hive 2.0+\nDESCRIPTION: Examples demonstrating the new DESCRIBE syntax in Hive 2.0+, showing how to use it with partitioned tables and complex data types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_101\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE FORMATTED default.src_table PARTITION (part_col = 100) columnA;\nDESCRIBE default.src_thrift lintString.$elem$.myint;\n```\n\n----------------------------------------\n\nTITLE: Creating Apache Weblog Table with RegexSerDe in Hive SQL\nDESCRIPTION: This snippet demonstrates how to create a table in Hive using RegexSerDe to parse Apache weblog data. It defines columns for host, identity, user, time, request, status, size, referer, and agent, using a regular expression to match the log format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE apachelog (\nhost STRING,\nidentity STRING,\nuser STRING,\ntime STRING,\nrequest STRING,\nstatus STRING,\nsize STRING,\nreferer STRING,\nagent STRING)\nROW FORMAT SERDE\n'org.apache.hadoop.hive.serde2.RegexSerDe'\nWITH SERDEPROPERTIES (\n\"input.regex\" = \"([^]*) ([^]*) ([^]*) (-|\\\\[^\\\\]*\\\\]) ([^ \\\"]*|\\\"[^\\\"]*\\\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\"]*|\\\".*\\\") ([^ \\\"]*|\\\".*\\\"))?\")\nSTORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: Altering Table Comments in Hive\nDESCRIPTION: SQL statement to modify the comment of an existing table by updating the 'comment' property in TBLPROPERTIES.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_27\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name SET TBLPROPERTIES ('comment' = new_comment);\n```\n\n----------------------------------------\n\nTITLE: Configuring Metastore for Standalone Mode in Markdown\nDESCRIPTION: This markdown table shows the configuration parameters that need to be changed when running the Metastore in standalone mode without Hive. It specifies the parameters and their corresponding values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-metastore-3-0-administration_75978150.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Configuration Parameter | Set to for Standalone Mode |\n| --- | --- |\n| metastore.task.threads.always | org.apache.hadoop.hive.metastore.events.EventCleanerTask,org.apache.hadoop.hive.metastore.MaterializationsCacheCleanerTask |\n| metastore.expression.proxy | org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy |\n```\n\n----------------------------------------\n\nTITLE: Defining View Creation and Deletion Syntax in Hive SQL\nDESCRIPTION: The syntax for creating and dropping views in Hive SQL. The CREATE VIEW statement allows optional column names with comments, and a table-level comment. The view definition is provided as a SELECT statement.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/viewdev_27362067.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ]\n[COMMENT table_comment]\nAS SELECT ...\n\nDROP VIEW view_name\n```\n\n----------------------------------------\n\nTITLE: DataNucleus Connection Pool Configuration\nDESCRIPTION: Configuration for DataNucleus connection pooling using HikariCP, including maximum pool size settings and validation parameters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_38\n\nLANGUAGE: properties\nCODE:\n```\ndatanucleus.connectionPoolingType=HikariCP\ndatanucleus.connectionPool.maxPoolSize=10\ndatanucleus.storeManagerType=rdbms\ndatanucleus.transactionIsolation=read-committed\n```\n\n----------------------------------------\n\nTITLE: Extracting Hive Release Package\nDESCRIPTION: Command to extract the Hive release tarball into a new directory named hive-x.y.z.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ tar -xzvf hive-x.y.z.tar.gz\n```\n\n----------------------------------------\n\nTITLE: JSON Response for Setting Table Property\nDESCRIPTION: This JSON output shows the response from the WebHCat API after successfully setting a property. It confirms the property name, table name, and database name where the property was set.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-putproperty_34017012.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"property\": \"fruit\",\n \"table\": \"test_table\",\n \"database\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Execution Engine in Hive Properties\nDESCRIPTION: Configuration property for setting Hive's execution engine. The 'hive.execution.engine' property supports 'tez' for native Tez DAGs optimized for MRR/MPJ, and 'mr' (default) for traditional MapReduce execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-tez_33296197.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nhive.execution.engine\n```\n\n----------------------------------------\n\nTITLE: PHP Thrift Client for Hive\nDESCRIPTION: PHP client implementation using Thrift to connect to a standalone Hive server. Shows basic connection setup and query execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveclient_27362101.md#2025-04-09_snippet_3\n\nLANGUAGE: php\nCODE:\n```\n<?php\n// set THRIFT_ROOT to php directory of the hive distribution\n$GLOBALS['THRIFT_ROOT'] = '/lib/php/';\n// load the required files for connecting to Hive\nrequire_once $GLOBALS['THRIFT_ROOT'] . 'packages/hive_service/ThriftHive.php';\nrequire_once $GLOBALS['THRIFT_ROOT'] . 'transport/TSocket.php';\nrequire_once $GLOBALS['THRIFT_ROOT'] . 'protocol/TBinaryProtocol.php';\n// Set up the transport/protocol/client\n$transport = new TSocket('localhost', 10000);\n$protocol = new TBinaryProtocol($transport);\n$client = new ThriftHiveClient($protocol);\n$transport->open();\n\n// run queries, metadata calls etc\n$client->execute('SELECT * from src');\nvar_dump($client->fetchAll());\n$transport->close();\n```\n\n----------------------------------------\n\nTITLE: Rebuilding Materialized View in Hive\nDESCRIPTION: Command to rebuild/refresh a materialized view's contents to maintain consistency with source tables\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views-in-hive_283118346.md#2025-04-09_snippet_9\n\nLANGUAGE: HiveQL\nCODE:\n```\nALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD;\n```\n\n----------------------------------------\n\nTITLE: Compaction Query Examples in HQL\nDESCRIPTION: Examples of using the SHOW COMPACTIONS command with various filters and options to view compaction status for specific tables, databases, partitions, and states.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_91\n\nLANGUAGE: hql\nCODE:\n```\nExamples\nSHOW COMPACTIONS.\n â€” show all compactions of all tables and partitions currently being compacted or scheduled  for compaction\n\nSHOW COMPACTIONS DATABASE db1\n â€” show all compactions of all tables from given database which are currently being compacted or scheduled for compaction\n\nSHOW COMPACTIONS SCHEMA db1\n â€” show all compactions of all tables from given database which are currently being compacted or scheduled for compaction\n\nSHOW COMPACTIONS tbl0\n â€” show all compactions from given table which are currently being compacted or scheduled for compaction\n\nSHOW COMPACTIONS compactionid =1\n â€” show all compactions with given compaction ID\n\nSHOW COMPACTIONS db1.tbl0 PARTITION (p=101,day='Monday') POOL 'pool0' TYPE 'minor' STATUS 'ready for clean' ORDER BY cq_table DESC, cq_state LIMIT 42\n â€” show all compactions from specific database/table filtered based on pool name/type.state/status and ordered with given clause\n```\n\n----------------------------------------\n\nTITLE: Checking WebHCat Server Status using cURL\nDESCRIPTION: This snippet demonstrates how to verify the WebHCat installation by sending a GET request to the status endpoint using cURL. It shows the expected response format and status code.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-installwebhcat_34015585.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -i http://localhost:50111/templeton/v1/status\nHTTP/1.1 200 OK\nContent-Type: application/json\nTransfer-Encoding: chunked\nServer: Jetty(7.6.0.v20120127)\n\n{\"status\":\"ok\",\"version\":\"v1\"}\n%\n```\n\n----------------------------------------\n\nTITLE: Using ORC File Dump Utility in Hive CLI\nDESCRIPTION: This snippet shows various command-line options for using the ORC file dump utility in different Hive versions. It includes options for dumping data, printing row indexes, displaying timezone information, and recovering corrupted files.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-orc_31818911.md#2025-04-09_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n// Hive version 0.11 through 0.14:\nhive --orcfiledump <location-of-orc-file>\n\n// Hive version 1.1.0 and later:\nhive --orcfiledump [-d] [--rowindex <col_ids>] <location-of-orc-file>\n\n// Hive version 1.2.0 and later:\nhive --orcfiledump [-d] [-t] [--rowindex <col_ids>] <location-of-orc-file>\n\n// Hive version 1.3.0 and later:\nhive --orcfiledump [-j] [-p] [-d] [-t] [--rowindex <col_ids>] [--recover] [--skip-dump] \n    [--backup-path <new-path>] <location-of-orc-file-or-directory>\n```\n\n----------------------------------------\n\nTITLE: Extracting Date from Timestamp in Hive SQL\nDESCRIPTION: Returns the date part of a timestamp string using the to_date function.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_30\n\nLANGUAGE: SQL\nCODE:\n```\nto_date(string timestamp)\n```\n\n----------------------------------------\n\nTITLE: Disabling Vectorized Execution in Hive SQL\nDESCRIPTION: SQL command to disable vectorized query execution in Hive, reverting to standard row-at-a-time execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/vectorized-query-execution_34838326.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.vectorized.execution.enabled = false;\n```\n\n----------------------------------------\n\nTITLE: Viewing Table Statistics with DESCRIBE EXTENDED\nDESCRIPTION: Command to view stored statistics for a table using DESCRIBE EXTENDED.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE EXTENDED TABLE1;\n```\n\n----------------------------------------\n\nTITLE: Creating a Remote Table with Netflix Iceberg in Hive SQL\nDESCRIPTION: This SQL statement shows how to create a remote table in Hive using Netflix Iceberg as the metadata catalog. It specifies a custom client factory for Iceberg integration and the Iceberg table path.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/80452092.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE REMOTE TABLE tbl_name\nVIA 'xxx.yyy.iceberg.hive.zzz.IcebergTableHiveClientFactory'\nWITH TBLPROPERTIES (\n  'iceberg.table.path' = 'an-atomic-store:/tables/tbl_name'\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Remote Table with Netflix Iceberg in Hive SQL\nDESCRIPTION: This SQL statement shows how to create a remote table in Hive using Netflix Iceberg as the metadata catalog. It specifies a custom client factory for Iceberg integration and the Iceberg table path.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/80452092.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE REMOTE TABLE tbl_name\nVIA 'xxx.yyy.iceberg.hive.zzz.IcebergTableHiveClientFactory'\nWITH TBLPROPERTIES (\n  'iceberg.table.path' = 'an-atomic-store:/tables/tbl_name'\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Metastore Setup with PostgreSQL\nDESCRIPTION: Docker command for running Metastore with external PostgreSQL database and volume mount\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hive-with-docker_282102281.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres \\\n     --env SERVICE_OPTS=\"-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password\" \\\n     --mount source=warehouse,target=/opt/hive/data/warehouse \\\n     --mount type=bind,source=`mvn help:evaluate -Dexpression=settings.localRepository -q -DforceStdout`/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar,target=/opt/hive/lib/postgres.jar \\\n     --name metastore-standalone apache/hive:4.0.0\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating a Hive Table\nDESCRIPTION: This snippet demonstrates how to create a simple table in Hive and insert a row of data. It creates a table named 'hive_table_1' with two columns and inserts a single record.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE hive_table_1 (col1 INT, col2 STRING);\nINSERT INTO hive_table_1 VALUES(1, '#en.wikipedia');\n```\n\n----------------------------------------\n\nTITLE: Authentication and Configuration Options in Beeline\nDESCRIPTION: Command options for specifying authentication types, configuration properties, and variables in Beeline.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nbeeline -w\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline -a\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --property-file /tmp/a\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --hiveconf prop1=value1\n```\n\nLANGUAGE: shell\nCODE:\n```\nbeeline --hivevar var1=value1\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Hive Tables\nDESCRIPTION: Shows how to load data from local and HDFS files into Hive tables using LOAD DATA statements, including partitioned loads.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nLOAD DATA LOCAL INPATH './examples/files/kv1.txt' OVERWRITE INTO TABLE pokes;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nLOAD DATA LOCAL INPATH './examples/files/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');\nLOAD DATA LOCAL INPATH './examples/files/kv3.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-08');\n```\n\nLANGUAGE: SQL\nCODE:\n```\nLOAD DATA INPATH '/user/myname/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');\n```\n\n----------------------------------------\n\nTITLE: Connecting to Beeline CLI\nDESCRIPTION: Command to connect to Hive's Beeline CLI interface inside the Docker container\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hive-with-docker_282102281.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it hiveserver2 beeline -u 'jdbc:hive2://hiveserver2:10000/'\n```\n\n----------------------------------------\n\nTITLE: Sampling with Clustered Tables in HiveQL\nDESCRIPTION: These examples show how to use TABLESAMPLE with clustered tables. The first example selects 2 clusters, while the second selects half of a cluster based on the table's clustering.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nTABLESAMPLE(BUCKET 3 OUT OF 16 ON id)\n```\n\nLANGUAGE: SQL\nCODE:\n```\nTABLESAMPLE(BUCKET 3 OUT OF 64 ON id)\n```\n\n----------------------------------------\n\nTITLE: Using the map_keys() function in Hive SQL\nDESCRIPTION: Returns an unordered array containing all the keys from the input map.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_53\n\nLANGUAGE: SQL\nCODE:\n```\nmap_keys(Map<K.V>)\n```\n\n----------------------------------------\n\nTITLE: Enabling/Disabling Materialized View Rewriting in Hive\nDESCRIPTION: SQL command to enable or disable query rewriting for a specific materialized view.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE|DISABLE REWRITE;\n```\n\n----------------------------------------\n\nTITLE: Using ngrams() function to extract frequent n-grams from Twitter data\nDESCRIPTION: Command that uses the ngrams() function to find the top 100 bigrams (2-word phrases) from Twitter data. The tweet text is first converted to lowercase and split into arrays of words using the sentences() function.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statisticsanddatamining_27362058.md#2025-04-09_snippet_0\n\nLANGUAGE: hql\nCODE:\n```\nSELECT context_ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter;\n```\n\n----------------------------------------\n\nTITLE: Listing Tables in Hive\nDESCRIPTION: Shows how to list all tables in the Hive warehouse.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_8\n\nLANGUAGE: HiveQL\nCODE:\n```\nSHOW TABLES;\n```\n\n----------------------------------------\n\nTITLE: Adding or Replacing Columns in Hive Tables\nDESCRIPTION: Command to add new columns to the end of existing columns or to replace all columns in a table. The ADD option is also supported for Avro-backed tables in Hive 0.14 and later. CASCADE/RESTRICT options are available in Hive 1.1.0 and later.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_43\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name \n  [PARTITION partition_spec]                 -- (Note: Hive 0.14.0 and later)\n  ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)\n  [CASCADE|RESTRICT]                         -- (Note: Hive 1.1.0 and later)\n```\n\n----------------------------------------\n\nTITLE: Partial Partition Specification for Column Alterations in Hive\nDESCRIPTION: Examples of using partial partition specifications to change columns across multiple partitions with a single command. This feature is available in Hive 0.14 and requires setting hive.exec.dynamic.partition to true.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_44\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE foo PARTITION (ds='2008-04-08', hr=11) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);\nALTER TABLE foo PARTITION (ds='2008-04-08', hr=12) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);\n...\n```\n\n----------------------------------------\n\nTITLE: Using explode() UDTF with Different Syntax Forms\nDESCRIPTION: Multiple examples showing different ways to use the explode() UDTF, including direct selection, with column aliases, and with LATERAL VIEW. This demonstrates the flexibility in how UDTFs can be used.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_65\n\nLANGUAGE: SQL\nCODE:\n```\nselect explode(array('A','B','C'));select explode(array('A','B','C')) as col;select tf.* from (select 0) t lateral view explode(array('A','B','C')) tf;select tf.* from (select 0) t lateral view explode(array('A','B','C')) tf as col;\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting MovieLens Data in Bash\nDESCRIPTION: This snippet shows how to download and extract the MovieLens dataset using wget and tar commands in a Bash shell.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/userguide_27362066.md#2025-04-09_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nwget http://www.grouplens.org/system/files/ml-data.tar+0.gz\ntar xvzf ml-data.tar+0.gz\n```\n\n----------------------------------------\n\nTITLE: Example of Insert Query with Bucketing/Sorting in Hive\nDESCRIPTION: Example query demonstrating the scenario where hive.optimize.bucketingsorting is applicable - inserting data from one table to another with the same bucketing/sorting keys.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\ninsert overwrite table T2 select * from T1;\n```\n\n----------------------------------------\n\nTITLE: Configuration Reset Commands in Beeline\nDESCRIPTION: Commands for resetting configuration values to defaults, either all configurations or specific variables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_3\n\nLANGUAGE: hql\nCODE:\n```\nreset;\nreset <key>;\n```\n\n----------------------------------------\n\nTITLE: Creating Schemas and Tables in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to create schemas and tables in PostgreSQL. It creates 'bob' and 'alice' schemas, each with a 'country' table and sample data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SCHEMA bob;\nCREATE TABLE bob.country\n(\n    id   int,\n    name varchar(20)\n);\n\ninsert into bob.country\nvalues (1, 'India');\ninsert into bob.country\nvalues (2, 'Russia');\ninsert into bob.country\nvalues (3, 'USA');\n\nCREATE SCHEMA alice;\nCREATE TABLE alice.country\n(\n    id   int,\n    name varchar(20)\n);\n\ninsert into alice.country\nvalues (4, 'Italy');\ninsert into alice.country\nvalues (5, 'Greece');\ninsert into alice.country\nvalues (6, 'China');\ninsert into alice.country\nvalues (7, 'Japan');\n```\n\n----------------------------------------\n\nTITLE: Map-Only Join Query with MAPJOIN Hint in Hive SQL\nDESCRIPTION: This snippet demonstrates a map-only join using the MAPJOIN hint, which is efficient when one of the tables is small.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT /*+ MAPJOIN(b) */ a.key, a.value\nFROM a JOIN b ON a.key = b.key\n```\n\n----------------------------------------\n\nTITLE: Counting Input Files by Key in Apache Hive SQL\nDESCRIPTION: This SQL query shows how to count the number of input files for each key in the 'src' table, grouped and ordered by key.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-virtualcolumns_27362048.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nselect key, count(`INPUT__FILE__NAME`) from src group by key order by key;\n```\n\n----------------------------------------\n\nTITLE: Histogram and Collection Functions in Hive SQL\nDESCRIPTION: Functions for creating histograms and collecting sets or lists of values from a column in a group.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nhistogram_numeric(col, b)\ncollect_set(col)\ncollect_list(col)\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from an Iceberg Table\nDESCRIPTION: Shows how to delete specific rows from an Iceberg table using a WHERE clause to identify records to be removed.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nDELETE FROM TBL_ICE WHERE ID=5;\n```\n\n----------------------------------------\n\nTITLE: Executing REPL STATUS Command in Hive SQL\nDESCRIPTION: The REPL STATUS command is used to check the current replication status of a database. It returns the last replication state (event ID) for the specified database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivereplicationv2development_66850051.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nREPL STATUS <dbname>;\n```\n\n----------------------------------------\n\nTITLE: Docker Pull Command for Apache Hive\nDESCRIPTION: Command to pull Apache Hive Docker image from DockerHub repository.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/Development/quickStart.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull apache/hive:4.0.0\n```\n\n----------------------------------------\n\nTITLE: Storing results in an external table on S3\nDESCRIPTION: This example shows how to create an external table in S3 and insert query results into it. This approach allows for structuring and schema definition of the result data while storing it persistently in S3.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nhive> create external table t1 (flag string, status string, double ...)\n  location 's3n://jssarma/tpcresults-1.sql';\nhive> insert overwrite table t1 select ...;\n```\n\n----------------------------------------\n\nTITLE: Beeline-Site Configuration for Basic JDBC URLs\nDESCRIPTION: A beeline-site.xml example defining bare JDBC URLs without credentials. When combined with beeline-hs2-connection.xml, credentials from that file will be added to these URLs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_35\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n<property>\n  <name>beeline.hs2.jdbc.url.tcpUrl</name>\n  <value>jdbc:hive2://localhost:10000/default</value>\n</property>\n\n<property>\n  <name>beeline.hs2.jdbc.url.httpUrl</name>\n  <value>jdbc:hive2://localhost:10000/default;transportMode=http;httpPath=cliservice</value>\n</property>\n\n<property>\n  <name>beeline.hs2.jdbc.url.default</name>\n  <value>tcpUrl</value>\n</property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Creating HIVEJDBC Connector in Hive\nDESCRIPTION: Creates a HIVEJDBC connector named 'hiveserver_connector' with the specified JDBC URL and credentials. This connector can be used to connect to a remote HiveServer instance.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/data-connector-for-hive-and-hive-like-engines_288885794.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE CONNECTOR hiveserver_connector TYPE 'hivejdbc' URL 'jdbc:hive2://<maskedhost>:10000'   \n WITH DCPROPERTIES (\"hive.sql.dbcp.username\"=\"hive\", \"hive.sql.dbcp.password\"=\"hive\");\n```\n\n----------------------------------------\n\nTITLE: Registering a Permanent UDF in Hive SQL\nDESCRIPTION: This SQL command registers a permanent UDF in Hive, making it available across sessions. The function can be created in a specific database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveplugins_27362098.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate function my_db.my_lower as 'com.example.hive.udf.Lower';\n```\n\n----------------------------------------\n\nTITLE: Counting Rows with Hive SQL\nDESCRIPTION: Various count functions to calculate the number of rows or distinct values in a dataset. These functions can handle NULL values and distinct counts.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\ncount(*)\ncount(expr)\ncount(DISTINCT expr[, expr...])\n```\n\n----------------------------------------\n\nTITLE: Building Hive for Hadoop 1.x\nDESCRIPTION: Maven command to build Hive specifically for Hadoop 1.x version.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ mvn clean package -Phadoop-1,dist\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Cluster Delegation Token Store in XML\nDESCRIPTION: Specifies the implementation class for the delegation token store in Hive clustering. Set to org.apache.hadoop.hive.thrift.ZooKeeperTokenStore for load-balanced clusters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_98\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.cluster.delegation.token.store.class</name>\n  <value>org.apache.hadoop.hive.thrift.MemoryTokenStore</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Advancing Write ID for Managed Tables in Hive\nDESCRIPTION: Simple code snippet showing how to advance the write ID for a managed table using AcidUtils.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/synchronized-metastore-cache_110692851.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nAcidUtils.advanceWriteId(conf, tbl);\n```\n\n----------------------------------------\n\nTITLE: Describing Table Structure in Hive\nDESCRIPTION: Displays the columns and column types of a specified table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_11\n\nLANGUAGE: HiveQL\nCODE:\n```\nDESCRIBE page_view;\n```\n\n----------------------------------------\n\nTITLE: Using the array_union() function in Hive SQL\nDESCRIPTION: Returns an array containing the union of two arrays without duplicates. Example: array_union(array(1, 2, 2, 4), array(2, 3)) returns [1, 2, 3, 4].\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_65\n\nLANGUAGE: SQL\nCODE:\n```\narray_union(array1, array2)\n```\n\n----------------------------------------\n\nTITLE: Unqualified Column References in Join Conditions (Hive 0.13.0+)\nDESCRIPTION: Example showing unqualified column references in join conditions introduced in Hive 0.13.0, where column names can be used without table prefixes if they are unambiguous.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE a (k1 string, v1 string);\nCREATE TABLE b (k2 string, v2 string);\n\nSELECT k1, v1, k2, v2\n FROM a JOIN b ON k1 = k2;\n```\n\n----------------------------------------\n\nTITLE: Querying MovieLens Data in Hive SQL\nDESCRIPTION: A simple SQL query to count the number of rows in the u_data table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/userguide_27362066.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT COUNT(1) FROM u_data;\n```\n\n----------------------------------------\n\nTITLE: Setting Multi Group By Single Reducer Optimization\nDESCRIPTION: Determines whether to optimize multi group by queries to generate a single M/R job plan.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\nhive.multigroupby.singlereducer=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Default File Format for Managed Tables\nDESCRIPTION: Property to set the default file format for CREATE TABLE statements applied to managed tables only. Options include none, TextFile, SequenceFile, RCfile, ORC, and Parquet.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_25\n\nLANGUAGE: properties\nCODE:\n```\nhive.default.fileformat.managed=none\n```\n\n----------------------------------------\n\nTITLE: Trigonometric Functions in Hive SQL\nDESCRIPTION: Standard trigonometric operations including sine, cosine, tangent and their inverse functions, plus degree/radian conversions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_45\n\nLANGUAGE: SQL\nCODE:\n```\nsin(double a)\nasin(double a)\ncos(double a)\nacos(double a)\ntan(double a)\natan(double a)\ndegrees(double a)\nradians(double a)\n```\n\n----------------------------------------\n\nTITLE: Using the array_intersect() function in Hive SQL\nDESCRIPTION: Returns an array containing the intersection of two arrays, without duplicates. Example: array_intersect(array(1, 2, 3, 4), array(1, 2, 3)) returns [1, 2, 3].\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_64\n\nLANGUAGE: SQL\nCODE:\n```\narray_intersect(array1, array2)\n```\n\n----------------------------------------\n\nTITLE: Using the array_intersect() function in Hive SQL\nDESCRIPTION: Returns an array containing the intersection of two arrays, without duplicates. Example: array_intersect(array(1, 2, 3, 4), array(1, 2, 3)) returns [1, 2, 3].\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_64\n\nLANGUAGE: SQL\nCODE:\n```\narray_intersect(array1, array2)\n```\n\n----------------------------------------\n\nTITLE: Example Hive SQL Query for Automatic Join Conversion\nDESCRIPTION: This SQL query shows a simple join that can be automatically converted to a map join by Hive's query processor.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoinoptimization_27362029.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from src1 x join src2 y on x.key=y.key;\n```\n\n----------------------------------------\n\nTITLE: Hashing Data in Hive SQL\nDESCRIPTION: Shows how to use the hash() function in Hive SQL to generate a hash value for one or more arguments.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_79\n\nLANGUAGE: SQL\nCODE:\n```\nhash(a1[, a2...])\n```\n\n----------------------------------------\n\nTITLE: Example of EXPLAIN with ANALYZE Clause Showing Row Counts\nDESCRIPTION: This snippet demonstrates output from EXPLAIN ANALYZE, which annotates the plan with actual row counts, showing both the estimated row count (500) and the actual row count (13) for a table scan operation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\n[...]\n              TableScan [TS_13] (rows=500/13 width=178)\n                Output:[\"key\",\"value\"]\n[...]\n```\n\n----------------------------------------\n\nTITLE: Rewritten Monthly Query Using Materialized View in Hive SQL\nDESCRIPTION: Rewritten version of the monthly character additions query using the materialized view with minute-level granularity. The query further aggregates the minute-level data to month-level.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nSELECT floor(time to month),\n    SUM(c_added)\nFROM mv3\nGROUP BY floor(time to month);\n```\n\n----------------------------------------\n\nTITLE: Querying Local Clone Table in Hive\nDESCRIPTION: Executes a SELECT COUNT(*) query on the local 'emr_clone' table to verify the number of rows copied from the remote table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/data-connector-for-hive-and-hive-like-engines_288885794.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nselect count(*) from default.emr_clone;\n```\n\n----------------------------------------\n\nTITLE: Executing GET Request for Column Information using cURL\nDESCRIPTION: This curl command sends a GET request to the WebHCat API to retrieve column information for a specific table in the default database. It includes the user.name parameter for authentication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getcolumns_34016970.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table/column?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Default Teradata Table Properties\nDESCRIPTION: Defines the default property values used for Teradata tables in Hive, including timestamp precision, character charset, and row length settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/teradatabinaryserde_89068127.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n'teradata.timestamp.precision'='6',\n'teradata.char.charset'='UNICODE',\n'teradata.row.length'='64KB'\n```\n\n----------------------------------------\n\nTITLE: Left Semi Join Query in Hive SQL\nDESCRIPTION: This example shows how to use a left semi join to implement uncorrelated IN/EXISTS subquery semantics efficiently.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.key, a.val\nFROM a LEFT SEMI JOIN b ON (a.key = b.key)\n```\n\n----------------------------------------\n\nTITLE: Sample Output from EXPLAIN DEPENDENCY Showing Partitions and Tables\nDESCRIPTION: This snippet shows JSON-formatted output from EXPLAIN DEPENDENCY, displaying the input partitions and tables accessed by a query, including partition names and table types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\"input_partitions\":[{\"partitionName\":\"default<at:var at:name=\"srcpart\" />ds=2008-04-08/hr=11\"},{\"partitionName\":\"default<at:var at:name=\"srcpart\" />ds=2008-04-08/hr=12\"},{\"partitionName\":\"default<at:var at:name=\"srcpart\" />ds=2008-04-09/hr=11\"},{\"partitionName\":\"default<at:var at:name=\"srcpart\" />ds=2008-04-09/hr=12\"}],\"input_tables\":[{\"tablename\":\"default@srcpart\",\"tabletype\":\"MANAGED_TABLE\"}]}\n```\n\n----------------------------------------\n\nTITLE: JSON Output from EXPLAIN AUTHORIZATION FORMATTED Command\nDESCRIPTION: This JSON-formatted output from EXPLAIN AUTHORIZATION FORMATTED provides the same authorization information as the standard output but in a structured format for easier parsing and integration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n\"OUTPUTS\":[\"hdfs://localhost:9000/tmp/.../-mr-10000\"],\"INPUTS\":[\"default@srcpart\",\"default@src\",\"default@srcpart@ds=2008-04-08/hr=11\",\"default@srcpart@ds=2008-04-08/hr=12\",\"default@srcpart@ds=2008-04-09/hr=11\",\"default@srcpart@ds=2008-04-09/hr=12\"],\"OPERATION\":\"QUERY\",\"CURRENT_USER\":\"navis\",\"AUTHORIZATION_FAILURES\":[\"Permission denied: Principal [name=navis, type=USER] does not have following privileges for operation QUERY [[SELECT] on Object [type=TABLE_OR_VIEW, name=default.src], [SELECT] on Object [type=TABLE_OR_VIEW, name=default.srcpart]]\"]\n```\n\n----------------------------------------\n\nTITLE: Partial Partition Specification Exchange in Hive\nDESCRIPTION: Shows how to exchange multiple partitions using a partial partition specification. Creates tables with multiple partition columns and exchanges all partitions matching a specific condition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/exchange-partition_30755801.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n--Create two tables with multiple partition columns.\nCREATE TABLE T1 (a string, b string) PARTITIONED BY (ds string, hr string);\nCREATE TABLE T2 (a string, b string) PARTITIONED BY (ds string, hr string);\nALTER TABLE T1 ADD PARTITION (ds = '1', hr = '00');\nALTER TABLE T1 ADD PARTITION (ds = '1', hr = '01');\nALTER TABLE T1 ADD PARTITION (ds = '1', hr = '03');\n\n--Alter the table, moving all the three partitions data where ds='1' from table T1 to table T2 (ds=1) \nALTER TABLE T2 EXCHANGE PARTITION (ds='1') WITH TABLE T1;\n```\n\n----------------------------------------\n\nTITLE: Copying Range Key File to a Specific Location in HDFS\nDESCRIPTION: HDFS command to copy the generated range key file to a specific file name for later reference in the bulk load process.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndfs -cp /tmp/hb_range_keys/* /tmp/hb_range_key_list;\n```\n\n----------------------------------------\n\nTITLE: Abort Transactions Command Example in Hive\nDESCRIPTION: Example showing how to use the ABORT TRANSACTIONS command to clean up multiple transaction IDs. This can be used in conjunction with SHOW TRANSACTIONS to identify candidates for cleanup.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_103\n\nLANGUAGE: sql\nCODE:\n```\nABORT TRANSACTIONS 0000007 0000008 0000010 0000015;\n```\n\n----------------------------------------\n\nTITLE: Describing Extended Partition Properties in Hive\nDESCRIPTION: Displays all properties of a specific partition of a table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_13\n\nLANGUAGE: HiveQL\nCODE:\n```\nDESCRIBE EXTENDED page_view PARTITION (ds='2008-08-08');\n```\n\n----------------------------------------\n\nTITLE: Configuring HiveServer2 Authentication Method\nDESCRIPTION: Sets the client authentication type for HiveServer2. Options include NONE, LDAP, KERBEROS, CUSTOM, PAM, and NOSASL.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_45\n\nLANGUAGE: properties\nCODE:\n```\nhive.server2.authentication=NONE\n```\n\n----------------------------------------\n\nTITLE: Using the CASE expression with conditional WHEN/THEN/ELSE in Hive SQL\nDESCRIPTION: Evaluates multiple boolean conditions sequentially. Returns the result corresponding to the first true condition, or the ELSE value if no conditions are true.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_75\n\nLANGUAGE: SQL\nCODE:\n```\nCASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END\n```\n\n----------------------------------------\n\nTITLE: Listing Job IDs with WebHCat REST API - Basic Query\nDESCRIPTION: Simple curl command to retrieve job IDs for a specific user from the WebHCat REST API\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-jobs_34835057.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/jobs?user.name=daijy'\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n{\"id\":\"job_201304291205_0015\",\"detail\":null}\n]\n```\n\n----------------------------------------\n\nTITLE: Loading HDFS Data into Hive Table\nDESCRIPTION: Shows how to load data from HDFS into a Hive table partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_21\n\nLANGUAGE: hql\nCODE:\n```\nLOAD DATA INPATH '/user/data/pv_2008-06-08_us.txt' INTO TABLE page_view PARTITION(date='2008-06-08', country='US')\n```\n\n----------------------------------------\n\nTITLE: Configuration Example for Compaction Thread Management\nDESCRIPTION: Example showing the default configuration value for metastore compactor history retention setting. This parameter determines how many compaction records in state 'did not initiate' will be retained in compaction history for a given table/partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_70\n\nLANGUAGE: markdown\nCODE:\n```\n```\n    \n\n```\n```\n\n----------------------------------------\n\nTITLE: Using EXPLAIN DEPENDENCY to Examine Input Dependencies in Hive\nDESCRIPTION: This example shows how to use EXPLAIN DEPENDENCY to view information about the input tables and partitions accessed by a Hive query that filters on a partition column and performs aggregation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN DEPENDENCY\n  SELECT key, count(1) FROM srcpart WHERE ds IS NOT NULL GROUP BY key\n```\n\n----------------------------------------\n\nTITLE: Dry Run Schema Upgrade\nDESCRIPTION: Lists required upgrade scripts without performing the actual upgrade\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ schematool -dbType derby -upgradeSchemaFrom 3.1.0 -dryRun Upgrading from the user input version 3.1.0\nMetastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\nMetastore connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\nMetastore connection User:\t APP\nStarting upgrade metastore schema from version 3.1.0 to 4.0.0-beta-2\nUpgrade script upgrade-3.1.0-to-3.2.0.derby.sql\nUpgrade script upgrade-3.2.0-to-4.0.0-alpha-1.derby.sql\nUpgrade script upgrade-4.0.0-alpha-1-to-4.0.0-alpha-2.derby.sql\nUpgrade script upgrade-4.0.0-alpha-2-to-4.0.0-beta-1.derby.sql\nUpgrade script upgrade-4.0.0-beta-1-to-4.0.0-beta-2.derby.sql\n```\n\n----------------------------------------\n\nTITLE: Masking First N Characters in Hive\nDESCRIPTION: Returns a masked version of the input string with the first n characters masked. Uppercase letters become 'X', lowercase letters become 'x', and numbers become 'n'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_42\n\nLANGUAGE: sql\nCODE:\n```\nmask_first_n(string str[, int n])\n```\n\n----------------------------------------\n\nTITLE: Altering a Table to Turn Off List Bucketing\nDESCRIPTION: This ALTER TABLE statement turns off list bucketing for a table while keeping it as a skewed table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE <T> (SCHEMA) NOT STORED AS DIRECTORIES;\n```\n\n----------------------------------------\n\nTITLE: Configuring Delete Modes for Iceberg Tables\nDESCRIPTION: Creates an Iceberg table with Copy-on-Write mode configured for delete, update, and merge operations, instead of the default Merge-on-Read behavior.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE tbl_x (id int) STORED BY ICEBERG TBLPROPERTIES (\n    'write.delete.mode'='copy-on-write',\n    'write.update.mode'='copy-on-write',\n    'write.merge.mode'='copy-on-write'\n);\n```\n\n----------------------------------------\n\nTITLE: Complex PARTITION BY and ORDER BY in HiveQL\nDESCRIPTION: Demonstrates PARTITION BY with two partitioning columns, two ORDER BY columns, and no window specification in a SELECT statement.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a, SUM(b) OVER (PARTITION BY c, d ORDER BY e, f)\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: Complete Top K Configuration Example in Hive\nDESCRIPTION: Example HQL commands showing a complete set of top K configuration settings. These settings enable top K collection with 4 values and no minimum percentage threshold.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_6\n\nLANGUAGE: hql\nCODE:\n```\nset hive.stats.topk.collect=true;\nset hive.stats.topk.num=4;\nset hive.stats.topk.minpercent=0;\nset hive.stats.topk.poolsize=100;\n```\n\n----------------------------------------\n\nTITLE: Using Custom MapReduce Job in Hive SQL\nDESCRIPTION: This set of SQL commands demonstrates how to use a custom Python mapper in a Hive query. It creates a new table, transforms data using the Python script, and then queries the results.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/userguide_27362066.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE u_data_new (\n  userid INT,\n  movieid INT,\n  rating INT,\n  weekday INT)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t';\n\nINSERT OVERWRITE TABLE u_data_new\nSELECT\n  TRANSFORM (userid, movieid, rating, unixtime)\n  USING 'python weekday_mapper.py'\n  AS (userid, movieid, rating, weekday)\nFROM u_data;\n\nSELECT weekday, COUNT(1)\nFROM u_data_new\nGROUP BY weekday;\n```\n\n----------------------------------------\n\nTITLE: Using Custom MapReduce Job in Hive SQL\nDESCRIPTION: This set of SQL commands demonstrates how to use a custom Python mapper in a Hive query. It creates a new table, transforms data using the Python script, and then queries the results.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/userguide_27362066.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE u_data_new (\n  userid INT,\n  movieid INT,\n  rating INT,\n  weekday INT)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t';\n\nINSERT OVERWRITE TABLE u_data_new\nSELECT\n  TRANSFORM (userid, movieid, rating, unixtime)\n  USING 'python weekday_mapper.py'\n  AS (userid, movieid, rating, weekday)\nFROM u_data;\n\nSELECT weekday, COUNT(1)\nFROM u_data_new\nGROUP BY weekday;\n```\n\n----------------------------------------\n\nTITLE: Updating Release Version in POM Files\nDESCRIPTION: Command to update the version property in all pom.xml files for the release, removing the SNAPSHOT suffix.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmvn versions:set -DnewVersion=0.7.0 -DgenerateBackupPoms=false\n```\n\n----------------------------------------\n\nTITLE: PARTITION BY with Window Specification in HiveQL\nDESCRIPTION: Shows various examples of using PARTITION BY with partitioning, ORDER BY, and different window specifications in SELECT statements.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a, SUM(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\nFROM T;\n\nSELECT a, AVG(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)\nFROM T;\n\nSELECT a, AVG(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING)\nFROM T;\n\nSELECT a, AVG(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: Executing and Analyzing Cross-Database Query Results\nDESCRIPTION: This snippet shows the execution of the cross-database query between Druid and Hive. It includes the query, job details, execution progress, and the final result. This demonstrates the practical application of joining data from Druid and Hive tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nhive> SELECT a.channel, b.col1\n    > FROM\n    > (\n    >   SELECT `channel`, max(delta) as m, sum(added)\n    >   FROM druid_table_1\n    >   GROUP BY `channel`, `floor_year`(`__time`)\n    >   ORDER BY m DESC\n    >   LIMIT 1000\n    > ) a\n    > JOIN\n    > (\n    >   SELECT col1, col2\n    >   FROM hive_table_1\n    > ) b\n    > ON a.channel = b.col2;\nQuery ID = user1_20160818202329_e9a8b3e8-18d3-49c7-bfe0-99d38d2402d3\nTotal jobs = 1\nLaunching Job 1 out of 1\n2016-08-18 20:23:30 Running Dag: dag_1471548210492_0001_1\n2016-08-18 20:23:30 Starting to run new task attempt: attempt_1471548210492_0001_1_00_000000_0\nStatus: Running (Executing on YARN cluster with App id application_1471548210492_0001)\n----------------------------------------------------------------------------------------------\n        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED\n----------------------------------------------------------------------------------------------\nMap 1 .......... container     SUCCEEDED      1          1        0        0       0       0\nMap 2 .......... container     SUCCEEDED      1          1        0        0       0       0\n----------------------------------------------------------------------------------------------\nVERTICES: 02/02  [==========================>>] 100%  ELAPSED TIME: 0.15 s\n----------------------------------------------------------------------------------------------\n2016-08-18 20:23:31 Completed running task attempt: attempt_1471548210492_0001_1_00_000000_0\nOK\n#en.wikipedia\t1\nTime taken: 1.835 seconds, Fetched: 2 row(s)\n```\n\n----------------------------------------\n\nTITLE: Querying Partitioned Views for Dependency Information in Apache Hive\nDESCRIPTION: Internal query used by Hive to provide dependency information when adding a partition to a view. This query is compiled and analyzed to capture table/partition inputs for hook invocation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/partitionedviews_27362053.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM view_name\nWHERE view_partition_col1 = 'val1' AND view_partition_col=2 = 'val2' ...\n```\n\n----------------------------------------\n\nTITLE: Creating an Iceberg V2 Table with ORC File Format\nDESCRIPTION: Creates an Iceberg table with both format version 2 and ORC file format specified through table properties and storage format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE V2_ORC_TABLE (ID INT) STORED BY ICEBERG STORED AS ORC TBLPROPERTIES ('format-version'='2');\n```\n\n----------------------------------------\n\nTITLE: Co-Groups Implementation in Hive\nDESCRIPTION: Shows how to implement co-group operations using UNION ALL and CLUSTER BY clauses with custom reducers.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_38\n\nLANGUAGE: sql\nCODE:\n```\nFROM (\n     FROM (\n             FROM action_video av\n             SELECT av.uid AS uid, av.id AS id, av.date AS date\n\n            UNION ALL\n\n             FROM action_comment ac\n             SELECT ac.uid AS uid, ac.id AS id, ac.date AS date\n     ) union_actions\n     SELECT union_actions.uid, union_actions.id, union_actions.date\n     CLUSTER BY union_actions.uid) map\n\nINSERT OVERWRITE TABLE actions_reduced\n    SELECT TRANSFORM(map.uid, map.id, map.date) USING 'reduce_script' AS (uid, id, reduced_val);\n```\n\n----------------------------------------\n\nTITLE: Creating Index with Index Properties in HiveQL\nDESCRIPTION: Creates a compact index on column8 of table07, defining custom index properties using the IDXPROPERTIES clause for additional metadata.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_6\n\nLANGUAGE: hiveql\nCODE:\n```\nCREATE INDEX table07_index ON TABLE table07 (column8) AS 'COMPACT' IDXPROPERTIES (\"prop1\"=\"value1\", \"prop2\"=\"value2\");\n```\n\n----------------------------------------\n\nTITLE: Hive Import as External Table Example\nDESCRIPTION: Example showing how to import a table as an external table in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-importexport_27837968.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nexport table department to 'hdfs_exports_location/department';\nimport external table department from 'hdfs_exports_location/department';\n\n```\n\n----------------------------------------\n\nTITLE: Analyzing All Partitions Statistics\nDESCRIPTION: Example of gathering statistics for all partitions in a table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nANALYZE TABLE Table1 PARTITION(ds, hr) COMPUTE STATISTICS;\n```\n\n----------------------------------------\n\nTITLE: Creating an external table for TPCH lineitem data in S3\nDESCRIPTION: This example shows how to create an external table for the TPCH lineitem dataset stored in S3. It defines the schema and specifies delimiter-separated fields format with pipe character as separator.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nhive> create external table lineitem (\n  l_orderkey int, l_partkey int, l_suppkey int, l_linenumber int, l_quantity double,\n  l_extendedprice double, l_discount double, l_tax double, l_returnflag string, \n  l_linestatus string, l_shipdate string, l_commitdate string, l_receiptdate string,\n  l_shipinstruct string, l_shipmode string, l_comment string) \n  row format delimited fields terminated by '|' \n  location 's3n://data.s3ndemo.hive/tpch/lineitem';\n```\n\n----------------------------------------\n\nTITLE: Creating Temporary Functions in HiveQL\nDESCRIPTION: Syntax for creating a temporary function associated with a Java class. These functions exist for the duration of the session and can reference user-defined functions (UDFs).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_61\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TEMPORARY FUNCTION function_name AS class_name;\n```\n\n----------------------------------------\n\nTITLE: Specifying Column Type for Top K in JDBC Storage\nDESCRIPTION: HQL command to specify the column type for storing top K values when using JDBC implementations like Derby or MySQL. Different database systems may require specific column types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_5\n\nLANGUAGE: hql\nCODE:\n```\nset hive.stats.topk.column.type='LONG VARCHAR';\n```\n\n----------------------------------------\n\nTITLE: Hive JOIN Query\nDESCRIPTION: Demonstrates how to perform a JOIN operation between two tables in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nFROM pokes t1 JOIN invites t2 ON (t1.bar = t2.bar) INSERT OVERWRITE TABLE events SELECT t1.bar, t1.foo, t2.foo;\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum RPC Message Size for Hive-Spark Communication\nDESCRIPTION: Sets the maximum message size in bytes for communication between the Hive client and remote Spark driver. Default is 50 MB.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_61\n\nLANGUAGE: properties\nCODE:\n```\nhive.spark.client.rpc.max.size = 52428800\n```\n\n----------------------------------------\n\nTITLE: Enabling User Impersonation in HiveServer2\nDESCRIPTION: When set to true, HiveServer2 will execute Hive operations as the user making the calls to it, enabling user impersonation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_46\n\nLANGUAGE: properties\nCODE:\n```\nhive.server2.enable.doAs=true\n```\n\n----------------------------------------\n\nTITLE: Adding a JAR to Hive's Classpath\nDESCRIPTION: This Hive command adds a JAR file to the classpath, allowing Hive to use custom UDFs or SerDes contained in the JAR.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveplugins_27362098.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nadd jar my_jar.jar;\n```\n\n----------------------------------------\n\nTITLE: Calculating Sum and Average in Hive SQL\nDESCRIPTION: Functions to calculate the sum and average of values in a column, including options for distinct values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nsum(col), sum(DISTINCT col)\navg(col), avg(DISTINCT col)\n```\n\n----------------------------------------\n\nTITLE: Calculating Sum and Average in Hive SQL\nDESCRIPTION: Functions to calculate the sum and average of values in a column, including options for distinct values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nsum(col), sum(DISTINCT col)\navg(col), avg(DISTINCT col)\n```\n\n----------------------------------------\n\nTITLE: Granting Permissions in Oracle\nDESCRIPTION: This snippet shows how to grant necessary permissions to users in Oracle for selecting and inserting data across different schemas.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT SELECT ANY TABLE TO bob;\nGRANT SELECT ANY TABLE TO alice;\nGRANT INSERT ANY TABLE TO bob;\nGRANT INSERT ANY TABLE TO alice;\n```\n\n----------------------------------------\n\nTITLE: Configuring MapReduce to Use Tez\nDESCRIPTION: Property setting to make MapReduce jobs execute on the Tez framework. This allows executing MapReduce plans against Tez by changing the framework name to 'yarn-tez'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-tez_33296197.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nmapreduce.framework.name = yarn-tez\n```\n\n----------------------------------------\n\nTITLE: Creating an Iceberg Table with ORC File Format\nDESCRIPTION: Creates an Iceberg table that uses ORC as the underlying file format instead of the default Parquet format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE ORC_TABLE (ID INT) STORED BY ICEBERG STORED AS ORC;\n```\n\n----------------------------------------\n\nTITLE: Complex Join Query with Left-Associativity in Hive SQL\nDESCRIPTION: This snippet demonstrates the left-associative nature of joins in Hive, which can lead to unintuitive results in complex queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.val1, a.val2, b.val, c.val\nFROM a\nJOIN b ON (a.key = b.key)\nLEFT OUTER JOIN c ON (a.key = c.key)\n```\n\n----------------------------------------\n\nTITLE: Creating External Table and Loading Data in Hive\nDESCRIPTION: Creates an external table for staging page views and loads data using a combination of table creation, HDFS file loading, and data insertion with filtering.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_19\n\nLANGUAGE: hql\nCODE:\n```\nCREATE EXTERNAL TABLE page_view_stg(viewTime INT, userid BIGINT,\n                    page_url STRING, referrer_url STRING,\n                    ip STRING COMMENT 'IP Address of the User',\n                    country STRING COMMENT 'country of origination')\n    COMMENT 'This is the staging page view table'\n    ROW FORMAT DELIMITED FIELDS TERMINATED BY '44' LINES TERMINATED BY '12'\n    STORED AS TEXTFILE\n    LOCATION '/user/data/staging/page_view';\n\nhadoop dfs -put /tmp/pv_2008-06-08.txt /user/data/staging/page_view\n\nFROM page_view_stg pvs\nINSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='US')\nSELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip\nWHERE pvs.country = 'US';\n```\n\n----------------------------------------\n\nTITLE: Complex Join Query with Multiple Conditions in Hive SQL\nDESCRIPTION: This example shows a join with multiple conditions in the ON clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)\n```\n\n----------------------------------------\n\nTITLE: Complex Join Query with Multiple Conditions in Hive SQL\nDESCRIPTION: This example shows a join with multiple conditions in the ON clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)\n```\n\n----------------------------------------\n\nTITLE: Python Thrift Client for Hive\nDESCRIPTION: Python client implementation using Thrift to connect to a standalone Hive server. Demonstrates table creation, data loading, and query execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveclient_27362101.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python\n\nimport sys\n\nfrom hive import ThriftHive\nfrom hive.ttypes import HiveServerException\nfrom thrift import Thrift\nfrom thrift.transport import TSocket\nfrom thrift.transport import TTransport\nfrom thrift.protocol import TBinaryProtocol\n\ntry:\n    transport = TSocket.TSocket('localhost', 10000)\n    transport = TTransport.TBufferedTransport(transport)\n    protocol = TBinaryProtocol.TBinaryProtocol(transport)\n\n    client = ThriftHive.Client(protocol)\n    transport.open()\n\n    client.execute(\"CREATE TABLE r(a STRING, b INT, c DOUBLE)\")\n    client.execute(\"LOAD TABLE LOCAL INPATH '/path' INTO TABLE r\")\n    client.execute(\"SELECT * FROM r\")\n    while (1):\n      row = client.fetchOne()\n      if (row == None):\n        break\n      print row\n    client.execute(\"SELECT * FROM r\")\n    print client.fetchAll()\n\n    transport.close()\n\nexcept Thrift.TException, tx:\n    print '%s' % (tx.message)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Replication Root Directory in XML\nDESCRIPTION: Specifies the HDFS root directory under which Hive's REPL DUMP command will operate, creating dumps for replication to other warehouses.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_106\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.repl.rootdir</name>\n  <value>/usr/hive/repl/</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring Sort-Merge-Bucket Join Cache Size\nDESCRIPTION: Sets the number of rows with the same key value to be cached in memory per sort-merge-bucket joined table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_12\n\nLANGUAGE: properties\nCODE:\n```\nhive.smbjoin.cache.rows=10000\n```\n\n----------------------------------------\n\nTITLE: Enhanced SHOW COLUMNS with Pattern Matching in Hive 3.0+\nDESCRIPTION: Extended syntax for SHOW COLUMNS introduced in Hive 3.0 that supports pattern matching with the LIKE clause. This allows filtering columns based on name patterns with wildcards like '*' and '|'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_84\n\nLANGUAGE: sql\nCODE:\n```\nSHOW COLUMNS (FROM|IN) table_name [(FROM|IN) db_name]  [ LIKE 'pattern_with_wildcards'];\n```\n\n----------------------------------------\n\nTITLE: Creating an Iceberg V2 Table with Format Version Property\nDESCRIPTION: Creates an Iceberg table with format version 2, which offers additional features compared to the default v1 format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE V2_TABLE (ID INT) STORED BY ICEBERG TBLPROPERTIES ('format-version'='2');\n```\n\n----------------------------------------\n\nTITLE: Creating Hive Table with Binary Columns Using Column-Specific Settings\nDESCRIPTION: Example of creating a Hive table with binary storage type specified for specific columns using the #b suffix in the column mapping. Uses default string storage type for other columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE hbase_table_1 (key int, value string, foobar double)\nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n\"hbase.columns.mapping\" = \":key#b,cf:val,cf:foo#b\"\n);\n\n```\n\n----------------------------------------\n\nTITLE: Dropping Connectors in Hive SQL\nDESCRIPTION: Command for removing existing connectors from Hive. Includes optional IF EXISTS clause to prevent errors if connector doesn't exist.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nDROP CONNECTOR [IF EXISTS] connector_name;\n```\n\n----------------------------------------\n\nTITLE: Basic Hive Variable Setting\nDESCRIPTION: Example of setting a basic Hive configuration variable.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-variablesubstitution_30754722.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nset x=myvalue\n```\n\n----------------------------------------\n\nTITLE: Displaying Filesystem Layout for ACID Table in Hive\nDESCRIPTION: This snippet shows the directory structure for an ACID-enabled Hive table named 't', including base and delta directories. It demonstrates how data files are organized for transactional operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118453.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhive> dfs -ls -R /user/hive/warehouse/t;\ndrwxr-xr-x   - ekoifman staff          0 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022\n-rw-r--r--   1 ekoifman staff        602 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022/bucket_00000\ndrwxr-xr-x   - ekoifman staff          0 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000\n-rw-r--r--   1 ekoifman staff        611 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000/bucket_00000\ndrwxr-xr-x   - ekoifman staff          0 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000\n-rw-r--r--   1 ekoifman staff        610 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000/bucket_00000\n```\n\n----------------------------------------\n\nTITLE: Initializing New Hive Schema with Derby\nDESCRIPTION: Initializes a new Hive metastore schema using Derby database to the current version (4.0.0-beta-2)\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ schematool -dbType derby -initSchema Initializing the schema to: 4.0.0-beta-2\nMetastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\nMetastore connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\nMetastore connection User:\t APP\nStarting metastore schema initialization to 4.0.0-beta-2\nInitialization script hive-schema-4.0.0-beta-2.derby.sql\nInitialization script completed\n```\n\n----------------------------------------\n\nTITLE: General Data Masking in Hive\nDESCRIPTION: Returns a masked version of the input string with customizable mask characters for uppercase letters, lowercase letters, and numbers. By default, uppercase letters become 'X', lowercase letters become 'x', and numbers become 'n'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_41\n\nLANGUAGE: sql\nCODE:\n```\nmask(string str[, string upper[, string lower[, string number]]])\n```\n\n----------------------------------------\n\nTITLE: Transparent HLL Distinct Count with BI Mode in Hive\nDESCRIPTION: Illustrates how to use HyperLogLog (HLL) sketch to compute distinct values transparently through BI mode in Hive, which automatically rewrites COUNT(DISTINCT) to use sketches.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/datasketches-integration_177050456.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.optimize.bi.enabled=true;\nselect category,count(distinct id) from sketch_input group by category;\nselect count(distinct id) from sketch_input;\n```\n\n----------------------------------------\n\nTITLE: Configuring Map-Side Aggregation Hash Table Memory Usage\nDESCRIPTION: Sets the maximum memory to be used by map-side group aggregation hash table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\nhive.map.aggr.hash.force.flush.memory.threshold=0.9\n```\n\n----------------------------------------\n\nTITLE: Describe Table With Database Specification\nDESCRIPTION: SQL syntax for describing a table/view/materialized view with database specification. Column information is provided after a space.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_95\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE [EXTENDED|FORMATTED]  \n  [db_name.]table_name[ col_name ( [.field_name] | [.'$elem$'] | [.'$key$'] | [.'$value$'] )* ];\n```\n\n----------------------------------------\n\nTITLE: Creating Skewed Table with Single Column in HiveQL\nDESCRIPTION: Creates a table 'list_bucket_single' with a skewed column 'key' and specified skew values. Optionally stores as directories for list bucketing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE list_bucket_single (key STRING, value STRING)\n  SKEWED BY (key) ON (1,5,6) [STORED AS DIRECTORIES];\n```\n\n----------------------------------------\n\nTITLE: Creating Skewed Table with Single Column in HiveQL\nDESCRIPTION: Creates a table 'list_bucket_single' with a skewed column 'key' and specified skew values. Optionally stores as directories for list bucketing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE list_bucket_single (key STRING, value STRING)\n  SKEWED BY (key) ON (1,5,6) [STORED AS DIRECTORIES];\n```\n\n----------------------------------------\n\nTITLE: Checking Current Roles in Hive Authorization\nDESCRIPTION: Command to display the current active roles for the authenticated user in Hive's SQL Standard Based Authorization model. This allows users to verify which privilege roles are currently active for their session.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nshow current roles;\n```\n\n----------------------------------------\n\nTITLE: Configuring LLAP Security ACLs\nDESCRIPTION: Configuration properties for setting access control lists (ACLs) for LLAP daemon and management service. Default value of '*' allows all access.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_69\n\nLANGUAGE: properties\nCODE:\n```\nhive.llap.daemon.acl=*\nhive.llap.management.acl=*\n```\n\n----------------------------------------\n\nTITLE: Creating an Iceberg Table Using CTLT (Create Table Like Table)\nDESCRIPTION: Creates an empty Iceberg table with the same schema as an existing source table using the CTLT (Create Table Like Table) syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE_CTLT LIKE SRCTABLE STORED BY ICEBERG;\n```\n\n----------------------------------------\n\nTITLE: Using Databases in Hive SQL\nDESCRIPTION: Commands for switching between databases in Hive. Allows setting the current database context for subsequent HiveQL statements and reverting to the default database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nUSE database_name;\nUSE DEFAULT;\n```\n\n----------------------------------------\n\nTITLE: Creating Managed Druid Table (Pre-3.0.0)\nDESCRIPTION: Creates a managed (non-external) Druid table for Hive versions before 3.0.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE druid_table_1\n(`__time` TIMESTAMP, `dimension1` STRING, `dimension2` STRING, `metric1` INT, `metric2` FLOAT)\nSTORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler';\n```\n\n----------------------------------------\n\nTITLE: Basic SHOW COLUMNS Command in Hive 0.10+\nDESCRIPTION: Syntax for the SHOW COLUMNS command introduced in Hive 0.10. This command lists all columns in a table, including partition columns, and can be qualified with a database name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_83\n\nLANGUAGE: sql\nCODE:\n```\nSHOW COLUMNS (FROM|IN) table_name [(FROM|IN) db_name];\n```\n\n----------------------------------------\n\nTITLE: Basic SHOW COLUMNS Command in Hive 0.10+\nDESCRIPTION: Syntax for the SHOW COLUMNS command introduced in Hive 0.10. This command lists all columns in a table, including partition columns, and can be qualified with a database name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_83\n\nLANGUAGE: sql\nCODE:\n```\nSHOW COLUMNS (FROM|IN) table_name [(FROM|IN) db_name];\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Scratch Directory in Java\nDESCRIPTION: Configures the HDFS scratch space for Hive jobs. This directory is used for storing query plans and intermediate outputs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nhiveConf.setVar(HiveConf.ConfVars.SCRATCHDIR, \"/tmp/hive-${user.name}\");\n```\n\n----------------------------------------\n\nTITLE: Using the sort_array() function in Hive SQL\nDESCRIPTION: Sorts an array in ascending order according to the natural ordering of the array elements and returns the sorted array.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_56\n\nLANGUAGE: SQL\nCODE:\n```\nsort_array(Array<T>)\n```\n\n----------------------------------------\n\nTITLE: Querying XML with XPath in Hive SQL\nDESCRIPTION: Demonstrates using the xpath UDF to extract values from an XML string using an XPath expression. This example shows how to get a list of attribute values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-xpathudf_27362051.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nselect xpath('<a><b id=\"1\"><c/></b><b id=\"2\"><c/></b></a>','/descendant::c/ancestor::b/@id') from t1 limit 1 ;\n```\n\n----------------------------------------\n\nTITLE: Setting Group By Skew Data Optimization\nDESCRIPTION: Determines whether to optimize group by queries when there is skew in the data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\nhive.groupby.skewindata=false\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Iceberg Table in Hive\nDESCRIPTION: Creates a simple Iceberg table named 'TBL_ICE' with a single integer column using the STORED BY ICEBERG syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE TBL_ICE (ID INT) STORED BY ICEBERG;\n```\n\n----------------------------------------\n\nTITLE: Querying Remote Table with Predicate in Hive\nDESCRIPTION: Demonstrates how to query the remote 'test_emr_tbl' table with a WHERE clause, filtering for rows where 'tblkey' is greater than 1.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/data-connector-for-hive-and-hive-like-engines_288885794.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from test_emr_tbl where tblkey > 1;\n```\n\n----------------------------------------\n\nTITLE: Enabling Correlation Optimizer in Hive\nDESCRIPTION: Configuration setting to enable the Correlation Optimizer feature in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/correlation-optimizer_34019487.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nset hive.optimize.correlation=true;\n```\n\n----------------------------------------\n\nTITLE: Example View Creation in Hive\nDESCRIPTION: Practical example of creating a view that selects distinct referrer URLs from a page_view table for The Onion website.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_47\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW onion_referrers(url COMMENT 'URL of Referring page')\n  COMMENT 'Referrers to The Onion website'\n  AS\n  SELECT DISTINCT referrer_url\n  FROM page_view\n  WHERE page_url='http://www.theonion.com';\n```\n\n----------------------------------------\n\nTITLE: Map Operations in Hive\nDESCRIPTION: Shows how to work with map type columns including property access and size calculation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\nINSERT OVERWRITE page_views_map\nSELECT pv.userid, pv.properties['page type']\nFROM page_views pv;\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT size(pv.properties)\nFROM page_view pv;\n```\n\n----------------------------------------\n\nTITLE: Altering Table Compaction Properties in Hive\nDESCRIPTION: Examples demonstrating how to modify compaction properties for existing tables using ALTER TABLE statements with minor and major compaction options.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-transactions_40509723.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name COMPACT 'minor' \n   WITH OVERWRITE TBLPROPERTIES (\"compactor.mapreduce.map.memory.mb\"=\"3072\");  -- specify compaction map job properties\nALTER TABLE table_name COMPACT 'major'\n   WITH OVERWRITE TBLPROPERTIES (\"tblprops.orc.compress.size\"=\"8192\");         -- change any other Hive table properties\n```\n\n----------------------------------------\n\nTITLE: Retrieving Hive Schema Information\nDESCRIPTION: Gets current schema version information from the Derby metastore database\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ schematool -dbType derby -info \nMetastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\nMetastore connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\nMetastore connection User:\t APP\nHive distribution version:\t 4.0.0-beta-2\nMetastore schema version:\t 4.0.0-beta-2\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query with Monthly Granularity\nDESCRIPTION: Query showing how to use floor_month function for time-based grouping with aggregations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nSELECT `floor_month`(`__time`), max(delta), sum(added)\nFROM druid_table_1\nGROUP BY `floor_month`(`__time`);\n```\n\n----------------------------------------\n\nTITLE: Creating a Partitioned Table and Dependent Table in Hive\nDESCRIPTION: This snippet demonstrates how to create a partitioned table 'T' and a dependent table 'Tdep' that inherits the schema of 'T' and depends on it. The dependent table is partitioned by a prefix of the original table's partitioning columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/dependent-tables_30151205.md#2025-04-09_snippet_0\n\nLANGUAGE: HiveQL\nCODE:\n```\ncreate table T (key string, value string) partitioned by (ds string, hr string);\n\n-- create a dependent table which specifies the dependencies explicitly\n-- Tdep inherits the schema of T\n-- Tdep is partitioned by a prefix of T (either ds or ds,hr)\ncreate dependent table Tdep partitioned by (ds string) depends on table T;\n```\n\n----------------------------------------\n\nTITLE: Running Apache Hive Unit Tests\nDESCRIPTION: Maven command to execute unit tests for Apache Hive. This builds the project and runs all unit tests with the iceberg profile enabled.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/building-hive-from-source_282102252.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -Piceberg\n```\n\n----------------------------------------\n\nTITLE: Querying LLAP Status via HTTP in JSON\nDESCRIPTION: Example of retrieving LLAP status information through the /status web service endpoint. Returns JSON data showing the LLAP status, uptime, and build information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/llap_62689557.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl localhost:15002/status\n\n{\n  \"status\" : \"STARTED\",\n  \"uptime\" : 139093,\n  \"build\" : \"2.1.0-SNAPSHOT from 77474581df4016e3899a986e079513087a945674 by gopal source checksum a9caa5faad5906d5139c33619f1368bb\"\n}\n```\n\n----------------------------------------\n\nTITLE: Altering View Properties\nDESCRIPTION: Syntax for modifying table properties of an existing view.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_50\n\nLANGUAGE: sql\nCODE:\n```\nALTER VIEW [db_name.]view_name SET TBLPROPERTIES table_properties;\n```\n\n----------------------------------------\n\nTITLE: Example of SHOW TABLE EXTENDED Output in Hive\nDESCRIPTION: An example showing the output of the SHOW TABLE EXTENDED command for a partitioned table. The output includes table metadata, storage information, column structure, and file system statistics like total file count and sizes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_79\n\nLANGUAGE: sql\nCODE:\n```\nhive> show table extended like part_table;\nOK\ntableName:part_table\nowner:thejas\nlocation:file:/tmp/warehouse/part_table\ninputformat:org.apache.hadoop.mapred.TextInputFormat\noutputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\ncolumns:struct columns { i32 i}\npartitioned:true\npartitionColumns:struct partition_columns { string d}\ntotalNumberFiles:1\ntotalFileSize:2\nmaxFileSize:2\nminFileSize:2\nlastAccessTime:0\nlastUpdateTime:1459382233000\n```\n\n----------------------------------------\n\nTITLE: Example of SHOW TABLE EXTENDED Output in Hive\nDESCRIPTION: An example showing the output of the SHOW TABLE EXTENDED command for a partitioned table. The output includes table metadata, storage information, column structure, and file system statistics like total file count and sizes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_79\n\nLANGUAGE: sql\nCODE:\n```\nhive> show table extended like part_table;\nOK\ntableName:part_table\nowner:thejas\nlocation:file:/tmp/warehouse/part_table\ninputformat:org.apache.hadoop.mapred.TextInputFormat\noutputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\ncolumns:struct columns { i32 i}\npartitioned:true\npartitionColumns:struct partition_columns { string d}\ntotalNumberFiles:1\ntotalFileSize:2\nmaxFileSize:2\nminFileSize:2\nlastAccessTime:0\nlastUpdateTime:1459382233000\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Service Metrics Component in XML\nDESCRIPTION: Sets the component name for Hive service metrics in the Hadoop2 metrics system. Used when hive.service.metrics.class is set to org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics and hive.service.metrics.reporter is set to HADOOP2.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_96\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.service.metrics.hadoop2.component</name>\n  <value>hive</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Hive Sentence Tokenization Example\nDESCRIPTION: Example showing how to use the sentences() function to tokenize natural language text into arrays of words grouped by sentences.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nSELECT sentences('Hello there! How are you?');\n-- Returns ((\"Hello\", \"there\"), (\"How\", \"are\", \"you\"))\n```\n\n----------------------------------------\n\nTITLE: Creating Parquet Table in Hive 0.13 and Later\nDESCRIPTION: HiveQL syntax for creating a Parquet table in Hive version 0.13 and later. This snippet shows the simplified syntax using STORED AS PARQUET clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/parquet_38570914.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE parquet_test (\n id int,\n str string,\n mp MAP<STRING,STRING>,\n lst ARRAY<STRING>,\n strct STRUCT<A:STRING,B:STRING>) \nPARTITIONED BY (part string)\nSTORED AS PARQUET;\n```\n\n----------------------------------------\n\nTITLE: LEAD Function in HiveQL\nDESCRIPTION: Shows the usage of the LEAD function with default 1 row lead and no specified default value.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a, LEAD(a) OVER (PARTITION BY b ORDER BY C)\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: User Actions UNION Example\nDESCRIPTION: Practical example showing how to combine video and comment actions from different tables using UNION ALL, then joining with a users table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-union_27362049.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n    SELECT u.id, actions.date\n    FROM (\n        SELECT av.uid AS uid \n        FROM action_video av \n        WHERE av.date = '2008-06-03' \n        UNION ALL \n        SELECT ac.uid AS uid \n        FROM action_comment ac \n        WHERE ac.date = '2008-06-03' \n     ) actions JOIN users u ON (u.id = actions.uid) \n```\n\n----------------------------------------\n\nTITLE: User Actions UNION Example\nDESCRIPTION: Practical example showing how to combine video and comment actions from different tables using UNION ALL, then joining with a users table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-union_27362049.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n    SELECT u.id, actions.date\n    FROM (\n        SELECT av.uid AS uid \n        FROM action_video av \n        WHERE av.date = '2008-06-03' \n        UNION ALL \n        SELECT ac.uid AS uid \n        FROM action_comment ac \n        WHERE ac.date = '2008-06-03' \n     ) actions JOIN users u ON (u.id = actions.uid) \n```\n\n----------------------------------------\n\nTITLE: Altering a Table to Set Skewed Location\nDESCRIPTION: This ALTER TABLE statement changes the list bucketing location map for a table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE <T> (SCHEMA) SET SKEWED LOCATION (key1=\"loc1\", key2=\"loc2\");\n```\n\n----------------------------------------\n\nTITLE: Configuring Metastore Client Cache in Apache Hive XML Configuration\nDESCRIPTION: This configuration enables a Caffeine Cache for the Metastore client, improving performance by caching metadata locally. This parameter is defined in the MetastoreConf configuration file.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-configurations_283118321.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\nhive.metastore.client.cache.v2.enabled\n```\n\n----------------------------------------\n\nTITLE: HLL Distinct Count without Intermediate Table in Hive\nDESCRIPTION: Shows how to use HyperLogLog (HLL) sketch to compute distinct values directly from the input table without using an intermediate table in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/datasketches-integration_177050456.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nselect category, ds_hll_estimate(ds_hll_sketch(id)) from sketch_input group by category;\nselect ds_hll_estimate(ds_hll_sketch(id)) from sketch_input;\n```\n\n----------------------------------------\n\nTITLE: Querying Data from HBase-backed Hive Table\nDESCRIPTION: Example of querying data from a Hive table that is stored in HBase. Shows the results of a SELECT statement on the previously created table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nhive> select * from hbase_table_1;\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\n...\nOK\n100\tval_100\t101\t102\n98\tval_98\t99\t100\nTime taken: 4.054 seconds\n\n```\n\n----------------------------------------\n\nTITLE: Creating Parquet Table in Hive 0.10-0.12\nDESCRIPTION: HiveQL syntax for creating a Parquet table in Hive versions 0.10 to 0.12. This snippet demonstrates how to specify the Parquet storage format, SerDe, InputFormat, and OutputFormat.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/parquet_38570914.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE parquet_test (\n id int,\n str string,\n mp MAP<STRING,STRING>,\n lst ARRAY<STRING>,\n strct STRUCT<A:STRING,B:STRING>) \nPARTITIONED BY (part string)\nROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'\n STORED AS\n INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'\n OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat';\n```\n\n----------------------------------------\n\nTITLE: Mapping Hive MAP Type to HBase Column Family\nDESCRIPTION: Example of using Hive MAP datatype to access an entire HBase column family. This allows each row to have a different set of columns with map keys as column names and map values as column values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE hbase_table_1(value map<string,int>, row_key int) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n\"hbase.columns.mapping\" = \"cf:,:key\"\n);\nINSERT OVERWRITE TABLE hbase_table_1 SELECT map(bar, foo), foo FROM pokes \nWHERE foo=98 OR foo=100;\n\n```\n\n----------------------------------------\n\nTITLE: Renaming HCatalog Table Using cURL in Bash\nDESCRIPTION: This curl command demonstrates how to rename an HCatalog table named 'test_table' to 'test_table_2' in the 'default' database using the WebHCat API. It includes the required 'user.name' parameter in the query string.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-posttable_34016548.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -d rename=test_table_2 \\\n       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ekoifman'\n```\n\n----------------------------------------\n\nTITLE: Fast Statistics Collection with NOSCAN\nDESCRIPTION: Example of gathering basic file statistics without scanning file contents using NOSCAN option.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr) COMPUTE STATISTICS NOSCAN;\n```\n\n----------------------------------------\n\nTITLE: Analyzing Non-Partitioned Table Statistics\nDESCRIPTION: Example of gathering basic statistics for a non-partitioned table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nANALYZE TABLE Table1 COMPUTE STATISTICS;\n```\n\n----------------------------------------\n\nTITLE: Creating a List Bucketing Table with Multiple Skewed Columns\nDESCRIPTION: This example creates a table T with two skewed columns c1 and c2, with multiple skewed value combinations, stored as directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table T (c1 string, c2 string, c3 string) skewed by (c1, c2) on (('x1', 'x2'), ('y1', 'y2')) stored as directories;\n```\n\n----------------------------------------\n\nTITLE: Invalid Subpartition Configuration in Hive\nDESCRIPTION: Example showing an invalid case where static partition (hr) is attempted to be used as a subpartition of a dynamic partition (ds), which is not allowed due to directory hierarchy constraints.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/dynamicpartitions_27823715.md#2025-04-09_snippet_2\n\nLANGUAGE: hql\nCODE:\n```\n-- throw an exception\nINSERT OVERWRITE TABLE T PARTITION (ds, hr = 11) \nSELECT key, value, ds/*, hr*/ FROM srcpart WHERE ds is not null and hr=11;\n```\n\n----------------------------------------\n\nTITLE: Successful JSON Response from WebHCat Database Description API\nDESCRIPTION: Example of a successful JSON response from the WebHCat API when describing a database. It includes the database location, parameters, comment, and name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getdb_34016250.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"location\":\"hdfs://localhost:9000/warehouse/newdb.db\",\n \"params\":\"{a=b}\",\n \"comment\":\"Hello there\",\n \"database\":\"newdb\"\n}\n```\n\n----------------------------------------\n\nTITLE: Hive 0.14+ Configuration Settings\nDESCRIPTION: Configuration settings required in hive-site.xml and hiveserver2-site.xml for Hive version 0.14 and newer to enable authorization\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_23\n\nLANGUAGE: xml\nCODE:\n```\nhive.server2.enable.doAs=false\nhive.users.in.admin.role=user1,user2\nhive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly\nhive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory\n```\n\n----------------------------------------\n\nTITLE: Altering View Definition\nDESCRIPTION: Syntax for changing the SELECT statement definition of an existing view. Available from Hive 0.11 onwards.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_51\n\nLANGUAGE: sql\nCODE:\n```\nALTER VIEW [db_name.]view_name AS select_statement;\n```\n\n----------------------------------------\n\nTITLE: Creating List Bucketing Table in Hive SQL\nDESCRIPTION: Example of creating a list bucketing table with partitioning in Hive, showing the skewed by clause with stored as directories functionality.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\ncreate table t2 (x string) skewed by (error) on ('a', 'b') partitioned by dt location '/user/hive/warehouse/t2';\n```\n\n----------------------------------------\n\nTITLE: Creating List Bucketing Table in Hive SQL\nDESCRIPTION: Example of creating a list bucketing table with partitioning in Hive, showing the skewed by clause with stored as directories functionality.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\ncreate table t2 (x string) skewed by (error) on ('a', 'b') partitioned by dt location '/user/hive/warehouse/t2';\n```\n\n----------------------------------------\n\nTITLE: Number Base Conversion Functions in Hive SQL\nDESCRIPTION: Functions for converting numbers between different bases including binary, hexadecimal, and custom base conversions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_44\n\nLANGUAGE: SQL\nCODE:\n```\nbin(bigint a)\nhex(bigint a)\nunhex(STRING a)\nconv(bigint num,int from_base,int to_base)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partitioning in Pig with Different Partial Specification\nDESCRIPTION: Another example of partial partition specification, specifying a different partition key than the previous example.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-dynamicpartitions_34014006.md#2025-04-09_snippet_5\n\nLANGUAGE: pig\nCODE:\n```\nstore A into 'mytable' using HCatStorer(\"b=1\");\n```\n\n----------------------------------------\n\nTITLE: Enabling Multiple ReExecution Strategies in Hive\nDESCRIPTION: Example showing how to enable both Overlay and Reoptimize strategies for query reexecution in Hive. This is useful for situations with missing or incorrect statistics, or queries with many joins.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/query-reexecution_87298873.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nset hive.query.reexecution.strategies=overlay,reoptimize;\n```\n\n----------------------------------------\n\nTITLE: Defining Table Sample Syntax for Bucketized Tables in HiveQL\nDESCRIPTION: This snippet shows the syntax for sampling bucketized tables in Hive. It allows users to query a subset of data based on specified buckets and columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\ntable_sample: TABLESAMPLE (BUCKET x OUT OF y [ON colname])\n```\n\n----------------------------------------\n\nTITLE: Using context_ngrams() to find words following a specific phrase\nDESCRIPTION: Query that finds the top 100 words following the phrase \"i love\" in Twitter data. The null parameter specifies which position in the n-gram should be estimated.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statisticsanddatamining_27362058.md#2025-04-09_snippet_1\n\nLANGUAGE: hql\nCODE:\n```\nSELECT context_ngrams(sentences(lower(tweet)), array(\"i\",\"love\",null), 100, [, 1000]) FROM twitter;\n```\n\n----------------------------------------\n\nTITLE: Initializing Hive System Schema\nDESCRIPTION: Initializes Hive system schema using Derby database with verbose output\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ ./schematool -dbType hive -metaDbType derby -initSchema  --verbose -url=\"jdbc:hive2://localhost:10000\"\n```\n\n----------------------------------------\n\nTITLE: Creating Permanent Functions in HiveQL\nDESCRIPTION: Syntax for registering permanent functions in the metastore. Functions can be created with optional JAR, FILE, or ARCHIVE resources that will be loaded when the function is first used.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_63\n\nLANGUAGE: hql\nCODE:\n```\nCREATE FUNCTION [db_name.]function_name AS class_name\n  [USING JAR|FILE|ARCHIVE 'file_uri' [, JAR|FILE|ARCHIVE 'file_uri'] ];\n```\n\n----------------------------------------\n\nTITLE: Using the array_join() function in Hive SQL\nDESCRIPTION: Concatenates array elements with a specified delimiter and NULL replacement. Example: array_join(array(1, 2, NULL, 4), ',',':') returns '1,2,:,4'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_62\n\nLANGUAGE: SQL\nCODE:\n```\narray_join(array, delimiter, replaceNull)\n```\n\n----------------------------------------\n\nTITLE: Handling Union Operations in Hive on Spark\nDESCRIPTION: Explains how Spark's native union transformation will be used to implement SQL union operations more efficiently than the MapReduce-based approach, similar to optimizations already implemented in Tez.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n### Union\n\nWhile it's mentioned above that we will use MapReduce primitives to implement SQL semantics in the Spark execution engine, union is one exception. While it's possible to implement it with MapReduce primitives, it takes up to three MapReduce jobs to union two datasets. Using Spark's union transformation should significantly reduce the execution time and promote interactivity.\n\nIn fact, Tez has already deviated from MapReduce practice with respect to union. There is an existing UnionWork where a union operator is translated to a work unit.\n```\n\n----------------------------------------\n\nTITLE: Hive Import with Custom Location Example\nDESCRIPTION: Example demonstrating how to specify a custom location when importing a table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-importexport_27837968.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nexport table department to 'hdfs_exports_location/department';\nimport table department from 'hdfs_exports_location/department' \n       location 'import_target_location/department';\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Overlay Strategy for Hive Query ReExecution\nDESCRIPTION: Example showing how to use the Overlay strategy to change settings for query reexecution. This example demonstrates setting the 'zzz' variable to 1, but overlaying it with a value of 2 during reexecution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/query-reexecution_87298873.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nset zzz=1;\nset reexec.overlay.zzz=2;\n\nset hive.query.reexecution.enabled=true;\nset hive.query.reexecution.strategies=overlay;\n\ncreate table t(a int);\ninsert into t values (1);\nselect assert_true(${hiveconf:zzz} > a) from t group by a;\n```\n\n----------------------------------------\n\nTITLE: Setting Test Mode Prefix for Hive Output Tables in XML\nDESCRIPTION: Specifies the prefix to be added to output table names when Hive is running in test mode.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_118\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.test.mode.prefix</name>\n  <value>test_</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Using the array_distinct() function in Hive SQL\nDESCRIPTION: Returns an array containing distinct values from the input array. Example: array_distinct(array('b', 'd', 'd', 'a')) returns ['b', 'd', 'a'].\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_61\n\nLANGUAGE: SQL\nCODE:\n```\narray_distinct(array(obj1, obj2, obj3...))\n```\n\n----------------------------------------\n\nTITLE: Creating Hive JDBC Table with Secured Password\nDESCRIPTION: Shows how to create a JDBC table in Hive using a secured password stored in a keystore, specifying the keystore location and password key.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE EXTERNAL TABLE student_jdbc\n(\n  name string,\n  age int,\n  gpa double\n)\nSTORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler'\nTBLPROPERTIES (\n    . . . . . .\n    \"hive.sql.dbcp.password.keystore\" = \"jceks://hdfs/user/foo/test.jceks\",\n    \"hive.sql.dbcp.password.key\" = \"host1.password\",\n    . . . . . .\n);\n```\n\n----------------------------------------\n\nTITLE: Creating an ORC Table Without Compression in HiveQL\nDESCRIPTION: This snippet demonstrates how to create a table in Hive using the ORC file format without compression. It specifies the table schema and sets the 'orc.compress' property to 'NONE' in the table properties.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-orc_31818911.md#2025-04-09_snippet_0\n\nLANGUAGE: HiveQL\nCODE:\n```\ncreate table Addresses (\n  name string,\n  street string,\n  city string,\n  state string,\n  zip int\n) stored as orc tblproperties (\"orc.compress\"=\"NONE\");\n```\n\n----------------------------------------\n\nTITLE: Creating Index Stored as Text File with Delimiter in HiveQL\nDESCRIPTION: Creates a compact index on column7 of table06, storing it as a text file with tab-delimited fields. Shows how to customize the storage format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_5\n\nLANGUAGE: hiveql\nCODE:\n```\nCREATE INDEX table06_index ON TABLE table06 (column7) AS 'COMPACT' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: Running Apache Hive Integration Tests\nDESCRIPTION: Commands to execute integration tests for Apache Hive. First navigating to the itests directory, then running a Maven command that specifically targets the integration test module with the iceberg profile.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/building-hive-from-source_282102252.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean test -pl itest -Piceberg\n```\n\n----------------------------------------\n\nTITLE: Creating Updatable Views in Apache Hive\nDESCRIPTION: This example demonstrates how to create a table and an updatable view on top of it. The view selects specific columns from the underlying table and maintains the same partitioning structure.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/updatableviews_27824044.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate table t1 (id int, key string, value string) partitioned by (ds string, hr string);\n\ncreate view v partitioned on (ds, hr) as select id, value, ds, hr from t1;\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Explicit Skew Specification\nDESCRIPTION: HQL commands to create a table with explicit skew specification. When skew is explicitly defined, top K statistics collection is bypassed in favor of the user-defined skew information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_11\n\nLANGUAGE: hql\nCODE:\n```\nset hive.internal.ddl.list.bucketing.enable=true;\nCREATE TABLE table1 (key STRING, value STRING) PARTITIONED BY (ds STRING) SKEWED BY (key) on ('38', '49');\nINSERT OVERWRITE TABLE table1 PARTITION (ds='2012-09-07') SELECT * FROM table_src;\n```\n\n----------------------------------------\n\nTITLE: Example of Show Role Grant in Hive\nDESCRIPTION: Example demonstrating the grant of a role and displaying role grants for a user.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n0: jdbc:hive2://localhost:10000> GRANT role1 TO USER user1;\nNo rows affected (0.058 seconds)\n\n0: jdbc:hive2://localhost:10000> SHOW ROLE GRANT USER user1;\n+---------+---------------+----------------+----------+\n|  role   | grant_option  |   grant_time   | grantor  |\n+---------+---------------+----------------+----------+\n| public  | false         | 0              |          |\n| role1   | false         | 1398284083000  | uadmin   |\n+---------+---------------+----------------+----------+\n```\n\n----------------------------------------\n\nTITLE: Creating Table with Column Default in Hive SQL\nDESCRIPTION: Demonstrates the syntax for creating a table with a DEFAULT constraint on a column definition. The DEFAULT value must be a literal, datetime function, CURRENT_USER(), NULL, or a CAST of these types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/75969407.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE <tableName> (<columnName> <dataType> DEFAULT <defaultValue>)\n```\n\n----------------------------------------\n\nTITLE: Sample Data File Setup Commands\nDESCRIPTION: Commands to create a sample password file and upload files to HDFS for Pig processing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-pig_34017169.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n% cat fake-passwd\nctdean:Chris Dean:secret\npauls:Paul Stolorz:good\ncarmas:Carlos Armas:evil\ndra:Deirdre McClure:marvelous\n\n% hadoop fs -put id.pig .\n% hadoop fs -put fake-passwd passwd\n```\n\n----------------------------------------\n\nTITLE: Defining Thrift API Methods for Column Statistics Management\nDESCRIPTION: Thrift service method definitions for updating, retrieving, and deleting column statistics for both tables and partitions. These methods enable statistics operations through the metastore API.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/column-statistics-in-hive_29131019.md#2025-04-09_snippet_7\n\nLANGUAGE: Thrift\nCODE:\n```\nbool update_table_column_statistics(1:ColumnStatistics stats_obj) throws (1:NoSuchObjectException o1,   \n 2:InvalidObjectException o2, 3:MetaException o3, 4:InvalidInputException o4)  \n bool update_partition_column_statistics(1:ColumnStatistics stats_obj) throws (1:NoSuchObjectException o1,   \n 2:InvalidObjectException o2, 3:MetaException o3, 4:InvalidInputException o4)\n\nColumnStatistics get_table_column_statistics(1:string db_name, 2:string tbl_name, 3:string col_name) throws  \n (1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidInputException o3, 4:InvalidObjectException o4)   \n ColumnStatistics get_partition_column_statistics(1:string db_name, 2:string tbl_name, 3:string part_name,  \n 4:string col_name) throws (1:NoSuchObjectException o1, 2:MetaException o2,   \n 3:InvalidInputException o3, 4:InvalidObjectException o4)\n\nbool delete_partition_column_statistics(1:string db_name, 2:string tbl_name, 3:string part_name, 4:string col_name) throws   \n (1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidObjectException o3,   \n 4:InvalidInputException o4)  \n bool delete_table_column_statistics(1:string db_name, 2:string tbl_name, 3:string col_name) throws   \n (1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidObjectException o3,   \n 4:InvalidInputException o4)\n```\n\n----------------------------------------\n\nTITLE: Using a Custom UDF in a Hive SQL Query\nDESCRIPTION: This Hive SQL query demonstrates the usage of a custom UDF (my_lower) in a SELECT statement with grouping and aggregation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveplugins_27362098.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect my_lower(title), sum(freq) from titles group by my_lower(title);\n```\n\n----------------------------------------\n\nTITLE: Defining Command Whitelist for Hive Security\nDESCRIPTION: Comma-separated list of non-SQL Hive commands that users are authorized to execute. This setting can be used to restrict the set of authorized commands, enhancing security by limiting user actions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_83\n\nLANGUAGE: markdown\nCODE:\n```\n##### hive.security.command.whitelist\n\n* Default Value: `set,reset,dfs,add,delete,compile[,list,reload]`\n* Added In: Hive 0.13.0 with [HIVE-5400](https://issues.apache.org/jira/browse/HIVE-5400) and [HIVE-5252](https://issues.apache.org/jira/browse/HIVE-5252)\n* Changed in Hive 0.14.0 to include \"list\" and \"reload\" with [HIVE-7592](https://issues.apache.org/jira/browse/HIVE-7592) (\"list\") and [HIVE-7553](https://issues.apache.org/jira/browse/HIVE-7553) (\"reload\")\n```\n\n----------------------------------------\n\nTITLE: Setting Materialized View Time Window for Stale Data Acceptance in Hive\nDESCRIPTION: Configuration setting to define the acceptable time window for using stale materialized view data. This parameter can be set globally or overridden for a specific materialized view as a table property.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nSET hive.materializedview.rewriting.time.window=10min;\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes in Hive\nDESCRIPTION: Legacy syntax for creating indexes in Hive versions prior to 3.0. Includes options for index type, properties, and storage specifications.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_55\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX index_name\n  ON TABLE base_table_name (col_name, ...)\n  AS index_type\n  [WITH DEFERRED REBUILD]\n  [IDXPROPERTIES (property_name=property_value, ...)]\n  [IN TABLE index_table_name]\n  [\n     [ ROW FORMAT ...] STORED AS ...\n     | STORED BY ...\n  ]\n  [LOCATION hdfs_path]\n  [TBLPROPERTIES (...)]\n  [COMMENT \"index comment\"];\n```\n\n----------------------------------------\n\nTITLE: Using inline() UDTF in Hive SQL\nDESCRIPTION: The inline() function explodes an array of structs into multiple rows. It returns a row-set with N columns (where N is the number of fields in the struct), with one row per struct from the input array.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\ninline(ARRAY<STRUCT<f1:T1,...,fn:Tn>> a)\n```\n\n----------------------------------------\n\nTITLE: Map-Join Partition Pruning Algorithm in Hive\nDESCRIPTION: A detailed pseudo-code algorithm that describes how to implement partition pruning for Map-Join operations in Hive. It includes steps for traversing the Task DAG, identifying eligible Map-Join operators, analyzing join conditions, and modifying the execution plan to incorporate partition pruning logic.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoin-and-partition-pruning_34015666.md#2025-04-09_snippet_1\n\nLANGUAGE: pseudo\nCODE:\n```\n1. Walk through Task DAG looking for MapredTask. Perform #2 - #6 for each such MapRedTask.  \n\n2. Skip Task if it contains backup join plan (i.e if not MAPJOIN_ONLY_NOBACKUP or if backupTask is not null).  \n```\n\n----------------------------------------\n\nTITLE: Input Split Generation Property\nDESCRIPTION: Property to control the desired number of splits per partition for input processing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-configuration-properties_39622369.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\nhcat.desired.partition.num.splits=not set\n```\n\n----------------------------------------\n\nTITLE: Rebalance Compaction with Bucket Specification in Hive SQL\nDESCRIPTION: Performs rebalance compaction while specifying the desired number of implicit buckets. This allows for controlling the final number of buckets in the table after redistribution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/rebalance-compaction_240884502.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name COMPACT 'REBALANCE' CLUSTERED INTO n BUCKETS;\n```\n\n----------------------------------------\n\nTITLE: Exploding Arrays in Hive\nDESCRIPTION: Examples of using the explode function to transform array elements into multiple rows. Shows different syntax variations including direct usage and lateral view.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_49\n\nLANGUAGE: HiveQL\nCODE:\n```\nselect explode(array('A','B','C'));\nselect explode(array('A','B','C')) as col;\nselect tf.* from (select 0) t lateral view explode(array('A','B','C')) tf;\nselect tf.* from (select 0) t lateral view explode(array('A','B','C')) tf as col;\n```\n\n----------------------------------------\n\nTITLE: Showing Only First N Characters in Masked String in Hive\nDESCRIPTION: Returns a masked version of the input string, showing only the first n characters unmasked. All other characters are masked where uppercase letters become 'X', lowercase letters become 'x', and numbers become 'n'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_44\n\nLANGUAGE: sql\nCODE:\n```\nmask_show_first_n(string str[, int n])\n```\n\n----------------------------------------\n\nTITLE: Preparing WriterContext in Java\nDESCRIPTION: This code demonstrates how to obtain an HCatWriter instance and prepare a WriterContext. This preparation step occurs on the master node before the context is serialized and made available to slave nodes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-readerwriter_34013921.md#2025-04-09_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nHCatWriter writer = DataTransferFactory.getHCatWriter(entity, config);\nWriterContext info = writer.prepareWrite();\n```\n\n----------------------------------------\n\nTITLE: Conditional XPath Querying in Hive SQL\nDESCRIPTION: Shows how to use xpath UDF with a conditional XPath expression to get node texts based on attribute values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-xpathudf_27362051.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT xpath ('<a><b class=\"bb\">b1</b><b>b2</b><b>b3</b><c class=\"bb\">c1</c><c>c2</c></a>', 'a/*[@class=\"bb\"]/text()') FROM src LIMIT 1 ;\n```\n\n----------------------------------------\n\nTITLE: Launching Hive Metastore Service\nDESCRIPTION: Command to start the Hive Metastore service, which runs as a separate process and provides metadata services to Hive and other components that need to access Hive metadata.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/using-tidb-as-the-hive-metastore-database_158872426.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n${HIVE_HOME}/bin/hive --service metastore\n```\n\n----------------------------------------\n\nTITLE: Dropping a Database in Hive SQL\nDESCRIPTION: Syntax for dropping an existing database or schema in Hive. Includes options for conditional dropping and cascading deletion of contained tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nDROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];\n```\n\n----------------------------------------\n\nTITLE: Creating Index Stored as RCFile in HiveQL\nDESCRIPTION: Creates a compact index on column6 of table05, specifying that the index should be stored in RCFile format for optimized storage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_4\n\nLANGUAGE: hiveql\nCODE:\n```\nCREATE INDEX table05_index ON TABLE table05 (column6) AS 'COMPACT' STORED AS RCFILE;\n```\n\n----------------------------------------\n\nTITLE: Converting an Existing Hive Table to Iceberg Format\nDESCRIPTION: Migrates an existing external Hive table to Iceberg format using ALTER TABLE CONVERT statement, preserving the original data files but changing the table format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE TABLE1 CONVERT TO ICEBERG TBLPROPERTIES ('format-version'='2');\n```\n\n----------------------------------------\n\nTITLE: Starting Hive and Creating Metastore\nDESCRIPTION: Commands to start Hive and trigger metastore creation with initial query.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivederbyservermode_27362068.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncd /opt/hadoop/hive\nbin/hive\nhive> show tables;\n```\n\n----------------------------------------\n\nTITLE: Configuring Group Filter for LDAP Authentication in HiveServer2\nDESCRIPTION: This XML snippet shows how to set the hive.server2.authentication.ldap.groupFilter property to specify a comma-separated list of groups that users must belong to for authentication to succeed.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hive.server2.authentication.ldap.groupFilter</name>\n  <value>group1,group2</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating a Non-Partitioned Table in Hive\nDESCRIPTION: HQL commands to create a non-partitioned table and insert data. Top K statistics are computed for the entire table during insertion if enabled.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_10\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE table1 (key STRING, value STRING);\nINSERT OVERWRITE TABLE table1 SELECT * FROM table_src;\n```\n\n----------------------------------------\n\nTITLE: Testing Hive ODBC Connection with isql\nDESCRIPTION: Command to test the Hive ODBC driver connection using the isql interactive SQL tool.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ isql -v Hive\n```\n\n----------------------------------------\n\nTITLE: Configuring Metastore Database Connection in XML\nDESCRIPTION: The basic configuration parameters required to connect the Metastore to an external RDBMS. These parameters define the JDBC connection details including the connection URL, driver class, username, and password.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-metastore-3-0-administration_75978150.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>javax.jdo.option.ConnectionURL</name>\n  <value>jdbc:mysql://<HOST>:<PORT>/<SCHEMA></value>\n</property>\n<property>\n  <name>javax.jdo.option.ConnectionDriverName</name>\n  <value>com.mysql.jdbc.Driver</value>\n</property>\n<property>\n  <name>javax.jdo.option.ConnectionUserName</name>\n  <value>username</value>\n</property>\n<property>\n  <name>javax.jdo.option.ConnectionPassword</name>\n  <value>password</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Publishing Maven Artifacts to Apache Staging Repository\nDESCRIPTION: Maven command to deploy artifacts to the Apache staging repository for release.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nmvn deploy -DskipTests -Papache-release,iceberg -Dmaven.javadoc.skip=true\n```\n\n----------------------------------------\n\nTITLE: Implementation of PartitionDescGenSink in Hive\nDESCRIPTION: Detailed steps for implementing the PartitionDescGenSink operator, which is responsible for generating partition descriptors that will be used for pruning. It includes logic for maintaining mappings between columns and value sets, and generating partition information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoin-and-partition-pruning_34015666.md#2025-04-09_snippet_6\n\nLANGUAGE: pseudo\nCODE:\n```\n  Implementation of PartitionDescGenSink  \n\n     a) A map is maintained between BigTable column and HashSet.  \n\n     b) From each tuple extract values corresponding to each column with in set-generation-key.  \n\n     c) Add these to a HashSet  \n\n     d) On Close of PartitionDescGenSink consult Metadata to get partitions for the key columns corresponding. This requires potential enhancements to Hive Metadata handling to provide an api \"Get all partitions where column1 has set1 of values, or column2 has set2 of values.  \n\n     e) Write the partition info to file. The file name & location needs to be finalized.\n```\n\n----------------------------------------\n\nTITLE: Original Query Using Star Schema Tables in Hive SQL\nDESCRIPTION: Example of a query that calculates the sum of discounted prices filtered by year and discount range. This query can be optimized by rewriting it to use the materialized view.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views_80447331.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nSELECT SUM(lo_extendedprice * lo_discount)\nFROM lineorder, dates\nWHERE lo_orderdate = d_datekey\n  AND d_year = 2013\n  AND lo_discount between 1 and 3;\n```\n\n----------------------------------------\n\nTITLE: Creating a List Bucketing Table with Single Skewed Column\nDESCRIPTION: This example creates a table T with a single skewed column c1, with skewed value 'x1', stored as directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table T (c1 string, c2 string) skewed by (c1) on ('x1') stored as directories;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Transaction Manager\nDESCRIPTION: Configuration property to enable Hive transactions by setting the transaction manager implementation. The DbTxnManager must be used for transaction support, while DummyTxnManager provides pre-Hive-0.13 behavior without transactions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_67\n\nLANGUAGE: properties\nCODE:\n```\nhive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager\n```\n\n----------------------------------------\n\nTITLE: JDBC Connection URLs for Different Database Systems\nDESCRIPTION: Examples of JDBC connection URL formats for the various supported RDBMS systems that can be used with the Metastore, including SQL Server, MySQL, MariaDB, Oracle, and PostgreSQL.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-metastore-3-0-administration_75978150.md#2025-04-09_snippet_1\n\nLANGUAGE: jdbc\nCODE:\n```\n# MS SQL Server\njdbc:sqlserver://<HOST>:<PORT>;DatabaseName=<SCHEMA>\n\n# MySQL\njdbc:mysql://<HOST>:<PORT>/<SCHEMA>\n\n# MariaDB\njdbc:mysql://<HOST>:<PORT>/<SCHEMA>\n\n# Oracle (thin client)\njdbc:oracle:thin:@//<HOST>:<PORT>/xe\n\n# Postgres\njdbc:postgresql://<HOST>:<PORT>/<SCHEMA>\n```\n\n----------------------------------------\n\nTITLE: Using the size() function for maps in Hive SQL\nDESCRIPTION: Returns the number of elements in a map type collection.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_51\n\nLANGUAGE: SQL\nCODE:\n```\nsize(Map<K.V>)\n```\n\n----------------------------------------\n\nTITLE: Starting HiveServer2 using the hive service command\nDESCRIPTION: Alternative command to start HiveServer2 by using the hive script with the service parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hiveserver2_30758712.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$HIVE_HOME/bin/hive --service hiveserver2\n```\n\n----------------------------------------\n\nTITLE: Executing REPL DUMP Command in Hive SQL\nDESCRIPTION: The REPL DUMP command is used to create a replication dump for a database or specific tables. It supports various options for specifying the replication policy, event range, and additional configurations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivereplicationv2development_66850051.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nREPL DUMP <repl_policy> {REPLACE <old_repl_policy>} {FROM <init-evid> {TO <end-evid>} {LIMIT <num-evids>} } {WITH ('key1'='value1', 'key2'='value2')};\n```\n\n----------------------------------------\n\nTITLE: Hive Example Showing Key Uniqueness Differences with HBase Tables\nDESCRIPTION: SQL statements showing how HBase silently eliminates duplicate keys when inserting data, unlike standard Hive tables. This demonstrates a key behavioral difference between Hive and HBase tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE pokes3(foo INT, bar STRING)\nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n\"hbase.columns.mapping\" = \":key,cf:bar\"\n);\nINSERT OVERWRITE TABLE pokes3 SELECT * FROM pokes;\n-- this will return 1 instead of 3\nSELECT COUNT(1) FROM pokes3 WHERE foo=498;\n```\n\n----------------------------------------\n\nTITLE: Creating Scheduled Query in Hive SQL\nDESCRIPTION: Example of creating a basic scheduled query that runs every 10 minutes\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/scheduled-queries_145724128.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\ncreate scheduled query sc1 cron '0 */10 * * * ? *' as select 1;\n```\n\n----------------------------------------\n\nTITLE: JSON Response Example for Successful DDL Command in WebHCat\nDESCRIPTION: Example JSON response from a successful WebHCat DDL command showing the standard output containing a list of tables, standard error with warnings, and an exit code of 0 indicating success.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-ddl_34015990.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"stdout\": \"important_table\n            my_other_table\n            my_table\n            my_table_2\n            pokes\n            \",\n \"stderr\": \"WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated...\n            Hive history file=/tmp/ctdean/hive_job_log_ctdean_201111111258_2117356679.txt\n            OK\n            Time taken: 1.202 seconds\n            \",\n \"exitcode\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: HCatOutputFormat API in Java for Apache Hive\nDESCRIPTION: API methods for HCatOutputFormat, including setOutput for specifying output table, setSchema for defining output schema, and getTableSchema for retrieving table schema.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-inputoutput_34013776.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n  /**\n   * Set the information about the output to write for the job. This queries the metadata\n   * server to find the StorageHandler to use for the table. It throws an error if the\n   * partition is already published.\n   * @param job the job object\n   * @param outputJobInfo the table output information for the job\n   * @throws IOException the exception in communicating with the metadata server\n   */\n  @SuppressWarnings(\"unchecked\")\n  public static void setOutput(Job job, OutputJobInfo outputJobInfo) throws IOException;\n\n  /**\n   * Set the schema for the data being written out to the partition. The\n   * table schema is used by default for the partition if this is not called.\n   * @param job the job object\n   * @param schema the schema for the data\n   * @throws IOException\n   */\n  public static void setSchema(final Job job, final HCatSchema schema) throws IOException;\n\n  /**\n   * Get the table schema for the table specified in the HCatOutputFormat.setOutput call\n   * on the specified job context.\n   * @param context the context\n   * @return the table schema\n   * @throws IOException if HCatOutputFormat.setOutput has not been called\n   *                     for the passed context\n   */\n  public static HCatSchema getTableSchema(JobContext context) throws IOException;\n```\n\n----------------------------------------\n\nTITLE: Executing REPL LOAD Command in Hive SQL\nDESCRIPTION: The REPL LOAD command is used to load a replication dump into a target database. It supports optional database renaming and additional configurations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivereplicationv2development_66850051.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nREPL LOAD {<dbname>} FROM <dirname> {WITH ('key1'='value1', 'key2'='value2')};\n```\n\n----------------------------------------\n\nTITLE: Altering a Table to Add or Modify Skewed Properties\nDESCRIPTION: This ALTER TABLE statement can convert a non-skewed table to a skewed table or modify the skewed properties of an existing skewed table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE <T> (SCHEMA) SKEWED BY  (keys) ON ('c1', 'c2') [STORED AS DIRECTORIES];\n```\n\n----------------------------------------\n\nTITLE: Creating Hive Table with Custom Accumulo Table Name\nDESCRIPTION: Creates a user table with custom column mappings and overrides the default Accumulo table name using TBLPROPERTIES.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/accumulointegration_46633569.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE users(key int, userid int, username string) \nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES (\"accumulo.columns.mapping\" = \":rowID,f:userid,f:nickname\")\nWITH TBLPROPERTIES (\"accumulo.table.name\" = \"hive_users\");\n```\n\n----------------------------------------\n\nTITLE: Defining Filter String Representation in Java\nDESCRIPTION: Example of how filter conditions are represented as strings in Hive's filter pushdown implementation. This string format is used as the primary representation for passing filters to input formats.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/filterpushdowndev_27362092.md#2025-04-09_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n((key >= 100) and (key < 200))\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Security Settings in hive-site.xml\nDESCRIPTION: Required security configuration settings in hive-site.xml to enable proxy user support with storage-based authorization and authentication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-installwebhcat_34015585.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\nhive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider\nhive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator\nhive.metastore.pre.event.listeners=org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener\nhive.metastore.execute.setugi=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Top K Parameters in Hive\nDESCRIPTION: HQL commands to configure the number of K values to collect and the minimum percentage threshold for inclusion in top K results. These settings control the behavior of the top K statistics collection.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_2\n\nLANGUAGE: hql\nCODE:\n```\nset hive.stats.topk.num=8;\nset hive.stats.topk.minpercent=5.0;\n```\n\n----------------------------------------\n\nTITLE: Renaming Partitions in Hive Using ALTER TABLE\nDESCRIPTION: Syntax for renaming partitions in Hive tables, available since Hive 0.9. This allows changing partition column values and can be used to normalize legacy partition column values to conform to their defined types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_36\n\nLANGUAGE: hql\nCODE:\n```\nALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec;\n\n```\n\n----------------------------------------\n\nTITLE: Mixed Resource Management with Ivy URLs and Filepaths\nDESCRIPTION: Syntax for combining both Ivy URLs and local filepaths in the same ADD and DELETE commands, allowing flexibility in resource management.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-cli_27362033.md#2025-04-09_snippet_7\n\nLANGUAGE: hive\nCODE:\n```\nADD { FILE[S] | JAR[S] | ARCHIVE[S] } { <ivyurl> | <filepath> } <ivyurl>* <filepath>* \nDELETE { FILE[S] | JAR[S] | ARCHIVE[S] } { <ivyurl> | <filepath> } <ivyurl>* <filepath>*\n```\n\n----------------------------------------\n\nTITLE: Example SQL Aggregation Query\nDESCRIPTION: Example of an aggregation query without group-by clause referenced in hive.fetch.task.aggr parameter documentation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_22\n\nLANGUAGE: sql\nCODE:\n```\nselect count(*) from src\n```\n\n----------------------------------------\n\nTITLE: JSON Response Format for WebHCat GetPartitions API\nDESCRIPTION: Example JSON response from the WebHCat GetPartitions API showing partition information. The response includes the partition values (with column names and values), partition names, and identifies the database and table name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getpartitions_34016583.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"partitions\": [\n    {\n      \"values\": [\n        {\n          \"columnName\": \"dt\",\n          \"columnValue\": \"20120101\"\n        },\n        {\n          \"columnName\": \"country\",\n          \"columnValue\": \"US\"\n        }\n      ],\n      \"name\": \"dt='20120101',country='US'\"\n    }\n  ],\n  \"database\": \"default\",\n  \"table\": \"my_table\"\n}\n```\n\n----------------------------------------\n\nTITLE: Partitioned Table Write with HCatStorer\nDESCRIPTION: Demonstrates writing to a specific partition in a partitioned table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_13\n\nLANGUAGE: pig\nCODE:\n```\nstore z into 'web_data' using org.apache.hive.hcatalog.pig.HCatStorer('datestamp=20110924');\n```\n\n----------------------------------------\n\nTITLE: Using Reflect UDF with Java Standard Library Methods in Hive\nDESCRIPTION: Demonstrates how to use the reflect() function to call various Java methods from String and Math classes directly in Hive queries. The example shows string operations, mathematical calculations, and type conversions. Note that as of Hive 0.9.0, java_method() can be used as a synonym for reflect().\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/reflectudf_30754716.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT reflect(\"java.lang.String\", \"valueOf\", 1),\n       reflect(\"java.lang.String\", \"isEmpty\"),\n       reflect(\"java.lang.Math\", \"max\", 2, 3),\n       reflect(\"java.lang.Math\", \"min\", 2, 3),\n       reflect(\"java.lang.Math\", \"round\", 2.5),\n       reflect(\"java.lang.Math\", \"exp\", 1.0),\n       reflect(\"java.lang.Math\", \"floor\", 1.9)\nFROM src LIMIT 1;\n\n1\ttrue\t3\t2\t3\t2.7182818284590455\t1.0\n```\n\n----------------------------------------\n\nTITLE: Setting Admin Role in Hive SQL\nDESCRIPTION: SQL command to set the admin role for a user in Hive. This is necessary to gain admin privileges after configuring the user in hive-site.xml.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nset role admin;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Hive Version\nDESCRIPTION: Demonstrates how to use the version() function in Hive SQL to retrieve the current Hive version, including build number and build hash.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_82\n\nLANGUAGE: SQL\nCODE:\n```\nselect version();\n```\n\n----------------------------------------\n\nTITLE: Failed Metastore Access Example\nDESCRIPTION: Example showing the error when trying to access Hive with incompatible schema versions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-schema-tool_34835119.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ build/dist/bin/hive -e \"show tables\"\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n```\n\n----------------------------------------\n\nTITLE: Using the array_slice() function in Hive SQL\nDESCRIPTION: Returns a subset or range of elements from an array. Example: array_slice(array(1, 2, 3, 4), 2, 2) returns [3, 4].\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_58\n\nLANGUAGE: SQL\nCODE:\n```\narray_slice(array, start, length)\n```\n\n----------------------------------------\n\nTITLE: Viewing Column Statistics using HiveQL\nDESCRIPTION: HiveQL command to view formatted column statistics for a specific column in a table. This provides detailed statistics information after statistics have been computed.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/column-statistics-in-hive_29131019.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\ndescribe formatted [table_name] [column_name];\n```\n\n----------------------------------------\n\nTITLE: Accessing Fields in Deserialized Records with StructObjectInspector in Hive\nDESCRIPTION: This snippet demonstrates how to use the StructObjectInspector's getStructFieldData() method to extract individual fields from a deserialized record in Hive's execution engine.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/serde_27362059.md#2025-04-09_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nStructObjectInspector.getStructFieldData()\n```\n\n----------------------------------------\n\nTITLE: Using HiveServer2 Clear Dangling Scratch Directory Tool\nDESCRIPTION: Command line tool for cleaning up dangling scratch directories left from improper Hive shutdowns. Supports dry-run mode, verbose logging, and custom scratch directory specification.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hiveserver2_30758712.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhive --service cleardanglingscratchdir [-r] [-v] [-s scratchdir]\n    -r   \tdry-run mode, which produces a list on console\n\t-v   \tverbose mode, which prints extra debugging information\n\t-s   \tif you are using non-standard scratch directory\n```\n\n----------------------------------------\n\nTITLE: Ordered Rebalance Compaction in Hive SQL\nDESCRIPTION: Executes rebalance compaction with a specified ordering of data. This allows for simultaneously redistributing and reordering the data within the buckets.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/rebalance-compaction_240884502.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name COMPACT 'REBALANCE' ORDER BY column_name DESC;\n```\n\n----------------------------------------\n\nTITLE: Querying Compaction Status in HQL\nDESCRIPTION: Returns a list of all compaction requests being processed or scheduled in Hive. Supports filtering by database, table, partition, pool, type, and state with ordering and limiting options. Available since Hive 0.13.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_90\n\nLANGUAGE: hql\nCODE:\n```\nSHOW COMPACTIONS [DATABASE.][TABLE] [PARTITION (<partition_spec>)] [POOL_NAME] [TYPE] [STATE] [ORDER BY `start` DESC] [LIMIT 10];\n```\n\n----------------------------------------\n\nTITLE: Tagging Release Candidate in Git\nDESCRIPTION: Git commands to create and push a tag for the release candidate.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ngit tag -a release-X.Y.Z-rcR -m \"Hive X.Y.Z-rcR release.\"\ngit push origin release-X.Y.Z-rcR\n```\n\n----------------------------------------\n\nTITLE: Skew Join Example Query\nDESCRIPTION: Demonstrates how Hive rewrites a join query to handle data skew when most values are distributed around x=1. The query is split into a union of two joins to better handle the skewed distribution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/cost-based-optimization-in-hive_42566775.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nR1 PR1.x=R2.a and PR1.x=1 - R2) union all (R1 PR1.x=R2.a and PR1.x<>1 - R2\n```\n\n----------------------------------------\n\nTITLE: Type Casting with Decimal Values in Hive\nDESCRIPTION: Shows how to cast a Decimal value to another primitive type like BOOLEAN using the CAST operator in Hive SQL.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nselect cast(t as boolean) from decimal_2;\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Metastore Authorization Manager\nDESCRIPTION: Property to specify the authorization manager classes for metastore security. Supports both default grant/revoke model and storage-based authorization using HDFS permissions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_85\n\nLANGUAGE: properties\nCODE:\n```\nhive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider\n```\n\n----------------------------------------\n\nTITLE: Using EXTRACT Function in Hive SQL\nDESCRIPTION: Shows how to use the EXTRACT function to retrieve specific fields from date, timestamp, or interval values. Supports various fields like day, month, year, etc.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nselect extract(month from \"2016-10-20\")\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect extract(hour from \"2016-10-20 05:06:07\")\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect extract(dayofweek from \"2016-10-20 05:06:07\")\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect extract(month from interval '1-3' year to month)\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect extract(minute from interval '3 12:20:30' day to second)\n```\n\n----------------------------------------\n\nTITLE: Starting Beeline with Debug Mode\nDESCRIPTION: Demonstrates how to start the Beeline CLI with debug mode enabled, which opens port 8000 for remote debugger attachment. This allows interactive debugging of Hive queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ beeline --debug\nListening for transport dt_socket at address: 8000\n```\n\n----------------------------------------\n\nTITLE: Committing and Pushing Changes to Apache Hive\nDESCRIPTION: This snippet demonstrates how to commit changes to a feature branch and push them to a personal fork of Apache Hive. It includes best practices for commit messages.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocontribute_27362107.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# make your changes; you should include the ticketid + message in the first commit message\ngit commit -m 'HIVE-9999: Something' -a\n# a simple push will deliver your changes to the github branch\ngit push\n```\n\n----------------------------------------\n\nTITLE: Retrieving HCatalog Table Partitions Using cURL in Bash\nDESCRIPTION: A curl command example that demonstrates how to list all partitions in a Hive table named 'my_table' from the 'default' database. The command uses the WebHCat API endpoint with user authentication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getpartitions_34016583.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table/partition?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partition Insert in Hive\nDESCRIPTION: Example demonstrating dynamic partition insert syntax where country partitions are automatically created based on input data values. This approach requires only one MapReduce job and handles unknown partition values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nFROM page_view_stg pvs\nINSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country)\n       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.country\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partition Insert in Hive\nDESCRIPTION: Example demonstrating dynamic partition insert syntax where country partitions are automatically created based on input data values. This approach requires only one MapReduce job and handles unknown partition values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nFROM page_view_stg pvs\nINSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country)\n       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.country\n```\n\n----------------------------------------\n\nTITLE: Example EXPLAIN Query in Apache Hive\nDESCRIPTION: An example of using the EXPLAIN command to analyze a query that inserts data into a table after grouping and aggregation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nEXPLAIN\nFROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Metastore Pre-Event Listeners\nDESCRIPTION: Configuration property to enable security event listeners on the metastore side that run when databases, tables and partitions are modified. Setting this to the authorization listener class enables metastore security.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_84\n\nLANGUAGE: properties\nCODE:\n```\nhive.metastore.pre.event.listeners=org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener\n```\n\n----------------------------------------\n\nTITLE: Printing Data from RC Files with hive --rcfilecat\nDESCRIPTION: Command used to print the rows stored in an RCFile with options to specify start offset, length, and verbosity level. Columns are tab separated and rows are newline separated.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/rcfilecat_30748712.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhive --rcfilecat [--start=start_offset] [--length=len] [--verbose] fileName\n\n--start=start_offset           Start offset to begin reading in the file\n--length=len                   Length of data to read from the file\n--verbose                      Prints periodic stats about the data read,\n                               how many records, how many bytes, scan rate\n```\n\n----------------------------------------\n\nTITLE: Deleting a Partition using WebHCat API with Curl\nDESCRIPTION: Example curl command to delete a partition named 'country='algeria'' from the 'test_table' in the 'default' database using the WebHCat API. The command is executed as the user 'ctdean'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletepartition_34016611.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X DELETE \\\n       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/partition/country=%27algeria%27?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Starting Hive in Debug Mode\nDESCRIPTION: Command to start Hive directly in debug mode without using Ant.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n    >  ./build/dist/bin/hive --debug\n```\n\n----------------------------------------\n\nTITLE: Rendering Catalog Committers Table in Markdown\nDESCRIPTION: A markdown table listing Catalog committers for Apache Hive, including their Apache usernames, full names, and affiliated organizations with hyperlinks.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/community/people.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Apache username | name                     | organization                             |\n|-----------------|--------------------------|------------------------------------------|\n```\n\n----------------------------------------\n\nTITLE: Managing Skewed Tables in Hive\nDESCRIPTION: Collection of SQL statements for managing skewed table properties including setting skewed columns, disabling skewing, and managing directory storage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name SKEWED BY (col_name1, col_name2, ...)\n  ON ([(col_name1_value, col_name2_value, ...) [, (col_name1_value, col_name2_value), ...]\n  [STORED AS DIRECTORIES];\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name NOT SKEWED;\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name NOT STORED AS DIRECTORIES;\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name SET SKEWED LOCATION (col_name1=\"location1\" [, col_name2=\"location2\", ...] );\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Kafka Ingestion\nDESCRIPTION: Creates an external table for Kafka ingestion with Druid, including Kafka connection and ingestion parameters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE druid_kafka_table_1(`__time` timestamp,`dimension1` string, `dimension1` string, `metric1` int, `metric2 double ....)\n        STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'\n        TBLPROPERTIES (\n        \"kafka.bootstrap.servers\" = \"localhost:9092\",\n        \"kafka.topic\" = \"topic1\",\n        \"druid.kafka.ingestion.useEarliestOffset\" = \"true\",\n        \"druid.kafka.ingestion.maxRowsInMemory\" = \"5\",\n        \"druid.kafka.ingestion.startDelay\" = \"PT1S\",\n        \"druid.kafka.ingestion.period\" = \"PT1S\",\n        \"druid.kafka.ingestion.consumer.retries\" = \"2\"\n        );\n```\n\n----------------------------------------\n\nTITLE: Using Hive Variables in Table Location\nDESCRIPTION: Demonstrates the use of Hive variables to decouple SQL scripts from specific environments, allowing for more flexible and testable code.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-testing-hive-sql_61328063.md#2025-04-09_snippet_1\n\nLANGUAGE: HiveQL\nCODE:\n```\nLOCATION ${myTableLocation}\n```\n\n----------------------------------------\n\nTITLE: Configuring DataNucleus Auto Start Mechanism in Hive XML\nDESCRIPTION: XML configuration snippet for setting up the DataNucleus auto start mechanism in Hive. This is highly recommended for optimal performance and functionality of the metastore.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-metastore-administration_27362076.md#2025-04-09_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n    <name>datanucleus.autoStartMechanism</name>\n    <value>SchemaTable</value>\n  </property>\n```\n\n----------------------------------------\n\nTITLE: Alternative Range Filter\nDESCRIPTION: Shows an equivalent range filter with reversed condition order.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_10\n\nLANGUAGE: pig\nCODE:\n```\na = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader();\nb = filter a by datestamp <= '20110925' and datestamp >= '20110924';\n```\n\n----------------------------------------\n\nTITLE: Using TSV Format for Query Results in Beeline (Deprecated)\nDESCRIPTION: The legacy tsv format in Beeline that's maintained for backward compatibility. Similar to the csv format, but with tab separators instead of commas. Values are always quoted with single quotes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_26\n\nLANGUAGE: text\nCODE:\n```\n'id'\t'value'\t'comment'\n'1'\t'Value1'\t'Test comment 1'\n'2'\t'Value2'\t'Test comment 2'\n'3'\t'Value3'\t'Test comment 3'\n```\n\n----------------------------------------\n\nTITLE: Linear Regression Functions in Hive SQL\nDESCRIPTION: A set of functions for performing linear regression analysis, including slope, intercept, and coefficient of determination calculations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nregr_avgx(independent, dependent)\nregr_avgy(independent, dependent)\nregr_count(independent, dependent)\nregr_intercept(independent, dependent)\nregr_r2(independent, dependent)\nregr_slope(independent, dependent)\nregr_sxx(independent, dependent)\nregr_sxy(independent, dependent)\nregr_syy(independent, dependent)\n```\n\n----------------------------------------\n\nTITLE: Retrieving WebHCat Version - Curl Example\nDESCRIPTION: Command to retrieve the WebHCat version information using curl. Makes a GET request to the version endpoint.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-version_34015986.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/version'\n```\n\n----------------------------------------\n\nTITLE: Configuration Setting Commands in Beeline\nDESCRIPTION: Commands for setting and viewing configuration variables in Beeline, including options to view overridden and all configuration values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_4\n\nLANGUAGE: hql\nCODE:\n```\nset <key>=<value>;\nset;\nset -v;\n```\n\n----------------------------------------\n\nTITLE: Adding Required HBase JARs to HDFS for Hive\nDESCRIPTION: Commands to upload necessary JAR files to HDFS for Hive-HBase integration. This step adds the HBase and Hive-HBase handler libraries to make them accessible during the bulk load process.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhadoop dfs -put /usr/lib/hive/lib/hbase-VERSION.jar /user/hive/hbase-VERSION.jar\nhadoop dfs -put /usr/lib/hive/lib/hive-hbase-handler-VERSION.jar /user/hive/hive-hbase-handler-VERSION.jar\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Thrift Worker Threads for HiveServer2\nDESCRIPTION: Specifies the maximum number of Thrift worker threads for HiveServer2. The default value was increased from 100 to 500 in Hive 0.12.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_44\n\nLANGUAGE: properties\nCODE:\n```\nhive.server2.thrift.max.worker.threads=500\n```\n\n----------------------------------------\n\nTITLE: Creating Apache Access Log Table in Hive SQL\nDESCRIPTION: This snippet shows how to create a table for Apache access logs using a custom RegexSerDe. It defines the table structure and specifies a regex pattern for parsing the log entries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/userguide_27362066.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nadd jar ../build/contrib/hive_contrib.jar;\n\nCREATE TABLE apachelog (\n  host STRING,\n  identity STRING,\n  user STRING,\n  time STRING,\n  request STRING,\n  status STRING,\n  size STRING,\n  referer STRING,\n  agent STRING)\nROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'\nWITH SERDEPROPERTIES (\n  \"input.regex\" = \"([^]*) ([^]*) ([^]*) (-|\\\\[^\\\\]*\\\\]) ([^ \\\"]*|\\\"[^\\\"]*\\\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\"]*|\\\"[^\\\"]*\\\") ([^ \\\"]*|\\\"[^\\\"]*\\\"))?\",\n  \"output.format.string\" = \"%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s\"\n)\nSTORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: Enabling Bucketed Group By Optimization\nDESCRIPTION: Controls whether to enable the bucketed group by from bucketed partitions/tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\nhive.optimize.groupby=true\n```\n\n----------------------------------------\n\nTITLE: Running Maven Thrift Profile\nDESCRIPTION: Maven commands to generate Thrift code in the Hive project using the thriftif profile.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocontribute_27362107.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -Pthriftif -DskipTests -Dthrift.home=/usr/local\n```\n\n----------------------------------------\n\nTITLE: Multi Table/File Inserts in Hive\nDESCRIPTION: Shows how to perform multiple inserts from a single source into different tables and HDFS locations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_27\n\nLANGUAGE: hql\nCODE:\n```\nFROM pv_users\nINSERT OVERWRITE TABLE pv_gender_sum\n    SELECT pv_users.gender, count_distinct(pv_users.userid)\n    GROUP BY pv_users.gender\n\nINSERT OVERWRITE DIRECTORY '/user/data/tmp/pv_age_sum'\n    SELECT pv_users.age, count_distinct(pv_users.userid)\n    GROUP BY pv_users.age;\n```\n\n----------------------------------------\n\nTITLE: Using XML Attribute Format for Query Results in Beeline\nDESCRIPTION: The xmlattr output format in Beeline displays the query results in XML format where each row is represented as a \"result\" element. The column values are shown as attributes on the result element with attribute names matching the column names.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_16\n\nLANGUAGE: xml\nCODE:\n```\n<resultset>\n  <result id=\"1\" value=\"Value1\" comment=\"Test comment 1\"/>\n  <result id=\"2\" value=\"Value2\" comment=\"Test comment 2\"/>\n  <result id=\"3\" value=\"Value3\" comment=\"Test comment 3\"/>\n</resultset>\n```\n\n----------------------------------------\n\nTITLE: Using TSV2 Format for Query Results in Beeline\nDESCRIPTION: The tsv2 format in Beeline (available from Hive 0.14) displays results as tab-separated values with a header row. It follows the same quoting rules as csv2 but uses tabs as delimiters instead of commas.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_21\n\nLANGUAGE: text\nCODE:\n```\nid\tvalue\tcomment\n1\tValue1\tTest comment 1\n2\tValue2\tTest comment 2\n3\tValue3\tTest comment 3\n```\n\n----------------------------------------\n\nTITLE: Showing Grants in Hive\nDESCRIPTION: Shows privileges granted on tables or views for specific users or roles.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nSHOW GRANT [principal_specification] ON (ALL | [TABLE] table_or_view_name);\n \nprincipal_specification\n  : USER user\n  | ROLE role\n```\n\n----------------------------------------\n\nTITLE: Character Length Function\nDESCRIPTION: Returns count of UTF-8 characters in input string\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\ncharacter_length(string str)\n```\n\n----------------------------------------\n\nTITLE: Simple Table Scan Example\nDESCRIPTION: Basic example of scanning a table using HCatLoader with field projection\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_5\n\nLANGUAGE: pig\nCODE:\n```\na = load 'student_data' using org.apache.hive.hcatalog.pig.HCatLoader();\nb = foreach a generate name, age;\n```\n\n----------------------------------------\n\nTITLE: Revoking Roles in Hive\nDESCRIPTION: Revokes role membership from users or roles. Can revoke just admin option while keeping the role.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nREVOKE [ADMIN OPTION FOR] role_name [, role_name] ...\nFROM principal_specification [, principal_specification] ... ;\n\nprincipal_specification\n  : USER user\n  | ROLE role\n```\n\n----------------------------------------\n\nTITLE: HiveStorageHandler Interface Definition in Java\nDESCRIPTION: This Java interface defines the structure for Hive Storage Handlers. It includes methods for specifying input/output formats, SerDe class, metadata hook, and table job properties configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/storagehandlers_27362063.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npackage org.apache.hadoop.hive.ql.metadata;\n\nimport java.util.Map;\n\nimport org.apache.hadoop.conf.Configurable;\nimport org.apache.hadoop.hive.metastore.HiveMetaHook;\nimport org.apache.hadoop.hive.ql.plan.TableDesc;\nimport org.apache.hadoop.hive.serde2.SerDe;\nimport org.apache.hadoop.mapred.InputFormat;\nimport org.apache.hadoop.mapred.OutputFormat;\n\npublic interface HiveStorageHandler extends Configurable {\n  public Class<? extends InputFormat> getInputFormatClass();\n  public Class<? extends OutputFormat> getOutputFormatClass();\n  public Class<? extends SerDe> getSerDeClass();\n  public HiveMetaHook getMetaHook();\n  public void configureTableJobProperties(\n    TableDesc tableDesc,\n    Map<String, String> jobProperties);\n}\n```\n\n----------------------------------------\n\nTITLE: Executing TPC-DS Query in Hive SQL\nDESCRIPTION: This SQL query is a TPC-DS benchmark query rewritten for Hive. It performs a complex analysis on web sales data, joining with item and date dimension tables, filtering by category and date, and calculating revenue ratios.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-tez_33296197.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nselect\n  i_item_desc\n  ,i_category\n  ,i_class\n  ,i_current_price\n  ,i_item_id\n  ,itemrevenue\n  ,itemrevenue*100/sum(itemrevenue) over\n    (partition by i_class) as revenueratio\nfrom\n  (select\n     i_item_desc\n     ,i_category\n     ,i_class\n     ,i_current_price\n     ,i_item_id\n     ,sum(ws_ext_sales_price) as itemrevenue\n   from\n     web_sales\n     join item on (web_sales.ws_item_sk = item.i_item_sk)\n     join date_dim on (web_sales.ws_sold_date_sk = date_dim.d_date_sk)\n   where\n     i_category in ('1', '2', '3')\n     and year(d_date) = 2001 and month(d_date) = 10\n   group by\n     i_item_id\n     ,i_item_desc\n     ,i_category\n     ,i_class\n     ,i_current_price) tmp\norder by\n  i_category\n  ,i_class\n  ,i_item_id\n  ,i_item_desc\n  ,revenueratio;\n```\n\n----------------------------------------\n\nTITLE: Executing TPC-DS Query in Hive SQL\nDESCRIPTION: This SQL query is a TPC-DS benchmark query rewritten for Hive. It performs a complex analysis on web sales data, joining with item and date dimension tables, filtering by category and date, and calculating revenue ratios.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-tez_33296197.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nselect\n  i_item_desc\n  ,i_category\n  ,i_class\n  ,i_current_price\n  ,i_item_id\n  ,itemrevenue\n  ,itemrevenue*100/sum(itemrevenue) over\n    (partition by i_class) as revenueratio\nfrom\n  (select\n     i_item_desc\n     ,i_category\n     ,i_class\n     ,i_current_price\n     ,i_item_id\n     ,sum(ws_ext_sales_price) as itemrevenue\n   from\n     web_sales\n     join item on (web_sales.ws_item_sk = item.i_item_sk)\n     join date_dim on (web_sales.ws_sold_date_sk = date_dim.d_date_sk)\n   where\n     i_category in ('1', '2', '3')\n     and year(d_date) = 2001 and month(d_date) = 10\n   group by\n     i_item_id\n     ,i_item_desc\n     ,i_category\n     ,i_class\n     ,i_current_price) tmp\norder by\n  i_category\n  ,i_class\n  ,i_item_id\n  ,i_item_desc\n  ,revenueratio;\n```\n\n----------------------------------------\n\nTITLE: Successful Response Format\nDESCRIPTION: Example JSON response showing a successful property retrieval with the property value, table name and database name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getproperty_34017004.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"property\": {\n   \"fruit\": \"apple\"\n },\n \"table\": \"test_table\",\n \"database\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying HiveServer2 help information\nDESCRIPTION: Command to show the HiveServer2 usage help message with the -H or --help option.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hiveserver2_30758712.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$HIVE_HOME/bin/hive --service hiveserver2 -H\nStarting HiveServer2\nusage: hiveserver2\n -H,--help                        Print help information\n    --hiveconf <property=value>   Use value for given property\n```\n\n----------------------------------------\n\nTITLE: Implementing IndexPredicateAnalyzer Class in Java\nDESCRIPTION: Definition of the IndexPredicateAnalyzer class used for detecting simple sargable subexpressions in an ExprNodeDesc tree. This class provides methods for registering comparison operators, managing allowed column names, and analyzing predicates.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/filterpushdowndev_27362092.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class IndexPredicateAnalyzer\n{\n  public IndexPredicateAnalyzer();\n\n  /**\n * Registers a comparison operator as one which can be satisfied\n * by an index search.  Unless this is called, analyzePredicate\n * will never find any indexable conditions.\n   *\n * @param udfName name of comparison operator as returned\n * by either {@link GenericUDFBridge#getUdfName} (for simple UDF's)\n * or udf.getClass().getName() (for generic UDF's).\n   */\n  public void addComparisonOp(String udfName);\n\n  /**\n * Clears the set of column names allowed in comparisons.  (Initially, all\n * column names are allowed.)\n   */\n  public void clearAllowedColumnNames();\n\n  /**\n * Adds a column name to the set of column names allowed.\n   *\n * @param columnName name of column to be allowed\n   */\n  public void allowColumnName(String columnName);\n\n  /**\n * Analyzes a predicate.\n   *\n * @param predicate predicate to be analyzed\n   *\n * @param searchConditions receives conditions produced by analysis\n   *\n * @return residual predicate which could not be translated to\n * searchConditions\n   */\n  public ExprNodeDesc analyzePredicate(\n    ExprNodeDesc predicate,\n    final List<IndexSearchCondition> searchConditions);\n\n  /**\n * Translates search conditions back to ExprNodeDesc form (as\n * a left-deep conjunction).\n   *\n * @param searchConditions (typically produced by analyzePredicate)\n   *\n * @return ExprNodeDesc form of search conditions\n   */\n  public ExprNodeDesc translateSearchConditions(\n    List<IndexSearchCondition> searchConditions);\n}\n```\n\n----------------------------------------\n\nTITLE: Hive Logging Configuration\nDESCRIPTION: Example showing how to configure Hive logging to output to console with INFO level.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-cli_27362033.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$HIVE_HOME/bin/hive --hiveconf hive.root.logger=INFO,console\n```\n\n----------------------------------------\n\nTITLE: Pig Load Examples with Filtering\nDESCRIPTION: Examples of loading data with partition filtering in Pig scripts using HCatLoader\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_4\n\nLANGUAGE: pig\nCODE:\n```\n/* myscript.pig */\nA = LOAD 'tablename' USING org.apache.hive.hcatalog.pig.HCatLoader();\n\n-- date is a partition column; age is not\nB = filter A by date == '20100819' and age < 30;\n\n-- both date and country are partition columns\nC = filter A by date == '20100819' and country == 'US';\n```\n\n----------------------------------------\n\nTITLE: Output Compression Configuration\nDESCRIPTION: Controls compression of query output files. When enabled, uses Hadoop's mapred.output.compress configuration for codec settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_14\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.compress.output=false\n```\n\n----------------------------------------\n\nTITLE: Querying Table Privileges for All Users in Hive\nDESCRIPTION: Shows how to list all privileges granted on a specific table (hivejiratable) for all users and roles\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nshow grant on table hivejiratable;\n```\n\n----------------------------------------\n\nTITLE: Bucket Map Join Configuration in Hive\nDESCRIPTION: This example shows the configuration setting required to enable bucket map joins in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.optimize.bucketmapjoin = true\n```\n\n----------------------------------------\n\nTITLE: Resource Deletion Commands in Beeline\nDESCRIPTION: Commands for removing resources from the distributed cache, supporting both filepath and Ivy URL formats.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_7\n\nLANGUAGE: hql\nCODE:\n```\ndelete FILE[S] <filepath>*\ndelete JAR[S] <filepath>*\ndelete ARCHIVE[S] <filepath>*\ndelete FILE[S] <ivyurl> <ivyurl>*\ndelete JAR[S] <ivyurl> <ivyurl>*\ndelete ARCHIVE[S] <ivyurl> <ivyurl>*\n```\n\n----------------------------------------\n\nTITLE: Setting JDBC Driver for Statistics Database in Hive\nDESCRIPTION: Specifies the JDBC driver for the database storing temporary Hive statistics. Default is the Derby embedded driver.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_78\n\nLANGUAGE: properties\nCODE:\n```\nhive.stats.jdbcdriver=org.apache.derby.jdbc.EmbeddedDriver\n```\n\n----------------------------------------\n\nTITLE: Executing ACID Operations in Hive SQL\nDESCRIPTION: SQL commands demonstrating INSERT, UPDATE, and DELETE operations on an ACID-enabled Hive table. These operations are part of the new grammar support for transactions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118453.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n-- Insert new records\nINSERT INTO acid_table VALUES (1, 'John'), (2, 'Jane');\n\n-- Update existing records\nUPDATE acid_table SET name = 'John Doe' WHERE id = 1;\n\n-- Delete records\nDELETE FROM acid_table WHERE id = 2;\n```\n\n----------------------------------------\n\nTITLE: Setting Query Result File Format\nDESCRIPTION: Property to specify the file format for storing intermediate query results. Options include TextFile, SequenceFile, and RCfile.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_27\n\nLANGUAGE: properties\nCODE:\n```\nhive.query.result.fileformat=SequenceFile\n```\n\n----------------------------------------\n\nTITLE: Derby Environment Configuration\nDESCRIPTION: Environment variable configuration for Derby installation in /etc/profile.d/derby.sh.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivederbyservermode_27362068.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDERBY_INSTALL=/opt/hadoop/db-derby-10.4.1.3-bin\nDERBY_HOME=/opt/hadoop/db-derby-10.4.1.3-bin\nexport DERBY_INSTALL\nexport DERBY_HOME\n```\n\n----------------------------------------\n\nTITLE: Using CSV2 Format for Query Results in Beeline\nDESCRIPTION: The csv2 format in Beeline (available from Hive 0.14) displays results as comma-separated values with header row. It handles quoting correctly for values containing special characters, and embedded quotes are escaped with another quote character.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_20\n\nLANGUAGE: text\nCODE:\n```\nid,value,comment\n1,Value1,Test comment 1\n2,Value2,Test comment 2\n3,Value3,Test comment 3\n \n```\n\n----------------------------------------\n\nTITLE: Simplified Remote Database Creation in Hive SQL\nDESCRIPTION: This SQL statement demonstrates a simplified syntax for creating a remote database in Hive, using default values for the connection method and remote database name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/80452092.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE REMOTE DATABASE db_name\nWITH DBPROPERTIES (\n  'hive.metastore.uris' = 'thrift://remote-hms:9083'\n);\n```\n\n----------------------------------------\n\nTITLE: M2_REPO Path Examples\nDESCRIPTION: Example paths for M2_REPO configuration in Eclipse for different operating systems\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n/Users/$USER/.m2/repository\n```\n\nLANGUAGE: bash\nCODE:\n```\n/home/$USER/.m2/repository\n```\n\nLANGUAGE: bash\nCODE:\n```\nC:/users/$USER/.m2/repository\n```\n\n----------------------------------------\n\nTITLE: Building Apache Hive Distribution\nDESCRIPTION: Maven command to build the Hive distribution package. It skips tests for faster build time and includes several profiles (dist, iceberg, itests) that enable specific build features.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/building-hive-from-source_282102252.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -DskipTests -Pdist -Piceberg -Pitests\n```\n\n----------------------------------------\n\nTITLE: Creating Table with Spatial Column in Hive\nDESCRIPTION: Example of creating a table with a GEOMETRY data type column for spatial data storage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/spatial-queries_34022710.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE IF NOT EXISTS spatial_table (\\n  tile_id STRING,\\n  d_id STRING,\\n  rec_id STRING,\\n  outline GEOMETRY\\n) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n```\n\n----------------------------------------\n\nTITLE: Querying All User Privileges in Hive\nDESCRIPTION: Demonstrates how to view all privileges assigned to a specific user (ashutosh) across all objects in the Hive database\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\nshow grant user ashutosh on all;\n```\n\n----------------------------------------\n\nTITLE: Basic UNION Syntax in Hive\nDESCRIPTION: Shows the fundamental syntax for combining multiple SELECT statements using UNION operations. Supports both UNION ALL for keeping duplicates and UNION DISTINCT for removing duplicates.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-union_27362049.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect_statement UNION [ALL | DISTINCT] select_statement UNION [ALL | DISTINCT] select_statement ...\n```\n\n----------------------------------------\n\nTITLE: Defining ExecuteStatement Request/Response Structs in Thrift\nDESCRIPTION: Defines structures for executing SQL statements. Request includes session, statement text and optional configuration overlay. Response contains status and operation handle for tracking execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-thrift-api_27843687.md#2025-04-09_snippet_3\n\nLANGUAGE: thrift\nCODE:\n```\nstruct TExecuteStatementReq {\n  1: required TSessionHandle session\n  2: required string statement\n  3: map<string, string> conf_overlay\n}\n\nstruct TExecuteStatementResp {\n  1: required TStatus status\n  2: TOperationHandle op_handle\n}\n```\n\n----------------------------------------\n\nTITLE: Showing Tables in HiveQL\nDESCRIPTION: Syntax for listing tables and views in the current or specified database, with optional filtering using wildcards to match table names.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_68\n\nLANGUAGE: hql\nCODE:\n```\nSHOW TABLES [IN database_name] ['identifier_with_wildcards'];\n```\n\n----------------------------------------\n\nTITLE: Running Hive Web Interface as Background Process\nDESCRIPTION: Command to run HWI as a background process with output redirection\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivewebinterface_27362110.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnohup bin/hive --service hwi > /dev/null 2> /dev/null &\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple Dependent Tables with Different Dependencies in Hive\nDESCRIPTION: This extended example demonstrates creating a dependent table 'Tdep' that initially depends on table 'T', then creating a new table 'T2' and altering 'Tdep' to depend on 'T2' for newer partitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/dependent-tables_30151205.md#2025-04-09_snippet_2\n\nLANGUAGE: HiveQL\nCODE:\n```\ncreate table T (key string, value string) partitioned by (ds string, hr string);\n\n-- create a dependent table which specifies the dependencies explicitly\n-- Tdep inherits the schema of T\n-- Tdep is partitioned by a prefix of T (either ds or ds,hr)\ncreate dependent table Tdep partitioned by (ds string) depends on table T;\n\n-- create partition T@ds=1/hr=1 to T@ds=1/hr=24\nalter table Tdep add partition (ds=1);\n\n-- repeat this for T@ds=1 to T@ds=10\n\n-- T is being deprecated, and T2 is the new table (with the same schema, possibly partitioned on different keys)\n\ncreate table T2 (key string, value string) partitioned by (ds string, hr string, min string);\n\nalter table Tdep depends on table T2;\n\n-- create partition T2@ds=11/hr=1/min=1 to T2@ds=11/hr=24/min=60\nalter table Tdep add partition (ds=1);\n```\n\n----------------------------------------\n\nTITLE: Setting Secret Bits for Hive-Spark Communication\nDESCRIPTION: Configures the number of bits of randomness in the generated secret for communication between the Hive client and remote Spark driver.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_58\n\nLANGUAGE: properties\nCODE:\n```\nhive.spark.client.secret.bits = 256\n```\n\n----------------------------------------\n\nTITLE: Launching HiveServer2 with Embedded Metastore\nDESCRIPTION: Docker command to run HiveServer2 with embedded Derby metastore database\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hive-with-docker_282102281.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --name hive4 apache/hive:${HIVE_VERSION}\n```\n\n----------------------------------------\n\nTITLE: UPDATE Syntax in HiveQL\nDESCRIPTION: Standard syntax for updating records in ACID-compliant Hive tables. Available starting in Hive 0.14. Supports conditional updates with WHERE clause and multiple column updates.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nStandard Syntax:\nUPDATE tablename SET column = value [, column = value ...] [WHERE expression]\n```\n\n----------------------------------------\n\nTITLE: Extracting Node Text Values with XPath in Hive SQL\nDESCRIPTION: Shows how to use the xpath UDF to get a list of node text values from an XML string.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-xpathudf_27362051.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nselect xpath('<a><b>b1</b><b>b2</b></a>','a/*/text()') from src limit 1 ;\n```\n\n----------------------------------------\n\nTITLE: Configuring Fall Back Authorizer in HiveServer2\nDESCRIPTION: XML configuration for enabling Fall Back Authorizer in hiveserver2-site.xml. This configuration enables basic security controls like disallowing local file access and restricting certain commands to admin users.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-authorization_27362032.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hive.security.authorization.enabled</name>\n  <value>true</value>\n</property>\n<property>\n  <name>hive.security.authorization.manager</name>\n  <value>org.apache.hadoop.hive.ql.security.authorization.plugin.fallback.FallbackHiveAuthorizerFactory</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Granting Roles in Hive\nDESCRIPTION: Grants roles to users or other roles with optional admin privileges. Admin option allows recipients to grant the role to others.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nGRANT role_name [, role_name] ...\nTO principal_specification [, principal_specification] ... \n[ WITH ADMIN OPTION ];\n\nprincipal_specification\n  : USER user\n  | ROLE role\n```\n\n----------------------------------------\n\nTITLE: Supported EXISTS Subquery in SELECT\nDESCRIPTION: Example of a supported EXISTS subquery that checks if any parts exist. Both correlated and uncorrelated EXISTS subqueries will be supported in the implementation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/subqueries-in-select_68717850.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nSELECT EXISTS(SELECT p_size FROM part)\nFROM part\n```\n\n----------------------------------------\n\nTITLE: Defining Internal Variable List in Hive\nDESCRIPTION: Comma-separated list of configuration options which are internally used and should not be settable via set command. This helps prevent unintended modifications to critical internal variables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_82\n\nLANGUAGE: markdown\nCODE:\n```\n##### hive.conf.internal.variable.list\n\n* Default Value: `hive.added.files.path,hive.added.jars.path,hive.added.archives.path`\n* Added In: Hive 1.3.0 with [HIVE-12346](https://issues.apache.org/jira/browse/HIVE-12346)\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Frontmatter in YAML\nDESCRIPTION: This snippet shows the frontmatter for a Hugo blog post. It includes a dynamically generated title, current date, and draft status. The title is created by replacing hyphens with spaces in the filename and capitalizing words.\nSOURCE: https://github.com/apache/hive-site/blob/main/archetypes/default.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"{{ replace .Name \"-\" \" \" | title }}\"\ndate: {{ .Date }}\ndraft: true\n---\n```\n\n----------------------------------------\n\nTITLE: Renaming Table in Hive\nDESCRIPTION: Alters an existing table by renaming it to a new name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_14\n\nLANGUAGE: HiveQL\nCODE:\n```\nALTER TABLE old_table_name RENAME TO new_table_name;\n```\n\n----------------------------------------\n\nTITLE: Creating Database and User for Hive Metastore in TiDB\nDESCRIPTION: SQL commands to create a database named 'hive', create a user with password, and grant necessary privileges for Hive Metastore to use TiDB.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/using-tidb-as-the-hive-metastore-database_158872426.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Create a database for Hive Metastore.\ncreate database hive;\n-- Create a user and password for Hive Metastore.\ncreate user 'hive'@'%' identified by '123456';\n-- Grant privileges to the user.\ngrant all privileges on hive.* to 'hive'@'%' identified by '123456';\n-- Flush privileges.\nflush privileges;\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC File Memory Usage\nDESCRIPTION: Property to set the maximum fraction of heap that can be used by ORC file writers.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_28\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.orc.memory.pool=0.5\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive via hive-site.xml\nDESCRIPTION: XML configuration example showing how to set the scratch directory in hive-site.xml configuration file. This method applies the setting globally for the entire Hive installation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-configuration_27362070.md#2025-04-09_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n    <name>hive.exec.scratchdir</name>\n    <value>/tmp/mydir</value>\n    <description>Scratch space for Hive jobs</description>\n  </property>\n```\n\n----------------------------------------\n\nTITLE: Successful Response - WebHCat Table List JSON\nDESCRIPTION: Example successful JSON response showing a list of tables matching the query criteria from the default database\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-gettables_34016290.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"tables\": [\n   \"my_table\",\n   \"my_table_2\",\n   \"my_table_3\"\n ],\n \"database\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching Apache Hive Source Code via Git\nDESCRIPTION: Command to clone the Apache Hive repository from GitHub and checkout a specific release tag (4.0.0). This allows developers to get the exact version of the source code for building.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/building-hive-from-source_282102252.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --branch rel/release-4.0.0 https://github.com/apache/hive.git\n```\n\n----------------------------------------\n\nTITLE: Using ExampleUseCase Class for Hive Streaming Implementation\nDESCRIPTION: Reference to the ExampleUseCase example class in the Hive codebase that demonstrates how to use the Streaming API. This example provides a complete implementation of the streaming mutation process.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118454.md#2025-04-09_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nExampleUseCase\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Version Environment Variable\nDESCRIPTION: Export command to set Hive version as environment variable for later use\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hive-with-docker_282102281.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport HIVE_VERSION=4.0.0\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response\nDESCRIPTION: Sample JSON response from WebHCat showing job ID and execution information after submitting a Pig job.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-pig_34017169.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n \"id\": \"job_201111101627_0018\",\n \"info\": {\n          \"stdout\": \"templeton-job-id:job_201111101627_0018\\n                    \",\n          \"stderr\": \"\",\n          \"exitcode\": 0\n         }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Connectors in Hive SQL\nDESCRIPTION: Syntax for creating data connectors in Hive 4.0.0+. Supports connecting to external datasources like MySQL, Postgres, and Derby with configurable properties and authentication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE CONNECTOR [IF NOT EXISTS] connector_name\n  [TYPE datasource_type]\n  [URL datasource_url]\n  [COMMENT connector_comment]\n  [WITH DCPROPERTIES (property_name=property_value, ...)];\n```\n\n----------------------------------------\n\nTITLE: Initializing Hive with HBase Handler - Distributed\nDESCRIPTION: Command to start Hive CLI with HBase handler configuration for a distributed cluster using ZooKeeper quorum.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$HIVE_SRC/build/dist/bin/hive --auxpath $HIVE_SRC/build/dist/lib/hive-hbase-handler-0.9.0.jar,$HIVE_SRC/build/dist/lib/hbase-0.92.0.jar,$HIVE_SRC/build/dist/lib/zookeeper-3.3.4.jar,$HIVE_SRC/build/dist/lib/guava-r09.jar --hiveconf hbase.zookeeper.quorum=zk1.yoyodyne.com,zk2.yoyodyne.com,zk3.yoyodyne.com\n```\n\n----------------------------------------\n\nTITLE: IN Subquery Example in WHERE Clause (Hive)\nDESCRIPTION: Shows how to use an uncorrelated subquery with the IN operator in the WHERE clause, which was introduced in Hive 0.13. This example filters records from table A based on results from table B.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-subqueries_27362044.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT *\nFROM A\nWHERE A.a IN (SELECT foo FROM B);\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Metastore Event Listeners for Replication in Hive\nDESCRIPTION: XML configuration for hive-site.xml to enable metastore event listeners for replication. Sets the event listener class and time-to-live for database events.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/replication_61336919.md#2025-04-09_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n  <property>\n    <name>hive.metastore.event.listeners</name>\n    <value>org.apache.hive.hcatalog.listener.DbNotificationListener</value>\n  </property>\n  <property>\n    <name>hive.metastore.event.db.listener.timetolive</name>\n    <value>86400s</value>\n  </property>\n```\n\n----------------------------------------\n\nTITLE: Creating LZO Index File using Hadoop\nDESCRIPTION: Command to create an LZO index file using the Hadoop LzoIndexer utility. This enables splitting of LZO files for parallel processing in Hadoop.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lzo_33298193.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nhadoop jar /path/to/jar/hadoop-lzo-cdh4-0.4.15-gplextras.jar com.hadoop.compression.lzo.LzoIndexer  /path/to/HDFS/dir/containing/lzo/files\n```\n\n----------------------------------------\n\nTITLE: Dropping Partition in Hive\nDESCRIPTION: Removes a specific partition from a partitioned table in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_18\n\nLANGUAGE: HiveQL\nCODE:\n```\nALTER TABLE pv_users DROP PARTITION (ds='2008-08-08')\n```\n\n----------------------------------------\n\nTITLE: Analyzing Data with Hive Using HCatalog\nDESCRIPTION: This Hive SQL query demonstrates how HCatalog simplifies data analysis by abstracting partition management, allowing direct querying without table alterations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-usinghcat_34013260.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect advertiser_id, count(clicks)\nfrom processedevents\nwhere date = '20100819'\ngroup by advertiser_id;\n```\n\n----------------------------------------\n\nTITLE: Setting Codahale Metrics Implementation for Hive\nDESCRIPTION: Specifies the Hive metrics subsystem implementation class. The Codahale implementation is the newer metrics system, replacing the legacy implementation that was used before Hive 1.3 and 2.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_92\n\nLANGUAGE: xml\nCODE:\n```\norg.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint to Column Statistics Table\nDESCRIPTION: SQL statement to add a foreign key constraint from the COLUMN_STATISTICS table to the TBLS table, establishing a relationship between statistics and table definitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/column-statistics-in-hive_29131019.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE COLUMN_STATISTICS ADD CONSTRAINT COLUMN_STATISTICS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID) INITIALLY DEFERRED ;\n```\n\n----------------------------------------\n\nTITLE: Unarchiving a Hive Table Partition\nDESCRIPTION: Shows the SQL command to unarchive a previously archived partition, reverting it back to its original file structure.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-archiving_27362031.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE srcpart UNARCHIVE PARTITION(ds='2008-04-08', hr='12')\n```\n\n----------------------------------------\n\nTITLE: Sample Output from EXPLAIN AUTHORIZATION Command\nDESCRIPTION: This output shows what authorization is required for a query, listing input tables and partitions, output locations, the current user, operation type, and any authorization failures encountered.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nINPUTS: \n  default@srcpart\n  default@src\n  default@srcpart@ds=2008-04-08/hr=11\n  default@srcpart@ds=2008-04-08/hr=12\n  default@srcpart@ds=2008-04-09/hr=11\n  default@srcpart@ds=2008-04-09/hr=12\nOUTPUTS: \n  hdfs://localhost:9000/tmp/.../-mr-10000\nCURRENT_USER: \n  navis\nOPERATION: \n  QUERY\nAUTHORIZATION_FAILURES: \n  Permission denied: Principal [name=navis, type=USER] does not have following privileges for operation QUERY [[SELECT] on Object [type=TABLE_OR_VIEW, name=default.src], [SELECT] on Object [type=TABLE_OR_VIEW, name=default.srcpart]]\n```\n\n----------------------------------------\n\nTITLE: Using SOURCE Command in Hive\nDESCRIPTION: Shows the usage of the SOURCE command to combine multiple smaller SQL scripts, which can improve modularity and testability.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-testing-hive-sql_61328063.md#2025-04-09_snippet_2\n\nLANGUAGE: HiveQL\nCODE:\n```\n[SOURCE]\n```\n\n----------------------------------------\n\nTITLE: DELETE Syntax in HiveQL\nDESCRIPTION: Standard syntax for deleting records from ACID-compliant Hive tables. Available starting in Hive 0.14. Supports conditional deletion with WHERE clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-dml_27362036.md#2025-04-09_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nStandard Syntax:\nDELETE FROM tablename [WHERE expression]\n```\n\n----------------------------------------\n\nTITLE: Building Spark Distribution Without Hive (Pre-2.0.0)\nDESCRIPTION: Command to build Spark distribution without Hive dependencies for versions prior to Spark 2.0.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/44302539.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./make-distribution.sh --name \"hadoop2-without-hive\" --tgz \"-Pyarn,hadoop-provided,hadoop-2.4,parquet-provided\"\n```\n\n----------------------------------------\n\nTITLE: Listing Databases with WebHCat API using curl\nDESCRIPTION: This curl command demonstrates how to list databases in HCatalog using the WebHCat API. It includes a 'like' parameter to filter database names and specifies the user.name for authentication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getdbs_34016238.md#2025-04-09_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database?user.name=ctdean&like=n*'\n```\n\n----------------------------------------\n\nTITLE: Displaying Hive Parallel Testing Help Options\nDESCRIPTION: Command to view all available options for the Hive parallel testing script.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhive_repo/testutils/ptest/hivetest.py --help\n\n```\n\n----------------------------------------\n\nTITLE: Executing 'Create Table Like' Operation with Curl in Bash\nDESCRIPTION: This curl command demonstrates how to use the WebHCat API to create a new table 'test_table_2' based on an existing table 'test_table' in the 'default' database. It sends a PUT request with an empty JSON payload and includes the user.name parameter for authentication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-puttablelike_34016572.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X PUT -HContent-type:application/json -d {} \\\n 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/like/test_table_2?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Implementation Notes for Hive Partition Pruning\nDESCRIPTION: Additional implementation notes for the partition pruning algorithm, suggesting modifications to MapWork class and discussing potential optimizations for passing partition descriptor information between tasks.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoin-and-partition-pruning_34015666.md#2025-04-09_snippet_8\n\nLANGUAGE: pseudo\nCODE:\n```\nNOTE:  \n\n   â€¢ In Mapwork, we may need to maintain a map of Table alias to List. One choice is to introduce a new \"addPathToPartitionInfo\" method and switch current callers to use the new convenience method; this method then could maintain a Map of table alias to list of PartitionDesc.  \n\n  â€¢ Current design assumes the partition descriptor info generated by Local Task would be communicated to MapRed Task through files. This is obviously sub optimal. As an enhancement different mechanisms can be brought in to pass this info.\n```\n\n----------------------------------------\n\nTITLE: Hive Authorization Verification Process\nDESCRIPTION: Pseudocode describing the authorization verification steps in Hive, showing how user privileges, group memberships, and roles are evaluated to determine access permissions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/authdev_27362078.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nusername, \n\nlist of group names, \n\nlist of privileges and roles that has been directly granted, \n\nlist of privileges and roles that been directly granted to groups that users belongs to\n```\n\n----------------------------------------\n\nTITLE: Using EXPLAIN AUTHORIZATION to Check Query Permissions\nDESCRIPTION: This example demonstrates using EXPLAIN AUTHORIZATION to view all entities requiring authorization for a join query, including inputs, outputs, current user, and any authorization failures.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN AUTHORIZATION\n  SELECT * FROM src JOIN srcpart;\n```\n\n----------------------------------------\n\nTITLE: Intermediate File Compression Setting\nDESCRIPTION: Controls compression of intermediate files between MapReduce jobs. Uses Hadoop's compression settings when enabled.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_15\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.compress.intermediate=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop MapReduce Properties for Hive\nDESCRIPTION: Essential configuration properties for Hive-Hadoop integration including map/reduce task settings and job user permissions. Addresses HADOOP-5861 limitation for map tasks and HIVE-490 for reducer determination.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_11\n\nLANGUAGE: properties\nCODE:\n```\nmapred.map.tasks\nmapred.reduce.tasks\njob.ugi\n```\n\n----------------------------------------\n\nTITLE: Timestamp Filter Query in Hive SQL\nDESCRIPTION: SELECT query with timestamp filtering to demonstrate interval extraction in Druid.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nSELECT `__time`\nFROM druid_table_1\nWHERE `__time` >= '2010-01-01 00:00:00' AND `__time` <= '2011-01-01 00:00:00'\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Querying with SMB Join Across Tables with Different Keys (Working)\nDESCRIPTION: This SQL query shows a working solution for the previous error case. By changing the order of the tables in the join, the query executes successfully despite the difference in sort columns between the tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joinoptimization_33293167.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nSELECT p.*, py.*\nFROM emp_pay_history py INNER JOIN emp_person p\nON   p.empid = py.empid\n```\n\n----------------------------------------\n\nTITLE: Error Tolerance Properties\nDESCRIPTION: Properties that configure error tolerance thresholds for HCatRecordReader when processing data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-configuration-properties_39622369.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nhcat.input.bad.record.threshold=0.0001f\nhcat.input.bad.record.min=2\n```\n\n----------------------------------------\n\nTITLE: Installing Python HiveServer2 Client\nDESCRIPTION: Installation command for the pyhs2 Python client driver using pip package manager.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hiveserver2_30758712.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install pyhs2\n```\n\n----------------------------------------\n\nTITLE: Multiple Intervals Query in Hive SQL\nDESCRIPTION: Query demonstrating multiple time interval filtering using BETWEEN clauses.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nSELECT `__time`\nFROM druid_table_1\nWHERE (`__time` BETWEEN '2010-01-01 00:00:00' AND '2011-01-01 00:00:00')\n    OR (`__time` BETWEEN '2012-01-01 00:00:00' AND '2013-01-01 00:00:00')\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Executing GET Request for Hadoop Version in WebHCat API\nDESCRIPTION: This curl command demonstrates how to make a GET request to the WebHCat API endpoint for retrieving the Hadoop version. It includes the required user.name parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-versionhadoop_44303410.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/version/hadoop?user.name=ekoifman'\n```\n\n----------------------------------------\n\nTITLE: Error JSON Response for HCatalog Table Rename\nDESCRIPTION: This JSON object represents an error response from the WebHCat API when attempting to rename a non-existent table. It includes an error message, error code, and details about the requested operation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-posttable_34016548.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": \"Table test_table does not exist\",\n  \"errorCode\": 404,\n  \"database\": \"default\",\n  \"table\": \"test_table_2\"\n}\n```\n\n----------------------------------------\n\nTITLE: Listing JARs in Hive's Classpath\nDESCRIPTION: This Hive command lists all JAR files that have been added to the classpath in the current session.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveplugins_27362098.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nlist jars;\n```\n\n----------------------------------------\n\nTITLE: Securing JDBC Password with Hadoop Credential Store\nDESCRIPTION: Demonstrates how to store JDBC passwords securely using Hadoop Credential Store, creating a keystore file on HDFS with multiple password entries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nhadoop credential create host1.password -provider jceks://hdfs/user/foo/test.jceks -v passwd1\nhadoop credential create host2.password -provider jceks://hdfs/user/foo/test.jceks -v passwd2\n```\n\n----------------------------------------\n\nTITLE: Sample Output for Complex View Dependencies\nDESCRIPTION: This JSON output demonstrates a complex dependency hierarchy with multiple views (V1, V2, V4) and their relationships to underlying tables (src, srcpart), including multi-parent relationships.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\"input_partitions\":[{\"partitionParents\":\"[default@v2]\",\"partitionName\":\"default<at:var at:name=\"srcpart\" />ds=2008-04-08/hr=11\"},{\"partitionParents\":\"[default@v2]\",\"partitionName\":\"default<at:var at:name=\"srcpart\" />ds=2008-04-08/hr=12\"},{\"partitionParents\":\"[default@v2]\",\"partitionName\":\"default<at:var at:name=\"srcpart\" />ds=2008-04-09/hr=11\"},{\"partitionParents\":\"[default@v2]\",\"partitionName\":\"default<at:var at:name=\"srcpart\" />ds=2008-04-09/hr=12\"}],\"input_tables\":[{\"tablename\":\"default@v4\",\"tabletype\":\"VIRTUAL_VIEW\"},{\"tablename\":\"default@v2\",\"tabletype\":\"VIRTUAL_VIEW\",\"tableParents\":\"[default@v4]\"},{\"tablename\":\"default@v1\",\"tabletype\":\"VIRTUAL_VIEW\",\"tableParents\":\"[default@v4]\"},{\"tablename\":\"default@src\",\"tabletype\":\"MANAGED_TABLE\",\"tableParents\":\"[default@v4, default@v1]\"},{\"tablename\":\"default@srcpart\",\"tabletype\":\"MANAGED_TABLE\",\"tableParents\":\"[default@v2]\"}]}\n```\n\n----------------------------------------\n\nTITLE: Adding Columns to Hive Table\nDESCRIPTION: Alters an existing table by adding new columns with optional comments and default values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_16\n\nLANGUAGE: HiveQL\nCODE:\n```\nALTER TABLE tab1 ADD COLUMNS (c1 INT COMMENT 'a new int column', c2 STRING DEFAULT 'def val');\n```\n\n----------------------------------------\n\nTITLE: Configuring unixODBC Build\nDESCRIPTION: Command to configure the unixODBC build process, disabling GUI and specifying installation directory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ ./configure --enable-gui=no --prefix=<unixODBC_INSTALL_DIR>\n```\n\n----------------------------------------\n\nTITLE: Creating Avro-backed Hive Table with Simplified Syntax (Hive 0.14+)\nDESCRIPTION: Example showing the simplified table creation syntax available in Hive 0.14 and later using 'STORED AS AVRO'. This approach automatically creates the appropriate Avro schema from the Hive table schema.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE kst (\n    string1 string,\n    string2 string,\n    int1 int,\n    boolean1 boolean,\n    long1 bigint,\n    float1 float,\n    double1 double,\n    inner_record1 struct<int_in_inner_record1:int,string_in_inner_record1:string>,\n    enum1 string,\n    array1 array<string>,\n    map1 map<string,string>,\n    union1 uniontype<float,boolean,string>,\n    fixed1 binary,\n    null1 void,\n    unionnullint int,\n    bytes1 binary)\n  PARTITIONED BY (ds string)\n  STORED AS AVRO;\n```\n\n----------------------------------------\n\nTITLE: LLAP Status Command Options in HiveServer2\nDESCRIPTION: Detailed options for the LLAP status command, including parameters for app timeout, refresh interval, watch mode, and output configurations. These options allow for flexible monitoring of LLAP daemon status.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/llap_62689557.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n -f,--findAppTimeout <findAppTimeout>                 Amount of time(s) that the tool will sleep to wait for the YARN application to start. negative values=wait\n                                                      forever, 0=Do not wait. default=20s\n -H,--help                                            Print help information\n    --hiveconf <property=value>                       Use value for given property. Overridden by explicit parameters\n -i,--refreshInterval <refreshInterval>               Amount of time in seconds to wait until subsequent status checks in watch mode. Valid only for watch mode.\n                                                      (Default 1s)\n -n,--name <name>                                     LLAP cluster name\n -o,--outputFile <outputFile>                         File to which output should be written (Default stdout)\n -r,--runningNodesThreshold <runningNodesThreshold>   When watch mode is enabled (-w), wait until the specified threshold of nodes are running (Default 1.0\n                                                      which means 100% nodes are running)\n -t,--watchTimeout <watchTimeout>                     Exit watch mode if the desired state is not attained until the specified timeout. (Default 300s)\n -w,--watch                                           Watch mode waits until all LLAP daemons are running or subset of the nodes are running (threshold can be\n                                                      specified via -r option) (Default wait until all nodes are running)\n```\n\n----------------------------------------\n\nTITLE: Updating TBL_TYPE column in Hive Metastore TBLS table\nDESCRIPTION: This SQL script updates the TBL_TYPE column in the TBLS table of Hive's metastore database. It sets the value to 'MANAGED_TABLE' for regular tables (where VIEW_ORIGINAL_TEXT is NULL and no EXTERNAL parameter exists) and 'EXTERNAL_TABLE' for external tables (where the EXTERNAL parameter is set to TRUE).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/viewdev_27362067.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nUPDATE TBLS SET TBL_TYPE='MANAGED_TABLE'\nWHERE VIEW_ORIGINAL_TEXT IS NULL\nAND NOT EXISTS(\n    SELECT * FROM TABLE_PARAMS \n    WHERE TABLE_PARAMS.TBL_ID=TBLS.TBL_ID\n    AND PARAM_KEY='EXTERNAL'\n    AND PARAM_VALUE='TRUE'\n);\nUPDATE TBLS SET TBL_TYPE='EXTERNAL_TABLE'\nWHERE EXISTS(\n    SELECT * FROM TABLE_PARAMS \n    WHERE TABLE_PARAMS.TBL_ID=TBLS.TBL_ID\n    AND PARAM_KEY='EXTERNAL'\n    AND PARAM_VALUE='TRUE'\n);\n```\n\n----------------------------------------\n\nTITLE: Simple Table Description Response\nDESCRIPTION: Sample JSON response from the WebHCat API showing basic table information including column names, types and comments.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-gettable_34016519.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"columns\": [\n   {\n     \"name\": \"id\",\n     \"type\": \"bigint\"\n   },\n   {\n     \"name\": \"user\",\n     \"comment\": \"The user name\",\n     \"type\": \"string\"\n   },\n   {\n     \"name\": \"my_p\",\n     \"type\": \"string\"\n   },\n   {\n     \"name\": \"my_q\",\n     \"type\": \"string\"\n   }\n ],\n \"database\": \"default\",\n \"table\": \"my_table\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Group By Syntax in HiveQL\nDESCRIPTION: Specifies the syntax for GROUP BY clauses in Hive SQL, including the structure for groupByClause, groupByExpression, and groupByQuery.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\ngroupByClause: GROUP BY groupByExpression (, groupByExpression)*\n\ngroupByExpression: expression\n\ngroupByQuery: SELECT expression (, expression)* FROM src groupByClause?\n```\n\n----------------------------------------\n\nTITLE: Running a Hive Script from Hadoop Filesystem\nDESCRIPTION: Example of executing a Hive script file stored on a Hadoop-compatible filesystem.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/replacing-the-implementation-of-hive-cli-using-beeline_61311909.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$HIVE_HOME/bin/hive -f hdfs://<namenode>:<port>/hive-script.sql\n```\n\n----------------------------------------\n\nTITLE: Starting HiveServer2 using the dedicated script\nDESCRIPTION: Command to start HiveServer2 using the dedicated shell script in the Hive bin directory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hiveserver2_30758712.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$HIVE_HOME/bin/hiveserver2\n```\n\n----------------------------------------\n\nTITLE: Delete Database API Request - cURL Example\nDESCRIPTION: Example cURL command to delete a database named 'newdb' via the WebHCat REST API. The request requires user authentication through the user.name parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletedb_34016281.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X DELETE \"http://localhost:50111/templeton/v1/ddl/database/newdb?user.name=ctdean\"\n```\n\n----------------------------------------\n\nTITLE: Creating HQL Table for MovieLens User Ratings Data\nDESCRIPTION: Creates a Hive table called 'u_data' with tab-delimited text file format to store MovieLens user ratings data with columns for userid, movieid, rating, and unixtime.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_21\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE u_data (\n  userid INT,\n  movieid INT,\n  rating INT,\n  unixtime STRING)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Extraction\nDESCRIPTION: Extracts substring from text using regex pattern matching. Returns specific captured group based on index parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\nregexp_extract(string subject, string pattern, int index)\n```\n\n----------------------------------------\n\nTITLE: Creating Skewed Table in Hive SQL\nDESCRIPTION: Example of creating a skewed table with partitioning in Hive, demonstrating the skewed by clause for error values 'a' and 'b'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\ncreate table t1 (x string) skewed by (error) on ('a', 'b') partitioned by dt location '/user/hive/warehouse/t1';\n```\n\n----------------------------------------\n\nTITLE: Creating Remote Database in Hive\nDESCRIPTION: Creates a remote database named 'hiveserver_remote' in Hive using the previously defined 'hiveserver_connector'. This maps the remote 'default' database to the local 'hiveserver_remote' database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/data-connector-for-hive-and-hive-like-engines_288885794.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE REMOTE DATABASE hiveserver_remote USING hiveserver_connector   \n WITH DBPROPERTIES (\"connector.remoteDbName\"=\"default\");\n```\n\n----------------------------------------\n\nTITLE: HiveServer2 Configuration Property Definition\nDESCRIPTION: Example of property definition showing property name, default value and version information in markdown format\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_47\n\nLANGUAGE: markdown\nCODE:\n```\n* Default Value: `true`\n* Added In: Hive 0.13.0\n```\n\n----------------------------------------\n\nTITLE: Assumption about MapRedTask and MapRedLocalTask in Hive\nDESCRIPTION: An important assumption in the algorithm that different MapRedTask instances containing MapJoin Operators will result in different MapRedLocalTask instances, even if they share the same small tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoin-and-partition-pruning_34015666.md#2025-04-09_snippet_5\n\nLANGUAGE: pseudo\nCODE:\n```\nAssumption:\n\n  Two different MapRedTask (that contains MapJoin Operators) would result in two different MapRedLocalTask even if they share the same set of small tables.\n```\n\n----------------------------------------\n\nTITLE: Updating NameNode Location with Hive MetaTool in Shell\nDESCRIPTION: This command uses the metatool to update the NameNode location in the Hive metastore. It demonstrates the use of updateLocation, tablePropKey, and serdePropKey options to change the NameNode from hdfs://namenode2:8020 to hdfs://localhost:9000.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-metatool_55156221.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./hive --service metatool -updateLocation hdfs://localhost:9000 hdfs://namenode2:8020 -tablePropKey avro.schema.url -serdePropKey avro.schema.url\n```\n\n----------------------------------------\n\nTITLE: JSON Response for WebHCat List Databases API\nDESCRIPTION: This JSON output shows the response format for the WebHCat List Databases API. It contains an array of database names that match the specified criteria.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getdbs_34016238.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"databases\": [\n   \"newdb\",\n   \"newdb2\"\n ]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying a Byte Length Sample in HiveQL\nDESCRIPTION: This example demonstrates how to select all columns from a 100M byte sample of the 'source' table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT *\nFROM source TABLESAMPLE(100M) s;\n```\n\n----------------------------------------\n\nTITLE: Ideal Simplified HBase Bulk Load Syntax in Hive\nDESCRIPTION: Example of the simplified bulk load workflow that Hive aims to support in the future. This demonstrates creating an HBase table, enabling bulk load mode, and inserting data in one streamlined process.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE new_hbase_table(rowkey string, x int, y int) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf:x,cf:y\");\n\nSET hive.hbase.bulk=true;\n\nINSERT OVERWRITE TABLE new_hbase_table\nSELECT rowkey_expression, x, y FROM ...any_hive_query...;\n```\n\n----------------------------------------\n\nTITLE: Illustrating Rubber Banding Events in Hive SQL\nDESCRIPTION: SQL commands demonstrating a sequence of partition operations that cause rubber banding issues in replication. This sequence shows adding a partition, dropping it, and then re-adding it with potentially different data, which creates challenges for point-in-time replication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivereplicationv2development_66850051.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nEvent 100: ALTER TABLE tbl ADD PARTITION (p=1) SET LOCATION <location>;   \nEvent 110: ALTER TABLE tbl DROP PARTITION (p=1);  \nEvent 120: ALTER TABLE tbl ADD PARTITION (p=1) SET LOCATION <location>;\n```\n\n----------------------------------------\n\nTITLE: Implementing GroupByAge MapReduce Program in Java with HCatalog\nDESCRIPTION: A MapReduce program that reads from a table, groups records by age column, and counts occurrences. Uses HCatRecord for input/output and demonstrates HCatalog integration with MapReduce.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-inputoutput_34013776.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\npublic class GroupByAge extends Configured implements Tool {\n\n    public static class Map extends\n            Mapper<WritableComparable, HCatRecord, IntWritable, IntWritable> {\n\n        int age;\n\n        @Override\n        protected void map(\n                WritableComparable key,\n                HCatRecord value,\n                org.apache.hadoop.mapreduce.Mapper<WritableComparable, HCatRecord,\n                        IntWritable, IntWritable>.Context context)\n                throws IOException, InterruptedException {\n            age = (Integer) value.get(1);\n            context.write(new IntWritable(age), new IntWritable(1));\n        }\n    }\n\n    public static class Reduce extends Reducer<IntWritable, IntWritable,\n    WritableComparable, HCatRecord> {\n\n      @Override\n      protected void reduce(\n              IntWritable key,\n              java.lang.Iterable<IntWritable> values,\n              org.apache.hadoop.mapreduce.Reducer<IntWritable, IntWritable,\n                      WritableComparable, HCatRecord>.Context context)\n              throws IOException, InterruptedException {\n          int sum = 0;\n          Iterator<IntWritable> iter = values.iterator();\n          while (iter.hasNext()) {\n              sum++;\n              iter.next();\n          }\n          HCatRecord record = new DefaultHCatRecord(2);\n          record.set(0, key.get());\n          record.set(1, sum);\n\n          context.write(null, record);\n        }\n    }\n\n    public int run(String[] args) throws Exception {\n        Configuration conf = getConf();\n        args = new GenericOptionsParser(conf, args).getRemainingArgs();\n\n        String inputTableName = args[0];\n        String outputTableName = args[1];\n        String dbName = null;\n\n        Job job = new Job(conf, \"GroupByAge\");\n        HCatInputFormat.setInput(job, InputJobInfo.create(dbName,\n                inputTableName, null));\n        // initialize HCatOutputFormat\n\n        job.setInputFormatClass(HCatInputFormat.class);\n        job.setJarByClass(GroupByAge.class);\n        job.setMapperClass(Map.class);\n        job.setReducerClass(Reduce.class);\n        job.setMapOutputKeyClass(IntWritable.class);\n        job.setMapOutputValueClass(IntWritable.class);\n        job.setOutputKeyClass(WritableComparable.class);\n        job.setOutputValueClass(DefaultHCatRecord.class);\n        HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName,\n                outputTableName, null));\n        HCatSchema s = HCatOutputFormat.getTableSchema(job);\n        System.err.println(\"INFO: output schema explicitly set for writing:\"\n                + s);\n        HCatOutputFormat.setSchema(job, s);\n        job.setOutputFormatClass(HCatOutputFormat.class);\n        return (job.waitForCompletion(true) ? 0 : 1);\n    }\n\n    public static void main(String[] args) throws Exception {\n        int exitCode = ToolRunner.run(new GroupByAge(), args);\n        System.exit(exitCode);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC URL for Hive Cookie Replay in HTTP Mode\nDESCRIPTION: This snippet demonstrates how to configure the JDBC connection URL to enable cookie replay in HTTP mode for Hive. It includes parameters for transport mode, HTTP path, cookie authentication, and cookie name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_50\n\nLANGUAGE: sql\nCODE:\n```\njdbc:hive2://<host>:<port>/<db>?transportMode=http;httpPath=<http_endpoint>;cookieAuth=true;cookieName=<cookie_name>\n```\n\n----------------------------------------\n\nTITLE: Using JSONFILE Format for Query Results in Beeline (Hive 4.0)\nDESCRIPTION: The jsonfile output format in Beeline (from Hive 4.0) displays each row as a separate JSON object, with each line containing one complete JSON object. This format matches the expected format for tables created with JSONFILE format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\"String\":\"aaa\",\"Int\":1,\"Decimal\":3.14,\"Bool\":true,\"Null\":null,\"Binary\":\"SGVsbG8sIFdvcmxkIQ\"}\n{\"String\":\"bbb\",\"Int\":2,\"Decimal\":2.718,\"Bool\":false,\"Null\":null,\"Binary\":\"RWFzdGVyCgllZ2cu\"}\n```\n\n----------------------------------------\n\nTITLE: Running Hive Script with Schema Parameter\nDESCRIPTION: Bash command showing how to execute a Hive script with a schema parameter passed as a variable.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhive --hiveconf schema=\"${SCHEMA}\" -f your_script_file.sql\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration Properties\nDESCRIPTION: Configuration properties that control storage behavior in HCatalog, including external location specification and dynamic partitioning patterns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-configuration-properties_39622369.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nhcat.pig.storer.external.location=not set\nhcat.dynamic.partitioning.custom.pattern=not set\nhcat.append.limit=not set\nhcat.input.ignore.invalid.path=false\n```\n\n----------------------------------------\n\nTITLE: Case J2: Demonstrating Join Predicate on Null Supplying Table in Hive\nDESCRIPTION: This example shows a case where a join predicate (s2.key > '2') is applied to a null supplying table (s2) in a left outer join. The execution plan shows the predicate is pushed down and applied as a filter operator before the join.\nSOURCE: https://github.com/apache/hive-site/blob/main/themes/hive/static/attachments/27362075/35193191.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nexplain \nselect s1.key, s2.key \nfrom src s1 left join src s2 on s2.key > '2';\n```\n\n----------------------------------------\n\nTITLE: Initializing Hive Streaming Connection in Non-secure Mode\nDESCRIPTION: This Java code demonstrates how to set up a Hive streaming connection in non-secure mode. It includes creating a HiveEndPoint and establishing a StreamingConnection.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest_40509746.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nString dbName = \"testing\";\nString tblName = \"alerts\";\nArrayList<String> partitionVals = new ArrayList<String>(2);\npartitionVals.add(\"Asia\");\npartitionVals.add(\"India\");\nString serdeClass = \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\";\n\nHiveEndPoint hiveEP = new HiveEndPoint(\"thrift://x.y.com:9083\", dbName, tblName, partitionVals);\n\n.. spin up threads ..\n\n//-------   Thread 1  -------//\nStreamingConnection connection = hiveEP.newConnection(true);\nDelimitedInputWriter writer =\n                     new DelimitedInputWriter(fieldNames,\",\", hiveEP);\nTransactionBatch txnBatch = connection.fetchTransactionBatch(10, writer);\n```\n\n----------------------------------------\n\nTITLE: Initializing Hive Metastore Schema in TiDB\nDESCRIPTION: Command to initialize the Hive metastore schema in TiDB using the schematool utility, which creates all necessary tables for storing Hive metadata.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/using-tidb-as-the-hive-metastore-database_158872426.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n${HIVE_HOME}/bin/schematool -dbType mysql -initSchema --verbose\n```\n\n----------------------------------------\n\nTITLE: Enabling Hive Test Mode in XML\nDESCRIPTION: Controls whether Hive is running in test mode. When enabled, it turns on sampling and prefixes the output tablename.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_117\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.test.mode</name>\n  <value>false</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Tagging and Pushing Hive Release in Git\nDESCRIPTION: Commands for tagging the release, pushing it to origin, and deleting the release candidate tag. This is done from the release branch.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ngit tag -s rel/release-X.Y.Z -m \"HiveX.Y.Z release.\"\ngit push origin rel/release-X.Y.Z\ngit tag -d release-X.Y.Z-rcR\ngit push origin :release-X.Y.Z-rcR\n```\n\n----------------------------------------\n\nTITLE: Hive EXPORT and IMPORT Command Syntax\nDESCRIPTION: The syntax for the EXPORT command that exports table data and metadata to a specified location, and the IMPORT command that imports this data into a new or existing table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-importexport_27837968.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nEXPORT TABLE tablename [PARTITION (part_column=\"value\"[, ...])] \n  TO 'export_target_path' [ FOR replication('eventid') ]\n```\n\nLANGUAGE: sql\nCODE:\n```\nIMPORT [[EXTERNAL] TABLE new_or_original_tablename [PARTITION (part_column=\"value\"[, ...])]] \n  FROM 'source_path' \n  [LOCATION 'import_target_path']\n\n```\n\n----------------------------------------\n\nTITLE: Numeric XPath Evaluation in Hive SQL\nDESCRIPTION: Demonstrates using xpath_int UDF to perform numeric operations on XML data using XPath expressions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-xpathudf_27362051.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT xpath_int ('<a><b class=\"odd\">1</b><b class=\"even\">2</b><b class=\"odd\">4</b><c>8</c></a>', 'sum(a/*)') FROM src LIMIT 1 ;\n```\n\n----------------------------------------\n\nTITLE: Querying Java Regular Expression Syntax in Hive SQL\nDESCRIPTION: Example of using the regexp_replace function in Hive SQL to replace substrings matching a Java regular expression pattern. This function takes three string arguments: the input string, the regex pattern, and the replacement string.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nregexp_replace('foobar', 'oo|ar', '')\n```\n\n----------------------------------------\n\nTITLE: Sample Output Showing View and Table Dependencies\nDESCRIPTION: This JSON output from EXPLAIN DEPENDENCY shows the dependency relationship between a view (V1) and its underlying table (src), including table types and parent-child relationships.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\"input_partitions\":[],\"input_tables\":[{\"tablename\":\"default@v1\",\"tabletype\":\"VIRTUAL_VIEW\"},{\"tablename\":\"default@src\",\"tabletype\":\"MANAGED_TABLE\",\"tableParents\":\"[default@v1]\"}]}\n```\n\n----------------------------------------\n\nTITLE: JSON Response Format for Table Properties\nDESCRIPTION: Example JSON response from the WebHCat API when retrieving table properties. The response includes various metadata properties such as comments, colors, and driver configurations for the table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getproperties_34016995.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"properties\": {\n   \"fruit\": \"apple\",\n   \"last_modified_by\": \"ctdean\",\n   \"hcat.osd\": \"org.apache.hcatalog.rcfile.RCFileOutputDriver\",\n   \"color\": \"blue\",\n   \"last_modified_time\": \"1331620706\",\n   \"hcat.isd\": \"org.apache.hcatalog.rcfile.RCFileInputDriver\",\n   \"transient_lastDdlTime\": \"1331620706\",\n   \"comment\": \"Best table made today\",\n   \"country\": \"Albania\"\n },\n \"table\": \"test_table\",\n \"database\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Metastore Server with Custom Port\nDESCRIPTION: Command to start a Thrift metastore server with a custom port number using the Hive CLI. This allows running multiple metastore instances or avoiding port conflicts.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-metastore-administration_27362076.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhive --service metastore -p <port_num>\n```\n\n----------------------------------------\n\nTITLE: Including Current Database in Hive CLI Prompt in XML\nDESCRIPTION: Determines whether to display the current database name in the Hive Command Line Interface (CLI) prompt.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_103\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.cli.print.current.db</name>\n  <value>false</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Displaying View Information with DESCRIBE FORMATTED in Hive SQL\nDESCRIPTION: An example of how DESCRIBE FORMATTED shows view information in Hive, including the original and expanded text of the view query. This demonstrates that views are not opaque as claimed in the discussion.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/links_27847416.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n# View Information\t \t \nView Original Text: \tSELECT value FROM src WHERE key=86\t \nView Expanded Text: \tSELECT `src`.`value` FROM `src` WHERE `src`.`key`=86\t \n```\n\n----------------------------------------\n\nTITLE: Querying Table Description - Simple Format\nDESCRIPTION: Example curl command for retrieving a simple table description through the WebHCat API. Uses the default format that returns basic column information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-gettable_34016519.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Hive Interactive Shell Example\nDESCRIPTION: Sample usage of Hive interactive shell commands including configuration settings and query execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-cli_27362033.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhive> set mapred.reduce.tasks=32;\nhive> set;\nhive> select a.* from tab1;\nhive> !ls;\nhive> dfs -ls;\n```\n\n----------------------------------------\n\nTITLE: Extracting Substring with Specific Length in Hive\nDESCRIPTION: Returns the substring of A starting from the start position with a specific length. Takes a string or binary input, an integer start position, and an integer length.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nsubstr(string|binary A, int start, int len)\nsubstring(string|binary A, int start, int len)\n```\n\n----------------------------------------\n\nTITLE: Static Partitioning in Pig with Single Partition\nDESCRIPTION: Example of storing data to a single partition in Pig using HCatStorer with explicit partition key-value pairs. This approach requires that all data belongs to the specified partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-dynamicpartitions_34014006.md#2025-04-09_snippet_2\n\nLANGUAGE: pig\nCODE:\n```\nstore A into 'mytable' using HCatStorer(\"a=1, b=1\");\n```\n\n----------------------------------------\n\nTITLE: HCatInputFormat API in Java for Apache Hive\nDESCRIPTION: API methods for HCatInputFormat, including setInput for specifying input table, setOutputSchema for defining output fields, and getTableSchema for retrieving table schema.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-inputoutput_34013776.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n  /**\n   * Set the input to use for the Job. This queries the metadata server with\n   * the specified partition predicates, gets the matching partitions, puts\n   * the information in the conf object. The inputInfo object is updated with\n   * information needed in the client context\n   * @param job the job object\n   * @param inputJobInfo the input info for table to read\n   * @throws IOException the exception in communicating with the metadata server\n   */\n  public static void setInput(Job job,\n      InputJobInfo inputJobInfo) throws IOException;\n\n  /**\n   * Set the schema for the HCatRecord data returned by HCatInputFormat.\n   * @param job the job object\n   * @param hcatSchema the schema to use as the consolidated schema\n   */\n  public static void setOutputSchema(Job job,HCatSchema hcatSchema)\n    throws IOException;\n\n  /**\n   * Get the HCatTable schema for the table specified in the HCatInputFormat.setInput\n   * call on the specified job context. This information is available only after\n   * HCatInputFormat.setInput has been called for a JobContext.\n   * @param context the context\n   * @return the table schema\n   * @throws IOException if HCatInputFormat.setInput has not been called\n   *                     for the current context\n   */\n  public static HCatSchema getTableSchema(JobContext context)\n    throws IOException;\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Execution Engine to Spark\nDESCRIPTION: Basic configuration to enable Spark as the execution engine for Hive queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/44302539.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nset hive.execution.engine=spark;\n```\n\n----------------------------------------\n\nTITLE: Example of Show Principals in Hive\nDESCRIPTION: Example showing how to display all principals (users and roles) belonging to a specific role.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n0: jdbc:hive2://localhost:10000> SHOW PRINCIPALS role1;\n+-----------------+-----------------+---------------+----------+---------------+----------------+\n| principal_name  | principal_type  | grant_option  | grantor  | grantor_type  |   grant_time   |\n+-----------------+-----------------+---------------+----------+---------------+----------------+\n| role2           | ROLE            | false         | uadmin   | USER          | 1398285926000  |\n| role3           | ROLE            | true          | uadmin   | USER          | 1398285946000  |\n| user1           | USER            | false         | uadmin   | USER          | 1398285977000  |\n+-----------------+-----------------+---------------+----------+---------------+----------------+\n```\n\n----------------------------------------\n\nTITLE: Error Log for Schema Version Incompatibility\nDESCRIPTION: Log output showing the specific error message when Hive cannot find version information in the metastore database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n...\nCaused by: MetaException(message:Version information not found in metastore. )\n...\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Array Access in Hive SQL\nDESCRIPTION: Shows how to access elements in an array using index notation. The example demonstrates accessing the first and second elements of an array containing string values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nA[0] -- returns 'foo'\nA[1] -- returns 'bar'\n```\n\n----------------------------------------\n\nTITLE: Data Promotion Properties\nDESCRIPTION: Properties that control data type promotion behavior when reading data through HCatalog.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-configuration-properties_39622369.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\nhcat.data.convert.boolean.to.integer=false\nhcat.data.tiny.small.int.promotion=false\n```\n\n----------------------------------------\n\nTITLE: Vectorized Execution Configuration Properties\nDESCRIPTION: Settings controlling vectorized query execution in Hive, including execution modes, MapJoin optimizations, and GroupBy parameters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_36\n\nLANGUAGE: properties\nCODE:\n```\nhive.vectorized.execution.enabled=false\nhive.vectorized.execution.reduce.enabled=true\nhive.vectorized.execution.reduce.groupby.enabled=true\nhive.vectorized.execution.reducesink.new.enabled=true\nhive.vectorized.execution.mapjoin.native.enabled=true\nhive.vectorized.execution.mapjoin.native.multikey.only.enabled=false\nhive.vectorized.execution.mapjoin.minmax.enabled=false\nhive.vectorized.execution.mapjoin.overflow.repeated.threshold=-1\nhive.vectorized.execution.mapjoin.native.fast.hashtable.enabled=false\nhive.vectorized.groupby.checkinterval=100000\nhive.vectorized.groupby.maxentries=1000000\nhive.vectorized.use.vectorized.input.format=true\nhive.vectorized.use.vector.serde.deserialize=false\nhive.vectorized.use.row.serde.deserialize=false\n```\n\n----------------------------------------\n\nTITLE: Using Python Transformation in HQL with TRANSFORM\nDESCRIPTION: HQL commands that create a new table 'u_data_new' with weekday information, add the mapper script, transform the data using the Python script, and run a query to count records by weekday. This demonstrates Hive's ability to use external scripts for data processing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_28\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE u_data_new (\n  userid INT,\n  movieid INT,\n  rating INT,\n  weekday INT)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t';\n\nadd FILE weekday_mapper.py;\n\nINSERT OVERWRITE TABLE u_data_new\nSELECT\n  TRANSFORM (userid, movieid, rating, unixtime)\n  USING 'python weekday_mapper.py'\n  AS (userid, movieid, rating, weekday)\nFROM u_data;\n\nSELECT weekday, COUNT(*)\nFROM u_data_new\nGROUP BY weekday;\n```\n\n----------------------------------------\n\nTITLE: Error Response - WebHCat Database Not Found\nDESCRIPTION: Example error JSON response when attempting to query tables from a non-existent database\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-gettables_34016290.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"errorDetail\": \"\n    org.apache.hadoop.hive.ql.metadata.HiveException: ERROR: The database defaultsd does not exist.\n        at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3122)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:224)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\n        at org.apache.hcatalog.cli.HCatDriver.run(HCatDriver.java:42)\n        at org.apache.hcatalog.cli.HCatCli.processCmd(HCatCli.java:247)\n        at org.apache.hcatalog.cli.HCatCli.processLine(HCatCli.java:203)\n        at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:162)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n    \",\n  \"error\": \"FAILED: Error in metadata: ERROR: The database defaultsd does not exist.\",\n  \"errorCode\": 500,\n  \"database\": \"defaultsd\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Default ZooKeeper Partition Name for Lock Manager\nDESCRIPTION: Specifies the default partition name used by the ZooKeeperHiveLockManager when it is set as the Hive lock manager. This is used for creating locks in ZooKeeper.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_91\n\nLANGUAGE: plaintext\nCODE:\n```\n__HIVE_DEFAULT_ZOOKEEPER_PARTITION__\n```\n\n----------------------------------------\n\nTITLE: Error Message for Missing Partition Predicate in Hive\nDESCRIPTION: This error message is displayed when running the join query in strict mode before Hive 0.8.0, indicating that no partition predicate was found for the invites2 table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/transitivity-on-predicate-pushdown_27823388.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nError in semantic analysis: No Partition Predicate Found for Alias \"invites2\" Table \"invites2\"\n```\n\n----------------------------------------\n\nTITLE: Overwriting an Iceberg Table with Data from Another Table\nDESCRIPTION: Demonstrates using INSERT OVERWRITE to replace all data in an Iceberg table with data selected from another table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nINSERT OVERWRITE TBL_ICE SELECT * FROM TABLE1;\n```\n\n----------------------------------------\n\nTITLE: Selecting Hive JDBC Driver Class in SQuirrel SQL Client\nDESCRIPTION: This code snippet shows the fully qualified class name of the Hive JDBC driver to be selected when configuring the connection in SQuirrel SQL Client.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_49\n\nLANGUAGE: java\nCODE:\n```\n   org.apache.hive.jdbc.HiveDriver\n```\n\n----------------------------------------\n\nTITLE: Showing Table and Database Locks in HQL\nDESCRIPTION: Displays the locks on a table, partition, or database in Hive. Shows detailed lock information including state, type, transaction ID, and user information when Hive transactions are enabled.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_87\n\nLANGUAGE: hql\nCODE:\n```\nSHOW LOCKS <table_name>;\nSHOW LOCKS <table_name> EXTENDED;\nSHOW LOCKS <table_name> PARTITION (<partition_spec>);\nSHOW LOCKS <table_name> PARTITION (<partition_spec>) EXTENDED;\nSHOW LOCKS (DATABASE|SCHEMA) database_name;     -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)\n```\n\n----------------------------------------\n\nTITLE: Generating Hive Javadocs with Maven\nDESCRIPTION: Maven command to generate javadocs for the Hive release, including aggregate javadocs and Iceberg profile.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install javadoc:javadoc javadoc:aggregate -DskipTests -Pjavadoc,iceberg\n```\n\n----------------------------------------\n\nTITLE: Basic Rebalance Compaction in Hive SQL\nDESCRIPTION: Executes a basic rebalance compaction operation on a table to redistribute data evenly among implicit buckets. This operation requires an exclusive write lock on the table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/rebalance-compaction_240884502.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name COMPACT 'REBALANCE';\n```\n\n----------------------------------------\n\nTITLE: Starting Hive CLI Client for Testing\nDESCRIPTION: Command to start the Hive command-line interface (CLI) client for testing the Hive installation with TiDB as the metastore database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/using-tidb-as-the-hive-metastore-database_158872426.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n${HIVE_HOME}/bin/hive\n```\n\n----------------------------------------\n\nTITLE: Setting Default HiveServer2 Authorization Task Factory Implementation\nDESCRIPTION: Specifies the default implementation class for handling authorization DDL tasks in HiveServer2. This class implements the HiveAuthorizationTaskFactory interface.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_88\n\nLANGUAGE: xml\nCODE:\n```\norg.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl\n```\n\n----------------------------------------\n\nTITLE: Creating a Column in HCatalog Table Using curl\nDESCRIPTION: A curl command example that demonstrates how to create a new column named 'brand' of type 'string' with a comment in a HCatalog table via the WebHCat REST API.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-putcolumn_34016987.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X PUT -HContent-type:application/json \\\n       -d '{\"type\": \"string\", \"comment\": \"The brand name\"}' \\\n       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/column/brand?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Preparing ReaderContext in Java\nDESCRIPTION: This code demonstrates how to obtain a ReaderContext from the HCatReader instance. This context is created on the master node and then serialized and sent to all slave nodes to perform parallel reading operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-readerwriter_34013921.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nReaderContext cntxt = reader.prepareRead();\n```\n\n----------------------------------------\n\nTITLE: Dropping Temporary Macros in HiveQL\nDESCRIPTION: Syntax for removing a temporary macro, with an optional IF EXISTS clause to prevent errors if the macro doesn't exist.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_60\n\nLANGUAGE: hql\nCODE:\n```\nDROP TEMPORARY MACRO [IF EXISTS] macro_name;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Metastore Authentication Manager\nDESCRIPTION: Property to set the authenticator manager class for metastore authentication. The class must implement the HiveAuthenticationProvider interface.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_86\n\nLANGUAGE: properties\nCODE:\n```\nhive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator\n```\n\n----------------------------------------\n\nTITLE: JSON Response from Column Creation Request\nDESCRIPTION: The JSON response returned after successfully creating a column in a HCatalog table, showing the name of the column created along with the table and database where it was added.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-putcolumn_34016987.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"column\": \"brand\",\n \"table\": \"test_table\",\n \"database\": \"default\"\n }\n}\n```\n\n----------------------------------------\n\nTITLE: Describe Table Command Syntax in Hive 2.0+\nDESCRIPTION: New syntax for the DESCRIBE command in Hive 2.0 and later. The syntax is backward incompatible, requiring space-separated table and column names instead of dot-separated, and placing partition specification between table name and column name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_100\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE [EXTENDED | FORMATTED]\n    [db_name.]table_name [PARTITION partition_spec] [col_name ( [.field_name] | [.'$elem$'] | [.'$key$'] | [.'$value$'] )* ];\n```\n\n----------------------------------------\n\nTITLE: Using the isnull() function in Hive SQL\nDESCRIPTION: Checks if a value is NULL. Returns true if the argument is NULL and false otherwise.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_70\n\nLANGUAGE: SQL\nCODE:\n```\nisnull(a)\n```\n\n----------------------------------------\n\nTITLE: Hive Schema Tool Command Usage Syntax\nDESCRIPTION: Complete usage syntax for the schematool command, showing all available options for initializing, upgrading, and managing Hive metastore schemas and catalogs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ usage: schemaTool\n -alterCatalog <arg>                Alter a catalog, requires\n                                    --catalogLocation and/or\n                                    --catalogDescription parameter as well\n -catalogDescription <arg>          Description of new catalog\n -catalogLocation <arg>             Location of new catalog, required when\n                                    adding a catalog\n -createCatalog <arg>               Create a catalog, requires\n                                    --catalogLocation parameter as well\n -createLogsTable <arg>             Create table for Hive\n                                    warehouse/compute logs\n -createUser                        Create the Hive user, set hiveUser to\n                                    the db admin user and the hive\n                                    password to the db admin password with\n                                    this\n -dbOpts <databaseOpts>             Backend DB specific options\n -dbType <databaseType>             Metastore database type\n -driver <driver>                   driver name for connection\n -dropAllDatabases                  Drop all Hive databases (with\n                                    CASCADE). This will remove all managed\n                                    data!\n -dryRun                            list SQL scripts (no execute)\n -fromCatalog <arg>                 Catalog a moving database or table is\n                                    coming from.  This is required if you\n                                    are moving a database or table.\n -fromDatabase <arg>                Database a moving table is coming\n                                    from.  This is required if you are\n                                    moving a table.\n -help                              print this message\n -hiveDb <arg>                      Hive database (for use with\n                                    createUser)\n -hivePassword <arg>                Hive password (for use with\n                                    createUser)\n -hiveUser <arg>                    Hive user (for use with createUser)\n -ifNotExists                       If passed then it is not an error to\n                                    create an existing catalog\n -info                              Show config and schema details\n -initOrUpgradeSchema               Initialize or upgrade schema to latest\n                                    version\n -initSchema                        Schema initialization\n -initSchemaTo <initTo>             Schema initialization to a version\n -mergeCatalog <arg>                Merge databases from a catalog into\n                                    other, Argument is the source catalog\n                                    name Requires --toCatalog to indicate\n                                    the destination catalog\n -metaDbType <metaDatabaseType>     Used only if upgrading the system\n                                    catalog for hive\n -moveDatabase <arg>                Move a database between catalogs.\n                                    Argument is the database name.\n                                    Requires --fromCatalog and --toCatalog\n                                    parameters as well\n -moveTable <arg>                   Move a table to a different database.\n                                    Argument is the table name. Requires\n                                    --fromCatalog, --toCatalog,\n                                    --fromDatabase, and --toDatabase\n                                    parameters as well.\n -passWord <password>               Override config file password\n -retentionPeriod <arg>             Specify logs table retention period\n -servers <serverList>              a comma-separated list of servers used\n                                    in location validation in the format\n                                    of scheme://authority (e.g.\n                                    hdfs://localhost:8000)\n -toCatalog <arg>                   Catalog a moving database or table is\n                                    going to.  This is required if you are\n                                    moving a database or table.\n -toDatabase <arg>                  Database a moving table is going to.\n                                    This is required if you are moving a\n                                    table.\n -upgradeSchema                     Schema upgrade\n -upgradeSchemaFrom <upgradeFrom>   Schema upgrade from a version\n -url <url>                         connection url to the database\n -userName <user>                   Override config file user name\n -validate                          Validate the database\n -verbose                           only print SQL statements\n -yes                               Don't ask for confirmation when using\n                                    -dropAllDatabases.  \n```\n\n----------------------------------------\n\nTITLE: Error JSON Response from WebHCat Table Creation\nDESCRIPTION: This JSON response is returned when table creation fails. It includes the SQL statement that was attempted, the error message, and execution details including stdout, stderr, and exit code.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-puttable_34016540.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"statement\": \"use default; create table test_table_c(id bigint, price float comment ...\",\n  \"error\": \"unable to create table: test_table_c\",\n  \"exec\": {\n    \"stdout\": \"\",\n    \"stderr\": \"WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated...\n        Hive history file=/tmp/ctdean/hive_job_log_ctdean_201204051335_2016086186.txt\n        SLF4J: Class path contains multiple SLF4J bindings.\n        SLF4J: Found binding in ...\n        SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n        OK\n        Time taken: 0.448 seconds\n        FAILED: Error in semantic analysis: Operation not supported. HCatalog doesn't allow Clustered By in create table.\n        \",\n    \"exitcode\": 10\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Table Information with Hive MetaTool in Shell\nDESCRIPTION: This command uses the metatool to execute a JDOQL query against the Hive metastore. It retrieves table column statistics information for all metastore backends.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-metatool_55156221.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nHIVE_CONF_DIR=/etc/hive/conf/conf.server/ hive --service metatool -executeJDOQL 'select dbName+\".\"+tableName+\"::\"+colName+\"=\"+numDVs from org.apache.hadoop.hive.metastore.model.MTableColumnStatistics'\n```\n\n----------------------------------------\n\nTITLE: Bash Script for Running Hive JDBC Client with Auto Classpath Setup\nDESCRIPTION: A complete bash script that automates the process of setting up test data, building the classpath with all required dependencies, and running the Hive JDBC client. Useful for quickly testing JDBC connectivity.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_42\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nHADOOP_HOME=/your/path/to/hadoop\nHIVE_HOME=/your/path/to/hive\n\necho -e '1\\x01foo' > /tmp/a.txt\necho -e '2\\x01bar' >> /tmp/a.txt\n\nHADOOP_CORE=$(ls $HADOOP_HOME/hadoop-core*.jar)\nCLASSPATH=.:$HIVE_HOME/conf:$(hadoop classpath)\n\nfor i in ${HIVE_HOME}/lib/*.jar ; do\n    CLASSPATH=$CLASSPATH:$i\ndone\n\njava -cp $CLASSPATH HiveJdbcClient\n```\n\n----------------------------------------\n\nTITLE: JSON Response Example for Failed DDL Command in WebHCat\nDESCRIPTION: Example JSON response from a failed WebHCat DDL command showing an empty standard output, standard error with error details about a parse error, and a non-zero exit code indicating failure.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-ddl_34015990.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"stdout\": \"\",\n  \"stderr\": \"WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated...\n            Hive history file=/tmp/ctdean/hive_job_log_ctdean_201204051246_689834593.txt\n            FAILED: Parse Error: line 1:5 Failed to recognize predicate 'tab'...\n\n            \",\n  \"exitcode\": 11\n}\n```\n\n----------------------------------------\n\nTITLE: Setting LLAP Daemon Management Port\nDESCRIPTION: Configuration for specifying the RPC port used by LLAP daemon management service.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_68\n\nLANGUAGE: properties\nCODE:\n```\nhive.llap.management.rpc.port=15004\n```\n\n----------------------------------------\n\nTITLE: Setting Compaction History Reaper Interval in Hive\nDESCRIPTION: Controls how often the process to purge historical record of compactions runs. Default value is 2 minutes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_71\n\nLANGUAGE: properties\nCODE:\n```\nhive.compactor.history.reaper.interval=2m\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Parameters for Custom Java LZO File Creation\nDESCRIPTION: Hive query parameters for Option 2, which creates text files first and then converts them to LZO using custom Java code. This disables compression during the Hive query execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lzo_33298193.md#2025-04-09_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nSET hive.exec.compress.output=false\nSET mapreduce.output.fileoutputformat.compress=false\n```\n\n----------------------------------------\n\nTITLE: Installing Hive from Tarball - Extraction\nDESCRIPTION: Command to extract the Hive tarball package into a new directory named hive-x.y.z where x.y.z represents the version number.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-installation_27362077.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ tar -xzvf hive-x.y.z.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Initializing HiveServer2 Schema with Shell Commands\nDESCRIPTION: A sequence of shell commands to download the PostgreSQL JDBC driver, start the Docker Compose services, connect to the HiveServer2 container, and initialize the Hive schema against the PostgreSQL database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/Development/quickStart.md#2025-04-09_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nmvn dependency:get -Dartifact=org.postgresql:postgresql:42.7.5\ndocker compose up -d\ndocker compose exec hiveserver2-standalone /bin/bash\n/opt/hive/bin/schematool -initSchema -dbType hive -metaDbType postgres -url jdbc:hive2://localhost:10000/default\nexit\n```\n\n----------------------------------------\n\nTITLE: Truncating Date in Hive\nDESCRIPTION: Truncates date to specified unit (MONTH/MON/MM or YEAR/YYYY/YY). Returns first day of the specified period.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_39\n\nLANGUAGE: sql\nCODE:\n```\ntrunc(string date,string format)\n```\n\n----------------------------------------\n\nTITLE: Registering Hive JDBC Driver in SQuirrel SQL Client\nDESCRIPTION: This code snippet demonstrates how to register the Hive JDBC driver in the SQuirrel SQL Client. It includes the driver name and example URL to be used when setting up the connection.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_47\n\nLANGUAGE: sql\nCODE:\n```\n   Name: Hive\n   Example URL: jdbc:hive2://localhost:10000/default\n```\n\n----------------------------------------\n\nTITLE: Implementing Counters and Metrics in Hive on Spark\nDESCRIPTION: Describes how Spark's accumulators can be used to implement counter functionality similar to MapReduce counters, with notes about potential differences in metrics between execution engines.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n### Counters and Metrics\n\nSpark has accumulators which are variables that are only \"added\" to through an associative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric value types and standard mutable collections, and programmers can add support for new types. In Hive, we may use Spark accumulators to implement Hadoop counters, but this may not be done right way.\n\nSpark publishes runtime metrics for a running job. However, it's very likely that the metrics are different from either MapReduce or Tez, not to mention the way to extract the metrics. The topic around this deserves a separate document, but this can be certainly improved upon incrementally.\n```\n\n----------------------------------------\n\nTITLE: Configuring Handshake Timeout between Hive Client and Remote Spark Driver\nDESCRIPTION: Sets the timeout for the handshake process between the Hive client and the remote Spark driver. This is checked by both processes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_57\n\nLANGUAGE: properties\nCODE:\n```\nhive.spark.client.server.connect.timeout = 90000 miliseconds\n```\n\n----------------------------------------\n\nTITLE: Invalid Multiple DISTINCT Expressions in Hive\nDESCRIPTION: Shows an example of an invalid query that attempts to use multiple DISTINCT expressions on different columns in the same aggregation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT OVERWRITE TABLE pv_gender_agg\nSELECT pv_users.gender, count(DISTINCT pv_users.userid), count(DISTINCT pv_users.ip)\nFROM pv_users\nGROUP BY pv_users.gender;\n```\n\n----------------------------------------\n\nTITLE: Creating a Clustered Hive Table with Complex Configuration\nDESCRIPTION: This curl command creates a more complex Hive table with clustering, sorting, and custom row format. It demonstrates advanced table creation options including clusteredBy configuration and custom SerDe properties.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-puttable_34016540.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X PUT -HContent-type:application/json -d '{\n  \"comment\": \"Best table made today\",\n  \"columns\": [\n    { \"name\": \"id\", \"type\": \"bigint\"},\n    { \"name\": \"price\", \"type\": \"float\", \"comment\": \"The unit price\" } ],\n  \"partitionedBy\": [\n    { \"name\": \"country\", \"type\": \"string\" } ],\n  \"clusteredBy\": {\n    \"columnNames\": [\"id\"],\n    \"sortedBy\": [\n      { \"columnName\": \"id\", \"order\": \"ASC\" } ],\n    \"numberOfBuckets\": 10 },\n  \"format\": {\n    \"storedAs\": \"rcfile\",\n    \"rowFormat\": {\n      \"fieldsTerminatedBy\": \"\\u0001\",\n      \"serde\": {\n        \"name\": \"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe\",\n        \"properties\": {\n          \"key\": \"value\" } } } }\n  } ' \\\n  'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table_c?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Setting PYTHONPATH for Local Python Modules\nDESCRIPTION: Generic export command to update the PYTHONPATH environment variable to include locally installed Python modules.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport PYTHONPATH=\"${PYTHONPATH}:${HOME}/.python_modules/lib/pythonVERSION/site-packages\"\n\n```\n\n----------------------------------------\n\nTITLE: Using the array_min() function in Hive SQL\nDESCRIPTION: Returns the minimum value in an array with elements that support ordering. Example: array_min(array(1, 3, 0, NULL)) returns 0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_59\n\nLANGUAGE: SQL\nCODE:\n```\narray_min((array(obj1, obj2, obj3...))\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Metastore Security Settings in hive-site.xml\nDESCRIPTION: This XML snippet shows the default configuration settings for Hive metastore security in hive-site.xml. It includes properties for authorization manager, authenticator manager, and pre-event listeners. Users should modify these settings to enable storage based authorization and customize security behavior.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/storage-based-authorization-in-the-metastore-server_45876440.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hive.security.metastore.authorization.manager</name>\n  <value>org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider</value>\n  <description>authorization manager class name to be used in the metastore for authorization.\n  The user defined authorization class should implement interface\n  org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.\n  </description>\n </property>\n\n<property>\n  <name>hive.security.metastore.authenticator.manager</name>\n  <value>org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator</value>\n  <description>authenticator manager class name to be used in the metastore for authentication.\n  The user defined authenticator should implement interface \n  org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.\n  </description>\n</property>\n\n<property>\n  <name>hive.metastore.pre.event.listeners</name>\n  <value> </value>\n  <description>pre-event listener classes to be loaded on the metastore side to run code\n  whenever databases, tables, and partitions are created, altered, or dropped.\n  Set to org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener\n  if metastore-side authorization is desired.\n  </description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: WebHCat Job ID List Response Format\nDESCRIPTION: Example JSON response showing the format of job IDs returned by the queue endpoint. The response contains an array of job ID strings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-jobids_34017187.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"job_201111111311_0008\",\n \"job_201111111311_0012\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Dropping Roles in Hive SQL\nDESCRIPTION: SQL commands for creating and dropping roles in Hive's authorization system.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/45876173.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE ROLE role_name\n\nDROP ROLE role_name\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Hive Streaming Connection with Static Partitioning in Java\nDESCRIPTION: This snippet demonstrates how to create a Hive streaming connection with static partition values, write records in multiple transactions, and close the connection. It uses a StrictDelimitedInputWriter and specifies static partition values for 'continent' and 'country'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest-v2_85477610.md#2025-04-09_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nString dbName = \"testing\";\nString tblName = \"alerts\";\n\n.. spin up thread 1 ..\n// static partition values\nArrayList<String> partitionVals = new ArrayList<String>(2);\npartitionVals.add(\"Asia\");\npartitionVals.add(\"India\");\n\n// create delimited record writer whose schema exactly matches table schema\nStrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder()\n                                      .withFieldDelimiter(',')\n                                      .build();\n// create and open streaming connection (default.src table has to exist already)\nStreamingConnection connection = HiveStreamingConnection.newBuilder()\n                                    .withDatabase(dbName)\n                                    .withTable(tblName)\n                                    .withStaticPartitionValues(partitionVals)\n                                    .withAgentInfo(\"example-agent-1\")\n                                    .withRecordWriter(writer)\n                                    .withHiveConf(hiveConf)\n                                    .connect();\n// begin a transaction, write records and commit 1st transaction\nconnection.beginTransaction();\nconnection.write(\"1,val1\".getBytes());\nconnection.write(\"2,val2\".getBytes());\nconnection.commitTransaction();\n// begin another transaction, write more records and commit 2nd transaction\nconnection.beginTransaction();\nconnection.write(\"3,val3\".getBytes());\nconnection.write(\"4,val4\".getBytes());\nconnection.commitTransaction();\n// close the streaming connection\nconnection.close();\n```\n\n----------------------------------------\n\nTITLE: Retrieving Column Information with curl in WebHCat\nDESCRIPTION: Example curl command to retrieve information about the 'price' column in the 'test_table' table from the 'default' database using the WebHCat API.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getcolumn_34016979.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/column/price?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Converting String to Key-Value Map in Hive\nDESCRIPTION: Splits text into key-value pairs using delimiter1 to separate text into pairs and delimiter2 to split each pair. Default delimiters are ',' for delimiter1 and ':' for delimiter2.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_30\n\nLANGUAGE: sql\nCODE:\n```\nstr_to_map(text[, delimiter1, delimiter2])\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Web Interface Properties in XML\nDESCRIPTION: XML configuration properties for HWI including host address, port, and WAR file location settings\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivewebinterface_27362110.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hive.hwi.listen.host</name>\n  <value>0.0.0.0</value>\n  <description>This is the host address the Hive Web Interface will listen on</description>\n</property>\n\n<property>\n  <name>hive.hwi.listen.port</name>\n  <value>9999</value>\n  <description>This is the port the Hive Web Interface will listen on</description>\n</property>\n\n<property>\n  <name>hive.hwi.war.file</name>\n  <value>${HIVE_HOME}/lib/hive-hwi-<version>.war</value>\n  <description>This is the WAR file with the jsp content for Hive Web Interface</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Hugo Front Matter Configuration\nDESCRIPTION: YAML front matter configuration for a Hugo static site page, specifying the title, date, and draft status.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/Development/gettingStarted.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"Getting Started\"\ndate: 2023-01-10T12:35:11+05:30\ndraft: false\n---\n```\n\n----------------------------------------\n\nTITLE: UNION with Subquery Ordering\nDESCRIPTION: Shows how to apply ORDER BY and LIMIT clauses to individual SELECT statements within a UNION operation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-union_27362049.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT key FROM (SELECT key FROM src ORDER BY key LIMIT 10)subq1\nUNION\nSELECT key FROM (SELECT key FROM src1 ORDER BY key LIMIT 10)subq2\n```\n\n----------------------------------------\n\nTITLE: Hive Progress and Performance Settings\nDESCRIPTION: Settings related to progress monitoring, performance logging, and timeout configurations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_19\n\nLANGUAGE: properties\nCODE:\n```\nhive.auto.progress.timeout=0\nhive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Reduce Tasks for Spark Shuffle in Hive\nDESCRIPTION: Property to set a fixed number of reduce tasks for Spark shuffle stages. Default is -1, which means the number is dynamically calculated based on data statistics.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_54\n\nLANGUAGE: properties\nCODE:\n```\nmapreduce.job.reduces = -1\n```\n\n----------------------------------------\n\nTITLE: Installing Hive Client Libraries\nDESCRIPTION: Command to install Hive client libraries to the system using Ant build system. Requires sudo privileges.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo ant install -Dthrift.home=<THRIFT_HOME>\n```\n\n----------------------------------------\n\nTITLE: Starting Hive CLI\nDESCRIPTION: Command to start the Hive command line interface from the Hive home directory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-installation_27362077.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/hive\n```\n\n----------------------------------------\n\nTITLE: Decoding Base64 String in Hive\nDESCRIPTION: Converts the argument from a base64 string to BINARY format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\nunbase64(string str)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Test Mode No Sample List\nDESCRIPTION: Configuration property that specifies tables to exclude from sampling when Hive is running in test mode. This property was added in Hive 0.4.0 and accepts a comma-separated list of table names.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_120\n\nLANGUAGE: properties\nCODE:\n```\nhive.test.mode.nosamplelist=\n```\n\n----------------------------------------\n\nTITLE: Creating a ReadEntity for HCatReader in Java\nDESCRIPTION: This snippet demonstrates how to define a ReadEntity object to read all rows from a table in HCatalog. The ReadEntity represents the data source from which to read, specified by database and table name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-readerwriter_34013921.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nReadEntity.Builder builder = new ReadEntity.Builder();\nReadEntity entity = builder.withDatabase(\"mydb\").withTable(\"mytbl\").build();\n```\n\n----------------------------------------\n\nTITLE: Running Hive Tests in Debug Mode\nDESCRIPTION: Commands to run Hive unit tests with debug options enabled for remote debugging.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n    > export HADOOP_OPTS=$HIVE_DEBUG\n    > ant test -Dtestcase=TestCliDriver -Dqfile=<mytest>.q\n```\n\n----------------------------------------\n\nTITLE: Cloning Apache Hive Repository\nDESCRIPTION: Command to clone the Apache Hive source code repository from GitHub using git.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocontribute_27362107.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/apache/hive\n```\n\n----------------------------------------\n\nTITLE: Installing Hugo on macOS\nDESCRIPTION: Command to install Hugo static site generator on macOS using Homebrew package manager.\nSOURCE: https://github.com/apache/hive-site/blob/main/README.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install hugo\n```\n\n----------------------------------------\n\nTITLE: Checking Hive Job Output with Hadoop Commands\nDESCRIPTION: These Hadoop filesystem commands demonstrate how to check the output of a Hive job executed through WebHCat, including listing the output directory and viewing the contents.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-hive_34017180.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n% hadoop fs -ls pokes.output\nFound 2 items\n-rw-r--r--   1 ctdean supergroup        610 2011-11-11 13:22 /user/ctdean/pokes.output/stderr\n-rw-r--r--   1 ctdean supergroup         15 2011-11-11 13:22 /user/ctdean/pokes.output/stdout\n\n% hadoop fs -cat pokes.output/stdout\n1       a\n2       bb\n3       ccc\n```\n\n----------------------------------------\n\nTITLE: Creating Table with CHAR Type in Hive\nDESCRIPTION: Demonstrates how to create a table with a CHAR column type of length 10. CHAR is a fixed-length string type that was introduced in Hive 0.13.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE foo (bar CHAR(10))\n```\n\n----------------------------------------\n\nTITLE: Using the array_expect() function in Hive SQL\nDESCRIPTION: Returns an array of elements in the first array but not in the second array. Example: array_expect(array(1, 2, 3, 4), array(2, 3)) returns [1, 4].\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_63\n\nLANGUAGE: SQL\nCODE:\n```\narray_expect(array1, array2)\n```\n\n----------------------------------------\n\nTITLE: Setting Table Property using WebHCat API with Curl\nDESCRIPTION: This curl command demonstrates how to make a PUT request to the WebHCat API to set a property named 'fruit' with the value 'apples' on a table named 'test_table' in the 'default' database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-putproperty_34017012.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X PUT -HContent-type:application/json -d '{ \"value\": \"apples\" }' \\\n  'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/property/fruit?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Renaming Table with POST Request in WebHCat\nDESCRIPTION: Example of making a POST request to WebHCat API to rename a table with user authentication and rename parameters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-usingwebhcat_34015492.md#2025-04-09_snippet_1\n\nLANGUAGE: curl\nCODE:\n```\ncurl -s -d user.name=ctdean \\\n       -d rename=test_table_2 \\\n       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table'\n```\n\n----------------------------------------\n\nTITLE: Script Operator Environment Property\nDESCRIPTION: Configuration property that specifies which HiveConf values should be excluded from environment variables in script operators. Default excludes transaction list due to size and compression limitations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_13\n\nLANGUAGE: properties\nCODE:\n```\nhive.script.operator.env.blacklist=hive.txn.valid.txns,hive.script.operator.env.blacklist\n```\n\n----------------------------------------\n\nTITLE: Creating Hadoop Credential Store for Hive JDBC Passwords\nDESCRIPTION: These commands create a local keystore file using the Hadoop credential command. It stores the trustStorePassword and keyStorePassword aliases, which can be used with the storePasswordPath option in Hive JDBC connections.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_53\n\nLANGUAGE: bash\nCODE:\n```\nhadoop credential create trustStorePassword -value mytruststorepassword -provider localjceks://file/tmp/client_creds.jceks\n\nhadoop credential create keyStorePassword -value mykeystorepassword -provider localjceks://file/tmp/client_creds.jceks\n```\n\n----------------------------------------\n\nTITLE: Querying Table Description - Extended Format\nDESCRIPTION: Example curl command for retrieving an extended table description that includes additional metadata like partitioning and storage information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-gettable_34016519.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ctdean&format=extended'\n```\n\n----------------------------------------\n\nTITLE: Delete Database API Error Response\nDESCRIPTION: JSON error response structure when the database deletion fails. Includes detailed error information, error code, and the attempted database name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletedb_34016281.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"errorDetail\": \"\n    NoSuchObjectException(message:There is no database named my_db)\n        at org.apache.hadoop.hive.metastor...\n    \",\n  \"error\": \"There is no database named newdb\",\n  \"errorCode\": 404,\n  \"database\": \"newdb\"\n}\n```\n\n----------------------------------------\n\nTITLE: Hive Database Event Commands\nDESCRIPTION: SQL commands used for different database event types in Hive replication, including drops, exports, imports and alter table operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivereplicationdevelopment_55155632.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nDROP DATABASE CASCADE\n```\n\nLANGUAGE: SQL\nCODE:\n```\nEXPORT â€¦ FOR REPLICATION\n```\n\nLANGUAGE: SQL\nCODE:\n```\nIMPORT\n```\n\nLANGUAGE: SQL\nCODE:\n```\nDROP TABLE â€¦ FOR REPLICATION('id')\n```\n\nLANGUAGE: SQL\nCODE:\n```\nEXPORT â€¦ FOR METADATA REPLICATION\n```\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE â€¦ DROP PARTITION(â€¦) FOR REPLICATION('id')\n```\n\n----------------------------------------\n\nTITLE: Starting Derby Network Server\nDESCRIPTION: Command to start Derby in network server mode with specific host configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivederbyservermode_27362068.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd /opt/hadoop/db-derby-10.4.1.3-bin/data\n\nnohup /opt/hadoop/db-derby-10.4.1.3-bin/startNetworkServer -h 0.0.0.0 &\n```\n\n----------------------------------------\n\nTITLE: JSON Response for WebHCat Delete Job API\nDESCRIPTION: Example JSON output returned by the WebHCat API when deleting a job. It includes job status, profile information, and other metadata about the deleted job.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletejob_34017204.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"status\": {\n            \"startTime\": 1321047216471,\n            \"username\": \"ctdean\",\n            \"jobID\": {\n                      \"jtIdentifier\": \"201111111311\",\n                      \"id\": 9\n                     },\n            \"jobACLs\": {\n                       },\n            \"schedulingInfo\": \"NA\",\n            \"failureInfo\": \"NA\",\n            \"jobId\": \"job_201111111311_0009\",\n            \"jobPriority\": \"NORMAL\",\n            \"runState\": 1,\n            \"jobComplete\": false\n           },\n \"profile\": {\n             \"url\": \"http://localhost:50030/jobdetails.jsp?jobid=job_201111111311_0009\",\n             \"user\": \"ctdean\",\n             \"jobID\": {\n                       \"jtIdentifier\": \"201111111311\",\n                       \"id\": 9\n                      },\n             \"queueName\": \"default\",\n             \"jobFile\": \"hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201111111311_0009/job.xml\",\n             \"jobName\": \"streamjob3322518350676530377.jar\",\n             \"jobId\": \"job_201111111311_0009\"\n            }\n \"id\": \"job_201111111311_0009\",\n \"parentId\": \"job_201111111311_0008\",\n \"percentComplete\": \"10% complete\",\n \"exitValue\": 0,\n \"user\": \"ctdean\",\n \"callback\": null,\n \"completed\": \"false\"\n}\n```\n\n----------------------------------------\n\nTITLE: Dumping Query Results to File Using Silent Mode\nDESCRIPTION: Example of running a query in silent mode and redirecting the output to a file.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/replacing-the-implementation-of-hive-cli-using-beeline_61311909.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$HIVE_HOME/bin/hive -S -e 'select a.foo from pokes a' > a.txt\n```\n\n----------------------------------------\n\nTITLE: UNION within FROM Clause Template\nDESCRIPTION: Demonstrates how to embed UNION operations within a FROM clause for additional processing of combined results.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-union_27362049.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM (\n  select_statement\n  UNION ALL\n  select_statement\n) unionResult\n```\n\n----------------------------------------\n\nTITLE: Using show processlist Command in Hive Beeline\nDESCRIPTION: Demonstrates the 'show processlist' command in Hive Beeline which displays information about operations currently running on HiveServer2. This helps troubleshoot issues such as long running queries and connection starvation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-commands_34838882.md#2025-04-09_snippet_1\n\nLANGUAGE: hiveql\nCODE:\n```\n0: jdbc:hive2://localhost:10000> show processlist;\n+------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+----------+-------------------+-------------------+---------------+\n| User Name  |  Ip Addr   | Execution Engine  |              Session Id               | Session Active Time (s)  | Session Idle Time (s)  |                      Query ID                      |  State   | Opened Timestamp  | Elapsed Time (s)  |  Runtime (s)  |\n+------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+----------+-------------------+-------------------+---------------+\n| hive       | 127.0.0.1  | mr                | 66df357a-90bf-43cb-847f-279aa6df1c24  | 113                      | 7                      | rtrivedi_20240709124106_d0f00d7a-6fab-4fcd-9f41-d53bb296275d | RUNNING  | 1720546866774     | 16                | Not finished  |\n| hive       | 127.0.0.1  | mr                | 7daa873e-bb46-462e-bc38-94cb8d3e7c17  | 83                       | 29                     | rtrivedi_20240709124106_2dc53c4c-e522-4aed-a969-3d48ac01ba81 | RUNNING  | 1720546866774     | 17                | Not finished  |\n+------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+----------+-------------------+-------------------+---------------+\n```\n\n----------------------------------------\n\nTITLE: Starting Hive Web Interface with Ant\nDESCRIPTION: Command to start HWI service using Apache Ant library\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivewebinterface_27362110.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ANT_LIB=/opt/ant/lib\nbin/hive --service hwi\n```\n\n----------------------------------------\n\nTITLE: Case W2: Hive SQL Outer Join with Where Clause on Null Supplying Table\nDESCRIPTION: Example showing how Hive handles a where clause predicate on the null supplying table in a left outer join. The execution plan shows the predicate not being pushed down.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/outerjoinbehavior_35749927.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nexplain\nselect s1.key, s2.key \nfrom src s1 left join src s2 \nwhere s2.key > '2';\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Metrics JSON File Location\nDESCRIPTION: Specifies the default file path where the JSON metrics will be saved when using the JSON_FILE reporter with the Codahale metrics implementation. This file gets overwritten at every interval specified by hive.service.metrics.file.frequency.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_95\n\nLANGUAGE: plaintext\nCODE:\n```\n/tmp/report.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Pentaho Report Designer for Hive JDBC in Bash\nDESCRIPTION: This script sets up the classpath for Pentaho Report Designer to work with Hive JDBC. It includes Hadoop core and Hive libraries, and launches the Pentaho Report Designer with the necessary Java options.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivejdbcinterface_27362100.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/sh\n\nHADOOP_CORE={{ls $HADOOP_HOME/hadoop-*-core.jar}}\nCLASSPATH=.:$HADOOP_CORE:$HIVE_HOME/conf\n\nfor i in ${HIVE_HOME}/lib/*.jar ; do\n  CLASSPATH=$CLASSPATH:$i\ndone\n\nCLASSPATH=$CLASSPATH:launcher.jar\n\necho java -XX:MaxPermSize=512m -cp $CLASSPATH -jar launcher.jar\njava -XX:MaxPermSize=512m -cp $CLASSPATH org.pentaho.commons.launcher.Launcher\n```\n\n----------------------------------------\n\nTITLE: Running a Query from Command Line with Hive CLI\nDESCRIPTION: Example of executing a Hive query directly from the command line using the Hive CLI.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/replacing-the-implementation-of-hive-cli-using-beeline_61311909.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$HIVE_HOME/bin/hive -e 'select a.foo from pokes a'\n```\n\n----------------------------------------\n\nTITLE: JSON response from WebHCat MapReduce streaming job submission\nDESCRIPTION: This JSON output shows the response received after submitting a MapReduce streaming job through the WebHCat API. It includes the job ID and information about the job execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-mapreducestream_34017023.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n \"id\": \"job_201111111311_0008\",\n \"info\": {\n          \"stdout\": \"packageJobJar: [] [/Users/ctdean/var/hadoop/hadoop-0.20.205.0/share/hadoop/contrib/streaming/hadoop-streaming-0.20.205.0.jar...\n                    templeton-job-id:job_201111111311_0008\n                    \",\n          \"stderr\": \"11/11/11 13:26:43 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments\n                    11/11/11 13:26:43 INFO mapred.FileInputFormat: Total input paths to process : 2\n                    \",\n          \"exitcode\": 0\n         }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing UDAF Parameter Type Checking in Java\nDESCRIPTION: Detailed implementation of parameter type checking in the getEvaluator method. Validates numeric and integer parameters for histogram calculation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/genericudafcasestudy_27362093.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException {\n    TypeInfo [] parameters = info.getParameters();\n    if (parameters.length != 2) {\n      throw new UDFArgumentTypeException(parameters.length - 1,\n          \"Please specify exactly two arguments.\");\n    }\n    \n    // validate the first parameter, which is the expression to compute over\n    if (parameters[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {\n      throw new UDFArgumentTypeException(0,\n          \"Only primitive type arguments are accepted but \"\n          + parameters[0].getTypeName() + \" was passed as parameter 1.\");\n    }\n    switch (((PrimitiveTypeInfo) parameters[0]).getPrimitiveCategory()) {\n    case BYTE:\n    case SHORT:\n    case INT:\n    case LONG:\n    case FLOAT:\n    case DOUBLE:\n      break;\n    case STRING:\n    case BOOLEAN:\n    default:\n      throw new UDFArgumentTypeException(0,\n          \"Only numeric type arguments are accepted but \"\n          + parameters[0].getTypeName() + \" was passed as parameter 1.\");\n    }\n\n    // validate the second parameter, which is the number of histogram bins\n    if (parameters[1].getCategory() != ObjectInspector.Category.PRIMITIVE) {\n      throw new UDFArgumentTypeException(1,\n          \"Only primitive type arguments are accepted but \"\n          + parameters[1].getTypeName() + \" was passed as parameter 2.\");\n    }\n    if( ((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()\n        != PrimitiveObjectInspector.PrimitiveCategory.INT) {\n      throw new UDFArgumentTypeException(1,\n          \"Only an integer argument is accepted as parameter 2, but \"\n          + parameters[1].getTypeName() + \" was passed instead.\");\n    }\n    return new GenericUDAFHistogramNumericEvaluator();\n  }\n```\n\n----------------------------------------\n\nTITLE: Default CSV Serde Configuration Values\nDESCRIPTION: Shows the default values for separator, quote, and escape characters if not explicitly specified in the SerDe properties. By default, comma is the separator, double quote is the quote character, and backslash is the escape character.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/csv-serde_48202659.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nDEFAULT_ESCAPE_CHARACTER \\\nDEFAULT_QUOTE_CHARACTER  \"\nDEFAULT_SEPARATOR        ,\n```\n\n----------------------------------------\n\nTITLE: Setting ORC File Write Format Version\nDESCRIPTION: Configuration to define the version of ORC file format to write. Possible values are 0.11 and 0.12, with empty value defaulting to the latest encoding.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_29\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.orc.write.format=\n```\n\n----------------------------------------\n\nTITLE: Uploading Override JAR to HDFS for WebHCat\nDESCRIPTION: This command uploads an override JAR (ugi.jar) to HDFS, which is required for certain Hadoop versions to properly run WebHCat.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-installwebhcat_34015585.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhadoop fs -put ugi.jar /apps/templeton/ugi.jar\n```\n\n----------------------------------------\n\nTITLE: Advanced Date Manipulation in Hive SQL\nDESCRIPTION: Shows advanced date manipulation functions like adding months to a date, finding the last day of a month, or the next occurrence of a specific day of the week.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nadd_months('2009-08-31', 1)\n```\n\nLANGUAGE: SQL\nCODE:\n```\nadd_months('2017-12-31 14:15:16', 2, 'YYYY-MM-dd HH:mm:ss')\n```\n\nLANGUAGE: SQL\nCODE:\n```\nlast_day('2015-03-14')\n```\n\nLANGUAGE: SQL\nCODE:\n```\nnext_day('2015-01-14', 'TU')\n```\n\nLANGUAGE: SQL\nCODE:\n```\ntrunc('2015-03-17', 'MM')\n```\n\n----------------------------------------\n\nTITLE: Hive INT/INTEGER Type Definition\nDESCRIPTION: Shows the definition and range of the INT/INTEGER data type in Hive, supporting 4-byte signed integers.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nINT/INTEGER (4-byte signed integer, from -2,147,483,648 to 2,147,483,647)\n```\n\n----------------------------------------\n\nTITLE: JSON Response for Successful 'Create Table Like' Operation\nDESCRIPTION: This JSON output shows the response from the WebHCat API after successfully creating a new table. It includes the name of the newly created table and the database in which it was created.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-puttablelike_34016572.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"table\": \"test_table_2\",\n \"database\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Capitalizing First Letter of Each Word in Hive\nDESCRIPTION: Returns string with the first letter of each word in uppercase and all other letters in lowercase. Words are delimited by whitespace.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_38\n\nLANGUAGE: sql\nCODE:\n```\ninitcap(string A)\n```\n\n----------------------------------------\n\nTITLE: WebHCat Job Information API Response Format\nDESCRIPTION: Example JSON response from the WebHCat jobs API showing comprehensive job status information including run state, completion status, job profile details, and user arguments.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-job_34835065.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"status\": {\n            \"startTime\": 1324529476131,\n            \"username\": \"ctdean\",\n            \"jobID\": {\n                      \"jtIdentifier\": \"201112212038\",\n                      \"id\": 4\n                     },\n            \"jobACLs\": {\n                       },\n            \"schedulingInfo\": \"NA\",\n            \"failureInfo\": \"NA\",\n            \"jobId\": \"job_201112212038_0004\",\n            \"jobPriority\": \"NORMAL\",\n            \"runState\": 2,\n            \"jobComplete\": true\n           },\n \"profile\": {\n             \"url\": \"http://localhost:50030/jobdetails.jsp?jobid=job_201112212038_0004\",\n             \"jobID\": {\n                       \"jtIdentifier\": \"201112212038\",\n                        \"id\": 4\n                      },\n             \"user\": \"ctdean\",\n             \"queueName\": \"default\",\n             \"jobFile\": \"hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201112212038_0004/job.xml\",\n             \"jobName\": \"PigLatin:DefaultJobName\",\n             \"jobId\": \"job_201112212038_0004\"\n            },\n \"id\": \"job_201112212038_0004\",\n \"parentId\": \"job_201112212038_0003\",\n \"percentComplete\": \"100% complete\",\n \"exitValue\": 0,\n \"user\": \"ctdean\",\n \"callback\": null,\n \"completed\": \"done\",\n \"userargs\" => {\n    \"callback\"  => null,\n    \"define\"    => [],\n    \"execute\"   => \"select a,rand(b) from mynums\",\n    \"file\"      => null,\n    \"statusdir\" => null,\n    \"user.name\" => \"hadoopqa\",\n  },\n\n}\n```\n\n----------------------------------------\n\nTITLE: Uploading Hive TAR to HDFS for WebHCat\nDESCRIPTION: This command uploads the Hive tar.gz file to HDFS, which is required for WebHCat to access Hive resources from the Hadoop distributed cache.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-installwebhcat_34015585.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhadoop fs -put /tmp/hive-0.11.0.tar.gz /apps/templeton/hive-0.11.0.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Enabling the New Beeline-based Hive CLI Implementation\nDESCRIPTION: Command to enable the new Beeline-based Hive CLI tool instead of the old Hive client implementation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/replacing-the-implementation-of-hive-cli-using-beeline_61311909.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport USE_DEPRECATED_CLI=false\n```\n\n----------------------------------------\n\nTITLE: Manual Metastore Schema Upgrade for Hive Views\nDESCRIPTION: SQL script to manually alter the TBLS table in the Hive metastore to add columns necessary for view support. This is used when automatic schema updates are disabled.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/viewdev_27362067.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE TBLS ADD COLUMN VIEW_ORIGINAL_TEXT MEDIUMTEXT;\nALTER TABLE TBLS ADD COLUMN VIEW_EXPANDED_TEXT MEDIUMTEXT;\nALTER TABLE TBLS ADD COLUMN TBL_TYPE VARCHAR(128);\n```\n\n----------------------------------------\n\nTITLE: Displaying HWI Service Help\nDESCRIPTION: Command to show help information for the HWI service\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivewebinterface_27362110.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/hive --service hwi --help\n```\n\n----------------------------------------\n\nTITLE: Creating a Partition in Hive Table Using WebHCat REST API with Curl\nDESCRIPTION: This example demonstrates how to create a partition in a Hive table named 'test_table' with the partition value 'country=algeria' using the WebHCat REST API. The location for the partition is specified as 'loc_a' and the request is made as user 'ctdean'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-putpartition_34016600.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X PUT -HContent-type:application/json -d '{\"location\": \"loc_a\"}' \\\n       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/partition/country=%27algeria%27?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Adding TableScan for Big Table and Execution Flow in Hive\nDESCRIPTION: Steps for modifying the MapRedTask to add TableScan for the big table and establishing the execution flow. This includes setting up the prelaunch operator list and defining how partition pruning will be applied during execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoin-and-partition-pruning_34015666.md#2025-04-09_snippet_7\n\nLANGUAGE: pseudo\nCODE:\n```\n6. In the MapRedTask corresponding to MapJoin, add TableScan for the bigTable to the prelaunch operator list. Add to the TS the location of corresponding PartitionDescGenSink output.\n\n7. At execution time call prelaunch on each task. Task will call prelaunch on the work. Work will call prelaunch on the operators in the list in order. For TableScan, prelaunch will result in reading the PartitionDescriptor info and would find intersection of existing PartitionDesc and the new list produced by PartitionDescGenSink. Partition state info kept in MapWork would be updated with the new partitons (\"MapWork.pathToAliases\", \"MapWork.aliasToPartnInfo\", \"MapWork.pathToPartitionInfo\"). This would be then picked up by \"ExecDriver.execute\" to setup input paths for InputFormat.\n```\n\n----------------------------------------\n\nTITLE: Delete Database API Success Response\nDESCRIPTION: JSON response structure returned when a database is successfully deleted. Contains confirmation of the deleted database name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletedb_34016281.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"database\":\"newdb\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Beeline Connection with Hive Variables and Configuration Parameters\nDESCRIPTION: An example beeline-hs2-connection.xml that includes comma-separated hiveconf and hivevar settings. This allows passing multiple configuration parameters and variables to the Hive session.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_32\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n<property>\n  <name>beeline.hs2.connection.user</name>\n  <value>hive</value>\n</property>\n<property>\n  <name>beeline.hs2.connection.hiveconf</name>\n  <value>hive.cli.print.current.db=true, hive.cli.print.header=true</value>\n</property>\n<property>\n  <name>beeline.hs2.connection.hivevar</name>\n  <value>testVarName1=value1, testVarName2=value2</value>\n</property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Setting Druid Broker Address in Hive Configuration\nDESCRIPTION: Configuration setting to specify the default Druid broker address that Hive will connect to.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nSET hive.druid.broker.address.default=10.5.0.10:8082;\n```\n\n----------------------------------------\n\nTITLE: JSON Response for Column Information Request\nDESCRIPTION: This JSON output represents the response from the WebHCat API when requesting column information. It includes an array of columns with their names, types, and optional comments, as well as the database and table names.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getcolumns_34016970.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"columns\": [\n   {\n     \"name\": \"id\",\n     \"type\": \"bigint\"\n   },\n   {\n     \"name\": \"user\",\n     \"comment\": \"The user name\",\n     \"type\": \"string\"\n   },\n   {\n     \"name\": \"my_p\",\n     \"type\": \"string\"\n   },\n   {\n     \"name\": \"my_q\",\n     \"type\": \"string\"\n   }\n ],\n \"database\": \"default\",\n \"table\": \"my_table\"\n}\n```\n\n----------------------------------------\n\nTITLE: Rebuilding Index on a Specific Partition in HiveQL\nDESCRIPTION: Rebuilds an existing index for a specific partition defined by columnX and columnY values, showing how to maintain indexes for partitioned tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_9\n\nLANGUAGE: hiveql\nCODE:\n```\nALTER INDEX table10_index ON table10 PARTITION (columnX='valueQ', columnY='valueR') REBUILD;\n```\n\n----------------------------------------\n\nTITLE: ORC Configuration Properties\nDESCRIPTION: Configuration properties for ORC file format in Hive, including thread count, split strategy, corrupt data handling, and compression settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_34\n\nLANGUAGE: properties\nCODE:\n```\nhive.orc.compute.splits.num.threads=10\nhive.exec.orc.split.strategy=HYBRID\nhive.exec.orc.skip.corrupt.data=false\nhive.exec.orc.zerocopy=false\nhive.merge.orcfile.stripe.level=true\nhive.orc.row.index.stride.dictionary.check=true\nhive.exec.orc.compression.strategy=SPEED\n```\n\n----------------------------------------\n\nTITLE: Compiling unixODBC API Wrapper\nDESCRIPTION: Command to compile the unixODBC API wrapper after configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ make\n```\n\n----------------------------------------\n\nTITLE: Simple SELECT Query in Hive\nDESCRIPTION: Demonstrates a basic SELECT query that retrieves all columns and rows from a table t1.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM t1\n```\n\n----------------------------------------\n\nTITLE: Extended Table Description Response\nDESCRIPTION: Sample JSON response showing extended table metadata including partition information, storage formats, and owner details.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-gettable_34016519.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"partitioned\": true,\n  \"location\": \"hdfs://ip-10-77-6-151.ec2.internal:8020/apps/hive/warehouse/test_table\",\n  \"outputFormat\": \"org.apache.hadoop.hive.ql.io.RCFileOutputFormat\",\n  \"columns\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"bigint\"\n    },\n    {\n      \"name\": \"price\",\n      \"comment\": \"The unit price\",\n      \"type\": \"float\"\n    }\n  ],\n  \"owner\": \"ctdean\",\n  \"partitionColumns\": [\n    {\n      \"name\": \"country\",\n      \"type\": \"string\"\n    }\n  ],\n  \"inputFormat\": \"org.apache.hadoop.hive.ql.io.RCFileInputFormat\",\n  \"database\": \"default\",\n  \"table\": \"test_table\"\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Detailed Job Information with WebHCat REST API\nDESCRIPTION: Curl command showing how to get detailed job information using the fields parameter\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-jobs_34835057.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/jobs?user.name=daijy&fields=*'\n```\n\nLANGUAGE: json\nCODE:\n```\n[{\"id\":\"job_201304291205_0016\",\n  \"detail\":{\n    \"status\":{\n      \"jobACLs\":{\n        \"MODIFY_JOB\":{\"allAllowed\":false,\"aclstring\":\" \"},\n        \"VIEW_JOB\":{\"allAllowed\":false,\"aclstring\":\" \"}},\n      \"runState\":2,\n      \"startTime\":1367264912274,\n      \"schedulingInfo\":\"NA\",\n      \"failureInfo\":\"NA\",\n      \"jobPriority\":\"NORMAL\",\n      \"username\":\"daijy\",\n      \"jobID\":{\"id\":16,\"jtIdentifier\":\"201304291205\"},\n      \"jobId\":\"job_201304291205_0016\",\n      \"jobComplete\":true},\n    \"profile\":{\n      \"user\":\"daijy\",\n      \"jobFile\":\"hdfs://localhost:8020/Users/daijy/hadoop-1.0.3/tmp/mapred/staging/daijy/.staging/job_201304291205_0016/job.xml\",\n      \"url\":\"http://localhost:50030/jobdetails.jsp?jobid=job_201304291205_0016\",\n      \"queueName\":\"default\",\n      \"jobName\":\"word count\",\n      \"jobID\":{\"id\":16,\"jtIdentifier\":\"201304291205\"},\n      \"jobId\":\"job_201304291205_0016\"},\n      \"id\":\"job_201304291205_0016\",\n      \"parentId\":\"job_201304291205_0015\",\n      \"percentComplete\":\"map 100% reduce 100%\",\n      \"exitValue\":0,\n      \"user\":\"daijy\",\n      \"callback\":\"http://daijymacpro.local:57815/templeton/$jobId\",\n      \"completed\": \"done\",\n      \"userargs\" => {\n        \"callback\"  => null,\n        \"define\"    => [],\n        \"enablelog\" => \"false\",\n        \"execute\"   => \"select a,rand(b) from mynums\",\n        \"file\"      => null,\n        \"files\"     => [],\n        \"statusdir\" => null,\n        \"user.name\" => \"hadoopqa\",\n      },\n    }]\n```\n\n----------------------------------------\n\nTITLE: ASCII Value Function\nDESCRIPTION: Returns the numeric value of the first character of input string\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nascii(string str)\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: Main document structure showing table of contents and section organization for Hive on Spark join design documentation\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/50858744.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Apache Hive : Hive on Spark: Join Design Master\n\n* [Purpose and Prerequisites]({{< ref \"#purpose-and-prerequisites\" >}})\n* [MapReduce Summary]({{< ref \"#mapreduce-summary\" >}})\n\t+ [Figure 1. Join Processors for Hive on MapReduce]({{< ref \"#figure-1-join-processors-for-hive-on-mapreduce\" >}})\n* [Tez Comparison]({{< ref \"#tez-comparison\" >}})\n* [Spark MapJoin]({{< ref \"#spark-mapjoin\" >}})\n* [Spark Join Design]({{< ref \"#spark-join-design\" >}})\n\t+ [Figure 2: Join Processors for Hive on Spark]({{< ref \"#figure-2-join-processors-for-hive-on-spark\" >}})\n```\n\n----------------------------------------\n\nTITLE: Using CSV Format for Query Results in Beeline (Deprecated)\nDESCRIPTION: The legacy csv format in Beeline that's maintained for backward compatibility. Values are always quoted with single quotes, and embedded single quotes are not escaped, which can cause parsing issues.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_25\n\nLANGUAGE: text\nCODE:\n```\n'id','value','comment'\n'1','Value1','Test comment 1'\n'2','Value2','Test comment 2'\n'3','Value3','Test comment 3'\n```\n\n----------------------------------------\n\nTITLE: Schematool Command Usage\nDESCRIPTION: Complete command-line options for the Hive schematool utility.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-schema-tool_34835119.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ schematool -help\nusage: schemaTool\n -dbType <databaseType>             Metastore database type\n -driver <driver>                   Driver name for connection\n -dryRun                            List SQL scripts (no execute)\n -help                              Print this message\n -info                              Show config and schema details\n -initSchema                        Schema initialization\n -initSchemaTo <initTo>             Schema initialization to a version\n -metaDbType <metaDatabaseType>     Used only if upgrading the system catalog for hive\n -passWord <password>               Override config file password\n -upgradeSchema                     Schema upgrade\n -upgradeSchemaFrom <upgradeFrom>   Schema upgrade from a version\n -url <url>                         Connection url to the database\n -userName <user>                   Override config file user name\n -verbose                           Only print SQL statements\n[Additional catalog options...]\n```\n\n----------------------------------------\n\nTITLE: Executing ANALYZE TABLE CACHE METADATA in Hive SQL\nDESCRIPTION: This command was intended to explicitly cache file metadata in HBase metastore when Hive metastore was configured to use HBase. The goal was to cache file metadata and potentially some information about splits to speed up generation and achieve better cache locality. However, this feature is not implemented as Hive Metastore on HBase was discontinued.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nANALYZE TABLE Table1 CACHE METADATA;\n```\n\n----------------------------------------\n\nTITLE: Dropping Table in HiveQL\nDESCRIPTION: Drops a table, optionally using IF EXISTS to prevent errors if the table doesn't exist. The PURGE option can be used to skip the trash directory and permanently delete the data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nDROP TABLE [IF EXISTS] table_name [PURGE];\n```\n\n----------------------------------------\n\nTITLE: Creating a Hive Table with Partitioning using WebHCat API\nDESCRIPTION: This curl command demonstrates how to create a Hive table with columns, comments and partitioning. The table is stored in RCFile format and contains two columns along with a partition column.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-puttable_34016540.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X PUT -HContent-type:application/json -d '{\n \"comment\": \"Best table made today\",\n \"columns\": [\n   { \"name\": \"id\", \"type\": \"bigint\" },\n   { \"name\": \"price\", \"type\": \"float\", \"comment\": \"The unit price\" } ],\n \"partitionedBy\": [\n   { \"name\": \"country\", \"type\": \"string\" } ],\n \"format\": { \"storedAs\": \"rcfile\" } }' \\\n 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Schema Upgrade Example\nDESCRIPTION: Example demonstrating schema upgrade from version 0.10.0 to current version.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-schema-tool_34835119.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ schematool -dbType derby -upgradeSchemaFrom 0.10.0\nMetastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true\nMetastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver\nMetastore connection User:       APP\nStarting upgrade metastore schema from version 0.10.0 to 0.13.0\nUpgrade script upgrade-0.10.0-to-0.11.0.derby.sql\nCompleted upgrade-0.10.0-to-0.11.0.derby.sql\nUpgrade script upgrade-0.11.0-to-0.12.0.derby.sql\nCompleted upgrade-0.11.0-to-0.12.0.derby.sql\nUpgrade script upgrade-0.12.0-to-0.13.0.derby.sql\nCompleted upgrade-0.12.0-to-0.13.0.derby.sql\nschemaTool completed\n```\n\n----------------------------------------\n\nTITLE: Retrieving Job Information Using Curl in WebHCat\nDESCRIPTION: Example curl command to retrieve job information for a specific job ID via the WebHCat REST API. The command sends a GET request to the queue/:jobid endpoint with the user.name parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-jobinfo_34017194.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/queue/job_201112212038_0004?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Uploading Hadoop Streaming JAR to HDFS for WebHCat\nDESCRIPTION: This command uploads the Hadoop Streaming JAR file to HDFS, which is necessary for WebHCat to use Hadoop Streaming functionality.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-installwebhcat_34015585.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhadoop fs -put hadoop-2.1.0/share/hadoop/tools/lib/hadoop-streaming-2.1.0.jar \\\n  /apps/templeton/hadoop-streaming.jar\n```\n\n----------------------------------------\n\nTITLE: Metastore Version Error Log\nDESCRIPTION: Error log message showing version information not found in metastore.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-schema-tool_34835119.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n...\nCaused by: MetaException(message:Version information not found in metastore. )\n...\n```\n\n----------------------------------------\n\nTITLE: Hive Streaming Query\nDESCRIPTION: Demonstrates how to use Hive's TRANSFORM clause to stream data through an external script during query execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nFROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING '/bin/cat' WHERE a.ds > '2008-08-09';\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Task Submission in Hive\nDESCRIPTION: Configuration property that determines whether local tasks (like mapjoin hashtable generation) run in a separate JVM. Added in Hive 0.14.0, defaults to true for better memory management, though can be set to false for unit testing purposes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_121\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.submit.local.task.via.child=true\n```\n\n----------------------------------------\n\nTITLE: Creating Avro Table with Schema URL\nDESCRIPTION: SQL commands to create an Avro-formatted table using AvroSerDe and insert data from an existing table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE as_avro\n  ROW FORMAT SERDE\n  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\n  STORED as INPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n  OUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n  TBLPROPERTIES (\n    'avro.schema.url'='file:///path/to/the/schema/test_serializer.avsc');\n \nINSERT OVERWRITE TABLE as_avro SELECT * FROM test_serializer;\n```\n\n----------------------------------------\n\nTITLE: Enabling HFile Generation for HBase in Hive XML\nDESCRIPTION: Determines whether HBaseStorageHandler should generate HFiles instead of writing directly to online HBase tables. Set to true for bulk loading scenarios.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_105\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.hbase.generatehfiles</name>\n  <value>false</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Setting up SSH tunnel for Hadoop communication\nDESCRIPTION: This command creates an SSH tunnel with dynamic port forwarding to the Hadoop master node. This tunnel allows the Hive CLI to communicate with the Hadoop cluster through a SOCKS proxy.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ ssh -i <path to Hadoop private key path> -D 2600 ec2-12-34-56-78.compute-1.amazonaws.com\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Compression Codec\nDESCRIPTION: Property to set the default compression codec for ORC files. ZLIB is the default compression method.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_32\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.orc.default.compress=ZLIB\n```\n\n----------------------------------------\n\nTITLE: Storage API Release Signing Commands\nDESCRIPTION: Commands for signing release artifacts and verifying signatures\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngpg --armor --detach-sig hive-storage-X.Y.Z-rcR.tar.gz\nshasum -c hive-storage-X.Y.Z-rcR.tar.gz.sha256\ngpg hive-storage-X.Y.Z-rcR.tar.gz.asc\n```\n\n----------------------------------------\n\nTITLE: Setting up input data for MapReduce streaming job in Hadoop\nDESCRIPTION: This snippet demonstrates how to create sample input data files and upload them to Hadoop Distributed File System (HDFS) for use in a MapReduce streaming job.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-mapreducestream_34017023.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% cat mydata/file01 mydata/file02\nHello World Bye World\nHello Hadoop Goodbye Hadoop\n\n% hadoop fs -put mydata/ .\n\n% hadoop fs -ls mydata\nFound 2 items\n-rw-r--r--   1 ctdean supergroup         23 2011-11-11 13:29 /user/ctdean/mydata/file01\n-rw-r--r--   1 ctdean supergroup         28 2011-11-11 13:29 /user/ctdean/mydata/file02\n```\n\n----------------------------------------\n\nTITLE: Generating Soundex Code in Hive\nDESCRIPTION: Returns the soundex code of the input string, which is a phonetic algorithm for indexing names by sound.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\nsoundex(string A)\n```\n\n----------------------------------------\n\nTITLE: LDAP Group Entry Format Example\nDESCRIPTION: Example of an LDAP group entry structure that contains a list of users as members. This shows how a group is defined with its members in LDAP directory format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_8\n\nLANGUAGE: ldap\nCODE:\n```\n\"dn: uid=group1,ou=Groups,dc=example,dc=com\",  \n\"distinguishedName: uid=group1,ou=Groups,dc=example,dc=com\",  \n\"objectClass: top\",  \n\"objectClass: groupOfNames\",  \n\"objectClass: ExtensibleObject\",  \n\"cn: group1\",  \n\"ou: Groups\",  \n\"sn: group1\",  \n\"member: uid=user1,ou=People,dc=example,dc=com\"\n```\n\n----------------------------------------\n\nTITLE: Pulling Apache Hive Docker Image\nDESCRIPTION: Command to pull Apache Hive 4.0.0 image from DockerHub repository\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hive-with-docker_282102281.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull apache/hive:4.0.0\n```\n\n----------------------------------------\n\nTITLE: Local Hive Environment Setup\nDESCRIPTION: Environment configuration for running Hive without a Hadoop cluster, using local filesystem and Derby database\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport HIVE_OPTS='--hiveconf mapred.job.tracker=local --hiveconf fs.default.name=file:///tmp \\\n    --hiveconf hive.metastore.warehouse.dir=file:///tmp/warehouse \\\n    --hiveconf javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/tmp/metastore_db;create=true'\n```\n\n----------------------------------------\n\nTITLE: Setting Default ZooKeeper-based Lock Manager Implementation\nDESCRIPTION: Specifies the default lock manager implementation to use when Hive concurrency support is enabled. This implementation uses ZooKeeper for distributed lock management.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_90\n\nLANGUAGE: xml\nCODE:\n```\norg.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager\n```\n\n----------------------------------------\n\nTITLE: Enabling File Format Checking on Data Load\nDESCRIPTION: Configuration to enable or disable checking of file format when loading data files into Hive tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_26\n\nLANGUAGE: properties\nCODE:\n```\nhive.fileformat.check=true\n```\n\n----------------------------------------\n\nTITLE: Querying LLAP Peers via HTTP in JSON\nDESCRIPTION: Example of retrieving LLAP peers information through the /peers web service endpoint. Returns detailed JSON data about peer nodes, including identity, host, ports, and resource allocation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/llap_62689557.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl localhost:15002/peers\n{\n  \"dynamic\" : true,\n  \"identity\" : \"718264f1-722e-40f1-8265-ac25587bf336\",\n  \"peers\" : [ \n {\n    \"identity\" : \"940d6838-4dd7-4e85-95cc-5a6a2c537c04\",\n    \"host\" : \"sandbox121.hortonworks.com\",\n    \"management-port\" : 15004,\n    \"rpc-port\" : 15001,\n    \"shuffle-port\" : 15551,\n    \"resource\" : {\n      \"vcores\" : 24,\n      \"memory\" : 128000\n    },\n    \"host\" : \"sandbox121.hortonworks.com\"\n  }, \n]\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting a Job with WebHCat API using curl\nDESCRIPTION: Example curl command to delete a job with ID job_201111111311_0009 using the WebHCat API. This sends a DELETE request to the queue/:jobid endpoint.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletejob_34017204.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -X DELETE 'http://localhost:50111/templeton/v1/queue/job_201111111311_0009?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Creating Empty Druid Table in Hive\nDESCRIPTION: Creates an external table in Hive backed by Druid storage handler with timestamp, dimension, and metric columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE druid_table_1\n(`__time` TIMESTAMP, `dimension1` STRING, `dimension2` STRING, `metric1` INT, `metric2` FLOAT)\nSTORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler';\n```\n\n----------------------------------------\n\nTITLE: Custom Hadoop Version Build\nDESCRIPTION: Ant command to build Hive against a specific Hadoop version (0.17.1 example)\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nant -Dhadoop.version=0.17.1 package\n```\n\n----------------------------------------\n\nTITLE: Upgrading Hive Schema from Specific Version\nDESCRIPTION: Upgrades the Derby metastore schema from version 3.1.0 to current version\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ schematool -dbType derby -upgradeSchemaFrom 3.1.0 Upgrading from the user input version 3.1.0\nMetastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\nMetastore connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\nMetastore connection User:\t APP\nStarting upgrade metastore schema from version 3.1.0 to 4.0.0-beta-2\nUpgrade script upgrade-3.1.0-to-3.2.0.derby.sql\nCompleted upgrade-3.1.0-to-3.2.0.derby.sql\n...\nCompleted upgrade-4.0.0-beta-1-to-4.0.0-beta-2.derby.sql\n```\n\n----------------------------------------\n\nTITLE: Sample Output from EXPLAIN AST Command in Hive\nDESCRIPTION: This snippet shows the Abstract Syntax Tree output produced by the EXPLAIN AST command, displaying the parsed structure of a Hive query with tokens representing different query elements.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nABSTRACT SYNTAX TREE:\n  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest_g1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key)) (TOK_SELEXPR (TOK_FUNCTION sum (TOK_FUNCTION substr (TOK_COLREF src value) 4)))) (TOK_GROUPBY (TOK_COLREF src key))))\n```\n\n----------------------------------------\n\nTITLE: Describing an Avro-backed Hive Table\nDESCRIPTION: Example output of the 'DESCRIBE' command showing how Avro data types are mapped to Hive data types in a table created with AvroSerDe.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nhive> describe kst;\nOK\nstring1 string  from deserializer\nstring2 string  from deserializer\nint1    int     from deserializer\nboolean1        boolean from deserializer\nlong1   bigint  from deserializer\nfloat1  float   from deserializer\ndouble1 double  from deserializer\ninner_record1   struct<int_in_inner_record1:int,string_in_inner_record1:string> from deserializer\nenum1   string  from deserializer\narray1  array<string>   from deserializer\nmap1    map<string,string>      from deserializer\nunion1  uniontype<float,boolean,string> from deserializer\nfixed1  binary  from deserializer\nnull1   void    from deserializer\nunionnullint    int     from deserializer\nbytes1  binary  from deserializer\n\n```\n\n----------------------------------------\n\nTITLE: Filtering by Block Offset in Apache Hive SQL\nDESCRIPTION: This SQL query demonstrates how to filter rows based on the block offset inside the file, selecting only rows where the offset is greater than 12000, and ordering the results by key.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-virtualcolumns_27362048.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from src where `BLOCK__OFFSET__INSIDE__FILE` > 12000 order by key;\n```\n\n----------------------------------------\n\nTITLE: Complex TPCH query execution with results in HDFS\nDESCRIPTION: This example shows a complex Hive query based on TPCH query 1, which performs aggregations on the lineitem table. The results are stored in HDFS for later retrieval. This query requires a Hadoop cluster for distributed processing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nhive> insert overwrite directory '/tmp/tpcresults-1.sql' \n  select l_returnflag, l_linestatus, sum ( l_quantity ) as sum_qty, sum ( l_extendedprice ) as sum_base_price,\n  sum ( l_extendedprice * ( 1 - l_discount )) as sub_disc_price, \n  sum ( l_extendedprice * ( 1 - l_discount ) * ( 1 + l_tax )) as sum_charge,\n  avg ( l_quantity ) as avg_qty, avg ( l_extendedprice ) as avg_price, \n  avg ( l_discount ) as avg_disc, count ( 1 ) as count_order\n  from lineitem where l_shipdate <= to_date('1998-12-01') group by l_returnflag, l_linestatus;\n```\n\n----------------------------------------\n\nTITLE: Querying Partition Information with Hive MetaTool in Shell\nDESCRIPTION: This command uses the metatool to execute a JDOQL query against the Hive metastore. It retrieves partition column statistics information for all metastore backends.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-metatool_55156221.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nHIVE_CONF_DIR=/etc/hive/conf/conf.server/ hive --service metatool -executeJDOQL 'select dbName+\".\"+tableName+\"(\"+partitionName+\")::\"+colName+\"=\"+numDVs from org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics'\n```\n\n----------------------------------------\n\nTITLE: Establishing Hive JDBC Connection with Kerberos Authentication in Java\nDESCRIPTION: This code snippet demonstrates how to establish a JDBC connection to Hive using Kerberos authentication with a pre-authenticated subject. It uses Subject.doAs() to perform the connection within the context of the authenticated subject.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_45\n\nLANGUAGE: java\nCODE:\n```\nstatic Connection getConnection( Subject signedOnUserSubject ) throws Exception{\n       Connection conn = (Connection) Subject.doAs(signedOnUserSubject, new PrivilegedExceptionAction<Object>()\n           {\n               public Object run()\n               {\n                       Connection con = null;\n                       String JDBC_DB_URL = \"jdbc:hive2://HiveHost:10000/default;\" ||\n                                              \"principal=hive/localhost.localdomain@EXAMPLE.COM;\" || \n                                              \"kerberosAuthType=fromSubject\";\n                       try {\n                               Class.forName(JDBC_DRIVER);\n                               con =  DriverManager.getConnection(JDBC_DB_URL);\n                       } catch (SQLException e) {\n                               e.printStackTrace();\n                       } catch (ClassNotFoundException e) {\n                               e.printStackTrace();\n                       }\n                       return con;\n               }\n           });\n       return conn;\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying MapReduce streaming job results in HDFS\nDESCRIPTION: This snippet shows how to check the output of the MapReduce streaming job in HDFS using Hadoop filesystem commands to list and view the contents of the output directory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-mapreducestream_34017023.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n% hadoop fs -ls mycounts\nFound 3 items\n-rw-r--r--   1 ctdean supergroup          0 2011-11-11 13:27 /user/ctdean/mycounts/_SUCCESS\ndrwxr-xr-x   - ctdean supergroup          0 2011-11-11 13:26 /user/ctdean/mycounts/_logs\n-rw-r--r--   1 ctdean supergroup         10 2011-11-11 13:27 /user/ctdean/mycounts/part-00000\n\n% hadoop fs -cat mycounts/part-00000\n      8\n```\n\n----------------------------------------\n\nTITLE: MySQL Metastore Index Tables Creation\nDESCRIPTION: MySQL DDL statements for creating the metastore tables required to support indexing functionality, including IDXS and INDEX_PARAMS tables with their constraints.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/indexdev_27362104.md#2025-04-09_snippet_2\n\nLANGUAGE: mysql\nCODE:\n```\nDROP TABLE IF EXISTS {{IDXS}};\nCREATE TABLE {{IDXS}} (\n  {{INDEX_ID}} bigint(20) NOT NULL,\n  {{CREATE_TIME}} int(11) NOT NULL,\n  {{DEFERRED_REBUILD}} bit(1) NOT NULL,\n  {{INDEX_HANDLER_CLASS}} varchar(256) DEFAULT NULL,\n  {{INDEX_NAME}} varchar(128) DEFAULT NULL,\n  {{INDEX_TBL_ID}} bigint(20) DEFAULT NULL,\n  {{LAST_ACCESS_TIME}} int(11) NOT NULL,\n  {{ORIG_TBL_ID}} bigint(20) DEFAULT NULL,\n  {{SD_ID}} bigint(20) DEFAULT NULL,\n  PRIMARY KEY ({{INDEX_ID}}),\n  UNIQUE KEY {{UNIQUEINDEX}} ({{INDEX_NAME}},{{ORIG_TBL_ID}}),\n  KEY {{IDXS_FK1}} ({{SD_ID}}),\n  KEY {{IDXS_FK2}} ({{INDEX_TBL_ID}}),\n  KEY {{IDXS_FK3}} ({{ORIG_TBL_ID}}),\n  CONSTRAINT {{IDXS_FK3}} FOREIGN KEY ({{ORIG_TBL_ID}}) REFERENCES {{TBLS}} ({{TBL_ID}}),\n  CONSTRAINT {{IDXS_FK1}} FOREIGN KEY ({{SD_ID}}) REFERENCES {{SDS}} ({{SD_ID}}),\n  CONSTRAINT {{IDXS_FK2}} FOREIGN KEY ({{INDEX_TBL_ID}}) REFERENCES {{TBLS}} ({{TBL_ID}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\nDROP TABLE IF EXISTS {{INDEX_PARAMS}};\nCREATE TABLE {{INDEX_PARAMS}} (\n  {{INDEX_ID}} bigint(20) NOT NULL,\n  {{PARAM_KEY}} varchar(256) NOT NULL,\n  {{PARAM_VALUE}} varchar(767) DEFAULT NULL,\n  PRIMARY KEY ({{INDEX_ID}},{{PARAM_KEY}}),\n  CONSTRAINT {{INDEX_PARAMS_FK1}} FOREIGN KEY ({{INDEX_ID}}) REFERENCES {{IDXS}} ({{INDEX_ID}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n```\n\n----------------------------------------\n\nTITLE: PARTITION BY with ORDER BY in HiveQL\nDESCRIPTION: Illustrates using PARTITION BY with one partitioning column, one ORDER BY column, and no window specification in a SELECT statement.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a, SUM(b) OVER (PARTITION BY c ORDER BY d)\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: Derby Installation Commands\nDESCRIPTION: Commands for downloading and extracting Derby database binaries and creating data directory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivederbyservermode_27362068.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd /opt/hadoop\n<download>\ntar -xzf db-derby-10.4.1.3-bin.tar.gz\nmkdir db-derby-10.4.1.3-bin/data\n```\n\n----------------------------------------\n\nTITLE: Debug Specific Query Setup\nDESCRIPTION: Commands showing how to debug a specific query by performing setup steps first.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n    >  ./build/dist/bin/hive\n    >  perform steps before the query\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Cluster Connection Properties\nDESCRIPTION: Essential configuration variables needed to connect Hive CLI to different Hadoop clusters. These properties specify the filesystem and job tracker locations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws_27362103.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nfs.default.name\nmapred.job.tracker\n```\n\n----------------------------------------\n\nTITLE: Running UDAF Tests\nDESCRIPTION: Command to execute test cases for the new UDAF implementation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/genericudafcasestudy_27362093.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nant test -Dtestcase=TestCliDriver -Dqfile=udaf_XXXXX.q\n```\n\n----------------------------------------\n\nTITLE: Creating User and Granting Permissions in PostgreSQL\nDESCRIPTION: This snippet shows how to create a user in PostgreSQL, associate them with a default schema, and grant necessary permissions for accessing the schema and its tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE ROLE greg WITH LOGIN PASSWORD 'GregPass123!$';\nALTER ROLE greg SET search_path TO bob;\nGRANT USAGE ON SCHEMA bob TO greg;\nGRANT SELECT ON ALL TABLES IN SCHEMA bob TO greg;\n```\n\n----------------------------------------\n\nTITLE: Establishing JMS Connection in Java\nDESCRIPTION: Creates a connection to the ActiveMQ message bus for receiving HCatalog notifications.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-notification_34014558.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nConnectionFactory connFac = new ActiveMQConnectionFactory(amqurl);\nConnection conn = connFac.createConnection();\nconn.start();\n```\n\n----------------------------------------\n\nTITLE: Deleting a Job Using WebHCat API with cURL\nDESCRIPTION: This curl command demonstrates how to delete a job using the WebHCat API. It sends a DELETE request to the jobs endpoint with a specific job ID and user name parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletejobid_34835045.md#2025-04-09_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\n% curl -s -X DELETE 'http://localhost:50111/templeton/v1/jobs/job_201111111311_0009?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Defining Row Count Sampling Syntax in HiveQL\nDESCRIPTION: This snippet shows the syntax for sampling based on row count in Hive, which applies the specified row count to each input split.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nblock_sample: TABLESAMPLE (n ROWS)\n```\n\n----------------------------------------\n\nTITLE: Configuring ExIm for S3 Access in Hive\nDESCRIPTION: XML configuration for HiveConf to whitelist S3 protocols for the Export/Import (ExIm) functionality when replicating to S3 storage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/replication_61336919.md#2025-04-09_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n  <property>\n    <name>hive.exim.uri.scheme.whitelist</name>\n    <value>hdfs,s3a</value>\n  </property>\n```\n\n----------------------------------------\n\nTITLE: Failed Hive Query Due to Schema Version Incompatibility\nDESCRIPTION: Example of a Hive query failing due to schema incompatibility, which occurs when attempting to access a metastore with an outdated schema version.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ build/dist/bin/hive -e \"show tables\"\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n\n```\n\n----------------------------------------\n\nTITLE: Setting Map-Reduce Job Tracker Configuration\nDESCRIPTION: Hadoop configuration variable that specifies the Map-Reduce cluster for job submission.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\n  mapred.job.tracker\n```\n\n----------------------------------------\n\nTITLE: Static Partitioning in MapReduce with HCatalog\nDESCRIPTION: Java code example showing how to configure an HCatalog output for MapReduce with static partitioning, where partition key-value pairs are explicitly specified.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-dynamicpartitions_34014006.md#2025-04-09_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nMap<String, String> partitionValues = new HashMap<String, String>();\npartitionValues.put(\"a\", \"1\");\npartitionValues.put(\"b\", \"1\");\nHCatTableInfo info = HCatTableInfo.getOutputTableInfo(dbName, tblName, partitionValues);\nHCatOutputFormat.setOutput(job, info);\n```\n\n----------------------------------------\n\nTITLE: Unsupported Subquery in Non-Simple Expression\nDESCRIPTION: Example of a subquery used within an arithmetic expression, which will not be supported in the initial implementation. The implementation will only allow subqueries as top-level expressions in the SELECT list.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/subqueries-in-select_68717850.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- subquery in non-simple expression\nSELECT 1 + (SELECT SUM(ship_charge) FROM orders), customer.customer_num FROM customer\n \n-- subquery in CASE\nSELECT CASE WHEN (select count(*) from store_sales \n                  where ss_quantity between 1 and 20) > 409437\n            THEN (select avg(ss_ext_list_price) \n                  from store_sales \n                  where ss_quantity between 1 and 20) \n            ELSE (select avg(ss_net_paid_inc_tax)\n                  from store_sales\n                  where ss_quantity between 1 and 20) end bucket1\nFROM reason\nWHERE r_reason_sk = 1\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example for Hadoop Installation\nDESCRIPTION: Shows the recommended directory structure for Hadoop, Derby, and Hive installations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivederbyservermode_27362068.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n/opt/hadoop/hadoop-0.17.2.1\n/opt/hadoop/db-derby-10.4.1.3-bin\n/opt/hadoop/hive\n```\n\n----------------------------------------\n\nTITLE: JSON Response for Partition Description from WebHCat API\nDESCRIPTION: This JSON output represents the response from the WebHCat API when describing a partition. It includes details such as partition location, input/output formats, column information, owner, and partition columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getpartition_34016592.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"partitioned\": true,\n  \"location\": \"hdfs://ip-10-77-6-151.ec2.internal:8020/apps/hive/warehouse/mytest/loc1\",\n  \"outputFormat\": \"org.apache.hadoop.hive.ql.io.RCFileOutputFormat\",\n  \"columns\": [\n    {\n      \"name\": \"i\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"j\",\n      \"type\": \"bigint\"\n    },\n    {\n      \"name\": \"ip\",\n      \"comment\": \"IP Address of the User\",\n      \"type\": \"string\"\n    }\n  ],\n  \"owner\": \"rachel\",\n  \"partitionColumns\": [\n    {\n      \"name\": \"country\",\n      \"type\": \"string\"\n    }\n  ],\n  \"inputFormat\": \"org.apache.hadoop.hive.ql.io.RCFileInputFormat\",\n  \"database\": \"default\",\n  \"table\": \"mytest\",\n  \"partition\": \"country='US'\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Temporary Macros in HiveQL\nDESCRIPTION: Syntax for creating a temporary macro in Hive. Macros take optional typed column parameters and return an expression. They exist for the duration of the current session.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_58\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TEMPORARY MACRO macro_name([col_name col_type, ...]) expression;\n```\n\n----------------------------------------\n\nTITLE: Setting SASL QOP Security for Hive JDBC Connection\nDESCRIPTION: Example showing how to configure the SASL Quality of Protection (QOP) parameter for a Hive JDBC connection. This parameter controls the level of security for SASL-based authentication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_44\n\nLANGUAGE: java\nCODE:\n```\njdbc:hive://hostname/dbname;sasl.qop=auth-int\n```\n\n----------------------------------------\n\nTITLE: Showing Only Last N Characters in Masked String in Hive\nDESCRIPTION: Returns a masked version of the input string, showing only the last n characters unmasked. All other characters are masked where uppercase letters become 'X', lowercase letters become 'x', and numbers become 'n'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_45\n\nLANGUAGE: sql\nCODE:\n```\nmask_show_last_n(string str[, int n])\n```\n\n----------------------------------------\n\nTITLE: Installing Python Modules Locally for Hive Parallel Testing\nDESCRIPTION: Commands to install required Python modules in a user's home directory when root access is unavailable or to avoid system-wide installation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\neasy_install --prefix \"~/.python_modules\" argparse\neasy_install --prefix \"~/.python_modules\" mako\n\n```\n\n----------------------------------------\n\nTITLE: Configuring ZooKeeper Root Path for Hive Token Store in XML\nDESCRIPTION: Sets the root path in ZooKeeper for storing Hive delegation token data when using ZooKeeper-based token store implementation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_100\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.cluster.delegation.token.store.zookeeper.znode</name>\n  <value>/hive/cluster/delegation</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Running Yetus Test Patch Script in Bash\nDESCRIPTION: Executes the Yetus test-patch.sh script to run code quality checks on a specific patch file. This command should be run after checking out the target branch without the new commits.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/running-yetus_71012969.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./dev-support/test-patch.sh ~/Downloads/HIVE-16345.2.patch\n```\n\n----------------------------------------\n\nTITLE: Generating SHA256 Checksums for Release Artifacts\nDESCRIPTION: Commands to generate SHA256 checksums for the binary and source release artifacts.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncd packaging/target\nshasum -a 256 apache-hive-X.Y.Z-bin.tar.gz > apache-hive-X.Y.Z-bin.tar.gz.sha256\nshasum -a 256 apache-hive-X.Y.Z-src.tar.gz > apache-hive-X.Y.Z-src.tar.gz.sha256\n```\n\n----------------------------------------\n\nTITLE: Output of Top K Statistics in Partition Description\nDESCRIPTION: Example output from DESCRIBE FORMATTED showing how top K statistics appear in the skewed columns and values section for a partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n...\nSkewed Columns:         [value]                  \nSkewed Values:          [[val_348], [val_230], [val_401], [val_70]]      \n...\n```\n\n----------------------------------------\n\nTITLE: Creating Table Structure for Table Column Statistics in Metastore\nDESCRIPTION: SQL definition for the TAB_COL_STATS table which stores column statistics for non-partitioned tables. Includes fields for various statistics metrics and foreign key relationships.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/column-statistics-in-hive_29131019.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE TAB_COL_STATS  \n (\n CS_ID NUMBER NOT NULL,  \n TBL_ID NUMBER NOT NULL,  \n COLUMN_NAME VARCHAR(128) NOT NULL,  \n COLUMN_TYPE VARCHAR(128) NOT NULL,  \n TABLE_NAME VARCHAR(128) NOT NULL,  \n DB_NAME VARCHAR(128) NOT NULL,\n\nLOW_VALUE RAW,  \n HIGH_VALUE RAW,  \n NUM_NULLS BIGINT,  \n NUM_DISTINCTS BIGINT,\n\nBIT_VECTOR, BLOB,  /* introduced in [HIVE-16997](https://issues.apache.org/jira/browse/HIVE-16997) in Hive 3.0.0 */\n\nAVG_COL_LEN DOUBLE,  \n MAX_COL_LEN BIGINT,  \n NUM_TRUES BIGINT,  \n NUM_FALSES BIGINT,  \n LAST_ANALYZED BIGINT NOT NULL)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Archive Implementation for Hive\nDESCRIPTION: Specifies the implementation class for accessing Hadoop Archives in Hive. This setting is not applicable to Hadoop versions less than 0.20.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_89\n\nLANGUAGE: xml\nCODE:\n```\norg.apache.hadoop.hive.shims.HiveHarFileSystem\n```\n\n----------------------------------------\n\nTITLE: Finding Next Day of Week in Hive\nDESCRIPTION: Returns the next occurrence of specified day of week after given date. Day of week can be specified in 2-letter, 3-letter, or full name format.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_38\n\nLANGUAGE: sql\nCODE:\n```\nnext_day(string start_date,string day_of_week)\n```\n\n----------------------------------------\n\nTITLE: Running MapReduce with HCatalog in Apache Hive\nDESCRIPTION: Shell commands for setting up the environment and running a MapReduce job with HCatalog, including setting necessary environment variables and specifying required JAR files.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-inputoutput_34013776.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport HADOOP_HOME=<path_to_hadoop_install>\nexport HCAT_HOME=<path_to_hcat_install>\nexport HIVE_HOME=<path_to_hive_install>\nexport LIB_JARS=$HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar,\n$HIVE_HOME/lib/hive-metastore-0.10.0.jar,\n$HIVE_HOME/lib/libthrift-0.7.0.jar,\n$HIVE_HOME/lib/hive-exec-0.10.0.jar,\n$HIVE_HOME/lib/libfb303-0.7.0.jar,\n$HIVE_HOME/lib/jdo2-api-2.3-ec.jar,\n$HIVE_HOME/lib/slf4j-api-1.6.1.jar\n\nexport HADOOP_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar:\n$HIVE_HOME/lib/hive-metastore-0.10.0.jar:\n$HIVE_HOME/lib/libthrift-0.7.0.jar:\n$HIVE_HOME/lib/hive-exec-0.10.0.jar:\n$HIVE_HOME/lib/libfb303-0.7.0.jar:\n$HIVE_HOME/lib/jdo2-api-2.3-ec.jar:\n$HIVE_HOME/conf:$HADOOP_HOME/conf:\n$HIVE_HOME/lib/slf4j-api-1.6.1.jar\n\n$HADOOP_HOME/bin/hadoop --config $HADOOP_HOME/conf jar <path_to_jar>\n<main_class> -libjars $LIB_JARS <program_arguments>\n```\n\n----------------------------------------\n\nTITLE: Setting Findbugs Home Environment Variable in Bash\nDESCRIPTION: Sets the FINDBUGS_HOME environment variable to the installation directory of Findbugs. This is required for running Findbugs checks with Yetus.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/running-yetus_71012969.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport FINDBUGS_HOME=~/dev/upstream/findbugs-3.0.1/\n```\n\n----------------------------------------\n\nTITLE: Using Test Datasets in Query File Tests\nDESCRIPTION: Illustrates how to incorporate pre-defined test datasets into Query File Tests using the qt:dataset option.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/Development/qtest.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n--! qt:dataset:src\nSELECT * FROM src;\n```\n\n----------------------------------------\n\nTITLE: Building Legacy Hive with Ant\nDESCRIPTION: Commands for building older Hive versions using Ant build system.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ ant clean package -Dhadoop.version=0.23.3 -Dhadoop-0.23.version=0.23.3 -Dhadoop.mr.rev=23\n$ ant clean package -Dhadoop.version=2.0.0-alpha -Dhadoop-0.23.version=2.0.0-alpha -Dhadoop.mr.rev=23\n```\n\n----------------------------------------\n\nTITLE: Outer Lateral View for Handling Empty Arrays in Hive\nDESCRIPTION: Demonstrates the OUTER keyword with a lateral view, which ensures rows are generated even when the UDTF (explode in this case) doesn't produce any rows for an empty array. NULL values are used for the columns that would come from the UDTF.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lateralview_27362040.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM src LATERAL VIEW OUTER explode(array()) C AS a limit 10;\n\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Service Metrics Update Frequency in XML\nDESCRIPTION: Configures the frequency of updating the Hadoop2 metrics system for Hive service metrics. Applicable when using org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics as the metrics class and HADOOP2 as the reporter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_97\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.service.metrics.hadoop2.frequency</name>\n  <value>30s</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Using JSON Format for Query Results in Beeline (Hive 4.0)\nDESCRIPTION: The json output format in Beeline (available from Hive 4.0) displays query results in JSON format where all rows are contained within a \"resultset\" array. Each row is represented as a separate JSON object with column names as keys.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\"resultset\":[{\"String\":\"aaa\",\"Int\":1,\"Decimal\":3.14,\"Bool\":true,\"Null\":null,\"Binary\":\"SGVsbG8sIFdvcmxkIQ\"},{\"String\":\"bbb\",\"Int\":2,\"Decimal\":2.718,\"Bool\":false,\"Null\":null,\"Binary\":\"RWFzdGVyCgllZ2cu\"}]}\n```\n\n----------------------------------------\n\nTITLE: Double Lateral View Query Example in Hive\nDESCRIPTION: Demonstrates using two lateral views in sequence to explode two different array columns (col1 and col2). This creates a Cartesian product of the exploded values from both arrays.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lateralview_27362040.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT myCol1, myCol2 FROM baseTable\nLATERAL VIEW explode(col1) myTable1 AS myCol1\nLATERAL VIEW explode(col2) myTable2 AS myCol2;\n\n```\n\n----------------------------------------\n\nTITLE: Setting Replica Functions Root Directory for Hive Replication in XML\nDESCRIPTION: Configures the root directory on the replica warehouse where the replication sub-system will store jars from the primary warehouse.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_107\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.repl.replica.functions.root.dir</name>\n  <value>/usr/hive/repl/functions</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring HiveServer2 Transport Mode in Java\nDESCRIPTION: This snippet shows how to specify the transport mode for HiveServer2's Thrift service using a Hive configuration property.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-overview_65147648.md#2025-04-09_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nhive.server2.transport.mode\n```\n\n----------------------------------------\n\nTITLE: Querying Input File Name and Block Offset in Apache Hive SQL\nDESCRIPTION: This SQL query demonstrates how to select the input file name, key, and block offset inside the file from the 'src' table using Hive's virtual columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-virtualcolumns_27362048.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nselect `INPUT__FILE__NAME`, key, `BLOCK__OFFSET__INSIDE__FILE` from src;\n```\n\n----------------------------------------\n\nTITLE: Building Thrift 0.14.1 from Source on Mac\nDESCRIPTION: Steps to build Thrift 0.14.1 from source code on MacOS, including dependency installation and configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocontribute_27362107.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nwget http://archive.apache.org/dist/thrift/0.14.1/thrift-0.14.1.tar.gz\n\ntar xzf thrift-0.14.1.tar.gz\n\nbrew install libtool\nbrew install automake\n\n#If configure fails with \"syntax error near unexpected token `QT5\", then run \"brew install pkg-config\"\n\n./bootstrap.sh\n\nsudo ./configure --with-openssl=/usr/local/Cellar/openssl@1.1/1.1.1j --without-erlang --without-nodejs --without-python --without-py3 --without-perl --without-php --without-php_extension --without-ruby --without-haskell --without-go --without-swift --without-dotnetcore --without-qt5\n\nbrew install openssl\n\nsudo ln -s /usr/local/opt/openssl/include/openssl/ /usr/local/include/\n\nsudo make\n\nsudo make install\n\nmkdir -p /usr/local/share/fb303/if\n\ncp path/to/thrift-0.14.1/contrib/fb303/if/fb303.thrift /usr/local/share/fb303/if/fb303.thrift\n# or alternatively the following command\ncurl -o /usr/local/share/fb303/if/fb303.thrift https://raw.githubusercontent.com/apache/thrift/master/contrib/fb303/if/fb303.thrift\n```\n\n----------------------------------------\n\nTITLE: Acquiring Record Identifiers with OrcInputFormat in Hive Streaming\nDESCRIPTION: This code snippet shows how to use AcidRecordReader to get record identifiers when reading data from ORC files in Hive Streaming API. This is a crucial part of reading existing data before applying mutations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118454.md#2025-04-09_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nAcidRecordReader.getRecordIdentifier\n```\n\n----------------------------------------\n\nTITLE: Describing Data Connector Metadata in HQL\nDESCRIPTION: Shows information about a data connector including name, comment, datasource URL, and type. The EXTENDED option also shows connector properties. Available since Hive 4.0.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_93\n\nLANGUAGE: hql\nCODE:\n```\nDESCRIBE CONNECTOR [EXTENDED] connector_name;\n```\n\n----------------------------------------\n\nTITLE: Beeline Debug Session Example\nDESCRIPTION: Shows a complete example of starting Beeline in debug mode and the prompt that appears once the debugger is attached. The CLI remains functional for running queries while being debugged.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ beeline --debug\nListening for transport dt_socket at address: 8000\nBeeline version 1.2.0 by Apache Hive\nbeeline>\n```\n\n----------------------------------------\n\nTITLE: Configuring HiveServer2 Thrift Bind Host\nDESCRIPTION: Sets the bind host for running the HiveServer2 Thrift interface. This can be overridden by setting the HIVE_SERVER2_THRIFT_BIND_HOST environment variable.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_42\n\nLANGUAGE: properties\nCODE:\n```\nhive.server2.thrift.bind.host=localhost\n```\n\n----------------------------------------\n\nTITLE: Signing Release Artifacts with GPG\nDESCRIPTION: GPG commands to create detached signatures for the release artifacts.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ngpg --armor --output apache-hive-X.Y.Z-bin.tar.gz.asc --detach-sig apache-hive-X.Y.Z-bin.tar.gz\ngpg --armor --output apache-hive-X.Y.Z-src.tar.gz.asc --detach-sig apache-hive-X.Y.Z-src.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Reading Data on Slave Nodes with HCatReader in Java\nDESCRIPTION: This code snippet shows how slave nodes use the ReaderContext to read data in parallel. It processes each input split, creates an HCatReader for each split, and iterates through the records.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-readerwriter_34013921.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nfor(InputSplit split : readCntxt.getSplits()){\nHCatReader reader = DataTransferFactory.getHCatReader(split,\nreaderCntxt.getConf());\n       Iterator<HCatRecord> itr = reader.read();\n       while(itr.hasNext()){\n              HCatRecord read = itr.next();\n          }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading HFiles using Legacy LoadTable Script\nDESCRIPTION: Command for loading HFiles into HBase using the loadtable.rb script (pre-0.90.2).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nhbase org.jruby.Main loadtable.rb transactions /tmp/hbout\n```\n\n----------------------------------------\n\nTITLE: Implementing Local MapReduce Tasks in Hive on Spark\nDESCRIPTION: Describes the short-term approach for handling local MapReduce tasks in Spark, with potential future optimizations to avoid file I/O operations by using in-memory RDDs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n### Local MapReduce Tasks\n\nWhile we could see the benefits of running local jobs on Spark, such as avoiding sinking data to a file and then reading it from the file to memory, in the short term, those tasks will still be executed the same way as it is today. This means that Hive will always have to submit MapReduce jobs when executing locally. However, this can be further investigated and evaluated down the road.\n\nThe same applies for presenting the query result to the user. Presently, a fetch operator is used on the client side to fetch rows from the temporary file (produced by FileSink in the query plan). It's possible to have the FileSink to generate an in-memory RDD instead and the fetch operator can directly read rows from the RDD. Again this can be investigated and implemented as a future work.\n```\n\n----------------------------------------\n\nTITLE: Using storePasswordPath for Secure Password Storage in Hive JDBC\nDESCRIPTION: This snippet shows how to use the storePasswordPath option in a Hive JDBC connection URL. This allows for secure storage of truststore and keystore passwords in a separate file, enhancing security.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_52\n\nLANGUAGE: java\nCODE:\n```\njdbc:hive2://<host>:<port>/<db>;ssl=true;twoWay=true;sslTrustStore=<trust_store_path>;sslKeyStore=<key_store_path>;storePasswordPath=store_password_path>;transportMode=http;httpPath=<http_endpoint>\n```\n\n----------------------------------------\n\nTITLE: Avro Serialization Configuration Properties for HBase Columns\nDESCRIPTION: Key configuration properties required to tell Hive how to handle Avro-serialized data in HBase columns, including serialization type, schema location, and auto-generation of struct fields.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\n\"test_col_fam.test_col.serialization.type\" = \"avro\"\n```\n\nLANGUAGE: sql\nCODE:\n```\n\"test_col_fam.test_col.avro.schema.url\" = \"hdfs://testcluster/tmp/schema.avsc\"\n```\n\nLANGUAGE: sql\nCODE:\n```\n\"hbase.struct.autogenerate\" = \"true\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Hive Table for Avro Data Stored in HBase Columns\nDESCRIPTION: SQL statement to create an external Hive table that can read and write Avro objects stored in HBase columns. The configuration includes properties for Avro serialization and schema location.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE test_hbase_avro\nROW FORMAT SERDE 'org.apache.hadoop.hive.hbase.HBaseSerDe' \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' \nWITH SERDEPROPERTIES (\n\t\"hbase.columns.mapping\" = \":key,test_col_fam:test_col\", \n\t\"test_col_fam.test_col.serialization.type\" = \"avro\",\n\t\"test_col_fam.test_col.avro.schema.url\" = \"hdfs://testcluster/tmp/schema.avsc\")\nTBLPROPERTIES (\n    \"hbase.table.name\" = \"hbase_avro_table\",\n    \"hbase.mapred.output.outputtable\" = \"hbase_avro_table\",\n    \"hbase.struct.autogenerate\"=\"true\");\n```\n\n----------------------------------------\n\nTITLE: Hive DOUBLE PRECISION Type Definition\nDESCRIPTION: Defines the DOUBLE PRECISION type which is an alias for DOUBLE, available from Hive 2.2.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nDOUBLE PRECISION (alias for DOUBLE, only available starting with Hive [2.2.0])\n```\n\n----------------------------------------\n\nTITLE: Configuring 2-way SSL JDBC Connection for Hive\nDESCRIPTION: This snippet demonstrates how to set up a JDBC connection URL for Hive with 2-way SSL authentication. It includes parameters for SSL, truststore, keystore, and HTTP transport mode.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_51\n\nLANGUAGE: java\nCODE:\n```\njdbc:hive2://<host>:<port>/<db>;ssl=true;twoWay=true;sslTrustStore=<trust_store_path>;trustStorePassword=<trust_store_password>;sslKeyStore=<key_store_path>;keyStorePassword=<key_store_password>;transportMode=http;httpPath=<http_endpoint>\n```\n\n----------------------------------------\n\nTITLE: Getting Last Day of Month in Hive\nDESCRIPTION: Returns the last day of the month for a given date. Accepts date string in 'yyyy-MM-dd HH:mm:ss' or 'yyyy-MM-dd' format. Time component is ignored.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_37\n\nLANGUAGE: sql\nCODE:\n```\nlast_day(string date)\n```\n\n----------------------------------------\n\nTITLE: Creating Local Table from Remote Data in Hive\nDESCRIPTION: Executes a CREATE TABLE AS SELECT (CTAS) statement to create a local table 'emr_clone' in the 'default' database by copying all data from the remote 'test_emr_tbl' table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/data-connector-for-hive-and-hive-like-engines_288885794.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table default.emr_clone as select * from test_emr_tbl;\n```\n\n----------------------------------------\n\nTITLE: Locating HiveServer2 Thrift IDL File\nDESCRIPTION: This snippet provides the GitHub repository path to the Thrift IDL file for TCLIService, which defines the interface for HiveServer2.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-overview_65147648.md#2025-04-09_snippet_1\n\nLANGUAGE: Thrift\nCODE:\n```\nhttps://github.com/apache/hive/blob/master/service-rpc/if/TCLIService.thrift\n```\n\n----------------------------------------\n\nTITLE: JSON Data Structure Example in Hive\nDESCRIPTION: Example of a JSON data structure stored in a single-column table named src_json, showing nested objects with arrays and primitive values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_47\n\nLANGUAGE: json\nCODE:\n```\n{\n\"store\": {\n  \"fruit\":[{\"weight\":8,\"type\":\"apple\"},{\"weight\":9,\"type\":\"pear\"}],\n   \"bicycle\":{\"price\":19.95,\"color\":\"red\"}\n  },\n \"email\":\"amy@only_for_json_udf_test.net\",\n \"owner\":\"amy\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive to Use TiDB as Metastore Database\nDESCRIPTION: XML configuration for hive-site.xml that specifies TiDB connection parameters, including connection URL, username, password, and driver class. This is the minimum required configuration for connecting Hive to TiDB.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/using-tidb-as-the-hive-metastore-database_158872426.md#2025-04-09_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n  <property>\n    <name>javax.jdo.option.ConnectionURL</name>\n    <value>jdbc:mysql://host:port/hive</value>\n    <description>TiDB address</description>\n  </property>\n\n  <property>  \n    <name>javax.jdo.option.ConnectionUserName</name>\n    <value>hive</value>\n    <description>TiDB username</description>\n  </property>\n\n  <property>  \n    <name>javax.jdo.option.ConnectionPassword</name>\n    <value>123456</value>\n    <description>TiDB password</description>\n  </property>\n\n  <property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n    <value>com.mysql.jdbc.Driver</value>\n  </property>\n\n  <property>\n    <name>hive.metastore.uris</name>\n    <value>thrift://localhost:9083</value>\n  </property>\n\n  <property>\n    <name>hive.metastore.schema.verification</name>\n    <value>false</value>\n  </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Build Infrastructure for Hive on Spark Integration\nDESCRIPTION: Describes the dependency management approach for Spark integration with Hive, outlining how Spark jars will be handled at compile and runtime, and addressing potential library conflict challenges.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n### Build Infrastructure\n\nThere will be a new \"ql\" dependency on Spark. Currently Spark client library comes in a single jar. The spark jar will be handled the same way Hadoop jars are handled: they will be used during compile, but not included in the final distribution. Rather we will depend on them being installed separately. The spark jar will only have to be present to run Spark jobs, they are not needed for either MapReduce or Tez execution.\n\nOn the other hand, to run Hive code on Spark, certain Hive libraries and their dependencies need to be distributed to Spark cluster by calling SparkContext.addJar() method. As Spark also depends on Hadoop and other libraries, which might be present in Hive's dependents yet with different versions, there might be some challenges in identifying and resolving library conflicts. Jetty libraries posted such a challenge during the prototyping.\n```\n\n----------------------------------------\n\nTITLE: Managing Task Numbers in Spark-based Hive Execution\nDESCRIPTION: Explains how the number of tasks (particularly reducers) will be managed in Spark, maintaining compatibility with the approach used in MapReduce and Tez.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n### Number of Tasks\n\nAs specified above, Spark transformations such as partitionBy will be used to connect mapper-side's operations to reducer-side's operations. The number of partitions can be optionally given for those transformations, which basically dictates the number of reducers.\n\nThe determination of the number of reducers will be the same as it's for MapReduce and Tez.\n```\n\n----------------------------------------\n\nTITLE: DataNucleus Schema Validation Settings\nDESCRIPTION: Configuration parameters for DataNucleus schema validation including table, column and constraint validation flags.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_39\n\nLANGUAGE: properties\nCODE:\n```\ndatanucleus.schema.validateTables=false\ndatanucleus.schema.validateColumns=false\ndatanucleus.schema.validateConstraints=false\ndatanucleus.schema.autoCreateAll=false\n```\n\n----------------------------------------\n\nTITLE: Creating Hive Table with CSV Serde and Custom Properties\nDESCRIPTION: SQL command to create a Hive table using the OpenCSVSerde with custom separator, quote, and escape characters. This specifies tab as separator, single quote as quote character, and backslash as escape character.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/csv-serde_48202659.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE my_table(a string, b string, ...)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n   \"separatorChar\" = \"\\t\",\n   \"quoteChar\"     = \"'\",\n   \"escapeChar\"    = \"\\\\\"\n)   \nSTORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: Setting HiveServer2 Thrift Interface Port\nDESCRIPTION: Specifies the port number for HiveServer2's Thrift interface. This can be overridden by setting the HIVE_SERVER2_THRIFT_PORT environment variable.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_41\n\nLANGUAGE: properties\nCODE:\n```\nhive.server2.thrift.port=10000\n```\n\n----------------------------------------\n\nTITLE: Defining Labeled Worker Pools in Apache Hive\nDESCRIPTION: This configuration setting allows defining labeled worker pools with a specific thread count. The poolname in the property is dynamic and can be set to any desired pool name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/compaction-pooling_240884493.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nhive.compactor.worker.{poolname}.threads={thread_count}\n```\n\n----------------------------------------\n\nTITLE: Creating Wiki Events Table in Hive\nDESCRIPTION: Creates a table structure for storing wiki edit events with timestamp and character change tracking\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/materialized-views-in-hive_283118346.md#2025-04-09_snippet_7\n\nLANGUAGE: HiveQL\nCODE:\n```\nCREATE TABLE wiki (\ntime TIMESTAMP,\npage STRING,\nuser STRING,\ncharacters_added BIGINT,\ncharacters_removed BIGINT)\nSTORED AS ORC\nTBLPROPERTIES ('transactional'='true');\n```\n\n----------------------------------------\n\nTITLE: Building Hive from Source using Ant\nDESCRIPTION: Command to build Hive from source code using Apache Ant for versions 0.12.0 and earlier.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-installation_27362077.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ant package\n```\n\n----------------------------------------\n\nTITLE: HCatalog Server Configuration\nDESCRIPTION: XML configuration settings for enabling JMS notifications in HCatalog server.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-notification_34014558.md#2025-04-09_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n<name>hive.metastore.event.expiry.duration</name>\n<value>300L</value>\n<description>Duration after which events expire from events table (in seconds)</description>\n</property>\n\n<property>\n<name>hive.metastore.event.clean.freq</name>\n<value>360L</value>\n<description>Frequency at which timer task runs to purge expired events in metastore (in seconds).</description>\n</property>\n\n<property>\n<name>msgbus.brokerurl</name>\n<value>tcp://localhost:61616</value>\n<description></description>\n</property>\n\n<property>\n<name>msgbus.username</name>\n<value></value>\n<description></description>\n</property>\n\n<property>\n<name>msgbus.password</name>\n<value></value>\n<description></description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Resource Listing Commands in Beeline\nDESCRIPTION: Commands for listing and checking resources in the distributed cache.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_6\n\nLANGUAGE: hql\nCODE:\n```\nlist FILE[S]\nlist JAR[S]\nlist ARCHIVE[S]\nlist FILE[S] <filepath>*\nlist JAR[S] <filepath>*\nlist ARCHIVE[S] <filepath>*\n```\n\n----------------------------------------\n\nTITLE: Building Spark Distribution Without Hive (Spark 2.0.0+)\nDESCRIPTION: Command to build Spark distribution without Hive dependencies for Spark 2.0.0 and later versions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/44302539.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./dev/make-distribution.sh --name \"hadoop2-without-hive\" --tgz \"-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided\"\n```\n\n----------------------------------------\n\nTITLE: Creating Hive Table with Binary Default Storage Type\nDESCRIPTION: Example of creating a Hive table with binary as the default storage type, while explicitly specifying string type for specific columns using the #s suffix in the column mapping.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE hbase_table_1 (key int, value string, foobar double)\nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n\"hbase.columns.mapping\" = \":key,cf:val#s,cf:foo\",\n\"hbase.table.default.storage.type\" = \"binary\"\n);\n\n```\n\n----------------------------------------\n\nTITLE: Querying Tables Using Curl - WebHCat API Example\nDESCRIPTION: Example curl command to list tables from the default database whose names start with 'm'\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-gettables_34016290.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table?user.name=ctdean&like=m*'\n```\n\n----------------------------------------\n\nTITLE: Setting Custom HTTP Cookies in Hive JDBC Connection\nDESCRIPTION: This snippet shows how to include custom HTTP cookies in a Hive JDBC connection URL. This feature is useful for authentication mechanisms like Single Sign On that require passing cookies to intermediate services.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_55\n\nLANGUAGE: java\nCODE:\n```\njdbc:hive2://<host>:<port>/<db>;transportMode=http;httpPath=<http_endpoint>;http.cookie.<name1>=<value1>;http.cookie.<name2>=<value2>\n```\n\n----------------------------------------\n\nTITLE: Defining Block Sampling Syntax in HiveQL\nDESCRIPTION: This snippet shows the syntax for block sampling in Hive, which allows sampling a percentage of data based on size rather than row count.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nblock_sample: TABLESAMPLE (n PERCENT)\n```\n\n----------------------------------------\n\nTITLE: Hive Export and Import of Table Partition Example\nDESCRIPTION: Example demonstrating how to export a specific partition of a table and import it.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-importexport_27837968.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nexport table employee partition (emp_country=\"in\", emp_state=\"ka\") to 'hdfs_exports_location/employee';\nimport from 'hdfs_exports_location/employee';\n\n```\n\n----------------------------------------\n\nTITLE: Building Hive Client with Maven\nDESCRIPTION: Alternative command to compile the Hive client using Maven build system, specifying Thrift and Boost home directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd odbc\n$ mvn compile -Podbc,hadoop-1 -Dthrift.home=/usr/local -Dboost.home=/usr/local\n```\n\n----------------------------------------\n\nTITLE: Error JSON Response from WebHCat Database Description API\nDESCRIPTION: Example of an error JSON response from the WebHCat API when the requested database does not exist. It includes an error message and a 404 error code.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getdb_34016250.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": \"No such database: newdb\",\n  \"errorCode\": 404\n}\n```\n\n----------------------------------------\n\nTITLE: Computing Bucket IDs for Insert Operations in Java\nDESCRIPTION: This code reference shows how to use the BucketIdResolver and BucketIdResolverImpl classes to compute and append bucket IDs for insert operations, ensuring proper distribution and consistency with Hive's bucketing scheme.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118454.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nBucketIdResolver\nBucketIdResolverImpl\n```\n\n----------------------------------------\n\nTITLE: Creating Markdown Table for Hive-Tez Version Compatibility\nDESCRIPTION: This markdown snippet creates a table showing the compatibility between different versions of Apache Hive and Apache Tez. It includes Hive versions from 0.13 to 2.0 and their corresponding compatible Tez versions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-tez-compatibility_59689974.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Hive | (Works with) Tez |\n| --- | --- |\n| 0.13 | 0.4.0-incubating |\n| 0.14 | 0.5.2+, (through 0.7.0) |\n| 1.0 | 0.5.2, (through 0.7.0) |\n| 1.1 | 0.5.2, (through 0.7.0) |\n| 1.2* | 0.5.3, (through 0.7.0) |\n| 2.0 | 0.8.2 |\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Listener\nDESCRIPTION: Implementation of JMS MessageListener interface to handle incoming partition event messages.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-notification_34014558.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic void onMessage(Message msg) {\n  // We are interested in only add_partition events on this table.\n  // So, check message type first.\n  if(msg.getStringProperty(HCatConstants.HCAT_EVENT).equals(HCatConstants.HCAT_ADD_PARTITION_EVENT)){\n       Object obj = (((ObjectMessage)msg).getObject());\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Hadoop Environment Configuration\nDESCRIPTION: Setting Hadoop binary path in /etc/profile.d/hive.sh.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivederbyservermode_27362068.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nHADOOP=/opt/hadoop/hadoop-0.17.2.1/bin/hadoop\nexport HADOOP\n```\n\n----------------------------------------\n\nTITLE: Using xpath_string UDF in Hive SQL\nDESCRIPTION: Demonstrates the usage of xpath_string UDF to extract the text of the first matching node in an XML string.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-xpathudf_27362051.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT xpath_string ('<a><b>bb</b><c>cc</c></a>', 'a/b') FROM src LIMIT 1 ;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Column Size Metadata from RC Files\nDESCRIPTION: Command used to view metadata about column sizes in an RC file instead of the actual data. Available in Hive 0.11.0 and later, it provides options to display the column sizes in different formats.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/rcfilecat_30748712.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhive --rcfilecat [--column-sizes | --column-sizes-pretty] fileName\n```\n\n----------------------------------------\n\nTITLE: Using the array_remove() function in Hive SQL\nDESCRIPTION: Removes all occurrences of a specified element from an array. Example: array_remove(array(1, 2, 3, 4, 2), 2) returns [1, 3, 4].\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_66\n\nLANGUAGE: SQL\nCODE:\n```\narray_remove(array, element)\n```\n\n----------------------------------------\n\nTITLE: Show Granted Privileges in Hive SQL\nDESCRIPTION: SQL syntax for viewing granted privileges in Hive. Allows querying privileges for specific users, groups, or roles on particular tables or databases. Supports column-level privilege viewing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/45876173.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSHOW GRANT principal_specification\n[ON object_specification [(column_list)]]\n \nprincipal_specification:\n    USER user\n  | GROUP group\n  | ROLE role\n \nobject_specification:\n    TABLE tbl_name\n  | DATABASE db_name\n```\n\n----------------------------------------\n\nTITLE: Querying ODBC Configuration with odbcinst\nDESCRIPTION: Command to print the locations of Driver Manager configuration files including drivers, system data sources, and user data sources.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ odbcinst -j\nunixODBC 2.2.14\nDRIVERS............: /usr/local/etc/odbcinst.ini\nSYSTEM DATA SOURCES: /usr/local/etc/odbc.ini\nFILE DATA SOURCES..: /usr/local/etc/ODBCDataSources\nUSER DATA SOURCES..: /home/ehwang/.odbc.ini\nSQLULEN Size.......: 8\nSQLLEN Size........: 8\nSQLSETPOSIROW Size.: 8\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Links for Hive Release Changelogs\nDESCRIPTION: Collection of markdown reference-style link definitions that map version identifiers to their corresponding JIRA release notes URLs. Each link follows the pattern [HIVE_VERSION_CL] pointing to the Apache JIRA release notes page.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/general/downloads.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[HIVE_4_0_0_B_1_CL]: https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12353351&styleName=Text&projectId=12310843\n[HIVE_4_0_0_A_2_CL]: https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12351489&styleName=Html&projectId=12310843\n[HIVE_3_1_3_CL]: https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12346277&styleName=Html&projectId=12310843\n```\n\n----------------------------------------\n\nTITLE: Creating a Remote Database in Hive SQL\nDESCRIPTION: This SQL statement creates a remote database in Hive, exposing all tables from a remote database to the local Hive cluster. It specifies the connection method and properties for accessing the remote metastore.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/80452092.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE REMOTE DATABASE local_db_name\nCONNECTED TO remote_db_name\nVIA 'org.apache.hadoop.hive.metastore.ThriftHiveMetastoreClientFactory'\nWITH DBPROPERTIES (\n  'hive.metastore.uris' = 'thrift://remote-hms:9083'\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Jar Directory for Tez Mode\nDESCRIPTION: Sets the location where Hive in Tez mode will look for a site-wide installed Hive instance.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_63\n\nLANGUAGE: properties\nCODE:\n```\nhive.jar.directory = null\n```\n\n----------------------------------------\n\nTITLE: Inline Array of Structs Expansion in Hive\nDESCRIPTION: Examples of using the inline function to expand arrays of structs into multiple rows with named columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_52\n\nLANGUAGE: HiveQL\nCODE:\n```\nselect inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02')));\nselect inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) as (col1,col2,col3);\nselect tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf;\nselect tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf as col1,col2,col3;\n```\n\n----------------------------------------\n\nTITLE: Copying MySQL JDBC Driver to Hive Library Directory\nDESCRIPTION: Command to copy the MySQL JDBC driver to Hive's lib directory, which is necessary since TiDB is compatible with the MySQL protocol and requires this driver for connectivity.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/using-tidb-as-the-hive-metastore-database_158872426.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp ${MYSQL_JDBC_PATH}/mysql-connector-java-${version}.jar ${HIVE_HOME}/lib\n```\n\n----------------------------------------\n\nTITLE: Setting Table Properties in Hive\nDESCRIPTION: SQL statement to set or modify table properties in Hive. Used to add custom metadata to tables, with automatic tracking of last_modified_user and last_modified_time properties.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name SET TBLPROPERTIES table_properties;\n\ntable_properties:\n  : (property_name = property_value, property_name = property_value, ... )\n```\n\n----------------------------------------\n\nTITLE: Creating Indexed Accumulo-Backed Hive Table\nDESCRIPTION: Creates a table with comprehensive indexing support using a separate Accumulo index table for efficient searching across multiple data types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/accumulointegration_46633569.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE company_stats (\n   rowid string,\n   active_entry boolean,\n   num_offices tinyint,\n   num_personel smallint,\n   total_manhours int,\n   num_shareholders bigint,\n   eff_rating float,\n   err_rating double,\n   yearly_production decimal,\n   start_date date,\n   address varchar(100),\n   phone char(13),\n   last_update timestamp )\nROW FORMAT SERDE 'org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe'\nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES (\n   \"accumulo.columns.mapping\" = \":rowID,a:act,a:off,a:per,a:mhs,a:shs,a:eff,a:err,a:yp,a:sd,a:addr,a:ph,a:lu\",\n   \"accumulo.table.name\"=\"company_stats\",\n   \"accumulo.indextable.name\"=\"company_stats_idx\"\n );\n```\n\n----------------------------------------\n\nTITLE: Restarting Jenkins Service in Bash\nDESCRIPTION: Command to restart the Jenkins service on the Hive PTest2 Infrastructure master. This is useful when Jenkins needs to be restarted for configuration changes or troubleshooting.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-ptest2-infrastructure_33295254.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo service jenkins restart\n```\n\n----------------------------------------\n\nTITLE: Inline Array of Structs Expansion in Hive\nDESCRIPTION: Examples of using the inline function to expand arrays of structs into multiple rows with named columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_52\n\nLANGUAGE: HiveQL\nCODE:\n```\nselect inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02')));\nselect inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) as (col1,col2,col3);\nselect tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf;\nselect tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf as col1,col2,col3;\n```\n\n----------------------------------------\n\nTITLE: Multi-Group-By Inserts in Hive\nDESCRIPTION: Illustrates how to perform multiple GROUP BY operations in a single query and insert results into different tables or directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nFROM pv_users \nINSERT OVERWRITE TABLE pv_gender_sum\n  SELECT pv_users.gender, count(DISTINCT pv_users.userid) \n  GROUP BY pv_users.gender \nINSERT OVERWRITE DIRECTORY '/user/facebook/tmp/pv_age_sum'\n  SELECT pv_users.age, count(DISTINCT pv_users.userid) \n  GROUP BY pv_users.age; \n```\n\n----------------------------------------\n\nTITLE: Example 1: Query with Multiple Group By and Join Operations\nDESCRIPTION: SQL query demonstrating correlation between GROUP BY and JOIN operations where data shuffling can be optimized.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/correlation-optimizer_34019487.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT tmp1.key, count(*)\nFROM (SELECT key, avg(value) AS avg\n      FROM t1\n      GROUP BY /*AGG1*/ key) tmp1\nJOIN /*JOIN1*/ t1 ON (tmp1.key = t2.key)\nWHERE t1.value > tmp1.avg\nGROUP BY /*AGG2*/ tmp1.key;\n```\n\n----------------------------------------\n\nTITLE: Enabling Vectorized Execution in Hive SQL\nDESCRIPTION: SQL command to enable vectorized query execution in Hive. This setting must be turned on to utilize vectorized execution for queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/vectorized-query-execution_34838326.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.vectorized.execution.enabled = true;\n```\n\n----------------------------------------\n\nTITLE: Storage API Release Artifact Creation\nDESCRIPTION: Commands for downloading, packaging, and creating release artifacts with checksums\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/apache/hive/archive/storage-release-X.Y.Z-rcR.tar.gz\ntar xzvf storage-release-X.Y.Z-rcR.tar.gz\nmv storage-release-X.Y.Z-rcR/storage-api hive-storage-X.Y.Z\ntar czvf hive-storage-X.Y.Z-rcR.tar.gz hive-storage-X.Y.Z\nshasum -a 256 hive-storage-X.Y.Z-rcR.tar.gz > hive-storage-X.Y.Z-rcR.tar.gz.sha256\n```\n\n----------------------------------------\n\nTITLE: Listing All Accessible Jobs with WebHCat REST API - Show All\nDESCRIPTION: Curl command demonstrating how to list all jobs accessible to a user with the showall parameter\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-jobs_34835057.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/jobs?user.name=daijy&showall=true'\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n{\"id\":\"job_201304291205_0014\",\"detail\":null},\n{\"id\":\"job_201111111311_0015\",\"detail\":null},\n]\n```\n\n----------------------------------------\n\nTITLE: Cluster Properties Configuration JSON Structure\nDESCRIPTION: JSON configuration parameter structure for defining cluster properties including FileSystem and JobTracker mappings for each cluster name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/27837073.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"hive.cluster.properties\": {\"ClusterName\": {\"FileSystem\": \"fs_value\", \"JobTracker\": \"jt_value\"}}\n```\n\n----------------------------------------\n\nTITLE: Grant and Revoke Privileges in Hive SQL\nDESCRIPTION: SQL syntax for granting and revoking privileges in Hive. Supports various privilege types (ALL, ALTER, UPDATE, etc.) and can be applied to tables or databases. Includes support for multiple principals and grant option capability.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/45876173.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nGRANT\n    priv_type [(column_list)]\n      [, priv_type [(column_list)]] ...\n    [ON object_specification]\n    TO principal_specification [, principal_specification] ...\n    [WITH GRANT OPTION]\n\nREVOKE [GRANT OPTION FOR]\n    priv_type [(column_list)]\n      [, priv_type [(column_list)]] ...\n    [ON object_specification]\n    FROM principal_specification [, principal_specification] ...\n\nREVOKE ALL PRIVILEGES, GRANT OPTION\n    FROM user [, user] ...\n\npriv_type:\n    ALL | ALTER | UPDATE | CREATE | DROP\n  | INDEX | LOCK | SELECT | SHOW_DATABASE \n \nobject_specification:\n    TABLE tbl_name\n  | DATABASE db_name\n \nprincipal_specification:\n    USER user\n  | GROUP group\n  | ROLE role\n```\n\n----------------------------------------\n\nTITLE: Simplified Remote Table Creation in Hive SQL\nDESCRIPTION: This SQL statement shows a simplified syntax for creating a remote table in Hive, deriving the local database name from the user's currently selected database and assuming the remote table name is the same as the local name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/80452092.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE REMOTE TABLE tbl_name\n    WITH TBLPROPERTIES (\n      'hive.metastore.uris' = 'thrift://remote-hms:9083'\n    );\n```\n\n----------------------------------------\n\nTITLE: Creating HCatWriter on Slave Nodes in Java\nDESCRIPTION: This snippet shows how slave nodes obtain an HCatWriter instance using the WriterContext that was prepared on the master node. This is part of the parallel writing process.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-readerwriter_34013921.md#2025-04-09_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nHCatWriter writer = DataTransferFactory.getHCatWriter(context);\n```\n\n----------------------------------------\n\nTITLE: Building Hive from Source using Maven\nDESCRIPTION: Commands to clone and build Hive from source code using Maven build system.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://git-wip-us.apache.org/repos/asf/hive.git\n$ cd hive\n$ mvn clean package -Pdist [-DskipTests -Dmaven.javadoc.skip=true]\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Credential Provider Path\nDESCRIPTION: XML configuration snippet to specify the path to the JCEKS keystore file for credential storage in Hive configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-configuration_27362070.md#2025-04-09_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n  <!-- Configure credential store for passwords-->\n  <property>\n    <name>hadoop.security.credential.provider.path</name>\n    <value>jceks://file/usr/lib/hive/conf/hive.jceks</value>\n  </property>\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Metadata-Only Dumps for Hive Replication in XML\nDESCRIPTION: Controls whether the REPL DUMP command dumps only metadata information (true) or both data and metadata (false) during Hive replication operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_110\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.repl.dump.metadata.only</name>\n  <value>false</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Managing Hive PTest2 WebServer in Bash\nDESCRIPTION: Commands for stopping, starting, restarting the Hive PTest2 WebServer, and updating it with the latest test infrastructure code. These commands are essential for maintaining and updating the PTest2 WebServer.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-ptest2-infrastructure_33295254.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo /usr/local/hiveptest/bin/stop-server.sh \n$ sudo /usr/local/hiveptest/bin/start-server.sh \n$ sudo /usr/local/hiveptest/bin/restart-server.sh \n$ sudo /usr/local/hiveptest/bin/update.sh \n```\n\n----------------------------------------\n\nTITLE: Building Hive Client with Ant\nDESCRIPTION: Command to compile the Hive client using Ant build system. This step is necessary before building the unixODBC API wrapper.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ ant compile-cpp -Dthrift.home=<THRIFT_HOME>\n```\n\n----------------------------------------\n\nTITLE: Setting ORC Dictionary Encoding Threshold\nDESCRIPTION: Configuration to control when dictionary encoding is used in ORC files based on the fraction of unique keys to total non-null rows.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_31\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.orc.dictionary.key.size.threshold=0.8\n```\n\n----------------------------------------\n\nTITLE: Setting PYTHONPATH for Python 2.6 Local Modules\nDESCRIPTION: Example of updating PYTHONPATH for Python 2.6 specifically to include locally installed modules.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport PYTHONPATH=\"${PYTHONPATH}:${HOME}/.python_modules/lib/python2.6/site-packages\"\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Secure Hive Streaming Connection with Kerberos\nDESCRIPTION: This Java code demonstrates how to set up a secure Hive streaming connection using Kerberos authentication. It requires a UserGroupInformation object for authentication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest_40509746.md#2025-04-09_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nimport org.apache.hadoop.security.UserGroupInformation;\n\nHiveEndPoint hiveEP2 = ... ;\nUserGroupInformation ugi = .. authenticateWithKerberos(principal,keytab);\nStreamingConnection secureConn = hiveEP2.newConnection(true, null, ugi);\n\nDelimitedInputWriter writer3 = new DelimitedInputWriter(fieldNames, \",\", hiveEP2);\n\nTransactionBatch txnBatch3= secureConn.fetchTransactionBatch(10, writer3);\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive JDBC Connection in Pentaho\nDESCRIPTION: This snippet shows the JDBC connection parameters for connecting to a Hive server from Pentaho Report Designer. It specifies the JDBC URL, driver name, and leaves username and password empty as per Hive's configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivejdbcinterface_27362100.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nURL: jdbc:hive://localhost:10000/default\nDriver name: org.apache.hadoop.hive.jdbc.HiveDriver\nUsername and password are empty\n```\n\n----------------------------------------\n\nTITLE: Setting Up Message Consumer\nDESCRIPTION: Creates a JMS session and message consumer to receive notifications for a specific topic.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-notification_34014558.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nSession session = conn.createSession(true, Session.SESSION_TRANSACTED);\nDestination hcatTopic = session.createTopic(topicName);\nMessageConsumer consumer = session.createConsumer(hcatTopic);\nconsumer.setMessageListener(this);\n```\n\n----------------------------------------\n\nTITLE: Setting Query Plan Transmission Method for Hive\nDESCRIPTION: Configures whether to send the query plan via local resource or RPC in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_66\n\nLANGUAGE: properties\nCODE:\n```\nhive.rpc.query.plan = false\n```\n\n----------------------------------------\n\nTITLE: Launching Hive with Kudu Configuration\nDESCRIPTION: Command to start Hive with Kudu master address configuration using the -hiveconf parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/kudu-integration_133631955.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhive -hiveconf hive.kudu.master.addresses.default=localhost:7051\n```\n\n----------------------------------------\n\nTITLE: Querying context-specific ngrams results\nDESCRIPTION: Example showing how to use context_ngrams to find words that commonly follow \"he\" in text data, displaying the top 10 results with their frequencies.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statisticsanddatamining_27362058.md#2025-04-09_snippet_4\n\nLANGUAGE: hql\nCODE:\n```\nSELECT explode(context_ngrams(sentences(lower(val)), array(\"he\", null), 10)) AS x FROM kafka;\n{\"ngram\":[was],\"estfrequency\":17.0}\n{\"ngram\":[had],\"estfrequency\":16.0}\n{\"ngram\":[thought],\"estfrequency\":13.0}\n{\"ngram\":[could],\"estfrequency\":9.0}\n{\"ngram\":[would],\"estfrequency\":7.0}\n{\"ngram\":[lay],\"estfrequency\":5.0}\n{\"ngram\":[s],\"estfrequency\":4.0}\n{\"ngram\":[wanted],\"estfrequency\":4.0}\n{\"ngram\":[did],\"estfrequency\":4.0}\n{\"ngram\":[felt],\"estfrequency\":4.0}\n```\n\n----------------------------------------\n\nTITLE: Obtaining HCatReader Instance in Java\nDESCRIPTION: This snippet shows how to obtain an instance of HCatReader using a previously defined ReadEntity and the cluster configuration. This is part of the first step in the reading process that occurs on the master node.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-readerwriter_34013921.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nHCatReader reader = DataTransferFactory.getHCatReader(entity, config);\n```\n\n----------------------------------------\n\nTITLE: Creating a Remote Table in Hive SQL\nDESCRIPTION: This SQL statement creates a remote table in Hive, linking a local table to a remote one. It specifies the connection method and properties for accessing the remote metastore.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/80452092.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE REMOTE TABLE local_db.local_tbl\nCONNECTED TO remote_db.remote_tbl\nVIA 'org.apache.hadoop.hive.metastore.ThriftHiveMetastoreClientFactory'\nWITH TBLPROPERTIES (\n  'hive.metastore.uris' = 'thrift://remote-hms:9083'\n);\n```\n\n----------------------------------------\n\nTITLE: Inserting Data to Local Directory in Hive\nDESCRIPTION: Shows how to write query results to a local directory instead of a Hive table for external analysis.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/pv_gender_sum'\nSELECT pv_gender_sum.*\nFROM pv_gender_sum;\n```\n\n----------------------------------------\n\nTITLE: Security Error Response Format in WebHCat\nDESCRIPTION: JSON error response format returned when user authentication parameter is missing in the request.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-usingwebhcat_34015492.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": \"No user found.  Missing user.name parameter.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting to Maven Surefire Debug Port\nDESCRIPTION: Shows the debug port listening message that appears when running Maven tests with the debug option enabled. This message indicates that surefire is waiting for a remote debugger connection on port 5005.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nListening for transport dt_socket at address: 5005\n```\n\n----------------------------------------\n\nTITLE: Updating Version in Maven POM Files\nDESCRIPTION: Command to update the version property in all pom.xml files using Maven's Versions plugin.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmvn versions:set -DnewVersion=X.Y.0-SNAPSHOT -DgenerateBackupPoms=false\n```\n\n----------------------------------------\n\nTITLE: Union All Operation in Hive\nDESCRIPTION: Shows how to combine data from multiple tables using UNION ALL and JOIN operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\nINSERT OVERWRITE TABLE actions_users\nSELECT u.id, actions.date\nFROM (\n    SELECT av.uid AS uid\n    FROM action_video av\n    WHERE av.date = '2008-06-03'\n\n    UNION ALL\n\n    SELECT ac.uid AS uid\n    FROM action_comment ac\n    WHERE ac.date = '2008-06-03'\n    ) actions JOIN users u ON(u.id = actions.uid);\n```\n\n----------------------------------------\n\nTITLE: Hive Partition Pruning Implementation Note\nDESCRIPTION: A note about the aggressive approach taken in the algorithm regarding conditional tasks in Hive, highlighting that it assumes conditional tasks are typically only set for joins based on limited exposure to the codebase.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoin-and-partition-pruning_34015666.md#2025-04-09_snippet_2\n\nLANGUAGE: pseudo\nCODE:\n```\nThis is aggressive; in my limited exposure to the hive code, it seemed like conditional tasks are currently set only for joins.\n```\n\n----------------------------------------\n\nTITLE: Implementing IndexSearchCondition Class in Java\nDESCRIPTION: Definition of the IndexSearchCondition class used to represent search conditions in the form of 'column-ref comparison-op constant-value'. This class is used in conjunction with IndexPredicateAnalyzer for filter analysis.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/filterpushdowndev_27362092.md#2025-04-09_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic class IndexSearchCondition\n{\n  /**\n * Constructs a search condition, which takes the form\n * <pre>column-ref comparison-op constant-value</pre>.\n   *\n * @param columnDesc column being compared\n   *\n * @param comparisonOp comparison operator, e.g. \"=\"\n * (taken from GenericUDFBridge.getUdfName())\n   *\n * @param constantDesc constant value to search for\n   *\n * @Param comparisonExpr the original comparison expression\n   */\n  public IndexSearchCondition(\n    ExprNodeColumnDesc columnDesc,\n    String comparisonOp,\n    ExprNodeConstantDesc constantDesc,\n    ExprNodeDesc comparisonExpr);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting a Specific Role in Hive Authorization\nDESCRIPTION: Command reference for the 'set role' command which allows users to change their current active role. This is particularly important for users who need to activate the admin role, as it is not active by default.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nset role\n```\n\n----------------------------------------\n\nTITLE: Setting Top K Pool Size in Hive\nDESCRIPTION: HQL command to specify the number of values to monitor when computing top K. A larger pool size increases the accuracy of top K estimation at the cost of memory usage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_3\n\nLANGUAGE: hql\nCODE:\n```\nset hive.stats.topk.poolsize=200;\n```\n\n----------------------------------------\n\nTITLE: HiveStreamingConnection Concurrency Note\nDESCRIPTION: Important concurrency considerations for using the streaming connection APIs and record writer APIs, highlighting thread-safety requirements.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest-v2_85477610.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n* CONCURRENCY NOTE: The streaming connection APIs and record writer APIs are not thread-safe. Streaming connection creation,  \n* begin/commit/abort transactions, write and close has to be called in the same thread. If close() or  \n* abortTransaction() has to be triggered from a separate thread it has to be co-ordinated via external variables or  \n* synchronization mechanism\n```\n\n----------------------------------------\n\nTITLE: Exploding histogram_numeric results for visualization\nDESCRIPTION: Example showing the output format of the histogram_numeric function, which returns bin centers (x) and heights (y) for numeric data distributions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statisticsanddatamining_27362058.md#2025-04-09_snippet_7\n\nLANGUAGE: hql\nCODE:\n```\nSELECT explode(histogram_numeric(val, 10)) AS x FROM normal;\n{\"x\":-3.6505464999999995,\"y\":20.0}\n{\"x\":-2.7514727901960785,\"y\":510.0}\n{\"x\":-1.7956678951954481,\"y\":8263.0}\n{\"x\":-0.9878507685761995,\"y\":19167.0}\n{\"x\":-0.2625338380837097,\"y\":31737.0}\n{\"x\":0.5057392319427763,\"y\":31502.0}\n{\"x\":1.2774146480311135,\"y\":14526.0}\n{\"x\":2.083955560712489,\"y\":3986.0}\n{\"x\":2.9209550254545484,\"y\":275.0}\n{\"x\":3.674835214285715,\"y\":14.0}\n```\n\n----------------------------------------\n\nTITLE: Setting Connect Timeout for Remote Spark Driver to Hive Client\nDESCRIPTION: Configures the timeout for the remote Spark driver when connecting back to the Hive client.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_56\n\nLANGUAGE: properties\nCODE:\n```\nhive.spark.client.connect.timeout = 1000 miliseconds\n```\n\n----------------------------------------\n\nTITLE: Using DSV Format for Query Results in Beeline\nDESCRIPTION: The dsv format in Beeline (available from Hive 0.14) allows for custom delimiters with the default being pipe (|). It uses the same quoting rules as csv2/tsv2 and can be configured with the delimiterForDSV option.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_22\n\nLANGUAGE: text\nCODE:\n```\nid|value|comment\n1|Value1|Test comment 1\n2|Value2|Test comment 2\n3|Value3|Test comment 3\n```\n\n----------------------------------------\n\nTITLE: Creating and Inserting Data into Partitioned Hive Table\nDESCRIPTION: This SQL snippet demonstrates the creation of a partitioned table named 'blah' and subsequent insert operations. It's used to illustrate the concept of rubberbanding in Hive replication, showing how different states of the table can exist at different points in time.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivereplicationv2development_66850051.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE blah (a int) PARTITIONED BY (p string);\nINSERT INTO TABLE blah [PARTITION (p=\"a\") VALUES 5;\nINSERT INTO TABLE blah [PARTITION (p=\"b\") VALUES 10;\nINSERT INTO TABLE blah [PARTITION (p=\"a\") VALUES 15;\n```\n\n----------------------------------------\n\nTITLE: Sample Git Commit Message Format for Hive\nDESCRIPTION: Examples of proper Git commit message formatting for Apache Hive, showing how to attribute contributions and reviews correctly.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocommit_27362108.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nHIVE-123. Add awesomesauce to the optimizer. (jvs, reviewed by Ashutosh Chauhan)\nHIVE-123. Add awesomesauce to the optimizer. (Mike Brakestoner, reviewed by Ashutosh Chauhan)\nCo-authored-by: Ayush Saxena <ayushsaxena@apache.org>\n```\n\n----------------------------------------\n\nTITLE: Describe Partition With Database\nDESCRIPTION: SQL syntax for describing a partition with database specification. Column information follows a space.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_98\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE [EXTENDED|FORMATTED] [db_name.]table_name [column_name] PARTITION partition_spec;\n```\n\n----------------------------------------\n\nTITLE: JPOX Properties Configuration\nDESCRIPTION: JPOX configuration for Derby connection properties in jpox.properties file.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivederbyservermode_27362068.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njavax.jdo.PersistenceManagerFactoryClass=org.jpox.PersistenceManagerFactoryImpl\norg.jpox.autoCreateSchema=false\norg.jpox.validateTables=false\norg.jpox.validateColumns=false\norg.jpox.validateConstraints=false\norg.jpox.storeManagerType=rdbms\norg.jpox.autoCreateSchema=true\norg.jpox.autoStartMechanismMode=checked\norg.jpox.transactionIsolation=read_committed\njavax.jdo.option.DetachAllOnCommit=true\njavax.jdo.option.NontransactionalRead=true\njavax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.ClientDriver\njavax.jdo.option.ConnectionURL=jdbc:derby://hadoop1:1527/metastore_db;create=true\njavax.jdo.option.ConnectionUserName=APP\njavax.jdo.option.ConnectionPassword=mine\n```\n\n----------------------------------------\n\nTITLE: Creating a simple external table over S3 data\nDESCRIPTION: This example demonstrates how to create an external Hive table that points to data stored in an S3 bucket. The table contains key-value pairs with direct mapping to files in S3.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nhive> create external table kv (key int, values string)  location 's3n://data.s3ndemo.hive/kv';\n```\n\n----------------------------------------\n\nTITLE: Querying with SMB Join Across Tables with Different Keys (Error)\nDESCRIPTION: This SQL query demonstrates an error case when using SMB joins with tables having a different number of sort columns. In this example, emp_person has 1 sort column while emp_pay_history has 2, causing an index out of bounds exception.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joinoptimization_33293167.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nSELECT p.*, py.*\nFROM emp_person p INNER JOIN emp_pay_history py\nON   p.empid = py.empid\n```\n\n----------------------------------------\n\nTITLE: Setting Custom LDAP Query for Authentication in HiveServer2\nDESCRIPTION: This XML configuration demonstrates how to set a custom LDAP query string for authentication in HiveServer2. It allows for complex LDAP queries to determine user access based on specific LDAP attributes or group memberships.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hive.server2.authentication.ldap.customLDAPQuery</name>\n  <value><![CDATA[(&(objectClass=person)(|(memberOf=CN=Domain Admins,CN=Users,DC=apache,DC=org)(memberOf=CN=Administrators,CN=Builtin,DC=apache,DC=org)))]]>\n  </value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: WINDOW Clause in HiveQL\nDESCRIPTION: Demonstrates the use of a named WINDOW clause to define a window specification that can be reused in the query.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a, SUM(b) OVER w\nFROM T\nWINDOW w AS (PARTITION BY c ORDER BY d ROWS UNBOUNDED PRECEDING);\n```\n\n----------------------------------------\n\nTITLE: Setting User Install Directory for Hive in Tez Mode\nDESCRIPTION: Configures the directory where Hive in Tez mode will upload the Hive jar if it cannot find a usable one in the hive.jar.directory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_64\n\nLANGUAGE: properties\nCODE:\n```\nhive.user.install.directory = hdfs:///user/\n```\n\n----------------------------------------\n\nTITLE: Casting Expressions to Different Types in Hive\nDESCRIPTION: The cast() function converts the results of an expression to the specified data type. It returns null if the conversion fails. For boolean conversions, Hive returns true for non-empty strings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\ncast(expr as <type>)\n```\n\nLANGUAGE: sql\nCODE:\n```\ncast('1' as BIGINT)\n```\n\n----------------------------------------\n\nTITLE: Building Hive Release Artifacts\nDESCRIPTION: Maven command to build the Hive release artifacts, including binary and source versions, with checksums.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmvn install -Pdist,iceberg -DskipTests -Dmaven.javadoc.skip=true -DcreateChecksum=true\n```\n\n----------------------------------------\n\nTITLE: Executing HCatalog DDL Command with Curl in Apache Hive\nDESCRIPTION: Example curl command for sending a 'show tables' DDL command to the WebHCat DDL endpoint. The command specifies the username as a query parameter and the DDL statement as a form parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-ddl_34015990.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s -d 'exec=show tables;' \\\n       'http://localhost:50111/templeton/v1/ddl?user.name=ekoifman'\n```\n\n----------------------------------------\n\nTITLE: PARTITION BY with Multiple Columns in HiveQL\nDESCRIPTION: Shows how to use PARTITION BY with two partitioning columns and no ORDER BY or window specification in a SELECT statement.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a, COUNT(b) OVER (PARTITION BY c, d)\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: Copying and Committing Hive Javadocs\nDESCRIPTION: Commands to copy generated javadocs to the SVN repository, add them, and commit the changes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nmkdir <hive_javadocs_repo_dir>/rX.Y.Z/\ncd <hive_javadocs_repo_dir>\ncp -r <hive_source_dir>/target/site/apidocs ./rX.Y.Z/api\nsvn add rX.Y.Z\nsvn commit\n```\n\n----------------------------------------\n\nTITLE: Cost-Based CBO Plan Output\nDESCRIPTION: The detailed execution plan output from EXPLAIN CBO COST command, showing operator tree with cost estimates for rows, CPU, and IO operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_7\n\nLANGUAGE: plain text\nCODE:\n```\nCBO PLAN:\nHiveSortLimit(sort0=[$0], dir0=[ASC], fetch=[100]): rowcount = 100.0, cumulative cost = {2.395588892021712E26 rows, 1.197794434438787E26 cpu, 0.0 io}, id = 1683\n  HiveProject(c_customer_id=[$1]): rowcount = 1.1977944344387866E26, cumulative cost = {2.395588892021712E26 rows, 1.197794434438787E26 cpu, 0.0 io}, id = 1681\n    HiveJoin(condition=[AND(=($3, $7), >($4, $6))], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 1.1977944344387866E26, cumulative cost = {1.1977944575829254E26 rows, 4.160211553874922E10 cpu, 0.0 io}, id = 1679\n      HiveJoin(condition=[=($2, $0)], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 2.3144135067474273E18, cumulative cost = {2.3144137967122499E18 rows, 1.921860676139634E10 cpu, 0.0 io}, id = 1663\n        HiveProject(c_customer_sk=[$0], c_customer_id=[$1]): rowcount = 7.2E7, cumulative cost = {2.24E8 rows, 3.04000001E8 cpu, 0.0 io}, id = 1640\n          HiveFilter(condition=[IS NOT NULL($0)]): rowcount = 7.2E7, cumulative cost = {1.52E8 rows, 1.60000001E8 cpu, 0.0 io}, id = 1638\n            HiveTableScan(table=[[default, customer]], table:alias=[customer]): rowcount = 8.0E7, cumulative cost = {8.0E7 rows, 8.0000001E7 cpu, 0.0 io}, id = 1055\n        HiveJoin(condition=[=($3, $1)], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 2.1429754692105807E11, cumulative cost = {2.897408225471977E11 rows, 1.891460676039634E10 cpu, 0.0 io}, id = 1661\n          HiveProject(sr_customer_sk=[$0], sr_store_sk=[$1], $f2=[$2]): rowcount = 6.210443022113779E9, cumulative cost = {7.544327346205959E10 rows, 1.891460312135634E10 cpu, 0.0 io}, id = 1685\n            HiveAggregate(group=[{1, 2}], agg#0=[sum($3)]): rowcount = 6.210443022113779E9, cumulative cost = {6.92328304399458E10 rows, 2.8327405501500005E8 cpu, 0.0 io}, id = 1654\n              HiveJoin(condition=[=($0, $4)], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 6.2104430221137794E10, cumulative cost = {6.2246082040067795E10 rows, 2.8327405501500005E8 cpu, 0.0 io}, id = 1652\n                HiveProject(sr_returned_date_sk=[$0], sr_customer_sk=[$3], sr_store_sk=[$7], sr_fee=[$14]): rowcount = 4.198394835000001E7, cumulative cost = {1.4155904670000002E8 rows, 2.8311809440000004E8 cpu, 0.0 io}, id = 1645\n                  HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($7), IS NOT NULL($3))]): rowcount = 4.198394835000001E7, cumulative cost = {9.957509835000001E7 rows, 1.15182301E8 cpu, 0.0 io}, id = 1643\n                    HiveTableScan(table=[[default, store_returns]], table:alias=[store_returns]): rowcount = 5.759115E7, cumulative cost = {5.759115E7 rows, 5.7591151E7 cpu, 0.0 io}, id = 1040\n                HiveProject(d_date_sk=[$0]): rowcount = 9861.615, cumulative cost = {92772.23000000001 rows, 155960.615 cpu, 0.0 io}, id = 1650\n                  HiveFilter(condition=[AND(=($6, 2000), IS NOT NULL($0))]): rowcount = 9861.615, cumulative cost = {82910.615 rows, 146099.0 cpu, 0.0 io}, id = 1648\n                    HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim]): rowcount = 73049.0, cumulative cost = {73049.0 rows, 73050.0 cpu, 0.0 io}, id = 1043\n          HiveProject(s_store_sk=[$0]): rowcount = 230.04000000000002, cumulative cost = {2164.08 rows, 3639.04 cpu, 0.0 io}, id = 1659\n            HiveFilter(condition=[AND(=($24, _UTF-16LE'NM'), IS NOT NULL($0))]): rowcount = 230.04000000000002, cumulative cost = {1934.04 rows, 3409.0 cpu, 0.0 io}, id = 1657\n              HiveTableScan(table=[[default, store]], table:alias=[store]): rowcount = 1704.0, cumulative cost = {1704.0 rows, 1705.0 cpu, 0.0 io}, id = 1050\n      HiveProject(_o__c0=[*(/($1, $2), 1.2)], ctr_store_sk=[$0]): rowcount = 6.900492246793088E8, cumulative cost = {8.537206083312463E10 rows, 2.2383508777352882E10 cpu, 0.0 io}, id = 1677\n        HiveAggregate(group=[{1}], agg#0=[sum($2)], agg#1=[count($2)]): rowcount = 6.900492246793088E8, cumulative cost = {8.468201160844533E10 rows, 2.1003410327994267E10 cpu, 0.0 io}, id = 1675\n          HiveProject(sr_customer_sk=[$0], sr_store_sk=[$1], $f2=[$2]): rowcount = 6.900492246793088E9, cumulative cost = {8.381945007759619E10 rows, 2.1003410327994267E10 cpu, 0.0 io}, id = 1686\n            HiveAggregate(group=[{1, 2}], agg#0=[sum($3)]): rowcount = 6.900492246793088E9, cumulative cost = {7.69189578308031E10 rows, 3.01933587615E8 cpu, 0.0 io}, id = 1673\n              HiveJoin(condition=[=($0, $4)], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 6.900492246793088E10, cumulative cost = {6.915590405316087E10 rows, 3.01933587615E8 cpu, 0.0 io}, id = 1671\n                HiveProject(sr_returned_date_sk=[$0], sr_customer_sk=[$3], sr_store_sk=[$7], sr_fee=[$14]): rowcount = 4.66488315E7, cumulative cost = {1.50888813E8 rows, 3.01777627E8 cpu, 0.0 io}, id = 1667\n                  HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($7))]): rowcount = 4.66488315E7, cumulative cost = {1.042399815E8 rows, 1.15182301E8 cpu, 0.0 io}, id = 1665\n                    HiveTableScan(table=[[default, store_returns]], table:alias=[store_returns]): rowcount = 5.759115E7, cumulative cost = {5.759115E7 rows, 5.7591151E7 cpu, 0.0 io}, id = 1040\n                HiveProject(d_date_sk=[$0]): rowcount = 9861.615, cumulative cost = {92772.23000000001 rows, 155960.615 cpu, 0.0 io}, id = 1650\n```\n\n----------------------------------------\n\nTITLE: Using the least() function in Hive SQL\nDESCRIPTION: Returns the least value from a list of values. Returns NULL when one or more arguments are NULL, with relaxed type restrictions consistent with the '<' operator.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_47\n\nLANGUAGE: SQL\nCODE:\n```\nleast(T v1, T v2, ...)\n```\n\n----------------------------------------\n\nTITLE: Aliasing Window Functions in HiveQL\nDESCRIPTION: Shows how to use aliases with window functions, both with and without the AS keyword.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT \n a,\n COUNT(b) OVER (PARTITION BY c) AS b_count,\n SUM(b) OVER (PARTITION BY c) b_sum\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: Querying Hive Table with MAP Type Column\nDESCRIPTION: Example of querying a Hive table that uses MAP type to access an HBase column family. Shows how the column values are returned as a JSON object in the results.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nhive> select * from hbase_table_1;\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\n...\nOK\n{\"val_100\":100}\t100\n{\"val_98\":98}\t98\nTime taken: 3.808 seconds\n\n```\n\n----------------------------------------\n\nTITLE: Output of User-Defined Skew Information\nDESCRIPTION: Example output from DESCRIBE FORMATTED showing how user-defined skew information appears instead of top K statistics when skew is explicitly specified during table creation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n...\nSkewed Columns:         [key]                    \nSkewed Values:          [[38], [49]]   \n...\n```\n\n----------------------------------------\n\nTITLE: Casting String to BIGINT in Hive SQL\nDESCRIPTION: Demonstration of using the cast function in Hive SQL to convert a string representation of a number to its BIGINT equivalent. This is useful for type conversion when working with mixed data types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\ncast('1' as BIGINT)\n```\n\n----------------------------------------\n\nTITLE: Mapping Hive MAP to HBase Column Prefix\nDESCRIPTION: Example of using wildcards to retrieve all HBase columns with a specific prefix. This feature was introduced in Hive 0.12 and allows accessing multiple columns with a common prefix pattern.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE hbase_table_1(value map<string,int>, row_key int) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n\"hbase.columns.mapping\" = \"cf:col_prefix.*,:key\"\n);\n\n```\n\n----------------------------------------\n\nTITLE: Masking Last N Characters in Hive\nDESCRIPTION: Returns a masked version of the input string with the last n characters masked. Uppercase letters become 'X', lowercase letters become 'x', and numbers become 'n'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_43\n\nLANGUAGE: sql\nCODE:\n```\nmask_last_n(string str[, int n])\n```\n\n----------------------------------------\n\nTITLE: Initialize Schema Example\nDESCRIPTION: Example showing how to initialize a new Hive schema using Derby database.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-schema-tool_34835119.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ schematool -dbType derby -initSchema\nMetastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true\nMetastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver\nMetastore connection User:       APP\nStarting metastore schema initialization to 0.13.0\nInitialization script hive-schema-0.13.0.derby.sql\nInitialization script completed\nschemaTool completed\n```\n\n----------------------------------------\n\nTITLE: Testing Hive Streaming Updates and Deletes\nDESCRIPTION: Reference to the TestMutations.testUpdatesAndDeletes() method that demonstrates how to perform update and delete operations using the Hive Streaming API in a test environment.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/283118454.md#2025-04-09_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nTestMutations.testUpdatesAndDeletes()\n```\n\n----------------------------------------\n\nTITLE: Setting Hadoop Options for Snappy on Mac\nDESCRIPTION: This bash command sets environment variables to resolve Snappy initialization issues on Mac systems when running Hive or HiveServer2 with Spark.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/44302539.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport HADOOP_OPTS=\"-Dorg.xerial.snappy.tempdir=/tmp -Dorg.xerial.snappy.lib.name=libsnappyjava.jnilib $HADOOP_OPTS\"\n```\n\n----------------------------------------\n\nTITLE: Using the cosh() function in Hive SQL\nDESCRIPTION: Returns the hyperbolic cosine of a value in radians. For example, cosh(0) returns 1.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_49\n\nLANGUAGE: SQL\nCODE:\n```\ncosh(double x)\n```\n\n----------------------------------------\n\nTITLE: Scanning Accumulo Table Contents\nDESCRIPTION: Shows how to scan the contents of an Accumulo table using the Accumulo shell, displaying the key-value pairs resulting from Hive integration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/accumulointegration_46633569.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuser@accumulo accumulo_table> scan\nrow1\tperson:age []\t32\nrow1\tperson:height []\t72\nrow1\tperson:name []\tSteve\nrow1\tperson:weight []\t200\n```\n\n----------------------------------------\n\nTITLE: Enabling Operator Stats for Reducer Parallelism in Hive on Spark\nDESCRIPTION: Property to determine whether to use operator stats for reducer parallelism in Hive on Spark. When false, it uses an alternative algorithm for calculating partitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_51\n\nLANGUAGE: properties\nCODE:\n```\nhive.spark.use.op.stats = true\n```\n\n----------------------------------------\n\nTITLE: Limiting Maximum Partitions for Top K Collection\nDESCRIPTION: HQL command to set the maximum number of partitions for which to collect top K statistics. This setting helps prevent excessive memory usage when dealing with many partitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_4\n\nLANGUAGE: hql\nCODE:\n```\nset hive.stats.topk.maxpartnum=10;\n```\n\n----------------------------------------\n\nTITLE: Enabling Remote Debugging for Query File Tests\nDESCRIPTION: Configures the Query File Test framework to listen for a remote debugger connection, facilitating debugging of Hive internals during test execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/Development/qtest.md#2025-04-09_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n$ mvn -Pitests -pl itests/qtest -Dmaven.surefire.debug test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=alter1.q\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partitioning in Pig with No Partition Specification\nDESCRIPTION: Example of using HCatStorer without specifying any partition key-value pairs, allowing HCatalog to dynamically determine partitioning based on data values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-dynamicpartitions_34014006.md#2025-04-09_snippet_3\n\nLANGUAGE: pig\nCODE:\n```\nstore A into 'mytable' using HCatStorer();\n```\n\n----------------------------------------\n\nTITLE: Creating Release Branch in Git\nDESCRIPTION: Commands to create a new release branch from master and push it to the remote repository.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b branch-X.Y origin/master\ngit push -u origin branch-X.Y\n```\n\n----------------------------------------\n\nTITLE: Invalid GROUP BY Query in Hive\nDESCRIPTION: Demonstrates an invalid GROUP BY query where the SELECT clause includes a column not in the GROUP BY clause or an aggregation function.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT\n   a,\n   b\nFROM\n   t1\nGROUP BY\n   a;\n```\n\n----------------------------------------\n\nTITLE: Examining Hive Filter and Table Scan Operations in EXPLAIN Output\nDESCRIPTION: This snippet shows a portion of a Hive EXPLAIN output that details a filter operation (with condition on year=2000) and a table scan operation on the date_dim table, including row count estimates and cost information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nHiveFilter(condition=[AND(=($6, 2000), IS NOT NULL($0))]): rowcount = 9861.615, cumulative cost = {82910.615 rows, 146099.0 cpu, 0.0 io}, id = 1648\n  HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim]): rowcount = 73049.0, cumulative cost = {73049.0 rows, 73050.0 cpu, 0.0 io}, id = 1043\n```\n\n----------------------------------------\n\nTITLE: Example Hive Command for Custom Java LZO File Creation\nDESCRIPTION: Example of a complete Hive command that disables compression for the query execution, allowing the output to be processed by custom Java code that will handle the LZO compression.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lzo_33298193.md#2025-04-09_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nhive -e \"SET hive.exec.compress.output=false;SET mapreduce.output.fileoutputformat.compress=false;<query-string>\"\n```\n\n----------------------------------------\n\nTITLE: Using the binary() function in Hive SQL\nDESCRIPTION: Casts a string or binary parameter into a binary type. Used for explicit type conversion when needed.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_67\n\nLANGUAGE: SQL\nCODE:\n```\nbinary(string|binary)\n```\n\n----------------------------------------\n\nTITLE: Build Hive Docker Image from Source\nDESCRIPTION: Maven command to build Hive Docker image from source code.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/Development/quickStart.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmvn clean package -pl packaging -DskipTests -Pdocker\n```\n\n----------------------------------------\n\nTITLE: Creating a WriteEntity for HCatWriter in Java\nDESCRIPTION: This snippet shows how to create a WriteEntity object that specifies where to write data, similar to ReadEntity. It defines the target database and table for write operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-readerwriter_34013921.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nWriteEntity.Builder builder = new WriteEntity.Builder();\nWriteEntity entity = builder.withDatabase(\"mydb\").withTable(\"mytbl\").build();\n```\n\n----------------------------------------\n\nTITLE: JDBC Connection URL with Token Authentication\nDESCRIPTION: Example JDBC connection URL for token-based authentication. Includes token parameter for subsequent requests after SAML authentication.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/170266662.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\njdbc:hive2://HiveServer2-host:10001/default;transportMode=http;httpPath=cliservice;auth=token;token=<token_string>\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Integer Table in Hive\nDESCRIPTION: Demonstrates the creation of a basic table with two integer columns in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-groupby_27362038.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE t1(a INTEGER, b INTGER);\n```\n\n----------------------------------------\n\nTITLE: Identifying Map-Join Operators for Partition Pruning\nDESCRIPTION: Steps for identifying Map-Join operators within tasks and evaluating their eligibility for partition pruning. The algorithm looks for operators matching the pattern \"TS.*MAPJOIN\" and processes each one.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoin-and-partition-pruning_34015666.md#2025-04-09_snippet_3\n\nLANGUAGE: pseudo\nCODE:\n```\n3. With in the task Look for pattern \"TS.*MAPJOIN\". Perform #4 - #6 for each MAPJOIN operator.\n\n4. Flag a Map-Join Operator as candidate for Partition Pruning\n\n   4.1 Collect small tables that might participate in Big Table pruning  \n\n        a. Walk the join conditions. If Join Type is \"outer\" then check if big-table is on the outer side. If so then bailout.  \n\n        b. If big-table is on inner side then add the position of small table in to the set.\n\n  4.2 If set from #4.1 is empty then bailout. Otherwise collect join keys from big table which is not wrapped in a functions  \n\n        a) Get the join key from \"MapJoinDesc.getKeys().get(MapJoinDesc .getPosBigTable)\"  \n\n        b) Walk through list of \"ExpressionNodeDesc\"; if \"ExprNodeDesc\" is of type \"ExprNodeGenericFuncDesc\" then check if any of partition pruner candidate key is contained with in it (\"ExprNodeDescUtils.containsPredicate\"). If any candidate key is contained within the function then remove it from the partition-pruner-bigtable-candidate list.\n\n       c) Create a pair of \"ExprNodeColumnDesc position Integer within the list from #b\" and \"ExprNodecolumnDesc\" and add to partition-pruner-bigtable-candidate list.\n\n4.3 If partition-pruner-bigtable-candidate list is empty then bailout. Otherwise find join keys from #4.1 that is not wrapped in function using partition pruner candidate set.  \n\n      a) Walk the set from 4.1  \n\n      b) Get the join key for each element from 4.1  \n\n      c) Walk the join key list from #b checking if any of it is a function  \n\n      d) If any of the element from #c is a function then check if it contains any element from partition-pruner-bigtable-candidate list. If yes then remove that element from partition-pruner-bigtable-candidate List and set-generation-key-map.  \n\n      e) Create a pair of table position and join key element from #d.  \n\n      f) Add element to set-generation-key-map where key is the position of element within the partition-pruner-bigtable-candidate list and value is element from #e.\n\n4.4 If partition-pruner-bigtable-candidate set is empty then bail out. Otherwise find BigTable Columns from partition-pruner-bigtable-candidate set that is partitioned.  \n\n     a) Construct list of \"ExprNodeDesc\" from the set of #4.2  \n\n     b) Find out the root table column descriptors for #a (\"ExprNodeDescUtils.backtrack\")  \n\n     c) From Hive get Table metadata for big table  \n\n     d) Walk through the list from #b & check with Table meta data to see if any of those columns is partitioned (\"Table.isPartitionKey\"). If column is not partition key then remove it from the partition pruner candidate list.\n\n4.5 If partition-pruner-bigtable-candidate set is empty then bail out. Otherwise Check if any of the partition pruner element could potentially mutate the value before hitting the join conditions. We will have to introduce a new method to \"ExprNodeDescUtil\" similar to back track but checking if value could be mutated (ex functions).\n\n4.6 If partition-pruner-bigtable-candidate list from #4.5 is empty then bail out. Otherwise add partition-pruner-bigtable-candidate list and set-generation-key-map from #4.5 to the existing list of values in the PhysicalCtx.\n\n    a) Create a pair of partition-pruner-bigtable-candidate list & set-generation-key-map.  \n\n    b) Add it to the existing list in the physical context (this is to handle cascading mapjoin operators in the same MapRedTask)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Topic Name from HCatalog\nDESCRIPTION: Gets the topic name for a specific table from HCatalog metadata store using HiveMetaStoreClient.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-notification_34014558.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nHiveMetaStoreClient msc = new HiveMetaStoreClient(hiveConf);\nString topicName = msc.getTable(\"mydb\",\n                   \"myTbl\").getParameters().get(HCatConstants.HCAT_MSGBUS_TOPIC_NAME);\n```\n\n----------------------------------------\n\nTITLE: Querying WebHCat Job IDs with cURL\nDESCRIPTION: Example cURL command to retrieve job IDs from the WebHCat queue endpoint. The command uses the required user.name parameter.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-jobids_34017187.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/queue?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Example Usage of File Resources in Hive\nDESCRIPTION: Example demonstrating how to add a Python script file to a Hive session, list available files, and use the script in a MAP transformation on a dataset.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-cli_27362033.md#2025-04-09_snippet_5\n\nLANGUAGE: hive\nCODE:\n```\nhive> add FILE /tmp/tt.py;\nhive> list FILES;\n/tmp/tt.py\nhive> select from networks a \n             MAP a.networkid \n             USING 'python tt.py' as nn where a.ds = '2009-01-04' limit 10;\n```\n\n----------------------------------------\n\nTITLE: Installing Thrift 0.14.1 on Linux\nDESCRIPTION: Commands to build and install Thrift 0.14.1 on Linux systems with specific configuration options.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocontribute_27362107.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/thrift-0.14.1\n/configure -without-erlang --without-nodejs --without-python --without-py3 --without-perl --without-php --without-php_extension --without-ruby --without-haskell --without-go --without-swift --without-dotnetcore --without-qt5\nsudo make\nsudo make install \nsudo mkdir -p /usr/local/share/fb303/if\nsudo cp /path/to/thrift-0.14.1/contrib/fb303/if/fb303.thrift /usr/local/share/fb303/if/fb303.thrift\n```\n\n----------------------------------------\n\nTITLE: Setting up MapReduce code and data in HDFS\nDESCRIPTION: Commands to upload JAR files to HDFS and verify they exist before submitting a MapReduce job through WebHCat.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-mapreducejar_34017030.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% hadoop fs -put wordcount.jar .\n% hadoop fs -put transform.jar .\n\n% hadoop fs -ls .\nFound 2 items\n-rw-r--r--   1 ctdean supergroup         23 2011-11-11 13:29 /user/ctdean/wordcount.jar\n-rw-r--r--   1 ctdean supergroup         28 2011-11-11 13:29 /user/ctdean/transform.jar\n```\n\n----------------------------------------\n\nTITLE: WebHCat Resources Table in Markdown\nDESCRIPTION: A markdown table listing WebHCat resources categorized by their functionality, including the resource type, HTTP method, and a brief description for each endpoint.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference_34015762.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Category | Resource (Type) | Description |\n| --- | --- | --- |\n| General | [:version (GET)] | Return a list of supported response types. |\n|   | [status (GET)] | Return the WebHCat server status. |\n|   | [version (GET)] | Return a list of supported versions and the current version. |\n|  | [version/hive (GET)] | Return the Hive version being run. (Added in Hive 0.13.0.) |\n|  | [version/hadoop (GET)] | Return the Hadoop version being run. (Added in Hive 0.13.0.) |\n| [DDL] | [ddl (POST)] | Perform an HCatalog DDL command. |\n|   | [ddl/database (GET)] | List HCatalog databases. |\n|   | [ddl/database/:db (GET)] | Describe an HCatalog database. |\n|   | [ddl/database/:db (PUT)] | Create an HCatalog database. |\n|   | [ddl/database/:db (DELETE)] | Delete (drop) an HCatalog database. |\n|   | [ddl/database/:db/table (GET)] | List the tables in an HCatalog database. |\n|   | [ddl/database/:db/table/:table (GET)] | Describe an HCatalog table. |\n|   | [ddl/database/:db/table/:table (PUT)] | Create a new HCatalog table. |\n|   | [ddl/database/:db/table/:table (POST)] | Rename an HCatalog table. |\n|   | [ddl/database/:db/table/:table (DELETE)] | Delete (drop) an HCatalog table. |\n|   | [ddl/database/:db/table/:existingtable/like/:newtable (PUT)] | Create a new HCatalog table like an existing one. |\n|   | [ddl/database/:db/table/:table/partition (GET)] | List all partitions in an HCatalog table. |\n|   | [ddl/database/:db/table/:table/partition/:partition (GET)] | Describe a single partition in an HCatalog table. |\n|   | [ddl/database/:db/table/:table/partition/:partition (PUT)] | Create a partition in an HCatalog table. |\n|   | [ddl/database/:db/table/:table/partition/:partition (DELETE)] | Delete (drop) a partition in an HCatalog table. |\n|   | [ddl/database/:db/table/:table/column (GET)] | List the columns in an HCatalog table. |\n|   | [ddl/database/:db/table/:table/column/:column (GET)] | Describe a single column in an HCatalog table. |\n|   | [ddl/database/:db/table/:table/column/:column (PUT)] | Create a column in an HCatalog table. |\n|   | [ddl/database/:db/table/:table/property (GET)] | List table properties. |\n|   | [ddl/database/:db/table/:table/property/:property (GET)] | Return the value of a single table property. |\n|   | [ddl/database/:db/table/:table/property/:property (PUT)] | Set a table property. |\n| MapReduce | [mapreduce/streaming (POST)] | Create and queue Hadoop streaming MapReduce jobs. |\n|   | [mapreduce/jar (POST)] | Create and queue standard Hadoop MapReduce jobs. |\n| Pig | [pig (POST)] | Create and queue Pig jobs. |\n| Hive | [hive (POST)] | Run Hive queries and commands. |\n| Queue(deprecated in Hive 0.12,removed in Hive 0.14) | [queue (GET)] | Return a list of all job IDs. (Removed in Hive 0.14.0.) |\n|   | [queue/:jobid (GET)] | Return the status of a job given its ID. (Removed in Hive 0.14.0.) |\n|   | [queue/:jobid (DELETE)] | Kill a job given its ID. (Removed in Hive 0.14.0.) |\n| Jobs(Hive 0.12 and later) | [jobs (GET)] | Return a list of all job IDs. |\n|   | [jobs/:jobid (GET)] | Return the status of a job given its ID. |\n|   | [jobs/:jobid (DELETE)] | Kill a job given its ID. |\n```\n\n----------------------------------------\n\nTITLE: Multi-partition Write with HCatStorer\nDESCRIPTION: Shows how to write to multiple partitions when partition column is present in the data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_14\n\nLANGUAGE: pig\nCODE:\n```\nstore z into 'web_data' using org.apache.hive.hcatalog.pig.HCatStorer();\n```\n\n----------------------------------------\n\nTITLE: Examining View Dependencies in Hive with EXPLAIN DEPENDENCY\nDESCRIPTION: This example demonstrates creating a view and using EXPLAIN DEPENDENCY to show the relationship between the view and its underlying table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW V1 AS SELECT key, value from src;\nEXPLAIN DEPENDENCY SELECT * FROM V1;\n```\n\n----------------------------------------\n\nTITLE: Configuring JDO for Automatic Schema Updates in Hive\nDESCRIPTION: This configuration enables JDO to automatically alter tables in the metastore when schema changes are detected. It's set in the Hive configuration file.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/viewdev_27362067.md#2025-04-09_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n<name>datanucleus.autoCreateSchema</name>\n<value>true</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Installing unixODBC and Related Drivers\nDESCRIPTION: Command to install unixODBC and all related drivers to the system. Requires sudo privileges.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveodbc_27362099.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo make install\n```\n\n----------------------------------------\n\nTITLE: Setting HCatalog Output Format with Partition\nDESCRIPTION: Example of setting up HCatalog output format with partition specification for a specific date.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-inputoutput_34013776.md#2025-04-09_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nMap partitions = new HashMap<String, String>(1);\npartitions.put(\"ds\", \"20110924\");\nHCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName, outputTableName, partitions));\n```\n\n----------------------------------------\n\nTITLE: Checking Out Hive Javadocs SVN Repository\nDESCRIPTION: SVN command to check out the javadocs repository with empty depth for further operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nsvn co --depth empty https://svn.apache.org/repos/infra/websites/production/hive/content/javadocs\n```\n\n----------------------------------------\n\nTITLE: Sort By Example with Multiple Columns\nDESCRIPTION: Demonstrates sorting by multiple columns with different sort directions using SORT BY clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sortby_27362045.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT key, value FROM src SORT BY key ASC, value DESC\n```\n\n----------------------------------------\n\nTITLE: Dropping Permanent Functions in HiveQL\nDESCRIPTION: Syntax for removing a permanent function from the metastore, with an optional IF EXISTS clause to prevent errors if the function doesn't exist.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_64\n\nLANGUAGE: hql\nCODE:\n```\nDROP FUNCTION [IF EXISTS] function_name;\n```\n\n----------------------------------------\n\nTITLE: Executing Precommit Tests for Apache Hive using Jenkins Script\nDESCRIPTION: This bash script automates precommit testing for Apache Hive JIRA issues. It validates JIRA status, downloads patch attachments, determines the correct branch, and runs tests using the PTest client. The script includes checks for tags like 'NO PRECOMMIT TESTS', validates patch naming conventions, and supports different testing profiles.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jenkins-script_33295285.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nenv\nset -e\nset -x\n. ${HOME}/toolchain/toolchain.sh\nexport PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH\nROOT=$PWD\nBRANCH=trunk\nexport JIRA_NAME=\"HIVE-${ISSUE_NUM}\"\necho $JIRA_NAME\nexport JIRA_ROOT_URL=\"https://issues.apache.org\"\ntest -d hive/build/ || mkdir -p hive/build/\ncd hive/build/\nrm -rf ptest2\nsvn co http://svn.apache.org/repos/asf/hive/trunk/testutils/ptest2/ ptest2\ncd ptest2\n# process the jira\nJIRA_TEXT=$(mktemp)\ntrap \"rm -f $JIRA_TEXT\" EXIT\ncurl -s -S --location --retry 3 \"${JIRA_ROOT_URL}/jira/browse/${JIRA_NAME}\" > $JIRA_TEXT\nif grep -q \"NO PRECOMMIT TESTS\" $JIRA_TEXT\nthen\n  echo \"Test $JIRA_NAME has tag NO PRECOMMIT TESTS\"\n  exit 0\nfi\n# ensure the patch is actually in the correct state\nif ! grep -q 'Patch Available' $JIRA_TEXT\nthen\n  echo \"$JIRA_NAME is not \\\"Patch Available\\\". Exiting.\"\n  exit 1\nfi\n# pull attachments from JIRA (hack stolen from hadoop since rest api doesn't show attachments)\nPATCH_URL=$(grep -o '\"/jira/secure/attachment/[0-9]*/[^\"]*' $JIRA_TEXT | \\\n  grep -v -e 'htm[l]*$' | sort | tail -1 | \\\n  grep -o '/jira/secure/attachment/[0-9]*/[^\"]*')\nif [[ -z \"$PATCH_URL\" ]]\nthen\n  echo \"Unable to find attachment for $JIRA_NAME\"\n  exit 1\nfi\n# ensure attachment has not already been tested\nATTACHMENT_ID=$(basename $(dirname $PATCH_URL))\nif grep -q \"ATTACHMENT ID: $ATTACHMENT_ID\" $JIRA_TEXT\nthen\n  echo \"Attachment $ATTACHMENT_ID is already tested for $JIRA_NAME\"\n  exit 1\nfi\n# validate the patch name, parse branch if needed\nshopt -s nocasematch\nPATCH_NAME=$(basename $PATCH_URL)\n# Test examples:\n# HIVE-123.patch HIVE-123.1.patch HIVE-123.D123.patch HIVE-123.D123.1.patch HIVE-123-tez.patch HIVE-123.1-tez.patch\n# HIVE-XXXX.patch, HIVE-XXXX.XX.patch  HIVE-XXXX.XX-branch.patch HIVE-XXXX-branch.patch\nif [[ $PATCH_NAME =~ ^HIVE-[0-9]+(\\.[0-9]+)?(-[a-z0-9-]+)?\\.(patch|patch.\\txt)$ ]]\nthen\n  if [[ -n \"${BASH_REMATCH[2]}\" ]]\n  then\n    BRANCH=${BASH_REMATCH[2]#*-}\n  else\n    echo \"Assuming branch $BRANCH\"\n  fi\n# HIVE-XXXX.DXXXX.patch or HIVE-XXXX.DXXXX.XX.patch\nelif [[ $PATCH_NAME =~ ^(HIVE-[0-9]+\\.)?D[0-9]+(\\.[0-9]+)?\\.(patch|patch.\\txt)$ ]]\nthen\n  echo \"Assuming branch $BRANCH\"\nelse\n  echo \"Patch $PATCH_NAME does not appear to be a patch\"\n  exit 1\nfi\nshopt -u nocasematch\n# append mr1 if needed\nif [[ $BRANCH =~ (mr1|mr2)$ ]]\nthen\n  profile=$BRANCH\nelse\n  profile=${BRANCH}-mr1\nfi\n# sanity check the profile\ncase \"$profile\" in\n  trunk-mr1);;\n  trunk-mr2);;\n  maven-mr1);;\n  *)\n  echo \"Unknown profile '$profile'\"\n  exit 1\nesac\nTEST_OPTS=\"\"\nif grep -q \"CLEAR LIBRARY CACHE\" $JIRA_TEXT\nthen\n  echo \"Clearing library cache before starting test\"\n  TEST_OPTS=\"--clearLibraryCache\"\nfi\nmvn clean package -DskipTests -Drat.numUnapprovedLicenses=1000 -Dmaven.repo.local=$WORKSPACE/.m2\nset +e\njava -cp \"target/hive-ptest-1.0-classes.jar:target/lib/*\" org.apache.hive.ptest.api.client.PTestClient --endpoint http://ec2-174-129-184-35.compute-1.amazonaws.com --command testStart --profile $profile --password xxx --outputDir target/ --testHandle \"${BUILD_TAG##jenkins-}\" --patch \"${JIRA_ROOT_URL}${PATCH_URL}\" --jira \"$JIRA_NAME\" --clearLibraryCache\nret=$?\ncd target/\nif [[ -f test-results.tar.gz ]]\nthen\n  rm -rf $ROOT/hive/build/test-results/\n  tar zxf test-results.tar.gz -C $ROOT/hive/build/\nfi\nexit $ret\n```\n\n----------------------------------------\n\nTITLE: Creating Hive JDBC Table with Column Type Mapping\nDESCRIPTION: Illustrates creating a JDBC table with column name and type mapping, where Hive will attempt to convert types according to the specified schema.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE EXTERNAL TABLE student_jdbc\n(\n  sname string,\n  age int,\n  effective_gpa decimal(4,3)\n)\nSTORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler'\nTBLPROPERTIES (\n    . . . . . .\n    \"hive.sql.query\" = \"SELECT name, age, gpa FROM STUDENT\",\n);\n```\n\n----------------------------------------\n\nTITLE: Converting String to Uppercase in Hive\nDESCRIPTION: Returns the string resulting from converting all characters of A to uppercase.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_37\n\nLANGUAGE: sql\nCODE:\n```\nupper(string A)\nucase(string A)\n```\n\n----------------------------------------\n\nTITLE: Running Debug Query\nDESCRIPTION: Commands to start Hive in debug mode and run the specific query for debugging.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n    >  ./build/dist/bin/hive --debug\n    >  run the query\n```\n\n----------------------------------------\n\nTITLE: Failing Example with Invalid MAP Key Type\nDESCRIPTION: Example of an invalid table definition that fails because it tries to use a non-string type (int) as the key of a MAP that maps to HBase column names. The key must be string type.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE hbase_table_1(key int, value map<int,int>) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n\"hbase.columns.mapping\" = \":key,cf:\"\n);\nFAILED: Error in metadata: java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException org.apache.hadoop.hive.hbase.HBaseSerDe: hbase column family 'cf:' should be mapped to map<string,?> but is mapped to map<int,int>)\n```\n\n----------------------------------------\n\nTITLE: Showing All Roles in Hive\nDESCRIPTION: Lists all existing roles in the system. Only admin role has privileges to execute this command.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nSHOW ROLES;\n```\n\n----------------------------------------\n\nTITLE: Calculating MD5 Checksum in Hive SQL\nDESCRIPTION: Demonstrates the usage of the md5() function in Hive SQL to calculate an MD5 128-bit checksum for a string or binary input.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_80\n\nLANGUAGE: SQL\nCODE:\n```\nmd5('ABC')\n```\n\n----------------------------------------\n\nTITLE: Complex Filter with OR Condition\nDESCRIPTION: Shows how to filter data using OR operator to match multiple datestamp values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_8\n\nLANGUAGE: pig\nCODE:\n```\na = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader();\nb = filter a by datestamp == '20110924' or datestamp == '20110925';\n```\n\n----------------------------------------\n\nTITLE: Moving Hive Release Artifacts with SVN\nDESCRIPTION: SVN command to move the release artifacts from the dev area to the release area of the project.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nsvn mv https://dist.apache.org/repos/dist/dev/hive/hive-X.Y.Z https://dist.apache.org/repos/dist/release/hive/hive-X.Y.Z -m \"Move hive-X.Y.Z release from dev to release\"\n```\n\n----------------------------------------\n\nTITLE: Creating Views in Apache Hive\nDESCRIPTION: SQL syntax for creating a view in Hive with optional parameters for column names, comments, and table properties. Views are read-only logical objects with no associated storage.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_46\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW [IF NOT EXISTS] [db_name.]view_name [(column_name [COMMENT column_comment], ...) ]\n  [COMMENT view_comment]\n  [TBLPROPERTIES (property_name = property_value, ...)]\n  AS SELECT ...;\n```\n\n----------------------------------------\n\nTITLE: Building Hive Branch-1\nDESCRIPTION: Maven commands to build Hive branch-1 with Hadoop 2.x profile\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -DskipTests -Phadoop-2\ncd itests \nmvn clean install -DskipTests -Phadoop-2\n```\n\n----------------------------------------\n\nTITLE: HDFS Configuration Setting Reference\nDESCRIPTION: Link reference to Hive's configuration property for permission inheritance behavior.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/permission-inheritance-in-hive_48203008.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nhive.warehouse.subdir.inherit.perms\n```\n\n----------------------------------------\n\nTITLE: Creating Hive External Table for HBase Access\nDESCRIPTION: SQL command to create an external Hive table that maps to the newly created HBase table, enabling Hive queries on HBase data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE hbase_transactions(transaction_id string, user_name string, amount double, ...) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf:user_name,cf:amount,...\")\nTBLPROPERTIES(\"hbase.table.name\" = \"transactions\");\n```\n\n----------------------------------------\n\nTITLE: Downloading MovieLens Dataset with Wget\nDESCRIPTION: Command to download the MovieLens 100k dataset zip file using wget.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nwget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n```\n\n----------------------------------------\n\nTITLE: Creating Control Separated Table in Hive SQL\nDESCRIPTION: This snippet demonstrates creating a table with custom field, collection, and map delimiters. It uses control characters as separators for different elements in the data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/userguide_27362066.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE mylog (\nname STRING, language STRING, groups ARRAY<STRING>, entities MAP<INT, STRING>)\nROW FORMAT DELIMITED\n  FIELDS TERMINATED BY '\\001'\n  COLLECTION ITEMS TERMINATED BY '\\002'\n  MAP KEYS TERMINATED BY '\\003'\nSTORED AS TEXTFILE;\n```\n\n----------------------------------------\n\nTITLE: SFTP Upload Commands for Release Artifacts\nDESCRIPTION: SFTP commands for uploading release artifacts to Apache servers\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsftp YOUR-APACHE-ID@home.apache.org\ncd public_html\nmkdir hive-storage-X.Y.Z\ncd hive-storage-X.Y.Z\nput hive-storage-X.Y.Z-rcR.tar.gz*\nquit\n```\n\n----------------------------------------\n\nTITLE: Setting SerDe for Decimal Data Type in Hive\nDESCRIPTION: Demonstrates how to set the serialization-deserialization library (SerDe) for a table with Decimal type, supporting both LazySimpleSerDe and LazyBinarySerDe.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nalter table decimal_1 set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';\n```\n\nLANGUAGE: sql\nCODE:\n```\nalter table decimal_1 set serde 'org.apache.hadoop.hive.serde2.lazy.LazyBinarySerDe';\n```\n\n----------------------------------------\n\nTITLE: Case W1: Hive SQL Outer Join with Where Clause on Preserved Row Table\nDESCRIPTION: Example demonstrating how Hive processes a where clause predicate on the preserved row table in a left outer join. The execution plan shows the predicate being pushed down to the table scan.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/outerjoinbehavior_35749927.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nexplain \nselect s1.key, s2.key \nfrom src s1 left join src s2 \nwhere s1.key > '2';\n```\n\n----------------------------------------\n\nTITLE: Loading HFiles into HBase using CompleteBulkLoad\nDESCRIPTION: Command for loading HFiles into HBase using the completebulkload utility (HBase 0.90.2+).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nhadoop jar hbase-VERSION.jar completebulkload [-c /path/to/hbase/config/hbase-site.xml] /tmp/hbout transactions\n```\n\n----------------------------------------\n\nTITLE: Archiving Old Hive Releases\nDESCRIPTION: SVN command to archive old Hive releases from the main download site according to Apache guidelines.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nsvn del -m \"Archiving release Apache Hive 4.0.0-beta-1\" https://dist.apache.org/repos/dist/release/hive/hive-4.0.0-beta-1/\n```\n\n----------------------------------------\n\nTITLE: Querying Hive Tables with Case Insensitive Identifiers\nDESCRIPTION: Demonstrates how Hive SQL handles case insensitivity in table and column names, showing equivalent queries with different casing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-faq_27362095.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM MyTable WHERE myColumn = 3\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from mytable where mycolumn = 3\n```\n\n----------------------------------------\n\nTITLE: Querying Hive Version Using curl\nDESCRIPTION: Example curl command to request the Hive version from the WebHCat API. The command includes the required user.name parameter and uses the GET method.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-versionhive_44303406.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/version/hive?user.name=ekoifman'\n```\n\n----------------------------------------\n\nTITLE: Dropping Table in Hive\nDESCRIPTION: Removes a table and its associated data and metadata from Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_17\n\nLANGUAGE: HiveQL\nCODE:\n```\nDROP TABLE pv_users;\n```\n\n----------------------------------------\n\nTITLE: Adding, Listing and Deleting Resources in Hive\nDESCRIPTION: Basic syntax for adding, listing, and deleting resources (files, JARs, or archives) in a Hive session. These resources are distributed through Hadoop's Distributed Cache for execution across the cluster.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-cli_27362033.md#2025-04-09_snippet_4\n\nLANGUAGE: hive\nCODE:\n```\nADD { FILE[S] | JAR[S] | ARCHIVE[S] } <filepath1> [<filepath2>]*\nLIST { FILE[S] | JAR[S] | ARCHIVE[S] } [<filepath1> <filepath2> ..]\nDELETE { FILE[S] | JAR[S] | ARCHIVE[S] } [<filepath1> <filepath2> ..]\n```\n\n----------------------------------------\n\nTITLE: Checking Out Release Branch\nDESCRIPTION: Git commands to clone the Hive repository and checkout the release branch.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://git-wip-us.apache.org/repos/asf/hive.git/ <hive_src_dir>\ncd <hive_src_dir>\ngit checkout branch-X.Y\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Partitions in Hive 0.7 Using Separate Statements\nDESCRIPTION: Example showing the workaround for adding multiple partitions in Hive 0.7, which requires using separate ALTER TABLE statements for each partition to avoid failures in the partitioning scheme.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_35\n\nLANGUAGE: hql\nCODE:\n```\nALTER TABLE table_name ADD PARTITION (partCol = 'value1') location 'loc1';\nALTER TABLE table_name ADD PARTITION (partCol = 'value2') location 'loc2';\n...\nALTER TABLE table_name ADD PARTITION (partCol = 'valueN') location 'locN';\n```\n\n----------------------------------------\n\nTITLE: Defining Window Specifications in HiveQL\nDESCRIPTION: Demonstrates the syntax for defining window specifications in HiveQL, including ROWS and RANGE options with PRECEDING, CURRENT ROW, and FOLLOWING clauses.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)\n(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)\n(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING\n```\n\n----------------------------------------\n\nTITLE: Building and Running Hive\nDESCRIPTION: Commands for compiling Hive from source and launching the Hive CLI.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/genericudafcasestudy_27362093.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nant package\nbuild/dist/bin/hive\n```\n\n----------------------------------------\n\nTITLE: JIRA Issues Table in Markdown\nDESCRIPTION: Markdown table displaying key details of open JIRA issues including key, summary, type, dates, assignee, priority and status\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_18\n\nLANGUAGE: markdown\nCODE:\n```\n| Key | Summary | T | Created | Updated | Due | Assignee | Reporter | P | Status | Resolution |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n```\n\n----------------------------------------\n\nTITLE: Implementing UDAF Resolver Class Structure in Java\nDESCRIPTION: Basic skeleton of a UDAF implementation showing the resolver class structure with logging setup and evaluator instantiation. Extends AbstractGenericUDAFResolver and implements type checking functionality.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/genericudafcasestudy_27362093.md#2025-04-09_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver {\n  static final Log LOG = LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());\n\n  @Override\n  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException {\n    // Type-checking goes here!\n\n    return new GenericUDAFHistogramNumericEvaluator();\n  }\n\n  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator {\n    // UDAF logic goes here!\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Hive Quoting Example\nDESCRIPTION: Example showing the quoting behavior of Hive's quote() function for different input strings, including NULL values and strings containing single quotes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_27\n\nLANGUAGE: sql\nCODE:\n```\nSELECT quote(NULL);    -- Returns NULL\nSELECT quote('DONT');   -- Returns 'DONT'\nSELECT quote('DON\\'T'); -- Returns 'DON\\'T'\n```\n\n----------------------------------------\n\nTITLE: Examples of SHOW COLUMNS Command in Hive\nDESCRIPTION: Comprehensive examples of using the SHOW COLUMNS command with different pattern matching options and database specifications. These examples demonstrate various ways to list and filter columns in a table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_85\n\nLANGUAGE: sql\nCODE:\n```\n-- SHOW COLUMNS\nCREATE DATABASE test_db;\nUSE test_db;\nCREATE TABLE foo(col1 INT, col2 INT, col3 INT, cola INT, colb INT, colc INT, a INT, b INT, c INT);\n \n-- SHOW COLUMNS basic syntax\nSHOW COLUMNS FROM foo;                            -- show all column in foo\nSHOW COLUMNS FROM foo \"*\";                        -- show all column in foo\nSHOW COLUMNS IN foo \"col*\";                       -- show columns in foo starting with \"col\"                 OUTPUT col1,col2,col3,cola,colb,colc\nSHOW COLUMNS FROM foo '*c';                       -- show columns in foo ending with \"c\"                     OUTPUT c,colc\nSHOW COLUMNS FROM foo LIKE \"col1|cola\";           -- show columns in foo either col1 or cola                 OUTPUT col1,cola\nSHOW COLUMNS FROM foo FROM test_db LIKE 'col*';   -- show columns in foo starting with \"col\"                 OUTPUT col1,col2,col3,cola,colb,colc\nSHOW COLUMNS IN foo IN test_db LIKE 'col*';       -- show columns in foo starting with \"col\" (FROM/IN same)  OUTPUT col1,col2,col3,cola,colb,colc\n \n-- Non existing column pattern resulting in no match\nSHOW COLUMNS IN foo \"nomatch*\";\nSHOW COLUMNS IN foo \"col+\";                       -- + wildcard not supported\nSHOW COLUMNS IN foo \"nomatch\";\n```\n\n----------------------------------------\n\nTITLE: Defining GetInfo Request/Response Structs in Thrift\nDESCRIPTION: Defines structures for retrieving general information about the data source using ODBC-like keys. Includes session handle and optional info keys in request, returns status and key-value pairs in response.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-thrift-api_27843687.md#2025-04-09_snippet_2\n\nLANGUAGE: thrift\nCODE:\n```\nstruct TGetInfoReq {\n  1: required TSessionHandle session_handle\n  2: optional list<string> info_keys\n}\n\nstruct TGetInfoResp {\n  1: required TStatus status\n  2: map<string, string> info\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing ngrams() and context_ngrams() equivalent queries\nDESCRIPTION: Two equivalent queries showing how ngrams() and context_ngrams() can be used to achieve the same result, with ngrams() being slightly more efficient.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statisticsanddatamining_27362058.md#2025-04-09_snippet_2\n\nLANGUAGE: hql\nCODE:\n```\nSELECT ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter;\nSELECT context_ngrams(sentences(lower(tweet)), array(null,null), 100, [, 1000]) FROM twitter;\n```\n\n----------------------------------------\n\nTITLE: Setting TiDB Isolation Level Check Configuration\nDESCRIPTION: SQL command to set the global parameter to skip isolation level checks in TiDB, which is necessary because Hive uses the SERIALIZABLE isolation level that TiDB doesn't support.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/using-tidb-as-the-hive-metastore-database_158872426.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nset global tidb_skip_isolation_level_check=1;\n```\n\n----------------------------------------\n\nTITLE: Multiple Partition Columns Exchange in Hive\nDESCRIPTION: Demonstrates exchanging partitions with multiple partition columns. Creates tables with two partition columns and exchanges a specific partition between them.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/exchange-partition_30755801.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n-- Create two tables with multiple partition columns.\nCREATE TABLE T1 (a int) PARTITIONED BY (d1 int, d2 int);\nCREATE TABLE T2 (a int) PARTITIONED BY (d1 int, d2 int);\nALTER TABLE T1 ADD PARTITION (d1=1, d2=2);\n\n-- Alter the table, moving partition data d1=1, d2=2 from table T1 to table T2\nALTER TABLE T2 EXCHANGE PARTITION (d1 = 1, d2 = 2) WITH TABLE T1;\n```\n\n----------------------------------------\n\nTITLE: Configuring Supported Blobstore Schemes in Hive XML\nDESCRIPTION: Specifies the list of supported blobstore schemes that Hive uses to apply special read/write performance improvements, such as for Amazon S3.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_113\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.blobstore.supported.schemes</name>\n  <value>s3,s3a,s3n</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter for Space Details Page\nDESCRIPTION: Basic markdown frontmatter block defining the page title and date.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/_index.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"Space Details:\"\ndate: 2024-12-12\n---\n```\n\n----------------------------------------\n\nTITLE: Loading MovieLens Data into Hive Table\nDESCRIPTION: This SQL command loads the MovieLens dataset into the previously created u_data table in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/userguide_27362066.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nLOAD DATA LOCAL INPATH 'ml-data/u.data'\nOVERWRITE INTO TABLE u_data;\n```\n\n----------------------------------------\n\nTITLE: Setting Configuration Variables in Beeline CLI\nDESCRIPTION: Example of using the SET command in Beeline to configure Hadoop variables and view current settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n    beeline> SET mapred.job.tracker=myhost.mycompany.com:50030;\n    beeline> SET -v;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Proxy User for WebHCat\nDESCRIPTION: These XML configuration entries in Hadoop's core-site.xml file set up proxy user permissions for the WebHCat server, allowing it to impersonate other users when running jobs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-installwebhcat_34015585.md#2025-04-09_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hadoop.proxyuser.USER.groups</name>\n  <value>group1,group2</value>\n</property>\n<property>\n  <name>hadoop.proxyuser.USER.hosts</name>\n  <value>host1,host2</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Enabling Index File HDFS Ignore in Hive\nDESCRIPTION: When true, the HDFS location stored in the index file is ignored at runtime, allowing use of index data even if data is moved or cluster name changes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_73\n\nLANGUAGE: properties\nCODE:\n```\nhive.index.compact.file.ignore.hdfs=false\n```\n\n----------------------------------------\n\nTITLE: Plotting histogram data with Gnuplot\nDESCRIPTION: Command for Gnuplot that visualizes histogram data extracted from Hive. The command creates an impulse plot from the x,y pairs in the data file.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statisticsanddatamining_27362058.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nplot 'data.txt' u 1:2 w impulses lw 5\n```\n\n----------------------------------------\n\nTITLE: Unsupported Multi-Column Subquery in SELECT\nDESCRIPTION: Example of a subquery that returns multiple columns, which is not supported. Scalar subqueries in the SELECT list must return exactly one column, and Hive will validate this during compilation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/subqueries-in-select_68717850.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT customer.customer_num,\n\t(SELECT ship_charge, customer_num\n\t\tFROM orders LIMIT 1\n\t) AS total_ship_chg\nFROM customer\n```\n\n----------------------------------------\n\nTITLE: Enabling BI Mode in Apache Hive\nDESCRIPTION: Demonstrates how to enable BI mode in Apache Hive, which allows for automatic rewrites to sketch functions for performance optimization.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/datasketches-integration_177050456.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.optimize.bi.enabled=true;\n```\n\n----------------------------------------\n\nTITLE: Incorrect Left Outer Join with WHERE Clause in Hive SQL\nDESCRIPTION: This snippet demonstrates an incorrect way of using a WHERE clause with a left outer join, which negates the outer join effect.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-joins_27362039.md#2025-04-09_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key)\nWHERE a.ds='2009-07-07' AND b.ds='2009-07-07'\n```\n\n----------------------------------------\n\nTITLE: Creating a UDF with JAR Specification in Hive SQL\nDESCRIPTION: This SQL command creates a UDF and specifies the required JAR file in HDFS, automatically adding it to the classpath.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveplugins_27362098.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nCREATE FUNCTION myfunc AS 'myclass' USING JAR 'hdfs:///path/to/jar';\n```\n\n----------------------------------------\n\nTITLE: Configuring WebHCat Proxy Settings in webhcat-site.xml\nDESCRIPTION: WebHCat proxy user configuration settings in webhcat-site.xml to specify allowed groups and hosts for proxy user 'hue'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-installwebhcat_34015585.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\nwebhcat.proxyuser.hue.groups=<comma-separated-groups>\nwebhcat.proxyuser.hue.hosts=<comma-separated-hosts>\n```\n\n----------------------------------------\n\nTITLE: Configuring Beeline-HiveServer2 Connection with Host and Kerberos Principal Override\nDESCRIPTION: A beeline-hs2-connection.xml example that overrides hosts and Kerberos principal settings. Values defined here take precedence over the same properties in hive-site.xml for connection purposes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_31\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n<property>\n  <name>beeline.hs2.connection.hosts</name>\n  <value>localhost:10000</value>\n</property>\n<property>\n  <name>beeline.hs2.connection.principal</name>\n  <value>hive/dummy-hostname@domain.com</value>\n</property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Transaction Write IDs in Hive\nDESCRIPTION: Code snippet demonstrating how to obtain ValidTxnWriteIdList and table-specific write IDs using HiveTxnManager. Shows the process of mapping global transaction state to table-specific write IDs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/synchronized-metastore-cache_110692851.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nHiveTxnManager txnMgr = TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf);\nValidTxnList txnIds = txnMgr.getValidTxns(); // get global transaction state\nValidTxnWriteIdList\ntxnWriteIdsTblIds = txnMgr.getValidWriteIdsTableIds(txnTables, txnString); // map global transaction state to table specific write id\nint tblId = txnWriteIdsTblIds.getTableId(fullTableName);\nValidWriteIdList writeids = txnWriteIds.getTableValidWriteIdList(fullTableName); // get table specific writeid\n```\n\n----------------------------------------\n\nTITLE: Data Structure Representation for SharedCache.TableWrapper\nDESCRIPTION: Diagram showing the modification to SharedCache.TableWrapper data structure to include ValidWriteIdList, which represents the transaction state of the cached table. This is the only data structure change needed for the implementation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/synchronized-metastore-cache_110692851.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nThe only data structure change is adding ValidWriteIdList into SharedCache.TableWrapper, which represents the transaction state of the cached table.\n```\n\n----------------------------------------\n\nTITLE: Implementing HiveVolcanoCost for Hive Query Optimization in Java\nDESCRIPTION: This code snippet defines a HiveVolcanoCost class that extends VolcanoCost to implement a custom cost comparison algorithm for Hive. It prioritizes CPU and I/O costs over cardinality when comparing query plans.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/cost-based-optimization-in-hive_42566775.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nClass HiveVolcanoCost extends VolcanoCost {\n\nDouble m_sizeOfTuple;\n\n               @Override\n                  public boolean isLe(RelOptCost other) {\n                                    VolcanoCost that = (VolcanoCost) other;\n                                    if (((this.dCpu + this.dIo) < (that.dCpu + that.dIo))\n                                                                        || ((this.dCpu + this.dIo) == (that.dCpu + that.dIo)\n                                                            && this.dRows <= that.dRows)) {\n                                                      return true;\n                                    } else {\n                                                      return false;\n                                    }\n                  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Blobstore as Scratch Directory in Hive XML\nDESCRIPTION: Enables the use of scratch directories directly on blob storage systems. May cause performance penalties but can be useful in certain scenarios.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_115\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.blobstore.use.blobstore.as.scratchdir</name>\n  <value>false</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Running Standalone Metastore\nDESCRIPTION: Docker command to launch standalone Metastore service with Derby database\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/setting-up-hive-with-docker_282102281.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 9083:9083 --env SERVICE_NAME=metastore --name metastore-standalone apache/hive:${HIVE_VERSION}\n```\n\n----------------------------------------\n\nTITLE: Adding a Partition to a Dependent Table in Hive\nDESCRIPTION: This snippet shows how to add a partition to the dependent table 'Tdep', which implicitly creates dependencies on the corresponding partitions of the original table 'T'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/dependent-tables_30151205.md#2025-04-09_snippet_1\n\nLANGUAGE: HiveQL\nCODE:\n```\n-- create partition T@ds=1/hr=1 to T@ds=1/hr=24\nalter table Tdep add partition (ds=1);\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating a Partitioned Table in Hive\nDESCRIPTION: HQL commands to create a partitioned table without skew specification and insert data. Top K statistics are computed automatically during insertion if enabled.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_7\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE table1 (key STRING, value STRING) PARTITIONED BY (ds STRING);\nINSERT OVERWRITE TABLE table1 PARTITION (ds='2012-09-07') SELECT * FROM table_src;\n```\n\n----------------------------------------\n\nTITLE: Using the LLAP Status Command via HiveServer2\nDESCRIPTION: Command syntax for the LLAP status service through HiveServer2. Used to check the status of LLAP applications with various options for timeout, refresh intervals, and monitoring.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/llap_62689557.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n/current/hive-server2-hive2/bin/hive --service llapstatus --name {llap_app_name} [-f] [-w] [-i] [-t]\n```\n\n----------------------------------------\n\nTITLE: Showing Active Transactions in HQL\nDESCRIPTION: Lists all currently open and aborted transactions in the Hive system. Shows transaction ID, state, user, machine, and timestamps. Available since Hive 0.13.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_89\n\nLANGUAGE: hql\nCODE:\n```\nSHOW TRANSACTIONS;\n```\n\n----------------------------------------\n\nTITLE: Storage API Branch Creation Commands\nDESCRIPTION: Git commands for creating and pushing a new release branch and tag\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -a -m \"Preparing for storage-api X.Y.Z release\"\ngit push -u origin storage-branch-X.Y\ngit tag -a storage-release-X.Y.Z-rcR -m \"Hive Storage API X.Y.Z-rcR release.\"\ngit push origin storage-release-X.Y.Z-rcR\n```\n\n----------------------------------------\n\nTITLE: Defining Thrift Union and Container Structures for Column Statistics\nDESCRIPTION: Thrift definitions for the union that can hold any type of column statistics and container structures to group statistics objects with metadata like table and column information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/column-statistics-in-hive_29131019.md#2025-04-09_snippet_6\n\nLANGUAGE: Thrift\nCODE:\n```\nunion ColumnStatisticsData {  \n1: BooleanColumnStatsData booleanStats,  \n2: LongColumnStatsData longStats,  \n3: DoubleColumnStatsData doubleStats,  \n4: StringColumnStatsData stringStats,  \n5: BinaryColumnStatsData binaryStats,  \n6: DecimalColumnStatsData decimalStats,  \n7: DateColumnStatsData dateStats  \n}\n\nstruct ColumnStatisticsObj {  \n 1: required string colName,  \n 2: required string colType,  \n 3: required ColumnStatisticsData statsData  \n }\n\nstruct ColumnStatisticsDesc {  \n 1: required bool isTblLevel,   \n 2: required string dbName,  \n 3: required string tableName,  \n 4: optional string partName,  \n 5: optional i64 lastAnalyzed  \n }\n\nstruct ColumnStatistics {  \n 1: required ColumnStatisticsDesc statsDesc,  \n 2: required list<ColumnStatisticsObj> statsObj;  \n }\n```\n\n----------------------------------------\n\nTITLE: HCatalog CLI Usage Message\nDESCRIPTION: Shows the basic usage syntax for the HCatalog command line interface with available options for query execution, file processing, group settings, permissions, and property definitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-cli_34013932.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nUsage:  hcat  { -e \"<query>\" | -f <filepath> }  [-g <group>] [-p <perms>] [-D<name>=<value>]\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing Roles in Hive\nDESCRIPTION: Command reference for 'create role' and 'drop role' commands that are used by administrators to manage roles in the Hive authorization system. These commands are restricted to users who belong to the admin role.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\ncreate role\ndrop role\n```\n\n----------------------------------------\n\nTITLE: Using DEFAULT Keyword in UPDATE Statement\nDESCRIPTION: Example showing how to use the DEFAULT keyword in UPDATE statements to reset columns to their default values or NULL if no default is defined.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/75977362.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nUPDATE TABLE1 SET COL1=DEFAULT, COL2=DEFAULT WHERE <CONDITION>\n```\n\n----------------------------------------\n\nTITLE: Using the assert_true() function in Hive SQL\nDESCRIPTION: Throws an exception if the boolean condition is not true; otherwise returns null. Used for validation and debugging. Example: select assert_true(2<1).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_77\n\nLANGUAGE: SQL\nCODE:\n```\nassert_true(boolean condition)\n```\n\n----------------------------------------\n\nTITLE: Dropping Indexes in Hive\nDESCRIPTION: Legacy syntax for removing an index and its associated index table from Hive versions prior to 3.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_56\n\nLANGUAGE: sql\nCODE:\n```\nDROP INDEX [IF EXISTS] index_name ON table_name;\n```\n\n----------------------------------------\n\nTITLE: Enabling Blobstore Optimizations in Hive XML\nDESCRIPTION: Controls whether to enable various optimizations when running Hive on blobstores like Amazon S3. This is a global variable affecting multiple optimizations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_114\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.blobstore.optimizations.enabled</name>\n  <value>true</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Creating an Accumulo-Backed Hive Table\nDESCRIPTION: Shows how to create a Hive table that is backed by an Accumulo table, including the column mapping configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/accumulointegration_46633569.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE accumulo_table(rowid STRING, name STRING, age INT, weight DOUBLE, height INT)\nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,person:name,person:age,person:weight,person:height');\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Thrift Worker Threads for HiveServer2\nDESCRIPTION: Specifies the minimum number of Thrift worker threads for HiveServer2.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_43\n\nLANGUAGE: properties\nCODE:\n```\nhive.server2.thrift.min.worker.threads=5\n```\n\n----------------------------------------\n\nTITLE: WebHCat REST API Endpoints for DDL Operations\nDESCRIPTION: Table of REST API endpoints showing all available DDL operations in WebHCat, including paths and HTTP methods for managing databases, tables, partitions, columns and properties in HCatalog.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-allddl_34016001.md#2025-04-09_snippet_0\n\nLANGUAGE: REST\nCODE:\n```\nDDL Command: POST /ddl\nDatabase Operations:\nGET    /ddl/database\nGET    /ddl/database/:db\nPUT    /ddl/database/:db\nDELETE /ddl/database/:db\n\nTable Operations:\nGET    /ddl/database/:db/table\nGET    /ddl/database/:db/table/:table\nPUT    /ddl/database/:db/table/:table\nPOST   /ddl/database/:db/table/:table\nDELETE /ddl/database/:db/table/:table\nPUT    /ddl/database/:db/table/:existingtable/like/:newtable\n\nPartition Operations:\nGET    /ddl/database/:db/table/:table/partition\nGET    /ddl/database/:db/table/:table/partition/:partition\nPUT    /ddl/database/:db/table/:table/partition/:partition\nDELETE /ddl/database/:db/table/:table/partition/:partition\n\nColumn Operations:\nGET    /ddl/database/:db/table/:table/column\nGET    /ddl/database/:db/table/:table/column/:column\nPUT    /ddl/database/:db/table/:table/column/:column\n\nProperty Operations:\nGET    /ddl/database/:db/table/:table/property\nGET    /ddl/database/:db/table/:table/property/:property\nPUT    /ddl/database/:db/table/:table/property/:property\n```\n\n----------------------------------------\n\nTITLE: Setting Database Ownership in Hive\nDESCRIPTION: Reference to the 'alter database' command which can be used to change the ownership of a database to a specific role. This is part of the object ownership management in Hive's authorization model.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nalter database\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Hive Streaming Connection with Dynamic Partitioning in Java\nDESCRIPTION: This snippet shows how to create a Hive streaming connection with dynamic partitioning, write records in multiple transactions, and close the connection. It uses a StrictDelimitedInputWriter and demonstrates writing records with dynamic partition values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest-v2_85477610.md#2025-04-09_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n.. spin up thread 2 ..\n// dynamic partitioning\n// create delimited record writer whose schema exactly matches table schema\nStrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder()\n                                      .withFieldDelimiter(',')\n                                      .build();\n// create and open streaming connection (default.src table has to exist already)\nStreamingConnection connection = HiveStreamingConnection.newBuilder()\n                                    .withDatabase(dbName)\n                                    .withTable(tblName)\n                                    .withAgentInfo(\"example-agent-1\")\n                                    .withRecordWriter(writer)\n                                    .withHiveConf(hiveConf)\n                                    .connect();\n// begin a transaction, write records and commit 1st transaction\nconnection.beginTransaction();\n// dynamic partition mode where last 2 columns are partition values\nconnection.write(\"11,val11,Asia,China\".getBytes());\nconnection.write(\"12,val12,Asia,India\".getBytes());\nconnection.commitTransaction();\n// begin another transaction, write more records and commit 2nd transaction\nconnection.beginTransaction();\nconnection.write(\"13,val13,Europe,Germany\".getBytes());\nconnection.write(\"14,val14,Asia,India\".getBytes());\nconnection.commitTransaction();\n// close the streaming connection\nconnection.close();\n```\n\n----------------------------------------\n\nTITLE: Using assert_true Function in Hive SQL\nDESCRIPTION: Throws an exception if the condition is not true, otherwise returns null. This function is useful for validating conditions and debugging in Hive queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_26\n\nLANGUAGE: SQL\nCODE:\n```\nassert_true (2<1)\n```\n\n----------------------------------------\n\nTITLE: Altering a Table to Remove Skewed Properties\nDESCRIPTION: This ALTER TABLE statement removes the skewed properties from a table, making it a non-skewed table and turning off list bucketing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE <T> (SCHEMA) NOT SKEWED;\n```\n\n----------------------------------------\n\nTITLE: Subquery Syntax in FROM Clause (Hive)\nDESCRIPTION: Demonstrates the basic syntax for using subqueries in the FROM clause in Hive. Shows both the standard syntax and the syntax with the optional AS keyword that was introduced in Hive 0.13.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-subqueries_27362044.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT ... FROM (subquery) name ...\nSELECT ... FROM (subquery) AS name ...   (Note: Only valid starting with Hive 0.13.0)\n```\n\n----------------------------------------\n\nTITLE: Setting Thread Pool Size in Metastore Configuration\nDESCRIPTION: This code snippet shows the configuration parameter for setting the thread pool size in the Metastore. It's presented as a standalone configuration option.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-metastore-3-0-administration_75978150.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nTHREAD_POOL_SIZE\n```\n\n----------------------------------------\n\nTITLE: Invalid Mapping of Primitive Type to HBase Column Family\nDESCRIPTION: Example of an invalid table definition that fails because it attempts to map a primitive Hive type (string) to an entire HBase column family. Column families must be mapped to MAP types.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbaseintegration_27362089.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE hbase_table_1(key int, value string) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\n\"hbase.columns.mapping\" = \":key,cf:\"\n);\nFAILED: Error in metadata: java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException org.apache.hadoop.hive.hbase.HBaseSerDe: hbase column family 'cf:' should be mapped to map<string,?> but is mapped to string)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Data with Hive Without HCatalog\nDESCRIPTION: This Hive SQL snippet shows how to manually add a partition and query data without using HCatalog's features.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-usinghcat_34013260.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nalter table processedevents add partition 20100819 hdfs://data/processedevents/20100819/data\n\nselect advertiser_id, count(clicks)\nfrom processedevents\nwhere date = '20100819'\ngroup by advertiser_id;\n```\n\n----------------------------------------\n\nTITLE: Extracting Substring from End Position in Hive\nDESCRIPTION: Returns the substring of A starting from the start position till the end of string. The function takes a string or binary input and an integer start position.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nsubstr(string|binary A, int start)\nsubstring(string|binary A, int start)\n```\n\n----------------------------------------\n\nTITLE: Using nullif Function in Hive SQL\nDESCRIPTION: Returns NULL if the two input values are equal, otherwise returns the first value. This is a shorthand for a CASE expression and is useful for handling special cases where equality should result in NULL.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_25\n\nLANGUAGE: SQL\nCODE:\n```\nnullif( a, b )\n```\n\n----------------------------------------\n\nTITLE: Creating Teradata Binary Table in Hive\nDESCRIPTION: Creates a Hive table using TeradataBinarySerde with specific column definitions and Teradata properties. The table includes various data types and configures Teradata-specific properties like timestamp precision, character charset, and row length.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/teradatabinaryserde_89068127.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE `teradata_binary_table_1mb`(\n  `test_tinyint` tinyint,\n  `test_smallint` smallint,\n  `test_int` int,\n  `test_bigint` bigint,\n  `test_double` double,\n  `test_decimal` decimal(15,2),\n  `test_date` date,\n  `test_timestamp` timestamp,\n  `test_char` char(1),\n  `test_varchar` varchar(40),\n  `test_binary` binary\n )\nROW FORMAT SERDE\n  'org.apache.hadoop.hive.serde2.teradata.TeradataBinarySerde'\nSTORED AS INPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.TeradataBinaryFileInputFormat'\nOUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.TeradataBinaryFileOutputFormat'\nTBLPROPERTIES (\n  'teradata.timestamp.precision'='6',\n  'teradata.char.charset'='UNICODE',\n  'teradata.row.length'='1MB'\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Scratch Directory via CLI\nDESCRIPTION: Command to set the Hive scratch directory using the CLI set command. This directory is used by Hive to store temporary output and plans.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-configuration_27362070.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nset hive.exec.scratchdir=/tmp/mydir;\n```\n\n----------------------------------------\n\nTITLE: Configuring Failed Compactions Threshold in Hive\nDESCRIPTION: Sets the number of consecutive failed compactions for a partition before the Initiator stops scheduling automatic compactions. Default is 2.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_72\n\nLANGUAGE: properties\nCODE:\n```\nhive.compactor.initiator.failed.compacts.threshold=2\n```\n\n----------------------------------------\n\nTITLE: Simple Table Write with HCatStorer\nDESCRIPTION: Shows how to write data to a non-partitioned table using HCatStorer.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_12\n\nLANGUAGE: pig\nCODE:\n```\nstore z into 'web_data' using org.apache.hive.hcatalog.pig.HCatStorer();\n```\n\n----------------------------------------\n\nTITLE: Starting Remote Metastore Server via Direct Java Execution\nDESCRIPTION: Command to start a Thrift metastore server by directly executing Java. This method is used in Hive versions earlier than 0.5.0 and requires correctly set environment variables for JAVA_HOME, HIVE_HOME, and HADOOP_HOME.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-metastore-administration_27362076.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$JAVA_HOME/bin/java  -Xmx1024m -Dlog4j.configuration=file://$HIVE_HOME/conf/hms-log4j.properties -Djava.library.path=$HADOOP_HOME/lib/native/Linux-amd64-64/ -cp $CLASSPATH org.apache.hadoop.hive.metastore.HiveMetaStore\n```\n\n----------------------------------------\n\nTITLE: Finding Minimum and Maximum Values in Hive SQL\nDESCRIPTION: Functions to find the minimum and maximum values in a column within a group.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nmin(col)\nmax(col)\n```\n\n----------------------------------------\n\nTITLE: Converting String to Binary Type in Hive\nDESCRIPTION: The binary() function casts a string or binary parameter into a binary type. This is useful for handling binary data formats in Hive queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nbinary(string|binary)\n```\n\n----------------------------------------\n\nTITLE: Hive Variable Configuration Example\nDESCRIPTION: Comprehensive example showing how to set and use Hive variables with the --hiveconf option, including table creation and description.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-variablesubstitution_30754722.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/hive --hiveconf a=b -e 'set a; set hiveconf:a; \\\ncreate table if not exists b (col int); describe ${hiveconf:a}'\n```\n\n----------------------------------------\n\nTITLE: Insert with Explicit Column Values in Hive SQL\nDESCRIPTION: Demonstrates an INSERT statement where all columns are explicitly specified. In this case, DEFAULT values will not be used since values are provided for all columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/75969407.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT INTO <tableName>(col1, col2, col3) values (<val1>, <val2>, <val3>)\n```\n\n----------------------------------------\n\nTITLE: Cloning Hive Website Repository\nDESCRIPTION: Git command to clone the Hive website repository for editing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/apache/hive-site.git\n```\n\n----------------------------------------\n\nTITLE: Pig Command with Additional Jars\nDESCRIPTION: Alternative way to specify HCatalog dependencies using pig.additional.jars parameter\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n<path_to_pig_install>/bin/pig -Dpig.additional.jars=\\\n$HCAT_HOME/share/hcatalog/hcatalog-core*.jar:\\\n$HCAT_HOME/share/hcatalog/hcatalog-pig-adapter*.jar:\\\n$HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:\\\n$HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:\\\n$HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/lib/slf4j-api-*.jar  <script.pig>\n```\n\n----------------------------------------\n\nTITLE: Explaining Query Execution Plan in Hive\nDESCRIPTION: This snippet demonstrates how to use the EXPLAIN command to view the execution plan for a join query between JDBC tables. It shows that the join operation is pushed down to MySQL.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_8\n\nLANGUAGE: HiveQL\nCODE:\n```\nexplain select * from student_jdbc join voter_jdbc on student_jdbc.name=voter_jdbc.name;\n```\n\n----------------------------------------\n\nTITLE: Timezone Conversion in Hive SQL\nDESCRIPTION: Functions for converting timestamps between UTC and specific timezones.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_34\n\nLANGUAGE: SQL\nCODE:\n```\nfrom_utc_timestamp({any primitive type} ts,string timezone)\nto_utc_timestamp({any primitive type} ts,string timezone)\n```\n\n----------------------------------------\n\nTITLE: Registering a Temporary UDF in Hive SQL\nDESCRIPTION: This SQL command registers a temporary UDF in Hive, making it available for use in the current session. The function is associated with the Java class that implements it.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveplugins_27362098.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate temporary function my_lower as 'com.example.hive.udf.Lower';\n```\n\n----------------------------------------\n\nTITLE: Error Response Example\nDESCRIPTION: Sample JSON error response when attempting to describe a non-existent table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-gettable_34016519.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": \"Table xtest_table does not exist\",\n  \"errorCode\": 404,\n  \"database\": \"default\",\n  \"table\": \"xtest_table\"\n}\n```\n\n----------------------------------------\n\nTITLE: Connect to Beeline CLI\nDESCRIPTION: Command to connect to Hive's Beeline CLI interface inside the Docker container.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/Development/quickStart.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it hiveserver2 beeline -u 'jdbc:hive2://hiveserver2:10000/'\n```\n\n----------------------------------------\n\nTITLE: Configuring Number of Reducers in Java\nDESCRIPTION: Sets the default number of reduce tasks per job. A value of -1 allows Hive to automatically determine the number of reducers.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nhiveConf.setInt(\"mapred.reduce.tasks\", -1);\n```\n\n----------------------------------------\n\nTITLE: Testing Multiple Patch Files in Hive\nDESCRIPTION: Command to apply and test multiple patches in sequence, maintaining the order specified on the command line.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhive_repo/testutils/ptest/hivetest.py --test --patch first.patch second.patch\n\n```\n\n----------------------------------------\n\nTITLE: Starting HiveServer in Hive 0.7 and Earlier\nDESCRIPTION: Command examples for starting the Thrift HiveServer in Hive 0.7 and earlier versions, showing the simplified options available in older versions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver_27362111.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ build/dist/bin/hive --service hiveserver --help\nusage HIVE_PORT=xxxx ./hive --service hiveserver\n  HIVE_PORT : Specify the server port\n\n$ bin/hive --service hiveserver\n```\n\n----------------------------------------\n\nTITLE: Using the width_bucket() function in Hive SQL\nDESCRIPTION: Maps a numeric expression into equally sized buckets. Returns an integer between 0 and num_buckets+1, with special handling for values outside the specified range.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_48\n\nLANGUAGE: SQL\nCODE:\n```\nwidth_bucket(NUMERIC expr, NUMERIC min_value, NUMERIC max_value, int num_buckets)\n```\n\n----------------------------------------\n\nTITLE: Building Spark Distribution Without Hive (Spark 2.3.0+)\nDESCRIPTION: Command to build Spark distribution without Hive dependencies for Spark 2.3.0 and later versions, including ORC support.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/44302539.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./dev/make-distribution.sh --name \"hadoop2-without-hive\" --tgz \"-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided,orc-provided\"\n```\n\n----------------------------------------\n\nTITLE: Configuring User DN Pattern for LDAP Authentication in HiveServer2\nDESCRIPTION: This XML snippet shows how to set the hive.server2.authentication.ldap.userDNPattern property to specify the pattern for user distinguished names in LDAP. It allows for a single DN or multiple DNs separated by colons.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>\n    hive.server2.authentication.ldap.userDNPattern\n  </name>\n  <value>\n    CN=%s,CN=Users,DC=apache,DC=org\n  </value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Case W1: Demonstrating Where Predicate on Preserved Row Table in Hive\nDESCRIPTION: This example shows a case where a where clause predicate (s1.key > '2') is applied to a preserved row table (s1) in a left outer join. The execution plan shows the predicate is pushed down and applied as a filter before the join.\nSOURCE: https://github.com/apache/hive-site/blob/main/themes/hive/static/attachments/27362075/35193191.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nexplain \nselect s1.key, s2.key \nfrom src s1 left join src s2 \nwhere s1.key > '2';\n```\n\n----------------------------------------\n\nTITLE: Testing a Differential Revision in Hive\nDESCRIPTION: Command to test a patch from Phabricator using the revision number.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhive_repo/testutils/ptest/hivetest.py --test --revision D123\n\n```\n\n----------------------------------------\n\nTITLE: Disabling Automatic Statistics Collection in Hive\nDESCRIPTION: Configuration setting to disable automatic statistics collection for newly created tables and partitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\nset hive.stats.autogather=false;\n```\n\n----------------------------------------\n\nTITLE: Using the array_contains() function in Hive SQL\nDESCRIPTION: Checks if an array contains a specific value. Returns TRUE if the value is found in the array.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_55\n\nLANGUAGE: SQL\nCODE:\n```\narray_contains(Array<T>, value)\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Mode Sampling Frequency in Hive XML\nDESCRIPTION: Sets the sampling frequency for non-bucketed tables when Hive is running in test mode.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_119\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.test.mode.samplefreq</name>\n  <value>32</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Case W2: Demonstrating Where Predicate on Null Supplying Table in Hive\nDESCRIPTION: This example shows a case where a where clause predicate (s2.key > '2') is applied to a null supplying table (s2) in a left outer join. The execution plan shows the predicate is not pushed down but applied as a filter after the join.\nSOURCE: https://github.com/apache/hive-site/blob/main/themes/hive/static/attachments/27362075/35193191.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nexplain\nselect s1.key, s2.key \nfrom src s1 left join src s2 \nwhere s2.key > '2';\n```\n\n----------------------------------------\n\nTITLE: Defining Frontmatter in Markdown for Privacy Policy Page\nDESCRIPTION: This snippet defines the frontmatter for the privacy policy page in a static site generator. It specifies the title, creation date, and draft status of the page.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/general/PrivacyPolicy.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"Privacy Policy\"\ndate: 2022-09-13T19:35:56+05:30\ndraft: false\n---\n```\n\n----------------------------------------\n\nTITLE: UNION with Global Ordering\nDESCRIPTION: Demonstrates applying ORDER BY and LIMIT clauses to the entire UNION result set.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-union_27362049.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT key FROM src\nUNION\nSELECT key FROM src1 \nORDER BY key LIMIT 10\n```\n\n----------------------------------------\n\nTITLE: Installing Python Modules Globally for Hive Parallel Testing\nDESCRIPTION: Commands to globally install the required Python modules (argparse and mako) using easy_install with administrative privileges.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo easy_install argparse\nsudo easy_install mako\n\n```\n\n----------------------------------------\n\nTITLE: Setting Statistics Configuration Values\nDESCRIPTION: Key configuration parameters for Hive statistics with their default values and version information. These parameters control statistics collection behavior, reliability thresholds, and optimization settings.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_79\n\nLANGUAGE: properties\nCODE:\n```\nhive.stats.reliable=false\nhive.stats.ndv.error=20.0\nhive.stats.collect.tablekeys=false\nhive.stats.collect.scancols=false\nhive.stats.key.prefix.max.length=150\nhive.stats.key.prefix.reserve.length=24\nhive.stats.max.variable.length=100\nhive.analyze.stmt.collect.partlevel.stats=true\nhive.stats.list.num.entries=10\nhive.stats.map.num.entries=10\nhive.stats.map.parallelism=1\nhive.stats.fetch.partition.stats=true\nhive.stats.fetch.column.stats=false\nhive.stats.join.factor=1.1\nhive.stats.deserialization.factor=10.0\nhive.stats.avg.row.size=10000\nhive.compute.query.using.stats=false\n```\n\n----------------------------------------\n\nTITLE: Adding Months to Date in Hive\nDESCRIPTION: Adds specified number of months to a given date. Handles end-of-month cases and supports optional output date formatting as of Hive 4.0.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\nadd_months(string start_date,int num_months, output_date_format)\n```\n\n----------------------------------------\n\nTITLE: Setting Codahale Metrics Reporter Classes\nDESCRIPTION: Specifies the reporter implementation classes for the Codahale metrics system. This setting overrides the hive.service.metrics.reporter configuration when present.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_94\n\nLANGUAGE: plaintext\nCODE:\n```\norg.apache.hadoop.hive.common.metrics.metrics2.JsonFileMetricsReporter, org.apache.hadoop.hive.common.metrics.metrics2.JmxMetricsReporter\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables in hive-env.sh\nDESCRIPTION: Bash script configuration for setting HADOOP_HOME and JAVA_HOME environment variables in the hive-env.sh file, which is required for Hive to locate these dependencies.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/using-tidb-as-the-hive-metastore-database_158872426.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport HADOOP_HOME=...\nexport JAVA_HOME=...\n```\n\n----------------------------------------\n\nTITLE: Writing Data to Hive using Streaming Transactions\nDESCRIPTION: This Java code shows how to write data to Hive using streaming transactions. It demonstrates beginning a transaction, writing data, and committing the transaction.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest_40509746.md#2025-04-09_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n///// Batch 1 - First TXN\ntxnBatch.beginNextTransaction();\ntxnBatch.write(\"1,Hello streaming\".getBytes());\ntxnBatch.write(\"2,Welcome to streaming\".getBytes());\ntxnBatch.commit();\n\nif(txnBatch.remainingTransactions() > 0) {\n///// Batch 1 - Second TXN\ntxnBatch.beginNextTransaction();\ntxnBatch.write(\"3,Roshan Naik\".getBytes());\ntxnBatch.write(\"4,Alan Gates\".getBytes());\ntxnBatch.write(\"5,Owen O'Malley\".getBytes());\ntxnBatch.commit();\n\ntxnBatch.close();\nconnection.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Incorrect GROUP BY with Column Alias\nDESCRIPTION: Example showing an incorrect approach to GROUP BY using a column alias that has a function applied to it. This query will fail with a semantic analysis error in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_60\n\nLANGUAGE: SQL\nCODE:\n```\nselect f(col) as fc, count(*) from table_name group by fc;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Authorization in XML\nDESCRIPTION: XML configuration snippets for enabling Hive authorization and setting default table owner grants in hive-site.xml.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/45876173.md#2025-04-09_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.security.authorization.enabled</name>\n  <value>true</value>\n  <description>enable or disable the hive client authorization</description>\n</property>\n\n<property>\n  <name>hive.security.authorization.createtable.owner.grants</name>\n  <value>ALL</value>\n  <description>the privileges automatically granted to the owner whenever a table gets created. \n   An example like \"select,drop\" will grant select and drop privilege to the owner of the table</description>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Using EXPLAIN LOCKS to View Lock Requirements for a Query\nDESCRIPTION: This example demonstrates using EXPLAIN LOCKS to understand what locks the system will acquire to run an UPDATE query with a subquery, showing which tables and partitions will be locked and the lock modes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN LOCKS UPDATE target SET b = 1 WHERE p IN (SELECT t.q1 FROM source t WHERE t.a1=5)\n```\n\n----------------------------------------\n\nTITLE: Configuring User Filter for LDAP Authentication in HiveServer2\nDESCRIPTION: This XML snippet shows how to set the hive.server2.authentication.ldap.userFilter property to specify a comma-separated list of usernames that are allowed to authenticate with HiveServer2.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>\n    hive.server2.authentication.ldap.userFilter\n  </name>\n  <value>\n    hive-admin,hive,hivetest,hive-user\n  </value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Using XML Elements Format for Query Results in Beeline\nDESCRIPTION: The xmlelements output format in Beeline presents query results in XML format where each row is a \"result\" element. Column values are shown as child elements of the result element, with element names corresponding to column names.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_17\n\nLANGUAGE: xml\nCODE:\n```\n<resultset>\n  <result>\n    <id>1</id>\n    <value>Value1</value>\n    <comment>Test comment 1</comment>\n  </result>\n  <result>\n    <id>2</id>\n    <value>Value2</value>\n    <comment>Test comment 2</comment>\n  </result>\n  <result>\n    <id>3</id>\n    <value>Value3</value>\n    <comment>Test comment 3</comment>\n  </result>\n</resultset>\n```\n\n----------------------------------------\n\nTITLE: Setting HBase as Statistics Storage in Hive\nDESCRIPTION: Configuration to use HBase as the implementation for temporary statistics storage instead of the default Derby or filesystem.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\nset hive.stats.dbclass=hbase;\n```\n\n----------------------------------------\n\nTITLE: Quote String Function Usage\nDESCRIPTION: Returns a quoted string with proper escape characters for single quotes. Includes examples of NULL handling and quote escaping.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_22\n\nLANGUAGE: sql\nCODE:\n```\nquote(string text)\n```\n\n----------------------------------------\n\nTITLE: GROUP BY with Subquery Workaround\nDESCRIPTION: A workaround using subqueries to GROUP BY a column that has a function applied to it. This approach uses an outer query to reference the function result from the subquery.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_61\n\nLANGUAGE: SQL\nCODE:\n```\nselect sq.fc,col1,col2,...,colN,count(*) from\n  (select f(col) as fc,col1,col2,...,colN from table_name) sq\n group by sq.fc,col1,col2,...,colN;\n```\n\n----------------------------------------\n\nTITLE: Parquet and Avro Configuration Properties\nDESCRIPTION: Configuration settings for Parquet and Avro file formats, including timestamp conversion handling.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_35\n\nLANGUAGE: properties\nCODE:\n```\nhive.parquet.timestamp.skip.conversion=true\nhive.avro.timestamp.skip.conversion=false\n```\n\n----------------------------------------\n\nTITLE: Analyzing Query Execution Plan for Druid-Hive Join\nDESCRIPTION: This snippet shows the output of the EXPLAIN command for the complex join query. It provides detailed information about the query plan, including the steps involved in executing the query across Druid and Hive, and how the query optimizer handles the join operation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/druid-integration_65866491.md#2025-04-09_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nhive> explain\n    > SELECT a.channel, b.col1\n    > FROM\n    > (\n    >   SELECT `channel`, max(delta) as m, sum(added)\n    >   FROM druid_table_1\n    >   GROUP BY `channel`, `floor_year`(`__time`)\n    >   ORDER BY m DESC\n    >   LIMIT 1000\n    > ) a\n    > JOIN\n    > (\n    >   SELECT col1, col2\n    >   FROM hive_table_1\n    > ) b\n    > ON a.channel = b.col2;\nOK\nPlan optimized by CBO.\nVertex dependency in root stage\nMap 2 <- Map 1 (BROADCAST_EDGE)\nStage-0\n  Fetch Operator\n    limit:-1\n    Stage-1\n      Map 2\n      File Output Operator [FS_11]\n        Select Operator [SEL_10] (rows=1 width=0)\n          Output:[\"_col0\",\"_col1\"]\n          Map Join Operator [MAPJOIN_16] (rows=1 width=0)\n            Conds:RS_7._col0=SEL_6._col1(Inner),HybridGraceHashJoin:true,Output:[\"_col0\",\"_col2\"]\n          <-Map 1 [BROADCAST_EDGE]\n            BROADCAST [RS_7]\n              PartitionCols:_col0\n              Filter Operator [FIL_2] (rows=1 width=0)\n                predicate:_col0 is not null\n                Select Operator [SEL_1] (rows=1 width=0)\n                  Output:[\"_col0\"]\n                  TableScan [TS_0] (rows=1 width=0)\n                    druid@druid_table_1,druid_table_1,Tbl:PARTIAL,Col:NONE,Output:[\"channel\"],properties:{\"druid.query.json\":\"{\\\"queryType\\\":\\\"groupBy\\\",\\\"dataSource\\\":\\\"wikiticker\\\",\\\"granularity\\\":\\\"all\\\",\\\"dimensions\\\":[{\\\"type\\\":\\\"default\\\",\\\"dimension\\\":\\\"channel\\\"},{\\\"type\\\":\\\"extraction\\\",\\\"dimension\\\":\\\"__time\\\",\\\"outputName\\\":\\\"floor_year\\\",\\\"extractionFn\\\":{\\\"type\\\":\\\"timeFormat\\\",\\\"format\\\":\\\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\\\",\\\"granularity\\\":\\\"year\\\",\\\"timeZone\\\":\\\"UTC\\\",\\\"locale\\\":\\\"en-US\\\"}}],\\\"limitSpec\\\":{\\\"type\\\":\\\"default\\\",\\\"limit\\\":1000,\\\"columns\\\":[{\\\"dimension\\\":\\\"$f2\\\",\\\"direction\\\":\\\"descending\\\",\\\"dimensionOrder\\\":\\\"numeric\\\"}]},\\\"aggregations\\\":[{\\\"type\\\":\\\"doubleMax\\\",\\\"name\\\":\\\"$f2\\\",\\\"fieldName\\\":\\\"delta\\\"},{\\\"type\\\":\\\"doubleSum\\\",\\\"name\\\":\\\"$f3\\\",\\\"fieldName\\\":\\\"added\\\"}],\\\"intervals\\\":[\\\"1900-01-01T00:00:00.000/3000-01-01T00:00:00.000\\\"]}\",\"druid.query.type\":\"groupBy\"}\n          <-Select Operator [SEL_6] (rows=1 width=15)\n              Output:[\"_col0\",\"_col1\"]\n              Filter Operator [FIL_15] (rows=1 width=15)\n                predicate:col2 is not null\n                TableScan [TS_4] (rows=1 width=15)\n                  druid@hive_table_1,hive_table_1,Tbl:COMPLETE,Col:NONE,Output:[\"col1\",\"col2\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Multiple Group DN Patterns for LDAP Authentication in HiveServer2\nDESCRIPTION: This XML configuration demonstrates how to specify multiple group DN patterns for LDAP authentication in HiveServer2. It uses a colon-separated list to define different possible locations for group entries in the LDAP directory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>\n    hive.server2.authentication.ldap.groupDNPattern\n  </name>\n  <value>\n      CN=%s,OU=Groups,DC=apache,DC=org:uid=%s,CN=Users,DC=apache,DC=org\n  </value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Multiple Lateral Views in Hive Query\nDESCRIPTION: Demonstrates how to use multiple lateral view clauses in a single query. This example shows chaining lateral views where the second view references columns created by the first view.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lateralview_27362040.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM exampleTable\nLATERAL VIEW explode(col1) myTable1 AS myCol1\nLATERAL VIEW explode(myCol1) myTable2 AS myCol2;\n\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Statistics Storage in Hive\nDESCRIPTION: Settings for using JDBC-based temporary statistics storage, including database connection string and driver configuration.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nset hive.stats.dbclass=jdbc:derby;\nset hive.stats.dbconnectionstring=\"jdbc:derby:;databaseName=TempStatsStore;create=true\";\nset hive.stats.jdbcdriver=\"org.apache.derby.jdbc.EmbeddedDriver\";\n```\n\n----------------------------------------\n\nTITLE: Basic CBO Plan Output\nDESCRIPTION: The execution plan output from the basic EXPLAIN CBO command, showing the operator tree without cost information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_5\n\nLANGUAGE: plain text\nCODE:\n```\nCBO PLAN:\nHiveSortLimit(sort0=[$0], dir0=[ASC], fetch=[100])\n  HiveProject(c_customer_id=[$1])\n    HiveJoin(condition=[AND(=($3, $7), >($4, $6))], joinType=[inner], algorithm=[none], cost=[not available])\n      HiveJoin(condition=[=($2, $0)], joinType=[inner], algorithm=[none], cost=[not available])\n        HiveProject(c_customer_sk=[$0], c_customer_id=[$1])\n          HiveFilter(condition=[IS NOT NULL($0)])\n            HiveTableScan(table=[[default, customer]], table:alias=[customer])\n        HiveJoin(condition=[=($3, $1)], joinType=[inner], algorithm=[none], cost=[not available])\n          HiveProject(sr_customer_sk=[$0], sr_store_sk=[$1], $f2=[$2])\n            HiveAggregate(group=[{1, 2}], agg#0=[sum($3)])\n              HiveJoin(condition=[=($0, $4)], joinType=[inner], algorithm=[none], cost=[not available])\n                HiveProject(sr_returned_date_sk=[$0], sr_customer_sk=[$3], sr_store_sk=[$7], sr_fee=[$14])\n                  HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($7), IS NOT NULL($3))])\n                    HiveTableScan(table=[[default, store_returns]], table:alias=[store_returns])\n                HiveProject(d_date_sk=[$0])\n                  HiveFilter(condition=[AND(=($6, 2000), IS NOT NULL($0))])\n                    HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim])\n          HiveProject(s_store_sk=[$0])\n            HiveFilter(condition=[AND(=($24, _UTF-16LE'NM'), IS NOT NULL($0))])\n              HiveTableScan(table=[[default, store]], table:alias=[store])\n      HiveProject(_o__c0=[*(/($1, $2), 1.2)], ctr_store_sk=[$0])\n        HiveAggregate(group=[{1}], agg#0=[sum($2)], agg#1=[count($2)])\n          HiveProject(sr_customer_sk=[$0], sr_store_sk=[$1], $f2=[$2])\n            HiveAggregate(group=[{1, 2}], agg#0=[sum($3)])\n              HiveJoin(condition=[=($0, $4)], joinType=[inner], algorithm=[none], cost=[not available])\n                HiveProject(sr_returned_date_sk=[$0], sr_customer_sk=[$3], sr_store_sk=[$7], sr_fee=[$14])\n                  HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($7))])\n                    HiveTableScan(table=[[default, store_returns]], table:alias=[store_returns])\n                HiveProject(d_date_sk=[$0])\n                  HiveFilter(condition=[AND(=($6, 2000), IS NOT NULL($0))])\n                    HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim])\n```\n\n----------------------------------------\n\nTITLE: Supported Scalar Subquery in SELECT\nDESCRIPTION: Example of a supported scalar subquery that computes the sum of ship charges for each customer. This represents the type of query that will be supported in the implementation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/subqueries-in-select_68717850.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT customer.customer_num,\n\t(SELECT SUM(ship_charge) \n\t\tFROM orders\n\t\tWHERE customer.customer_num = orders.customer_num\n\t) AS total_ship_chg\nFROM customer \n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Proxy Settings in core-site.xml\nDESCRIPTION: Hadoop core proxy user configuration settings in core-site.xml to specify allowed groups and hosts for proxy user 'hcat'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-installwebhcat_34015585.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\nhadoop.proxyuser.hcat.group=<comma-separated-groups>\nhadoop.proxyuser.hcat.hosts=<comma-separated-hosts>\n```\n\n----------------------------------------\n\nTITLE: Hive Basic Export and Import Example\nDESCRIPTION: A simple example showing how to export a department table to a specified HDFS location and then import it back.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-importexport_27837968.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nexport table department to 'hdfs_exports_location/department';\nimport from 'hdfs_exports_location/department';\n\n```\n\n----------------------------------------\n\nTITLE: Analyzing Multiple Partitions Statistics\nDESCRIPTION: Example of gathering statistics for multiple partitions by specifying partial partition specifications.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr) COMPUTE STATISTICS;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Attribute Values with XPath in Hive SQL\nDESCRIPTION: Demonstrates using the xpath UDF to get a list of values for a specific attribute in an XML string.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-xpathudf_27362051.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nselect xpath('<a><b id=\"foo\">b1</b><b id=\"bar\">b2</b></a>','//@id') from src limit 1 ;\n```\n\n----------------------------------------\n\nTITLE: Using the COALESCE() function in Hive SQL\nDESCRIPTION: Returns the first non-NULL value from the list of arguments, or NULL if all arguments are NULL. Useful for handling multiple potential NULL values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_73\n\nLANGUAGE: SQL\nCODE:\n```\nCOALESCE(T v1, T v2, ...)\n```\n\n----------------------------------------\n\nTITLE: Copying Data to HDFS and Adding Partition in HCatalog\nDESCRIPTION: This snippet demonstrates how to use 'distcp' to copy data to HDFS and then add a partition to a HCatalog table using the HCatalog CLI.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-usinghcat_34013260.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhadoop distcp file:///file.dat hdfs://data/rawevents/20100819/data\n\nhcat \"alter table rawevents add partition (ds='20100819') location 'hdfs://data/rawevents/20100819/data'\"\n```\n\n----------------------------------------\n\nTITLE: Defining HiveStoragePredicateHandler Interface in Java\nDESCRIPTION: This Java interface allows storage handlers to participate in filter decomposition. It includes a method for decomposing predicates and an inner class to represent the decomposed predicate.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/filterpushdowndev_27362092.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\npublic interface HiveStoragePredicateHandler {\n  public DecomposedPredicate decomposePredicate(\n    JobConf jobConf,\n    Deserializer deserializer,\n    ExprNodeDesc predicate);\n\n  public static class DecomposedPredicate {\n    public ExprNodeDesc pushedPredicate;\n    public ExprNodeDesc residualPredicate;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Archiving Settings\nDESCRIPTION: Sets the necessary configuration parameters for enabling and customizing Hive archiving functionality. This includes enabling archiving, allowing parent directory setting, and specifying the archive file size.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-archiving_27362031.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nhive> set hive.archive.enabled=true;\nhive> set hive.archive.har.parentdir.settable=true;\nhive> set har.partfile.size=1099511627776;\n```\n\n----------------------------------------\n\nTITLE: Range Filter Using AND Operator\nDESCRIPTION: Demonstrates filtering data within a date range using AND operator.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_9\n\nLANGUAGE: pig\nCODE:\n```\na = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader();\nb = filter a by datestamp >= '20110924' and datestamp <= '20110925';\n```\n\n----------------------------------------\n\nTITLE: Masking Non-Deterministic Outputs in Query File Tests\nDESCRIPTION: Demonstrates how to use the qt:replace option to mask non-deterministic outputs in test results, ensuring consistent test outcomes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/Development/qtest.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n--! qt:replace:/(non-deterministic-output\\s)[0-9]{4}-[0-9]{2}-[0-9]{2}/$1#Masked#/\nSELECT 'non-deterministic-output', CURRENT_DATE();\n```\n\n----------------------------------------\n\nTITLE: Using parse_url_tuple UDTF with LATERAL VIEW\nDESCRIPTION: Example of parse_url_tuple() UDTF with LATERAL VIEW to extract multiple components from a URL in a single operation. This function can extract host, path, query parameters and other URL parts efficiently.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_59\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT b.*\nFROM src LATERAL VIEW parse_url_tuple(fullurl, 'HOST', 'PATH', 'QUERY', 'QUERY:id') b as host, path, query, query_id LIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Configuring LZO Codecs in Hadoop core-site.xml\nDESCRIPTION: XML configuration snippet for adding LZO codec support to Hadoop. This defines the compression codecs and LZO class implementations that should be used.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lzo_33298193.md#2025-04-09_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n <name>io.compression.codecs</name>\n <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec</value>\n </property>\n\n<property>\n <name>io.compression.codec.lzo.class</name>\n <value>com.hadoop.compression.lzo.LzoCodec</value>\n </property>\n```\n\n----------------------------------------\n\nTITLE: Setting required Hive configuration for S3/EC2 access\nDESCRIPTION: This snippet shows how to configure Hive with the necessary settings for accessing S3 data through EC2. It includes setting up SOCKS proxy configuration, AWS credentials, and map-reduce task parameters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nhive> set hadoop.socks.server=localhost:2600; \nhive> set hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.SocksSocketFactory;\nhive> set hadoop.job.ugi=root,root;\nhive> set mapred.map.tasks=40;\nhive> set mapred.reduce.tasks=-1;\nhive> set fs.s3n.awsSecretAccessKey=2GAHKWG3+1wxcqyhpj5b1Ggqc0TIxj21DKkidjfz\nhive> set fs.s3n.awsAccessKeyId=1B5JYHPQCXW13GWKHAG2\n```\n\n----------------------------------------\n\nTITLE: Using length() UDF in Basic Query\nDESCRIPTION: Example of a simple UDF usage that shows how UDFs operate on each row during the map phase of a MapReduce job. The length() function calculates string length for each row.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_63\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT length(string_col) FROM table_name;\n```\n\n----------------------------------------\n\nTITLE: Setting HCatalog Input Format with Partition Filter\nDESCRIPTION: Example of setting up HCatalog input format with partition filtering for a specific date.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-inputoutput_34013776.md#2025-04-09_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nHCatInputFormat.setInput(job,\n                         InputJobInfo.create(dbName, inputTableName, \"ds=\\\"20110924\\\"\"));\n```\n\n----------------------------------------\n\nTITLE: Querying a Row Count Sample in HiveQL\nDESCRIPTION: This example demonstrates how to select the first 10 rows from each input split of the 'source' table using row count sampling.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM source TABLESAMPLE(10 ROWS);\n```\n\n----------------------------------------\n\nTITLE: Inefficient JSON Parsing with Multiple get_json_object Calls\nDESCRIPTION: Example of an inefficient approach to extract multiple values from a JSON string using multiple get_json_object() calls. This approach parses the same JSON string multiple times.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_57\n\nLANGUAGE: SQL\nCODE:\n```\nselect a.timestamp, get_json_object(a.appevents, '$.eventid'), get_json_object(a.appenvets, '$.eventname') from log a;\n```\n\n----------------------------------------\n\nTITLE: Contextual N-grams Function\nDESCRIPTION: Extracts top-k contextual N-grams from tokenized sentences\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\ncontext_ngrams(array<array<string>>, array<string>, int K, int pf)\n```\n\n----------------------------------------\n\nTITLE: Setting Hadoop Home for Direct Debugging\nDESCRIPTION: Environment variable setup for debugging Hive without using Ant build system.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n    > export HADOOP_HOME=<your hadoop home>\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Configuration via Command Line\nDESCRIPTION: Example of setting Hive configuration using the --hiveconf option when launching the Hive CLI or Beeline.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-configuration_27362070.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/hive --hiveconf hive.exec.scratchdir=/tmp/mydir\n```\n\n----------------------------------------\n\nTITLE: Creating and Modifying Metastore Tables for Roles and Privileges in MySQL\nDESCRIPTION: This SQL script creates and modifies tables for managing roles, global privileges, database privileges, table privileges, partition privileges, and column-level privileges in Apache Hive's metastore. It includes table definitions with primary keys, foreign keys, and indexes for efficient querying.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/authdev_27362078.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n--\n-- Table structure for table {{ROLES}}\n--\n\nDROP TABLE IF EXISTS {{ROLES}};\nCREATE TABLE {{ROLES}} (\n  {{ROLE_ID}} bigint(20) NOT NULL,\n  {{CREATE_TIME}} int(11) NOT NULL,\n  {{OWNER_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{ROLE_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  PRIMARY KEY  ({{ROLE_ID}}),\n  UNIQUE KEY {{ROLEENTITYINDEX}} ({{ROLE_NAME}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\n--\n-- Table structure for table {{ROLE_MAP}}\n--\n\nDROP TABLE IF EXISTS {{ROLE_MAP}};\nCREATE TABLE {{ROLE_MAP}} (\n  {{ROLE_GRANT_ID}} bigint(20) NOT NULL,\n  {{ADD_TIME}} int(11) NOT NULL,\n  {{GRANT_OPTION}} smallint(6) NOT NULL,\n  {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{ROLE_ID}} bigint(20) default NULL,\n  PRIMARY KEY  ({{ROLE_GRANT_ID}}),\n  UNIQUE KEY {{USERROLEMAPINDEX}} ({{PRINCIPAL_NAME}},{{ROLE_ID}},{{GRANTOR}},{{GRANTOR_TYPE}}),\n  KEY {{ROLE_MAP_N49}} ({{ROLE_ID}}),\n  CONSTRAINT {{ROLE_MAP_FK1}} FOREIGN KEY ({{ROLE_ID}}) REFERENCES {{ROLES}} ({{ROLE_ID}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\n--\n-- Table structure for table {{GLOBAL_PRIVS}}\n--\n\nDROP TABLE IF EXISTS {{GLOBAL_PRIVS}};\nCREATE TABLE {{GLOBAL_PRIVS}} (\n  {{USER_GRANT_ID}} bigint(20) NOT NULL,\n  {{CREATE_TIME}} int(11) NOT NULL,\n  {{GRANT_OPTION}} smallint(6) NOT NULL,\n  {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{USER_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  PRIMARY KEY  ({{USER_GRANT_ID}}),\n  UNIQUE KEY {{GLOBALPRIVILEGEINDEX}} ({{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{USER_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\n--\n-- Table structure for table {{DB_PRIVS}}\n--\n\nDROP TABLE IF EXISTS {{DB_PRIVS}};\nCREATE TABLE {{DB_PRIVS}} (\n  {{DB_GRANT_ID}} bigint(20) NOT NULL,\n  {{CREATE_TIME}} int(11) NOT NULL,\n  {{DB_ID}} bigint(20) default NULL,\n  {{GRANT_OPTION}} smallint(6) NOT NULL,\n  {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{DB_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  PRIMARY KEY  ({{DB_GRANT_ID}}),\n  UNIQUE KEY {{DBPRIVILEGEINDEX}} ({{DB_ID}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{DB_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}),\n  KEY {{DB_PRIVS_N49}} ({{DB_ID}}),\n  CONSTRAINT {{DB_PRIVS_FK1}} FOREIGN KEY ({{DB_ID}}) REFERENCES {{DBS}} ({{DB_ID}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\n--\n-- Table structure for table {{TBL_PRIVS}}\n--\n\nDROP TABLE IF EXISTS {{TBL_PRIVS}};\n\nCREATE TABLE {{TBL_PRIVS}} (\n  {{TBL_GRANT_ID}} bigint(20) NOT NULL,\n  {{CREATE_TIME}} int(11) NOT NULL,\n  {{GRANT_OPTION}} smallint(6) NOT NULL,\n  {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{TBL_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{TBL_ID}} bigint(20) default NULL,\n  PRIMARY KEY  ({{TBL_GRANT_ID}}),\n  KEY {{TBL_PRIVS_N49}} ({{TBL_ID}}),\n  KEY {{TABLEPRIVILEGEINDEX}} ({{TBL_ID}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{TBL_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}),\n  CONSTRAINT {{TBL_PRIVS_FK1}} FOREIGN KEY ({{TBL_ID}}) REFERENCES {{TBLS}} ({{TBL_ID}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\n--\n-- Table structure for table {{PART_PRIVS}}\n--\n\nDROP TABLE IF EXISTS {{PART_PRIVS}};\n\nCREATE TABLE {{PART_PRIVS}} (\n  {{PART_GRANT_ID}} bigint(20) NOT NULL,\n  {{CREATE_TIME}} int(11) NOT NULL,\n  {{GRANT_OPTION}} smallint(6) NOT NULL,\n  {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PART_ID}} bigint(20) default NULL,\n  {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PART_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  PRIMARY KEY  ({{PART_GRANT_ID}}),\n  KEY {{PARTPRIVILEGEINDEX}} ({{PART_ID}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{PART_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}),\n  KEY {{PART_PRIVS_N49}} ({{PART_ID}}),\n  CONSTRAINT {{PART_PRIVS_FK1}} FOREIGN KEY ({{PART_ID}}) REFERENCES {{PARTITIONS}} ({{PART_ID}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\n--\n-- Table structure for table {{TBL_COL_PRIVS}}\n--\n\nDROP TABLE IF EXISTS {{TBL_COL_PRIVS}};\nCREATE TABLE {{TBL_COL_PRIVS}} (\n  {{TBL_COLUMN_GRANT_ID}} bigint(20) NOT NULL,\n  {{COLUMN_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{CREATE_TIME}} int(11) NOT NULL,\n  {{GRANT_OPTION}} smallint(6) NOT NULL,\n  {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{TBL_COL_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{TBL_ID}} bigint(20) default NULL,\n  PRIMARY KEY  ({{TBL_COLUMN_GRANT_ID}}),\n  KEY {{TABLECOLUMNPRIVILEGEINDEX}} ({{TBL_ID}},{{COLUMN_NAME}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{TBL_COL_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}),\n  KEY {{TBL_COL_PRIVS_N49}} ({{TBL_ID}}),\n  CONSTRAINT {{TBL_COL_PRIVS_FK1}} FOREIGN KEY ({{TBL_ID}}) REFERENCES {{TBLS}} ({{TBL_ID}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\nDROP TABLE IF EXISTS {{PART_COL_PRIVS}};\nCREATE TABLE {{PART_COL_PRIVS}} (\n  {{PART_COLUMN_GRANT_ID}} bigint(20) NOT NULL,\n  {{COLUMN_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{CREATE_TIME}} int(11) NOT NULL,\n  {{GRANT_OPTION}} smallint(6) NOT NULL,\n  {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PART_ID}} bigint(20) default NULL,\n  {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  {{PART_COL_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL,\n  PRIMARY KEY  ({{PART_COLUMN_GRANT_ID}}),\n  KEY {{PART_COL_PRIVS_N49}} ({{PART_ID}}),\n  KEY {{PARTITIONCOLUMNPRIVILEGEINDEX}} ({{PART_ID}},{{COLUMN_NAME}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{PART_COL_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}),\n  CONSTRAINT {{PART_COL_PRIVS_FK1}} FOREIGN KEY ({{PART_ID}}) REFERENCES {{PARTITIONS}} ({{PART_ID}})\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n```\n\n----------------------------------------\n\nTITLE: Example SQL Filter in Apache Hive\nDESCRIPTION: This SQL snippet demonstrates a composite filter condition that might be used in a Hive query. It combines a simple range condition with a function call.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/filterpushdowndev_27362092.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nx > 3 AND upper(y) = 'XYZ'\n```\n\n----------------------------------------\n\nTITLE: Creating Index in a Specific Table in HiveQL\nDESCRIPTION: Creates a compact index on column5 of table04 with deferred rebuild, storing the index in a specified table called table04_index_table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_3\n\nLANGUAGE: hiveql\nCODE:\n```\nCREATE INDEX table04_index ON TABLE table04 (column5) AS 'COMPACT' WITH DEFERRED REBUILD IN TABLE table04_index_table;\n```\n\n----------------------------------------\n\nTITLE: Altering Decimal Columns in Partitioned Tables using Dynamic Partitioning in Hive SQL\nDESCRIPTION: Shows how to alter decimal columns in all partitions of a table using dynamic partitioning. This requires setting hive.exec.dynamic.partition to true.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-types_27838462.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nSET hive.exec.dynamic.partition = true;\n\n-- hive.exec.dynamic.partition needs to be set to true to enable dynamic partitioning with ALTER PARTITION\n-- This will alter all existing partitions of the table - be sure you know what you are doing!\nALTER TABLE foo PARTITION (ds, hr) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Frontmatter for Hugo Page in Markdown\nDESCRIPTION: This snippet defines the YAML frontmatter for a Hugo page, specifying the title, date, and draft status of the content.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/_index.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"Apache Hive\"\ndate: 2023-01-27T19:16:15+05:30\ndraft: false\n---\n```\n\n----------------------------------------\n\nTITLE: Building Apache Hive Website Locally\nDESCRIPTION: Command to build the Hive website locally using CMS build scripts. Used to test website changes before committing to production.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/how-to-edit-the-website_33294834.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbuildsite/build_site.pl --source-base hive-site --target-base hive-website\n```\n\n----------------------------------------\n\nTITLE: Building Hive Master Branch\nDESCRIPTION: Maven commands to build Hive master branch without running tests\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -DskipTests\ncd itests \nmvn clean install -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Granting and Revoking Roles in Hive SQL\nDESCRIPTION: SQL syntax for granting and revoking roles to users, groups, or other roles in Hive, including the WITH ADMIN OPTION clause.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/45876173.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT ROLE role_name [, role_name] ...\nTO principal_specification [, principal_specification] ...\n[WITH ADMIN OPTION]\n\nREVOKE [ADMIN OPTION FOR] ROLE role_name [, role_name] ...\nFROM principal_specification [, principal_specification] ...\n\nprincipal_specification:\n    USER user\n  | GROUP group\n  | ROLE role\n```\n\n----------------------------------------\n\nTITLE: Explain Plan for Join Query Before Hive 0.8.0\nDESCRIPTION: This explain plan shows how Hive processes the join query before version 0.8.0, demonstrating that the filter is only applied to the 'invites' table and not to 'invites2'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/transitivity-on-predicate-pushdown_27823388.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nSTAGE DEPENDENCIES:\n  Stage-1 is a root stage\n  Stage-2 depends on stages: Stage-1\n  Stage-0 is a root stage\n\nSTAGE PLANS:\n  Stage: Stage-1\n    Map Reduce\n      Alias -> Map Operator Tree:\n        invites \n          TableScan\n            alias: invites\n            Filter Operator\n              predicate:\n                  expr: (ds = '2011-01-01')\n                  type: boolean\n              Reduce Output Operator\n                key expressions:\n                      expr: ds\n                      type: string\n                sort order: +\n                Map-reduce partition columns:\n                      expr: ds\n                      type: string\n                tag: 0\n                value expressions:\n                      expr: ds\n                      type: string\n        invites2 \n          TableScan\n            alias: invites2\n            Reduce Output Operator\n              key expressions:\n                    expr: ds\n                    type: string\n              sort order: +\n              Map-reduce partition columns:\n                    expr: ds\n                    type: string\n              tag: 1\n      Reduce Operator Tree:\n        Join Operator\n          condition map:\n               Inner Join 0 to 1\n          condition expressions:\n            0 {VALUE._col2}\n            1 \n          handleSkewJoin: false\n          outputColumnNames: _col2\n          Select Operator\n            Group By Operator\n              aggregations:\n                    expr: count()\n              bucketGroup: false\n              mode: hash\n              outputColumnNames: _col0\n              File Output Operator\n                compressed: false\n                GlobalTableId: 0\n                table:\n                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n\n  Stage: Stage-2\n    Map Reduce\n      Alias -> Map Operator Tree:\n        file:/var/folders/nt/ng21tg0n1jl4547lw0k8lg6hq_nw87/T/charleschen/hive_2011-08-04_10-59-05_697_8934329734633337337/-mr-10002 \n            Reduce Output Operator\n              sort order: \n              tag: -1\n              value expressions:\n                    expr: _col0\n                    type: bigint\n      Reduce Operator Tree:\n        Group By Operator\n          aggregations:\n                expr: count(VALUE._col0)\n          bucketGroup: false\n          mode: mergepartial\n          outputColumnNames: _col0\n          Select Operator\n            expressions:\n                  expr: _col0\n                  type: bigint\n            outputColumnNames: _col0\n            File Output Operator\n              compressed: false\n              GlobalTableId: 0\n              table:\n                  input format: org.apache.hadoop.mapred.TextInputFormat\n                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n```\n\n----------------------------------------\n\nTITLE: Using the array() function in Hive SQL\nDESCRIPTION: Creates an array from the given parameters. Returns an array containing all the input objects.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_57\n\nLANGUAGE: SQL\nCODE:\n```\narray(obj1, obj2, ....  objN)\n```\n\n----------------------------------------\n\nTITLE: Sample Data Format for LZO Compression in Hive\nDESCRIPTION: Example of a simple data file containing four records with columns for id, first name, and last name that will be compressed using LZO.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lzo_33298193.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n19630001     john          lennon\n19630002     paul          mccartney\n19630003     george        harrison\n19630004     ringo         starr\n\n```\n\n----------------------------------------\n\nTITLE: FROM Clause Subquery with UNION ALL in Hive\nDESCRIPTION: Demonstrates a more complex example where the subquery in the FROM clause contains a UNION ALL operation combining results from two different tables before the outer query processes the results.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-subqueries_27362044.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT t3.col\nFROM (\n  SELECT a+b AS col\n  FROM t1\n  UNION ALL\n  SELECT c+d AS col\n  FROM t2\n) t3\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Struct Fields in Hive SQL\nDESCRIPTION: Demonstrates how to access fields within a struct using dot notation. The example shows accessing the 'foo' field of a struct named 'foobar' that contains integer fields.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nfoobar.foo -- returns the integer stored in the foo field of the struct\n```\n\n----------------------------------------\n\nTITLE: Building Hive Package with Ant\nDESCRIPTION: Basic command to build Hive package using Ant build system against Hadoop version 0.19.0\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/developerguide_27362074.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nant package\n```\n\n----------------------------------------\n\nTITLE: Available Record Writers in Hive 3.0.0\nDESCRIPTION: List of supported record writer implementations for handling different data formats in Hive streaming operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/streaming-data-ingest-v2_85477610.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nStrictDelimitedInputWriter\n```\n\nLANGUAGE: java\nCODE:\n```\nStrictJsonWriter\n```\n\nLANGUAGE: java\nCODE:\n```\nStrictRegexWriter\n```\n\n----------------------------------------\n\nTITLE: Unsupported Correlated Variable in Subquery Expression\nDESCRIPTION: Example of a subquery where the correlated variable is used outside of a filter clause, which is not supported. Correlated variables are only permitted in WHERE or HAVING clauses.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/subqueries-in-select_68717850.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT customer.customer_num,\n\t(SELECT customer.customer_num \n\t\tFROM orders\n\t\tWHERE customer.customer_num = orders.customer_num\n\t) AS total_ship_chg\nFROM customer \n```\n\n----------------------------------------\n\nTITLE: WebHCat Status API JSON Response\nDESCRIPTION: This JSON output represents the response from the WebHCat status API. It includes the status of the server (\"ok\" if contacted successfully) and the API version number.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-status_34015941.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"status\": \"ok\",\n \"version\": \"v1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using EXPLAIN FORMATTED LOCKS for JSON Output\nDESCRIPTION: This command allows getting the lock information in JSON format by using FORMATTED keyword with the EXPLAIN LOCKS command.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_22\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN FORMATTED LOCKS <sql>\n```\n\n----------------------------------------\n\nTITLE: Basic Scheduled Query Example in Hive SQL\nDESCRIPTION: Demonstrates creating and managing a basic scheduled query that inserts data into a table every 10 minutes. Shows how to create, enable, monitor and execute scheduled queries using information_schema views.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/scheduled-queries_145724128.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\ncreate table t (a integer);\n\n-- create a scheduled query; every 10 minute insert a new row\ncreate scheduled query sc1 cron '0 */10 * * * ? *' as insert into t values (1);\n-- depending on hive.scheduled.queries.create.as.enabled the query might get create in disabled mode\n-- it can be enabled using:\nalter scheduled query sc1 enabled;\n\n-- inspect scheduled queries using the information_schema\nselect * from information_schema.scheduled_queries s where schedule_name='sc1';\n\n-- wait 10 minutes or execute by issuing:\nalter scheduled query sc1 execute;\n\nselect * from information_schema.scheduled_executions s where schedule_name='sc1' order by scheduled_execution_id desc limit 1;\n```\n\n----------------------------------------\n\nTITLE: Git Branch Management for Storage API Release\nDESCRIPTION: Commands for checking out master branch and preparing for a new Storage API release\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout master\n```\n\n----------------------------------------\n\nTITLE: MySQL Connection Timeout Fix\nDESCRIPTION: SQL command to resolve MySQL connection timeout issues when using MySQL as Hive metastore by setting the global wait status.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-faq_27362095.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nset global wait_status=120;\n```\n\n----------------------------------------\n\nTITLE: Configuring Lineage Information Capture for Hive Queries (YAML)\nDESCRIPTION: Examples of how to set the 'hive.lineage.hook.info.query.type' configuration to capture lineage information for different query types in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/capture-lineage-info.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nhive.lineage.hook.info.query.type=ALL                                -- will capture lineage info for all the queries.\nhive.lineage.hook.info.query.type=CREATE_VIEW,CREATE_TABLE_AS_SELECT -- will capture lineage info related to these 2 particulare query types only.\nhive.lineage.hook.info.query.type=NONE                               -- will not capture lineage info for any query.\n```\n\n----------------------------------------\n\nTITLE: Showing Connectors in HiveQL\nDESCRIPTION: Syntax for listing all connectors defined in the Hive metastore. Available since Hive 4.0.0 and depends on user access permissions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_67\n\nLANGUAGE: hql\nCODE:\n```\nSHOW CONNECTORS;\n```\n\n----------------------------------------\n\nTITLE: Dependency Graph Output from EXPLAIN in Apache Hive\nDESCRIPTION: The dependency graph output from the EXPLAIN command, showing the relationships between different stages of query execution.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nSTAGE DEPENDENCIES:\n  Stage-1 is a root stage\n  Stage-2 depends on stages: Stage-1\n  Stage-0 depends on stages: Stage-2\n```\n\n----------------------------------------\n\nTITLE: Traditional Multi-Partition Insert in Hive\nDESCRIPTION: Example showing how to insert data into multiple static partitions using separate INSERT statements for each country partition. This approach requires prior knowledge of partition values and creates multiple MapReduce jobs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nFROM page_view_stg pvs\nINSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='US')\n       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'US'\nINSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='CA')\n       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'CA'\nINSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='UK')\n       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'UK';\n```\n\n----------------------------------------\n\nTITLE: LAG Function in HiveQL\nDESCRIPTION: Demonstrates the LAG function with a specified lag of 3 rows and a default value of 0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT a, LAG(a, 3, 0) OVER (PARTITION BY b ORDER BY C)\nFROM T;\n```\n\n----------------------------------------\n\nTITLE: HCatalog Environment Setup\nDESCRIPTION: Bash environment configuration for running Pig with HCatalog when not using -useHCatalog flag\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport HADOOP_HOME=<path_to_hadoop_install>\n\nexport HIVE_HOME=<path_to_hive_install>\n\nexport HCAT_HOME=<path_to_hcat_install>\n\nexport PIG_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-core*.jar:\\\n$HCAT_HOME/share/hcatalog/hcatalog-pig-adapter*.jar:\\\n$HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:\\\n$HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:\\\n$HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/conf:$HADOOP_HOME/conf:\\\n$HIVE_HOME/lib/slf4j-api-*.jar\n\nexport PIG_OPTS=-Dhive.metastore.uris=thrift://<hostname>:<port>\n```\n\n----------------------------------------\n\nTITLE: Viewing Partition Statistics\nDESCRIPTION: Command to view stored statistics for a specific partition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE EXTENDED TABLE1 PARTITION(ds='2008-04-09', hr=11);\n```\n\n----------------------------------------\n\nTITLE: Supported Database Types for Hive Schema Tool\nDESCRIPTION: List of database types supported by the Hive Schema Tool, which includes popular relational databases and the Hive option for system schema only.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n derby|mysql|postgres|oracle|mssql|hive\n```\n\n----------------------------------------\n\nTITLE: ASCII Character Conversion\nDESCRIPTION: Returns ASCII character corresponding to numeric input\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\nchr(bigint|double A)\n```\n\n----------------------------------------\n\nTITLE: Displaying Configuration Properties in HQL\nDESCRIPTION: Returns information about a specified Hive configuration property, including its default value, required type, and description. Available since Hive 0.14.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_88\n\nLANGUAGE: hql\nCODE:\n```\nSHOW CONF <configuration_name>;\n```\n\n----------------------------------------\n\nTITLE: Basic Exchange Partition Syntax in Hive\nDESCRIPTION: Shows the basic syntax for the EXCHANGE PARTITION command in Hive. This command moves a partition from a source table to a destination table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/exchange-partition_30755801.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <dest_table> EXCHANGE PARTITION (<[partial] partition spec>) WITH TABLE <src_table>\n```\n\n----------------------------------------\n\nTITLE: Default Authorization Configuration Whitelist\nDESCRIPTION: Example of the default configuration properties whitelist that can be modified when using SQL standard authorization in Hive 0.13.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_87\n\nLANGUAGE: properties\nCODE:\n```\nhive.exec.reducers.bytes.per.reducer, hive.exec.reducers.max, hive.map.aggr, hive.map.aggr.hash.percentmemory, hive.map.aggr.hash.force.flush.memory.threshold\n```\n\n----------------------------------------\n\nTITLE: Creating Schemas and Tables in Oracle\nDESCRIPTION: This snippet demonstrates how to create schemas (users) and tables in Oracle. It creates 'bob' and 'alice' users, each with a 'country' table and sample data.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SESSION SET CONTAINER = XEPDB1;\n\nCREATE USER bob IDENTIFIED BY bobpass;\nALTER USER bob QUOTA UNLIMITED ON users;\nGRANT CREATE SESSION TO bob;\n\nCREATE TABLE bob.country\n(\n    id   int,\n    name varchar(20)\n);\n\ninsert into bob.country\nvalues (1, 'India');\ninsert into bob.country\nvalues (2, 'Russia');\ninsert into bob.country\nvalues (3, 'USA');\n\nCREATE USER alice IDENTIFIED BY alicepass;\nALTER USER alice QUOTA UNLIMITED ON users;\n\nGRANT CREATE SESSION TO alice;\nCREATE TABLE alice.country\n(\n    id   int,\n    name varchar(20)\n);\n\ninsert into alice.country\nvalues (4, 'Italy');\ninsert into alice.country\nvalues (5, 'Greece');\ninsert into alice.country\nvalues (6, 'China');\ninsert into alice.country\nvalues (7, 'Japan');\n```\n\n----------------------------------------\n\nTITLE: Setting Seed Number for Block Sampling in HiveQL\nDESCRIPTION: This command shows how to set a seed number for block sampling, allowing for consistent sampling across multiple queries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sampling_27362042.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.sample.seednumber=<INTEGER>;\n```\n\n----------------------------------------\n\nTITLE: Cluster By and Distribute By Examples\nDESCRIPTION: Demonstrates various ways to use CLUSTER BY and DISTRIBUTE BY clauses for data distribution and sorting.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sortby_27362045.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT col1, col2 FROM t1 CLUSTER BY col1\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT col1, col2 FROM t1 DISTRIBUTE BY col1\n\nSELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SORT BY col1 ASC, col2 DESC\n```\n\nLANGUAGE: sql\nCODE:\n```\n  FROM (\n    FROM pv_users\n    MAP ( pv_users.userid, pv_users.date )\n    USING 'map_script'\n    AS c1, c2, c3\n    DISTRIBUTE BY c2\n    SORT BY c2, c1) map_output\n  INSERT OVERWRITE TABLE pv_users_reduced\n    REDUCE ( map_output.c1, map_output.c2, map_output.c3 )\n    USING 'reduce_script'\n    AS date, count;\n```\n\n----------------------------------------\n\nTITLE: Creating a List Bucketing Table in Hive\nDESCRIPTION: This SQL statement creates a table with list bucketing. It specifies skewed columns and values, and optionally stores them as directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/listbucketing_27846854.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE <T> (SCHEMA) SKEWED BY (keys) ON ('c1', 'c2') [STORED AS DIRECTORIES];\n```\n\n----------------------------------------\n\nTITLE: Replacing Columns in Hive Table\nDESCRIPTION: Alters an existing table by replacing all columns with a new set of columns.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_15\n\nLANGUAGE: HiveQL\nCODE:\n```\nALTER TABLE old_table_name REPLACE COLUMNS (col1 TYPE, ...);\n```\n\n----------------------------------------\n\nTITLE: Configuring Group DN Pattern for LDAP Authentication in HiveServer2\nDESCRIPTION: This XML snippet shows how to set the hive.server2.authentication.ldap.groupDNPattern property to specify the pattern for group distinguished names in LDAP. It allows for a single DN or multiple DNs separated by colons.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>\n    hive.server2.authentication.ldap.groupDNPattern\n  </name>\n  <value>CN=%s,OU=Groups,DC=apache,DC=org</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Session Idle Check Frequency in Hive\nDESCRIPTION: Property to set how frequently Hive checks for idle Spark sessions. The minimum value is 60 seconds.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_50\n\nLANGUAGE: properties\nCODE:\n```\nhive.spark.session.timeout.period = 60 seconds\n```\n\n----------------------------------------\n\nTITLE: Implementing IStatsAggregator Interface for Top K Statistics in Java\nDESCRIPTION: Java interface method to aggregate top K statistics from temporary storage. This method is added to the IStatsAggregator interface to support the collection of top K column values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n...\n\npublic interface IStatsAggregator {\n\n...\n\n  /**\n * This method aggregates top K statistics.\n   *\n * */\n  public List<String> aggregateStatsTopK(String keyPrefix, String statType);\n\n...\n\n}\n```\n\n----------------------------------------\n\nTITLE: Basic IRC Channel Reference\nDESCRIPTION: IRC channel information for Apache Hive community chat\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/home_27362069.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n#hive on irc.freenode.net\n```\n\n----------------------------------------\n\nTITLE: Enabling Local Mode Execution in Hive\nDESCRIPTION: Configuration to run Hive queries in local mode, which executes map-reduce jobs on the local workstation instead of a cluster.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n  hive> SET mapreduce.framework.name=local;\n```\n\n----------------------------------------\n\nTITLE: Basic LIMIT Clause Usage\nDESCRIPTION: Shows how to use the LIMIT clause to constrain the number of rows returned by a SELECT statement. The example limits the result to 5 arbitrary customers.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-select_27362043.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM customers LIMIT 5\n```\n\n----------------------------------------\n\nTITLE: Unsupported Non-Scalar Subquery in SELECT\nDESCRIPTION: Example of a subquery that may return multiple rows, which is not supported. Scalar subqueries in the SELECT list must return at most one row, and Hive will validate this at runtime.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/subqueries-in-select_68717850.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT customer.customer_num,\n\t(SELECT ship_charge \n\t\tFROM orders\n\t\tWHERE customer.customer_num = orders.customer_num\n\t) AS total_ship_chg\nFROM customer \n```\n\n----------------------------------------\n\nTITLE: Setting Channel Log Level for Remote Spark Driver\nDESCRIPTION: Configures the channel logging level for the remote Spark driver. If unset, TRACE is chosen.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_62\n\nLANGUAGE: properties\nCODE:\n```\nhive.spark.client.channel.log.level = \n```\n\n----------------------------------------\n\nTITLE: Setting Compaction Worker Pool Timeout in Apache Hive\nDESCRIPTION: This configuration property sets the timeout period for compaction requests in labeled pools. If a request is not processed within this time, it falls back to the default pool. Set to 0 to disable the timeout.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/compaction-pooling_240884493.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nhive.compactor.worker.pool.timeout\n```\n\n----------------------------------------\n\nTITLE: Creating Avro Table in Hive 0.14+\nDESCRIPTION: Simplified SQL commands for creating and populating an Avro table in Hive version 0.14 and later.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE as_avro(string1 STRING,\n                     int1 INT,\n                     tinyint1 TINYINT,\n                     smallint1 SMALLINT,\n                     bigint1 BIGINT,\n                     boolean1 BOOLEAN,\n                     float1 FLOAT,\n                     double1 DOUBLE,\n                     list1 ARRAY<STRING>,\n                     map1 MAP<STRING,INT>,\n                     struct1 STRUCT<sint:INT,sboolean:BOOLEAN,sstring:STRING>,\n                     union1 uniontype<FLOAT, BOOLEAN, STRING>,\n                     enum1 STRING,\n                     nullableint INT,\n                     bytes1 BINARY,\n                     fixed1 BINARY)\nSTORED AS AVRO;\nINSERT OVERWRITE TABLE as_avro SELECT * FROM test_serializer;\n```\n\n----------------------------------------\n\nTITLE: Moving Table Between Catalogs\nDESCRIPTION: Creates a new database in Spark catalog and moves a table from Hive catalog to Spark catalog\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nbeeline ... -e \"create database if not exists newdb\";\nschematool -moveDatabase newdb -fromCatalog hive -toCatalog spark\n\n# Now move the table to target db under the spark catalog.\nschematool -moveTable table1 -fromCatalog hive -toCatalog spark  -fromDatabase db1 -toDatabase newdb\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Kudu Tables\nDESCRIPTION: Examples of INSERT statements for loading data into Kudu tables through Hive, including both SELECT and VALUES syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/kudu-integration_133631955.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO kudu_table SELECT * FROM other_table;\n\nINSERT INTO TABLE kudu_table\nVALUES (1, 'test 1', 1.1), (2, 'test 2', 2.2);\n```\n\n----------------------------------------\n\nTITLE: Testing Approach for Hive on Spark\nDESCRIPTION: Outlines the testing strategy for Hive on Spark, leveraging Spark's local execution modes and integrating with Hive's existing test infrastructure while addressing coverage challenges.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n### Mini Spark Cluster\n\nSpark jobs can be run local by giving \"local\" as the master URL. Most testing will be performed in this mode. In the same time, Spark offers a way to run jobs in a local cluster, a cluster made of a given number of processes in the local machine. We will further determine if this is a good way to run Hive's Spark-related tests.\n\n### Testing\n\nTesting, including pre-commit testing, is the same as for Tez. Currently Hive has a coverage problem as there are a few variables that requires full regression suite run, such as Tez vs MapReduce, vectorization on vs off, etc. We propose rotating those variables in pre-commit test run so that enough coverage is in place while testing time isn't prolonged.\n```\n\n----------------------------------------\n\nTITLE: HiveMetaHook Interface Definition in Java\nDESCRIPTION: This Java interface defines the structure for Hive Metadata Hooks. It includes methods for handling table creation and deletion events, allowing for custom actions during these operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/storagehandlers_27362063.md#2025-04-09_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npackage org.apache.hadoop.hive.metastore;\n\nimport org.apache.hadoop.hive.metastore.api.MetaException;\nimport org.apache.hadoop.hive.metastore.api.Partition;\nimport org.apache.hadoop.hive.metastore.api.Table;\n\npublic interface HiveMetaHook {\n  public void preCreateTable(Table table)\n    throws MetaException;\n  public void rollbackCreateTable(Table table)\n    throws MetaException;\n  public void commitCreateTable(Table table)\n    throws MetaException;\n  public void preDropTable(Table table)\n    throws MetaException;\n  public void rollbackDropTable(Table table)\n    throws MetaException;\n  public void commitDropTable(Table table, boolean deleteData)\n    throws MetaException;\n```\n\n----------------------------------------\n\nTITLE: Configuring Split Generation Location for Hive in Tez\nDESCRIPTION: Determines whether to generate splits locally or in the ApplicationMaster for Hive running on Tez.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_65\n\nLANGUAGE: properties\nCODE:\n```\nhive.compute.splits.in.am = true\n```\n\n----------------------------------------\n\nTITLE: Enabling Top K Statistics Collection in Hive\nDESCRIPTION: HQL command to enable top K statistics collection which is disabled by default. This setting allows Hive to compute top K values and store them in skewed information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/top-k-stats_30150275.md#2025-04-09_snippet_1\n\nLANGUAGE: hql\nCODE:\n```\nset hive.stats.topk.collect=true;\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Table for Compressed Data Import in Hive\nDESCRIPTION: Creates a table with raw string data that can accept compressed files. The table uses tab delimiter for fields and newline for rows.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/compressedstorage_27362073.md#2025-04-09_snippet_0\n\nLANGUAGE: hql\nCODE:\n```\nCREATE TABLE raw (line STRING)\n   ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n';\n\nLOAD DATA LOCAL INPATH '/tmp/weblogs/20090603-access.log.gz' INTO TABLE raw;\n```\n\n----------------------------------------\n\nTITLE: String Concatenation Function\nDESCRIPTION: Concatenates multiple strings or binary inputs in order\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nconcat(string|binary A,string|binary B...)\n```\n\n----------------------------------------\n\nTITLE: Altering JDBC Table Properties in Hive SQL\nDESCRIPTION: Shows how to alter table properties of a JDBC external table using the ALTER TABLE statement, specifically changing the password property.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE student_jdbc SET TBLPROPERTIES (\"hive.sql.dbcp.password\" = \"passwd\");\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Size for Compact Index Usage in Hive\nDESCRIPTION: Specifies the minimum input size in bytes for automatic use of compact indexes. Default is 5368709120 bytes (5 GB).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_75\n\nLANGUAGE: properties\nCODE:\n```\nhive.optimize.index.filter.compact.minsize=5368709120\n```\n\n----------------------------------------\n\nTITLE: Altering Indexes in Hive\nDESCRIPTION: Legacy syntax for rebuilding an index or specific partition of an index in Hive versions prior to 3.0.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_57\n\nLANGUAGE: sql\nCODE:\n```\nALTER INDEX index_name ON table_name [PARTITION partition_spec] REBUILD;\n```\n\n----------------------------------------\n\nTITLE: Hive 0.13.x Configuration Settings\nDESCRIPTION: Configuration settings required in hive-site.xml and HiveServer2 command line options for Hive version 0.13.x to enable authorization\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_22\n\nLANGUAGE: xml\nCODE:\n```\nhive.server2.enable.doAs=false\nhive.users.in.admin.role=user1,user2\nhive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory\nhive.security.authorization.enabled=true\nhive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator\nhive.metastore.uris=' '\n```\n\n----------------------------------------\n\nTITLE: Publishing Release Artifacts to SVN Staging Area\nDESCRIPTION: SVN commands to publish the release artifacts to the Apache SVN staging area.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsvn co --depth empty https://dist.apache.org/repos/dist\ncd dist\n\nsvn update --set-depth empty dev\nsvn update --set-depth empty dev/hive\nmkdir dev/hive/hive-X.Y.Z/\n\ncp <hive-source-dir>/packaging/target/apache-hive-X.Y.Z*.tar.gz* dev/hive/hive-X.Y.Z/\nsvn add dev/hive/hive-X.Y.Z\nsvn commit -m \"Hive X.Y.Z release\"\n```\n\n----------------------------------------\n\nTITLE: Job Diagnostics in Hive on Spark\nDESCRIPTION: Outlines job monitoring capabilities in Spark, including WebUI for running applications, event logging for historical information, and support for Standalone, Mesos, and YARN cluster managers.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n### Job Diagnostics\n\nBasic \"job succeeded/failed\" as well as progress will be as discussed in \"Job monitoring\". Hive's current way of trying to fetch additional information about failed jobs may not be available immediately, but this is another area that needs more research.\n\nSpark provides WebUI for each SparkContext while it's running. Note that this information is only available for the duration of the application by default. To view the web UI after the fact, set spark.eventLog.enabled to true before starting the application. This configures Spark to log Spark events that encode the information displayed in the UI to persisted storage.\n\nSpark's Standalone Mode cluster manager also has its own web UI. If an application has logged events over the course of its lifetime, then the Standalone master's web UI will automatically re-render the application's UI after the application has finished.\n\nIf Spark is run on Mesos or YARN, it is still possible to reconstruct the UI of a finished application through Spark's history server, provided that the application's event logs exist.\n\nFor more information about Spark monitoring, visit<http://spark.apache.org/docs/latest/monitoring.html>.\n```\n\n----------------------------------------\n\nTITLE: Installing Thrift 0.14.1 via Homebrew on Mac\nDESCRIPTION: Commands to install a specific version of Thrift (0.14.1) using Homebrew on MacOS, including setting up fb303.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocontribute_27362107.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbrew tap-new $USER/local-tap\nbrew extract --version='0.14.1' thrift $USER/local-tap\nbrew install thrift@0.14.1\nmkdir -p /usr/local/share/fb303/if\ncp /usr/local/Cellar/thrift@0.14.1/0.14.1/share/fb303/if/fb303.thrift /usr/local/share/fb303/if\n```\n\n----------------------------------------\n\nTITLE: Configuring Statistics Storage in Hive\nDESCRIPTION: Specifies the storage type for temporary Hive statistics. Options include 'fs' (filesystem), 'jdbc:*database*', 'hbase', 'counter', and 'custom'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_76\n\nLANGUAGE: properties\nCODE:\n```\nhive.stats.dbclass=fs\n```\n\n----------------------------------------\n\nTITLE: Hive Multi-Table Insert\nDESCRIPTION: Shows how to insert data into multiple tables or partitions in a single query using Hive's multi-table insert feature.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\nFROM src\nINSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100\nINSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200\nINSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key >= 200 and src.key < 300\nINSERT OVERWRITE LOCAL DIRECTORY '/tmp/dest4.out' SELECT src.value WHERE src.key >= 300;\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partition Error Example\nDESCRIPTION: Example showing an error scenario when too many dynamic partitions are created, along with the corresponding error message.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nbeeline> set hive.exec.dynamic.partition.mode=nonstrict;\nbeeline> FROM page_view_stg pvs\n      INSERT OVERWRITE TABLE page_view PARTITION(dt, country)\n             SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip,\n                    from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country;\n...\n2010-05-07 11:10:19,816 Stage-1 map = 0%,  reduce = 0%\n[Fatal Error] Operator FS_28 (id=41): fatal error. Killing the job.\nEnded Job = job_201005052204_28178 with errors\n...\n```\n\n----------------------------------------\n\nTITLE: Creating a Deferred Rebuild Index in HiveQL\nDESCRIPTION: Creates an index with deferred rebuild, allowing the index to be built later with ALTER INDEX. Shows the formatted index with column names, then drops it.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_1\n\nLANGUAGE: hiveql\nCODE:\n```\nCREATE INDEX table02_index ON TABLE table02 (column3) AS 'COMPACT' WITH DEFERRED REBUILD;\nALTER INDEX table02_index ON table2 REBUILD;\nSHOW FORMATTED INDEX ON table02;\nDROP INDEX table02_index ON table02;\n```\n\n----------------------------------------\n\nTITLE: Using the nullif() function in Hive SQL\nDESCRIPTION: Returns NULL if the two arguments are equal; otherwise returns the first argument. This is shorthand for a CASE expression: CASE WHEN a = b THEN NULL ELSE a.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_76\n\nLANGUAGE: SQL\nCODE:\n```\nnullif(a, b)\n```\n\n----------------------------------------\n\nTITLE: Implementing UDAF Evaluator Class Structure in Java\nDESCRIPTION: Basic structure of the UDAF evaluator class showing required method implementations for initialization, aggregation, and result generation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/genericudafcasestudy_27362093.md#2025-04-09_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator {\n\n    // For PARTIAL1 and COMPLETE: ObjectInspectors for original data\n    private PrimitiveObjectInspector inputOI;\n    private PrimitiveObjectInspector nbinsOI;\n\n    // For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations (list of doubles)\n    private StandardListObjectInspector loi;\n\n    @Override\n    public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {\n      super.init(m, parameters);\n      // return type goes here\n    }\n\n    @Override\n    public Object terminatePartial(AggregationBuffer agg) throws HiveException {\n      // return value goes here\n    }\n\n    @Override\n    public Object terminate(AggregationBuffer agg) throws HiveException {\n      // final return value goes here\n    }\n\n    @Override\n    public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n    }\n\n    @Override\n    public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n    }\n\n    // Aggregation buffer definition and manipulation methods \n    static class StdAgg implements AggregationBuffer {\n    };\n\n    @Override\n    public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n    }\n\n    @Override\n    public void reset(AggregationBuffer agg) throws HiveException {\n    }    \n  }\n```\n\n----------------------------------------\n\nTITLE: Enabling Column Headers in Hive CLI Query Output in XML\nDESCRIPTION: Controls whether to print column names in query output when using the Hive Command Line Interface (CLI).\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_102\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.cli.print.header</name>\n  <value>false</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Explain Plan for Join Query After Hive 0.8.0\nDESCRIPTION: This explain plan demonstrates how Hive processes the join query after version 0.8.0, showing that the filter is now applied to both 'invites' and 'invites2' tables due to transitive predicate pushdown.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/transitivity-on-predicate-pushdown_27823388.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n STAGE DEPENDENCIES:\n  Stage-1 is a root stage\n  Stage-2 depends on stages: Stage-1\n  Stage-0 is a root stage\n\nSTAGE PLANS:\n  Stage: Stage-1\n    Map Reduce\n      Alias -> Map Operator Tree:\n        invites\n          TableScan\n            alias: invites\n            Filter Operator\n              predicate:\n                  expr: (ds = '2011-01-01')\n                  type: boolean\n              Reduce Output Operator\n                key expressions:\n                      expr: ds\n                      type: string\n                sort order: +\n                Map-reduce partition columns:\n                      expr: ds\n                      type: string\n                tag: 0\n                value expressions:\n                      expr: ds\n                      type: string\n        invites2\n          TableScan\n            alias: invites2\n            Filter Operator\n              predicate:\n                  expr: (ds = '2011-01-01')\n                  type: boolean\n              Reduce Output Operator\n                key expressions:\n                      expr: ds\n                      type: string\n                sort order: +\n                Map-reduce partition columns:\n                      expr: ds\n                      type: string\n                tag: 1\n      Reduce Operator Tree:\n        Join Operator\n          condition map:\n               Inner Join 0 to 1\n          condition expressions:\n            0 {VALUE._col2}\n            1\n          handleSkewJoin: false\n          outputColumnNames: _col2\n          Select Operator\n            Group By Operator\n              aggregations:\n                    expr: count()\n              bucketGroup: false\n              mode: hash\n              outputColumnNames: _col0\n              File Output Operator\n                compressed: false\n                GlobalTableId: 0\n                table:\n                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n\n  Stage: Stage-2\n    Map Reduce\n      Alias -> Map Operator Tree:\n        file:/var/folders/nt/ng21tg0n1jl4547lw0k8lg6hq_nw87/T/charleschen/hive_2011-08-04_10-56-09_896_8195257719501884918/-mr-10002\n            Reduce Output Operator\n              sort order:\n              tag: -1\n              value expressions:\n                    expr: _col0\n                    type: bigint\n      Reduce Operator Tree:\n        Group By Operator\n          aggregations:\n                expr: count(VALUE._col0)\n          bucketGroup: false\n          mode: mergepartial\n          outputColumnNames: _col0\n          Select Operator\n            expressions:\n                  expr: _col0\n                  type: bigint\n            outputColumnNames: _col0\n            File Output Operator\n              compressed: false\n              GlobalTableId: 0\n              table:\n                  input format: org.apache.hadoop.mapred.TextInputFormat\n                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n```\n\n----------------------------------------\n\nTITLE: Transform with Type Casting and Sort By\nDESCRIPTION: Shows how to handle type casting when using SORT BY after a transform operation to ensure proper numeric sorting.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-sortby_27362045.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nFROM (FROM (FROM src\n            SELECT TRANSFORM(value)\n            USING 'mapper'\n            AS value, count) mapped\n      SELECT cast(value as double) AS value, cast(count as int) AS count\n      SORT BY value, count) sorted\nSELECT TRANSFORM(value, count)\nUSING 'reducer'\nAS whatever\n```\n\n----------------------------------------\n\nTITLE: Referencing LLAP Monitoring Configuration in Python Templates\nDESCRIPTION: This snippet references the templates.py file that contains embedded configuration for LLAP monitoring, including resources.json, appConfig.json, and metainfo.xml. These configurations are used by Slider for LLAP deployment and monitoring.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/llap_62689557.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntemplates.py\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Replacement\nDESCRIPTION: Replaces text matching regex pattern with replacement string. Supports Java regular expression syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_24\n\nLANGUAGE: sql\nCODE:\n```\nregexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)\n```\n\n----------------------------------------\n\nTITLE: Addressing Concurrency and Thread Safety in Hive on Spark\nDESCRIPTION: Discusses the challenges with concurrency and thread safety when deploying Hive operator trees in Spark's multi-threaded environment, highlighting specific issues like static variables that need to be addressed.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-on-spark_42567714.md#2025-04-09_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n### Concurrency and Thread Safety\n\nSpark launches mappers and reducers differently from MapReduce in that a worker may process multiple HDFS splits in a single JVM. However, Hive's map-side operator tree or reduce-side operator tree operates in a single thread in an exclusive JVM. Reusing the operator trees and putting them in a shared JVM with each other will more than likely cause concurrency and thread safety issues. Such problems, such as static variables, have surfaced in the initial prototyping. For instance, variable ExecMapper.done is used to determine if a mapper has finished its work. If two ExecMapper instances exist in a single JVM, then one mapper that finishes earlier will prematurely terminate the other also. We expect there will be a fair amount of work to make these operator tree thread-safe and contention-free. However, this work should not have any impact on other execution engines.\n```\n\n----------------------------------------\n\nTITLE: Using the length String Function in Hive SQL\nDESCRIPTION: This example demonstrates how to use the built-in 'length' function in a Hive SQL query to evaluate the length of string values in each row of a table column. This illustrates the row-by-row execution pattern of Hive UDFs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT length(string_col) FROM table_name;\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Load Tasks for Hive Replication in XML\nDESCRIPTION: Provides an approximation of the maximum number of tasks that should be executed before dynamically generating the next set of tasks during Hive replication load operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/configuration-properties_27842758.md#2025-04-09_snippet_109\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.repl.approx.max.load.tasks</name>\n  <value>10000</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Defining Patch Naming Conventions for Hive PreCommit Testing\nDESCRIPTION: This regex pattern defines the accepted naming conventions for patches to be tested in the Hive PreCommit system. It allows for JIRA issue numbers, optional version numbers, branch specifications, and file extensions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-precommit-patch-testing_33295252.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nHIVE-XXXX(.XX)?(-branch)?.patch(.txt)?\n(HIVE-XXXX\\.)?DXXXX(.XX)?.patch(.txt)?\n```\n\n----------------------------------------\n\nTITLE: Calculating Months Between Dates in Hive\nDESCRIPTION: Calculates number of months between two dates with fractional precision. Handles end of month cases and time components.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\nmonths_between(date1, date2)\n```\n\n----------------------------------------\n\nTITLE: Creating a partitioned external table over S3 data\nDESCRIPTION: This snippet shows how to create a partitioned external table in Hive and add partitions that point to specific directories in S3. This enables efficient querying of data organized by partitions like dates.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nhive> create external table pkv (key int, values string) partitioned by (insertdate string);\nhive> alter table pkv add partition (insertdate='2008-01-01') location 's3n://data.s3ndemo.hive/pkv/2008-01-01';\n```\n\n----------------------------------------\n\nTITLE: Defining FilterPlan Abstract Class for HBase Query Plans\nDESCRIPTION: Abstract class that represents a filter plan for HBase queries, providing methods for combining filters using AND/OR operations and retrieving scan plans.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbase-execution-plans-for-rawstore-partition-filter-condition_55151993.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic static abstract class FilterPlan {\n    abstract FilterPlan and(FilterPlan other);\n    abstract FilterPlan or(FilterPlan other);\n    abstract List<ScanPlan> getPlans();\n}\n```\n\n----------------------------------------\n\nTITLE: String Repetition Function\nDESCRIPTION: Repeats input string specified number of times.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-udfs_282102277.md#2025-04-09_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\nrepeat(string str, int n)\n```\n\n----------------------------------------\n\nTITLE: WebHCat Job Information JSON Response in Hive 0.12.0+\nDESCRIPTION: Enhanced JSON response format for job information in Hive 0.12.0 and later versions. This response includes the standard job information plus additional user arguments that were specified when the job was submitted.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-jobinfo_34017194.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n \"status\": {\n            \"startTime\": 1324529476131,\n            \"username\": \"ctdean\",\n            \"jobID\": {\n                      \"jtIdentifier\": \"201112212038\",\n                      \"id\": 4\n                     },\n            \"jobACLs\": {\n                       },\n            \"schedulingInfo\": \"NA\",\n            \"failureInfo\": \"NA\",\n            \"jobId\": \"job_201112212038_0004\",\n            \"jobPriority\": \"NORMAL\",\n            \"runState\": 2,\n            \"jobComplete\": true\n           },\n \"profile\": {\n             \"url\": \"http://localhost:50030/jobdetails.jsp?jobid=job_201112212038_0004\",\n             \"jobID\": {\n                       \"jtIdentifier\": \"201112212038\",\n                        \"id\": 4\n                      },\n             \"user\": \"ctdean\",\n             \"queueName\": \"default\",\n             \"jobFile\": \"hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201112212038_0004/job.xml\",\n             \"jobName\": \"PigLatin:DefaultJobName\",\n             \"jobId\": \"job_201112212038_0004\"\n            },\n \"id\": \"job_201112212038_0004\",\n \"parentId\": \"job_201112212038_0003\",\n \"percentComplete\": \"100% complete\",\n \"exitValue\": 0,\n \"user\": \"ctdean\",\n \"callback\": null,\n \"completed\": \"done\",\n \"userargs\" => {\n                \"callback\"  => null,\n                \"define\"    => [],\n                \"enablelog\" => \"false\",\n                \"execute\"   => \"select a,rand(b) from mynums\",\n                \"file\"      => null,\n                \"files\"     => [],\n                \"statusdir\" => null,\n                \"user.name\" => \"hadoopqa\",\n               },\n}\n```\n\n----------------------------------------\n\nTITLE: Using if Conditional Function in Hive SQL\nDESCRIPTION: Returns the first value when the test condition is true, otherwise returns the second value. This function provides simple if-then-else logic within a query.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nif(boolean testCondition, T valueTrue, T valueFalseOrNull)\n```\n\n----------------------------------------\n\nTITLE: Monolithic Query Example in Hive SQL\nDESCRIPTION: An example of a complex monolithic Hive SQL query with multiple nested subqueries, joins, and groupings. This demonstrates the type of query that needs to be modularized for effective unit testing.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-testing-hive-sql_61328063.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT ... FROM (                  -- Query 1\n  SELECT ... FROM (                --  Query 2\n    SELECT ... FROM (              --   Query 3\n      SELECT ... FROM a WHERE ...  --    Query 4\n    ) A LEFT JOIN (                --   Query 3\n      SELECT ... FROM b            --    Query 5\n    ) B ON (...)                   --   Query 3 \n  ) ab FULL OUTER JOIN (           --  Query 2\n    SELECT ... FROM c WHERE ...    --   Query 6\n  ) C ON (...)                     --  Query 2\n) abc LEFT JOIN (                  -- Query 1\n  SELECT ... FROM d WHERE ...      --  Query 7\n) D ON (...)                       -- Query 1\nGROUP BY ...;                      -- Query 1\n```\n\n----------------------------------------\n\nTITLE: Disabling Fetch Task Conversion for Vectorized Execution in Hive SQL\nDESCRIPTION: SQL command to disable fetch task conversion, allowing vectorized execution for fetch operations.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/vectorized-query-execution_34838326.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.fetch.task.conversion=none;\n```\n\n----------------------------------------\n\nTITLE: Multi-Table Dynamic Partition Insert in Hive\nDESCRIPTION: Example showing multiple INSERT statements in a single query, demonstrating both dynamic and static partitioning across different target tables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/dynamicpartitions_27823715.md#2025-04-09_snippet_3\n\nLANGUAGE: hql\nCODE:\n```\nFROM S\nINSERT OVERWRITE TABLE T PARTITION (ds='2010-03-03', hr) \nSELECT key, value, ds, hr FROM srcpart WHERE ds is not null and hr>10\nINSERT OVERWRITE TABLE R PARTITION (ds='2010-03-03, hr=12)\nSELECT key, value, ds, hr from srcpart where ds is not null and hr = 12;\n```\n\n----------------------------------------\n\nTITLE: Setting Multiple User DN Patterns for LDAP Authentication in HiveServer2\nDESCRIPTION: This XML configuration demonstrates how to specify multiple user DN patterns for LDAP authentication in HiveServer2. It uses a colon-separated list to define different possible locations for user entries in the LDAP directory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>\n    hive.server2.authentication.ldap.userDNPattern\n  </name>\n  <value>\n      CN=%s,OU=Users,DC=apache,DC=org:uid=%s,CN=UnixUsers,DC=apache,DC=org\n  </value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Timezone Conversion in Hive SQL\nDESCRIPTION: Shows functions for converting timestamps between UTC and specific timezones. These functions handle various primitive types as input.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-udf_27362046.md#2025-04-09_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nfrom_utc_timestamp(2592000.0,'PST')\n```\n\nLANGUAGE: SQL\nCODE:\n```\nto_utc_timestamp(2592000.0,'PST')\n```\n\n----------------------------------------\n\nTITLE: Querying WebHCat Response Types with cURL\nDESCRIPTION: Example cURL command to request supported response types from the WebHCat API endpoint.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-responsetypes_34015937.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1'\n```\n\n----------------------------------------\n\nTITLE: Database Migration Examples\nDESCRIPTION: Examples showing how to move databases and tables between catalogs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-schema-tool_34835119.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbuild/dist/bin/schematool -moveDatabase db1 -fromCatalog hive -toCatalog spark\n```\n\nLANGUAGE: bash\nCODE:\n```\nbeeline ... -e \"create database if not exists newdb\";\nschematool -moveDatabase newdb -fromCatalog hive -toCatalog spark\nschematool -moveTable table1 -fromCatalog hive -toCatalog spark  -fromDatabase db1 -toDatabase newdb\n```\n\n----------------------------------------\n\nTITLE: Example 2: Complex Join Query with Subquery\nDESCRIPTION: SQL query showing correlation optimization potential in a complex join operation with subqueries.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/correlation-optimizer_34019487.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT tmp1.key, count(*)\nFROM t1\nJOIN /*JOIN1*/ (SELECT key, avg(value) AS avg\n                FROM t1\n                GROUP BY /*AGG1*/ key) tmp1 ON (t1.key = tmp1.key)\nJOIN /*JOIN1*/ t2 ON (tmp1.key = t2.key)\nWHERE t2.value > tmp1.avg\nGROUP BY /*AGG2*/ t1.key;\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tables and Executing Join Query in Hive\nDESCRIPTION: This SQL snippet sets Hive to strict mode, creates two partitioned tables (invites and invites2), and performs a join query with a filter condition on one table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/transitivity-on-predicate-pushdown_27823388.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nset hive.mapred.mode=strict;\ncreate table invites (foo int, bar string) partitioned by (ds string);\ncreate table invites2 (foo int, bar string) partitioned by (ds string);\nselect count(*) from invites join invites2 on invites.ds=invites2.ds where invites.ds='2011-01-01';\n```\n\n----------------------------------------\n\nTITLE: SHOW CREATE TABLE Command in Hive 0.10+\nDESCRIPTION: Syntax for the SHOW CREATE TABLE command introduced in Hive 0.10. This command displays the CREATE TABLE or CREATE VIEW statement that would create the specified table or view, useful for recreating objects or understanding their structure.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_81\n\nLANGUAGE: sql\nCODE:\n```\nSHOW CREATE TABLE ([db_name.]table_name|view_name);\n```\n\n----------------------------------------\n\nTITLE: Successful Response JSON Structure\nDESCRIPTION: Example JSON response showing the supported response types returned by the WebHCat API.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-responsetypes_34015937.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"responseTypes\": [\n    \"application/json\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Database-qualified SHOW PARTITIONS in Hive 0.13.0+\nDESCRIPTION: An example of using SHOW PARTITIONS with a database-qualified table name, introduced in Hive 0.13.0. This demonstrates how to view partitions of a table in a specific database with additional partition filtering.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_75\n\nLANGUAGE: sql\nCODE:\n```\nSHOW PARTITIONS databaseFoo.tableBar PARTITION(ds='2010-03-03', hr='12');   -- (Note: Hive 0.13.0 and later)\n```\n\n----------------------------------------\n\nTITLE: Sample Output from EXPLAIN LOCKS Command\nDESCRIPTION: This output shows the locks that will be acquired for an UPDATE operation, including the tables and partitions that will be locked and whether they will be locked in SHARED_READ or SHARED_WRITE mode.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nLOCK INFORMATION:\ndefault.source -> SHARED_READ\ndefault.target.p=1/q=2 -> SHARED_READ\ndefault.target.p=1/q=3 -> SHARED_READ\ndefault.target.p=2/q=2 -> SHARED_READ\ndefault.target.p=2/q=2 -> SHARED_WRITE\ndefault.target.p=1/q=3 -> SHARED_WRITE\ndefault.target.p=1/q=2 -> SHARED_WRITE\n```\n\n----------------------------------------\n\nTITLE: Listing Partitions of a Table in Hive\nDESCRIPTION: Shows how to list all partitions of a partitioned table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/tutorial_27362061.md#2025-04-09_snippet_10\n\nLANGUAGE: HiveQL\nCODE:\n```\nSHOW PARTITIONS page_view;\n```\n\n----------------------------------------\n\nTITLE: Setting Group Membership and Class Keys for LDAP Authentication in HiveServer2\nDESCRIPTION: This XML configuration demonstrates how to set custom group membership and class keys for LDAP authentication in HiveServer2. It allows for specifying non-standard LDAP attributes for group membership and object class.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>hive.server2.authentication.ldap.groupMembershipKey</name>\n  <value>memberUid</value>\n</property>\n<property>\n  <name>hive.server2.authentication.ldap.groupClassKey</name>\n  <value>group</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Removing SerDe Properties in Hive\nDESCRIPTION: SQL statement to remove specific SerDe properties from a table or partition. Supported from Hive 4.0.0 onwards.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name [PARTITION partition_spec] UNSET SERDEPROPERTIES (property_name, ... );\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name UNSET SERDEPROPERTIES ('field.delim');\n```\n\n----------------------------------------\n\nTITLE: Retrieving Database Description with cURL in WebHCat API\nDESCRIPTION: Example cURL command to retrieve the description of a database named 'newdb' from the WebHCat API. The request is authenticated with the username 'ctdean'.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getdb_34016250.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database/newdb?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Adding a JAR with Full Path to Hive's Classpath\nDESCRIPTION: This Hive command adds a JAR file to the classpath using its full path, useful when the JAR is not in the current directory.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveplugins_27362098.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nadd jar /tmp/my_jar.jar;\n```\n\n----------------------------------------\n\nTITLE: Analyzing Column Statistics for All Partitions\nDESCRIPTION: Example of gathering column statistics for all partitions in a table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nANALYZE TABLE Table1 PARTITION(ds, hr) COMPUTE STATISTICS FOR COLUMNS;\n```\n\n----------------------------------------\n\nTITLE: Unsupported Subquery in SELECT: Current Limitation Example\nDESCRIPTION: Example of a currently unsupported subquery in a SELECT statement that returns the sum of ship charges for each customer. This represents the type of functionality that is planned to be implemented in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/subqueries-in-select_68717850.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT customer.customer_num,\n\t(SELECT SUM(ship_charge) \n\t\tFROM orders\n\t\tWHERE customer.customer_num = orders.customer_num\n\t) AS total_ship_chg\nFROM customer \n```\n\n----------------------------------------\n\nTITLE: Hive Export Table and Import Partition Example\nDESCRIPTION: Example showing how to export an entire table but import only a specific partition of it.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-importexport_27837968.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nexport table employee to 'hdfs_exports_location/employee';\nimport table employee partition (emp_country=\"us\", emp_state=\"tn\") from 'hdfs_exports_location/employee';\n\n```\n\n----------------------------------------\n\nTITLE: WebHCat Version Response Format\nDESCRIPTION: Example JSON response showing the supported versions and current version of the WebHCat API. The response includes an array of supported versions and the current active version.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-version_34015986.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"supportedVersions\": [\n   \"v1\"\n ],\n \"version\": \"v1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Preparing HDFS Staging Location for HBase Bulk Load\nDESCRIPTION: HDFS commands to prepare the staging directory where the HBase HFiles will be generated during the sorting process. These commands ensure the directory is empty and ready for use.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndfs -rmr /tmp/hbsort;\ndfs -mkdir /tmp/hbsort;\n```\n\n----------------------------------------\n\nTITLE: Creating an External LZO-Compressed Hive Table\nDESCRIPTION: Hive command example for creating an external table that uses LZO compression. This defines the table schema, partitioning, and specifies the appropriate input/output formats for LZO.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lzo_33298193.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nhive -e \"CREATE EXTERNAL TABLE IF NOT EXISTS hive_table_name (column_1  datatype_1......column_N datatype_N)\n         PARTITIONED BY (partition_col_1 datatype_1 ....col_P  datatype_P)\n         ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\n         STORED AS INPUTFORMAT  \\\"com.hadoop.mapred.DeprecatedLzoTextInputFormat\\\"\n                   OUTPUTFORMAT \\\"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\\\";\"\n```\n\n----------------------------------------\n\nTITLE: Verifying SHA256 Checksums\nDESCRIPTION: Commands to verify the SHA256 checksums of the release artifacts.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtorelease_27362106.md#2025-04-09_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nshasum -a 256 -c apache-hive-X.Y.Z-bin.tar.gz.sha256\nshasum -a 256 -c apache-hive-X.Y.Z-src.tar.gz.sha256\n```\n\n----------------------------------------\n\nTITLE: Querying Table Data with GET Request in WebHCat\nDESCRIPTION: Example of making a GET request to WebHCat API to query table data with user authentication parameter specified in the URL.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-usingwebhcat_34015492.md#2025-04-09_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\ncurl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Initializing Hive Schema to Specific Version\nDESCRIPTION: Initializes the Derby metastore schema to a specific version (3.1.0)\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/284790216.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ schematool -dbType derby -initSchemaTo 3.1.0 \nMetastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\nMetastore connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\nMetastore connection User:\t APP\nStarting metastore schema initialization to 3.1.0\nInitialization script hive-schema-3.1.0.derby.sql\n```\n\n----------------------------------------\n\nTITLE: Hive Renaming Table on Import Example\nDESCRIPTION: Example showing how to export a table and import it with a different name.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-importexport_27837968.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nexport table department to 'hdfs_exports_location/department';\nimport table imported_dept from 'hdfs_exports_location/department';\n\n```\n\n----------------------------------------\n\nTITLE: JSON Response for Successful Partition Deletion\nDESCRIPTION: Example JSON output returned by the WebHCat API after successfully deleting a partition. It confirms the deletion by returning the partition, table, and database names.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-deletepartition_34016611.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"partition\": \"country='algeria'\",\n \"table\": \"test_table\",\n \"database\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Eclipse Project Setup\nDESCRIPTION: Commands to set up Hive project in Eclipse without using local Maven repository\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ mkdir workspace\n$ cd workspace\n$ git clone https://github.com/apache/hive.git\n$ cd hive\n$ mvn clean package eclipse:clean eclipse:eclipse -Pitests -DskipTests -DdownloadSources -DdownloadJavadocs\n```\n\n----------------------------------------\n\nTITLE: Example Hive Command for Direct LZO File Creation\nDESCRIPTION: Example of a complete Hive command that sets the necessary compression parameters before executing a query to produce LZO-compressed output files.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-lzo_33298193.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nhive -e \"SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzoCodec; SET hive.exec.compress.output=true;SET mapreduce.output.fileoutputformat.compress=true; <query-string>\"\n```\n\n----------------------------------------\n\nTITLE: Creating Required HDFS Directories\nDESCRIPTION: HDFS commands to create and set permissions for required Hive directories.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ $HADOOP_HOME/bin/hadoop fs -mkdir       /tmp\n$ $HADOOP_HOME/bin/hadoop fs -mkdir       /user/hive/warehouse\n$ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp\n$ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse\n```\n\n----------------------------------------\n\nTITLE: Testing a Local Patch File in Hive\nDESCRIPTION: Command to test a patch from the local file system.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhive_repo/testutils/ptest/hivetest.py --test --patch /path/to/my.patch\n\n```\n\n----------------------------------------\n\nTITLE: JSON Response from WebHCat Partition Creation API\nDESCRIPTION: This shows the JSON response received after successfully creating a partition in Hive. The response includes confirmation of the partition name, table name, and database name where the partition was created.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-putpartition_34016600.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"partition\": \"country='algeria'\",\n \"table\": \"test_table\",\n \"database\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching a Pull Request from GitHub for Apache Hive\nDESCRIPTION: This snippet shows how to fetch a specific pull request from GitHub and create a local branch for review or testing. It uses the pull request ID to fetch the changes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/howtocontribute_27362107.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch origin pull/ID/head:BRANCHNAME\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch origin pull/1234/head:radiator\n```\n\n----------------------------------------\n\nTITLE: Conditionally Dropping an Index in HiveQL\nDESCRIPTION: Demonstrates how to drop an index only if it exists, preventing errors when attempting to drop a non-existent index.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_8\n\nLANGUAGE: hiveql\nCODE:\n```\nDROP INDEX IF EXISTS table09_index ON table09;\n```\n\n----------------------------------------\n\nTITLE: Installing Hive from Source - SVN Checkout\nDESCRIPTION: Command to check out Hive source code from Apache SVN repository for versions 0.12.0 and earlier.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/adminmanual-installation_27362077.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ svn co http://svn.apache.org/repos/asf/hive/branches/branch-#.# hive\n```\n\n----------------------------------------\n\nTITLE: LDAP Query for Group Filtering\nDESCRIPTION: LDAP query example that filters for groups with specific names. This query returns entries for group1 and group2 based on their common name (cn) attribute.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417.md#2025-04-09_snippet_9\n\nLANGUAGE: ldap\nCODE:\n```\n(&(objectClass=groupOfNames)(|(cn=group1)(cn=group2)))\n```\n\n----------------------------------------\n\nTITLE: Using EXPLAIN AST to View the Abstract Syntax Tree of a Hive Query\nDESCRIPTION: This example demonstrates how to use the EXPLAIN AST command to view the abstract syntax tree representation of a Hive query that performs an aggregation and inserts results into a destination table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-explain_27362037.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN AST\nFROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;\n```\n\n----------------------------------------\n\nTITLE: Export Hive Version Environment Variable\nDESCRIPTION: Shell command to set Hive version as environment variable for subsequent commands.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/Development/quickStart.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport HIVE_VERSION=4.0.0\n```\n\n----------------------------------------\n\nTITLE: Analyzing Column Statistics for Specific Partition\nDESCRIPTION: Example of gathering column statistics for a specific partition in Hive 0.10.0 and later.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/statsdev_27362062.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr=11) COMPUTE STATISTICS FOR COLUMNS;\n```\n\n----------------------------------------\n\nTITLE: Running Single Test Commands\nDESCRIPTION: Maven commands for running individual tests or test patterns\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmvn test -Dtest=ClassName\nmvn test -Dtest=ClassName#methodName\nmvn test -Dtest='org.apache.hive.beeline.*'\n```\n\n----------------------------------------\n\nTITLE: Testing with Custom Report Name\nDESCRIPTION: Command to test a differential revision with a custom report name instead of the default timestamp-based naming.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/unit-test-parallel-execution_27833687.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhive_repo/testuitls/ptest/hivetest.py --test --revision D123 --report-name D123\n\n```\n\n----------------------------------------\n\nTITLE: Joining JDBC Tables in Hive\nDESCRIPTION: This snippet shows a join operation between two JDBC tables in Hive. The computation for this join will be pushed down to MySQL for better performance.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/jdbc-storage-handler_95651916.md#2025-04-09_snippet_7\n\nLANGUAGE: HiveQL\nCODE:\n```\nselect * from student_jdbc join voter_jdbc on student_jdbc.name=voter_jdbc.name;\n```\n\n----------------------------------------\n\nTITLE: JSON Response for Successful Database Creation in WebHCat\nDESCRIPTION: This is the JSON response returned by the WebHCat API after successfully creating a new database. The response confirms the database name that was created.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-putdb_34016273.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"database\":\"newdb\"\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Response Format for WebHCat Column Information\nDESCRIPTION: Example JSON response when retrieving column information, showing the database name, table name, and column details including name, comment, and data type.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getcolumn_34016979.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"database\": \"default\",\n \"table\": \"test_table\",\n \"column\": {\n   \"name\": \"price\",\n   \"comment\": \"The unit price\",\n   \"type\": \"float\"\n }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing PartitionFilterGenerator Visitor Pattern\nDESCRIPTION: Visitor implementation for ExpressionTree that generates ScanPlans for leaf nodes and combines them using AND/OR operations for higher level nodes.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbase-execution-plans-for-rawstore-partition-filter-condition_55151993.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n/**\n * Visitor for ExpressionTree.\n * It first generates the ScanPlan for the leaf nodes. The higher level nodes are\n * either AND or OR operations. It then calls FilterPlan.and and FilterPlan.or with\n * the child nodes to generate the plans for higher level nodes.\n */\n```\n\n----------------------------------------\n\nTITLE: Generating Test Output\nDESCRIPTION: Command to generate output file for UDAF test cases using ant.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/genericudafcasestudy_27362093.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nant test -Dtestcase=TestCliDriver -Dqfile=udaf_XXXXX.q -Doverwrite=true\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partitioning with Pig Script in HCatalog\nDESCRIPTION: This Pig script shows how to write data to multiple partitions simultaneously using dynamic partitioning in HCatalog. By not specifying partition values and including partition columns in the data, HCatalog automatically distributes rows to appropriate partitions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-dynamicpartitions_34014006.md#2025-04-09_snippet_1\n\nLANGUAGE: pig\nCODE:\n```\nA = load 'raw' using HCatLoader(); \n... \nstore Z into 'processed' using HCatStorer(); \n\n```\n\n----------------------------------------\n\nTITLE: Beeline-HS2-Connection Example for Combined Usage\nDESCRIPTION: A beeline-hs2-connection.xml example showing user credentials that will be merged with the URLs defined in beeline-site.xml. This demonstrates property combination across configuration files.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_34\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n<property>\n  <name>beeline.hs2.connection.user</name>\n  <value>hive</value>\n</property>\n<property>\n  <name>beeline.hs2.connection.password</name>\n  <value>hive</value>\n</property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Retrieving HCatalog Table Properties with curl\nDESCRIPTION: A curl command example that demonstrates how to call the WebHCat API endpoint to list all properties of a table named 'test_table' in the 'default' database, with 'ctdean' as the user.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getproperties_34016995.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/property?user.name=ctdean'\n```\n\n----------------------------------------\n\nTITLE: Successful JSON Response from WebHCat Table Creation\nDESCRIPTION: This JSON response is returned when a table is successfully created via the WebHCat API. It returns the table name and database where the table was created.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-puttable_34016540.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n \"table\": \"test_table\",\n \"database\": \"default\"\n}\n```\n\n----------------------------------------\n\nTITLE: Hive Version API Response Format\nDESCRIPTION: Example JSON response from the version/hive endpoint showing the module name and version number. The response is an array containing a single object with module and version properties.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-versionhive_44303406.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n{\"module\":\"hive\",\"version\":\"0.14.0-SNAPSHOT\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Setting PerfLogger Debug Level in Hive\nDESCRIPTION: Sets the logging level for PerfLogger to DEBUG in the log4j properties file to obtain performance metrics.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted_27362090.md#2025-04-09_snippet_11\n\nLANGUAGE: Properties\nCODE:\n```\nlog4j.logger.org.apache.hadoop.hive.ql.log.PerfLogger=DEBUG\n```\n\n----------------------------------------\n\nTITLE: Schema Information Query\nDESCRIPTION: Example showing how to retrieve schema version information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-schema-tool_34835119.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ schematool -dbType derby -info\nMetastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true\nMetastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver\nMetastore connection User:       APP\nHive distribution version:       0.13.0\nMetastore schema version:        0.13.0\nschemaTool completed\n```\n\n----------------------------------------\n\nTITLE: Basic Shell Variable Usage with Hive\nDESCRIPTION: Example showing basic shell variable usage with Hive command line, demonstrating the inefficient way of using shell variables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-variablesubstitution_30754722.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ a=b\n$ hive -e \" describe $a \"\n```\n\n----------------------------------------\n\nTITLE: Basic Filter Operation in Pig with HCatLoader\nDESCRIPTION: Demonstrates loading data from web_logs using HCatLoader and filtering by datestamp.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_7\n\nLANGUAGE: pig\nCODE:\n```\na = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader();\nb = filter a by datestamp > '20110924';\n```\n\n----------------------------------------\n\nTITLE: Basic HCatStorer Usage\nDESCRIPTION: Example of using HCatStorer to write processed data to a HCatalog-managed table.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-loadstore_34013511.md#2025-04-09_snippet_11\n\nLANGUAGE: pig\nCODE:\n```\nA = LOAD ...\nB = FOREACH A ...\n...\n...\nmy_processed_data = ...\n\nSTORE my_processed_data INTO 'tablename'\n   USING org.apache.hive.hcatalog.pig.HCatStorer();\n```\n\n----------------------------------------\n\nTITLE: Manual Split Alternative to Dynamic Partitioning in Pig\nDESCRIPTION: Example showing how to manually split and store data to different partitions, which is equivalent to using dynamic partitioning but more verbose.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-dynamicpartitions_34014006.md#2025-04-09_snippet_6\n\nLANGUAGE: pig\nCODE:\n```\nsplit A into A1 if b='1', A2 if b='2';\nstore A1 into 'mytable' using HCatStorer(\"a=1, b=1\");\nstore A2 into 'mytable' using HCatStorer(\"a=1, b=2\");\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Background Beeline Processing\nDESCRIPTION: This shell command sets the HADOOP_CLIENT_OPTS environment variable to work around terminal issues when running Beeline in the background with nohup. This addresses HIVE-11717 and HIVE-6758 bugs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Djline.terminal=jline.UnsupportedTerminal\"\n```\n\n----------------------------------------\n\nTITLE: Loading HiveServer2 JDBC Driver in Java\nDESCRIPTION: Code snippet showing how to load the HiveServer2 JDBC driver. As of Hive 1.2.0, applications no longer need to explicitly load JDBC drivers with Class.forName().\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_36\n\nLANGUAGE: java\nCODE:\n```\nClass.forName(\"org.apache.hive.jdbc.HiveDriver\");\n```\n\n----------------------------------------\n\nTITLE: Adding Hive and Hadoop JARs to SQuirrel SQL Client Classpath\nDESCRIPTION: This snippet shows the JAR files that need to be added to the SQuirrel SQL Client's classpath to enable Hive JDBC connections. It includes both Hive and Hadoop JARs.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveserver2-clients_30758725.md#2025-04-09_snippet_48\n\nLANGUAGE: bash\nCODE:\n```\n   HIVE_HOME/lib/hive-jdbc-*-standalone.jar\n   HADOOP_HOME/share/hadoop/common/hadoop-common-*.jar \n```\n\n----------------------------------------\n\nTITLE: SHOW INDEXES Command in Hive 0.7+\nDESCRIPTION: Syntax for the SHOW INDEXES command introduced in Hive 0.7 (removed in Hive 3.0). This command lists all indexes on a table along with their properties like index name, table name, key columns, and index type.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-ddl_27362034.md#2025-04-09_snippet_82\n\nLANGUAGE: sql\nCODE:\n```\nSHOW [FORMATTED] (INDEX|INDEXES) ON table_with_index [(FROM|IN) db_name];\n```\n\n----------------------------------------\n\nTITLE: Using DEFAULT Keyword in INSERT Statement\nDESCRIPTION: Examples demonstrating how to use the DEFAULT keyword in INSERT INTO statements. The DEFAULT keyword will use the column's defined default constraint value if available, otherwise NULL.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/75977362.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO TABLE1 VALUES(DEFAULT, DEFAULT)\n```\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO TABLE1(COL1) VALUES(DEFAULT)\n```\n\n----------------------------------------\n\nTITLE: Implementing gRPC Server Class for Hive Metastore in Java\nDESCRIPTION: Class definition for the gRPC server implementation in Hive Metastore. It extends the base class generated from the protobuf definition.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/158869886.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nprivate class HiveMetaStoreGrpcServer extends HiveMetaStoreGrpc.HiveMetaStoreImplBase\n```\n\n----------------------------------------\n\nTITLE: Querying with Grouping__ID Function in Hive SQL\nDESCRIPTION: Demonstrates usage of GROUPING__ID function with GROUP BY and ROLLUP to differentiate between NULL values in data versus NULL from aggregation.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/30151323.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT key, value, GROUPING__ID, count(*)\nFROM T1\nGROUP BY key, value WITH ROLLUP;\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive-Accumulo Connection Parameters\nDESCRIPTION: Demonstrates how to set up the required connection parameters for Hive to connect to Accumulo using the hive command line interface.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/accumulointegration_46633569.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhive -hiveconf accumulo.instance.name=accumulo -hiveconf accumulo.zookeepers=localhost -hiveconf accumulo.user.name=hive -hiveconf accumulo.user.pass=hive\n```\n\n----------------------------------------\n\nTITLE: Creating Hive Table with Binary Map Storage\nDESCRIPTION: Demonstrates creating a table with a map column using binary serialization instead of string representation for integer values.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/accumulointegration_46633569.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE hive_map(key int, value map<string,int>) \nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES (\n  \"accumulo.columns.mapping\" = \":rowID,cf:*\",\n  \"accumulo.default.storage\" = \"binary\"\n);\n```\n\n----------------------------------------\n\nTITLE: Starting Hive with Verbose Logging for Troubleshooting\nDESCRIPTION: Command to start Hive with increased logging verbosity to help diagnose AvroSerde exceptions. This configuration sets the root logger to INFO level and outputs to the console, providing more detailed error information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/avroserde_27850707.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhive --hiveconf hive.root.logger=INFO,console\n```\n\n----------------------------------------\n\nTITLE: Defining Thrift Data Structures for Column Statistics\nDESCRIPTION: Thrift struct definitions for different column data types (Boolean, Double, Long, String, Binary, Decimal, Date) to transport statistics between components. These structures contain type-specific statistics fields.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/column-statistics-in-hive_29131019.md#2025-04-09_snippet_5\n\nLANGUAGE: Thrift\nCODE:\n```\nstruct BooleanColumnStatsData {  \n 1: required i64 numTrues,  \n 2: required i64 numFalses,  \n 3: required i64 numNulls  \n }\n\nstruct DoubleColumnStatsData {  \n 1: required double lowValue,  \n 2: required double highValue,  \n 3: required i64 numNulls,  \n 4: required i64 numDVs,\n\n5: optional string bitVectors\n\n}\n\nstruct LongColumnStatsData {  \n 1: required i64 lowValue,  \n 2: required i64 highValue,  \n 3: required i64 numNulls,  \n 4: required i64 numDVs,\n\n5: optional string bitVectors  \n }\n\nstruct StringColumnStatsData {  \n 1: required i64 maxColLen,  \n 2: required double avgColLen,  \n 3: required i64 numNulls,  \n 4: required i64 numDVs,\n\n5: optional string bitVectors  \n }\n\nstruct BinaryColumnStatsData {  \n 1: required i64 maxColLen,  \n 2: required double avgColLen,  \n 3: required i64 numNulls  \n }\n\nstruct Decimal {  \n1: required binary unscaled,  \n3: required i16 scale  \n}\n\nstruct DecimalColumnStatsData {  \n1: optional Decimal lowValue,  \n2: optional Decimal highValue,  \n3: required i64 numNulls,  \n4: required i64 numDVs,  \n5: optional string bitVectors  \n}\n\nstruct Date {  \n1: required i64 daysSinceEpoch  \n}\n\nstruct DateColumnStatsData {  \n1: optional Date lowValue,  \n2: optional Date highValue,  \n3: required i64 numNulls,  \n4: required i64 numDVs,  \n5: optional string bitVectors  \n}\n```\n\n----------------------------------------\n\nTITLE: HLL Distinct Count with Materialized View in Hive\nDESCRIPTION: Demonstrates using HyperLogLog (HLL) sketch to compute distinct values transparently through BI mode while utilizing a Materialized View to store intermediate sketches in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/datasketches-integration_177050456.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\n-- create an MV to store precomputed HLL values\ncreate  materialized view mv_1 as\n  select category, ds_hll_sketch(id) from sketch_input group by category;\n\nset hive.optimize.bi.enabled=true;\nselect category,count(distinct id) from sketch_input group by category;\nselect count(distinct id) from sketch_input;\n```\n\n----------------------------------------\n\nTITLE: Building Apache Hive Project with Eclipse Files\nDESCRIPTION: Command to clean the project, package it, and generate Eclipse project files using Ant build system. This needs to be run from the top-level directory of the Hive source code.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/gettingstarted-eclipsesetup_27362091.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ ant clean package eclipse-files\n```\n\n----------------------------------------\n\nTITLE: Implementing ScanPlan Class for HBase Scan Operations\nDESCRIPTION: Class that extends FilterPlan to represent individual HBase scan operations with start/end markers and filters.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbase-execution-plans-for-rawstore-partition-filter-condition_55151993.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nScanPlan extends FilterPlan {\n    private ScanMarker startMarker;\n    private ScanMarker endMarker;\n    private ScanFilter filter;\n\n    public FilterPlan and(FilterPlan other) {\n        // calls this.and(otherScanPlan) on each scan plan in other\n    }\n\n    private ScanPlan and(ScanPlan other) {\n        // combines start marker and end marker and filters of this and other\n    }\n\n    public FilterPlan or(FilterPlan other) {\n        // just create a new FilterPlan from other, with this additional plan\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Sorting Data and Creating HFiles with Hive\nDESCRIPTION: Configuration and SQL commands for sorting data and generating HFiles using Hive. Sets up partitioning, compression, and table properties for bulk loading into HBase.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nset hive.execution.engine=mr;\nset mapred.reduce.tasks=12;\nset hive.mapred.partitioner=org.apache.hadoop.mapred.lib.TotalOrderPartitioner;\nset total.order.partitioner.path=/tmp/hb_range_key_list;\nset hfile.compression=gz;\n\ncreate table hbsort(transaction_id string, user_name string, amount double, ...)\nstored as\nINPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'\nTBLPROPERTIES ('hfile.family.path' = '/tmp/hbsort/cf');\n\ninsert overwrite table hbsort\nselect transaction_id, user_name, amount, ...\nfrom transactions\ncluster by transaction_id;\n```\n\n----------------------------------------\n\nTITLE: Copying HFiles to Local Directory\nDESCRIPTION: HDFS command to copy generated HFiles to a local directory for bulk loading into HBase.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hbasebulkload_27362088.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndfs -copyToLocal /tmp/hbsort/cf/* /tmp/hbout\n```\n\n----------------------------------------\n\nTITLE: Optimizing MapReduce with HCatalog using HDFS in Apache Hive\nDESCRIPTION: Shell commands for optimizing MapReduce job execution with HCatalog by using HDFS locations for JAR files, reducing the need to ship libjars repeatedly.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-inputoutput_34013776.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/hadoop fs -copyFromLocal $HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar /tmp\nbin/hadoop fs -copyFromLocal $HIVE_HOME/lib/hive-metastore-0.10.0.jar /tmp\nbin/hadoop fs -copyFromLocal $HIVE_HOME/lib/libthrift-0.7.0.jar /tmp\nbin/hadoop fs -copyFromLocal $HIVE_HOME/lib/hive-exec-0.10.0.jar /tmp\nbin/hadoop fs -copyFromLocal $HIVE_HOME/lib/libfb303-0.7.0.jar /tmp\nbin/hadoop fs -copyFromLocal $HIVE_HOME/lib/jdo2-api-2.3-ec.jar /tmp\nbin/hadoop fs -copyFromLocal $HIVE_HOME/lib/slf4j-api-1.6.1.jar /tmp\n\nexport LIB_JARS=hdfs:///tmp/hcatalog-core-0.5.0.jar,\nhdfs:///tmp/hive-metastore-0.10.0.jar,\nhdfs:///tmp/libthrift-0.7.0.jar,\nhdfs:///tmp/hive-exec-0.10.0.jar,\nhdfs:///tmp/libfb303-0.7.0.jar,\nhdfs:///tmp/jdo2-api-2.3-ec.jar,\nhdfs:///tmp/slf4j-api-1.6.1.jar\n\n# (Other statements remain the same.)\n```\n\n----------------------------------------\n\nTITLE: Processing Data with Pig Using HCatalog\nDESCRIPTION: This Pig Latin script demonstrates how to use HCatalog's HCatLoader and HCatStorer to simplify data loading and storing, abstracting away HDFS paths.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hcatalog-usinghcat_34013260.md#2025-04-09_snippet_2\n\nLANGUAGE: pig\nCODE:\n```\nA = load 'rawevents' using org.apache.hive.hcatalog.pig.HCatLoader();\nB = filter A by date = '20100819' and by bot_finder(zeta) = 0;\n...\nstore Z into 'processedevents' using org.apache.hive.hcatalog.pig.HCatStorer(\"date=20100819\");\n```\n\n----------------------------------------\n\nTITLE: Setting Hive to Local Mode in MRv1\nDESCRIPTION: Shows the configuration setting needed to run Hive in local mode when using MapReduce v1. This setting makes debugging distributed queries easier by running them locally.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hivedeveloperfaq_27823747.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nSET mapred.job.tracker=local\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive to use an EC2 Hadoop cluster\nDESCRIPTION: This snippet shows how to point the Hive CLI to use a specific Hadoop cluster running on EC2 by setting the HDFS and MapReduce job tracker locations using the public hostname of the master node.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hiveaws-hivings3nremotely_27362102.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nhive> set fs.default.name=hdfs://ec2-12-34-56-78.compute-1.amazonaws.com:50001;\nhive> set mapred.job.tracker=ec2-12-34-56-78.compute-1.amazonaws.com:50002;\n```\n\n----------------------------------------\n\nTITLE: Loading Data into an Iceberg Table from a Local File\nDESCRIPTION: Shows how to load data into an Iceberg table directly from a local file system path using the LOAD DATA command.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nLOAD DATA LOCAL INPATH '/data/files/doctors.avro' OVERWRITE INTO TABLE ice_avro;\n```\n\n----------------------------------------\n\nTITLE: Querying Data from an Iceberg Table\nDESCRIPTION: Demonstrates a basic SELECT query with filtering on an Iceberg table, showing that Iceberg tables support standard Hive query syntax.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/hive-iceberg-integration_282102247.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM TBL_ICE WHERE ID > 5;\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark JARs Location (Hive 2.2.0+)\nDESCRIPTION: XML configuration for specifying Spark JARs location in HDFS for Hive 2.2.0 and later versions.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/44302539.md#2025-04-09_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n  <name>spark.yarn.jars</name>\n  <value>hdfs://xxxx:8020/spark-jars/*</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Example of Archiving a Hive Table Partition\nDESCRIPTION: Provides a concrete example of archiving a partition in a table named 'srcpart' for a specific date and hour.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-archiving_27362031.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE srcpart ARCHIVE PARTITION(ds='2008-04-08', hr='12')\n```\n\n----------------------------------------\n\nTITLE: Creating, Showing, and Dropping a Basic Compact Index in HiveQL\nDESCRIPTION: Creates a compact index on column2 of table01, shows the index, then drops it. This is the most basic form of index creation in Hive.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-indexing_31822176.md#2025-04-09_snippet_0\n\nLANGUAGE: hiveql\nCODE:\n```\nCREATE INDEX table01_index ON TABLE table01 (column2) AS 'COMPACT';\nSHOW INDEX ON table01;\nDROP INDEX table01_index ON table01;\n```\n\n----------------------------------------\n\nTITLE: Aggregate Functions in OVER Clause in HiveQL\nDESCRIPTION: Shows how to use aggregate functions within the OVER clause, a feature added in Hive 2.1.0 and later.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/languagemanual-windowingandanalytics_31819589.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT rank() OVER (ORDER BY sum(b))\nFROM T\nGROUP BY a;\n```\n\n----------------------------------------\n\nTITLE: Showing Principals in Hive\nDESCRIPTION: Lists all roles and users belonging to a specific role. Only admin role can execute this command.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/sql-standard-based-hive-authorization_40509928.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSHOW PRINCIPALS role_name;\n```\n\n----------------------------------------\n\nTITLE: Displaying MySQL-like INFORMATION_SCHEMA.COLUMNS Structure\nDESCRIPTION: This SQL snippet shows a potential structure for storing type qualifier information, similar to MySQL's INFORMATION_SCHEMA.COLUMNS table. It includes columns for character length, numeric precision and scale, and character set information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/type-qualifiers-in-hive_33298524.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n|  CHARACTER_MAXIMUM_LENGTH  |  bigint(21) unsigned  |  YES  |   |  NULL  |   |\n|  CHARACTER_OCTET_LENGTH  |  bigint(21) unsigned  |  YES  |   |  NULL  |   |\n|  NUMERIC_PRECISION  |  bigint(21) unsigned  |  YES  |   |  NULL  |   |\n|  NUMERIC_SCALE  |  bigint(21) unsigned  |  YES  |   |  NULL  |   |\n|  CHARACTER_SET_NAME  |  varchar(32)  |  YES  |   |  NULL  |   |\n|  COLLATION_NAME  |  varchar(32)  |  YES  |   |  NULL  |   |\n```\n\n----------------------------------------\n\nTITLE: Downloading Hadoop Dependencies\nDESCRIPTION: Shell commands to manually download Hadoop dependencies when encountering connection timeout errors.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/user-faq_27362095.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/.ant/cache/hadoop/core/sources\nwget http://archive.apache.org/dist/hadoop/core/hadoop-0.20.1/hadoop-0.20.1.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Creating Python Mapper for MovieLens Data\nDESCRIPTION: This Python script serves as a mapper for a custom MapReduce job. It transforms the input data by extracting the day of the week from the timestamp.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/userguide_27362066.md#2025-04-09_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport sys\nimport datetime\n\nfor line in sys.stdin:\n  line = line.strip()\n  userid, movieid, rating, unixtime = line.split('\\t')\n  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()\n  print '\\t'.join([userid, movieid, rating, str(weekday)])\n```\n\n----------------------------------------\n\nTITLE: Explaining Vectorized Query Execution in Hive SQL\nDESCRIPTION: SQL commands to enable vectorized execution and explain a query plan to verify which parts are being vectorized.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/vectorized-query-execution_34838326.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nset hive.vectorized.execution.enabled = true;\nexplain select count(*) from vectorizedtable;\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Path Configuration in WebHCat\nDESCRIPTION: Example showing how to specify the Pig executable path using environment variables in WebHCat configuration files. This demonstrates the usage of the special 'env' variable to access environment variables.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-configure_34015738.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n${env.PIG_HOME}/bin/pig\n```\n\n----------------------------------------\n\nTITLE: Error Response Format\nDESCRIPTION: Example JSON response showing an error case when the requested table does not exist.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-getproperty_34017004.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": \"Table test_table does not exist\",\n  \"errorCode\": 404,\n  \"database\": \"default\",\n  \"table\": \"test_table\"\n}\n```\n\n----------------------------------------\n\nTITLE: WebHCat Hive Job API Response JSON\nDESCRIPTION: This JSON snippet shows the expected response format when submitting a Hive job through the WebHCat API. It includes the job ID and execution information.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-hive_34017180.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"id\": \"job_201111111311_0005\",\n \"info\": {\n          \"stdout\": \"templeton-job-id:job_201111111311_0005\n                    \",\n          \"stderr\": \"\",\n          \"exitcode\": 0\n         }\n}\n```\n\n----------------------------------------\n\nTITLE: Pig Script for ID Extraction\nDESCRIPTION: A simple Pig script that loads data from a 'passwd' file using PigStorage delimiter and extracts the first field as ID.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-pig_34017169.md#2025-04-09_snippet_0\n\nLANGUAGE: pig\nCODE:\n```\nA = load 'passwd' using PigStorage(':');\nB = foreach A generate $0 as id;\ndump B;\n```\n\n----------------------------------------\n\nTITLE: Retrieving WebHCat Server Status using Curl\nDESCRIPTION: This curl command sends a GET request to the WebHCat status endpoint to retrieve the current server status. It's useful for heartbeat monitoring and checking the API version.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/webhcat-reference-status_34015941.md#2025-04-09_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\n% curl -s 'http://localhost:50111/templeton/v1/status'\n```\n\n----------------------------------------\n\nTITLE: Case J1: Demonstrating Join Predicate on Preserved Row Table in Hive\nDESCRIPTION: This example shows a case where a join predicate (s1.key > '2') is applied to a preserved row table (s1) in a left outer join. The execution plan shows the predicate is not pushed down but instead applied as a filter predicate in the Join Operator.\nSOURCE: https://github.com/apache/hive-site/blob/main/themes/hive/static/attachments/27362075/35193191.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nexplain \nselect s1.key, s2.key \nfrom src s1 left join src s2 on s1.key > '2';\n```\n\n----------------------------------------\n\nTITLE: Modifying Local MapReduce Tasks for Partition Pruning\nDESCRIPTION: Steps to modify the corresponding LocalMRTask to introduce a new PartitionPrunerSink Operator for implementing the partition pruning functionality. This includes identifying relevant operators and establishing connections between them.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/docs/latest/mapjoin-and-partition-pruning_34015666.md#2025-04-09_snippet_4\n\nLANGUAGE: pseudo\nCODE:\n```\n5. If partition-pruner-bigtable-candidate set and set-generation-keys are non empty then Modify corresponding LocalMRTask to introduce the new PartitionPrunerSink Operator (if not already).  \n\n   a) Add to Physical Context a map of MapJoinOperator â€“ HashTableSink Operator. This needs to happen during HashTableSink generation time.  \n\n   b) From physical context get the HashTableSinkOperator corresponding to the MapJoinOperator.  \n\n   c) From all the parents of MapJoin Operator identify the ones representing small tables in the set-generation-key-map.  \n\n   d) Create a new PartitionDescGenSinkOp (with set-generation-key-map)  \n\n   e) Add it as child of elements from #c.\n```\n\n----------------------------------------\n\nTITLE: Including Apache License 2.0 Notice in HTML Comment\nDESCRIPTION: This snippet contains the Apache License 2.0 notice as an HTML comment. It provides information about copyright, distribution, and usage terms for the content.\nSOURCE: https://github.com/apache/hive-site/blob/main/content/general/PrivacyPolicy.md#2025-04-09_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License. -->\n```"
  }
]
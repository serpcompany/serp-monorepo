[
  {
    "owner": "genlm",
    "repo": "genlm-control",
    "content": "TITLE: Controlling LLM Generation with a JSON Schema (Python)\nDESCRIPTION: This Python example demonstrates how to guide an instruction-tuned LLM (Llama-3.2-1B-Instruct) to generate JSON objects conforming to a predefined `book_schema`. It initializes a `PromptedLLM`, sets a specific chat-templated prompt including examples, creates a `JsonSchema` potential from the schema, coerces it for the LLM, combines potentials with `AWRS`, and uses asynchronous `smc` generation. Parameters like `n_particles`, `ess_threshold`, and `max_tokens` control the SMC process. The final output (`sequences.decoded_posterior`) shows a distribution over valid JSON strings matching the schema.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/index.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom genlm.control import PromptedLLM, JsonSchema, AWRS\n\nperson_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"type\": \"string\",\n            \"enum\": [\"Alice\", \"Bob\", \"Charlie\"],\n            \"description\": \"The name of the person\"\n        },\n        \"age\": {\n            \"type\": \"integer\",\n            \"minimum\": 20,\n            \"maximum\": 80,\n            \"description\": \"The age of the person\"\n        },\n    },\n}\n\nbook_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"title\": {\n            \"type\": \"string\",\n            \"minLength\": 1,\n            \"description\": \"The title of the book\"\n        },\n        \"pages\": {\n            \"type\": \"integer\",\n            \"minimum\": 1,\n            \"maximum\": 2000,\n            \"description\": \"The number of pages in the book\"\n        },\n        \"genre\": {\n            \"type\": \"string\",\n            \"enum\": [\"fiction\", \"non-fiction\", \"mystery\"],\n            \"description\": \"The genre of the book\"\n        }\n    },\n}\n\n# Create a language model potential.\n# Since this task is harder, we use a larger model.\n# (You will need to login via the Hugging Face CLI and have access to the model.)\nllm = PromptedLLM.from_name(\n    \"meta-llama/Llama-3.2-1B-Instruct\",\n    eos_tokens=[b\"<|eom_id|>\", b\"<|eot_id|>\"],\n    temperature=0.8\n)\n\n# Set the prompt for the language model.\n# Since we are using an instruction-tuned model, we use the chat template.\n# The prompt contains an example of a schema and a generated object,\n# followed by the schema we want to match.\nllm.prompt_ids = llm.model.tokenizer.apply_chat_template(\n    conversation=[\n        {\"role\": \"system\", \"content\": \"You need to generate a JSON object that matches the schema below. Only generate the JSON object on a single line with no other text.\"},\n        {\"role\": \"user\", \"content\": json.dumps(person_schema)},\n        {\"role\": \"assistant\", \"content\": '{\"name\": \"Alice\", \"age\": 30}'},\n        {\"role\": \"user\", \"content\": json.dumps(book_schema)},\n    ],\n    tokenize=True,\n    add_generation_prompt=True\n)\n\n# Create a schema potential.\nschema_potential = JsonSchema(book_schema)\n\n# Coerce the schema potential so that it operates on the token type of the language model.\ncoerced_schema = schema_potential.coerce(llm, f=b\"\".join)\n\n# Create a token sampler that combines the language model and the schema potential.\ntoken_sampler = AWRS(llm, coerced_schema)\n\n# Generate text using SMC.\n# Generation is asynchronous; use `await` if calling in an async context (like in an async\n# function or in a Jupyter notebook) and `asyncio.run(token_sampler.smc(...))` otherwise.\nsequences = await token_sampler.smc(\n    n_particles=2, # Number of candidate sequences to maintain\n    ess_threshold=0.5, # Threshold for resampling\n    max_tokens=30, # Maximum sequence length\n    verbosity=1 # Print particles at each step\n)\n\n# Show the inferred posterior distribution over complete UTF-8 decodable sequences.\nsequences.decoded_posterior\n# Example output:\n# {\n#   '{\"title\": \"The Lord of the Rings\", \"pages\": 1200, \"genre\": \"fiction\"}': 0.5008318164809697,\n#   '{\"title\": \"The Great Gatsby\", \"pages\": 178, \"genre\": \"fiction\"}': 0.49916818351903025,\n# }\n```\n\n----------------------------------------\n\nTITLE: Generating JSON Objects Matching a Schema Using GenLM Control with Python\nDESCRIPTION: This Python snippet illustrates controlling LLM output to match a JSON schema using GenLM Control. It sets up example person and book schemas, configures a prompted LLM (optionally a larger, instruction-tuned model such as Llama), builds a JsonSchema potential, coerces it to match LLM tokens, and uses AWRS with SMC for generation. Required dependencies are genlm-control, an accessible HuggingFace-compatible model, and Python's json module. Parameters in token_sampler.smc control the number of candidate sequences, resampling threshold, sequence length, and verbosity. Output is a posterior over JSON object strings that strictly conform to the provided schema.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom genlm.control import PromptedLLM, JsonSchema, AWRS\n\nperson_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"type\": \"string\",\n            \"enum\": [\"Alice\", \"Bob\", \"Charlie\"],\n            \"description\": \"The name of the person\"\n        },\n        \"age\": {\n            \"type\": \"integer\",\n            \"minimum\": 20,\n            \"maximum\": 80,\n            \"description\": \"The age of the person\"\n        },\n    },\n}\n\nbook_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"title\": {\n            \"type\": \"string\",\n            \"minLength\": 1,\n            \"description\": \"The title of the book\"\n        },\n        \"pages\": {\n            \"type\": \"integer\",\n            \"minimum\": 1,\n            \"maximum\": 2000,\n            \"description\": \"The number of pages in the book\"\n        },\n        \"genre\": {\n            \"type\": \"string\",\n            \"enum\": [\"fiction\", \"non-fiction\", \"mystery\"],\n            \"description\": \"The genre of the book\"\n        }\n    },\n}\n\n# Create a language model potential.\n# Since this task is harder, we use a larger model.\n# (You will need to login via the Hugging Face CLI and have access to the model.)\nllm = PromptedLLM.from_name(\n    \"meta-llama/Llama-3.2-1B-Instruct\",\n    eos_tokens=[b\"<|eom_id|>\", b\"<|eot_id|>\"],\n    temperature=0.8\n)\n\n# Set the prompt for the language model.\n# Since we are using an instruction-tuned model, we use the chat template.\n# The prompt contains an example of a schema and a generated object,\n# followed by the schema we want to match.\nllm.prompt_ids = llm.model.tokenizer.apply_chat_template(\n    conversation=[\n        {\"role\": \"system\", \"content\": \"You need to generate a JSON object that matches the schema below. Only generate the JSON object on a single line with no other text.\"},\n        {\"role\": \"user\", \"content\": json.dumps(person_schema)},\n        {\"role\": \"assistant\", \"content\": '{\"name\": \"Alice\", \"age\": 30}'},\n        {\"role\": \"user\", \"content\": json.dumps(book_schema)},\n    ],\n    tokenize=True,\n    add_generation_prompt=True\n)\n\n# Create a schema potential.\nschema_potential = JsonSchema(book_schema)\n\n# Coerce the schema potential so that it operates on the token type of the language model.\ncoerced_schema = schema_potential.coerce(llm, f=b\"\".join)\n\n# Create a token sampler that combines the language model and the schema potential.\ntoken_sampler = AWRS(llm, coerced_schema)\n\n# Generate text using SMC.\n# Generation is asynchronous; use `await` if calling in an async context (like in an async\n# function or in a Jupyter notebook) and `asyncio.run(token_sampler.smc(...))` otherwise.\nsequences = await token_sampler.smc(\n    n_particles=2, # Number of candidate sequences to maintain\n    ess_threshold=0.5, # Threshold for resampling\n    max_tokens=30, # Maximum sequence length\n    verbosity=1 # Print particles at each step\n)\n\n# Show the inferred posterior distribution over complete UTF-8 decodable sequences.\nsequences.decoded_posterior\n# Example output:\n# {\n#   '{\"title\": \"The Lord of the Rings\", \"pages\": 1200, \"genre\": \"fiction\"}': 0.5008318164809697,\n#   '{\"title\": \"The Great Gatsby\", \"pages\": 178, \"genre\": \"fiction\"}': 0.49916818351903025,\n# }\n```\n\n----------------------------------------\n\nTITLE: Controlling LLM Output with Regular Expressions using GenLM Control in Python\nDESCRIPTION: This Python code demonstrates how to constrain an LLM's output using a regular expression enforced by a finite-state automaton (FSA) potential in GenLM Control. It creates a prompted LLM, builds an FSA from a regex, coerces it to match the LLM's tokenization, and uses AWRS to sample outputs via sequential Monte Carlo. Required dependencies include genlm-control and an installed, supported LLM (e.g., GPT-2). Input parameters configure the number of particles, ESS threshold, sequence length, and verbosity. Output is a posterior distribution over sequences that match the regex constraint.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom genlm.control import PromptedLLM, BoolFSA, AWRS\n\n# Create a language model potential.\nllm = PromptedLLM.from_name(\"gpt2\")\nllm.set_prompt_from_str(\"Here is my honest opinion:\")\n\n# Create a finite-state automaton potential using a regular expression.\nfsa = BoolFSA.from_regex(r\" SMC is (ðŸ”¥ðŸ”¥|ðŸ˜ðŸ˜|ðŸ¤ŒðŸ¤Œ) with LMs\")\n\n# Coerce the FSA so that it operates on the token type of the language model.\ncoerced_fsa = fsa.coerce(llm, f=b\"\".join)\n\n# Create a token sampler that combines the language model and FSA.\ntoken_sampler = AWRS(llm, coerced_fsa)\n\n# Generate text using SMC.\n# Generation is asynchronous; use `await` if calling in an async context (like in an async\n# function or in a Jupyter notebook) and `asyncio.run(token_sampler.smc(...))` otherwise.\nsequences = await token_sampler.smc(\n    n_particles=10, # Number of candidate sequences to maintain\n    ess_threshold=0.5, # Threshold for resampling\n    max_tokens=30, # Maximum sequence length\n    verbosity=1 # Print particles at each step\n)\n\nsequences.decoded_posterior\n# Example output:\n# {\n#   ' SMC is ðŸ”¥ðŸ”¥ with LMs': 1.0,\n# }\n```\n\n----------------------------------------\n\nTITLE: Applying Regex Constraints with BoolFSA and AWRS Sampler in Python\nDESCRIPTION: Shows how to enforce regular expression constraints during generation. It creates a `genlm.control.potential.built_in.wfsa.BoolFSA` from a regex pattern, demonstrates its byte-level operation, coerces it to match the language model's vocabulary (`product.coerce`), and uses the `genlm.control.sampler.token.AWRS` sampler. AWRS efficiently handles finite-state automata constraints alongside the product potential during SMC. Depends on `genlm.control.BoolFSA`, `genlm.control.AWRS`, and the `product` potential from the previous snippet. Outputs posterior distributions satisfying both the prompt intersection and the regex constraint.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/getting_started.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom genlm.control import BoolFSA, AWRS\n\n# Create a regex constraint that matches sequences containing the word \"the\"\n# followed by either \"best\" or \"worst\" and then anything else\nbest_fsa = BoolFSA.from_regex(r\"\\sthe\\s(best|worst).*\")\n\n# BoolFSA's are defined over individual bytes by default\n# Their `prefix` and `complete` methods are called on byte sequences\nprint(\"best_fsa.prefix(b'the bes') =\", await best_fsa.prefix(b\"the bes\"))\nprint(\n    \"best_fsa.complete(b'the best city') =\",\n    await best_fsa.complete(b\"the best city\"),\n)\n\n# Coerce the FSA to work with the LLM's vocabulary\ncoerced_fsa = best_fsa.coerce(product, f=b\"\".join)\n\n# Use the AWRS token sampler; it will only call the fsa on a subset of the product vocabulary\ntoken_sampler = AWRS(product, coerced_fsa)\n\nsequences = await token_sampler.smc(n_particles=5, max_tokens=25, ess_threshold=0.5)\n\nsequences.posterior\n\nsequences.decoded_posterior\n```\n\n----------------------------------------\n\nTITLE: Combining LLM Prompts using Product Potential in Python\nDESCRIPTION: Illustrates how to combine two language models (`PromptedLLM` instances) using the element-wise product (`*` operator), creating a `genlm.control.potential.Product` potential. It then uses `direct_token_sampler` with this product potential to sample sequences conditioned on both prompts simultaneously via SMC. Depends on the setup from the previous snippet (an existing `PromptedLLM` instance `mtl_llm`) and `genlm.control.potential.Product`. Outputs posterior distributions reflecting the combined constraints.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/getting_started.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Spawn a new language model (shallow copy, sharing the same underlying model)\nbos_llm = mtl_llm.spawn()\nbos_llm.set_prompt_from_str(\"Boston is\")\n\n# Take the product of the two language models\n# This defines a `Product` potential which is the element-wise product of the two LMs\nproduct = mtl_llm * bos_llm\n\n# Create a sampler that proposes tokens by sampling directly from the product\ntoken_sampler = direct_token_sampler(product)\n\nsequences = await token_sampler.smc(n_particles=5, max_tokens=25, ess_threshold=0.5)\n\nsequences.posterior\n\nsequences.decoded_posterior\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Sampling with PromptedLLM and direct_token_sampler in Python\nDESCRIPTION: Demonstrates basic language model sampling using `genlm.control.PromptedLLM` loaded with a HuggingFace model ('gpt2'). It sets a fixed prompt, uses `genlm.control.direct_token_sampler` to propose tokens based on the LM's distribution, and runs Sequential Monte Carlo (SMC) to generate sequences. Dependencies include `genlm.control`. Key parameters are `temperature`, `eos_tokens`, `n_particles`, `max_tokens`, and `ess_threshold`. Outputs posterior distributions over token sequences (bytes) and decoded strings.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/getting_started.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom genlm.control import PromptedLLM, direct_token_sampler\n\n# Load gpt2 (or any other HuggingFace model)\nmtl_llm = PromptedLLM.from_name(\"gpt2\", temperature=0.5, eos_tokens=[b'.'])\n\n# Set the fixed prompt prefix for the language model\n# All language model predictions will be conditioned on this prompt\nmtl_llm.set_prompt_from_str(\"Montreal is\")\n\n# Load a sampler that proposes tokens by sampling directly from the LM's distribution\ntoken_sampler = direct_token_sampler(mtl_llm)\n\n# Run SMC with 5 particles, a maximum of 25 tokens, and an ESS threshold of 0.5\nsequences = await token_sampler.smc(n_particles=5, max_tokens=25, ess_threshold=0.5)\n\n# Show the posterior over token sequences\nsequences.posterior\n\n# Show the posterior over complete UTF-8 decodable sequences\nsequences.decoded_posterior\n```\n\n----------------------------------------\n\nTITLE: Creating Weighted Finite-State Automaton (WFSA) from Regex (Python)\nDESCRIPTION: Here, a WFSA is constructed from a regular expression pattern, automatically normalizing transitions into probability distributions. This enables modeling soft constraints or preferences over byte-level token sequences. Dependencies include genlm-control and the WFSA class. The input is a regex string; the output is a WFSA object that can assign log-probabilities to matching sequences.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a WFSA from a regex pattern\\n# Transitions are automatically normalized to form probability distributions\\nwfsa = WFSA.from_regex(r\"\\\\sthe\\\\s(best|worst).*ðŸ˜Ž\")\n```\n\n----------------------------------------\n\nTITLE: Creating Boolean Finite-State Automaton (BoolFSA) from Regex (Python)\nDESCRIPTION: This snippet shows the creation of a BoolFSA from a regex, resulting in hard binary transitions (allowed or disallowed) on byte-level sequences, as opposed to weighted probabilities. BoolFSA operates under the same principle as WFSA but enforces strict (0 or -inf log weight) transitions. Prerequisites are the genlm-control library and a regex input; the output is a BoolFSA object for use in hard constraint enforcement.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create a boolean FSA from a regex pattern\\n# Transitions are binary (0 or -inf in log space)\\nfsa = BoolFSA.from_regex(r\"\\\\sthe\\\\s(best|worst).*ðŸ˜Ž\")\n```\n\n----------------------------------------\n\nTITLE: Implementing and Using a Custom Sentiment Potential as Critic in Python\nDESCRIPTION: Details the creation of a custom `genlm.control.potential.base.Potential` subclass (`SentimentAnalysis`) for sentiment analysis using a pre-trained HuggingFace transformer model (DistilBERT). This potential acts as a critic to guide generation towards positive sentiment. It demonstrates implementing `prefix`, `complete`, and optimized `batch_complete`, `batch_prefix` methods, testing the potential's consistency, coercing it (`critic = sentiment_analysis.coerce(...)`) for use with the `AWRS` sampler's target, and integrating it into the SMC process via the `critic` parameter. Depends on `torch`, `transformers`, `genlm.control.Potential`, `genlm.control.AWRS`, and previous setup (`product`, `coerced_fsa`, `token_sampler`). Outputs posterior distributions biased towards positive sentiment.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/getting_started.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import (\n    DistilBertTokenizer,\n    DistilBertForSequenceClassification,\n)\nfrom genlm.control import Potential\n\n# Create our own custom potential for sentiment analysis.\n# Custom potentials must subclass `Potential` and implement the `prefix` and `complete` methods.\n# They can also override other methods, like `batch_prefix`, and `batch_complete` for improved performance.\n# Each Potential needs to specify its vocabulary of tokens; this potential has a vocabulary of individual bytes.\nclass SentimentAnalysis(Potential):\n    def __init__(self, model, tokenizer, sentiment=\"POSITIVE\"):\n        self.model = model\n        self.tokenizer = tokenizer\n\n        self.sentiment_idx = model.config.label2id.get(sentiment, None)\n        if self.sentiment_idx is None:\n            raise ValueError(f\"Sentiment {sentiment} not found in model labels\")\n\n        super().__init__(vocabulary=list(range(256)))  # Defined over bytes\n\n    def _forward(self, contexts):\n        strings = [bytes(context).decode(\"utf-8\", errors=\"ignore\") for context in contexts]\n        inputs = self.tokenizer(strings, return_tensors=\"pt\", padding=True)\n        with torch.no_grad():\n            logits = self.model(**inputs).logits\n        return logits.log_softmax(dim=-1)[:, self.sentiment_idx].cpu().numpy()\n\n    async def prefix(self, context):\n        return self._forward([context])[0].item()\n\n    async def complete(self, context):\n        return self._forward([context])[0].item()\n\n    async def batch_complete(self, contexts):\n        return self._forward(contexts)\n\n    async def batch_prefix(self, contexts):\n        return self._forward(contexts)\n\n# Initialize sentiment analysis potential\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\nsentiment_analysis = SentimentAnalysis(\n    model=DistilBertForSequenceClassification.from_pretrained(model_name),\n    tokenizer=DistilBertTokenizer.from_pretrained(model_name),\n    sentiment=\"POSITIVE\",\n)\n\n# Test the potential\nprint(\"\\nSentiment analysis test:\")\nprint(\n    \"sentiment_analysis.prefix(b'so good') =\",\n    await sentiment_analysis.prefix(b\"so good\"),\n)\nprint(\n    \"sentiment_analysis.prefix(b'so bad') =\",\n    await sentiment_analysis.prefix(b\"so bad\"),\n)\n\n# Verify the potential satisfies required properties\nawait sentiment_analysis.assert_logw_next_consistency(b\"the best\", top=5)\nawait sentiment_analysis.assert_autoreg_fact(b\"the best\")\n\n# Set up efficient sampling with the sentiment analysis potential\ntoken_sampler = AWRS(product, coerced_fsa)\ncritic = sentiment_analysis.coerce(token_sampler.target, f=b\"\".join)\n\n# Run SMC using the sentiment analysis potential as a critic\nsequences = await token_sampler.smc(\n    n_particles=5,\n    max_tokens=25,\n    ess_threshold=0.5,\n    critic=critic, # Pass the critic to the SMC sampler; this will reweight samples at each step based on their positivity\n)\n\n# Show the posterior over complete UTF-8 decodable sequences\nsequences.decoded_posterior\n```\n\n----------------------------------------\n\nTITLE: Creating Weighted Context-Free Grammar (WCFG) from String (Python)\nDESCRIPTION: A WCFG is built from a grammar specification in string format, with each production weighted by a probability. This provides probabilistic soft constraints for valid sequences, using the Earley algorithm. Dependencies include WCFG from genlm-control; grammar rules must be specified in an expected format. The input is the production-rule string; output is a WCFG object for use as a potential.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncfg = WCFG.from_string(\"\"\"\\n    1.0: S -> NP VP\\n    0.5: NP -> the N\\n    0.5: NP -> a N\\n    1.0: VP -> V NP\\n    0.5: N -> cat\\n    0.5: N -> dog\\n    0.5: V -> saw\\n    0.5: V -> chased\\n\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Creating Boolean Context-Free Grammar (BoolCFG) from Lark Syntax (Python)\nDESCRIPTION: This snippet creates a BoolCFG using Lark syntax, enabling binary context-free constraints for sequences during generation. It requires Lark-compatible grammar in string form and the BoolCFG class. The input is the grammar string; output is a BoolCFG object. Earley parsing is used for recognition.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create a boolean CFG from a Lark grammar string\\ncfg = BoolCFG.from_lark(\"\"\"\\n    start: np vp\\n    np: (\\\"the\\\" | \\\"a\\\") WS n\\n    vp: WS v WS np\\n    n: \\\"cat\\\" | \\\"dog\\\"\\n    v: \\\"saw\\\" | \\\"chased\\\"\\n    %import common.WS\\n\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Coercing a Byte-Level FSA to Work with Tokenized Language Model in genlm-control (Python)\nDESCRIPTION: This snippet illustrates the use of the Coerced class to adapt a finite-state automaton (FSA) operating on bytes to work with a language model whose tokens are byte sequences. By supplying a coercion function (f=b''.join), the FSA is mapped to the tokenization of the LLM, enabling their combination via the product operator. Prerequisites include having BoolFSA, PromptedLLM, and both models pre-initialized.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Example: Coercing a byte-level FSA to work with a language model's tokens\nfsa = BoolFSA.from_regex(r\"\\sthe\\s(best|worst).*\")  # Works on bytes\nllm = PromptedLLM.from_name(\"gpt2\")  # Works on byte sequences\n\n# Coerce the FSA to work with the LLM's tokens by joining tokens into bytes\ncoerced_fsa = fsa.coerce(llm, f=b''.join)\n\n# Now we can combine them using the product operator!\nproduct = llm * coerced_fsa\n```\n\n----------------------------------------\n\nTITLE: Creating Product Potentials Using Multiplication Operator in genlm-control (Python)\nDESCRIPTION: This snippet demonstrates how to create a new product potential by multiplying two existing potentials, mtl_llm and bos_llm. The product potential will operate on the intersection of both vocabularies, combining their prefix, complete, and next-token potentials multiplicatively (in log-space, additively). This operation is subject to limitations when the vocabularies of the two potentials do not sufficiently overlap; a warning is raised in such cases.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Create product using multiplication operator\nproduct = mtl_llm * bos_llm\n```\n\n----------------------------------------\n\nTITLE: Combining Two Prompted LLM Potentials Using Product (Python)\nDESCRIPTION: This snippet shows how to combine two PromptedLLM potentials using the Product class, allowing intersection (joint constraint) of two sets of generation conditions. Each LLM is initialized from a model and prompt; one is 'spawned' to duplicate the conditioning state before modifying the prompt. Dependencies include PromptedLLM and Product from genlm-control. Inputs are initial prompt strings; the output Product instance combines both constraints.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Example: Prompt intersection\\nmtl_llm = PromptedLLM.from_name(\\\"gpt2\\\")\\nmtl_llm.set_prompt_from_str(\\\"Montreal is\\\")\\n\\nbos_llm = mtl_llm.spawn()\\nbos_llm.set_prompt_from_str(\\\"Boston is\\\")\n```\n\n----------------------------------------\n\nTITLE: Initializing PromptedLLM Language Model (Python)\nDESCRIPTION: This snippet demonstrates the initialization of a PromptedLLM from a model name (GPT-2) with a specified temperature for stochasticity in generation and the subsequent setting of a prompt prefix to condition generation. It requires the genlm-control library and a compatible GPT-2 model. The temperature parameter controls the randomness, and the prompt sets an initial string, affecting model outputs. Inputs are the model name and a prompt string; the main output is an instantiated, prompt-conditioned LLM object.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Load GPT-2 with temperature 0.5\\nllm = PromptedLLM.from_name(\\\"gpt2\\\", temperature=0.5)\\n\\n# Set a prompt prefix that all generations will be conditioned on\\nllm.set_prompt_from_str(\\\"Montreal is\\\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Autobatching for Potential Instances in Python\nDESCRIPTION: This snippet demonstrates how to enable autobatching on a Potential instance using the to_autobatched() method. Autobatching collects concurrent calls to the same method and processes them together through the corresponding batch method (e.g., batch_complete) for improved performance. It requires that the Potential class implements efficient batch methods. The resulting autobatched instance can be used like the original potential and integrates seamlessly with asyncio for parallel calls. Inputs include the original Potential object and an iterable of input sequences; output is a list of results corresponding to those sequences. Limitations: Only available if batch methods are implemented and beneficial compared to sequential execution.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/performance.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nautobatched_potential = potential.to_autobatched()\n# Use it exactly like a regular potential - batching happens automatically\nresults = await asyncio.gather(\n    *(autobatched.complete(seq) for seq in sequences) # These will batched and processed by batch_complete\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Multiprocessing for Potential Instances in Python\nDESCRIPTION: This snippet shows how to enable multiprocessing for a Potential instance by wrapping it with the to_multiprocess() method and specifying the desired number of worker processes. Multiprocessing uses a MultiProcPotential wrapper, distributing requests across workers asynchronously for improved throughput, especially for compute-heavy tasks. The method requires that the Potential implementation provides a picklable spawn() method to enable new process instantiation. Inputs are the Potential object, a count of worker processes, and an iterable of sequences; output is an asynchronously collected list of results. Limitation: Only available if the Potential supports pickling and implements spawn(); not all built-in or custom potentials provide this.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/performance.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a multiprocess wrapper with desired number of workers\nmp_potential = potential.to_multiprocess(num_workers=2)\n# Use it like a regular potential - requests are distributed across workers\nresults = await asyncio.gather(\n    *(mp_potential.complete(seq) for seq in sequences) # These will be distributed across workers\n)\n```\n\n----------------------------------------\n\nTITLE: Sampling Tokens Using AWRS for Constrained Generation in Python\nDESCRIPTION: Shows how to instantiate an AWRS token sampler for sampling from the product of two potentials (e.g., a language model and a constraint like a CFG) and use it to sample a token and its log importance weight. The sampler is most effective for exact, efficient sampling under boolean constraints and requires both an LLM and a constraint potential as inputs. The sample method is asynchronous and takes the context as input, returning the next token, log weight, and additional information.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/samplers.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a AWRS token sampler from an llm and a cfg\\ntoken_sampler = AWRS(llm, cfg)\\n# Sample a token and weight\\ntoken, logw, _ = await token_sampler.sample(context)\n```\n\n----------------------------------------\n\nTITLE: Sampling Tokens with DirectTokenSampler in Python\nDESCRIPTION: Demonstrates how to create a DirectTokenSampler from a potential and use it to sample the next token, returning the sampled token, log importance weight, and log-probability. Relies on the efficient computation of the potential's logw_next method and is best suited for situations where this operation is fast, such as standard language model generation. Requires the genlm-control Python package and a compatible potential object; outputs a tuple (token, logw, logp) from the async sample method given a context sequence.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/samplers.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Create a direct token sampler for a potential\\nsampler = DirectTokenSampler(potential)\\n\\n# Sample a token\\ntoken, logw, logp = await sampler.sample(context)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Custom Potential Performance with Autobatching in Python\nDESCRIPTION: Demonstrates performance optimization using the `to_autobatched` method on the custom sentiment potential (critic). This creates a wrapper potential (`autobatched_critic`) that automatically batches concurrent calls to its instance methods (`prefix`, `complete`, `logw_next`) for more efficient processing using the underlying batch methods. The example uses `arsenal.timer.timeit` to compare SMC run times with the original critic and the autobatched critic, highlighting the performance improvement. Depends on `arsenal.timer` and the setup from the previous snippets (`token_sampler`, `critic`). Outputs posterior distributions and timing information.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/getting_started.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom arsenal.timer import timeit\n\n# Create an autobatched version of the critic\n# This creates a new potential that automatically batches concurrent\n# requests to the instance methods (`prefix`, `complete`, `logw_next`)\n# and processes them using the batch methods (`batch_complete`, `batch_prefix`, `batch_logw_next`).\nautobatched_critic = critic.to_autobatched()\n\n# Run SMC with timing for comparison\nwith timeit(\"Timing sentiment-guided sampling with autobatching\"):\n    sequences = await token_sampler.smc(\n        n_particles=10,\n        max_tokens=25,\n        ess_threshold=0.5,\n        critic=autobatched_critic, # Pass the autobatched critic to the SMC sampler\n    )\n\nsequences.decoded_posterior\n\n# The autobatched version should be significantly faster than this version\nwith timeit(\"Timing sentiment-guided sampling without autobatching\"):\n    sequences = await token_sampler.smc(\n        n_particles=10,\n        max_tokens=25,\n        ess_threshold=0.5,\n        critic=critic,\n    )\n\nsequences.decoded_posterior\n```\n\n----------------------------------------\n\nTITLE: Defining a Length-Constrained Custom Potential (Python)\nDESCRIPTION: This code defines a custom potential limiting sequence length, by creating a subclass of Potential with specific log-weight logic for complete and partial sequences. Async methods allow interoperability with genlm-controlâ€™s batch and sampling tools. Initialization requires a vocabulary and a length value. Input is any sequence of tokens; output is a log-probability indicating validity as a prefix or complete sequence. It demonstrates correct distinction and implementation of 'complete' vs 'prefix'.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass LengthPotential(Potential):\\n    \\\"\\\"\\\" A potential that only allows sequences of a given length. \\\"\\\"\\\"\\n    def __init__(self, vocabulary, length):\\n        # Initialize the superclass with the potential's vocabulary.\\n        super().__init__(vocabulary)\\n        self.length = length\\n\\n    async def complete(self, context):\\n        # Note: 0.0 = log(1.0) and float('-inf') = log(0.0)\\n        return 0.0 if len(context) == self.length else float('-inf')\\n\\n    async def prefix(self, context):\\n        # Note: 0.0 = log(1.0) and float('-inf') = log(0.0)\\n        return 0.0 if len(context) <= self.length else float('-inf')\\n\\nlength_potential = LengthPotential(vocabulary=[b'the', b'a', b'cat', b'dog', b'saw', b'chased'], length=5)\n```\n\n----------------------------------------\n\nTITLE: Set-based Token Sampling with EagerSetSampler and SetTokenSampler (Python)\nDESCRIPTION: Illustrates the construction of a set-based token sampler by wrapping an EagerSetSampler with SetTokenSampler, allowing token sampling according to structured byte-level constraints. This approach uses an underlying set sampler to preselect a subset of candidate tokens, from which one is chosen by weight, facilitating enforcement of non-boolean constraints. Requires genlm-control, compatible LLM and FSA/constraint objects, and involves asynchronous sampling of the next token, log weight, and extra information by calling sample(context).\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/samplers.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create a set-based token sampler using a set sampler\\nset_sampler = EagerSetSampler(llm, fsa)\\ntoken_sampler = SetTokenSampler(set_sampler)\\n\\n# Sample a token and weight\\ntoken, logw, _ = await token_sampler.sample(context)\n```\n\n----------------------------------------\n\nTITLE: Creating Set-based Token Samplers Using Factory Methods in Python\nDESCRIPTION: Presents the use of topk_token_sampler and eager_token_sampler factory methods to build set-based token samplers for controlled generation tasks, specifying the core dependencies and parameters (e.g., K for top-K selection). These methods abstract the creation of appropriate underlying set samplers and higher-level token samplers for scenarios with structured constraints, returning ready-to-use sampler objects. genlm-control and its sampler submodule must be available, with llm and constraint objects provided; resulting samplers are compatible with the asynchronous sample API.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/samplers.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom genlm.control.sampler import topk_token_sampler, eager_token_sampler\\n\\ntopk_sampler = topk_token_sampler(llm, fsa, K=10)\\n\\neager_sampler = eager_token_sampler(llm, fsa)\n```\n\n----------------------------------------\n\nTITLE: Testing Potential Consistency with Built-in Test Suite (Python)\nDESCRIPTION: This demonstrates using built-in asynchronous assertions from genlm-control to test the implementation of a custom (or any) potential for contract conformance with logw_next and batch consistency properties. These assertions validate if the potentialâ€™s logic matches theoretical requirements, catching logical errors in implementation prior to deployment. Prerequisite is that the potential object implements required async methods; context(s) are input, exceptions are raised if tests fail.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/potentials.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# These will raise an exception if the potential implementation does not satisfy the properties\\nawait potential.assert_logw_next_consistency(context)\\nawait potential.assert_autoreg_fact(context)\\nawait potential.assert_batch_consistency(contexts)\n```\n\n----------------------------------------\n\nTITLE: Controlling LLM Generation with a Regular Expression (Python)\nDESCRIPTION: This Python snippet demonstrates constraining a prompted GPT-2 language model (`PromptedLLM`) to generate text matching a specific regular expression. It uses a Finite State Automaton (`BoolFSA`) derived from the regex, coerces it to match the LLM's token type, combines them using an `AWRS` sampler, and performs asynchronous generation using Sequential Monte Carlo (`smc`). Key parameters include `n_particles` (number of candidate sequences), `ess_threshold` (resampling threshold), and `max_tokens` (maximum sequence length). The expected output is a distribution over sequences satisfying the regex.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/index.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom genlm.control import PromptedLLM, BoolFSA, AWRS\n\n# Create a language model potential.\nllm = PromptedLLM.from_name(\"gpt2\")\nllm.set_prompt_from_str(\"Here is my honest opinion:\")\n\n# Create a finite-state automaton potential using a regular expression.\nfsa = BoolFSA.from_regex(r\" SMC is (ðŸ”¥ðŸ”¥|ðŸ˜ðŸ˜|ðŸ¤ŒðŸ¤Œ) with LMs\")\n\n# Coerce the FSA so that it operates on the token type of the language model.\ncoerced_fsa = fsa.coerce(llm, f=b\"\".join)\n\n# Create a token sampler that combines the language model and FSA.\ntoken_sampler = AWRS(llm, coerced_fsa)\n\n# Generate text using SMC.\n# Generation is asynchronous; use `await` if calling in an async context (like in an async\n# function or in a Jupyter notebook) and `asyncio.run(token_sampler.smc(...))` otherwise.\nsequences = await token_sampler.smc(\n    n_particles=10, # Number of candidate sequences to maintain\n    ess_threshold=0.5, # Threshold for resampling\n    max_tokens=30, # Maximum sequence length\n    verbosity=1 # Print particles at each step\n)\n\nsequences.decoded_posterior\n# Example output:\n# {\n#   ' SMC is ðŸ”¥ðŸ”¥ with LMs': 1.0,\n# }\n```\n\n----------------------------------------\n\nTITLE: Installing GenLM Control with pip in Bash\nDESCRIPTION: This Bash snippet demonstrates how to install the GenLM Control library from PyPI using pip. The installation requires Python (typically 3.8 or higher) and network connectivity. No code parameters are needed, and the command will download and install the package as well as all dependencies.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install genlm-control\n```\n\n----------------------------------------\n\nTITLE: Installing genlm-control via pip (Bash)\nDESCRIPTION: This command installs the `genlm-control` Python library using the pip package manager. It's the standard way to add the library to a Python environment.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/docs/index.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install genlm-control\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with pip - Bash\nDESCRIPTION: This snippet installs the genlm-control project in editable mode with optional extras for testing and documentation using pip. It expects Python >= 3.11 and pip to be available in the environment. The -e flag enables live reloading on code edit; [test,docs] adds dependencies defined in those extras sections. All required dependencies are specified in pyproject.toml.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/DEVELOPING.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[test,docs]\"\n```\n\n----------------------------------------\n\nTITLE: Running Test Suite with pytest - Bash\nDESCRIPTION: This snippet uses pytest to execute all tests located in the tests directory of the genlm-control project. It requires that all test dependencies are installed beforehand. The command will discover and run all compatible tests, outputting results to the console.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/DEVELOPING.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest tests\n```\n\n----------------------------------------\n\nTITLE: Serving Documentation Locally with mkdocs - Bash\nDESCRIPTION: This snippet uses mkdocs to launch a local development server, enabling real-time preview of documentation changes. mkdocs serve makes the documentation browsable at a localhost address. It assumes mkdocs and all documentation dependencies are installed.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/DEVELOPING.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdocs serve\n```\n\n----------------------------------------\n\nTITLE: Cloning and Entering the Repository - Bash\nDESCRIPTION: This snippet demonstrates how to clone the genlm-control git repository using SSH and change into the project directory. It assumes you have git installed and SSH keys configured. The first command retrieves the codebase from GitHub, and the second sets your shell to the project's root for subsequent operations.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/DEVELOPING.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:genlm/genlm-control.git\ncd genlm-control\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with mkdocs - Bash\nDESCRIPTION: This command instructs mkdocs to build the static site for the project's documentation. mkdocs must be installed as a dependency. The build output is typically placed within the site/ directory, ready for deployment to GitHub Pages or other static hosts.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/DEVELOPING.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdocs build\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks - Bash\nDESCRIPTION: This command registers the project's pre-commit hooks as defined in .pre-commit-config.yaml with the local git repository. It must be run in the root directory once pre-commit is installed. Hooks are triggered automatically on git commit operations for code formatting and validation.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/DEVELOPING.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Manually Running All Pre-commit Hooks - Bash\nDESCRIPTION: This command invokes all configured pre-commit hooks over every tracked file in the repository. It is useful for validating and auto-formatting code before commits or in CI workflows. Requires that pre-commit is already installed and initialized in the repository.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/DEVELOPING.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit with pipx - Bash\nDESCRIPTION: This command installs pre-commit globally using pipx, isolating it from local Python environments. pipx must be installed prior to this. Pre-commit automates running formatting and linting hooks locally before every code commit.\nSOURCE: https://github.com/genlm/genlm-control/blob/main/DEVELOPING.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npipx install pre-commit\n```"
  }
]
[
  {
    "owner": "sgl-project",
    "repo": "sglang",
    "content": "TITLE: Executing the Tool\nDESCRIPTION: Executes the selected tool function based on the information extracted from the model's response.  Loads the arguments, appends a message with tool_calls, retrieves the tool function from `available_tools`, calls the function with the parsed arguments, and prints the result. It also updates the message history.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncall_data = json.loads(full_arguments)\n\nmessages.append(\n    {\n        \"role\": \"user\",\n        \"content\": \"\",\n        \"tool_calls\": {\"name\": \"get_current_weather\", \"arguments\": full_arguments},\n    }\n)\n\n# Call the corresponding tool function\ntool_name = messages[-1][\"tool_calls\"][\"name\"]\ntool_to_call = available_tools[tool_name]\nresult = tool_to_call(**call_data)\nprint_highlight(f\"Function call result: {result}\")\nmessages.append({\"role\": \"tool\", \"content\": result, \"name\": tool_name})\n\nprint_highlight(f\"Updated message history: {messages}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Output Using Structural Tags with OpenAI API\nDESCRIPTION: This code demonstrates how to use structural tags with the OpenAI API to generate structured outputs based on defined schemas. It defines two tools, `get_current_weather` and `get_current_date`, along with their corresponding JSON schemas. The messages include instructions for the model to use these tools, and the response is formatted using structural tags to enclose the function calls and their parameters.  The `response_format` parameter defines the structure, the triggers, and the associated JSON schemas for each structural tag.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntool_get_current_weather = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"city\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n                },\n                \"state\": {\n                    \"type\": \"string\",\n                    \"description\": \"the two-letter abbreviation for the state that the city is\"\n                    \" in, e.g. 'CA' which would mean 'California'\",\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"description\": \"The unit to fetch the temperature in\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                },\n            },\n            \"required\": [\"city\", \"state\", \"unit\"],\n        },\n    },\n}\n\ntool_get_current_date = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_date\",\n        \"description\": \"Get the current date and time for a given timezone\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"timezone\": {\n                    \"type\": \"string\",\n                    \"description\": \"The timezone to fetch the current date and time for, e.g. 'America/New_York'\",\n                }\n            },\n            \"required\": [\"timezone\"],\n        },\n    },\n}\n\nschema_get_current_weather = tool_get_current_weather[\"function\"][\"parameters\"]\nschema_get_current_date = tool_get_current_date[\"function\"][\"parameters\"]\n\n\ndef get_messages():\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"\n# Tool Instructions\n- Always execute python code in messages that you share.\n- When looking for real time information use relevant functions if available else fallback to brave_search\nYou have access to the following functions:\nUse the function 'get_current_weather' to: Get the current weather in a given location\n{tool_get_current_weather[\"function\"]}\nUse the function 'get_current_date' to: Get the current date and time for a given timezone\n{tool_get_current_date[\"function\"]}\nIf a you choose to call a function ONLY reply in the following format:\n<{{start_tag}}={{function_name}}>{{parameters}}{{end_tag}}\nwhere\nstart_tag => `<function`\nparameters => a JSON dict with the function argument name as key and function argument value as value.\nend_tag => `</function>`\nHere is an example,\n<function=example_function_name>{{{{\"example_name\": \"example_value\"}}}}</function>\nReminder:\n- Function calls MUST follow the specified format\n- Required parameters MUST be specified\n- Only call one function at a time\n- Put the entire function call reply on one line\n- Always add your sources when using search results to answer the user query\nYou are a helpful assistant.\"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"You are in New York. Please get the current date and time, and the weather.\",\n        },\n    ]\n\n\nmessages = get_messages()\n\nresponse = client.chat.completions.create(\n    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n    messages=messages,\n    response_format={\n        \"type\": \"structural_tag\",\n        \"max_new_tokens\": 2048,\n        \"structures\": [\n            {\n                \"begin\": \"<function=get_current_weather>\",\n                \"schema\": schema_get_current_weather,\n                \"end\": \"</function>\",\n            },\n            {\n                \"begin\": \"<function=get_current_date>\",\n                \"schema\": schema_get_current_date,\n                \"end\": \"</function>\",\n            },\n        ],\n        \"triggers\": [\"<function=\"],\n    },\n)\n\nprint_highlight(\n    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang with SkyPilot (Bash)\nDESCRIPTION: This snippet demonstrates launching SGLang on any cloud or Kubernetes cluster using SkyPilot. It sets the HF_TOKEN environment variable and executes the `sky launch` command with a YAML configuration file.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/start/install.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Deploy on any cloud or Kubernetes cluster. Use --cloud <cloud> to select a specific cloud provider.\nHF_TOKEN=<secret> sky launch -c sglang --env HF_TOKEN sglang.yaml\n\n# Get the HTTP API endpoint\nsky status --endpoint 30000 sglang\n```\n\n----------------------------------------\n\nTITLE: Detailed Chat Completions API Request in Python\nDESCRIPTION: This example demonstrates a more detailed use case of the OpenAI Chat Completions API, including system, user, and assistant messages. It also sets parameters like temperature, max_tokens, top_p, presence_penalty, frequency_penalty, n, and seed for fine-grained control over the generated response.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=\"qwen/qwen2.5-0.5b-instruct\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a knowledgeable historian who provides concise responses.\",\n        },\n        {\"role\": \"user\", \"content\": \"Tell me about ancient Rome\"},\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Ancient Rome was a civilization centered in Italy.\",\n        },\n        {\"role\": \"user\", \"content\": \"What were their major achievements?\"},\n    ],\n    temperature=0.3,  # Lower temperature for more focused responses\n    max_tokens=128,  # Reasonable length for a concise response\n    top_p=0.95,  # Slightly higher for better fluency\n    presence_penalty=0.2,  # Mild penalty to avoid repetition\n    frequency_penalty=0.2,  # Mild penalty for more natural language\n    n=1,  # Single response is usually more stable\n    seed=42,  # Keep for reproducibility\n)\n\nprint_highlight(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with EAGLE-3 Speculative Decoding\nDESCRIPTION: This code snippet launches an SGLang server with EAGLE-3 speculative decoding enabled. It configures parameters such as the draft model path, number of steps, top-k branching factor, number of draft tokens, memory fraction, CUDA graph maximum batch size, and data type. The server uses Llama-3.1-8B-Instruct and jamesliu1/sglang-EAGLE3-Llama-3.1-Instruct-8B.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/speculative_decoding.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nserver_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct  --speculative-algorithm EAGLE3 \\\n    --speculative-draft-model-path jamesliu1/sglang-EAGLE3-Llama-3.1-Instruct-8B --speculative-num-steps 5 \\\n        --speculative-eagle-topk 8 --speculative-num-draft-tokens 32 --mem-fraction 0.6 \\\n        --cuda-graph-max-bs 2 --dtype float16\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: API Request with Structural Tagging\nDESCRIPTION: This example illustrates how to use structural tags to guide the language model's generation process. It defines structural tags with begin and end markers, along with associated schemas, and sends them in the `sampling_params` of a POST request to `/generate`. This approach allows for more structured and controlled output from the model. Dependencies include `transformers` and specifically the `AutoTokenizer` class.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\n\n# generate an answer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n\ntext = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\npayload = {\n    \"text\": text,\n    \"sampling_params\": {\n        \"structural_tag\": json.dumps(\n            {\n                \"type\": \"structural_tag\",\n                \"structures\": [\n                    {\n                        \"begin\": \"<function=get_current_weather>\",\n                        \"schema\": schema_get_current_weather,\n                        \"end\": \"</function>\",\n                    },\n                    {\n                        \"begin\": \"<function=get_current_date>\",\n                        \"schema\": schema_get_current_date,\n                        \"end\": \"</function>\",\n                    },\n                ],\n                \"triggers\": [\"<function=\"],\n            }\n        )\n    },\n}\n\n\n# Send POST request to the API endpoint\nresponse = requests.post(f\"http://localhost:{port}/generate\", json=payload)\nprint_highlight(response.json())\n```\n\n----------------------------------------\n\nTITLE: Install SGLang Router using pip\nDESCRIPTION: This command installs the sglang-router package using pip, the Python package installer. It is a prerequisite for using the SGLang Router.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install sglang-router\n```\n\n----------------------------------------\n\nTITLE: Generating Output Using EBNF Grammar with OpenAI API\nDESCRIPTION: This code defines an EBNF grammar to constrain the output of the OpenAI API. The grammar specifies the allowed structure for city descriptions, limiting the generated text to predefined cities, statuses, and countries. The response from the API, constrained by this grammar, is printed to the console. Dependencies include the openai library and a running SGLang server with the correct model. The 'extra_body' parameter is used to pass the EBNF grammar to the OpenAI API.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nebnf_grammar = \"\"\"\nroot ::= city | description\ncity ::= \\\"London\\\" | \\\"Paris\\\" | \\\"Berlin\\\" | \\\"Rome\\\"\ndescription ::= city \\\" is \\\" status\nstatus ::= \\\"the capital of \\\" country\ncountry ::= \\\"England\\\" | \\\"France\\\" | \\\"Germany\\\" | \\\"Italy\\\"\n\"\"\"\n\nresponse = client.chat.completions.create(\n    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful geography bot.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Give me the information of the capital of France.\",\n        },\n    ],\n    temperature=0,\n    max_tokens=2048,\n    extra_body={\"ebnf\": ebnf_grammar},\n)\n\nprint_highlight(\n    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Tool Calls (Non-Streaming)\nDESCRIPTION: Extracts the function name and arguments from a non-streaming response containing tool calls. It accesses the `tool_calls` attribute of the message within the response to retrieve the function's name and arguments. These are then printed for inspection.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nname_non_stream = response_non_stream.choices[0].message.tool_calls[0].function.name\narguments_non_stream = (\n    response_non_stream.choices[0].message.tool_calls[0].function.arguments\n)\n\nprint_highlight(f\"Final streamed function call name: {name_non_stream}\")\nprint_highlight(f\"Final streamed function call arguments: {arguments_non_stream}\")\n```\n\n----------------------------------------\n\nTITLE: Generating JSON Output Using Pydantic with OpenAI API\nDESCRIPTION: This code defines a Pydantic model (`CapitalInfo`) to represent a JSON schema for capital city information. It then uses the OpenAI API to generate content that conforms to this schema. The response is parsed as JSON and printed to the console. Dependencies include pydantic and openai libraries. The `CapitalInfo` model defines the expected fields (name and population) with appropriate types and descriptions, ensuring the output adheres to a predefined structure.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\n# Define the schema using Pydantic\nclass CapitalInfo(BaseModel):\n    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n    population: int = Field(..., description=\"Population of the capital city\")\n\n\nresponse = client.chat.completions.create(\n    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Please generate the information of the capital of France in the JSON format.\",\n        },\n    ],\n    temperature=0,\n    max_tokens=2048,\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"foo\",\n            # convert the pydantic model to json schema\n            \"schema\": CapitalInfo.model_json_schema(),\n        },\n    },\n)\n\nprint_highlight(\n    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Querying SGLang server using OpenAI Python Client\nDESCRIPTION: This code shows how to query the SGLang server using the OpenAI Python client. It initializes the OpenAI client with the SGLang server's base URL and then uses it to create a chat completion request with a message containing text and an image URL.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_vision.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n\nresponse = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"What is in this image?\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n                    },\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\nprint_highlight(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Launching server with JSON chat template file - Bash\nDESCRIPTION: This bash command launches the SGLang server and loads a custom chat template from a JSON file.  The `--chat-template` argument specifies the path to the JSON file (e.g., './my_model_template.json'). The server will parse the JSON file and use the defined chat template.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/custom_chat_template.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --chat-template ./my_model_template.json\n```\n\n----------------------------------------\n\nTITLE: Native API and SGLang Runtime (SRT) - Generate and Parse\nDESCRIPTION: Demonstrates using the Native API and SGLang Runtime (SRT) to generate text and parse function calls.  It prepares input using `tokenizer.apply_chat_template`, sends it to the `/generate` endpoint, and then uses the `/parse_function_call` endpoint to parse the generated text, which involves extracting the normal text, the function name, and its arguments.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\nimport requests\n\n# generate an answer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n\nmessages = get_messages()\n\ninput = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    tools=tools,\n)\n\ngen_url = f\"http://localhost:{port}/generate\"\ngen_data = {\n    \"text\": input,\n    \"sampling_params\": {\n        \"skip_special_tokens\": False,\n        \"max_new_tokens\": 1024,\n        \"temperature\": 0.1,\n        \"top_p\": 0.95,\n    },\n}\ngen_response = requests.post(gen_url, json=gen_data).json()[\"text\"]\nprint_highlight(\"==== Reponse ====\")\nprint(gen_response)\n\n# parse the response\nparse_url = f\"http://localhost:{port}/parse_function_call\"\n\nfunction_call_input = {\n    \"text\": gen_response,\n    \"tool_call_parser\": \"qwen25\",\n    \"tools\": tools,\n}\n\nfunction_call_response = requests.post(parse_url, json=function_call_input)\nfunction_call_response_json = function_call_response.json()\n\nprint_highlight(\"==== Text ====\")\nprint(function_call_response_json[\"normal_text\"])\nprint_highlight(\"==== Calls ====\")\nprint(\"function name: \", function_call_response_json[\"calls\"][0][\"name\"])\nprint(\"function arguments: \", function_call_response_json[\"calls\"][0][\"parameters\"])\n```\n\n----------------------------------------\n\nTITLE: Detailed Completions API Request in Python\nDESCRIPTION: This snippet illustrates a more detailed use case of the OpenAI Completions API with various parameters.  It uses parameters like temperature, max_tokens, top_p, stop, presence_penalty, frequency_penalty, n, and seed to control the output generation behavior and includes prompt for story writing.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.completions.create(\n    model=\"qwen/qwen2.5-0.5b-instruct\",\n    prompt=\"Write a short story about a space explorer.\",\n    temperature=0.7,  # Moderate temperature for creative writing\n    max_tokens=150,  # Longer response for a story\n    top_p=0.9,  # Balanced diversity in word choice\n    stop=[\"\\n\\n\", \"THE END\"],  # Multiple stop sequences\n    presence_penalty=0.3,  # Encourage novel elements\n    frequency_penalty=0.3,  # Reduce repetitive phrases\n    n=1,  # Generate one completion\n    seed=123,  # For reproducible results\n)\n\nprint_highlight(f\"Response: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Launching and Shutting Down SGLang Engine in Python\nDESCRIPTION: This Python code snippet launches an SGLang engine with a specified model path and then shuts it down.  It includes CI-specific logic to optimize memory usage.  It relies on the `sglang` library and `sglang.test.test_utils` for CI detection.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Launch Engine\nimport sglang as sgl\nimport asyncio\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    import patch\n\nllm = sgl.Engine(model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n\n# Terminalte Engine\nllm.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Sending requests to the router (Python)\nDESCRIPTION: This Python script demonstrates how to send a request to the SGLang Router using the `requests` library. It sends a POST request to the `/generate` endpoint with a JSON payload containing the input text. The script then prints the JSON response received from the router.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"http://localhost:30000/generate\"\ndata = {\"text\": \"What is the capital of France?\"}\n\nresponse = requests.post(url, json=data)\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Enable torch.compile and set cache directory in SGLang (Bash)\nDESCRIPTION: This example demonstrates how to enable `torch.compile` in SGLang and set a custom cache directory using the `TORCHINDUCTOR_CACHE_DIR` environment variable.  It accelerates model compilation and allows for sharing the cache across machines. The command launches an SGLang server with a specified model and enables `torch.compile`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/hyperparameter_tuning.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_CACHE_DIR=/root/inductor_root_cache python3 -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct --enable-torch-compile\n```\n\n----------------------------------------\n\nTITLE: Streaming Request\nDESCRIPTION: Sends a streaming request to the SGLang server. This example retrieves the response in chunks and aggregates the text content and tool calls. Iterates over the response chunks, appending the content and tool calls to respective lists. Finally, print the aggregated text and tool call details.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Streaming mode test\nprint_highlight(\"Streaming response:\")\nresponse_stream = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    temperature=0.1,\n    top_p=0.95,\n    max_tokens=1024,\n    stream=True,  # Enable streaming\n    tools=tools,\n)\n\ntexts = \"\"\ntool_calls = []\nname = \"\"\narguments = \"\"\nfor chunk in response_stream:\n    if chunk.choices[0].delta.content:\n        texts += chunk.choices[0].delta.content\n    if chunk.choices[0].delta.tool_calls:\n        tool_calls.append(chunk.choices[0].delta.tool_calls[0])\nprint_highlight(\"==== Text ====\")\nprint(texts)\n\nprint_highlight(\"==== Tool Call ====\")\nfor tool_call in tool_calls:\n    print(tool_call)\n```\n\n----------------------------------------\n\nTITLE: Generating JSON Output with SGLang (Outlines/XGrammar)\nDESCRIPTION: This code snippet demonstrates how to use SGLang to generate JSON output based on a predefined JSON schema. It sends a POST request to the SGLang server with the input text and the JSON schema defined within the sampling parameters.  The server then uses the schema to constrain the output.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/sampling_params.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.post(\n    \"http://localhost:30000/generate\",\n    json={\n        \"text\": \"Here is the information of the capital of France in the JSON format.\\n\",\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 64,\n            \"json_schema\": json_schema,\n        },\n    },\n)\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Generating JSON with Pydantic Schema\nDESCRIPTION: This snippet demonstrates generating JSON using a Pydantic model to define the schema. It defines a `CapitalInfo` model with `name` and `population` fields and their constraints, generates text using the SGLang engine, and prints the generated output for each prompt.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom pydantic import BaseModel, Field\n\n\nprompts = [\n    \"Give me the information of the capital of China in the JSON format.\",\n    \"Give me the information of the capital of France in the JSON format.\",\n    \"Give me the information of the capital of Ireland in the JSON format.\",\n]\n\n\n# Define the schema using Pydantic\nclass CapitalInfo(BaseModel):\n    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n    population: int = Field(..., description=\"Population of the capital city\")\n\n\nsampling_params = {\n    \"temperature\": 0,\n    \"top_p\": 0.95,\n    \"max_new_tokens\": 2048,\n    \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print(\"===============================\")\n    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Sending OpenAI API Requests to SGLang\nDESCRIPTION: This Python code demonstrates how to send requests to an SGLang server using the OpenAI API client.  It sets up a client connection to the server's base URL (defaulting to http://127.0.0.1:30000/v1) and sends a chat completion request.  The `openai` package is required.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nclient = openai.Client(\n    base_url=\"http://127.0.0.1:30000/v1\", api_key=\"EMPTY\")\n\n# Chat completion\nresponse = client.chat.completions.create(\n    model=\"default\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n    ],\n    temperature=0,\n    max_tokens=64,\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Shutting Down SGLang Engine\nDESCRIPTION: This snippet shuts down the SGLang engine to release resources.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nllm.shutdown()\n```\n\n----------------------------------------\n\nTITLE: New Model Detector Class Definition - Python\nDESCRIPTION: A template for creating a new detector class that inherits from `BaseFormatDetector` to handle the model’s specific function call format. The detector needs to be implemented with the specific format handling logic.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass NewModelDetector(BaseFormatDetector):\n```\n\n----------------------------------------\n\nTITLE: Sending Results Back to Model\nDESCRIPTION: Sends the result of the tool call back to the model to generate a final response. The model is expected to use the tool's output to refine its answer. The code creates a new chat completion request that includes the updated message history with the tool's result. Finally, prints the final response content.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfinal_response = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    temperature=0.1,\n    top_p=0.95,\n    stream=False,\n    tools=tools,\n)\nprint_highlight(\"Non-stream response:\")\nprint(final_response)\n\nprint_highlight(\"==== Text ====\")\nprint(final_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Sending Chat Completion Request using Python Requests\nDESCRIPTION: Sends a chat completion request to the SGLang server using the `requests` library. The code defines the API endpoint URL and constructs a JSON payload containing the model and message.  It then sends a POST request with the JSON data and prints the JSON response.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/send_request.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = f\"http://localhost:{port}/v1/chat/completions\"\n\ndata = {\n    \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n}\n\nresponse = requests.post(url, json=data)\nprint_highlight(response.json())\n```\n\n----------------------------------------\n\nTITLE: Handling Tool Calls (Streaming)\nDESCRIPTION: Parses and combines function call arguments from a streaming response containing tool calls.  The code iterates through the `tool_calls` list, extracting function names and arguments from each tool call, then concatenates argument fragments to form a complete JSON string, finally printing the aggregated arguments.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Parse and combine function call arguments\narguments = []\nfor tool_call in tool_calls:\n    if tool_call.function.name:\n        print_highlight(f\"Streamed function call name: {tool_call.function.name}\")\n\n    if tool_call.function.arguments:\n        arguments.append(tool_call.function.arguments)\n\n# Combine all fragments into a single JSON string\nfull_arguments = \"\".join(arguments)\nprint_highlight(f\"streamed function call arguments: {full_arguments}\")\n```\n\n----------------------------------------\n\nTITLE: Skipping Tokenizer Initialization\nDESCRIPTION: Demonstrates skipping the tokenizer initialization using the `--skip-tokenizer-init` flag when launching the server. In this case, the input and output are tokens rather than text.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntokenizer_free_server_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --skip-tokenizer-init\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"qwen/qwen2.5-0.5b-instruct\")\n\ninput_text = \"What is the capital of France?\"\n\ninput_tokens = tokenizer.encode(input_text)\nprint_highlight(f\"Input Text: {input_text}\")\nprint_highlight(f\"Tokenized Input: {input_tokens}\")\n\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"input_ids\": input_tokens,\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 256,\n            \"stop_token_ids\": [tokenizer.eos_token_id],\n        },\n        \"stream\": False,\n    },\n)\noutput = response.json()\noutput_tokens = output[\"output_ids\"]\n\noutput_text = tokenizer.decode(output_tokens, skip_special_tokens=False)\nprint_highlight(f\"Tokenized Output: {output_tokens}\")\nprint_highlight(f\"Decoded Output: {output_text}\")\nprint_highlight(f\"Output Text: {output['meta_info']['finish_reason']}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Text with /generate API\nDESCRIPTION: Demonstrates how to use the `/generate` API to generate completions from a text generation model.  It sends a POST request to the server with the input text and prints the response. The server URL is constructed using the port number obtained when launching the server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nurl = f\"http://localhost:{port}/generate\"\ndata = {\"text\": \"What is the capital of France?\"}\n\nresponse = requests.post(url, json=data)\nprint_highlight(response.json())\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: Launches the SGLang server with a specified model.  It utilizes `launch_server_cmd` to start the server process and then waits until the server is ready to accept requests. The `is_in_ci` check determines whether to use a patched version for Continuous Integration environments or the standard `sglang.utils` module.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/send_request.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sglang.test.test_utils import is_in_ci\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\n# This is equivalent to running the following command in your terminal\n\n# python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0\n\nserver_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct \\\n --host 0.0.0.0\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Send request to SGLang server with multiple LoRAs\nDESCRIPTION: This snippet sends a request to the SGLang server to generate text using multiple specified LoRA adapters. It constructs a JSON payload with input text, sampling parameters, and the LoRA path for each input. The response is then parsed to extract and print the generated text. It requires the `requests` library.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/lora.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nurl = f\"http://127.0.0.1:{port}\"\njson_data = {\n    \"text\": [\n        \"List 3 countries and their capitals.\",\n        \"AI is a field of computer science focused on\",\n    ],\n    \"sampling_params\": {\"max_new_tokens\": 32, \"temperature\": 0},\n    # The first input uses lora0, and the second input uses lora1\n    \"lora_path\": [\"lora0\", \"lora1\"],\n}\nresponse = requests.post(\n    url + \"/generate\",\n    json=json_data,\n)\nprint(f\"Output 0: {response.json()[0]['text']}\")\nprint(f\"Output 1: {response.json()[1]['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Submitting SGLang Server Job on SLURM\nDESCRIPTION: This code presents a SLURM job submission script for serving SGLang across multiple nodes. It includes configurations for resource allocation like number of nodes, tasks, CPUs per task, memory, and GPU requirements. It also defines environment activation, model parameters, NCCL initialization, and launches the SGLang server on each node, waiting for NCCL server readiness.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/multi_node.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash -l\n\n#SBATCH -o SLURM_Logs/%x_%j_master.out\n#SBATCH -e SLURM_Logs/%x_%j_master.err\n#SBATCH -D ./\n#SBATCH -J Llama-405B-Online-Inference-TP16-SGL\n\n#SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --ntasks-per-node=1  # Ensure 1 task per node\n#SBATCH --cpus-per-task=18\n#SBATCH --mem=224GB\n#SBATCH --partition=\"lmsys.org\"\n#SBATCH --gres=gpu:8\n#SBATCH --time=12:00:00\n\necho \"[INFO] Activating environment on node $SLURM_PROCID\"\nif ! source ENV_FOLDER/bin/activate; then\n    echo \"[ERROR] Failed to activate environment\" >&2\n    exit 1\nfi\n\n# Define parameters\nmodel=MODEL_PATH\ntp_size=16\n\necho \"[INFO] Running inference\"\necho \"[INFO] Model: $model\"\necho \"[INFO] TP Size: $tp_size\"\n\n# Set NCCL initialization address using the hostname of the head node\nHEAD_NODE=$(scontrol show hostname \"$SLURM_NODELIST\" | head -n 1)\nNCCL_INIT_ADDR=\"${HEAD_NODE}:8000\"\necho \"[INFO] NCCL_INIT_ADDR: $NCCL_INIT_ADDR\"\n\n# Launch the model server on each node using SLURM\nsrun --ntasks=2 --nodes=2 --output=\"SLURM_Logs/%x_%j_node$SLURM_NODEID.out\" \\\n    --error=\"SLURM_Logs/%x_%j_node$SLURM_NODEID.err\" \\\n    python3 -m sglang.launch_server \\\n    --model-path \"$model\" \\\n    --grammar-backend \"xgrammar\" \\\n    --tp \"$tp_size\" \\\n    --dist-init-addr \"$NCCL_INIT_ADDR\" \\\n    --nnodes 2 \\\n    --node-rank \"$SLURM_NODEID\" &\n\n# Wait for the NCCL server to be ready on port 30000\nwhile ! nc -z \"$HEAD_NODE\" 30000; do\n    sleep 1\n    echo \"[INFO] Waiting for $HEAD_NODE:30000 to accept connections\"\ndone\n\necho \"[INFO] $HEAD_NODE:30000 is ready to accept connections\"\n\n# Keep the script running until the SLURM job times out\nwait\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM v0.6.0\nDESCRIPTION: Installs vLLM version 0.6.0. This command ensures the correct version of vLLM is used for the benchmark comparison.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/benchmark_vllm_060/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# install vllm v0.6.0\npip install vllm==0.6.0\n```\n\n----------------------------------------\n\nTITLE: Defining Tools for Function Call\nDESCRIPTION: Defines a tool as a Python dictionary for use with function calling. The dictionary includes a tool name, a description, and a `parameters` field to define the expected arguments.  The `type` specifies \"function\", while `function` contains the tool details.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define tools\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n                    },\n                    \"state\": {\n                        \"type\": \"string\",\n                        \"description\": \"the two-letter abbreviation for the state that the city is\"\n                        \" in, e.g. 'CA' which would mean 'California'\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unit to fetch the temperature in\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                    },\n                },\n                \"required\": [\"city\", \"state\", \"unit\"],\n            },\n        },\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server in Python\nDESCRIPTION: This code snippet demonstrates how to launch the SGLang server programmatically using Python. It imports necessary modules, defines a command to start the server with a specified model path and host, waits for the server to initialize, and then prints the server address.  It uses conditional import based on whether it is running in a CI environment or not.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\n\nserver_process, port = launch_server_cmd(\n    \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0 --mem-fraction-static 0.8\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\nprint(f\"Server started on http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Offline Engine API Usage in Python\nDESCRIPTION: This snippet demonstrates how to use the SGLang Offline Engine API for text generation and reasoning parsing. It initializes the engine, tokenizes the input, generates text, and then uses the `ReasoningParser` to separate the reasoning content from the final answer. It assumes there's only one prompt.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport sglang as sgl\nfrom sglang.srt.reasoning_parser import ReasoningParser\nfrom sglang.utils import print_highlight\n\nllm = sgl.Engine(model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\ninput = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nsampling_params = {\n    \"max_new_tokens\": 1024,\n    \"skip_special_tokens\": False,\n    \"temperature\": 0.6,\n    \"top_p\": 0.95,\n}\nresult = llm.generate(prompt=input, sampling_params=sampling_params)\n\ngenerated_text = result[\"text\"]  # Assume there is only one prompt\n\nprint_highlight(\"==== Original Output ====\")\nprint_highlight(generated_text)\n\nparser = ReasoningParser(\"deepseek-r1\")\nreasoning_text, text = parser.parse_non_stream(generated_text)\nprint_highlight(\"==== Reasoning ====\")\nprint_highlight(reasoning_text)\nprint_highlight(\"==== Text ====\")\nprint_highlight(text)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with EBNF Grammar\nDESCRIPTION: This snippet defines an EBNF grammar to constrain the generated text.  It sends a request to the `/generate` endpoint, including the EBNF grammar in the `sampling_params`. The grammar defines a simple structure for describing cities and their countries.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": \"Give me the information of the capital of France.\",\n        \"sampling_params\": {\n            \"max_new_tokens\": 2048,\n            \"temperature\": 0,\n            \"n\": 3,\n            \"ebnf\": (\n                \"root ::= city | description\\n\"\n                'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n                'description ::= city \" is \" status\\n'\n                'status ::= \"the capital of \" country\\n'\n                'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n            ),\n        },\n        \"stream\": False,\n        \"return_logprob\": False,\n    },\n)\n\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Getting Model Info with /get_model_info API\nDESCRIPTION: Retrieves model information using the `/get_model_info` API. The API returns the model path, whether it's a generation model, and the tokenizer path. The example asserts the values of these keys to ensure they match the expected configuration.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nurl = f\"http://localhost:{port}/get_model_info\"\n\nresponse = requests.get(url)\nresponse_json = response.json()\nprint_highlight(response_json)\nassert response_json[\"model_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\nassert response_json[\"is_generation\"] is True\nassert response_json[\"tokenizer_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\nassert response_json.keys() == {\"model_path\", \"is_generation\", \"tokenizer_path\"}\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema Directly\nDESCRIPTION: This snippet defines a JSON schema for an object with `name` (string with alphanumeric characters only) and `population` (integer) properties.  It then makes a POST request to the `/generate` endpoint using the `json_schema` to guide generation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\njson_schema = json.dumps(\n    {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n            \"population\": {\"type\": \"integer\"},\n        },\n        \"required\": [\"name\", \"population\"],\n    }\n)\n\n# JSON\ntext = tokenizer.apply_chat_template(text, tokenize=False, add_generation_prompt=True)\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": text,\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 2048,\n            \"json_schema\": json_schema,\n        },\n    },\n)\n\nprint_highlight(response.json())\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings using Python Requests\nDESCRIPTION: Generates text embeddings using the Python `requests` library to interact with the SGLang server. It posts a JSON payload containing the model and input text to the embedding endpoint. Requires the `requests` module.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_embeddings.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\ntext = \"Once upon a time\"\n\nresponse = requests.post(\n    f\"http://localhost:{port}/v1/embeddings\",\n    json={\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"input\": text},\n)\n\ntext_embedding = response.json()[\"data\"][0][\"embedding\"]\n\nprint_highlight(f\"Text embedding (first 10): {text_embedding[:10]}\")\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Channel-INT8 Quantization - Bash\nDESCRIPTION: Launches the SGLang server with per-channel INT8 quantization for the DeepSeek-R1 model. This command specifies the model, tensor parallelism (TP), quantization method, distributed initialization address, number of nodes, node rank, and enables features like trusting remote code and torch compile. `MASTER_IP` must be properly configured.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n#master\npython3 -m sglang.launch_server --model meituan/DeepSeek-R1-Channel-INT8 --tp 32 --quantization w8a8_int8 \\\n\t--dist-init-addr MASTER_IP:5000 --nnodes 4 --node-rank 0 --trust-remote \\\n\t--enable-torch-compile --torch-compile-max-bs 32\n#cluster\npython3 -m sglang.launch_server --model meituan/DeepSeek-R1-Channel-INT8 --tp 32 --quantization w8a8_int8 \\\n\t--dist-init-addr MASTER_IP:5000 --nnodes 4 --node-rank 1 --trust-remote \\\n\t--enable-torch-compile --torch-compile-max-bs 32\npython3 -m sglang.launch_server --model meituan/DeepSeek-R1-Channel-INT8 --tp 32 --quantization w8a8_int8 \\\n\t--dist-init-addr MASTER_IP:5000 --nnodes 4 --node-rank 2 --trust-remote \\\n\t--enable-torch-compile --torch-compile-max-bs 32\npython3 -m sglang.launch_server --model meituan/DeepSeek-R1-Channel-INT8 --tp 32 --quantization w8a8_int8 \\\n\t--dist-init-addr MASTER_IP:5000 --nnodes 4 --node-rank 3 --trust-remote \\\n\t--enable-torch-compile --torch-compile-max-bs 32\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Structural Tags\nDESCRIPTION: This snippet demonstrates generating text with structural tags. It applies a chat template to the input `messages`, then sends a POST request to `/generate` with a `structural_tag` parameter in the `sampling_params`. The `structural_tag` defines begin and end tags, schemas, and triggers for specific functions, allowing the model to generate text with a defined structure.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntext = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\npayload = {\n    \"text\": text,\n    \"sampling_params\": {\n        \"max_new_tokens\": 2048,\n        \"structural_tag\": json.dumps(\n            {\n                \"type\": \"structural_tag\",\n                \"structures\": [\n                    {\n                        \"begin\": \"<function=get_current_weather>\",\n                        \"schema\": schema_get_current_weather,\n                        \"end\": \"</function>\",\n                    },\n                    {\n                        \"begin\": \"<function=get_current_date>\",\n                        \"schema\": schema_get_current_date,\n                        \"end\": \"</function>\",\n                    },\n                ],\n                \"triggers\": [\"<function=\"],\n            }\n        ),\n    },\n}\n\n\n# Send POST request to the API endpoint\nresponse = requests.post(f\"http://localhost:{port}/generate\", json=payload)\nprint_highlight(response.json())\n```\n\n----------------------------------------\n\nTITLE: Streaming Native Generation Response using Python Requests\nDESCRIPTION: Streams responses from the `/generate` endpoint using the `requests` library. Sets `stream=True` in the request. It iterates through the response chunks, decodes them, and extracts the text data. The extracted text is then printed incrementally.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/send_request.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport requests, json\n\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": \"The capital of France is\",\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 32,\n        },\n        \"stream\": True,\n    },\n    stream=True,\n)\n\nprev = 0\nfor chunk in response.iter_lines(decode_unicode=False):\n    chunk = chunk.decode(\"utf-8\")\n    if chunk and chunk.startswith(\"data:\"):\n        if chunk == \"data: [DONE]\":\n            break\n        data = json.loads(chunk[5:].strip(\"\\n\"))\n        output = data[\"text\"]\n        print(output[prev:], end=\"\", flush=True)\n        prev = len(output)\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server (Python)\nDESCRIPTION: Launches the SGLang server with specific configurations optimized for Jetson Orin.  Key parameters include `--model-path` for the model, `--device cuda` for GPU usage, `--dtype half` for half-precision, `--attention-backend flashinfer` for attention optimization, `--mem-fraction-static` for memory allocation, and `--context-length` to specify the context length.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/nvidia_jetson.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython -m sglang.launch_server \\\n  --model-path deepseek-ai/DeepSeek-R1-Distill-Llama-8B \\\n  --device cuda \\\n  --dtype half \\\n  --attention-backend flashinfer \\\n  --mem-fraction-static 0.8 \\\n  --context-length 8192\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: This command starts the SGLang server using a specified model path and port. It uses the meta-llama/Llama-2-7b-chat-hf model and runs the server on port 30000. This is a prerequisite for benchmarking SGLang.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hellaswag/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings using OpenAI Python Client\nDESCRIPTION: Generates text embeddings using the OpenAI Python client to interact with the SGLang server. It creates an OpenAI client with the SGLang server's base URL and uses the `embeddings.create` method to generate the embedding. Requires the `openai` module.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_embeddings.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nclient = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n\n# Text embedding example\nresponse = client.embeddings.create(\n    model=\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\",\n    input=text,\n)\n\nembedding = response.data[0].embedding[:10]\nprint_highlight(f\"Text embedding (first 10): {embedding}\")\n```\n\n----------------------------------------\n\nTITLE: Generating JSON Output Using Direct JSON Schema with OpenAI API\nDESCRIPTION: This code defines a JSON schema as a string and then uses it to generate structured output using the OpenAI API. The JSON schema specifies the expected properties (name and population) and their types. The response from the OpenAI API is then printed to the console. This allows defining and enforcing a strict structure for the output, ensuring that it conforms to the specified JSON schema.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\njson_schema = json.dumps(\n    {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n            \"population\": {\"type\": \"integer\"},\n        },\n        \"required\": [\"name\", \"population\"],\n    }\n)\n\nresponse = client.chat.completions.create(\n    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Give me the information of the capital of France in the JSON format.\",\n        },\n    ],\n    temperature=0,\n    max_tokens=2048,\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": {\"name\": \"foo\", \"schema\": json.loads(json_schema)},\n    },\n)\n\nprint_highlight(\n    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI-like Client\nDESCRIPTION: Initializes an OpenAI-like client to interact with the SGLang server. It uses a dummy API key and the server's base URL.  It retrieves the model name from the server's model list.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize OpenAI-like client\nclient = OpenAI(api_key=\"None\", base_url=f\"http://0.0.0.0:{port}/v1\")\nmodel_name = client.models.list().data[0].id\n```\n\n----------------------------------------\n\nTITLE: JSON Output Using Pydantic with SGLang Runtime (SRT)\nDESCRIPTION: This code defines a Pydantic model (`CapitalInfo`) to represent a JSON schema for capital city information. It uses the `transformers` library to apply a chat template to the message. This example demonstrates how to prepare input for the SGLang runtime, defining a JSON schema using pydantic.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom pydantic import BaseModel, Field\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n\n\n# Define the schema using Pydantic\nclass CapitalInfo(BaseModel):\n    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n    population: int = Field(..., description=\"Population of the capital city\")\n\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Here is the information of the capital of France in the JSON format.\\n\",\n    }\n]\ntext = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\n```\n\n----------------------------------------\n\nTITLE: OpenAI Completions API Usage in Python\nDESCRIPTION: This code snippet shows a basic example of using the OpenAI Completions API. It creates a completion based on a prompt, specifying the model, prompt, temperature, and max_tokens parameters, and prints the response. It also includes n and stop parameters.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.completions.create(\n    model=\"qwen/qwen2.5-0.5b-instruct\",\n    prompt=\"List 3 countries and their capitals.\",\n    temperature=0,\n    max_tokens=64,\n    n=1,\n    stop=None,\n)\n\nprint_highlight(f\"Response: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings using cURL\nDESCRIPTION: Generates text embeddings using a cURL command to interact with the SGLang server. It constructs a cURL command with the model and input text, then parses the JSON response to extract the embedding. Requires `subprocess` and `json` modules.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport subprocess, json\n\ntext = \"Once upon a time\"\n\ncurl_text = f\"\"\"curl -s http://localhost:{port}/v1/embeddings \\\n  -d '{{\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"input\": \"{text}\"}}'\"\"\"\n\ntext_embedding = json.loads(subprocess.check_output(curl_text, shell=True))[\"data\"][0][\n    \"embedding\"\n]\n\nprint_highlight(f\"Text embedding (first 10): {text_embedding[:10]}\")\n```\n\n----------------------------------------\n\nTITLE: Quantize Model with LLM Compressor to FP8\nDESCRIPTION: Demonstrates offline quantization of a model to FP8 using the `llmcompressor` library. It loads the original model and tokenizer, configures the quantization recipe, applies the quantization, and saves the quantized model and tokenizer. Requires the `transformers` and `llmcompressor` libraries.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\nfrom llmcompressor.transformers import SparseAutoModelForCausalLM\nfrom llmcompressor.transformers import oneshot\nfrom llmcompressor.modifiers.quantization import QuantizationModifier\n\n# Step 1: Load the original model.\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nmodel = SparseAutoModelForCausalLM.from_pretrained(\n  MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Step 2: Perform offline quantization.\n# Step 2.1: Configure the simple PTQ quantization.\nrecipe = QuantizationModifier(\n  targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"])\n\n# Step 2.2: Apply the quantization algorithm.\noneshot(model=model, recipe=recipe)\n\n# Step 3: Save the model.\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-FP8-Dynamic\"\nmodel.save_pretrained(SAVE_DIR)\ntokenizer.save_pretrained(SAVE_DIR)\n```\n\n----------------------------------------\n\nTITLE: OpenAI Chat Completions API Usage in Python\nDESCRIPTION: This code snippet shows how to use the OpenAI Chat Completions API with SGLang. It initializes an OpenAI client, makes a chat completion request with a system and user message, and prints the response. It specifies the model, messages, temperature, and max_tokens parameters.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nclient = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n\nresponse = client.chat.completions.create(\n    model=\"qwen/qwen2.5-0.5b-instruct\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n    ],\n    temperature=0,\n    max_tokens=64,\n)\n\nprint_highlight(f\"Response: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings from Input IDs with cURL\nDESCRIPTION: Generates text embeddings from input IDs using a cURL command to interact with the SGLang server. It first tokenizes the input text using the `transformers` library, then constructs a cURL command with the model and input IDs. Requires `json`, `os`, and `transformers` modules.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_embeddings.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom transformers import AutoTokenizer\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\")\ninput_ids = tokenizer.encode(text)\n\ncurl_ids = f\"\"\"curl -s http://localhost:{port}/v1/embeddings \\\n  -d '{{\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"input\": {json.dumps(input_ids)}}}'\"\"\"\n\ninput_ids_embedding = json.loads(subprocess.check_output(curl_ids, shell=True))[\"data\"][\n    0\n][\"embedding\"]\n\nprint_highlight(f\"Input IDs embedding (first 10): {input_ids_embedding[:10]}\")\n```\n\n----------------------------------------\n\nTITLE: Streaming Synchronous Generation with SGLang\nDESCRIPTION: This code demonstrates streaming synchronous generation using the SGLang engine. It defines prompts and sampling parameters, and then iterates through each prompt, using the `stream_and_merge` utility to generate and print the output in a streaming fashion, removing potential overlap in the generated text. This is useful for displaying real-time output as it's generated.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/offline_engine_api.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n]\n\nsampling_params = {\n    \"temperature\": 0.2,\n    \"top_p\": 0.9,\n}\n\nprint(\"\\n=== Testing synchronous streaming generation with overlap removal ===\\n\")\n\nfor prompt in prompts:\n    print(f\"Prompt: {prompt}\")\n    merged_output = stream_and_merge(llm, prompt, sampling_params)\n    print(\"Generated text:\", merged_output)\n    print()\n\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Completion Response using OpenAI Client\nDESCRIPTION: Demonstrates streaming responses from the SGLang server using the OpenAI Python client.  The `stream=True` parameter enables streaming.  The response is then iterated over, printing the content from each chunk as it becomes available.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/send_request.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nclient = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n\n# Use stream=True for streaming responses\nresponse = client.chat.completions.create(\n    model=\"qwen/qwen2.5-0.5b-instruct\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n    ],\n    temperature=0,\n    max_tokens=64,\n    stream=True,\n)\n\n# Handle the streaming output\nfor chunk in response:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Sample Function Calling Response (JSON)\nDESCRIPTION: This JSON snippet represents the expected response from the SGLang server for the function calling request. It contains the assistant's message, tool call information (name and arguments), and usage statistics.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deepseek.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"id\": \"62af80528930423a82c806651ec66e7c\", \"object\": \"chat.completion\", \"created\": 1744431333, \"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": null, \"reasoning_content\": null, \"tool_calls\": [{\"id\": \"0\", \"type\": \"function\", \"function\": {\"name\": \"query_weather\", \"arguments\": \"{\\\\\\\"city\\\\\\\": \\\\\\\"Guangzhou\\\\\\\"}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\", \"matched_stop\": null}], \"usage\": {\"prompt_tokens\": 118, \"total_tokens\": 140, \"completion_tokens\": 22, \"prompt_tokens_details\": null}}\n```\n\n----------------------------------------\n\nTITLE: Checking OFED Driver Version with ofed_info\nDESCRIPTION: This command uses `ofed_info` to display the installed OFED (OpenFabrics Enterprise Distribution) driver version.  The `-s` flag provides summary information. Proper OFED driver installation is necessary for RDMA functionality.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nofed_info -s\nOFED-internal-23.07-0.5.0:\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server\nDESCRIPTION: This command starts the SGLang server, loading the Llama-3.1-8B-Instruct model from the specified path and listening on port 30000.  It uses the `sglang.launch_server` module and requires the model path to be specified.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_schema/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --port 30000\n```\n\n----------------------------------------\n\nTITLE: Sending Native Generation Request using Python Requests\nDESCRIPTION: Sends a native generation request to the `/generate` endpoint using the `requests` library. The request includes the input text and sampling parameters such as temperature and maximum new tokens.  The JSON response is printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/send_request.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": \"The capital of France is\",\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 32,\n        },\n    },\n)\n\nprint_highlight(response.json())\n```\n\n----------------------------------------\n\nTITLE: Launching server with predefined chat template - Bash\nDESCRIPTION: This bash command launches the SGLang server with a specific model and chat template. It uses the `--chat-template` argument to specify a pre-defined chat template (e.g., 'llama-2'). The command sets the model path and port number. The template provided overrides the default template from the Hugging Face model tokenizer.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/custom_chat_template.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --chat-template llama-2\n```\n\n----------------------------------------\n\nTITLE: Generating Output Using Regular Expression with OpenAI API\nDESCRIPTION: This code uses a regular expression to constrain the output of the OpenAI API.  The regular expression `(Paris|London)` limits the generated text to either \"Paris\" or \"London\". The response from the API is then printed to the console. This ensures that the output conforms to the specified pattern, providing a simple way to enforce specific keywords or phrases.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    ],\n    temperature=0,\n    max_tokens=2048,\n    extra_body={\"regex\": \"(Paris|London)\"},\n)\n\nprint_highlight(\n    f\"reasoing_content: {response.choices[0].message.reasoning_content}\\n\\ncontent: {response.choices[0].message.content}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Sending Chat Completion Request using OpenAI Python Client\nDESCRIPTION: Sends a chat completion request to the SGLang server using the OpenAI Python client.  It initializes the client with the server's base URL and a dummy API key.  Then, it creates a chat completion with a specified model, message, temperature, and max tokens.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/send_request.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nclient = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n\nresponse = client.chat.completions.create(\n    model=\"qwen/qwen2.5-0.5b-instruct\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n    ],\n    temperature=0,\n    max_tokens=64,\n)\nprint_highlight(response)\n```\n\n----------------------------------------\n\nTITLE: Quantize Model with GPTQModel\nDESCRIPTION: Demonstrates how to quantize a model using the `GPTQModel` library.  It loads a dataset, configures quantization settings, loads the model, performs quantization, and saves the quantized model. Requires the `datasets` and `gptqmodel` libraries.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom gptqmodel import GPTQModel, QuantizeConfig\n\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"\nquant_path = \"Llama-3.2-1B-Instruct-gptqmodel-4bit\"\n\ncalibration_dataset = load_dataset(\n    \"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\",\n    split=\"train\"\n  ).select(range(1024))[\"text\"]\n\nquant_config = QuantizeConfig(bits=4, group_size=128) # quantization config\nmodel = GPTQModel.load(model_id, quant_config) # load model\n\nmodel.quantize(calibration_dataset, batch_size=2) # quantize\nmodel.save(quant_path) # save model\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Tensor Parallelism (TP)\nDESCRIPTION: This command launches the SGLang server with tensor parallelism enabled. The `--tp 2` argument specifies that the model weights will be sharded across two GPUs. The `--enable-p2p-check` argument is added to handle potential peer access issues between devices.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/server_arguments.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --tp 2\n```\n\n----------------------------------------\n\nTITLE: Send request to SGLang server with single LoRA\nDESCRIPTION: This snippet sends a request to the SGLang server to generate text using the specified LoRA adapter and the base model. It constructs a JSON payload with input text, sampling parameters, and the LoRA path for each input. The response is then parsed to extract and print the generated text. It requires the `requests` library.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/lora.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nurl = f\"http://127.0.0.1:{port}\"\njson_data = {\n    \"text\": [\n        \"List 3 countries and their capitals.\",\n        \"AI is a field of computer science focused on\",\n    ],\n    \"sampling_params\": {\"max_new_tokens\": 32, \"temperature\": 0},\n    # The first input uses lora0, and the second input uses the base model\n    \"lora_path\": [\"lora0\", None],\n}\nresponse = requests.post(\n    url + \"/generate\",\n    json=json_data,\n)\nprint(f\"Output 0: {response.json()[0]['text']}\")\nprint(f\"Output 1: {response.json()[1]['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Streaming Asynchronous Generation with SGLang\nDESCRIPTION: This example demonstrates streaming asynchronous text generation. It uses an asynchronous `main` function to iterate through prompts and generate text streams using the `async_stream_and_merge` function, which removes potential overlap in the generated text. The generated chunks are printed to the console as they become available, providing a real-time output. It uses `asyncio.run` to execute the asynchronous main function.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/offline_engine_api.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n]\n\nsampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n\nprint(\"\\n=== Testing asynchronous streaming generation (no repeats) ===\")\n\n\nasync def main():\n    for prompt in prompts:\n        print(f\"\\nPrompt: {prompt}\")\n        print(\"Generated text: \", end=\"\", flush=True)\n\n        # Replace direct calls to async_generate with our custom overlap-aware version\n        async for cleaned_chunk in async_stream_and_merge(llm, prompt, sampling_params):\n            print(cleaned_chunk, end=\"\", flush=True)\n\n        print()  # New line after each prompt\n\n\nasyncio.run(main())\n\n```\n\n----------------------------------------\n\nTITLE: Running SGLang Online Benchmark\nDESCRIPTION: These commands run the SGLang online benchmark using the `sglang.bench_serving` module. The parameters include setting the backend to 'sglang', dataset name to 'random', random input/output sizes, number of prompts, request rate, and output file name. The script then extracts a specific column from the generated JSONL file using `cat` and `cut`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 300 --request-rate 1 --output-file online.jsonl\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 600 --request-rate 2 --output-file online.jsonl\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 1200 --request-rate 4 --output-file online.jsonl\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 2400 --request-rate 8 --output-file online.jsonl\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 3200 --request-rate 16 --output-file online.jsonl\ncat online.jsonl | cut -d':' -f9 | cut -d',' -f1\n```\n\n----------------------------------------\n\nTITLE: Health Check with /health and /health_generate APIs\nDESCRIPTION: Performs health checks on the server using the `/health` and `/health_generate` APIs. `/health` checks the general health, while `/health_generate` attempts to generate a token to verify the model is functioning correctly. The server's responses are printed to the console.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nurl = f\"http://localhost:{port}/health_generate\"\n\nresponse = requests.get(url)\nprint_highlight(response.text)\n```\n\nLANGUAGE: python\nCODE:\n```\nurl = f\"http://localhost:{port}/health\"\n\nresponse = requests.get(url)\nprint_highlight(response.text)\n```\n\n----------------------------------------\n\nTITLE: Non-streaming Synchronous Generation with SGLang\nDESCRIPTION: This snippet demonstrates non-streaming synchronous text generation using the SGLang engine. It defines a list of prompts and sampling parameters, then calls `llm.generate` to generate text for each prompt. The generated text is then printed along with the corresponding prompt. The `sampling_params` dictionary controls parameters like temperature and top_p for the generation process.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/offline_engine_api.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\n\nsampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print(\"===============================\")\n    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Completions API in Python\nDESCRIPTION: This code snippet demonstrates how to use the OpenAI Chat Completions API in streaming mode. It creates a stream and iterates through chunks of data, printing the content as it becomes available, useful for real-time applications.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstream = client.chat.completions.create(\n    model=\"qwen/qwen2.5-0.5b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: Launches the SGLang server with a specified model and tool call parser. It checks if the code is running in a Continuous Integration (CI) environment and uses different import statements accordingly. It waits for the server to become available before proceeding.  The `--tool-call-parser` argument is crucial for determining how responses are interpreted.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport json\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\n\nserver_process, port = launch_server_cmd(\n    \"python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct --tool-call-parser qwen25 --host 0.0.0.0\"  # qwen25\n)\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server with Per-Channel Quantized Model\nDESCRIPTION: Launches the SGLang server with a per-channel quantized (INT8 or FP8) model and specifies `--quantization w8a8_int8` or `--quantization w8a8_fp8` to invoke the CUTLASS kernel, overriding the Hugging Face config's quantization settings. It requires the `sgl-kernel` to be properly configured.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server \\\n    --model-path neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic \\\n    --quantization w8a8_fp8 \\\n    --port 30000 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: API Request with JSON Schema\nDESCRIPTION: This snippet demonstrates how to make an API request to a local server, sending a prompt and a JSON schema for response validation. It uses the `requests` library to send a POST request to the `/generate` endpoint, including the text prompt and sampling parameters containing the JSON schema. The server response is then printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Here is the information of the capital of France in the JSON format.\\n\",\n    }\n]\ntext = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": text,\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 64,\n            \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n        },\n    },\n)\nprint_highlight(response.json())\n\n\nresponse_data = json.loads(response.json()[\"text\"])\n# validate the response by the pydantic model\ncapital_info = CapitalInfo.model_validate(response_data)\nprint_highlight(f\"Validated response: {capital_info.model_dump_json()}\")\n```\n\n----------------------------------------\n\nTITLE: Launching Llama 4 with SGLang\nDESCRIPTION: This command launches the SGLang server with a specified Llama 4 model. It utilizes Python and the sglang.launch_server module. Key parameters include --model-path to define the model to be used, --tp for tensor parallelism, and --context-length to specify the maximum context length for the model to avoid OOM errors.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/llama4.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model-path meta-llama/Llama-4-Scout-17B-16E-Instruct --tp 8 --context-length 1000000\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completion Batch Job OpenAI (Python)\nDESCRIPTION: This code snippet demonstrates how to create a batch job for chat completions using the OpenAI Python client. It first defines a list of requests, writes them to a JSONL file, uploads the file to OpenAI, and then creates a batch job using the uploaded file. It uses `client.files.create` to upload the file and `client.batches.create` to create the batch job. Dependencies: `json`, `time`, `openai`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport time\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n\nrequests = [\n    {\n        \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/chat/completions\",\n        \"body\": {\n            \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"Tell me a joke about programming\"}\n            ],\n            \"max_tokens\": 50,\n        },\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/chat/completions\",\n        \"body\": {\n            \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"What is Python?\"}],\n            \"max_tokens\": 50,\n        },\n    },\n]\n\ninput_file_path = \"batch_requests.jsonl\"\n\nwith open(input_file_path, \"w\") as f:\n    for req in requests:\n        f.write(json.dumps(req) + \"\\n\")\n\nwith open(input_file_path, \"rb\") as f:\n    file_response = client.files.create(file=f, purpose=\"batch\")\n\nbatch_response = client.batches.create(\n    input_file_id=file_response.id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\",\n)\n\nprint_highlight(f\"Batch job created with ID: {batch_response.id}\")\n```\n\n----------------------------------------\n\nTITLE: Non-streaming Asynchronous Generation with SGLang\nDESCRIPTION: This code showcases non-streaming asynchronous generation with the SGLang engine. An asynchronous `main` function calls `llm.async_generate` to generate text for a batch of prompts. The results are then printed in a loop after the generation completes. This allows other tasks to be performed while the text is being generated in the background.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/offline_engine_api.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n]\n\nsampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n\nprint(\"\\n=== Testing asynchronous batch generation ===\")\n\n\nasync def main():\n    outputs = await llm.async_generate(prompts, sampling_params)\n\n    for prompt, output in zip(prompts, outputs):\n        print(f\"\\nPrompt: {prompt}\")\n        print(f\"Generated text: {output['text']}\")\n\n\nasyncio.run(main())\n\n```\n\n----------------------------------------\n\nTITLE: Token Length Normalized Choice Selection in SGLang (Python)\nDESCRIPTION: This code demonstrates the use of the `token_length_normalized` choices method in SGLang. It defines an SGLang function that prompts the user for the capital of France and uses the `sgl.gen` primitive to generate an answer from a predefined list of choices. The `choices_method` argument is explicitly set to `sgl.token_length_normalized`, although it is the default method and can be omitted.  The function relies on the SGLang library and assumes a functional SGLang environment is set up.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/frontend/choices_methods.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@sgl.function\ndef example(s):\n    s += sgl.user(\"What is the capital of France?\")\n    s += sgl.assistant(\n        sgl.gen(\n            \"answer\",\n            choices=[\"London\", \"Paris\", \"Berlin\"],\n            choices_method=sgl.token_length_normalized,\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server with LLM Compressor Quantized Model\nDESCRIPTION: Launches the SGLang server with a model that was previously quantized using LLM Compressor.  The `--model-path` is set to the directory where the quantized model and tokenizer were saved.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server \\\n    --model-path $PWD/Meta-Llama-3-8B-Instruct-FP8-Dynamic \\\n    --port 30000 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Defining a Tool Function\nDESCRIPTION: Defines a sample tool function, `get_current_weather`, with type hints.  This function takes city, state, and unit as input and returns a formatted string describing the weather. The `available_tools` dictionary maps the function name to the actual function object.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# This is a demonstration, define real function according to your usage.\ndef get_current_weather(city: str, state: str, unit: \"str\"):\n    return (\n        f\"The weather in {city}, {state} is 85 degrees {unit}. It is \"\n        \"partly cloudly, with highs in the 90's.\"\n    )\n\n\navailable_tools = {\"get_current_weather\": get_current_weather}\n```\n\n----------------------------------------\n\nTITLE: Terminate SGLang server process\nDESCRIPTION: This snippet terminates the SGLang server process using the `terminate_process` function.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/lora.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Launching VLLM API Server\nDESCRIPTION: This command launches the vllm API server with a specified model path, tokenizer mode, and port.  It disables request logging and requires the vllm library to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_chain_reasoning/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with FlashInfer Backend\nDESCRIPTION: This command launches the SGLang server using the FlashInfer attention backend. It specifies the model to use (e.g., meta-llama/Meta-Llama-3.1-8B-Instruct or deepseek-ai/DeepSeek-V3) and optionally sets tensor parallelism (tp). The --trust-remote-code flag may be required for certain models.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/attention_backend.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model meta-llama/Meta-Llama-3.1-8B-Instruct --attention-backend flashinfer\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --tp 8 --model deepseek-ai/DeepSeek-V3 --attention-backend flashinfer --trust-remote-code\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with ModelScope via Docker (bash)\nDESCRIPTION: Launches the SGLang server within a Docker container, configuring GPU access, port mapping, and volume mounting for ModelScope cache. It also sets the SGLANG_USE_MODELSCOPE environment variable. Requires Docker to be installed and configured with GPU support. The model is specified with the `--model-path` argument.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/modelscope.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all \\\n    -p 30000:30000 \\\n    -v ~/.cache/modelscope:/root/.cache/modelscope \\\n    --env \"SGLANG_USE_MODELSCOPE=true\" \\\n    --ipc=host \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct --host 0.0.0.0 --port 30000\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server with TorchAO int4wo-128 Quantization\nDESCRIPTION: Launches the SGLang server with `torchao` quantization, specifically `int4wo-128`. The `--torchao-config int4wo-128` flag specifies the quantization configuration to use with `torchao`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server \\\n    --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n    --torchao-config int4wo-128 \\\n    --port 30000 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM\nDESCRIPTION: These commands install vLLM version 0.5.2 and jsonschema version 4.21.1 using pip. These specific versions are used due to potential issues with newer versions.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# vLLM\npip install vllm==0.5.2\npip install jsonschema==4.21.1\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Qwen2.5-VL, Python\nDESCRIPTION: This code snippet demonstrates how to launch an SGLang server with a specific model (Qwen/Qwen2.5-VL-7B-Instruct) and chat template (qwen2-vl). It checks if the code is running in a CI environment to use different import paths. It then launches the server process and waits for it to initialize.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_vision.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\nvision_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct \\\n    --chat-template=qwen2-vl\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Non-Streaming Request with Reasoning in Python\nDESCRIPTION: This code snippet demonstrates how to make a non-streaming request to the SGLang server using the OpenAI-compatible API, enabling reasoning separation. It prints both the reasoning content and the final answer.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresponse_non_stream = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    temperature=0.6,\n    top_p=0.95,\n    stream=False,  # Non-streaming\n    extra_body={\"separate_reasoning\": True},\n)\nprint_highlight(\"==== Reasoning ====\")\nprint_highlight(response_non_stream.choices[0].message.reasoning_content)\n\nprint_highlight(\"==== Text ====\")\nprint_highlight(response_non_stream.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Setting up Tracing and SGLang Backend - Python\nDESCRIPTION: This snippet initializes Parea for tracing OpenAI calls and sets OpenAI as the default backend for SGLang. It loads environment variables using `dotenv`, disables tokenizers parallelism, initializes Parea with an API key and project name, integrates Parea with SGLang, and sets the default SGLang backend to OpenAI's `gpt-3.5-turbo` model.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n\nfrom dotenv import load_dotenv\n\nfrom sglang import function, user, assistant, gen, set_default_backend, OpenAI\nfrom sglang.lang.interpreter import ProgramState\nfrom parea import Parea, trace\n\n\nload_dotenv()\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\np = Parea(api_key=os.getenv(\"PAREA_API_KEY\"), project_name=\"rag_sglang\")\np.integrate_with_sglang()\n\nset_default_backend(OpenAI(\"gpt-3.5-turbo\"))\n```\n\n----------------------------------------\n\nTITLE: Parse Text with FunctionCallParser - Python\nDESCRIPTION: Initializes a `FunctionCallParser` with a list of `Tool` objects and a specified tool call parser.  It parses the `generated_text` using the `parse_non_stream` method, separating normal text from function calls.  The results are then printed to the console. It relies on a list of `tools` created by the previous step, and `generated_text`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntools = [convert_dict_to_tool(raw_tool) for raw_tool in tools]\n\nparser = FunctionCallParser(tools=tools, tool_call_parser=\"qwen25\")\nnormal_text, calls = parser.parse_non_stream(generated_text)\n\nprint(\"=== Parsing Result ===\")\nprint(\"Normal text portion:\", normal_text)\nprint(\"Function call portion:\")\nfor call in calls:\n    # call: ToolCallItem\n    print(f\"  - tool name: {call.name}\")\n    print(f\"    parameters: {call.parameters}\")\n```\n\n----------------------------------------\n\nTITLE: Installing SGL Kernel with CUDA 12.1 or 12.4\nDESCRIPTION: This command installs the latest sgl-kernel package. It is used when the environment uses CUDA 12.1 or CUDA 12.4. It fetches the package from the default PyPI index.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install sgl-kernel\n```\n\n----------------------------------------\n\nTITLE: Launching Embedding Server\nDESCRIPTION: Launches an SGLang server configured to use an embedding model. It specifies the model path, sets the server to embedding mode, defines the host and port, and optionally provides a chat template. Some models may require `--trust-remote-code`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/supported_models/embedding_models.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m sglang.launch_server \\\n  --model-path Alibaba-NLP/gme-Qwen2-VL-2B-Instruct \\  # example HF/local path\n  --is-embedding \\\n  --host 0.0.0.0 \\\n  --chat-template gme-qwen2-vl \\                     # set chat template\n  --port 30000 \\\n```\n\n----------------------------------------\n\nTITLE: Generating Text with EBNF using SGLang Engine\nDESCRIPTION: This snippet generates text using EBNF grammar constraints with the SGLang engine directly. It defines an EBNF grammar and passes it as the `ebnf` parameter in `sampling_params` to the `llm.generate` function.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Give me the information of the capital of France.\",\n    \"Give me the information of the capital of Germany.\",\n    \"Give me the information of the capital of Italy.\",\n]\n\nsampling_params = {\n    \"temperature\": 0.8,\n    \"top_p\": 0.95,\n    \"ebnf\": (\n        \"root ::= city | description\\n\"\n        'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n        'description ::= city \" is \" status\\n'\n        'status ::= \"the capital of \" country\\n'\n        'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n    ),\n}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print(\"===============================\")\n    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Import necessary modules for LoRA serving\nDESCRIPTION: This snippet imports the necessary modules for launching the server, waiting for it to start, and terminating it. It also handles conditional imports based on the CI environment.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/lora.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nfrom sglang.utils import wait_for_server, terminate_process\n\nimport json\nimport requests\n```\n\n----------------------------------------\n\nTITLE: Terminating SGLang Server\nDESCRIPTION: Terminates the SGLang server process using the `terminate_process` function. This ensures the server is properly shut down after the examples are run.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/send_request.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Launching LightLLM API Server\nDESCRIPTION: This command launches the lightllm API server with a specified model directory, maximum total token number, and port. Requires lightllm to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_chain_reasoning/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# A10G\npython -m lightllm.server.api_server --tokenizer_mode auto --model_dir ~/model_weights/llama-2-7b-chat-hf --max_total_token_num 16000 --port 22000\n```\n\n----------------------------------------\n\nTITLE: Initializing SGLang Engine for Offline Inference\nDESCRIPTION: This code snippet initializes the SGLang engine with a specified model path. It also includes necessary imports for asynchronous operations, image handling, and SGLang functionalities. The `is_in_ci` function checks if the code is running in a Continuous Integration environment and conditionally applies the `nest_asyncio` patch if not.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/offline_engine_api.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# launch the offline engine\nimport asyncio\nimport io\nimport os\n\nfrom PIL import Image\nimport requests\nimport sglang as sgl\n\nfrom sglang.srt.conversation import chat_templates\nfrom sglang.test.test_utils import is_in_ci\nfrom sglang.utils import async_stream_and_merge, stream_and_merge\n\nif is_in_ci():\n    import patch\nelse:\n    import nest_asyncio\n\n    nest_asyncio.apply()\n\n\nllm = sgl.Engine(model_path=\"qwen/qwen2.5-0.5b-instruct\")\n\n```\n\n----------------------------------------\n\nTITLE: Offline Engine: JSON with Pydantic\nDESCRIPTION: This snippet uses SGLang's offline engine to generate structured output based on a Pydantic model for JSON validation. It defines prompts and a `CapitalInfo` Pydantic model, then generates text using `llm.generate`, validating the output against the model. The validated output is then printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom pydantic import BaseModel, Field\n\n\nprompts = [\n    \"Give me the information of the capital of China in the JSON format.\",\n    \"Give me the information of the capital of France in the JSON format.\",\n    \"Give me the information of the capital of Ireland in the JSON format.\",\n]\n\n\n# Define the schema using Pydantic\nclass CapitalInfo(BaseModel):\n    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n    population: int = Field(..., description=\"Population of the capital city\")\n\n\nsampling_params = {\n    \"temperature\": 0.1,\n    \"top_p\": 0.95,\n    \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print_highlight(\"===============================\")\n    print_highlight(f\"Prompt: {prompt}\")  # validate the output by the pydantic model\n    capital_info = CapitalInfo.model_validate_json(output[\"text\"])\n    print_highlight(f\"Validated output: {capital_info.model_dump_json()}\")\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Router separately\nDESCRIPTION: This command launches the SGLang Router and connects it to pre-existing worker Runtimes. The `--worker-urls` argument specifies the URLs of the worker Runtimes. This setup is useful for multi-node data parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang_router.launch_router --worker-urls http://worker_url_1 http://worker_url_2\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: Launches an SGLang server with specified model path and host address.  It uses `launch_server_cmd` function to start the server process and waits for the server to be ready using `wait_for_server`. The example also shows how to terminate the server process.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\n\nserver_process, port = launch_server_cmd(\n    \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server in Multi-Node Setup (Bash)\nDESCRIPTION: These commands launch the SGLang server on two nodes, configuring them for distributed processing. The `dist-init-addr` parameter specifies the address for inter-node communication, while `nnodes` indicates the total number of nodes and `node-rank` defines the rank of each node. `tp` refers to tensor parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# node 1\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --tp 16 --dist-init-addr 10.0.0.1:5000 --nnodes 2 --node-rank 0 --trust-remote-code\n```\n\nLANGUAGE: bash\nCODE:\n```\n# node 2\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --tp 16 --dist-init-addr 10.0.0.1:5000 --nnodes 2 --node-rank 1 --trust-remote-code\n```\n\n----------------------------------------\n\nTITLE: Greedy Token Selection in SGLang (Python)\nDESCRIPTION: This code shows how to use the `greedy_token_selection` choices method within SGLang. An SGLang function is defined to query the user for the capital of France, then utilizes `sgl.gen` to produce an answer from a fixed list of options. The `choices_method` parameter is set to `sgl.greedy_token_selection`, which selects the choice with the highest logprob for its initial token.  This method requires the SGLang library.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/frontend/choices_methods.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@sgl.function\ndef example(s):\n    s += sgl.user(\"What is the capital of France?\")\n    s += sgl.assistant(\n        sgl.gen(\n            \"answer\",\n            choices=[\"London\", \"Paris\", \"Berlin\"],\n            choices_method=sgl.greedy_token_selection,\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Router with Data and Tensor Parallelism (DP & TP)\nDESCRIPTION: This command launches the SGLang router server with both data and tensor parallelism enabled.  `--dp 2` specifies data parallelism across two GPUs, and `--tp 2` specifies tensor parallelism across another two GPUs, resulting in a total of 4 GPUs being utilized.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/server_arguments.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang_router.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --dp 2 --tp 2\n```\n\n----------------------------------------\n\nTITLE: Querying SGLang server using cURL, Python\nDESCRIPTION: This snippet shows how to send a request to the SGLang server using a cURL command executed from Python. The cURL command constructs a JSON payload with the model, messages (including text and image URL), and max_tokens. The output is then printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_vision.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport subprocess\n\ncurl_command = f\"\"\"\ncurl -s http://localhost:{port}/v1/chat/completions \\\n  -d '{{\n    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n    \"messages\": [\n      {{\n        \"role\": \"user\",\n        \"content\": [\n          {{\n            \"type\": \"text\",\n            \"text\": \"What’s in this image?\"\n          }},\n          {{\n            \"type\": \"image_url\",\n            \"image_url\": {{\n              \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n            }}\n          }}\n        ]\n      }}\n    ],\n    \"max_tokens\": 300\n  }}'\n\"\"\"\n\nresponse = subprocess.check_output(curl_command, shell=True).decode()\nprint_highlight(response)\n\n\nresponse = subprocess.check_output(curl_command, shell=True).decode()\nprint_highlight(response)\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server with Offline Quantized Model (AWQ)\nDESCRIPTION: Launches the SGLang server with a pre-quantized AWQ model. It loads model weights and configurations directly from the specified path without requiring the `--quantization` flag, as the quantization method is parsed from the Hugging Face config.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server \\\n    --model-path hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4 \\\n    --port 30000 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Registering a Model\nDESCRIPTION: This Python snippet demonstrates how to register a new model with the `ModelRegistry` in SGLang. It shows how to add a single model and how to use a function to import and register multiple models. This allows for integration without modifying the original source code.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/supported_models/support_new_models.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sglang.srt.models.registry import ModelRegistry\nfrom sglang.srt.entrypoints.http_server import launch_server\n\n# For a single model, add it to the registry:\nModelRegistry.models[model_name] = model_class\n\n# For multiple models, you can imitate the import_model_classes() function:\nfrom functools import lru_cache\n\n@lru_cache()\ndef import_new_model_classes():\n    model_arch_name_to_cls = {}\n    # Populate model_arch_name_to_cls with your new model classes.\n    ...\n    return model_arch_name_to_cls\n\nModelRegistry.models.update(import_new_model_classes())\n\n# Launch the server with your server arguments:\nlaunch_server(server_args)\n```\n\n----------------------------------------\n\nTITLE: Running sglang-triton Docker Container\nDESCRIPTION: This command runs the `sglang-triton` Docker image with GPU support and network host mode. It mounts the `./models` directory to `/mnt/models` inside the container, and names the container `sglang-triton`.  Requires the image to be built first.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/triton/README.md#_snippet_1\n\nLANGUAGE: docker\nCODE:\n```\ndocker run -ti --gpus=all --network=host --name sglang-triton -v ./models:/mnt/models sglang-triton\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with FA3 Backend\nDESCRIPTION: This command launches the SGLang server using the FlashAttention 3 (FA3) backend, which is the default for Hopper architecture machines. It specifies the model to use and optionally sets tensor parallelism. The --trust-remote-code flag may be required for certain models.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/attention_backend.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model meta-llama/Meta-Llama-3.1-8B-Instruct --attention-backend fa3\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --tp 8 --model deepseek-ai/DeepSeek-V3 --trust-remote-code --attention-backend fa3\n```\n\n----------------------------------------\n\nTITLE: Generating Regex-Constrained Output (Outlines)\nDESCRIPTION: This snippet showcases how to generate text that conforms to a specified regular expression using SGLang. It sends a request to the server with a prompt and a regular expression. The server constrains the generated text to match the regex pattern.  The `regex` parameter within `sampling_params` defines the allowed pattern.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/sampling_params.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.post(\n    \"http://localhost:30000/generate\",\n    json={\n        \"text\": \"Paris is the capital of\",\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 64,\n            \"regex\": \"(France|England)\",\n        },\n    },\n)\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Getting Server Info with /get_server_info API\nDESCRIPTION: Retrieves server information including CLI arguments, token limits, and memory pool sizes using the `/get_server_info` API. The example makes a GET request to the endpoint and prints the server's response.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# get_server_info\n\nurl = f\"http://localhost:{port}/get_server_info\"\n\nresponse = requests.get(url)\nprint_highlight(response.text)\n```\n\n----------------------------------------\n\nTITLE: Launching Embedding Server with SGLang\nDESCRIPTION: Launches the SGLang server for embedding generation using a specified model path. It configures the server to run on a specific host and marks it as an embedding server. The code waits for the server to initialize before proceeding.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\nembedding_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-1.5B-instruct \\\n    --host 0.0.0.0 --is-embedding\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Encoding Text with /encode API\nDESCRIPTION: Encodes text into embeddings using the `/encode` API.  It sends a POST request to the server with the text to be encoded and prints the first 10 elements of the resulting embedding vector. The `model` parameter must also be specified.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# successful encode for embedding model\n\nurl = f\"http://localhost:{port}/encode\"\ndata = {\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"text\": \"Once upon a time\"}\n\nresponse = requests.post(url, json=data)\nresponse_json = response.json()\nprint_highlight(f\"Text embedding (first 10): {response_json['embedding'][:10]}\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Logit Processor (Deterministic)\nDESCRIPTION: This Python code defines a custom logit processor called `DeterministicLogitProcessor` that forces the model to always sample a specific token ID. It inherits from the `CustomLogitProcessor` class and overrides the `__call__` method to modify the logits to assign the highest probability to the specified token ID.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/sampling_params.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sglang.srt.sampling.custom_logit_processor import CustomLogitProcessor\n\nclass DeterministicLogitProcessor(CustomLogitProcessor):\n    \"\"\"A dummy logit processor that changes the logits to always\n    sample the given token id.\n    \"\"\"\n\n    def __call__(self, logits, custom_param_list):\n        # Check that the number of logits matches the number of custom parameters\n        assert logits.shape[0] == len(custom_param_list)\n        key = \"token_id\"\n\n        for i, param_dict in enumerate(custom_param_list):\n            # Mask all other tokens\n            logits[i, :] = -float(\"inf\")\n            # Assign highest probability to the specified token\n            logits[i, param_dict[key]] = 0.0\n        return logits\n```\n\n----------------------------------------\n\nTITLE: Offline Engine: EBNF Grammar\nDESCRIPTION: This code demonstrates the use of EBNF grammar with SGLang's offline engine. It defines prompts and an EBNF grammar, then generates text based on these constraints. The generated text and the corresponding prompt are then printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Give me the information of the capital of France.\",\n    \"Give me the information of the capital of Germany.\",\n    \"Give me the information of the capital of Italy.\",\n]\n\nsampling_params = {\n    \"temperature\": 0.8,\n    \"top_p\": 0.95,\n    \"ebnf\": (\n        \"root ::= city | description\\n\"\n        'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n        'description ::= city \" is \" status\\n'\n        'status ::= \"the capital of \" country\\n'\n        'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n    ),\n}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print_highlight(\"===============================\")\n    print_highlight(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Modules for SGLang and OpenAI\nDESCRIPTION: This code snippet imports necessary modules for launching the server, interacting with OpenAI, and handling CI environment checks. It imports modules differently based on whether the code is running in a CI environment or not.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/speculative_decoding.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\nimport openai\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Alias for SGLang\nDESCRIPTION: This snippet defines a convenient alias `drun` for running SGLang Docker containers.  It includes options for network configuration, device access for GPUs (/dev/kfd, /dev/dri), IPC, shared memory, user group access (video), security options (seccomp), and volume mounting for persistent data. It also defines mounting points for /data and $HOME/dockerx.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/amd.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nalias drun='docker run -it --rm --network=host --privileged --device=/dev/kfd --device=/dev/dri \\\n    --ipc=host --shm-size 16G --group-add video --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    -v $HOME/dockerx:/dockerx \\\n    -v /data:/data'\n```\n\n----------------------------------------\n\nTITLE: Send Streaming Request\nDESCRIPTION: This curl command sends a POST request to the `/generate_stream` endpoint of the custom server. The request includes a JSON payload with a 'prompt' field. The `--no-buffer` option ensures that the output is streamed to the console in real-time.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/readme.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8000/generate_stream  -H \"Content-Type: application/json\"  -d '{\"prompt\": \"The Transformer architecture is...\"}' --no-buffer\n```\n\n----------------------------------------\n\nTITLE: Launching Llama 3.1 405B (fp8) on a Single Node with SGLang\nDESCRIPTION: This snippet shows how to launch the Llama 3.1 405B model with fp8 precision on a single node using SGLang. It specifies the model path and the tensor parallelism degree (--tp).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/multi_node.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 --tp 8\n```\n\n----------------------------------------\n\nTITLE: Sample Function Calling Request (cURL)\nDESCRIPTION: This cURL command shows a sample request to the SGLang server for function calling with a DeepSeek model. It includes a JSON payload with the model, tools definition (query_weather function), and user message. It expects a JSON response with the tool call details.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deepseek.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"http://127.0.0.1:30000/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"temperature\": 0, \"max_tokens\": 100, \"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"tools\": [{\\\"type\\\": \\\"function\\\", \\\"function\\\": {\\\"name\\\": \\\"query_weather\\\", \\\"description\\\": \\\"Get weather of an city, the user should supply a city first\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"city\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city, e.g. Beijing\\\"}}, \\\"required\\\": [\\\"city\\\"]}}}], \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hows the weather like in Qingdao today\\\"}]}'\n```\n\n----------------------------------------\n\nTITLE: Capture Expert Distribution with MoE Models\nDESCRIPTION: Capture expert selection distribution in MoE models by using APIs like `/start_expert_distribution_record`, `/stop_expert_distribution_record` and `/dump_expert_distribution_record`. After recording, the script reads the output CSV file and prints the first 10 lines to the console for demonstration.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nexpert_record_server_process, port = launch_server_cmd(\n    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.post(f\"http://localhost:{port}/start_expert_distribution_record\")\nprint_highlight(response)\n\nurl = f\"http://localhost:{port}/generate\"\ndata = {\"text\": \"What is the capital of France?\"}\n\nresponse = requests.post(url, json=data)\nprint_highlight(response.json())\n\nresponse = requests.post(f\"http://localhost:{port}/stop_expert_distribution_record\")\nprint_highlight(response)\n\nresponse = requests.post(f\"http://localhost:{port}/dump_expert_distribution_record\")\nprint_highlight(response)\n\nimport glob\n\noutput_file = glob.glob(\"expert_distribution_*.csv\")[0]\nwith open(output_file, \"r\") as f:\n    print_highlight(\"\\n| Layer ID | Expert ID | Count |\")\n    print_highlight(\"|----------|-----------|--------|\")\n    next(f)\n    for i, line in enumerate(f):\n        if i < 9:\n            layer_id, expert_id, count = line.strip().split(\",\")\n            print_highlight(f\"| {layer_id:8} | {expert_id:9} | {count:6} |\")\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Custom Logit Processor\nDESCRIPTION: This bash command demonstrates how to launch the SGLang server with the `--enable-custom-logit-processor` flag. This flag enables the use of custom logit processors, allowing for fine-grained control over token sampling during text generation. The command specifies the model path, port, and the custom logit processor flag.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/sampling_params.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --port 30000 --enable-custom-logit-processor\n```\n\n----------------------------------------\n\nTITLE: Launching sglang server with Llama-7B (Python)\nDESCRIPTION: This command launches the sglang server with the Llama-2-7b-chat-hf model on port 30000. It uses the python3 interpreter to execute the sglang launch server module.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython3 -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with a Vision Language Model\nDESCRIPTION: This command launches the SGLang server with a specified Vision Language Model (VLM). It requires the `--model-path` to point to either a HuggingFace model identifier or a local path, and `--chat-template` to specify the correct chat template for the VLM to handle vision prompts. The `--host` and `--port` arguments configure the server's network settings.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/supported_models/vision_language_models.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m sglang.launch_server \\\n  --model-path meta-llama/Llama-3.2-11B-Vision-Instruct \\  # example HF/local path\n  --chat-template llama_3_vision \\                        # required chat template\n  --host 0.0.0.0 \\\n  --port 30000 \\\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for RAG Pipeline - Python\nDESCRIPTION: This code snippet installs the necessary Python packages for the RAG pipeline: `sglang` with OpenAI support, `parea-ai` for tracing and evaluation, and `chromadb` for the vector database. The comment provides instructions for specific cases like Mac M1 chips.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# note, if you use a Mac M1 chip, you might need to install grpcio 1.59.0 first such that installing chromadb works\n# !pip install grpcio==1.59.0\n\n!pip install sglang[openai] parea-ai chromadb\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Block-INT8 Quantization - Bash\nDESCRIPTION: Launches the SGLang server with block-wise INT8 quantization for the DeepSeek-R1 model.  This command specifies the model, tensor parallelism (TP), distributed initialization address, number of nodes, node rank, and enables features like trusting remote code, torch compile, and setting a maximum batch size for torch compile. It requires `sglang` package to be installed. `MASTER_IP` should be set correctly.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n#master\npython3 -m sglang.launch_server \\\n\t--model meituan/DeepSeek-R1-Block-INT8 --tp 16 --dist-init-addr \\\n\tMASTER_IP:5000 --nnodes 2 --node-rank 0 --trust-remote-code --enable-torch-compile --torch-compile-max-bs 8\n#cluster\npython3 -m sglang.launch_server \\\n\t--model meituan/DeepSeek-R1-Block-INT8 --tp 16 --dist-init-addr \\\n\tMASTER_IP:5000 --nnodes 2 --node-rank 1 --trust-remote-code --enable-torch-compile --torch-compile-max-bs 8\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: This command launches the SGLang server with a specified model and port. It uses CodeLlama-7b-instruct-hf as the model and runs on port 30000. The command is executed from the command line.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_document_qa/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m sglang.launch_server --model-path codellama/CodeLlama-7b-instruct-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Non-Streaming Request\nDESCRIPTION: Sends a non-streaming request to the SGLang server to generate a response. It sets the model, messages, temperature, top_p, max_tokens, stream, and tools parameters. The response includes the content and any tool calls. The code then extracts and prints the content and tool call information from the response object.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Non-streaming mode test\nresponse_non_stream = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    temperature=0.1,\n    top_p=0.95,\n    max_tokens=1024,\n    stream=False,  # Non-streaming\n    tools=tools,\n)\nprint_highlight(\"Non-stream response:\")\nprint(response_non_stream)\nprint_highlight(\"==== content ====\")\nprint(response_non_stream.choices[0].message.content)\nprint_highlight(\"==== tool_calls ====\")\nprint(response_non_stream.choices[0].message.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Classifying Text with /classify API\nDESCRIPTION: Classifies text using a reward model via the `/classify` API.  It prepares prompts using `transformers` library, then sends a POST request to the server. The responses contain scores representing reward values, which are extracted and printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\n\nPROMPT = (\n    \"What is the range of the numeric output of a sigmoid node in a neural network?\"\n)\n\nRESPONSE1 = \"The output of a sigmoid node is bounded between -1 and 1.\"\nRESPONSE2 = \"The output of a sigmoid node is bounded between 0 and 1.\"\n\nCONVS = [\n    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE1}],\n    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE2}],\n]\n\ntokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\")\nprompts = tokenizer.apply_chat_template(CONVS, tokenize=False)\n\nurl = f\"http://localhost:{port}/classify\"\ndata = {\"model\": \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\", \"text\": prompts}\n\nresponses = requests.post(url, json=data).json()\nfor response in responses:\n    print_highlight(f\"reward: {response['embedding'][0]}\")\n```\n\n----------------------------------------\n\nTITLE: Unconditional Likelihood Normalized Choice Selection in SGLang (Python)\nDESCRIPTION: This snippet illustrates the use of `unconditional_likelihood_normalized` as a choices method in SGLang. The code defines an SGLang function that poses a question about the capital of France and uses the `sgl.gen` primitive to select an answer from a list of options. The `choices_method` is explicitly set to `sgl.unconditional_likelihood_normalized`, which involves an additional LLM call to obtain unconditional likelihoods.  Requires the SGLang library.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/frontend/choices_methods.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@sgl.function\ndef example(s):\n    s += sgl.user(\"What is the capital of France?\")\n    s += sgl.assistant(\n        sgl.gen(\n            \"answer\",\n            choices=[\"London\", \"Paris\", \"Berlin\"],\n            choices_method=sgl.unconditional_likelihood_normalized,\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Messages for Function Call\nDESCRIPTION: Defines the messages to be sent to the model. This example sets up a user message asking for the weather in Boston, instructing the model to perform reasoning and use tools. The `get_messages` function encapsulates message creation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_messages():\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": \"What's the weather like in Boston today? Output a reasoning before act, then use the tools to help you.\",\n        }\n    ]\n\n\nmessages = get_messages()\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server\nDESCRIPTION: This command launches the SGLang server. It specifies the model path (meta-llama/Llama-2-7b-chat-hf) and the port number (30000). The server will be running and listening for incoming requests on the specified port.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tip_suggestion/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server in Docker\nDESCRIPTION: This snippet demonstrates how to launch the SGLang server within a Docker container. It uses the `drun` alias defined earlier, maps port 30000, mounts the Hugging Face cache directory, passes the Hugging Face token as an environment variable, specifies the model path, host, and port.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/amd.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndrun -p 30000:30000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --env \"HF_TOKEN=<secret>\" \\\n    sglang_image \\\n    python3 -m sglang.launch_server \\\n    --model-path NousResearch/Meta-Llama-3.1-8B \\\n    --host 0.0.0.0 \\\n    --port 30000\n```\n\n----------------------------------------\n\nTITLE: Remove a worker from the Router via API (Example)\nDESCRIPTION: This command uses `curl` to send a POST request to the `/remove_worker` API endpoint of the SGLang Router, removing a worker running on localhost:30001.  It's a concrete example building on the general `/remove_worker` usage.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:30000/remove_worker?url=http://127.0.0.1:30001\n```\n\n----------------------------------------\n\nTITLE: JSON Chat Template Example\nDESCRIPTION: This JSON object defines a custom chat template with specific fields for 'name', 'system', 'user', 'assistant', 'sep_style', 'sep', and 'stop_str'. These fields dictate how different parts of the conversation are formatted and separated. The `conversation.py` file in the SGLang project defines the format for this JSON object.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/custom_chat_template.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"my_model\",\n  \"system\": \"<|im_start|>system\",\n  \"user\": \"<|im_start|>user\",\n  \"assistant\": \"<|im_start|>assistant\",\n  \"sep_style\": \"CHATML\",\n  \"sep\": \"<|im_end|>\",\n  \"stop_str\": [\"<|im_end|>\", \"<|im_start|>\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang with pip for DeepSeek V3\nDESCRIPTION: This command installs SGLang using pip and launches a server for the DeepSeek V3 model. The `--trust-remote-code` flag is crucial for executing custom model code. This setup requires Python 3 and a working internet connection to download dependencies.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Installation\npip install \"sglang[all]>=0.4.5.post3\"\n\n# Launch\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code\n```\n\n----------------------------------------\n\nTITLE: Defining the Generation Step with SGLang - Python\nDESCRIPTION: This code defines the generation step of the RAG pipeline using SGLang. `generation_sglang` function takes a question and context, constructs a prompt with the question and context, and generates an answer using the `gen` function from SGLang. The `generation` function then runs the SGLang program and returns the generated answer while tracing with Parea.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@function\ndef generation_sglang(s, question: str, *context: str):\n    context = \"\\n\".join(context)\n    s += user(\n        f\"Given this question:\\n{question}\\n\\nAnd this context:\\n{context}\\n\\nAnswer the question.\"\n    )\n    s += assistant(gen(\"answer\"))\n\n\n@trace\ndef generation(question: str, *context):\n    state: ProgramState = generation_sglang.run(question, *context)\n    while not state.stream_executor.is_finished:\n        time.sleep(1)\n    return state.stream_executor.variables[\"answer\"]\n```\n\n----------------------------------------\n\nTITLE: Co-launch Router and Runtimes with SGLang\nDESCRIPTION: This command launches the SGLang Runtime with a specified model and data parallelism size. The `--dp-size` argument determines the number of worker processes to launch. The `--host` argument specifies the host address for the server to listen on.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang_router.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --dp-size 4 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Display SGLang Router launch_router usage\nDESCRIPTION: This command executes the launch_router.py script within the sglang_router package and displays the help message. The help message outlines available command-line arguments and their purposes. This is helpful for understanding how to configure and run the router.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang_router.launch_router --help\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Reasoning Parser\nDESCRIPTION: This snippet launches the SGLang server with a specified model and enables the reasoning parser. It sets up the environment, starts the server process, waits for the server to become available, and then creates an OpenAI client connected to the SGLang server. Dependencies include sglang and openai libraries. The `launch_server_cmd` function is used to start the server process, and the `wait_for_server` function ensures that the server is running before proceeding.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport os\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\nserver_process, port = launch_server_cmd(\n    \"python -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --host 0.0.0.0 --reasoning-parser deepseek-r1\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\nclient = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n```\n\n----------------------------------------\n\nTITLE: Start SGLang Server with Metrics Enabled (Python)\nDESCRIPTION: This command starts the SGLang server with metrics enabled, allowing Prometheus to collect performance data. It specifies the model path, port, and the `--enable-metrics` flag which exposes the metrics endpoint. The metrics are served on the specified port by default.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/monitoring/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --port 30000 --enable-metrics\n```\n\n----------------------------------------\n\nTITLE: Generating EBNF-Constrained Output (XGrammar)\nDESCRIPTION: This example demonstrates how to use an EBNF grammar to constrain the output generated by SGLang. It sends a POST request to the server with a prompt and an EBNF grammar definition. The server then ensures that the generated text adheres to the grammar rules specified in the `ebnf` parameter.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/sampling_params.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.post(\n    \"http://localhost:30000/generate\",\n    json={\n        \"text\": \"Write a greeting.\",\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 64,\n            \"ebnf\": 'root ::= \"Hello\" | \"Hi\" | \"Hey\"',\n        },\n    },\n)\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Running SGLang with Docker (Bash)\nDESCRIPTION: This snippet shows how to run SGLang using Docker, including specifying GPU usage, shared memory size, port mapping, volume mounting, environment variables for Hugging Face token, IPC mode, and launching the SGLang server. Replace `<secret>` with your Hugging Face token.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/start/install.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all \\\n    --shm-size 32g \\\n    -p 30000:30000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --env \"HF_TOKEN=<secret>\" \\\n    --ipc=host \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Llama-3 Model\nDESCRIPTION: This command launches the SGLang server, specifying the model path, host, and port. It uses the meta-llama/Llama-3.2-1B-Instruct model, sets the host to 0.0.0.0, and the port to 30000. The model path can be a Hugging Face identifier or a local path.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/supported_models/generative_models.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m sglang.launch_server \\\n  --model-path meta-llama/Llama-3.2-1B-Instruct \\  # example HF/local path\n  --host 0.0.0.0 \\\n  --port 30000 \\\n\n```\n\n----------------------------------------\n\nTITLE: Benchmark with Dummy Weights (Bash)\nDESCRIPTION: This command benchmarks a model using dummy weights by providing only the config.json file.  Add `--load-format dummy` and make sure the checkpoint folder has a correct `config.json`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --batch 32 --input-len 256 --output-len 32 --load-format dummy\n```\n\n----------------------------------------\n\nTITLE: Running Model Test\nDESCRIPTION: This command runs a specific test case for the newly added model. It uses unittest to execute the test within the `test_generation_models` module, focusing on the `test_others` method of the `TestGenerationModels` class. `ONLY_RUN` limits testing to the specified model, accelerating development.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/supported_models/support_new_models.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nONLY_RUN=Qwen/Qwen2-1.5B python3 -m unittest test_generation_models.TestGenerationModels.test_others\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with ModelScope (bash)\nDESCRIPTION: Launches the SGLang server using the python command, specifying the model path to a ModelScope model and the port. The `sglang.launch_server` script is part of the SGLang library. Requires SGLang to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/modelscope.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path qwen/Qwen2-7B-Instruct --port 30000\n```\n\n----------------------------------------\n\nTITLE: Profile SGLang with Nsys\nDESCRIPTION: This snippet uses the NVIDIA Nsight Systems profiler (nsys) to profile SGLang's performance. It runs a benchmark with specified batch size, input/output lengths, model, and other parameters and saves the profiling data to a file. `--disable-cuda-graph` is used for easier analysis.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/development_guide_using_docker.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Change batch size, input, output and add `disable-cuda-graph` (for easier analysis)\n# e.g. DeepSeek V3\nnsys profile -o deepseek_v3 python3 -m sglang.bench_one_batch --batch-size 1 --input 128 --output 256 --model deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 8 --disable-cuda-graph\n```\n\n----------------------------------------\n\nTITLE: Adding a worker to the Router via API (Bash)\nDESCRIPTION: This command uses `curl` to send a POST request to the `/add_worker` API endpoint of the SGLang Router. The `url` parameter specifies the URL of the worker to be added. This allows dynamically scaling the number of workers.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:30000/add_worker?url=http://worker_url_1\n```\n\n----------------------------------------\n\nTITLE: Running DeepSeek-V3 with SGLang in Docker\nDESCRIPTION: This snippet provides an example of how to launch the SGLang server with the DeepSeek-V3 model. It uses the `drun` alias to run a Docker container, maps port 30000, mounts the Hugging Face cache, passes the Hugging Face token, sets the model path to `deepseek-ai/DeepSeek-V3`, enables tensor parallelism (`--tp 8`), trusts remote code, and sets the host and port.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/amd.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndrun -p 30000:30000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    --env \"HF_TOKEN=<secret>\" \\\n    sglang_image \\\n    python3 -m sglang.launch_server \\\n    --model-path deepseek-ai/DeepSeek-V3 # <- here\n    --tp 8 \\\n    --trust-remote-code \\\n    --host 0.0.0.0 \\\n    --port 30000\n```\n\n----------------------------------------\n\nTITLE: Terminating SGLang Server\nDESCRIPTION: Terminates the SGLang server process. Uses the `terminate_process` function from `sglang.utils` to kill the server process, ensuring that resources are freed up after the script finishes.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Convert Dictionary to Tool - Python\nDESCRIPTION: Converts a dictionary representation of a tool into a `Tool` object with a `Function` object. It retrieves the 'function' dictionary from the input `tool_dict` and extracts the name, description, and parameters to construct a `Function` object.  It assumes the existence of `Tool` and `Function` classes.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef convert_dict_to_tool(tool_dict: dict) -> Tool:\n    function_dict = tool_dict.get(\"function\", {})\n    return Tool(\n        type=tool_dict.get(\"type\", \"function\"),\n        function=Function(\n            name=function_dict.get(\"name\"),\n            description=function_dict.get(\"description\"),\n            parameters=function_dict.get(\"parameters\"),\n        ),\n    )\n```\n\n----------------------------------------\n\nTITLE: Terminating the Embedding Server\nDESCRIPTION: Terminates the launched embedding server process.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_embeddings.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(embedding_process)\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with EAGLE-2 and torch.compile\nDESCRIPTION: This snippet launches an SGLang server with EAGLE-2 speculative decoding and `torch.compile` enabled for further optimizations. It configures parameters like the draft model path, number of steps, top-k branching factor, number of draft tokens, memory fraction, and torch compile batch size. It then waits for the server to start.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/speculative_decoding.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nserver_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --speculative-algorithm EAGLE \\\n    --speculative-draft-model-path lmsys/sglang-EAGLE-llama2-chat-7B --speculative-num-steps 5 \\\n        --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-fraction 0.6 \\\n            --enable-torch-compile --torch-compile-max-bs 2\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking sglang (short output) (Python)\nDESCRIPTION: This command runs the bench_sglang.py script to benchmark sglang with the Llama-2-7b-chat-hf tokenizer, generating a short output.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --tokenizer meta-llama/Llama-2-7b-chat-hf\n```\n\n----------------------------------------\n\nTITLE: Launching Embedding Model Server\nDESCRIPTION: Launches an SGLang server for an embedding model.  It uses `launch_server_cmd` function to start the server with the `--is-embedding` flag to indicate that it's serving an embedding model. The code waits for the server to be ready using `wait_for_server`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nembedding_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-1.5B-instruct \\\n    --host 0.0.0.0 --is-embedding\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Benchmark SGLang\nDESCRIPTION: Executes the benchmarking script for SGLang. The `--num-questions` argument determines the number of questions used during the benchmark, impacting the evaluation's comprehensiveness. This script measures the performance of the SGLang server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/gsm8k/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 200\n```\n\n----------------------------------------\n\nTITLE: Launch a SGLang Server for dynamic worker addition\nDESCRIPTION: This command launches a SGLang server on port 30001. The model-path argument specifies the pre-trained model to use. This server is used as a worker that can be dynamically added to the SGLang Router.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --port 30001\n```\n\n----------------------------------------\n\nTITLE: Initializing SGLang Engine\nDESCRIPTION: This snippet initializes the SGLang Engine with a specified model path, reasoning parser, and grammar backend.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport sglang as sgl\n\nllm = sgl.Engine(\n    model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n    reasoning_parser=\"deepseek-r1\",\n    grammar_backend=\"xgrammar\",\n)\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Metrics Enabled (Bash)\nDESCRIPTION: This command launches the SGLang server with metrics enabled. It specifies the model path, port, host, and the `--enable-metrics` flag, which is necessary for collecting metrics.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/production_metrics.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n--port 30000 --host 0.0.0.0 --enable-metrics\n```\n\n----------------------------------------\n\nTITLE: Running SGLang Benchmark with llama.cpp Backend (Python)\nDESCRIPTION: This command runs the SGLang benchmark script using the llama.cpp backend. It sets the `OPENAI_BASE_URL` environment variable to point to the llama.cpp server and then runs the `bench_sglang.py` script. It uses GPT-4 Vision Preview and limits the number of questions to 1.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOPENAI_BASE_URL=http://localhost:23000/v1 python3 bench_sglang.py --backend gpt-4-vision-preview --num-q 1\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Regular Expression using SGLang Engine\nDESCRIPTION: This snippet uses a regular expression to constrain the generated text when using the SGLang engine. It defines a regular expression and passes it as the `regex` parameter in `sampling_params` to the `llm.generate` function.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Please provide information about London as a major global city:\",\n    \"Please provide information about Paris as a major global city:\",\n]\n\nsampling_params = {\"temperature\": 0.8, \"top_p\": 0.95, \"regex\": \"(France|England)\"}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print(\"===============================\")\n    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server\nDESCRIPTION: This command launches the SGLang server with the specified model (meta-llama/Llama-2-7b-chat-hf) on port 30000. It utilizes the sglang.launch_server module.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_deep/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server in Multi-Node Docker Setup (Bash)\nDESCRIPTION: These commands launch SGLang servers within Docker containers on two nodes, setting up a multi-node distributed processing environment.  Environment variables, shared memory, and network configurations are critical. `--host 0.0.0.0` allows external access to the container.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# node 1\ndocker run --gpus all \\\n    --shm-size 32g \\\n    --network=host \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --name sglang_multinode1 \\\n    -it \\\n    --rm \\\n    --env \"HF_TOKEN=$HF_TOKEN\" \\\n    --ipc=host \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --tp 16 --dist-init-addr 192.168.114.10:20000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 40000\n```\n\nLANGUAGE: bash\nCODE:\n```\n# node 2\ndocker run --gpus all \\\n    --shm-size 32g \\\n    --network=host \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --name sglang_multinode2 \\\n    -it \\\n    --rm \\\n    --env \"HF_TOKEN=$HF_TOKEN\" \\\n    --ipc=host \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --tp 16 --dist-init-addr 192.168.114.10:20000 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 40000\n```\n\n----------------------------------------\n\nTITLE: Serving Outlines with vLLM\nDESCRIPTION: This command serves the Outlines framework using vLLM. It specifies the model as `meta-llama/Llama-2-7b-chat-hf`, disables log requests, and sets the port to 21000. This setup is necessary to run Outlines benchmarks.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m outlines.serve.serve --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf  --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Benchmark Offline Processing Throughput (Bash)\nDESCRIPTION: This command benchmarks offline processing using `sglang.bench_offline_throughput`. The script starts an offline engine and runs the benchmark with the specified model and number of prompts.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.bench_offline_throughput --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --num-prompts 10\n```\n\n----------------------------------------\n\nTITLE: Making API Request to /generate endpoint\nDESCRIPTION: This snippet makes an API request to the `/generate` endpoint of a local SGLang server. It includes a text prompt and sampling parameters like temperature, max_new_tokens, and json_schema to control the output. The response from the server is printed to the console.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": text,\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 2048,\n            \"json_schema\": json.dumps(CapitalInfo.model_json_schema()),\n        },\n    },\n)\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI-like Client in Python\nDESCRIPTION: This code snippet initializes an OpenAI-like client, specifying the API key and base URL, and retrieves the model name from the server. This allows interacting with the SGLang server using the OpenAI API.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize OpenAI-like client\nclient = OpenAI(api_key=\"None\", base_url=f\"http://0.0.0.0:{port}/v1\")\nmodel_name = client.models.list().data[0].id\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What is 1+3?\",\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server on Four Nodes (Bash)\nDESCRIPTION: These commands launch the SGLang server on four nodes for distributed processing using the BF16 converted model.  It specifies the model path, sets up tensor parallelism (`tp`), initializes distributed communication using `dist-init-addr`, indicates the number of nodes (`nnodes`), and assigns a rank to each node (`node-rank`).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# node 1\npython3 -m sglang.launch_server --model-path /path/to/DeepSeek-V3-BF16 --tp 32 --dist-init-addr 10.0.0.1:5000 --nnodes 4 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 30000\n```\n\nLANGUAGE: bash\nCODE:\n```\n# node 2\npython3 -m sglang.launch_server --model-path /path/to/DeepSeek-V3-BF16 --tp 32 --dist-init-addr 10.0.0.1:5000 --nnodes 4 --node-rank 1 --trust-remote-code\n```\n\nLANGUAGE: bash\nCODE:\n```\n# node 3\npython3 -m sglang.launch_server --model-path /path/to/DeepSeek-V3-BF16 --tp 32 --dist-init-addr 10.0.0.1:5000 --nnodes 4 --node-rank 2 --trust-remote-code\n```\n\nLANGUAGE: bash\nCODE:\n```\n# node 4\npython3 -m sglang.launch_server --model-path /path/to/DeepSeek-V3-BF16 --tp 32 --dist-init-addr 10.0.0.1:5000 --nnodes 4 --node-rank 3 --trust-remote-code\n```\n\n----------------------------------------\n\nTITLE: Benchmark SGLang\nDESCRIPTION: These commands run the bench_sglang.py script to benchmark SGLang with different configurations. The first command runs with 32 questions. The second command runs with 16 questions in parallel mode.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_deep/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_sglang.py --num-questions 32\n```\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_sglang.py --num-questions 16 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Container with AMD GPU\nDESCRIPTION: This command runs a Docker container with AMD GPU support using the specified sglang image.  It mounts a volume for sharing Hugging Face model weights cache. The command uses `/tmp/huggingface` as an example. This configuration allows access to all AMD GPUs.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/setup_github_runner.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# AMD\ndocker run --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 128g -it -v /tmp/huggingface:/hf_home lmsysorg/sglang:v0.4.5.post3-rocm630 /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server with TorchAO int8dq and Disable Cuda Graph\nDESCRIPTION: Launches the SGLang server with `torchao` `int8dq` quantization and disables CUDA graph capture due to known issues. The `--disable-cuda-graph` flag ensures stability when using `int8dq`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server \\\n    --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n    --torchao-config int8dq \\\n    --disable-cuda-graph \\\n    --port 30000 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server with Online FP8 Quantization\nDESCRIPTION: Launches the SGLang server with online FP8 quantization enabled. The `--quantization fp8` flag enables dynamic quantization during runtime.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server \\\n    --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n    --quantization fp8 \\\n    --port 30000 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Remove Worker from SGLang Router (Bash)\nDESCRIPTION: This code snippet demonstrates how to remove a worker from the SGLang Router using a curl command. It sends a POST request to the `/remove_worker` endpoint with the worker's URL as a parameter. The router should be running and accessible at `localhost:30000`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/v0.1.0.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ curl -X POST http://localhost:30000/remove_worker?url=http://worker_url_1\n```\n\n----------------------------------------\n\nTITLE: API Request with Direct JSON Schema\nDESCRIPTION: This code snippet illustrates making an API request using a directly defined JSON schema. It constructs the schema inline and sends it as part of the sampling parameters in the POST request to the `/generate` endpoint. The response from the server is then printed to the console.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\njson_schema = json.dumps(\n    {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n            \"population\": {\"type\": \"integer\"},\n        },\n        \"required\": [\"name\", \"population\"],\n    }\n)\n\n# JSON\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": text,\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 64,\n            \"json_schema\": json_schema,\n        },\n    },\n)\n\nprint_highlight(response.json())\n```\n\n----------------------------------------\n\nTITLE: Benchmarking LightLLM\nDESCRIPTION: This command runs the benchmark script with a specified number of questions using the lightllm backend. Requires the bench_other.py script and a running lightllm server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_chain_reasoning/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 64 --backend lightllm\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Regular Expression Constraint\nDESCRIPTION: This snippet uses a regular expression to constrain the generated text to either \"France\" or \"England\". It makes a POST request to `/generate` with the `regex` parameter in `sampling_params`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": \"Paris is the capital of\",\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 2048,\n            \"regex\": \"(France|England)\",\n        },\n    },\n)\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Add a worker to the Router via API (Example)\nDESCRIPTION: This command uses `curl` to send a POST request to the `/add_worker` API endpoint of the SGLang Router, adding a worker running on localhost:30001.  It's a concrete example building on the general `/add_worker` usage.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:30000/add_worker?url=http://127.0.0.1:30001\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: This command launches the SGLang server with a specified model path and port. It uses the meta-llama/Llama-2-7b-chat-hf model and runs on port 30000. The server must be running before benchmarks can be executed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llm_judge/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Querying SGLang server using Python Requests\nDESCRIPTION: This example demonstrates how to send a request to the SGLang server using the Python requests library. It constructs a JSON payload similar to the cURL example, including the model, messages (containing text and an image URL), and max_tokens, and sends it as a POST request.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_vision.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = f\"http://localhost:{port}/v1/chat/completions\"\n\ndata = {\n    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n                    },\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 300,\n}\n\nresponse = requests.post(url, json=data)\nprint_highlight(response.text)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking LMQL character generation\nDESCRIPTION: This command benchmarks the LMQL framework for character generation. It executes the `bench_other.py` script with the `--backend lmql` option, specifying parallel requests.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_other.py --mode character --backend lmql --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Running Llama3.1 with SGLang in Docker\nDESCRIPTION: This snippet illustrates launching the SGLang server with the Llama3.1 model inside a Docker container. It mirrors the DeepSeek-V3 example, but changes the model path to `meta-llama/Meta-Llama-3.1-8B-Instruct`. The parameters include port mapping, Hugging Face cache mounting, token passing, model specification, tensor parallelism setup (`--tp 8`), trust remote code, host, and port configuration.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/amd.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndrun -p 30000:30000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    --env \"HF_TOKEN=<secret>\" \\\n    sglang_image \\\n    python3 -m sglang.launch_server \\\n    --model-path meta-llama/Meta-Llama-3.1-8B-Instruct # <- here\n    --tp 8 \\\n    --trust-remote-code \\\n    --host 0.0.0.0 \\\n    --port 30000\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM API Server\nDESCRIPTION: This command launches the vLLM API server with a specified model, tokenizer mode, and GPU allocation. It disables request logging and runs on port 21000.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_document_qa/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model codellama/CodeLlama-7b-instruct-hf  --disable-log-requests --port 21000 --gpu 0.97\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server on Multiple Nodes with Tensor Parallelism\nDESCRIPTION: These commands launch the SGLang server on two different nodes (Node 0 and Node 1) with tensor parallelism enabled. The `--dist-init-addr` argument specifies the address for distributed initialization, `--nnodes 2` indicates the number of nodes, and `--node-rank` specifies the rank of each node. `--tp 4` sets the tensor parallelism size to 4.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/server_arguments.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Node 0\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --tp 4 --dist-init-addr sgl-dev-0:50000 --nnodes 2 --node-rank 0\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Node 1\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --tp 4 --dist-init-addr sgl-dev-0:50000 --nnodes 2 --node-rank 1\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM Server\nDESCRIPTION: Launches the vLLM API server with specified model and configuration options.  It disables request logging and sets the port to 21000. Requires the vllm package to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_v0/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang\nDESCRIPTION: This command runs a benchmark test on the SGLang server. It specifies the number of questions and the level of parallelism. The script `bench_sglang.py` is executed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_document_qa/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython3 bench_sglang.py --num-questions 10 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Router with Data Parallelism\nDESCRIPTION: This command launches the SGLang router with data parallelism enabled.  The `--dp-size` parameter specifies the degree of data parallelism. Using `sglang_router` requires a separate installation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/reasoning_benchmark/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang_router.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --port 30000 --dp-size 4\n```\n\n----------------------------------------\n\nTITLE: Launch vLLM Server\nDESCRIPTION: Launches the vLLM server with specific configurations.  The `--tokenizer-mode auto` enables automatic tokenizer selection, `--model` specifies the model to use, `--disable-log-requests` prevents request logging, and `--port` sets the server port. This command initializes the vLLM inference engine.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/gsm8k/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang with Docker for DeepSeek V3\nDESCRIPTION: This command pulls the latest SGLang Docker image and launches a server for the DeepSeek V3 model.  It maps port 30000, mounts the Hugging Face cache, and utilizes shared memory. GPU access is configured with `--gpus all`.  `--trust-remote-code` is required to run custom model code.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Pull latest image\n# https://hub.docker.com/r/lmsysorg/sglang/tags\ndocker pull lmsysorg/sglang:latest\n\n# Launch\ndocker run --gpus all --shm-size 32g -p 30000:30000 -v ~/.cache/huggingface:/root/.cache/huggingface --ipc=host --network=host --privileged lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code --port 30000\n```\n\n----------------------------------------\n\nTITLE: SkyPilot YAML configuration for SGLang\nDESCRIPTION: This YAML file configures SkyPilot for deploying SGLang on a Kubernetes cluster or cloud provider. It defines environment variables, resources (image, accelerator, ports), and the run command to launch the SGLang server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/start/install.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# sglang.yaml\nenvs:\n  HF_TOKEN: null\n\nresources:\n  image_id: docker:lmsysorg/sglang:latest\n  accelerators: A100\n  ports: 30000\n\nrun: |\n  conda deactivate\n  python3 -m sglang.launch_server \\\n    --model-path meta-llama/Llama-3.1-8B-Instruct \\\n    --host 0.0.0.0 \\\n    --port 30000\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang EAGLE Server (Python)\nDESCRIPTION: This command launches the SGLang server with EAGLE speculative decoding enabled. It uses the meta-llama/Meta-Llama-3-8B-Instruct model and the lmsys/sglang-EAGLE-LLaMA3-Instruct-8B draft model. Various parameters control the speculative decoding process, such as the number of steps, top-k selection, and number of draft tokens. The server also utilizes float16 data type and listens on port 30000.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mtbench/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 -m sglang.launch_server --model meta-llama/Meta-Llama-3-8B-Instruct --speculative-algo EAGLE \\\n    --speculative-draft lmsys/sglang-EAGLE-LLaMA3-Instruct-8B --speculative-num-steps 5 \\\n    --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --dtype float16 --port 30000\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance\nDESCRIPTION: This command executes a benchmark using the Guidance backend. It sets the CUDA device visibility, number of questions, parallelism, context size, and model path. The model path should point to a GGUF model.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hellaswag/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1 python3 bench_other.py --num-questions 200 --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Enabling DeepSeek MTP with SGLang (Python)\nDESCRIPTION: This Python command demonstrates how to launch the SGLang server with DeepSeek V3 MTP enabled. It includes arguments for speculative decoding algorithm, draft model path, number of steps, top-k value for EAGLE, and number of draft tokens. `--trust-remote-code` is required because the draft model needs custom operators to work.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deepseek.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3-0324 --speculative-algorithm EAGLE --speculative-draft-model-path lmsys/DeepSeek-V3-0324-NextN --speculative-num-steps 1 --speculative-eagle-topk 1 --speculative-num-draft-tokens 2 --trust-remote-code --tp 8\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server (Python)\nDESCRIPTION: This command launches the SGLang server using the CodeLlama-7b-instruct-hf model. It specifies the model path and the port number for the server to listen on.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/long_json_decode/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython3 -m sglang.launch_server --model-path codellama/CodeLlama-7b-instruct-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Benchmark vLLM\nDESCRIPTION: This command runs the bench_other.py script to benchmark vLLM with 32 questions. It specifies vllm as the backend.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_deep/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_other.py --num-questions 32 --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Generating JSON Directly with JSON Schema\nDESCRIPTION: This snippet generates JSON using a directly defined JSON schema. It defines a schema for an object with `name` and `population` properties, then uses the SGLang engine to generate text based on the schema, printing the generated output for each prompt.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Give me the information of the capital of China in the JSON format.\",\n    \"Give me the information of the capital of France in the JSON format.\",\n    \"Give me the information of the capital of Ireland in the JSON format.\",\n]\n\njson_schema = json.dumps(\n    {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n            \"population\": {\"type\": \"integer\"},\n        },\n        \"required\": [\"name\", \"population\"],\n    }\n)\n\nsampling_params = {\"temperature\": 0, \"max_new_tokens\": 2048, \"json_schema\": json_schema}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print(\"===============================\")\n    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM Server\nDESCRIPTION: These commands launch the vLLM server using the `vllm.entrypoints.openai.api_server` module. The parameters include setting the model path, disabling log requests, and setting the tensor parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Meta-Llama-3-8B-Instruct\npython -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-8B-Instruct --disable-log-requests\n\n# meta-llama/Meta-Llama-3-70B-Instruct\npython -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-70B-Instruct --disable-log-requests --tensor 8\n\n# neuralmagic/Meta-Llama-3-70B-Instruct-FP8\npython -m vllm.entrypoints.openai.api_server --model neuralmagic/Meta-Llama-3-70B-Instruct-FP8 --disable-log-requests --tensor 8\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang on AIME 2024 dataset\nDESCRIPTION: This command benchmarks SGLang on the AIME 2024 dataset. It specifies the data path (`--data-path`), question key (`--question-key`), answer key (`--answer-key`), number of tries (`--num-tries`), the degree of parallelism (`--parallel`) and the port the server is running on (`--port`).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/reasoning_benchmark/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_sglang.py --parallel 256 --port 30000 --data-path Maxwell-Jia/AIME_2024 --question-key Problem --answer-key Answer --num-tries 64\n```\n\n----------------------------------------\n\nTITLE: Benchmark Online Serving (Bash)\nDESCRIPTION: This command benchmarks online serving using `sglang.bench_serving`. It requires a server launched with `sglang.launch_server`. The script measures the performance of the server with the specified backend and number of prompts.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.bench_serving --backend sglang --num-prompt 10\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Memory Fraction Static Configuration\nDESCRIPTION: This command launches the SGLang server and reduces the memory usage of the KV cache pool by setting `--mem-fraction-static` to `0.7`. This is useful for mitigating out-of-memory errors during serving by adjusting the memory allocation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/server_arguments.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --mem-fraction-static 0.7\n```\n\n----------------------------------------\n\nTITLE: Add Worker to SGLang Router (Bash)\nDESCRIPTION: This code snippet demonstrates how to add a worker to the SGLang Router using a curl command. It sends a POST request to the `/add_worker` endpoint with the worker's URL as a parameter.  The router should be running and accessible at `localhost:30000`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/v0.1.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ curl -X POST http://localhost:30000/add_worker?url=http://worker_url_1\n```\n\n----------------------------------------\n\nTITLE: Launch vLLM Server (Python)\nDESCRIPTION: This command launches the vLLM server using the specified model. It uses the meta-llama/Llama-2-7b-chat-hf model. The --tokenizer-mode auto argument sets the tokenizer mode to auto, and --disable-log-requests disables request logging. The server listens on port 21000.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mtbench/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Benchmarking vLLM\nDESCRIPTION: Runs the benchmark script against the vLLM backend. This command evaluates the performance of vLLM using a specified number of questions. The `--backend` argument specifies the backend framework, and `--num-questions` defines the number of questions.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/react/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 100 --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Shutting Down SGLang Engine\nDESCRIPTION: This code snippet shuts down the SGLang engine, releasing any resources that it may be holding. This is important to do at the end of a program to ensure that the engine is properly closed and to prevent any potential resource leaks. Call the shutdown method when the llm object is no longer needed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/offline_engine_api.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nllm.shutdown()\n\n```\n\n----------------------------------------\n\nTITLE: Benchmarking vLLM\nDESCRIPTION: This command executes the benchmark script using vLLM as the backend. It runs the benchmark with 25 questions. The script measures the performance of vLLM.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llm_judge/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --backend vllm --num-questions 25\n```\n\n----------------------------------------\n\nTITLE: Flushing Cache with /flush_cache API\nDESCRIPTION: Flushes the radix cache using the `/flush_cache` API. This is useful after updating model weights. The code sends a POST request to the endpoint and prints the server's response.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# flush cache\n\nurl = f\"http://localhost:{port}/flush_cache\"\n\nresponse = requests.post(url)\nprint_highlight(response.text)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Accuracy and Latency (Bash)\nDESCRIPTION: These commands benchmark the accuracy and latency of the deployed SGLang server.  `bench_sglang.py` measures accuracy, while `bench_one_batch_server.py` measures latency.  The target server's address and port are specified using `--host` and `--port`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# bench accuracy\npython3 benchmark/gsm8k/bench_sglang.py --num-questions 1319 --host http://10.0.0.1 --port 30000\n```\n\nLANGUAGE: bash\nCODE:\n```\n# bench latency\npython3 -m sglang.bench_one_batch_server --model None --base-url http://10.0.0.1:30000 --batch-size 1 --input-len 128 --output-len 128\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang server\nDESCRIPTION: This command launches an SGLang server using the `sglang.launch_server` module. The `--model-path` argument specifies the path to the codellama/CodeLlama-7b-hf model. The `--port` argument sets the server's port number to 30000.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/line_retrieval/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython3 -m sglang.launch_server --model-path codellama/CodeLlama-7b-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang server with multiple LoRA adapters\nDESCRIPTION: This snippet launches an SGLang server with a specified base model and multiple LoRA adapters. It defines the LoRA paths for two adapters, sets the maximum number of LoRAs per batch to 2, uses the Triton backend, and disables CUDA graph and radix cache for compatibility. The command is executed using `launch_server_cmd` from `sglang.utils` or `patch` (in CI environment).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/lora.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nserver_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n    --lora-paths lora0=algoprog/fact-generation-llama-3.1-8b-instruct-lora \\\n    lora1=Nutanix/Meta-Llama-3.1-8B-Instruct_lora_4_alpha_16 \\\n    --max-loras-per-batch 2 --lora-backend triton \\\n    --disable-cuda-graph --disable-radix-cache\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang server\nDESCRIPTION: Launches the SGLang server with a specified model path and port. This command is essential for running benchmarks against the SGLang backend.  It specifies the model to be used and the port on which the server will listen for requests.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/dspy/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Running SGLang Benchmark (Python)\nDESCRIPTION: This command executes the SGLang benchmark script. It uses the `bench_sglang.py` script and allows specifying the number of questions and the backend to use. The backend can be either a local model or a remote service like GPT-4 Vision Preview.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Run with local models\npython3 bench_sglang.py --num-questions 60\n\n# Run with OpenAI models\npython3 bench_sglang.py --num-questions 60 --backend gpt-4-vision-preview\n```\n\n----------------------------------------\n\nTITLE: SGLang Client Script for Profiling\nDESCRIPTION: This script (client.sh) starts and stops profiling on the SGLang server using curl commands, benchmarks the server using a random dataset and tokenizer, and converts tracing files to CSV and JSON formats for analysis.  It uses sqlite3 to convert the trace.rpd file to a CSV and a python script to convert it to JSON.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n\n# Start profiling via API\ncurl http://localhost:30000/start_profile -H \"Content-Type: application/json\"\n\n# Benchmark serving using sglang with random dataset and tokenizer\n# Define the log file with a timestamp\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nLOGFILE=\"sglang_client_log_$TIMESTAMP.json\"\n\n# Run the benchmark with specified parameters and save logs\npython3 -m sglang.bench_serving \\\n    --backend sglang \\\n    --tokenizer Xenova/grok-1-tokenizer \\\n    --dataset-name random \\\n    --random-input 1024\\\n    --random-output 1024 \\\n    --num-prompts 120 \\\n    --request-rate 8 \\\n    --output-file online.jsonl 2>&1 | tee \"$LOGFILE\"\n\n# Stop profiling via API\ncurl http://localhost:30000/stop_profile -H \"Content-Type: application/json\"\n\n# Convert tracing file to csv & json\nsqlite3 trace.rpd \".mode csv\" \".header on\" \".output trace.csv\" \"select * from top;\" \".output stdout\"\npython3 ./rocmProfileData/tools/rpd2tracing.py trace.rpd trace.json\n\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Structural Tags using SGLang Engine\nDESCRIPTION: This snippet generates text with structural tags using the SGLang engine directly. It applies a chat template, then calls `llm.generate` with `structural_tag` defined in the `sampling_params`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntext = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nprompts = [text]\n\n\nsampling_params = {\n    \"temperature\": 0.8,\n    \"top_p\": 0.95,\n    \"max_new_tokens\": 2048,\n    \"structural_tag\": json.dumps(\n        {\n            \"type\": \"structural_tag\",\n            \"structures\": [\n                {\n                    \"begin\": \"<function=get_current_weather>\",\n                    \"schema\": schema_get_current_weather,\n                    \"end\": \"</function>\",\n                },\n                {\n                    \"begin\": \"<function=get_current_date>\",\n                    \"schema\": schema_get_current_date,\n                    \"end\": \"</function>\",\n                },\n            ],\n            \"triggers\": [\"<function=\"],\n        }\n    ),\n}\n\n\n# Send POST request to the API endpoint\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print(\"===============================\")\n    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Debugging SGLang Output\nDESCRIPTION: This bash script compares the outputs of Hugging Face/Transformers and SGLang for interactive debugging. It helps ensure that the SGLang implementation produces the same text output and similar prefill logits as the reference Hugging Face model.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/supported_models/support_new_models.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 scripts/playground/reference_hf.py --model-path [new model] --model-type {text,vlm}\n```\n\n----------------------------------------\n\nTITLE: Sending Chat Completion Request using cURL\nDESCRIPTION: Sends a chat completion request to the SGLang server using `curl`. It constructs a `curl` command with the necessary headers and JSON payload, then executes it using `subprocess`. The JSON response is parsed and printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/send_request.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport subprocess, json\n\ncurl_command = f\"\"\"\ncurl -s http://localhost:{port}/v1/chat/completions \\\n  -H \\\"Content-Type: application/json\\\" \\\n  -d '{{\\\"model\\\": \\\"qwen/qwen2.5-0.5b-instruct\\\", \\\"messages\\\": [{{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the capital of France?\\\"}}]}}'\n\"\"\"\n\nresponse = json.loads(subprocess.check_output(curl_command, shell=True))\nprint_highlight(response)\n```\n\n----------------------------------------\n\nTITLE: Removing a worker from the Router via API\nDESCRIPTION: This command uses `curl` to send a POST request to the `/remove_worker` API endpoint of the SGLang Router. The `url` parameter specifies the URL of the worker to be removed. This allows dynamically scaling down the number of workers.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:30000/remove_worker?url=http://worker_url_1\n```\n\n----------------------------------------\n\nTITLE: Launch LMQL Server (LMQL)\nDESCRIPTION: Launches the LMQL server with a specified model and CUDA device.  The `CUDA_VISIBLE_DEVICES` environment variable specifies the CUDA devices to use, and the `lmql serve-model` command starts the server with the specified model, CUDA support, and port.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1 lmql serve-model meta-llama/Llama-2-7b-chat-hf --cuda --port 23000\n```\n\n----------------------------------------\n\nTITLE: Offline Engine API: Generate Text\nDESCRIPTION: Demonstrates using the Offline Engine API to generate text without needing a running server. The code initializes an `sgl.Engine`, prepares the input using `tokenizer.apply_chat_template`, sets the sampling parameters, then uses `llm.generate` to generate text based on the input and sampling configuration, finally printing the generated text.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport sglang as sgl\nfrom sglang.srt.function_call_parser import FunctionCallParser\nfrom sglang.srt.managers.io_struct import Tool, Function\n\nllm = sgl.Engine(model_path=\"Qwen/Qwen2.5-7B-Instruct\")\ntokenizer = llm.tokenizer_manager.tokenizer\ninput_ids = tokenizer.apply_chat_template(\n    messages, tokenize=True, add_generation_prompt=True, tools=tools\n)\n\nsampling_params = {\n    \"max_new_tokens\": 1024,\n    \"temperature\": 0.1,\n    \"top_p\": 0.95,\n    \"skip_special_tokens\": False,\n}\n\n# 1) Offline generation\nresult = llm.generate(input_ids=input_ids, sampling_params=sampling_params)\ngenerated_text = result[\"text\"]  # Assume there is only one prompt\n\nprint(\"=== Offline Engine Output Text ===\")\nprint(generated_text)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Logit Processor in SGLang\nDESCRIPTION: This Python code demonstrates how to use the custom logit processor defined earlier with SGLang. It sends a POST request to the SGLang server with the input text, the string representation of the custom logit processor, and custom parameters specifying the token ID to always sample. The `custom_params` dictionary passes the `token_id` to the logit processor.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/sampling_params.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresponse = requests.post(\n    \"http://localhost:30000/generate\",\n    json={\n        \"text\": \"The capital of France is\",\n        \"custom_logit_processor\": DeterministicLogitProcessor().to_str(),\n        \"sampling_params\": {\n            \"temperature\": 0.0,\n            \"max_new_tokens\": 32,\n            \"custom_params\": {\"token_id\": 5},\n        },\n    },\n)\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server\nDESCRIPTION: Launches the SGLang server with a specified model and port. This command initializes the SGLang service, making it ready to receive requests. The `--model-path` argument specifies the path to the model, and `--port` sets the port for the server to listen on.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/gsm8k/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Set Jetson Orin to High-Performance Mode (Bash)\nDESCRIPTION: Sets the NVIDIA Jetson AGX Orin to high-performance mode. This is crucial for maximizing performance when running SGLang.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/nvidia_jetson.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo nvpmodel -m 0\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with EAGLE-2 and Frequency-Ranked Sampling\nDESCRIPTION: This code snippet launches an SGLang server with EAGLE-2 speculative decoding using a frequency-ranked token vocabulary. It sets the draft model path, number of steps, top-k branching factor, number of draft tokens, and also specifies a token map for frequency-ranked speculative sampling. It also sets the memory fraction, CUDA graph maximum batch size, and data type.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/speculative_decoding.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nserver_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model meta-llama/Meta-Llama-3-8B-Instruct --speculative-algorithm EAGLE \\\n    --speculative-draft-model-path lmsys/sglang-EAGLE-LLaMA3-Instruct-8B --speculative-num-steps 5 \\\n    --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --speculative-token-map thunlp/LLaMA3-Instruct-8B-FR-Spec/freq_32768.pt \\\n    --mem-fraction 0.7 --cuda-graph-max-bs 2 --dtype float16 \n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Running Shared Prefix Benchmark with Specific Parameters (Python)\nDESCRIPTION: This snippet shows how to run the `bench_serving.py` script for a shared prefix benchmark with specific parameters. It configures the model, backend, dataset path and name, request rate, number of prompts, port, enables shared prefix, and disables shuffling of conversations.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hicache/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_serving.py --model mistralai/Mistral-7B-Instruct-v0.3 --backend sglang \\\n--dataset-path longdep_qa.json --dataset-name loogle --request-rate 10 --num-prompts 10  \\\n--port 8001 --enable-shared-prefix --disable-shuffle\n```\n\n----------------------------------------\n\nTITLE: Setting ModelScope Environment Variable (bash)\nDESCRIPTION: Sets the environment variable SGLANG_USE_MODELSCOPE to true, enabling the use of models from ModelScope within SGLang. This variable informs SGLang to fetch models from the ModelScope platform. No external dependencies are required.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/modelscope.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport SGLANG_USE_MODELSCOPE=true\n```\n\n----------------------------------------\n\nTITLE: Downloading Benchmark Data\nDESCRIPTION: Downloads the test data required for the grade school math benchmark from a specified URL using wget.  This is a prerequisite step before running any of the benchmarks.  The downloaded data is stored as a JSONL file.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_v0/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF using PyPDF2\nDESCRIPTION: This Python script extracts text from a PDF file (`llama2.pdf`) using the PyPDF2 library. It iterates through each page, extracts the text, and writes the combined text to an output file (`output.txt`).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_document_qa/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport PyPDF2\n\nwith open('llama2.pdf', 'rb') as file:\n    reader = PyPDF2.PdfReader(file)\n    text = ''\n    for page_num in range(len(reader.pages)):\n        text += reader.pages[page_num].extract_text()\n    with open('output.txt', 'w') as text_file:\n        text_file.write(text)\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM Server\nDESCRIPTION: This command launches the vLLM API server with a specified model and disables request logging. It uses the meta-llama/Llama-2-7b-chat-hf model and runs on port 21000. The --tokenizer-mode auto flag allows vLLM to automatically detect the correct tokenizer.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llm_judge/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Benchmark dspy-ai with vLLM\nDESCRIPTION: Runs the dspy-ai benchmark script, specifying vLLM as the backend. This command evaluates the performance of dspy-ai using the vLLM server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/dspy/README.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\npython3 bench_dspy_intro.py --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Benchmarking LMQL\nDESCRIPTION: Runs the benchmark script with the LMQL backend, specifying the number of questions and parallel execution. This command evaluates the performance of LMQL.  The `--backend` argument selects the LMQL framework, and `--parallel` controls the number of parallel processes.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/react/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 100 --backend lmql --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server for Multi-modality Benchmark (Python)\nDESCRIPTION: This snippet demonstrates how to launch the SGLang server for multi-modality benchmarking using LLaVA-NeXT-Video-7B model. It specifies model path, tensor parallelism (tp), data parallelism (dp), port, host, memory fraction static, tokenizer path, and overrides JSON model arguments for architecture and model type.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hicache/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model-path lmms-lab/LLaVA-NeXT-Video-7B  --tp 2 --dp 1 --port 8001 \\\n--host 0.0.0.0 --mem-fraction-static 0.9 --tokenizer-path llava-hf/llava-1.5-7b-hf \\\n--json-model-override-args \"{\\\"architectures\\\": [\\\"LlavaVidForCausalLM\\\"], \\\"model_type\\\":\\\"llava\\\", \\\"mm_spatial_pool_stride\\\":2}\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Function Calling for DeepSeek (Python)\nDESCRIPTION: This Python command shows how to launch the SGLang server with function calling enabled for DeepSeek models. It uses the `--tool-call-parser deepseekv3` argument to enable the feature. Adjust `--tp` based on your hardware.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deepseek.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3-0324 --tp 8 --port 30000 --host 0.0.0.0 --mem-fraction-static 0.9 --disable-cuda-graph --tool-call-parser deepseekv3\n```\n\n----------------------------------------\n\nTITLE: Retrieving Batch Job Results OpenAI (Python)\nDESCRIPTION: This snippet retrieves the results of a completed batch job. It polls the batch job status until it is completed, failed, or cancelled.  Upon completion, it retrieves the results from the output file, parses the JSONL content, and prints the responses. It uses `client.batches.retrieve` to check job status, `client.files.content` to fetch file contents and `client.files.delete` for cleanup. Dependencies: `json`, `time`, `openai`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwhile batch_response.status not in [\"completed\", \"failed\", \"cancelled\"]:\n    time.sleep(3)\n    print(f\"Batch job status: {batch_response.status}...trying again in 3 seconds...\")\n    batch_response = client.batches.retrieve(batch_response.id)\n\nif batch_response.status == \"completed\":\n    print(\"Batch job completed successfully!\")\n    print(f\"Request counts: {batch_response.request_counts}\")\n\n    result_file_id = batch_response.output_file_id\n    file_response = client.files.content(result_file_id)\n    result_content = file_response.read().decode(\"utf-8\")\n\n    results = [\n        json.loads(line) for line in result_content.split(\"\\n\") if line.strip() != \"\"\n    ]\n\n    for result in results:\n        print_highlight(f\"Request {result['custom_id']}:\")\n        print_highlight(f\"Response: {result['response']}\")\n\n    print_highlight(\"Cleaning up files...\")\n    # Only delete the result file ID since file_response is just content\n    client.files.delete(result_file_id)\nelse:\n    print_highlight(f\"Batch job failed with status: {batch_response.status}\")\n    if hasattr(batch_response, \"errors\"):\n        print_highlight(f\"Errors: {batch_response.errors}\")\n```\n\n----------------------------------------\n\nTITLE: Install GPTQModel\nDESCRIPTION: Installs the `gptqmodel` package, which is used for performing GPTQ quantization. The `--no-build-isolation` flag is included during installation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# install\npip install gptqmodel --no-build-isolation -v\n```\n\n----------------------------------------\n\nTITLE: Build SGLang Container (Bash)\nDESCRIPTION: Builds the SGLang Docker container using jetson-containers. The `CUDA_VERSION` environment variable specifies the CUDA version to use for the build.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/nvidia_jetson.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VERSION=12.6 jetson-containers build sglang\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server for Profiling (Bash)\nDESCRIPTION: This command launches an SGLang server with a specified model path, designed for profiling with the PyTorch profiler. The `SGLANG_TORCH_PROFILER_DIR` must be set correctly for trace files to be generated.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Installing SGLang from source (Bash)\nDESCRIPTION: This snippet demonstrates how to install SGLang from source by cloning the repository, navigating to the directory, and installing the necessary dependencies using pip. It includes installing the python package with all dependencies.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/start/install.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Use the last release branch\ngit clone -b v0.4.5.post3 https://github.com/sgl-project/sglang.git\ncd sglang\n\npip install --upgrade pip\npip install -e \"python[all]\"\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang Server - Bash\nDESCRIPTION: Benchmarks the launched SGLang server for accuracy and serving performance. The accuracy benchmark uses the GSM8k dataset, and the serving benchmark uses a ShareGPT dataset.  Requires `sglang` package to be installed and dataset to be available at the specified path. `--parallel` can be used to accelerate accuracy benchmarking.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# bench accuracy\npython3 benchmark/gsm8k/bench_sglang.py --num-questions 1319\n\n# bench serving\npython3 -m sglang.bench_serving --dataset-path /path/to/ShareGPT_V3_unfiltered_cleaned_split.json --dataset-name random  --random-input 128 --random-output 128 --num-prompts 1000 --request-rate 128 --random-range-ratio 1.0\n```\n\n----------------------------------------\n\nTITLE: Benchmark Guidance\nDESCRIPTION: Benchmarks the Guidance framework. `--num-questions` sets the number of questions for the benchmark. `--backend guidance` specifies Guidance as the target. `--parallel 1` indicates a single parallel process. `--n-ctx 4096` sets the context length, and `--model-path` points to the model file.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/gsm8k/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 200 --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang and vLLM Servers (Online Benchmark, Llama 3.1 70B)\nDESCRIPTION: Launches SGLang and vLLM servers for online benchmarking with Llama 3.1 70B Instruct model on 4 H100 GPUs. SGLang disables radix cache and sets the tensor parallel size to 4. vLLM disables request logging, sets the number of scheduler steps, sets tensor parallelism to 4, and sets the maximum model length.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/benchmark_vllm_060/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Llama 3.1 70B Instruct on 4 x H100\npython -m sglang.launch_server --model-path meta-llama/Llama-3.1-70B-Instruct --disable-radix-cache --tp 4\npython -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-3.1-70B-Instruct --disable-log-requests --num-scheduler-steps 10 --tensor 4 --max_model_len 4096\n```\n\n----------------------------------------\n\nTITLE: SGLang Native API Generation and Parsing in Python\nDESCRIPTION: This snippet demonstrates how to use the SGLang native API to generate text and then parse the reasoning content using the `/separate_reasoning` endpoint.  It tokenizes the input, sends a request to the `/generate` endpoint, and then sends the response to the `/separate_reasoning` endpoint.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\ninput = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n\ngen_url = f\"http://localhost:{port}/generate\"\ngen_data = {\n    \"text\": input,\n    \"sampling_params\": {\n        \"skip_special_tokens\": False,\n        \"max_new_tokens\": 1024,\n        \"temperature\": 0.6,\n        \"top_p\": 0.95,\n    },\n}\ngen_response = requests.post(gen_url, json=gen_data).json()[\"text\"]\n\nprint_highlight(\"==== Original Output ====\")\nprint_highlight(gen_response)\n\nparse_url = f\"http://localhost:{port}/separate_reasoning\"\nseparate_reasoning_data = {\n    \"text\": gen_response,\n    \"reasoning_parser\": \"deepseek-r1\",\n}\nseparate_reasoning_response_json = requests.post(\n    parse_url, json=separate_reasoning_data\n).json()\nprint_highlight(\"==== Reasoning ====\")\nprint_highlight(separate_reasoning_response_json[\"reasoning_text\"])\nprint_highlight(\"==== Text ====\")\nprint_highlight(separate_reasoning_response_json[\"text\"])\n```\n\n----------------------------------------\n\nTITLE: Send Non-Streaming Request\nDESCRIPTION: This curl command sends a POST request to the `/generate` endpoint of the custom server. The request includes a JSON payload with a 'prompt' field. The server is expected to respond with a non-streaming generated text.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/readme.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8000/generate  -H \"Content-Type: application/json\"  -d '{\"prompt\": \"The Transformer architecture is...\"}'\n```\n\n----------------------------------------\n\nTITLE: Start Monitoring Stack (Docker Compose)\nDESCRIPTION: This command starts the Prometheus and Grafana monitoring stack using Docker Compose. It reads the `docker-compose.yaml` file in the current directory, which defines the services, networks, and volumes required to run Prometheus and Grafana. Requires Docker and Docker Compose to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/monitoring/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/monitoring\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance\nDESCRIPTION: This command benchmarks Guidance with specified parameters such as the number of questions, parallelism, context length, and model path. It uses the `bench_other.py` script.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_document_qa/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython3 bench_other.py --backend guidance --num-questions 32 --parallel 1 --n-ctx 11000 --model-path path/to/code-llama/gguf\n```\n\n----------------------------------------\n\nTITLE: Launch LightLLM Server\nDESCRIPTION: Launches the LightLLM server with specified configurations. `--tokenizer_mode auto` automatically selects the tokenizer. `--model_dir` points to the model weights directory. `--max_total_token_num` limits the maximum number of tokens, and `--port` sets the server port. This command starts the LightLLM inference engine.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/gsm8k/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython -m lightllm.server.api_server --tokenizer_mode auto --model_dir ~/model_weights/llama-2-7b-chat-hf --max_total_token_num 16000 --port 22000\n```\n\n----------------------------------------\n\nTITLE: Running Multi-turn Benchmark with Specific Parameters (Python)\nDESCRIPTION: This snippet demonstrates how to run the `bench_serving.py` script for a multi-turn benchmark with specific parameters. It configures the model, backend, dataset path and name, request rate, number of prompts, port, enables multi-turn chat, and disables shuffling of conversations.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hicache/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_serving.py --model mistralai/Mistral-7B-Instruct-v0.3 --backend sglang \\\n--dataset-path longdep_qa.json --dataset-name loogle --request-rate 10 --num-prompts 10  \\\n--port 8001 --enable-multiturn --disable-shuffle\n```\n\n----------------------------------------\n\nTITLE: Profile Single Batch with Nsight Systems (Bash)\nDESCRIPTION: This command profiles a single batch using Nsight Systems. It uses `nsys profile` with specific tracing options and executes `sglang.bench_one_batch`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nnsys profile --trace-fork-before-exec=true --cuda-graph-trace=node python3 -m sglang.bench_one_batch --model meta-llama/Meta-Llama-3-8B --batch-size 64 --input-len 512\n```\n\n----------------------------------------\n\nTITLE: Implementing Reasoning Parser Subclass in Python\nDESCRIPTION: This code defines a subclass of `BaseReasoningFormatDetector` for the DeepSeek-R1 model.  It detects the reasoning content wrapped in `<think>` and `</think>` tags.  The `DeepSeekR1Detector` class inherits from `BaseReasoningFormatDetector`, and the `ReasoningParser` class uses a map to find the appropriate detector based on the `model_type`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass DeepSeekR1Detector(BaseReasoningFormatDetector):\n    \"\"\"\n    Detector for DeepSeek-R1 model.\n    Assumes reasoning format:\n      (<think>)*(.*)</think>\n    Returns all the text before the </think> tag as `reasoning_text`\n    and the rest of the text as `normal_text`.\n\n    Args:\n        stream_reasoning (bool): If False, accumulates reasoning content until the end tag.\n            If True, streams reasoning content as it arrives.\n    \"\"\"\n\n    def __init__(self, stream_reasoning: bool = False):\n        # DeepSeek-R1 is assumed to be reasoning until `</think>` token\n        super().__init__(\"<think>\", \"</think>\", True, stream_reasoning=stream_reasoning)\n        # https://github.com/sgl-project/sglang/pull/3202#discussion_r1950153599\n\n\nclass ReasoningParser:\n    \"\"\"\n    Parser that handles both streaming and non-streaming scenarios for extracting\n    reasoning content from model outputs.\n\n    Args:\n        model_type (str): Type of model to parse reasoning from\n        stream_reasoning (bool): If Flase, accumulates reasoning content until complete.\n            If True, streams reasoning content as it arrives.\n    \"\"\"\n\n    DetectorMap: Dict[str, BaseReasoningFormatDetector] = {\n        \"deepseek-r1\": DeepSeekR1Detector\n    }\n\n    def __init__(self, model_type: str = None, stream_reasoning: bool = True):\n        if not model_type:\n            raise ValueError(\"Model type must be specified\")\n\n        detector_class = self.DetectorMap.get(model_type.lower())\n        if not detector_class:\n            raise ValueError(f\"Unsupported model type: {model_type}\")\n\n        self.detector = detector_class(stream_reasoning=stream_reasoning)\n\n    def parse_non_stream(self, full_text: str) -> StreamingParseResult:\n        \"\"\"Non-streaming call: one-time parsing\"\"\"\n        ret = self.detector.detect_and_parse(full_text)\n        return ret.reasoning_text, ret.normal_text\n\n    def parse_stream_chunk(self, chunk_text: str) -> StreamingParseResult:\n        \"\"\"Streaming call: incremental parsing\"\"\"\n        ret = self.detector.parse_streaming_increment(chunk_text)\n        return ret.reasoning_text, ret.normal_text\n```\n\n----------------------------------------\n\nTITLE: SGLang Prompt Alignment Example (Python)\nDESCRIPTION: This Python code snippet demonstrates a basic prompt format for SGLang.  It shows how to structure a prompt with clear User and Assistant roles to ensure proper alignment with the expected behavior of the SGLang models. The assistant is instructed to answer concisely and accurately.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"\"\"You are an AI assistant. Answer concisely and accurately.\n\nUser: What is the capital of France?\nAssistant: The capital of France is Paris.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Benchmarking vLLM (short output) (Python)\nDESCRIPTION: This command runs the bench_other.py script to benchmark vLLM with the Llama-2-7b-chat-hf tokenizer, generating a short output.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --tokenizer meta-llama/Llama-2-7b-chat-hf --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Installing antlr4-python3-runtime\nDESCRIPTION: This command installs the `antlr4-python3-runtime` package, which is a dependency for `parse_latex` used in symbolic equality checks during benchmarking.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/reasoning_benchmark/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install antlr4-python3-runtime\n```\n\n----------------------------------------\n\nTITLE: Starting Triton Server in Docker\nDESCRIPTION: This command starts the Triton server inside the Docker container. It sets the model repository to `/mnt/models`, which is where the models are mounted from the host machine. Requires Triton server to be installed in the container, and assumes models are present in the `/mnt/models` directory.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/triton/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -ti sglang-triton /bin/bash\ncd /mnt\ntritonserver --model-repository=/mnt/models\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM API Server\nDESCRIPTION: Starts the vLLM API server with specified configurations, including tokenizer mode, model path, and disabling request logs. This command is necessary for benchmarking vLLM. The server listens on a specified port.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/react/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Torch Native Backend\nDESCRIPTION: This command launches the SGLang server using the Torch Native attention backend. It specifies the model to use.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/attention_backend.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model meta-llama/Meta-Llama-3.1-8B-Instruct --attention-backend torch_native\n```\n\n----------------------------------------\n\nTITLE: Launching Reward Model Server - SGLang\nDESCRIPTION: Launches an SGLang server with a specified reward model. This command sets the model path, enables embedding mode, specifies the host and port, and configures tensor parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/supported_models/reward_models.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m sglang.launch_server \\\n  --model-path Qwen/Qwen2.5-Math-RM-72B \\  # example HF/local path\n  --is-embedding \\\n  --host 0.0.0.0 \\\n  --tp-size=4 \\                          # set for tensor parallelism\n  --port 30000 \\\n```\n\n----------------------------------------\n\nTITLE: Running DeepGemm FP8 GEMM Benchmark (bash)\nDESCRIPTION: This command executes the `benchmark_deepgemm_fp8_gemm.py` script with correctness verification enabled and sets the tensor parallelism size to 1. It benchmarks FP8 GEMM operations using DeepGemm and compares its performance with other implementations.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/kernels/deepseek/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark_deepgemm_fp8_gemm.py --run_correctness --tp_size 1\n```\n\n----------------------------------------\n\nTITLE: Exploring the Relationship Between Num_tries and SE\nDESCRIPTION: This command is run with varying `--num-tries` parameters. The purpose of running the command multiple times is to analyze the relationship between standard error and the number of attempts. The command specifies the data path (`--data-path`), question key (`--question-key`), answer key (`--answer-key`), number of tries (`--num-tries`), the degree of parallelism (`--parallel`) and the port the server is running on (`--port`).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/reasoning_benchmark/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_sglang.py --parallel 64 --port 30000 --data-path Maxwell-Jia/AIME_2024 --question-key Problem --answer-key Answer --num-tries <num_tries>\n```\n\n----------------------------------------\n\nTITLE: Querying SGLang Metrics\nDESCRIPTION: This code snippet demonstrates how to query SGLang metrics using curl. It fetches the metrics from the /metrics endpoint, which can then be ingested by Prometheus for monitoring and analysis.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/production_metrics.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ curl http://localhost:30000/metrics\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance Llama-7B (long output) (Python)\nDESCRIPTION: This command benchmarks Guidance with Llama-7B using bench_other.py. It specifies the tokenizer, backend, parallelization, context window size, and the path to the GGUF model. A long output is generated.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --tokenizer meta-llama/Llama-2-7b-chat-hf --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf --long\n```\n\n----------------------------------------\n\nTITLE: Compiling and Serving Documentation Locally (Make)\nDESCRIPTION: This command compiles the documentation and serves it locally using the make command. It is an alternative way to serve the documentation, potentially with custom configurations defined in the Makefile. This is an alternative to the `bash serve.sh` command.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake serve\n```\n\n----------------------------------------\n\nTITLE: Launching Server with Reasoning Parser in Python\nDESCRIPTION: This code snippet demonstrates how to launch the SGLang server with the `--reasoning-parser` option, specifying the model path and the desired reasoning parser. It also waits for the server to start and prints the port number.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom openai import OpenAI\nfrom sglang.test.test_utils import is_in_ci\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\n\nserver_process, port = launch_server_cmd(\n    \"python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --host 0.0.0.0 --reasoning-parser deepseek-r1\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Running SGLang Offline Benchmark\nDESCRIPTION: These commands run the SGLang offline benchmark using the `sglang.bench_serving` module. The parameters include setting the backend to 'sglang', dataset name to 'random' or 'sharegpt', number of prompts, random input/output sizes, and output file name. Finally, the script extracts a specific column from the output.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 4000 --random-input 1024 --random-output 1024 --output-file offline.jsonl\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 5000 --random-input 1024 --random-output 512 --output-file offline.jsonl\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 1000 --random-input 4096 --random-output 2048 --output-file offline.jsonl\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 2000 --random-input 4096 --random-output 1024 --output-file offline.jsonl\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 6000 --random-input 256 --random-output 512 --output-file offline.jsonl\npython3 -m sglang.bench_serving --backend sglang --dataset-name sharegpt --num-prompts 3000 --output-file offline.jsonl\ncat offline.jsonl | cut -d':' -f12 | cut -d',' -f1\n```\n\n----------------------------------------\n\nTITLE: Defining the Retrieval Step - Python\nDESCRIPTION: This function defines the retrieval step of the RAG pipeline. It takes a question as input, queries the Chroma collection for relevant contexts, and returns the retrieved documents. The `@trace` decorator from Parea automatically traces inputs, outputs, and latency of the function.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@trace\ndef retrieval(question: str) -> List[str]:\n    return collection.query(query_texts=[question], n_results=1)[\"documents\"][0]\n```\n\n----------------------------------------\n\nTITLE: Benchmarking vLLM\nDESCRIPTION: This command executes the `bench_other.py` script to benchmark vLLM with a specified number of events and parallel execution. The `--backend` parameter specifies the backend as vllm.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/generative_agents/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-events 1000 --backend vllm --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Benchmark Guidance\nDESCRIPTION: This command benchmarks Guidance using the bench_other.py script. It sets the backend to guidance, runs the benchmark with 32 questions and parallelism of 1, sets the context length to 4096, and specifies the path to the GGUF model file. It relies on having a guidance compatible model file and the bench_other script.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tip_suggestion/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_other.py --backend guidance --num-questions 32 --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Benchmark SGLang\nDESCRIPTION: These commands run the SGLang benchmark script (bench_sglang.py). The first command benchmarks with 64 questions. The second command benchmarks with 32 questions and a parallelism of 1. The number of questions and parallelism affect the benchmark runtime and results.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tip_suggestion/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_sglang.py --num-questions 64\n```\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_sglang.py --num-questions 32 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: SGLang Prometheus Metrics\nDESCRIPTION: This section displays the metrics exposed by SGLang in Prometheus format. These metrics include token usage, cache hit rate, time to first token, end-to-end request latency, and time per output token.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/production_metrics.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n# HELP sglang:prompt_tokens_total Number of prefill tokens processed.\n# TYPE sglang:prompt_tokens_total counter\nsglang:prompt_tokens_total{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 8.128902e+06\n# HELP sglang:generation_tokens_total Number of generation tokens processed.\n# TYPE sglang:generation_tokens_total counter\nsglang:generation_tokens_total{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.557572e+06\n# HELP sglang:token_usage The token usage\n# TYPE sglang:token_usage gauge\nsglang:token_usage{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.28\n# HELP sglang:cache_hit_rate The cache hit rate\n# TYPE sglang:cache_hit_rate gauge\nsglang:cache_hit_rate{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.007507552643049313\n# HELP sglang:time_to_first_token_seconds Histogram of time to first token in seconds.\n# TYPE sglang:time_to_first_token_seconds histogram\nsglang:time_to_first_token_seconds_sum{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 2.3518979474117756e+06\nsglang:time_to_first_token_seconds_bucket{le=\"0.001\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.005\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.01\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.02\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.04\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 1.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.06\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 3.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.08\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.1\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.25\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.5\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:time_to_first_token_seconds_bucket{le=\"0.75\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:time_to_first_token_seconds_bucket{le=\"1.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 27.0\nsglang:time_to_first_token_seconds_bucket{le=\"2.5\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 140.0\nsglang:time_to_first_token_seconds_bucket{le=\"5.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 314.0\nsglang:time_to_first_token_seconds_bucket{le=\"7.5\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 941.0\nsglang:time_to_first_token_seconds_bucket{le=\"10.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 1330.0\nsglang:time_to_first_token_seconds_bucket{le=\"15.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 1970.0\nsglang:time_to_first_token_seconds_bucket{le=\"20.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 2326.0\nsglang:time_to_first_token_seconds_bucket{le=\"25.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 2417.0\nsglang:time_to_first_token_seconds_bucket{le=\"30.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 2513.0\nsglang:time_to_first_token_seconds_bucket{le=\"+Inf\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 11008.0\nsglang:time_to_first_token_seconds_count{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 11008.0\n# HELP sglang:e2e_request_latency_seconds Histogram of End-to-end request latency in seconds\n# TYPE sglang:e2e_request_latency_seconds histogram\nsglang:e2e_request_latency_seconds_sum{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 3.116093850019932e+06\nsglang:e2e_request_latency_seconds_bucket{le=\"0.3\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nsglang:e2e_request_latency_seconds_bucket{le=\"0.5\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:e2e_request_latency_seconds_bucket{le=\"0.8\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:e2e_request_latency_seconds_bucket{le=\"1.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:e2e_request_latency_seconds_bucket{le=\"1.5\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:e2e_request_latency_seconds_bucket{le=\"2.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:e2e_request_latency_seconds_bucket{le=\"2.5\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 6.0\nsglang:e2e_request_latency_seconds_bucket{le=\"5.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.0\nsglang:e2e_request_latency_seconds_bucket{le=\"10.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 10.0\nsglang:e2e_request_latency_seconds_bucket{le=\"15.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 11.0\nsglang:e2e_request_latency_seconds_bucket{le=\"20.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 14.0\nsglang:e2e_request_latency_seconds_bucket{le=\"30.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 247.0\nsglang:e2e_request_latency_seconds_bucket{le=\"40.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 486.0\nsglang:e2e_request_latency_seconds_bucket{le=\"50.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 845.0\nsglang:e2e_request_latency_seconds_bucket{le=\"60.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 1513.0\nsglang:e2e_request_latency_seconds_bucket{le=\"+Inf\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 11228.0\nsglang:e2e_request_latency_seconds_count{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 11228.0\n# HELP sglang:time_per_output_token_seconds Histogram of time per output token in seconds.\n# TYPE sglang:time_per_output_token_seconds histogram\nsglang:time_per_output_token_seconds_sum{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 866964.5791549598\nsglang:time_per_output_token_seconds_bucket{le=\"0.005\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 1.0\nsglang:time_per_output_token_seconds_bucket{le=\"0.01\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 73.0\nsglang:time_per_output_token_seconds_bucket{le=\"0.015\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 382.0\nsglang:time_per_output_token_seconds_bucket{le=\"0.02\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 593.0\nsglang:time_per_output_token_seconds_bucket{le=\"0.025\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 855.0\nsglang:time_per_output_token_seconds_bucket{le=\"0.03\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 1035.0\nsglang:time_per_output_token_seconds_bucket{le=\"0.04\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 1815.0\nsglang:time_per_output_token_seconds_bucket{le=\"0.05\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 11685.0\nsglang:time_per_output_token_seconds_bucket{le=\"0.075\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 433413.0\nsglang:time_per_output_token_seconds_bucket{le=\"0.1\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 4.950195e+06\nsglang:time_per_output_token_seconds_bucket{le=\"0.15\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.039435e+06\nsglang:time_per_output_token_seconds_bucket{le=\"0.2\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.171662e+06\nsglang:time_per_output_token_seconds_bucket{le=\"0.3\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.266055e+06\nsglang:time_per_output_token_seconds_bucket{le=\"0.4\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.296752e+06\nsglang:time_per_output_token_seconds_bucket{le=\"0.5\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.312226e+06\nsglang:time_per_output_token_seconds_bucket{le=\"0.75\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.339675e+06\nsglang:time_per_output_token_seconds_bucket{le=\"1.0\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.357747e+06\nsglang:time_per_output_token_seconds_bucket{le=\"2.5\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.389414e+06\nsglang:time_per_output_token_seconds_bucket{le=\"+Inf\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.400757e+06\nsglang:time_per_output_token_seconds_count{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 7.400757e+06\n# HELP sglang:func_latency_seconds Function latency in seconds\n```\n\n----------------------------------------\n\nTITLE: API Request with Regular Expression\nDESCRIPTION: This code demonstrates the use of regular expressions to constrain the output of the language model. It sends a POST request to the `/generate` endpoint, including a regular expression in the `sampling_params`. The model's output will be forced to match this regular expression. The server response is printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Paris is the capital of\",\n    }\n]\ntext = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": text,\n        \"sampling_params\": {\n            \"temperature\": 0,\n            \"max_new_tokens\": 64,\n            \"regex\": \"(France|England)\",\n        },\n    },\n)\nprint_highlight(response.json())\n```\n\n----------------------------------------\n\nTITLE: Benchmark LightLLM (Python)\nDESCRIPTION: This command runs the benchmark script bench_other.py for the LightLLM backend. The --num-questions argument specifies the number of questions to use for the benchmark (80), and --backend lightllm specifies the backend to use. The script evaluates the performance of the LightLLM server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mtbench/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 80 --backend lightllm\n```\n\n----------------------------------------\n\nTITLE: Launch vLLM Server\nDESCRIPTION: This command launches the vLLM server. It sets the tokenizer mode to auto, specifies the model path (meta-llama/Llama-2-7b-chat-hf), disables request logging, and sets the port to 21000. This configures the server to handle inference requests.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tip_suggestion/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Llama 4 Maverick with lm_eval\nDESCRIPTION: This set of commands launches the SGLang server with the Llama-4-Maverick-17B-128E-Instruct model and then runs an evaluation using lm_eval. The launch command configures tensor parallelism, memory fraction, and context length. The lm_eval command benchmarks the model's accuracy on the mmlu_pro dataset using a local chat completions endpoint.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/llama4.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Llama-4-Maverick-17B-128E-Instruct\npython -m sglang.launch_server --model-path meta-llama/Llama-4-Maverick-17B-128E-Instruct --port 30000 --tp 8 --mem-fraction-static 0.8 --context-length 65536\nlm_eval --model local-chat-completions --model_args model=meta-llama/Llama-4-Maverick-17B-128E-Instruct,base_url=http://localhost:30000/v1/chat/completions,num_concurrent=128,timeout=999999,max_gen_toks=2048 --tasks mmlu_pro --batch_size 128 --apply_chat_template --num_fewshot 0\n```\n\n----------------------------------------\n\nTITLE: Terminating Server Process in Python\nDESCRIPTION: This snippet demonstrates how to terminate the SGLang server process.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Install LLM Compressor\nDESCRIPTION: Installs the `llmcompressor` package, used for offline quantization. This tool helps to compress large language models.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/quantization.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# install\npip install llmcompressor\n```\n\n----------------------------------------\n\nTITLE: Benchmarking LMQL\nDESCRIPTION: This command benchmarks LMQL using `bench_other.py`. It specifies the backend as lmql and sets the number of events and parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/generative_agents/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-events 1000 --backend lmql --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Streaming Request with Buffered Reasoning in Python\nDESCRIPTION: This code snippet demonstrates a streaming request where the reasoning content is buffered and only updated with the last reasoning chunk. It iterates through the response chunks, accumulating the final answer and updating the reasoning content with the latest chunk.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresponse_stream = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    temperature=0.6,\n    top_p=0.95,\n    stream=True,  # Non-streaming\n    extra_body={\"separate_reasoning\": True, \"stream_reasoning\": False},\n)\n\nreasoning_content = \"\"\ncontent = \"\"\nfor chunk in response_stream:\n    if chunk.choices[0].delta.content:\n        content += chunk.choices[0].delta.content\n    if chunk.choices[0].delta.reasoning_content:\n        reasoning_content = chunk.choices[0].delta.reasoning_content\n\nprint_highlight(\"==== Reasoning ====\")\nprint_highlight(reasoning_content)\n\nprint_highlight(\"==== Text ====\")\nprint_highlight(content)\n```\n\n----------------------------------------\n\nTITLE: Installing llama.cpp Dependencies (Bash/Python)\nDESCRIPTION: This command installs the necessary dependencies for llama.cpp and related tools. It includes setting CMake arguments for CUDA support and installing llama-cpp-python, sse_starlette, starlette_context, and pydantic_settings.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Install\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\npip install sse_starlette starlette_context pydantic_settings\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang character generation\nDESCRIPTION: This command runs the `bench_sglang.py` script in character generation mode. It benchmarks the performance of SGLang for generating characters.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_sglang.py --mode character\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang on AIME 2025 I dataset\nDESCRIPTION: This command benchmarks SGLang on the AIME 2025 I dataset.  It specifies the data path (`--data-path`), question key (`--question-key`), answer key (`--answer-key`), number of tries (`--num-tries`), the degree of parallelism (`--parallel`) and the port the server is running on (`--port`).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/reasoning_benchmark/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_sglang.py --parallel 256 --port 30000 --data-path opencompass/AIME2025 --question-key question --answer-key answer --num-tries 64\n```\n\n----------------------------------------\n\nTITLE: Set Torch Profiler Directory (Bash)\nDESCRIPTION: This command sets the environment variable `SGLANG_TORCH_PROFILER_DIR` to specify the directory where PyTorch Profiler trace files will be stored. This is necessary for both server and client.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport SGLANG_TORCH_PROFILER_DIR=/root/sglang/profile_log\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server (Python)\nDESCRIPTION: This command launches an SGLang server using a specified model and tokenizer. The command starts a local server on port 30000 using the `liuhaotian/llava-v1.6-vicuna-7b` model and `llava-hf/llava-1.5-7b-hf` tokenizer. The server is required to serve requests from the benchmark script.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 -m sglang.launch_server --model-path liuhaotian/llava-v1.6-vicuna-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Launching Llama 3.1 405B (fp16) on Two Nodes with SGLang\nDESCRIPTION: This code snippet demonstrates how to launch the Llama 3.1 405B model with fp16 precision on two nodes using SGLang. It involves specifying the model path, tensor parallelism degree (--tp), distributed initialization address (--dist-init-addr), number of nodes (--nnodes), and the rank of the node (--node-rank).  Replace `172.16.4.52:20000` with the actual IP address and port of the first node.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/multi_node.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# replace 172.16.4.52:20000 with your own node ip address and port of the first node\n\npython3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-405B-Instruct --tp 16 --dist-init-addr 172.16.4.52:20000 --nnodes 2 --node-rank 0\n\npython3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-405B-Instruct --tp 16 --dist-init-addr 172.16.4.52:20000 --nnodes 2 --node-rank 1\n```\n\n----------------------------------------\n\nTITLE: Benchmark LMQL (Python)\nDESCRIPTION: Runs the benchmark script for LMQL with a specified number of subtasks, backend, and parallelism level.  The `--nsub` argument sets the number of subtasks, `--backend` sets the backend to lmql, and `--parallel` sets the level of parallelism. This utilizes `bench_other.py`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --nsub 10 --backend lmql --parallel 2\n```\n\n----------------------------------------\n\nTITLE: Fetching External Content (flash-attention)\nDESCRIPTION: Declares and populates the flash-attention repository using FetchContent. This fetches the flash-attention library from GitHub at a specific tag. GIT_SHALLOW is set to OFF indicating that the full history should be fetched.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nFetchContent_Declare(\n    repo-flash-attention\n    GIT_REPOSITORY https://github.com/sgl-project/sgl-attn\n    GIT_TAG        sgl-kernel\n    GIT_SHALLOW    OFF\n)\nFetchContent_Populate(repo-flash-attention)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance city information retrieval\nDESCRIPTION: This command benchmarks the Guidance framework for city information retrieval. It executes the `bench_other.py` script with the `--backend guidance` option, specifying the model path, context length, and parallel requests.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_other.py --mode city --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Installing SGLang\nDESCRIPTION: This snippet details the steps to install the latest version of SGLang from GitHub. It includes cloning the repository, checking out a specific version (v0.2.7), upgrading pip, installing the required Python packages, and installing flashinfer with a specific CUDA version.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\ngit checkout v0.2.7\n\npip install --upgrade pip\npip install -e \"python[all]\"\n\npip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.3/\n```\n\n----------------------------------------\n\nTITLE: Launch LightLLM Server\nDESCRIPTION: This command launches the LightLLM API server, specifying the model directory, max total token number, and port.  It uses the lightllm.server.api_server module.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_deep/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\npython -m lightllm.server.api_server --tokenizer_mode auto --model_dir ~/model_weights/llama-2-7b-chat-hf --max_total_token_num 16000 --port 22000\n```\n\n----------------------------------------\n\nTITLE: Installing and Applying Nest Asyncio - Python\nDESCRIPTION: This code snippet installs the `nest-asyncio` package and applies it to the Jupyter notebook event loop. This is necessary to avoid issues when running asynchronous code within a Jupyter notebook environment.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n!pip install nest-asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang on LIMO dataset\nDESCRIPTION: This command benchmarks SGLang on the LIMO dataset. It uses parallel processing (`--parallel`), specifies the number of tries (`--num-tries`), and the port the server is running on (`--port`).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/reasoning_benchmark/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_sglang.py --parallel 256 --num-tries 64 --port 30000\n```\n\n----------------------------------------\n\nTITLE: Running Experiment on RAG Pipeline - Python\nDESCRIPTION: This code snippet initializes and runs an experiment to evaluate the performance of the RAG pipeline on the entire dataset. It uses `p.experiment` to initialize the experiment with a name, the data (a list of dictionaries containing questions and target answers), and the `rag_pipeline` function. The `run` method is then called on the experiment to execute it. The `target` key in the data dictionary is used as the target answer for evaluation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ne = p.experiment(\n    \"RAG\",\n    data=[\n        {\n            \"question\": qca[\"question\"],\n            \"target\": qca[\"answer\"],\n        }\n        for qca in question_context_answers\n    ],\n    func=rag_pipeline,\n).run()\n```\n\n----------------------------------------\n\nTITLE: Benchmark LightLLM (Python)\nDESCRIPTION: Runs the benchmark script for LightLLM with a specified number of subtasks and backend.  The `--nsub` argument sets the number of subtasks, and the `--backend` argument specifies the backend as lightllm. This utilizes `bench_other.py`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --nsub 10 --backend lightllm\n```\n\n----------------------------------------\n\nTITLE: Build Dataset (Python)\nDESCRIPTION: This command installs the wikipedia package and then runs the build_dataset.py script to build the dataset used for benchmarking.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/long_json_decode/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npip install wikipedia\n```\n\nLANGUAGE: python\nCODE:\n```\npython3 build_dataset.py\n```\n\n----------------------------------------\n\nTITLE: Downloading Benchmark Data\nDESCRIPTION: This command downloads the test dataset from a specified URL using wget. The dataset is in JSONL format and is used for benchmarking language models.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_chain_reasoning/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM API Server\nDESCRIPTION: This command starts the vLLM API server with the specified model, tokenizer mode, and port. It also disables request logging. Key parameters include `--tokenizer-mode`, `--model`, and `--disable-log-requests`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/generative_agents/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Outlines character generation\nDESCRIPTION: This command benchmarks the Outlines framework for character generation. It executes the `bench_other.py` script in character mode with the `--backend outlines` option.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_other.py --mode character --backend outlines\n```\n\n----------------------------------------\n\nTITLE: Cancelling Batch Job OpenAI (Python)\nDESCRIPTION: This example shows how to cancel a batch job using the OpenAI Python client. It creates requests, uploads a file, creates a batch job, and then attempts to cancel the job using `client.batches.cancel`. It then monitors the cancellation status until the job is cancelled. Finally, it cleans up the input file and removes the local JSONL file. Dependencies: `json`, `time`, `openai`, `os`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport time\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n\nrequests = []\nfor i in range(5000):\n    requests.append(\n        {\n            \"custom_id\": f\"request-{i}\",\n            \"method\": \"POST\",\n            \"url\": \"/chat/completions\",\n            \"body\": {\n                \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": f\"{i}: You are a helpful AI assistant\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"Write a detailed story about topic. Make it very long.\",\n                    },\n                ],\n                \"max_tokens\": 128,\n            },\n        }\n    )\n\ninput_file_path = \"batch_requests.jsonl\"\nwith open(input_file_path, \"w\") as f:\n    for req in requests:\n        f.write(json.dumps(req) + \"\\n\")\n\nwith open(input_file_path, \"rb\") as f:\n    uploaded_file = client.files.create(file=f, purpose=\"batch\")\n\nbatch_job = client.batches.create(\n    input_file_id=uploaded_file.id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\",\n)\n\nprint_highlight(f\"Created batch job with ID: {batch_job.id}\")\nprint_highlight(f\"Initial status: {batch_job.status}\")\n\ntime.sleep(10)\n\ntry:\n    cancelled_job = client.batches.cancel(batch_id=batch_job.id)\n    print_highlight(f\"Cancellation initiated. Status: {cancelled_job.status}\")\n    assert cancelled_job.status == \"cancelling\"\n\n    # Monitor the cancellation process\n    while cancelled_job.status not in [\"failed\", \"cancelled\"]:\n        time.sleep(3)\n        cancelled_job = client.batches.retrieve(batch_job.id)\n        print_highlight(f\"Current status: {cancelled_job.status}\")\n\n    # Verify final status\n    assert cancelled_job.status == \"cancelled\"\n    print_highlight(\"Batch job successfully cancelled\")\n\nexcept Exception as e:\n    print_highlight(f\"Error during cancellation: {e}\")\n    raise e\n\nfinally:\n    try:\n        del_response = client.files.delete(uploaded_file.id)\n        if del_response.deleted:\n            print_highlight(\"Successfully cleaned up input file\")\n        if os.path.exists(input_file_path):\n            os.remove(input_file_path)\n            print_highlight(\"Successfully deleted local batch_requests.jsonl file\")\n    except Exception as e:\n        print_highlight(f\"Error cleaning up: {e}\")\n        raise e\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server (Python)\nDESCRIPTION: This command launches the SGLang server using the specified model. It uses the meta-llama/Llama-2-7b-chat-hf model and listens on port 30000. It utilizes the sglang.launch_server module to start the server process.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mtbench/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Benchmark dspy-ai with TGI\nDESCRIPTION: Runs the dspy-ai benchmark script, specifying TGI as the backend. This evaluates the performance of dspy-ai using the TGI server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/dspy/README.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\npython3 bench_dspy_intro.py --backend tgi\n```\n\n----------------------------------------\n\nTITLE: Setting Target Include Directories\nDESCRIPTION: Sets the include directories for the `common_ops` target.  These directories contain header files needed to compile the source code, including those from the cutlass and flash-attention libraries.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(common_ops PRIVATE\n    ${repo-cutlass_SOURCE_DIR}/examples/77_blackwell_fmha\n    ${repo-cutlass_SOURCE_DIR}/examples/common\n    ${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Compile Definitions for Target\nDESCRIPTION: Defines compile-time definitions for the `common_ops` target. These definitions can be used to conditionally compile code or enable/disable features. Here, they disable backward compatibility, dropout, and uneven K in flash-attention.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_compile_definitions(common_ops PRIVATE\n    FLASHATTENTION_DISABLE_BACKWARD\n    FLASHATTENTION_DISABLE_DROPOUT\n    FLASHATTENTION_DISABLE_UNEVEN_K\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up ulimit and HF_TOKEN\nDESCRIPTION: This snippet sets up the ulimit and exports the HF_TOKEN environment variable. The ulimit is set to 65535, and the HF_TOKEN should be replaced with a valid Hugging Face token with access permissions for the Llama 3 models.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nulimit -n 65535\n# Change the token to a real and usable one, with access permissions for the Llama 3 models.\nexport HF_TOKEN=hf_token\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM server with Mixtral-8x7B (Python)\nDESCRIPTION: This command launches the vLLM API server with the Mixtral-8x7B-Instruct-v0.1 model on port 21000, using tensor parallelism with a size of 8.  It disables request logging and sets the tokenizer mode to auto.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model mistralai/Mixtral-8x7B-Instruct-v0.1 --disable-log-requests --port 21000 --tensor-parallel-size 8\n```\n\n----------------------------------------\n\nTITLE: Sending Multiple-Image Inputs with OpenAI Client, Python\nDESCRIPTION: This snippet demonstrates sending a request to the SGLang server with multiple images using the OpenAI Python client. It creates a chat completion request with a message containing two image URLs and interleaved text, showcasing the server's ability to handle multiple images and text within a single request.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_vision.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\n\nresponse = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\",\n                    },\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png\",\n                    },\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"I have two very different images. They are not related at all. \"\n                    \"Please describe the first image in one sentence, and then describe the second image in another sentence.\",\n                },\n            ],\n        }\n    ],\n    temperature=0,\n)\n\nprint_highlight(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Installing SGLang with pip or uv (Bash)\nDESCRIPTION: This snippet shows how to install SGLang using pip or uv, including upgrading pip and installing the sglang package with all dependencies. It's recommended to use uv for faster installation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/start/install.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade pip\npip install uv\nuv pip install \"sglang[all]>=0.4.5.post3\"\n```\n\n----------------------------------------\n\nTITLE: Benchmark Single Static Batch Latency (Bash)\nDESCRIPTION: This command benchmarks the latency of running a single static batch without a server using `sglang.bench_one_batch`. It is a simplified test script that takes arguments similar to `launch_server.py`. However, it might run out of memory for large batch sizes due to the lack of dynamic batching.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --batch 32 --input-len 256 --output-len 32\n```\n\n----------------------------------------\n\nTITLE: Benchmarking lightLLM\nDESCRIPTION: Runs the benchmark script against the lightLLM backend using bench_other.py with specified arguments.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_v0/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 32 --backend lightllm\n```\n\n----------------------------------------\n\nTITLE: Example: Add Worker with Python and Curl (Bash)\nDESCRIPTION: This example first launches an SGLang server with a specific model path on port 30001 using a Python command. Then, it adds this server as a worker to the SGLang Router running on localhost:30000 via a curl command. Requires sglang package installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/v0.1.0.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --port 30001\n$ curl -X POST http://localhost:30000/add_worker?url=http://127.0.0.1:30001\n```\n\n----------------------------------------\n\nTITLE: Benchmarking LMQL\nDESCRIPTION: This command runs the benchmark script with a specified number of questions using the lmql backend with parallel execution enabled. Requires the bench_other.py script, an LMQL environment, and a running LMQL server if needed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_chain_reasoning/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 64 --backend lmql --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files\nDESCRIPTION: Defines a list of source files that make up the project. This includes both CUDA (.cu) and C++ (.cc) files, as well as files from the fetched external repositories.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nset(SOURCES\n    \"csrc/allreduce/custom_all_reduce.cu\"\n    \"csrc/attention/cascade.cu\"\n    \"csrc/attention/merge_attn_states.cu\"\n    \"csrc/attention/cutlass_mla_kernel.cu\"\n    \"csrc/attention/lightning_attention_decode_kernel.cu\"\n    \"csrc/elementwise/activation.cu\"\n    \"csrc/elementwise/fused_add_rms_norm_kernel.cu\"\n    \"csrc/elementwise/rope.cu\"\n    \"csrc/gemm/awq_kernel.cu\"\n    \"csrc/gemm/bmm_fp8.cu\"\n    \"csrc/gemm/fp8_blockwise_gemm_kernel.cu\"\n    \"csrc/gemm/fp8_gemm_kernel.cu\"\n    \"csrc/gemm/int8_gemm_kernel.cu\"\n    \"csrc/gemm/nvfp4_quant_entry.cu\"\n    \"csrc/gemm/nvfp4_quant_kernels.cu\"\n    \"csrc/gemm/nvfp4_scaled_mm_entry.cu\"\n    \"csrc/gemm/nvfp4_scaled_mm_kernels.cu\"\n    \"csrc/gemm/per_tensor_quant_fp8.cu\"\n    \"csrc/gemm/per_token_group_quant_8bit.cu\"\n    \"csrc/gemm/per_token_quant_fp8.cu\"\n    \"csrc/moe/moe_align_kernel.cu\"\n    \"csrc/moe/moe_fused_gate.cu\"\n    \"csrc/moe/moe_topk_softmax_kernels.cu\"\n    \"csrc/moe/fp8_blockwise_moe_kernel.cu\"\n    \"csrc/speculative/eagle_utils.cu\"\n    \"csrc/speculative/speculative_sampling.cu\"\n    \"csrc/speculative/packbit.cu\"\n    \"csrc/grammar/apply_token_bitmask_inplace_cuda.cu\"\n    \"csrc/common_extension.cc\"\n    \"${repo-flashinfer_SOURCE_DIR}/csrc/norm.cu\"\n    \"${repo-flashinfer_SOURCE_DIR}/csrc/renorm.cu\"\n    \"${repo-flashinfer_SOURCE_DIR}/csrc/sampling.cu\"\n    \"${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src/flash_fwd_sparse_hdim128_bf16_causal_sm80.cu\"\n    \"${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src/flash_fwd_sparse_hdim128_bf16_sm80.cu\"\n    \"${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src/flash_fwd_sparse_hdim128_fp16_causal_sm80.cu\"\n    \"${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src/flash_fwd_sparse_hdim128_fp16_sm80.cu\"\n    \"${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/flash_sparse_api.cpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking sglang (long output) (Python)\nDESCRIPTION: This command runs the bench_sglang.py script to benchmark sglang with the Llama-2-7b-chat-hf tokenizer, generating a long output.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --tokenizer meta-llama/Llama-2-7b-chat-hf --long\n```\n\n----------------------------------------\n\nTITLE: Outlines+vLLM: Launching Llama-7B Server\nDESCRIPTION: This command launches the Outlines server using vLLM with the Llama-2-7b-chat-hf model.  It disables request logging and sets the port to 21000.  Requires the outlines package and vLLM to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_decode_regex/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\npython3 -m outlines.serve.serve --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf  --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Terminating Process (Python)\nDESCRIPTION: This code snippet is a standalone function call to terminate a process. It assumes `server_process` is defined elsewhere and aims to terminate that process. The specific implementation of `terminate_process` is not provided. It is assumed that `terminate_process` handles the server shutdown operation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Install Python Package in Editable Mode with Pip\nDESCRIPTION: This command installs the Python package in editable mode using `pip install -e .`. This allows changes to the source code to be reflected immediately without needing to reinstall the package. However, it can lead to performance degradation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing SGLang Dependencies (Python)\nDESCRIPTION: This command installs necessary dependencies for SGLang and related tools using pip. It installs SGLang with all optional dependencies, PyTorch, Transformers, and Pillow. It ensures the environment has the required libraries for running SGLang and related benchmarks.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip3 install \"sglang[all]\"\npip3 install \"torch>=2.1.2\" \"transformers>=4.36\" pillow\n```\n\n----------------------------------------\n\nTITLE: Serving LMQL Model\nDESCRIPTION: This command starts an LMQL server, specifying the model to serve, enabling CUDA, and setting the port. It serves the meta-llama/Llama-2-7b-chat-hf model on port 23000.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hellaswag/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlmql serve-model meta-llama/Llama-2-7b-chat-hf --cuda --port 23000\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance\nDESCRIPTION: Executes the benchmark script using the Guidance backend, with specific parameters for parallel execution, context length, and model path. This command benchmarks the Guidance framework's performance.  Parameters include `--parallel` for parallel processing, `--n-ctx` for context length, and `--model-path` for the model's file location.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/react/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 100 --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Install dspy-ai with pip\nDESCRIPTION: Installs the dspy-ai package using pip, which is a prerequisite for running the benchmarks. This command ensures that the necessary libraries are available.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/dspy/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip3 install dspy-ai\n```\n\n----------------------------------------\n\nTITLE: TensorRT LLM Configuration\nDESCRIPTION: This JSON configuration is designed for use with TensorRT LLM. It specifies the model architecture (LlamaForCausalLM), data types (float16), quantization settings (FP8), and various other parameters crucial for TensorRT optimization. Key parameters include vocab_size, hidden_size, num_hidden_layers, and mapping configurations for distributed inference.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/config.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"architecture\": \"LlamaForCausalLM\",\n    \"dtype\": \"float16\",\n    \"logits_dtype\": \"float32\",\n    \"vocab_size\": 128256,\n    \"max_position_embeddings\": 8192,\n    \"hidden_size\": 16384,\n    \"num_hidden_layers\": 126,\n    \"num_attention_heads\": 128,\n    \"num_key_value_heads\": 16,\n    \"head_size\": 128,\n    \"qk_layernorm\": false,\n    \"hidden_act\": \"silu\",\n    \"intermediate_size\": 53248,\n    \"norm_epsilon\": 1e-05,\n    \"position_embedding_type\": \"rope_gpt_neox\",\n    \"use_parallel_embedding\": false,\n    \"embedding_sharding_dim\": 0,\n    \"share_embedding_table\": false,\n    \"mapping\": {\n        \"world_size\": 8,\n        \"tp_size\": 8,\n        \"pp_size\": 1,\n        \"gpus_per_node\": 8\n    },\n    \"quantization\": {\n        \"quant_algo\": \"FP8\",\n        \"kv_cache_quant_algo\": null,\n        \"group_size\": 128,\n        \"smoothquant_val\": null,\n        \"has_zero_point\": false,\n        \"pre_quant_scale\": false,\n        \"exclude_modules\": [\n            \"lm_head\"\n        ]\n    },\n    \"kv_dtype\": \"float16\",\n    \"rotary_scaling\": null,\n    \"residual_mlp\": false,\n    \"moe_normalization_mode\": null,\n    \"rotary_base\": 500000.0,\n    \"moe_num_experts\": 0,\n    \"moe_top_k\": 0,\n    \"moe_tp_mode\": 2,\n    \"attn_bias\": false,\n    \"disable_weight_only_quant_plugin\": false,\n    \"mlp_bias\": false\n}\n\n```\n\n----------------------------------------\n\nTITLE: Benchmarking LightLLM\nDESCRIPTION: This command runs a benchmark using the LightLLM backend. It sets the number of questions and specifies the LightLLM backend.  A running LightLLM server is needed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hellaswag/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 200 --backend lightllm\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Different Configurations (Python)\nDESCRIPTION: This snippet demonstrates how to launch the SGLang server with different configurations, including disabling radix cache, enabling first-come-first-serve scheduling policy, using the default long-prefix-match policy, and enabling hierarchical radix cache. It uses the `sglang.launch_server` module and specifies model path, port, and cache-related arguments.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hicache/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# SGLang server with radix cache disabled\npython -m sglang.launch_server --model-path Qwen/Qwen2.5-14B-Instruct --port 30000 --disable-radix-cache\n\n# SGLang server with radix cache on and first-come-first-serve policy\npython -m sglang.launch_server --model-path Qwen/Qwen2.5-14B-Instruct --port 30000 --schedule-policy fcfs\n\n# The default SGLang server with radix cache on and long-prefix-match policy\npython -m sglang.launch_server --model-path Qwen/Qwen2.5-14B-Instruct --port 30000\n\n# SGLang server with hierarchical radix cache enabled\npython -m sglang.launch_server --model-path Qwen/Qwen2.5-14B-Instruct --port 30000 --enable-hierarchical-cache\n```\n\n----------------------------------------\n\nTITLE: Benchmark Guidance (Python)\nDESCRIPTION: This command benchmarks Guidance using the bench_other.py script. It specifies the backend as guidance, the number of questions, parallelism, context window size, and model path.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/long_json_decode/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --backend guidance --num-questions 5 --parallel 1 --n-ctx 11000 --model-path path/to/code-llama/gguf\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang and vLLM (Online)\nDESCRIPTION: Benchmarks SGLang and vLLM in an online serving scenario using the sharegpt dataset with varying prompt numbers and request rates. Uses the `sglang.bench_serving` script.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/benchmark_vllm_060/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# bench serving\npython3 -m sglang.bench_serving --backend sglang --dataset-name sharegpt --num-prompts 1200 --request-rate 4\npython3 -m sglang.bench_serving --backend sglang --dataset-name sharegpt --num-prompts 2400 --request-rate 8\npython3 -m sglang.bench_serving --backend vllm --dataset-name sharegpt --num-prompts 1200 --request-rate 4\npython3 -m sglang.bench_serving --backend vllm --dataset-name sharegpt --num-prompts 2400 --request-rate 8\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang\nDESCRIPTION: This command executes the SGLang benchmark script. The --num-questions parameter specifies the number of questions to use for the benchmark. It depends on the SGLang server being already running.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hellaswag/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 200\n```\n\n----------------------------------------\n\nTITLE: Outlines+vLLM: Benchmark Execution\nDESCRIPTION: This command executes the bench_other.py script to benchmark Outlines with vLLM. It uses 10 questions for evaluation and specifies the 'outlines' backend. Requires the bench_other.py script and necessary dependencies.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_decode_regex/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\npython3 bench_other.py --backend outlines --num-questions 10\n```\n\n----------------------------------------\n\nTITLE: Indexing Data into ChromaDB - Python\nDESCRIPTION: This code snippet downloads the data (if not already present) and indexes it into a ChromaDB collection named `contexts`. It reads question-context-answer pairs from a JSON file and adds the contexts as documents in the Chroma collection. It uses the `chromadb` library to create and manage the vector database.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom typing import List\n\nimport chromadb\n\npath_qca = \"airbnb-2023-10k-qca.json\"\n\nif not os.path.exists(path_qca):\n    !wget https://virattt.github.io/datasets/abnb-2023-10k.json -O airbnb-2023-10k-qca.json\n\nwith open(path_qca, \"r\") as f:\n    question_context_answers = json.load(f)\n\nchroma_client = chromadb.PersistentClient()\ncollection = chroma_client.get_or_create_collection(name=\"contexts\")\nif collection.count() == 0:\n    collection.add(\n        documents=[qca[\"context\"] for qca in question_context_answers],\n        ids=[str(i) for i in range(len(question_context_answers)]],\n    )\n```\n\n----------------------------------------\n\nTITLE: Downloading bench_serving.py\nDESCRIPTION: This command downloads the `bench_serving.py` script from the sglang repository using wget.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/sgl-project/sglang/main/python/sglang/bench_serving.py\n```\n\n----------------------------------------\n\nTITLE: Benchmark SGLang EAGLE (Python)\nDESCRIPTION: This command runs the SGLang EAGLE benchmark script bench_sglang_eagle.py. The --num-questions argument specifies the number of questions to use for the benchmark (80), and --parallel specifies the number of parallel requests (1). The script benchmarks the performance of the SGLang server with EAGLE enabled.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mtbench/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang_eagle.py --num-questions 80 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Downloading Benchmark Datasets (Bash)\nDESCRIPTION: This bash script downloads the specified dataset (sharegpt, ultragpt, loogle, nextqa, or all) to the current working directory. It uses the `./download.sh` script with the dataset name as an argument.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hicache/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./download.sh {sharegpt|ultragpt|loogle|nextqa|all}\n```\n\n----------------------------------------\n\nTITLE: Running Runner in a Loop\nDESCRIPTION: This command executes the `run.sh` script in a loop, restarting it if it exits. This ensures the runner is always running. The `sleep 2` command introduces a delay of 2 seconds before restarting.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/setup_github_runner.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nwhile true; do ./run.sh; echo \"Restarting...\"; sleep 2; done\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Commit Hooks\nDESCRIPTION: These commands install and run pre-commit hooks to ensure code style consistency. `pip3 install pre-commit` installs the pre-commit tool. `pre-commit install` sets up the hooks in your local repository. `pre-commit run --all-files` runs the hooks against all files to check for style issues before committing.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/contribution_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install pre-commit\n```\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang server\nDESCRIPTION: This command launches the SGLang server using Python 3. It specifies the model path to `meta-llama/Llama-2-7b-chat-hf` and sets the port to 30000. This is necessary to run SGLang benchmarks.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: API Request with EBNF Grammar\nDESCRIPTION: This snippet showcases the use of EBNF (Extended Backus-Naur Form) grammar to constrain the generation of text by the language model. It sends an API request to the `/generate` endpoint, including an EBNF grammar in the `sampling_params`. The grammar defines the structure of the generated text. The response from the server is then printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Give me the information of the capital of France.\",\n    }\n]\ntext = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nresponse = requests.post(\n    f\"http://localhost:{port}/generate\",\n    json={\n        \"text\": text,\n        \"sampling_params\": {\n            \"max_new_tokens\": 128,\n            \"temperature\": 0,\n            \"n\": 3,\n            \"ebnf\": (\n                \"root ::= city | description\\n\"\n                'city ::= \"London\" | \"Paris\" | \"Berlin\" | \"Rome\"\\n'\n                'description ::= city \" is \" status\\n'\n                'status ::= \"the capital of \" country\\n'\n                'country ::= \"England\" | \"France\" | \"Germany\" | \"Italy\"'\n            ),\n        },\n        \"stream\": False,\n        \"return_logprob\": False,\n    },\n)\n\nprint_highlight(response.json())\n```\n\n----------------------------------------\n\nTITLE: Streaming Request with Reasoning in Python\nDESCRIPTION: This code snippet shows how to make a streaming request to the SGLang server, enabling reasoning separation. It iterates through the response chunks, accumulating both the reasoning content and the final answer, and prints them separately.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresponse_stream = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    temperature=0.6,\n    top_p=0.95,\n    stream=True,  # Non-streaming\n    extra_body={\"separate_reasoning\": True},\n)\n\nreasoning_content = \"\"\ncontent = \"\"\nfor chunk in response_stream:\n    if chunk.choices[0].delta.content:\n        content += chunk.choices[0].delta.content\n    if chunk.choices[0].delta.reasoning_content:\n        reasoning_content += chunk.choices[0].delta.reasoning_content\n\nprint_highlight(\"==== Reasoning ====\")\nprint_highlight(reasoning_content)\n\nprint_highlight(\"==== Text ====\")\nprint_highlight(content)\n```\n\n----------------------------------------\n\nTITLE: Set DSP cache environment variable\nDESCRIPTION: Sets the environment variable to disable DSP cache. This is an alternative way to disable the cache.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/dspy/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nexport DSP_CACHEBOOL=false\n```\n\n----------------------------------------\n\nTITLE: Checking Infiniband Status with ibstatus\nDESCRIPTION: This bash command is used to check the status of Infiniband devices on a node. It provides information such as the default GID, base LID, state, physical state, rate, and link layer, which is essential for verifying the RoCE network configuration.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n[root@node1 ~]# ibstatus\nInfiniband device 'mlx5_bond_0' port 1 status:\n        default gid:     fe80:0000:0000:0000:0225:9dff:fe64:c79a\n        base lid:        0x0\n        sm lid:          0x0\n        state:           4: ACTIVE\n        phys state:      5: LinkUp\n        rate:            200 Gb/sec (2X NDR)\n        link_layer:      Ethernet\n\nInfiniband device 'mlx5_bond_1' port 1 status:\n        default gid:     fe80:0000:0000:0000:0225:9dff:fe6e:c3ec\n        base lid:        0x0\n        sm lid:          0x0\n        state:           4: ACTIVE\n        phys state:      5: LinkUp\n        rate:            200 Gb/sec (2X NDR)\n        link_layer:      Ethernet\n\nInfiniband device 'mlx5_bond_2' port 1 status:\n        default gid:     fe80:0000:0000:0000:0225:9dff:fe73:0dd7\n        base lid:        0x0\n        sm lid:          0x0\n        state:           4: ACTIVE\n        phys state:      5: LinkUp\n        rate:            200 Gb/sec (2X NDR)\n        link_layer:      Ethernet\n\nInfiniband device 'mlx5_bond_3' port 1 status:\n        default gid:     fe80:0000:0000:0000:0225:9dff:fe36:f7ff\n        base lid:        0x0\n        sm lid:          0x0\n        state:           4: ACTIVE\n        phys state:      5: LinkUp\n        rate:            200 Gb/sec (2X NDR)\n        link_layer:      Ethernet\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Compiler Flags\nDESCRIPTION: Defines the CUDA compiler flags to be used when compiling CUDA code. This includes optimization flags, architecture-specific flags, and other flags related to features like FP16, BF16, and Tensor Core. It also includes warning suppression flags.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nset(SGL_KERNEL_CUDA_FLAGS\n    \"-DNDEBUG\"\n    \"-DOPERATOR_NAMESPACE=sgl-kernel\"\n    \"-O3\"\n    \"-Xcompiler\"\n    \"-fPIC\"\n    \"-gencode=arch=compute_75,code=sm_75\"\n    \"-gencode=arch=compute_80,code=sm_80\"\n    \"-gencode=arch=compute_89,code=sm_89\"\n    \"-gencode=arch=compute_90,code=sm_90\"\n    \"-std=c++17\"\n    \"-DFLASHINFER_ENABLE_F16\"\n    \"-DCUTE_USE_PACKED_TUPLE=1\"\n    \"-DCUTLASS_ENABLE_TENSOR_CORE_MMA=1\"\n    \"-DCUTLASS_VERSIONS_GENERATED\"\n    \"-DCUTLASS_TEST_LEVEL=0\"\n    \"-DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1\"\n    \"-DCUTLASS_DEBUG_TRACE_LEVEL=0\"\n    \"--expt-relaxed-constexpr\"\n    \"--expt-extended-lambda\"\n    \"--threads=32\"\n\n    # Supress warnings\n    \"-Xcompiler=-Wconversion\"\n    \"-Xcompiler=-fno-strict-aliasing\"\n\n    # uncomment to debug\n    # \"--ptxas-options=-v\"\n    # \"--ptxas-options=--verbose,--register-usage-level=10,--warn-on-local-memory-usage\"\n)\n```\n\n----------------------------------------\n\nTITLE: Build Python Wheel Package with Pip\nDESCRIPTION: These commands build a Python wheel package using `pip` and the `build` module. First, it installs the necessary build tools. Then, it executes the build process. This creates a distributable wheel file that can be installed using pip.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install setuptools-rust wheel build\n$ python -m build\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang server with single LoRA adapter\nDESCRIPTION: This snippet launches an SGLang server with a specified base model and a single LoRA adapter.  It defines the LoRA path, sets the maximum number of LoRAs per batch to 1, uses the Triton backend, and disables CUDA graph and radix cache for compatibility. The command is executed using `launch_server_cmd` from `sglang.utils` or `patch` (in CI environment).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/lora.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nserver_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n    --lora-paths lora0=algoprog/fact-generation-llama-3.1-8b-instruct-lora \\\n    --max-loras-per-batch 1 --lora-backend triton \\\n    --disable-cuda-graph --disable-radix-cache\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Run Custom Server\nDESCRIPTION: This command runs the custom server example.  It assumes that the `custom_server.py` file is in the current directory and that Sanic is installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/readme.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython custom_server\n```\n\n----------------------------------------\n\nTITLE: Collecting Accuracy Data for AIME 2024\nDESCRIPTION: This command collects accuracy data on the AIME 2024 dataset. The command is intended to be run multiple times to establish a mean accuracy and standard error for the model. It specifies the data path (`--data-path`), question key (`--question-key`), answer key (`--answer-key`), number of tries (`--num-tries`), the degree of parallelism (`--parallel`) and the port the server is running on (`--port`).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/reasoning_benchmark/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_sglang.py --parallel 64 --port 30000 --data-path Maxwell-Jia/AIME_2024 --question-key Problem --answer-key Answer --num-tries 64\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang\nDESCRIPTION: Executes the SGLang benchmark script with a specified number of questions. This script measures the performance of SGLang.  The `--num-questions` argument determines the number of questions used in the benchmark.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/react/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 100\n```\n\n----------------------------------------\n\nTITLE: Benchmark vLLM\nDESCRIPTION: This command benchmarks vLLM using the bench_other.py script. It specifies the backend as vllm and runs the benchmark with 64 questions. The script will send requests to the vLLM server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tip_suggestion/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_other.py --backend vllm --num-questions 64\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang and vLLM (Offline)\nDESCRIPTION: Benchmarks SGLang and vLLM in an offline serving scenario using the sharegpt dataset with 5000 prompts. Uses the `sglang.bench_serving` script.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/benchmark_vllm_060/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# bench serving\npython3 -m sglang.bench_serving --backend sglang --dataset-name sharegpt --num-prompts 5000\npython3 -m sglang.bench_serving --backend vllm --dataset-name sharegpt --num-prompts 5000\n```\n\n----------------------------------------\n\nTITLE: Launch LightLLM Server (Python)\nDESCRIPTION: Launches the LightLLM server with specified configurations, including model directory, max total token number, and port. Two examples are provided, one for A10G and one for V100 GPUs.  The `--tokenizer_mode auto` sets the tokenizer mode to auto, `--model_dir` specifies the path to the model weights, `--max_total_token_num` sets the maximum number of tokens, and `--port` sets the server port.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# A10G\npython -m lightllm.server.api_server --tokenizer_mode auto --model_dir ~/model_weights/llama-2-7b-chat-hf --max_total_token_num 16000 --port 22000\n\n# V100\npython -m lightllm.server.api_server --tokenizer_mode auto --model_dir ~/model_weights/llama-2-7b-chat-hf --max_total_token_num 4500 --port 22000\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to Target\nDESCRIPTION: Links the specified libraries to the `common_ops` target.  These include Torch libraries, c10, CUDA runtime, cublas, and cublasLt.  These libraries provide necessary functionality for the CUDA kernels.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(common_ops PRIVATE ${TORCH_LIBRARIES} c10 cuda cublas cublasLt)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking vLLM (long output) (Python)\nDESCRIPTION: This command runs the bench_other.py script to benchmark vLLM with the Llama-2-7b-chat-hf tokenizer, generating a long output.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --tokenizer meta-llama/Llama-2-7b-chat-hf --backend vllm --long\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang and vLLM Servers (Online Benchmark, Llama 3.1 8B)\nDESCRIPTION: Launches SGLang and vLLM servers for online benchmarking with Llama 3.1 8B Instruct model on a single A100 GPU. SGLang enables torch compile and disables radix cache. vLLM disables request logging, sets the number of scheduler steps, and sets the maximum model length.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/benchmark_vllm_060/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Llama 3.1 8B Instruct on 1 x A100\npython -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --enable-torch-compile --disable-radix-cache\npython -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-3.1-8B-Instruct --disable-log-requests --num-scheduler-steps 10 --max_model_len 4096\n```\n\n----------------------------------------\n\nTITLE: Check Port Usage (lsof)\nDESCRIPTION: These commands use `lsof` to check if ports 9090 and 3000 are being used by any process. This helps to identify potential port conflicts with other services.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/monitoring/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nlsof -i :9090\nlsof -i :3000\n```\n\n----------------------------------------\n\nTITLE: Updating GRUB Settings for AMD GPUs\nDESCRIPTION: This snippet shows how to update the GRUB configuration file to optimize the system for AMD GPUs. It involves appending specific parameters to the `GRUB_CMDLINE_LINUX` variable. A reboot is required after running `sudo update-grub`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/amd.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npci=realloc=off iommu=pt\n```\n\n----------------------------------------\n\nTITLE: Benchmark SGLang (Python)\nDESCRIPTION: This command runs the SGLang benchmark script bench_sglang.py. The --num-questions argument specifies the number of questions to use for the benchmark, set to 80 in this case. The script evaluates the performance of the SGLang server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mtbench/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 80\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: This snippet demonstrates how to launch the SGLang server with different Meta Llama 3 models. It includes commands for launching with the 8B, 70B, and 70B FP8 models, specifying parameters like model path, enabling torch compile, disabling radix cache, and setting tensor parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Meta-Llama-3.1-8B-Instruct\npython -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --enable-torch-compile --disable-radix-cache\n\n# Meta-Llama-3.1-70B-Instruct\npython -m sglang.launch_server --model-path meta-llama/Llama-3.1-70B-Instruct --disable-radix-cache --tp 8\n\n# Meta-Llama-3-70B-Instruct-FP8\npython -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3-70B-Instruct-FP8 --disable-radix-cache --tp 8\n```\n\n----------------------------------------\n\nTITLE: Generate data using Python script\nDESCRIPTION: This command executes a Python script named `gen_data.py` to generate data. The `--number` argument specifies the number of data points to generate. This script likely populates or prepares data used in later benchmarks.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/line_retrieval/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython3 gen_data.py --number 1000\n```\n\n----------------------------------------\n\nTITLE: Patch Scheduler for PyTorch Profiler\nDESCRIPTION: This patch modifies the SGLang scheduler to enable PyTorch CUDA profiling.  It removes CPU profiling and modifies the profiler output to write profiling stats to a file if the tensor parallel rank is 0. The patch affects the scheduler.py file within the sglang library. It modifies the Scheduler class to enable profiling activities and to export profiler statistics.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_10\n\nLANGUAGE: diff\nCODE:\n```\ndiff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 62d1ff9..6ecd78c 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -240,7 +240,6 @@ class Scheduler:\n             )\n             self.profiler = torch.profiler.profile(\n                 activities=[\n-                    torch.profiler.ProfilerActivity.CPU,\n                     torch.profiler.ProfilerActivity.CUDA,\n                 ],\n                 with_stack=True,\n@@ -1033,9 +1032,11 @@ class Scheduler:\n         if self.profiler is None:\n             raise RuntimeError(\"Profiler is not enabled.\")\n         self.profiler.stop()\n-        self.profiler.export_chrome_trace(\n-            self.torch_profiler_trace_dir + \"/\" + str(time.time()) + \".trace.json.gz\"\n-        )\n+        if self.tp_rank == 0:\n+            with open(f\"stats_repro_{int(time.time())}.txt\", \"w\") as f:\n+                print(self.profiler.key_averages(group_by_input_shape=True).table(sort_by=\"cuda_time_total\", row_limit=-1), file=f)\n+                print(\"Profiling stats done.\")\n+\n         logger.info(\"Profiler is done\")\n\n```\n\n----------------------------------------\n\nTITLE: Creating Python Library\nDESCRIPTION: Creates a Python library module named `common_ops` using the specified source files.  `USE_SABI` indicates ABI compatibility, and the library will be installed in the `sgl_kernel` directory during the install step.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nPython_add_library(common_ops MODULE USE_SABI ${SKBUILD_SABI_VERSION} WITH_SOABI ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Disable dspy-ai cache\nDESCRIPTION: This code snippet shows how to disable the dspy-ai cache. It is possible to turn off cache directly in the `cache_utils.py` file or setting the environment variable.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/dspy/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncache_turn_on = False\n```\n\n----------------------------------------\n\nTITLE: Run SGLang Docker Container (H100)\nDESCRIPTION: This snippet creates and runs a Docker container for SGLang development on an H100 GPU. It maps the /opt/dlami/nvme/.cache directory to /root/.cache within the container to reuse cached data and sets up shared memory, network, and privileged access for RDMA.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/development_guide_using_docker.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Change the name to yours\ndocker run -itd --shm-size 32g --gpus all -v /opt/dlami/nvme/.cache:/root/.cache --ipc=host --network=host --privileged --name sglang_zhyncs lmsysorg/sglang:dev /bin/zsh\ndocker exec -it sglang_zhyncs /bin/zsh\n```\n\n----------------------------------------\n\nTITLE: Benchmarking LMQL\nDESCRIPTION: This command executes the benchmark script using LMQL as the backend. It runs the benchmark with 25 questions and parallelism set to 1. The script measures the performance of LMQL.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llm_judge/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --backend lmql --num-questions 25 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang and vLLM Servers (Offline Benchmark, Llama 3.1 8B)\nDESCRIPTION: Launches SGLang and vLLM servers for offline benchmarking with Llama 3.1 8B Instruct model on a single A100 GPU. SGLang enables torch compile and disables radix cache. vLLM disables request logging, sets the number of scheduler steps, and sets the maximum model length.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/benchmark_vllm_060/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Llama 3.1 8B Instruct on 1 x A100\npython -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --enable-torch-compile --disable-radix-cache\npython -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-3.1-8B-Instruct --disable-log-requests --num-scheduler-steps 10 --max_model_len 4096\n```\n\n----------------------------------------\n\nTITLE: Initializing SGLang Engine (Offline)\nDESCRIPTION: This code initializes the SGLang engine for offline use. It specifies the model path and grammar backend.  Requires `sglang` to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport sglang as sgl\n\nllm = sgl.Engine(\n    model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", grammar_backend=\"xgrammar\"\n)\n```\n\n----------------------------------------\n\nTITLE: Offline Engine: JSON Schema Directly\nDESCRIPTION: This snippet shows how to use SGLang's offline engine with a directly defined JSON schema. It generates text based on prompts and validates the output against the provided JSON schema. The generated text and corresponding prompt are then printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Give me the information of the capital of China in the JSON format.\",\n    \"Give me the information of the capital of France in the JSON format.\",\n    \"Give me the information of the capital of Ireland in the JSON format.\",\n]\n\njson_schema = json.dumps(\n    {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n            \"population\": {\"type\": \"integer\"},\n        },\n        \"required\": [\"name\", \"population\"],\n    }\n)\n\nsampling_params = {\"temperature\": 0.1, \"top_p\": 0.95, \"json_schema\": json_schema}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print_highlight(\"===============================\")\n    print_highlight(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: Launches the SGLang server with a specified model and port. This command is essential for running SGLang benchmarks. The model path specifies the language model to be used, and the port defines the communication endpoint.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/react/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Download Dataset (Shell)\nDESCRIPTION: This command downloads the question.jsonl dataset file from a GitHub URL using wget. The downloaded file is saved as question.jsonl in the current directory. This dataset is used for benchmarking in subsequent steps.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mtbench/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nwget -O question.jsonl https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/llm_judge/data/mt_bench/question.jsonl\n```\n\n----------------------------------------\n\nTITLE: Removing CUDA Flags\nDESCRIPTION: Removes specific CUDA flags from the `CMAKE_CUDA_FLAGS` variable. This ensures that certain flags are not included during compilation, potentially overriding default behavior. This snippet removes flags related to half-precision and bfloat16 operators and conversions.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nstring(REPLACE \"-D__CUDA_NO_HALF_OPERATORS__\"       \"\" CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS}\")\nstring(REPLACE \"-D__CUDA_NO_HALF_CONVERSIONS__\"     \"\" CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS}\")\nstring(REPLACE \"-D__CUDA_NO_BFLOAT16_CONVERSIONS__\" \"\" CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS}\")\nstring(REPLACE \"-D__CUDA_NO_HALF2_OPERATORS__\"      \"\" CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS}\")\n```\n\n----------------------------------------\n\nTITLE: Benchmark vLLM\nDESCRIPTION: Runs the benchmarking script against the vLLM server. The `--num-questions` parameter sets the number of questions for the benchmark. The `--backend vllm` argument specifies that the benchmark should target the vLLM server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/gsm8k/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 200 --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Extracting Data from JSONL File (Online)\nDESCRIPTION: This script extracts specific information from `online_trt_70b.jsonl`. It pipes the file to `cut`, extracting the 9th field (delimited by ':'), then extracts the 1st field (delimited by ','). It is intended to retrieve particular data points from the benchmark results.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncat online_trt_70b.jsonl | cut -d':' -f9 | cut -d',' -f1\n```\n\n----------------------------------------\n\nTITLE: Launch vLLM Server (Python)\nDESCRIPTION: Launches the vLLM server with specified configurations. The `--tokenizer-mode auto` sets the tokenizer mode to auto, `--model` specifies the model path, `--disable-log-requests` disables logging requests, and `--port` sets the server port.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Benchmark LightLLM\nDESCRIPTION: Executes the benchmarking script against the LightLLM server. `--num-questions` sets the number of questions used for the benchmark. `--backend lightllm` specifies LightLLM as the target for benchmarking.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/gsm8k/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 200 --backend lightllm\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: This command launches the sglang server with a specified model path, port, and scheduling conservativeness. It requires the sglang library to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_chain_reasoning/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000  --schedule-conservativeness 1.3\n```\n\n----------------------------------------\n\nTITLE: Tuning Fused MoE Triton Kernel\nDESCRIPTION: This script tunes the `fused_moe_triton` kernel using various configurations. It supports specifying the model, tensor parallelism size, data type, and the number of shared experts for fusion.  The script generates a JSON configuration file with tuned parameters.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/kernels/fused_moe_triton/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Tune Mixtral-8x7B with default settings\npython benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py \\\n    --model mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n    --tune\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Tune Qwen2-57B with FP8 and TP=4\npython benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py \\\n    --model Qwen/Qwen2-57B-A14B-Instruct \\\n    --tp-size 4 \\\n    --dtype fp8_w8a8 \\\n    --tune\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Tune DeepSeek-V3 with FP8, TP=8 and n_share_experts_fusion=8\npython benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py \\\n    --model deepseek-ai/DeepSeek-V3-0324 \\\n    --tp-size 8 \\\n    --n-share-experts-fusion 8 \\\n    --dtype fp8_w8a8 \\\n    --tune\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Tune DeepSeek-R1 with channel-wise INT8, TP=16 and n_share_experts_fusion=16\npython benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py \\\n    --model meituan/DeepSeek-R1-Channel-INT8 \\\n    --tp-size 16 \\\n    --n-share-experts-fusion 16 \\\n    --dtype int8_w8a8 \\\n    --tune\n```\n\n----------------------------------------\n\nTITLE: Launching lightLLM Server\nDESCRIPTION: Launches the lightLLM API server with specified model directory and configuration options. It sets maximum total tokens and a specific port for the server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_v0/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython -m lightllm.server.api_server --tokenizer_mode auto --model_dir ~/model_weights/llama-2-7b-chat-hf --max_total_token_num 16000 --port 22000\n```\n\n----------------------------------------\n\nTITLE: Data Type Conversion with pytorch_library_compatible_type\nDESCRIPTION: This C++ code defines a specialization of `pytorch_library_compatible_type` to map `int` to `int64_t` for PyTorch compatibility. It includes a conversion function with checks to ensure the value is within the valid range.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// Map `int` -> `int64_t`\ntemplate <>\nstruct pytorch_library_compatible_type<int> {\n  using type = int64_t;\n  static int convert_from_type(int64_t arg) {\n    TORCH_CHECK(arg <= std::numeric_limits<int>::max(), \"int64_t value is too large to be converted  to int\");\n    TORCH_CHECK(arg >= std::numeric_limits<int>::min(), \"int64_t value is too small to be converted to int\");\n    return arg;\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Run SGLang Docker Container (H200)\nDESCRIPTION: This snippet creates and runs a Docker container for SGLang development on an H200 GPU. It maps the /mnt/co-research/shared-models directory to /root/.cache/huggingface within the container to reuse cached model data and sets up shared memory, network, and privileged access for RDMA.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/development_guide_using_docker.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -itd --shm-size 32g --gpus all -v /mnt/co-research/shared-models:/root/.cache/huggingface --ipc=host --network=host --privileged --name sglang_zhyncs lmsysorg/sglang:dev /bin/zsh\ndocker exec -it sglang_zhyncs /bin/zsh\n```\n\n----------------------------------------\n\nTITLE: Enabling CUDA Language and Finding CUDAToolkit\nDESCRIPTION: Enables the CUDA language for the project and finds the CUDA toolkit. The REQUIRED keyword ensures that the configuration will fail if the CUDA toolkit isn't found. CUDA_SEPARABLE_COMPILATION is set to ON for improved compilation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nenable_language(CUDA)\nfind_package(CUDAToolkit REQUIRED)\nset_property(GLOBAL PROPERTY CUDA_SEPARABLE_COMPILATION ON)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance\nDESCRIPTION: This command runs the benchmark script with a specified number of questions using the guidance backend with specified parameters.  Requires the bench_other.py script and a running guidance-compatible model.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_chain_reasoning/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 8 --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Greedy Token Selection with Deceptive Options in SGLang (Python)\nDESCRIPTION: This code demonstrates a scenario where `greedy_token_selection` can lead to an incorrect answer.  The example presents a multiple-choice question about US presidents with options designed to mislead the model. The function uses `sgl.gen` with `choices_method` set to `sgl.greedy_token_selection`.  The effectiveness of this method depends on the model's behavior and requires a functioning SGLang setup.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/frontend/choices_methods.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@sgl.function\ndef us_president_example(s):\n    s += sgl.user(\"Name a US president.\")\n    s += sgl.assistant(\n        sgl.gen(\n            \"answer\",\n            choices=[\"Donald Duck\", \"Millard Fillmore\"],\n            choices_method=sgl.greedy_token_selection,\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: SGLang: Launching Mixtral-8x7B Server\nDESCRIPTION: This command launches the SGLang server with the Mixtral-8x7B-Instruct-v0.1 model. It sets the port to 30000 and specifies a tensor parallel size of 8. Requires the sglang package and appropriate hardware setup for TP.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_decode_regex/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npython3 -m sglang.launch_server --model-path mistralai/Mixtral-8x7B-Instruct-v0.1 --port 30000 --tp-size 8\n```\n\n----------------------------------------\n\nTITLE: Benchmark Guidance\nDESCRIPTION: This command runs the bench_other.py script to benchmark Guidance with 8 questions in parallel mode. It specifies guidance as the backend, sets the context size to 4096, and provides the model path.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_deep/README.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_other.py --num-questions 8 --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Finding Python Package\nDESCRIPTION: Finds the Python package and its components, including the interpreter, development module, and the SKBUILD_SABI_COMPONENT. The REQUIRED keyword ensures that the configuration will fail if the package isn't found.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(Python COMPONENTS Interpreter Development.Module ${SKBUILD_SABI_COMPONENT} REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Offline Benchmark\nDESCRIPTION: These commands execute the offline benchmark using the `bench_serving.py` script with vLLM as the backend. It configures parameters like the dataset, number of prompts, input/output lengths, and output file. Then, extracts a column from the resulting JSONL file.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# vLLM Offline\n\npython3 bench_serving.py --backend vllm --dataset-name random --num-prompts 4000 --random-input 1024 --random-output 1024 --output-file offline_vllm.jsonl\npython3 bench_serving.py --backend vllm --dataset-name random --num-prompts 5000 --random-input 1024 --random-output 512 --output-file offline_vllm.jsonl\npython3 bench_serving.py --backend vllm --dataset-name random --num-prompts 1000 --random-input 4096 --random-output 2048 --output-file offline_vllm.jsonl\npython3 bench_serving.py --backend vllm --dataset-name random --num-prompts 2000 --random-input 4096 --random-output 1024 --output-file offline_vllm.jsonl\npython3 bench_serving.py --backend vllm --dataset-name random --num-prompts 6000 --random-input 256 --random-output 512 --output-file offline_vllm.jsonl\npython3 bench_serving.py --backend vllm --dataset-name sharegpt --num-prompts 3000 --output-file offline_vllm.jsonl\ncat offline_vllm.jsonl | cut -d':' -f12 | cut -d',' -f1\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang with AWQ Quantization (Bash)\nDESCRIPTION: This command launches the SGLang server with AWQ quantization enabled, specified by the `--quantization moe_wna16` flag. AWQ quantization is used to improve performance. `--trust-remote-code` is used to run the quantization kernel.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model cognitivecomputations/DeepSeek-R1-AWQ --tp 8 --trust-remote-code --quantization moe_wna16\n```\n\n----------------------------------------\n\nTITLE: Running Pre-Commit Checks\nDESCRIPTION: This command runs all configured pre-commit checks on all files in the project. It helps ensure code quality and consistency by automatically formatting and linting code before committing changes. This should be run prior to creating a Pull Request.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Dummy Grok1 Config\nDESCRIPTION: This config.json file provides dummy configurations for a Grok1 model. It defines architecture, scaling factors, vocabulary size, hidden sizes, attention heads, layers, and other hyperparameters necessary for the model's initialization and functioning.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncat ../dummy_grok1/config.json\n{\n  \"architectures\": [\n    \"Grok1ModelForCausalLM\"\n  ],\n  \"embedding_multiplier_scale\": 78.38367176906169,\n  \"output_multiplier_scale\": 0.5773502691896257,\n  \"vocab_size\": 131072,\n  \"hidden_size\": 6144,\n  \"intermediate_size\": 32768,\n  \"max_position_embeddings\": 8192,\n  \"num_experts_per_tok\": 2,\n  \"num_local_experts\": 8,\n  \"num_attention_heads\": 48,\n  \"num_hidden_layers\": 64,\n  \"num_key_value_heads\": 8,\n  \"head_dim\": 128,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"model_type\": \"mixtral\",\n  \"torch_dtype\": \"bfloat16\"\n}\n\n```\n\n----------------------------------------\n\nTITLE: Updating Weights with /update_weights_from_disk API\nDESCRIPTION: Updates model weights from disk using the `/update_weights_from_disk` API. It demonstrates both successful and failed weight updates. A successful update requires the same architecture and parameter size. The code sends a POST request with the model path and asserts the response based on whether the update was expected to succeed or fail.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# successful update with same architecture and size\n\nurl = f\"http://localhost:{port}/update_weights_from_disk\"\ndata = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct\"}\n\nresponse = requests.post(url, json=data)\nprint_highlight(response.text)\nassert response.json()[\"success\"] is True\nassert response.json()[\"message\"] == \"Succeeded to update model weights.\"\n```\n\nLANGUAGE: python\nCODE:\n```\n# failed update with different parameter size or wrong name\n\nurl = f\"http://localhost:{port}/update_weights_from_disk\"\ndata = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct-wrong\"}\n\nresponse = requests.post(url, json=data)\nresponse_json = response.json()\nprint_highlight(response_json)\nassert response_json[\"success\"] is False\nassert response_json[\"message\"] == (\n    \"Failed to get weights iterator: \"\n    \"qwen/qwen2.5-0.5b-instruct-wrong\"\n    \" (repository not found).\"\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Benchmark Images (Python)\nDESCRIPTION: This command downloads the images used for the benchmark from a specified source. It utilizes a Python script, `download_images.py`, to perform the download operation. No external dependencies are explicitly listed, but the script itself likely uses common libraries for downloading files from URLs.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython3 download_images.py\n```\n\n----------------------------------------\n\nTITLE: Enabling Ccache\nDESCRIPTION: Enables the use of ccache for faster builds. It checks if ccache is found and if the environment variable `CCACHE_DIR` is defined. If both conditions are met, it enables ccache.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\noption(ENABLE_CCACHE \"Whether to use ccache\" ON)\nfind_program(CCACHE_FOUND ccache)\nif(CCACHE_FOUND AND ENABLE_CCACHE AND DEFINED ENV{CCACHE_DIR})\n    message(STATUS \"Building with CCACHE enabled\")\n    set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE \"ccache\")\n    set_property(GLOBAL PROPERTY RULE_LAUNCH_LINK \"ccache\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example: Remove Worker (Bash)\nDESCRIPTION: Example bash command to remove a worker from the SGLang Router, specifying the worker URL.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/v0.1.0.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ curl -X POST http://localhost:30000/remove_worker?url=http://127.0.0.1:30001\n```\n\n----------------------------------------\n\nTITLE: Checking Batch Job Status OpenAI (Python)\nDESCRIPTION: This code snippet shows how to check the status of a batch job using the OpenAI Python client. It creates requests, uploads a file, creates a batch job, and then polls the job status using `client.batches.retrieve`. It prints the batch job details including the ID, status, creation time, and file IDs. Dependencies: `json`, `time`, `openai`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_completions.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport time\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n\nrequests = []\nfor i in range(20):\n    requests.append(\n        {\n            \"custom_id\": f\"request-{i}\",\n            \"method\": \"POST\",\n            \"url\": \"/chat/completions\",\n            \"body\": {\n                \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": f\"{i}: You are a helpful AI assistant\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"Write a detailed story about topic. Make it very long.\",\n                    },\n                ],\n                \"max_tokens\": 64,\n            },\n        }\n    )\n\ninput_file_path = \"batch_requests.jsonl\"\nwith open(input_file_path, \"w\") as f:\n    for req in requests:\n        f.write(json.dumps(req) + \"\\n\")\n\nwith open(input_file_path, \"rb\") as f:\n    uploaded_file = client.files.create(file=f, purpose=\"batch\")\n\nbatch_job = client.batches.create(\n    input_file_id=uploaded_file.id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\",\n)\n\nprint_highlight(f\"Created batch job with ID: {batch_job.id}\")\nprint_highlight(f\"Initial status: {batch_job.status}\")\n\ntime.sleep(10)\n\nmax_checks = 5\nfor i in range(max_checks):\n    batch_details = client.batches.retrieve(batch_id=batch_job.id)\n\n    print_highlight(\n        f\"Batch job details (check {i+1} / {max_checks}) // ID: {batch_details.id} // Status: {batch_details.status} // Created at: {batch_details.created_at} // Input file ID: {batch_details.input_file_id} // Output file ID: {batch_details.output_file_id}\"\n    )\n    print_highlight(\n        f\"<strong>Request counts: Total: {batch_details.request_counts.total} // Completed: {batch_details.request_counts.completed} // Failed: {batch_details.request_counts.failed}</strong>\"\n    )\n\n    time.sleep(3)\n```\n\n----------------------------------------\n\nTITLE: Build and Install Python Wheel Package with Pip (Force Reinstall)\nDESCRIPTION: This command combines building and installing the Python wheel package in a single command. It uses `python -m build` to build the wheel and then uses `pip install --force-reinstall` to install it, forcing a reinstall if the package is already present. This is useful for development when frequent changes are made.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m build && pip install --force-reinstall dist/*.whl\n```\n\n----------------------------------------\n\nTITLE: Defining the RAG Pipeline - Python\nDESCRIPTION: This code snippet defines the RAG pipeline by combining the retrieval and generation steps. It takes a question as input, retrieves relevant contexts using the `retrieval` function, generates an answer using the `generation` function, and returns the generated answer. The `@trace` decorator ensures that the entire pipeline is traced by Parea.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@trace\ndef rag_pipeline(question: str) -> str:\n    contexts = retrieval(question)\n    return generation(question, *contexts)\n\n\nrag_pipeline(\n    \"When did the World Health Organization formally declare an end to the COVID-19 global health emergency?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Install Sanic\nDESCRIPTION: This command installs the Sanic web framework, which is used in the custom server example. Sanic is required to run the custom server example.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/readme.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install sanic\n```\n\n----------------------------------------\n\nTITLE: Install Rustup using Curl\nDESCRIPTION: This bash script installs Rustup, the Rust installer and version manager. It fetches the installer script from the specified URL, executes it, and then configures the shell environment to use the installed Rust toolchain. It verifies the installation by checking the versions of `rustc` and `cargo`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# Follow the installation prompts, then reload your shell\nsource $HOME/.cargo/env\n\n# Verify installation\nrustc --version\ncargo --version\n```\n\n----------------------------------------\n\nTITLE: Testing Multi-Node SGLang Setup from Docker (Bash)\nDESCRIPTION: This command runs a benchmark within a Docker container to test the functionality of a multi-node SGLang setup. It connects to the SGLang server and performs basic testing. The `sglang.bench_serving` script is used for benchmarking.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all \\\n    --shm-size 32g \\\n    --network=host \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --name sglang_multinode_client \\\n    -it \\\n    --rm \\\n    --env \"HF_TOKEN=$HF_TOKEN\" \\\n    --ipc=host \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input 1 --random-output 512 --random-range-ratio 1 --num-prompts 1 --host 0.0.0.0 --port 40000 --output-file \"deepseekv3_multinode.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT LLM Offline Benchmark\nDESCRIPTION: These commands run the offline benchmark using `bench_serving.py` with TensorRT LLM (trt) as the backend. It sets the model, dataset, number of prompts, input/output lengths, and output file. The script extracts a specific column from the output jsonl file.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# TensorRT LLM Offline 8B\n\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --num-prompts 4000 --random-input 1024 --random-output 1024 --output-file offline_trt_8b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --num-prompts 5000 --random-input 1024 --random-output 512 --output-file offline_trt_8b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --num-prompts 1000 --random-input 4096 --random-output 2048 --output-file offline_trt_8b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --num-prompts 2000 --random-input 4096 --random-output 1024 --output-file offline_trt_8b.jsonl\npython3 bench_serving.py --backend trt --dataset-name random --num-prompts 6000 --random-input 256 --random-output 512 --output-file offline_trt_8b.jsonl --model meta-llama/Meta-Llama-3-8B-Instruct\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name sharegpt --num-prompts 3000 --output-file offline_trt_8b.jsonl\ncat offline_trt_8b.jsonl | cut -d':' -f12 | cut -d',' -f1\n```\n\n----------------------------------------\n\nTITLE: Fetching External Content (flashinfer)\nDESCRIPTION: Declares and populates the flashinfer repository using FetchContent. This fetches the flashinfer library from GitHub at a specific tag. GIT_SHALLOW is set to OFF indicating that the full history should be fetched.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nFetchContent_Declare(\n    repo-flashinfer\n    GIT_REPOSITORY https://github.com/flashinfer-ai/flashinfer.git\n    GIT_TAG        9220fb3443b5a5d274f00ca5552f798e225239b7\n    GIT_SHALLOW    OFF\n)\nFetchContent_Populate(repo-flashinfer)\n```\n\n----------------------------------------\n\nTITLE: Checking for Mellanox Ethernet Controllers using lspci\nDESCRIPTION: This command uses `lspci` to list PCI devices and filters the output to find Mellanox Ethernet controllers. It verifies that Mellanox NICs are available in the system, which is a prerequisite for RoCE.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ lspci -nn | grep Eth | grep Mellanox\n0000:7f:00.0 Ethernet controller [0200]: Mellanox Technologies MT43244 BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n0000:7f:00.1 Ethernet controller [0200]: Mellanox Technologies MT43244 BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n0000:c7:00.0 Ethernet controller [0200]: Mellanox Technologies MT43244 BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n0000:c7:00.1 Ethernet controller [0200]: Mellanox Technologies MT43244 BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n0001:08:00.0 Ethernet controller [0200]: Mellanox Technologies MT43244 BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n0001:08:00.1 Ethernet controller [0200]: Mellanox Technologies MT43244 BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n0001:a2:00.0 Ethernet controller [0200]: Mellanox Technologies MT43244 BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n0001:a2:00.1 Ethernet controller [0200]: Mellanox Technologies MT43244 BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang VLM Server\nDESCRIPTION: This command launches an SGLang server hosting a specified VLM. It uses the `sglang.launch_server` module to start the server with the given model path, chat template, and port.  The `--model-path` argument specifies the path to the VLM, `--chat-template` indicates the chat template to use, and `--port` sets the server's port.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmmu/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython -m sglang.launch_server --model-path Qwen/Qwen2-VL-7B-Instruct --chat-template qwen2-vl --port 30000\n```\n\n----------------------------------------\n\nTITLE: Testing RoCE Network Speed with qperf\nDESCRIPTION: This code snippet describes how to test the RoCE network speed using `qperf`. First `qperf` must be installed. Then, on the server, `qperf` is executed. On the client, `qperf` is used with specific parameters to test the RDMA write bandwidth. `-t 60` runs the test for 60 seconds, `-cm1` specifies connection mode 1, `<server_ip>` is the IP address of the server. It requires `qperf` to be installed on both the client and the server. It measures the `rc_rdma_write_bw` (RDMA write bandwidth).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nyum install qperf\n# for server：\nexecute qperf\n# for client\nqperf -t 60 -cm1 <server_ip>   rc_rdma_write_bw\n```\n\n----------------------------------------\n\nTITLE: RPD Tracer Loader Script\nDESCRIPTION: This bash script sets up the environment for running RPD tracer. It takes an optional output file name as an argument. It creates an RPD file using `rocpd.schema`, and sets environment variables `RPDT_FILENAME` and `RPDT_AUTOSTART`. It then preloads the `librocm-smi_64` and `librpd_tracer.so` libraries using `LD_PRELOAD` and executes the command provided as arguments.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n################################################################################\n# Copyright (c) 2021 - 2023 Advanced Micro Devices, Inc. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n################################################################################\nOUTPUT_FILE=\"trace.rpd\"\n\nif [ \"$1\" = \"-o\" ] ; then\n  OUTPUT_FILE=$2\n  shift\n  shift\nfi\n\nif [ -e ${OUTPUT_FILE} ] ; then\n  rm ${OUTPUT_FILE}\nfi\n\npython3 -m rocpd.schema --create ${OUTPUT_FILE}\nif [ $? != 0 ] ; then\n  echo \"Error: Could not create rpd file. Please run 'python setup.py install' from the rocpd_python dir\"\n  exit\nfi\n\nexport RPDT_FILENAME=${OUTPUT_FILE}\nexport RPDT_AUTOSTART=0\nLD_PRELOAD=librocm-smi_64:librpd_tracer.so \"$@\"\n```\n\n----------------------------------------\n\nTITLE: Clone Jetson Containers Repository (Bash)\nDESCRIPTION: Clones the jetson-containers GitHub repository.  This repository contains scripts and configurations needed to build and run SGLang within a Docker container on Jetson devices.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/nvidia_jetson.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/dusty-nv/jetson-containers.git\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Triton Backend\nDESCRIPTION: This command launches the SGLang server using the Triton attention backend.  It specifies the model and optionally sets tensor parallelism.  The --trust-remote-code flag may be needed for specific models.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/attention_backend.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model meta-llama/Meta-Llama-3.1-8B-Instruct --attention-backend triton\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --tp 8 --model deepseek-ai/DeepSeek-V3 --attention-backend triton --trust-remote-code\n```\n\n----------------------------------------\n\nTITLE: Precompile DeepGEMM Kernels for DeepSeek Model\nDESCRIPTION: This command precompiles DeepGEMM kernels, which are optimized for FP8 matrix multiplications, before serving the DeepSeek model. It requires the `sglang` package to be installed. The `--model` argument specifies the DeepSeek model to use, `--tp` specifies the tensor parallelism degree, and `--trust-remote-code` allows the execution of code from the model repository.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deepseek.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code\n```\n\n----------------------------------------\n\nTITLE: Terminate SGLang server process\nDESCRIPTION: This snippet terminates the SGLang server process using the `terminate_process` function.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/lora.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Benchmark Serving with Limited Prompts (Bash)\nDESCRIPTION: This command benchmarks the serving performance with a limited number of prompts and output length, generating smaller trace files suitable for browsers. Used in conjunction with PyTorch Profiler.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 2 --sharegpt-output-len 100 --profile\n```\n\n----------------------------------------\n\nTITLE: Applying Evaluation Metrics to Generation - Python\nDESCRIPTION: This code snippet applies evaluation metrics to the generation step of the RAG pipeline using Parea. It imports `answer_context_faithfulness_statement_level_factory` and `answer_matches_target_llm_grader_factory` from `parea.evals` and creates instances of them.  The `max_tokens` argument is also updated for the `generation_sglang` function to avoid token cut-offs.  The `@trace` decorator for the `generation` function is updated to include these evaluation functions.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom parea.evals.general import answer_matches_target_llm_grader_factory\nfrom parea.evals.rag import answer_context_faithfulness_statement_level_factory\n\n\nanswer_context_faithfulness = answer_context_faithfulness_statement_level_factory()\nanswer_matches_target_llm_grader = answer_matches_target_llm_grader_factory()\n\n\n@function\ndef generation_sglang(s, question: str, *context: str):\n    context = \"\\n\".join(context)\n    s += user(\n        f\"Given this question:\\n{question}\\n\\nAnd this context:\\n{context}\\n\\nAnswer the question.\"\n    )\n    s += assistant(gen(\"answer\", max_tokens=1_000))\n\n\n@trace(eval_funcs=[answer_context_faithfulness, answer_matches_target_llm_grader])\ndef generation(question: str, *context):\n    state: ProgramState = generation_sglang.run(question, *context)\n    while not state.stream_executor.is_finished:\n        time.sleep(1)\n    return state.stream_executor.variables[\"answer\"]\n```\n\n----------------------------------------\n\nTITLE: Generating SGLang Requests for Metric Collection (Bash)\nDESCRIPTION: This command generates SGLang requests to populate the metrics. It uses `sglang.bench_serving` to simulate requests with random inputs and outputs.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/production_metrics.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 3000 --random-input 1024 --random-output 1024 --random-range-ratio 0.5\n```\n\n----------------------------------------\n\nTITLE: Sending Request to sglang-triton Server\nDESCRIPTION: This command sends a POST request to the sglang-triton server to generate text based on the input text. It specifies the model endpoint as `character_generation` and provides the input text as `harry`. Requires `curl` and a running sglang-triton server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/triton/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8000/v2/models/character_generation/generate \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"INPUT_TEXT\": [\"harry\"]\n}'\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"INPUT_TEXT\": [\"harry\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang and vLLM Servers (Offline Benchmark, Llama 3.1 70B)\nDESCRIPTION: Launches SGLang and vLLM servers for offline benchmarking with Llama 3.1 70B Instruct model on 4 H100 GPUs. SGLang disables radix cache, sets tensor parallelism to 4, and sets `mem-frac` to 0.88. vLLM disables request logging, sets the number of scheduler steps, sets tensor parallelism to 4, and sets the maximum model length.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/benchmark_vllm_060/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Llama 3.1 70B Instruct on 4 x H100\npython -m sglang.launch_server --model-path meta-llama/Llama-3.1-70B-Instruct --disable-radix-cache --tp 4 --mem-frac 0.88\npython -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-3.1-70B-Instruct --disable-log-requests --num-scheduler-steps 10 --tensor 4 --max_model_len 4096\n```\n\n----------------------------------------\n\nTITLE: Profile One Batch with bench_one_batch.py (Bash)\nDESCRIPTION: This command profiles a single batch using `sglang.bench_one_batch` with the PyTorch profiler. The batch size can be controlled using the `--batch` argument.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.bench_one_batch --model-path meta-llama/Llama-3.1-8B-Instruct --batch 32 --input-len 1024 --output-len 10 --profile\n```\n\n----------------------------------------\n\nTITLE: Non-Streaming Request Without Reasoning in Python\nDESCRIPTION: This code snippet shows how to make a non-streaming request to the SGLang server without enabling reasoning separation. It sets the `separate_reasoning` option to `False` and prints the original output without separation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresponse_non_stream = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    temperature=0.6,\n    top_p=0.95,\n    stream=False,  # Non-streaming\n    extra_body={\"separate_reasoning\": False},\n)\n\nprint_highlight(\"==== Original Output ====\")\nprint_highlight(response_non_stream.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Launch vLLM Server (Python)\nDESCRIPTION: This command launches the vLLM server using the CodeLlama-7b-instruct-hf model. It specifies the tokenizer mode, disables log requests, sets the port, and allocates GPU resources.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/long_json_decode/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model codellama/CodeLlama-7b-instruct-hf  --disable-log-requests --port 21000 --gpu 0.97\n```\n\n----------------------------------------\n\nTITLE: Install build and twine\nDESCRIPTION: Installs the `build` and `twine` packages using pip. `build` is used to build the package, and `twine` is used to upload it to PyPI. This command is executed from the terminal.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/release_process.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install build twine\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for sglang-triton\nDESCRIPTION: This command builds a Docker image named `sglang-triton` using the Dockerfile in the current directory.  The `-t` flag specifies the tag for the image. Requires Docker to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/triton/README.md#_snippet_0\n\nLANGUAGE: docker\nCODE:\n```\ndocker build -t sglang-triton .\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files for Flash Attention Kernel (BF16, FP16, FP8)\nDESCRIPTION: This section uses the `file(GLOB)` command to find source files for different floating-point precisions (BF16, FP16, FP8) of the Flash Attention kernel.  It appends the lists of generated sources into combined lists for each precision.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\nif (SGL_KERNEL_ENABLE_FA3)\n    file(GLOB FA3_BF16_GEN_SRCS\n        \"${repo-flash-attention_SOURCE_DIR}/hopper/instantiations/flash_fwd_hdimall_bf16*_sm90.cu\")\n    file(GLOB FA3_BF16_GEN_SRCS_\n        \"${repo-flash-attention_SOURCE_DIR}/hopper/instantiations/flash_fwd_hdimdiff_bf16*_sm90.cu\")\n    list(APPEND FA3_BF16_GEN_SRCS ${FA3_BF16_GEN_SRCS_})\n\n    # FP16 source files\n    file(GLOB FA3_FP16_GEN_SRCS\n        \"${repo-flash-attention_SOURCE_DIR}/hopper/instantiations/flash_fwd_hdimall_fp16*_sm90.cu\")\n    file(GLOB FA3_FP16_GEN_SRCS_\n        \"${repo-flash-attention_SOURCE_DIR}/hopper/instantiations/flash_fwd_hdimdiff_fp16*_sm90.cu\")\n    list(APPEND FA3_FP16_GEN_SRCS ${FA3_FP16_GEN_SRCS_})\n\n    # FP8 source files\n    file(GLOB FA3_FP8_GEN_SRCS\n        \"${repo-flash-attention_SOURCE_DIR}/hopper/instantiations/flash_fwd_hdimall_e4m3*_sm90.cu\")\n    file(GLOB FA3_FP8_GEN_SRCS_\n        \"${repo-flash-attention_SOURCE_DIR}/hopper/instantiations/flash_fwd_hdimdiff_e4m3*_sm90.cu\")\n    list(APPEND FA3_FP8_GEN_SRCS ${FA3_FP8_GEN_SRCS_})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Run TGI Docker container\nDESCRIPTION: Runs a Docker container for Text Generation Inference (TGI) with specified configurations, including model path, number of shards, and resource limits. This command sets up the TGI server for benchmarking.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/dspy/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\ndocker run --name tgi --rm -ti --gpus all --network host \\\n  -v /home/ubuntu/model_weights/Llama-2-7b-chat-hf:/Llama-2-7b-chat-hf \\\n  ghcr.io/huggingface/text-generation-inference:1.3.0 \\\n  --model-id /Llama-2-7b-chat-hf --num_shard 1  --trust-remote-code \\\n  --max-input-length 2048 --max-total-tokens 4096 \\\n  --port 24000\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server with TorchAO (Python)\nDESCRIPTION: Launches the SGLang server with TorchAO's int4 weight-only quantization for memory efficiency on Jetson Orin. The `--torchao-config int4wo-128` enables this quantization with a 128-group size, `--dtype bfloat16` enables bfloat16 precision and `--context-length` sets the context length.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/nvidia_jetson.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython -m sglang.launch_server \\\n    --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n    --device cuda \\\n    --dtype bfloat16 \\\n    --attention-backend flashinfer \\\n    --mem-fraction-static 0.8 \\\n    --context-length 8192 \\\n    --torchao-config int4wo-128\n```\n\n----------------------------------------\n\nTITLE: Offline Engine: Regular Expression\nDESCRIPTION: This example showcases the use of regular expressions with SGLang's offline engine. It defines prompts and a regular expression, generating text that adheres to the specified pattern. The generated text and the associated prompt are then printed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"Please provide information about London as a major global city:\",\n    \"Please provide information about Paris as a major global city:\",\n]\n\nsampling_params = {\"temperature\": 0.8, \"top_p\": 0.95, \"regex\": \"(France|England)\"}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print_highlight(\"===============================\")\n    print_highlight(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Terminating the SGLang Server Process\nDESCRIPTION: This snippet terminates the SGLang server process that was launched earlier. It uses the `terminate_process` function from `sglang.utils` to stop the server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/speculative_decoding.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: This command launches the sglang server using the specified model path and port number. It uses the `sglang.launch_server` module.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/generative_agents/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Download and Setup VSCode Tunnel (CLI)\nDESCRIPTION: This snippet downloads the VSCode CLI, extracts it, and starts a tunnel for remote access. This allows connecting to the development environment from a local machine.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/development_guide_using_docker.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://vscode.download.prss.microsoft.com/dbazure/download/stable/fabdb6a30b49f79a7aba0f2ad9df9b399473380f/vscode_cli_alpine_x64_cli.tar.gz\ntar xf vscode_cli_alpine_x64_cli.tar.gz\n\n# https://code.visualstudio.com/docs/remote/tunnels\n./code tunnel\n```\n\n----------------------------------------\n\nTITLE: Launch vLLM Server\nDESCRIPTION: This command launches the vLLM API server with the specified model (meta-llama/Llama-2-7b-chat-hf) on port 21000. It disables request logging and sets the tokenizer mode to auto.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_deep/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Installing Target\nDESCRIPTION: Installs the `common_ops` library to the `sgl_kernel` directory. This makes the library available for use by other projects.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_22\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS common_ops LIBRARY DESTINATION sgl_kernel)\n```\n\n----------------------------------------\n\nTITLE: Benchmark SGLang (Python)\nDESCRIPTION: Runs the SGLang benchmark script with a specified number of subtasks.  `--nsub` argument sets the number of subtasks for the benchmark.  This command uses `bench_sglang.py`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --nsub 10\n```\n\n----------------------------------------\n\nTITLE: SGLang: Launching Llama-7B Server\nDESCRIPTION: This command launches the SGLang server with the Llama-2-7b-chat-hf model.  It specifies the model path and sets the port to 30000. Requires the sglang package to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_decode_regex/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npython3 -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard and Flags\nDESCRIPTION: Configures the C++ standard to C++17 and sets the compiler flags. `-O3` enables aggressive optimization.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -O3\")\n```\n\n----------------------------------------\n\nTITLE: Setting NCCL Debug Level\nDESCRIPTION: This bash command is for debugging NCCL communication problems. Setting the environment variable `NCCL_DEBUG=TRACE` enables detailed logging of NCCL activities, which helps in identifying the root cause of communication issues within the distributed inference setup.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nNCCL_DEBUG=TRACE\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: Launches the SGLang server with a specified model path and port. This command initiates the SGLang server, which is necessary for running benchmarks against the SGLang framework. The server uses a Llama-2-7b-chat-hf model.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_v0/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies using pip\nDESCRIPTION: This command installs the required Python packages for the project using pip and the requirements.txt file. It ensures that all necessary dependencies are available before running the documentation tools or contributing to the project.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Project Declaration\nDESCRIPTION: Declares the project name and the programming languages used in the project. Here, the project is named \"sgl-kernel\" and uses both C++ (CXX) and CUDA.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nproject(sgl-kernel LANGUAGES CXX CUDA)\n```\n\n----------------------------------------\n\nTITLE: Shutdown SGLang Engine\nDESCRIPTION: This code shuts down the SGLang engine, releasing any resources held by it.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nllm.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Run SGLang Container (Bash)\nDESCRIPTION: Runs the SGLang Docker container with NVIDIA runtime. `--runtime nvidia` enables GPU acceleration, `--it` provides interactive terminal access, `--rm` removes the container upon exit, and `--network=host` uses the host network.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/nvidia_jetson.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --runtime nvidia -it --rm --network=host IMAGE_NAME\n```\n\n----------------------------------------\n\nTITLE: Fetching External Content (DeepGEMM)\nDESCRIPTION: Declares and populates the DeepGEMM repository using FetchContent. This fetches the DeepGEMM library from GitHub at a specific tag. GIT_SHALLOW is set to OFF indicating that the full history should be fetched.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nFetchContent_Declare(\n    repo-deepgemm\n    GIT_REPOSITORY https://github.com/deepseek-ai/DeepGEMM\n    GIT_TAG        4499c4ccbb5d3958b1a069f29ef666156a121278\n    GIT_SHALLOW    OFF\n)\nFetchContent_Populate(repo-deepgemm)\n```\n\n----------------------------------------\n\nTITLE: Nsight Systems - Stop Profiler (Bash)\nDESCRIPTION: This command stops the Nsight Systems profiler for a specific session, allowing manual termination and generation of report files.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nnsys stop --session=profile-XXXXX\n```\n\n----------------------------------------\n\nTITLE: Configuring Runner Inside Docker Container\nDESCRIPTION: These commands update the package lists, install required packages (curl, python3-pip, git), and set an environment variable to allow running as root. These steps are necessary to prepare the environment for configuring the GitHub Actions runner.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/setup_github_runner.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napt update && apt install -y curl python3-pip git\nexport RUNNER_ALLOW_RUNASROOT=1\n```\n\n----------------------------------------\n\nTITLE: Send Profiling Request from Client (Bash)\nDESCRIPTION: This command sends a profiling request from a client to the SGLang server using `sglang.bench_serving`.  It specifies the backend, model, number of prompts, output length, and enables profiling.  The `SGLANG_TORCH_PROFILER_DIR` must be set correctly for trace files to be generated.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 10 --sharegpt-output-len 100 --profile\n```\n\n----------------------------------------\n\nTITLE: Benchmark Guidance (Python)\nDESCRIPTION: Runs the benchmark script for Guidance with specified parameters. The `--nsub` argument sets the number of subtasks, `--backend` sets the backend to guidance, `--parallel` sets the parallelism level, `--n-ctx` sets the context length, and `--model-path` specifies the path to the GGUF model.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --nsub 10 --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Launching LightLLM API Server\nDESCRIPTION: This command launches the LightLLM API server, specifying the model directory, maximum total token number, and port. It serves the model from the specified directory on port 22000.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hellaswag/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython -m lightllm.server.api_server --tokenizer_mode auto --model_dir ~/model_weights/llama-2-7b-chat-hf --max_total_token_num 16000 --port 22000\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang\nDESCRIPTION: Executes the SGLang benchmark script with specified parameters for the number of questions and parallel processing.  The script `bench_sglang.py` is executed with different configurations to measure performance with varying levels of parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_v0/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 32 --parallel 16\n```\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 10 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Installing Wikipedia package with pip\nDESCRIPTION: This command installs the Wikipedia Python package using pip. It is a prerequisite for building the dataset used in the long document information retrieval benchmark.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install wikipedia\n```\n\n----------------------------------------\n\nTITLE: Run SGLang Benchmark\nDESCRIPTION: This command executes the benchmark script `bench_sglang.py`. It assumes that the SGLang server is already running and accessible. The script likely interacts with the server to measure performance metrics.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_schema/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_sglang.py\n```\n\n----------------------------------------\n\nTITLE: Installing SGLang from source on AMD ROCm (Bash)\nDESCRIPTION: This snippet details the process of installing SGLang from source specifically for AMD ROCm systems, including cloning the repository, installing dependencies, building the sgl-kernel, and installing the python package with hip-specific dependencies.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/start/install.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Use the last release branch\ngit clone -b v0.4.5.post3 https://github.com/sgl-project/sglang.git\ncd sglang\n\npip install --upgrade pip\ncd sgl-kernel\npython setup_rocm.py install\ncd ..\npip install -e \"python[all_hip]\"\n```\n\n----------------------------------------\n\nTITLE: Removing quantization_config from config.json - JSON\nDESCRIPTION: This JSON snippet shows how to remove the `quantization_config` block from a `config.json` file to resolve a ValueError related to weight partitioning and quantization block size. This is a workaround for potential incompatibility issues with fp16/bf16 checkpoints.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"quantization_config\": {\n        \"activation_scheme\": \"dynamic\",\n        \"fmt\": \"e4m3\",\n        \"quant_method\": \"fp8\",\n        \"weight_block_size\": [128, 128]\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Shutdown LLM - Python\nDESCRIPTION: Shuts down the language model. This code snippet assumes that `llm` is an instance of a language model class and that it has a `shutdown()` method.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/function_calling.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nllm.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance character generation\nDESCRIPTION: This command benchmarks the Guidance framework for character generation. It executes the `bench_other.py` script with the `--backend guidance` option, specifying the model path, context length, and parallel requests.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_other.py --mode character --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Container with Specific AMD GPUs\nDESCRIPTION: This command runs a Docker container with support for only the specified AMD GPUs, using the sglang image. It mounts a volume for sharing Hugging Face model weights cache. The command uses `/tmp/huggingface` as an example. The device flags renderD176 and renderD184 specify the specific GPUs to be used.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/setup_github_runner.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# AMD just the last 2 GPUs\ndocker run --rm --device=/dev/kfd --device=/dev/dri/renderD176 --device=/dev/dri/renderD184 --group-add video --shm-size 128g -it -v /tmp/huggingface:/hf_home lmsysorg/sglang:v0.4.5.post3-rocm630 /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM server with Llama-7B (Python)\nDESCRIPTION: This command launches the vLLM API server with the Llama-2-7b-chat-hf model on port 21000. It disables request logging and sets the tokenizer mode to auto.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf  --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Offline Engine: Structural Tagging\nDESCRIPTION: This snippet demonstrates using structural tags with the SGLang offline engine to guide text generation. Structural tags are defined with begin and end markers and associated schemas, allowing for structured control over the output. Requires the tokenizer to be initialized.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntext = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nprompts = [text]\n\n\nsampling_params = {\n    \"temperature\": 0.8,\n    \"top_p\": 0.95,\n    \"structural_tag\": json.dumps(\n        {\n            \"type\": \"structural_tag\",\n            \"structures\": [\n                {\n                    \"begin\": \"<function=get_current_weather>\",\n                    \"schema\": schema_get_current_weather,\n                    \"end\": \"</function>\",\n                },\n                {\n                    \"begin\": \"<function=get_current_date>\",\n                    \"schema\": schema_get_current_date,\n                    \"end\": \"</function>\",\n                },\n            ],\n            \"triggers\": [\"<function=\"],\n        }\n    ),\n}\n\n\n# Send POST request to the API endpoint\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print_highlight(\"===============================\")\n    print_highlight(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\n----------------------------------------\n\nTITLE: RPD Tracer Makefile Patch\nDESCRIPTION: This patch modifies the RPD tracer Makefile to remove RocmSmiDataSource.cpp from the build process. It updates the `RPD_SRCS` variable by removing `RocmSmiDataSource.cpp`, affecting which source files are compiled during the build process.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndiff --git a/rpd_tracer/Makefile b/rpd_tracer/Makefile\nindex e9d9feb..b2e9e1a 100644\n--- a/rpd_tracer/Makefile\n+++ b/rpd_tracer/Makefile\n@@ -16,7 +16,7 @@ ifneq (,$(HIP_PATH))\n         $(info Building with roctracer)\n         RPD_LIBS += -L/opt/rocm/lib -lroctracer64 -lroctx64 -lamdhip64 -lrocm_smi64\n         RPD_INCLUDES += -I/opt/rocm/include -I/opt/rocm/include/roctracer -I/opt/rocm/include/hsa\n-        RPD_SRCS += RoctracerDataSource.cpp RocmSmiDataSource.cpp\n+        RPD_SRCS += RoctracerDataSource.cpp\n         RPD_INCLUDES += -D__HIP_PLATFORM_AMD__\n endif\n```\n\n----------------------------------------\n\nTITLE: Launching vLLM API Server\nDESCRIPTION: This command starts the vLLM API server, specifying the model, disabling request logging, and setting the port.  It uses the meta-llama/Llama-2-7b-chat-hf model and listens on port 21000.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hellaswag/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 -m vllm.entrypoints.api_server --tokenizer-mode auto --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests --port 21000\n```\n\n----------------------------------------\n\nTITLE: Launching sglang server with Mixtral-8x7B (Python)\nDESCRIPTION: This command launches the sglang server with the Mixtral-8x7B-Instruct-v0.1 model on port 30000, using tensor parallelism with a size of 8. The `--mem-fraction-static` parameter might need adjustment to prevent CUDA out-of-memory errors.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython3 -m sglang.launch_server --model-path mistralai/Mixtral-8x7B-Instruct-v0.1 --port 30000 --tp-size 8\n```\n\n----------------------------------------\n\nTITLE: Launching server with Jinja chat template file - Bash\nDESCRIPTION: This bash command launches the SGLang server and loads a custom chat template from a Jinja template file. The `--chat-template` argument specifies the path to the Jinja file (e.g., './my_model_template.jinja'). The Jinja template format is defined by Hugging Face Transformers.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/custom_chat_template.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --chat-template ./my_model_template.jinja\n```\n\n----------------------------------------\n\nTITLE: Applying Evaluation Metrics to Retrieval - Python\nDESCRIPTION: This code snippet applies evaluation metrics to the retrieval step of the RAG pipeline using Parea. It imports the `context_query_relevancy_factory` and `percent_target_supported_by_context_factory` from `parea.evals.rag` and creates instances of these evaluation functions. The `@trace` decorator is updated to include these evaluation functions, allowing Parea to automatically log the results of these metrics to the dashboard.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/rag_using_parea/trace_and_evaluate_rag_using_parea.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom parea.evals.rag import (\n    context_query_relevancy_factory,\n    percent_target_supported_by_context_factory,\n)\n\n\ncontext_relevancy_eval = context_query_relevancy_factory()\npercent_target_supported_by_context = percent_target_supported_by_context_factory()\n\n\n@trace(eval_funcs=[context_relevancy_eval, percent_target_supported_by_context])\ndef retrieval(question: str) -> List[str]:\n    return collection.query(query_texts=[question], n_results=1)[\"documents\"][0]\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with Chunked Prefill Size Configuration\nDESCRIPTION: This command launches the SGLang server and sets a smaller chunked prefill size using `--chunked-prefill-size 4096`. This is helpful to reduce memory consumption during the prefill phase for long prompts, addressing out-of-memory errors.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/server_arguments.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --chunked-prefill-size 4096\n```\n\n----------------------------------------\n\nTITLE: Run Jetson Containers Installation Script (Bash)\nDESCRIPTION: Executes the installation script from the jetson-containers repository.  This script performs necessary setup steps for building and using Docker containers on the Jetson device.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/nvidia_jetson.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash jetson-containers/install.sh\n```\n\n----------------------------------------\n\nTITLE: Running SGLang Benchmark in Docker\nDESCRIPTION: This snippet shows how to run the SGLang benchmark within a Docker container.  It utilizes the `drun` alias, specifying the sglang backend, dataset name (random), number of prompts, and random input and output sizes.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/amd.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndrun sglang_image \\\n    python3 -m sglang.bench_serving \\\n    --backend sglang \\\n    --dataset-name random \\\n    --num-prompts 4000 \\\n    --random-input 128 \\\n    --random-output 128\n```\n\n----------------------------------------\n\nTITLE: Benchmark SGLang (Python)\nDESCRIPTION: This command benchmarks the SGLang server using the bench_sglang.py script. It specifies the number of questions to test and the degree of parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/long_json_decode/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 5 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Benchmarking vLLM\nDESCRIPTION: This command benchmarks vLLM using `bench_other.py`. It sets the backend to vllm and specifies the number of questions to benchmark.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_document_qa/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython3 bench_other.py --backend vllm --num-questions 64\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance\nDESCRIPTION: This command runs the `bench_other.py` script to benchmark guidance. Key parameters include `--backend guidance`, `--n-ctx` (context length), and `--model-path`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/generative_agents/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-events 1000 --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Benchmarking vLLM\nDESCRIPTION: This command executes a benchmark script against a vLLM backend. The --num-questions parameter specifies the number of questions, and --backend vllm designates the vLLM backend. It requires a running vLLM server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hellaswag/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 200 --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Annotate Code Regions with NVTX (Python)\nDESCRIPTION: This code snippet demonstrates how to use NVTX to annotate critical code regions in Python. The `nvtx.annotate` context manager allows specifying a description and color for the annotated region, which will be visible in Nsight Systems.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport nvtx\nwith nvtx.annotate(\"description\", color=\"color\"):\n    # some critical code\n```\n\n----------------------------------------\n\nTITLE: Display SGLang Router launch_server usage\nDESCRIPTION: This command executes the launch_server.py script within the sglang_router package and displays the help message. The help message outlines available command-line arguments and their purposes.  This is helpful for understanding how to configure and run the server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/router/router.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang_router.launch_server --help\n```\n\n----------------------------------------\n\nTITLE: Running LLaVA Benchmark (Bash)\nDESCRIPTION: This command executes the LLaVA benchmark script. It sets the `CUDA_VISIBLE_DEVICES` environment variable and then runs the `bench_hf_llava_bench.sh` script. This setup ensures that the LLaVA benchmark runs on the specified GPU.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/sglang/benchmark/llava_bench\nCUDA_VISIBLE_DEVICES=0 bash bench_hf_llava_bench.sh\n```\n\n----------------------------------------\n\nTITLE: Launch LightLLM Server (Python)\nDESCRIPTION: This command launches the LightLLM server using the specified model. It uses the llama-2-7b-chat-hf model, assumed to be located in the ~/model_weights/ directory. The --max_total_token_num argument sets the maximum total token number to 16000, and the server listens on port 22000.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mtbench/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython -m lightllm.server.api_server --tokenizer_mode auto --model_dir ~/model_weights/llama-2-7b-chat-hf --max_total_token_num 16000 --port 22000\n```\n\n----------------------------------------\n\nTITLE: Applying nest_asyncio for Offline Engine in IPython\nDESCRIPTION: If using the SGLang Offline Engine in an IPython environment or other nested loop contexts, this code snippet applies the `nest_asyncio` patch. This allows the asyncio event loop to be nested, preventing errors when using asynchronous functions within a synchronous environment. It is a prerequisite for running the asynchronous examples in interactive Python environments.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/offline_engine_api.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n\n```\n\n----------------------------------------\n\nTITLE: Build Dataset using Wikipedia\nDESCRIPTION: This snippet installs the Wikipedia Python package and then executes the build_dataset.py script. The script likely uses the Wikipedia package to generate a dataset for benchmarking purposes. Requires Python 3.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_decode_regex/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install wikipedia\npython3 build_dataset.py\n```\n\n----------------------------------------\n\nTITLE: Benchmark LMQL\nDESCRIPTION: This command runs the bench_other.py script to benchmark LMQL with 8 questions in parallel mode. It specifies lmql as the backend.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_deep/README.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_other.py --num-questions 8 --backend lmql --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang\nDESCRIPTION: This command executes the `bench_sglang.py` script to benchmark sglang with a specified number of events and parallel execution. The `--num-events` parameter sets the number of events to process, and `--parallel` controls the level of parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/generative_agents/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-events 1000 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Benchmark LMQL\nDESCRIPTION: This command benchmarks LMQL using the bench_other.py script. It specifies the backend as lmql, and runs the benchmark with 32 questions and a parallelism of 1.  It assumes the LMQL environment is set up correctly.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tip_suggestion/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_other.py --backend lmql --num-questions 32 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Upload package to PyPI\nDESCRIPTION: This command changes the current directory to `python` and then executes the `upload_pypi.sh` script. This script likely contains the commands to build the package and upload it to PyPI using `twine`. Assumes `upload_pypi.sh` exists in the python directory.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/release_process.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd python\nbash upload_pypi.sh\n```\n\n----------------------------------------\n\nTITLE: Benchmark vLLM (Python)\nDESCRIPTION: This command runs the benchmark script bench_other.py for the vLLM backend. The --num-questions argument specifies the number of questions to use for the benchmark (80), and --backend vllm specifies the backend to use. The script evaluates the performance of the vLLM server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mtbench/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 80 --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Llama 4 Scout with lm_eval\nDESCRIPTION: This set of commands launches the SGLang server with the Llama-4-Scout-17B-16E-Instruct model and then runs an evaluation using lm_eval.  The launch command configures tensor parallelism, memory fraction, and context length. The lm_eval command benchmarks the model's accuracy on the mmlu_pro dataset using a local chat completions endpoint.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/llama4.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Llama-4-Scout-17B-16E-Instruct model\npython -m sglang.launch_server --model-path meta-llama/Llama-4-Scout-17B-16E-Instruct --port 30000 --tp 8 --mem-fraction-static 0.8 --context-length 65536\nlm_eval --model local-chat-completions --model_args model=meta-llama/Llama-4-Scout-17B-16E-Instruct,base_url=http://localhost:30000/v1/chat/completions,num_concurrent=128,timeout=999999,max_gen_toks=2048 --tasks mmlu_pro --batch_size 128 --apply_chat_template --num_fewshot 0\n```\n\n----------------------------------------\n\nTITLE: Install Nsight Systems (Bash)\nDESCRIPTION: These commands install Nsight Systems using apt. They update the package list, install gnupg, add the NVIDIA developer tools repository, fetch the repository key, update the package list again, and finally install the `nsight-systems-cli` package.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# install nsys\n# https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html\napt update\napt install -y --no-install-recommends gnupg\necho \"deb http://developer.download.nvidia.com/devtools/repos/ubuntu$(source /etc/lsb-release; echo \"$DISTRIB_RELEASE\" | tr -d .)/$(dpkg --print-architecture) /\" | tee /etc/apt/sources.list.d/nvidia-devtools.list\napt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\napt update\napt install nsight-systems-cli\n```\n\n----------------------------------------\n\nTITLE: Querying the SGLang Server using OpenAI Client\nDESCRIPTION: This snippet uses the OpenAI client to query the SGLang server. It sends a chat completion request with a simple question. The response is then printed to the console.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/speculative_decoding.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n    ],\n    temperature=0,\n    max_tokens=64,\n)\n\nprint_highlight(f\"Response: {response}\")\n```\n\n----------------------------------------\n\nTITLE: Disabling NUMA Auto-Balancing\nDESCRIPTION: This snippet disables NUMA auto-balancing, a recommended system-level optimization for AMD GPUs. It uses `sudo sh -c` to write the value 0 to the `/proc/sys/kernel/numa_balancing` file, effectively disabling the feature.  This can be automated or verified by external scripts.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/amd.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server\nDESCRIPTION: This command launches the SGLang server with a specified model path and port.  It is necessary to run this command before running benchmark.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/reasoning_benchmark/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --port 30000\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Online Benchmark\nDESCRIPTION: These commands execute the online benchmark using the `bench_serving.py` script with vLLM as the backend. Parameters include the dataset, input/output lengths, number of prompts, request rate, and the output file. The final command extracts specific data from the generated JSONL file.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# vLLM Online\n\npython3 bench_serving.py --backend vllm --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 300 --request-rate 1 --output-file online_vllm.jsonl\npython3 bench_serving.py --backend vllm --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 600 --request-rate 2 --output-file online_vllm.jsonl\npython3 bench_serving.py --backend vllm --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 1200 --request-rate 4 --output-file online_vllm.jsonl\npython3 bench_serving.py --backend vllm --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 2400 --request-rate 8 --output-file online_vllm.jsonl\npython3 bench_serving.py --backend vllm --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 3200 --request-rate 16 --output-file online_vllm.jsonl\ncat online_vllm.jsonl | cut -d':' -f9 | cut -d',' -f1\n```\n\n----------------------------------------\n\nTITLE: Benchmarking LMQL\nDESCRIPTION: This command executes a benchmark using the LMQL backend. It sets the number of questions, specifies the LMQL backend and port, and sets the parallelism. An LMQL server instance is a prerequisite.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hellaswag/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 200 --backend lmql --port 23000 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Benchmark vLLM (Python)\nDESCRIPTION: Runs the benchmark script for vLLM with a specified number of subtasks and backend. The `--nsub` argument sets the number of subtasks, and the `--backend` argument specifies the backend as vllm. This utilizes `bench_other.py`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --nsub 10 --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build Options with uv\nDESCRIPTION: This command configures CMake build options by adding `-Ccmake.define.<option>=<value>` to the `uv build` flags. For example, to enable building FP4 kernels, use:\n\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m uv build --wheel -Cbuild-dir=build -Ccmake.define.SGL_KERNEL_ENABLE_FP4=1 --color=always .\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT LLM Online Benchmark\nDESCRIPTION: These commands execute the online benchmark using `bench_serving.py` with TensorRT LLM. The parameters include backend, model, dataset, input/output length, number of prompts and request rate. The output from the benchmark is stored in a jsonl file and parsed to get the required data.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# TensorRT LLM Online 8B\n\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 300 --request-rate 1 --output-file online_trt_8b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 600 --request-rate 2 --output-file online_trt_8b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 1200 --request-rate 4 --output-file online_trt_8b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 2400 --request-rate 8 --output-file online_trt_8b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 3200 --request-rate 16 --output-file online_trt_8b.jsonl\ncat online_trt_8b.jsonl | cut -d':' -f9 | cut -d',' -f1\n```\n\n----------------------------------------\n\nTITLE: Compiling and Serving Documentation with Custom Port\nDESCRIPTION: This command compiles the documentation and serves it locally on a custom port specified by the PORT environment variable. It allows developers to avoid port conflicts and easily access the documentation preview.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nPORT=8080 make serve\n```\n\n----------------------------------------\n\nTITLE: Terminating SGLang Server Process, Python\nDESCRIPTION: This code terminates the launched SGLang server process. It uses the `terminate_process` function, which is assumed to be defined elsewhere in the project (likely in `sglang.utils`).\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/openai_api_vision.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(vision_process)\n```\n\n----------------------------------------\n\nTITLE: Running Multi-turn Benchmark (Python)\nDESCRIPTION: This snippet shows how to run the `bench_multiturn.py` script to execute a multi-turn benchmark using the Qwen model. It specifies the model path as an argument.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hicache/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython bench_multiturn.py --model-path Qwen/Qwen2.5-14B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Benchmark vLLM (Python)\nDESCRIPTION: This command benchmarks vLLM using the bench_other.py script. It specifies the backend as vllm and the number of questions to test.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/long_json_decode/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --backend vllm --num-questions 5\n```\n\n----------------------------------------\n\nTITLE: Kubectl Apply Output\nDESCRIPTION: This output shows the status of the deployed SGLang pods after applying the `lws.yaml` file using `kubectl apply`. It indicates that the leader pod (`sglang-0`) and one worker pod (`sglang-0-1`) are running, but the leader may not be fully ready yet. The user needs to wait for the leader's status to become 1/1, indicating that it is ready.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nNAME           READY   STATUS    RESTARTS       AGE\nsglang-0       0/1     Running   0              9s\nsglang-0-1     1/1     Running   0              9s\n```\n\n----------------------------------------\n\nTITLE: Starting Monitoring Dashboard (Bash)\nDESCRIPTION: These commands navigate to the `examples/monitoring` directory and start the monitoring dashboard using Docker Compose. The dashboard includes Prometheus and Grafana for visualizing SGLang metrics.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/production_metrics.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose -f compose.yaml -p monitoring up\n```\n\n----------------------------------------\n\nTITLE: Building the SGL Kernel\nDESCRIPTION: This command is used to build the SGL kernel. It's a shortcut for development builds that compiles the kernel code.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Launching Reward Model Server\nDESCRIPTION: Launches an SGLang server for a reward model.  It uses `launch_server_cmd` function to start the server with the `--is-embedding` flag, which is how reward models are currently handled. The code waits for the server to be ready using `wait_for_server`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nreward_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model-path Skywork/Skywork-Reward-Llama-3.1-8B-v0.2 --host 0.0.0.0 --is-embedding\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Benchmark dspy-ai with SGLang\nDESCRIPTION: Executes the dspy-ai benchmark script, specifying SGLang as the backend. This command evaluates the performance of dspy-ai using the SGLang server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/dspy/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\npython3 bench_dspy_intro.py --backend sglang\n```\n\n----------------------------------------\n\nTITLE: Launch LMQL Server\nDESCRIPTION: Launches the LMQL server with CUDA support. `CUDA_VISIBLE_DEVICES` specifies the GPUs to use. `lmql serve-model` starts the server. `--cuda` enables CUDA, and `--port` sets the server port. This command sets up the LMQL server for receiving requests.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/gsm8k/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1 lmql serve-model meta-llama/Llama-2-7b-chat-hf --cuda --port 23000\n```\n\n----------------------------------------\n\nTITLE: Benchmarking VLLM\nDESCRIPTION: This command runs the benchmark script with a specified number of questions using the vllm backend.  It requires the bench_other.py script and a running vllm server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_chain_reasoning/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 64 --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang city information retrieval\nDESCRIPTION: This command runs the `bench_sglang.py` script in city information retrieval mode. It benchmarks the performance of SGLang for retrieving city-related information.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_sglang.py --mode city\n```\n\n----------------------------------------\n\nTITLE: Benchmark SGLang OpenAI (Python)\nDESCRIPTION: Runs the SGLang benchmark script using the OpenAI backend. The `--backend` argument specifies the backend to use, and the `--parallel` argument sets the level of parallelism.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# OpenAI models\npython3 bench_sglang.py --backend gpt-3.5-turbo --parallel 8\n```\n\n----------------------------------------\n\nTITLE: LWS LeaderWorkerSet Configuration YAML\nDESCRIPTION: This YAML file defines a LeaderWorkerSet resource for deploying SGLang on Kubernetes. It configures the leader and worker pods with specific settings for RDMA RoCE, including environment variables for NCCL, resource limits for GPUs, and volume mounts for shared memory, models, and Infiniband devices. Key parameters include `replicas`, `leaderTemplate`, `workerTemplate`, `NCCL_IB_GID_INDEX`, and resource limits.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: leaderworkerset.x-k8s.io/v1\nkind: LeaderWorkerSet\nmetadata:\n  name: sglang\nspec:\n  replicas: 1\n  leaderWorkerTemplate:\n    size: 2\n    restartPolicy: RecreateGroupOnPodRestart\n    leaderTemplate:\n      metadata:\n        labels:\n          role: leader\n      spec:\n        dnsPolicy: ClusterFirstWithHostNet\n        hostNetwork: true\n        hostIPC: true\n        containers:\n          - name: sglang-leader\n            image: sglang:latest\n            securityContext:\n              privileged: true\n            env:\n              - name: NCCL_IB_GID_INDEX\n                value: \"3\"\n            command:\n              - python3\n              - -m\n              - sglang.launch_server\n              - --model-path\n              - /work/models\n              - --mem-fraction-static\n              -  \"0.93\"\n              - --torch-compile-max-bs\n              - \"8\"\n              - --max-running-requests\n              - \"20\"\n              - --tp\n              - \"16\" # Size of Tensor Parallelism\n              - --dist-init-addr\n              - $(LWS_LEADER_ADDRESS):20000\n              - --nnodes\n              - $(LWS_GROUP_SIZE)\n              - --node-rank\n              - $(LWS_WORKER_INDEX)\n              - --trust-remote-code\n              - --host\n              - \"0.0.0.0\"\n              - --port\n              - \"40000\"\n            resources:\n              limits:\n                nvidia.com/gpu: \"8\"\n            ports:\n              - containerPort: 40000\n            readinessProbe:\n              tcpSocket:\n                port: 40000\n              initialDelaySeconds: 15\n              periodSeconds: 10\n            volumeMounts:\n              - mountPath: /dev/shm\n                name: dshm\n              - name: model\n                mountPath: /work/models\n              - name: ib\n                mountPath: /dev/infiniband\n        volumes:\n          - name: dshm\n            emptyDir:\n              medium: Memory\n          - name: model\n            hostPath:\n              path: '< your models dir >' # modify it according your models dir\n          - name: ib\n            hostPath:\n              path: /dev/infiniband\n    workerTemplate:\n      spec:\n        dnsPolicy: ClusterFirstWithHostNet\n        hostNetwork: true\n        hostIPC: true\n        containers:\n          - name: sglang-worker\n            image: sglang:latest\n            securityContext:\n              privileged: true\n            env:\n            - name: NCCL_IB_GID_INDEX\n              value: \"3\"\n            command:\n              - python3\n              - -m\n              - sglang.launch_server\n              - --model-path\n              - /work/models\n              - --mem-fraction-static\n              - \"0.93\"\n              - --torch-compile-max-bs\n              - \"8\"\n              - --max-running-requests\n              - \"20\"\n              - --tp\n              - \"16\" # Size of Tensor Parallelism\n              - --dist-init-addr\n              - $(LWS_LEADER_ADDRESS):20000\n              - --nnodes\n              - $(LWS_GROUP_SIZE)\n              - --node-rank\n              - $(LWS_WORKER_INDEX)\n              - --trust-remote-code\n            resources:\n              limits:\n                nvidia.com/gpu: \"8\"\n            volumeMounts:\n              - mountPath: /dev/shm\n                name: dshm\n              - name: model\n                mountPath: /work/models\n              - name: ib\n                mountPath: /dev/infiniband\n        volumes:\n          - name: dshm\n            emptyDir:\n              medium: Memory\n          - name: ib\n            hostPath:\n              path: /dev/infiniband\n          - name: model\n            hostPath:\n              path: /data1/models/deepseek_v3_moe\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: sglang-leader\nspec:\n  selector:\n    leaderworkerset.sigs.k8s.io/name: sglang\n    role: leader\n  ports:\n    - protocol: TCP\n      port: 40000\n      targetPort: 40000\n```\n\n----------------------------------------\n\nTITLE: Benchmark Hugging Face Model\nDESCRIPTION: This command benchmarks a Hugging Face model using the `bench_hf.py` script. The `--model-path` argument specifies the path to the HF model to benchmark.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmmu/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython benchmark/mmmu/bench_hf.py --model-path Qwen/Qwen2-VL-7B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server Script\nDESCRIPTION: This script (server.sh) launches the SGLang server with specified configurations, including the model path, tokenizer path, load format, quantization, tensor parallelism, and port. It also redirects standard output and standard error to a timestamped log file for debugging and monitoring purposes.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n\n# export SGLANG_TORCH_PROFILER_DIR=/data/sglang/\nexport SGLANG_TORCH_PROFILER_DIR=/sgl-workspace/sglang/profile/\n\n# Get the current timestamp\nTIMESTAMP=$(date +\"%Y%m%d_%H%M%S\")\n\n# Define the log file with a timestamp\nLOGFILE=\"sglang_server_log_$TIMESTAMP.json\"\n\n# Run the Python command and save the output to the log file\nloadTracer.sh python3 -m sglang.launch_server \\\n    --model-path /sgl-workspace/sglang/dummy_grok1 \\\n    --tokenizer-path Xenova/grok-1-tokenizer \\\n    --load-format dummy \\\n    --quantization fp8 \\\n    --tp 8 \\\n    --port 30000 \\\n    --disable-radix-cache 2>&1 | tee \"$LOGFILE\"\n\n```\n\n----------------------------------------\n\nTITLE: Configure Rust Analyzer in VSCode\nDESCRIPTION: This JSON configuration sets the `rust-analyzer.linkedProjects` setting in VSCode to the absolute path of the `Cargo.toml` file. This allows Rust Analyzer to properly analyze and provide language support for the Rust project.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/README.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"rust-analyzer.linkedProjects\":  [\"/workspaces/sglang/sgl-router/Cargo.toml\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Benchmark LMQL\nDESCRIPTION: Benchmarks the LMQL framework. `--num-questions` defines the number of questions to evaluate. `--backend lmql` sets the benchmark target to LMQL. `--parallel 2` indicates two parallel processes will be used for benchmarking.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/gsm8k/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 100 --backend lmql --parallel 2\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance\nDESCRIPTION: Executes the benchmark script against the Guidance backend, specifying the model path, number of questions, and parallel processing settings. The script uses a GGUF model file.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_v0/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 32 --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: SGLang: Benchmark Execution\nDESCRIPTION: This command executes the bench_sglang.py script to benchmark SGLang. It uses 10 questions for evaluation. Requires the bench_sglang.py script and necessary dependencies.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_decode_regex/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npython3 bench_sglang.py --num-questions 10\n```\n\n----------------------------------------\n\nTITLE: Stop Docker Container\nDESCRIPTION: This command stops a specified Docker container using its container ID. It is used to resolve port conflicts by stopping any existing Prometheus or Grafana containers.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/monitoring/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker stop <container_id>\n```\n\n----------------------------------------\n\nTITLE: Launching and Terminating SGLang Server in Python\nDESCRIPTION: This Python code snippet launches an SGLang server with a specified model path, host address. It uses dynamic port allocation and includes CI-specific logic to manage server instances.  The script waits for the server to start and then terminates it. It uses `sglang.test.test_utils` for CI detection and server termination and `sglang.utils` for server launching.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sglang.test.test_utils import is_in_ci\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\nif is_in_ci():\n    from patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nserver_process, port = launch_server_cmd(\n    \"\"\"\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n --host 0.0.0.0\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n\n# Terminate Server\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Patching SGLang Server for RPD Profiler\nDESCRIPTION: This patch modifies the sglang/srt/server.py file to integrate with the RPD profiler. It imports rpdTracerControl, skips its creation, and modifies the start and stop profile endpoints to enable Python trace, start RPD, push/pop ranges and flush RPD data on the server side.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndiff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py\nindex 7111c93..2bd722c 100644\n--- a/python/sglang/srt/server.py\n+++ b/python/sglang/srt/server.py\n@@ -30,6 +30,8 @@ import threading\n import time\n from http import HTTPStatus\n from typing import Dict, List, Optional, Union\n+from rpdTracerControl import rpdTracerControl\n+rpdTracerControl.skipCreate()\n\n# Fix a bug of Python threading\nsetattr(threading, \"_register_atexit\", lambda *args, **kwargs: None)\n@@ -152,6 +154,11 @@ async def flush_cache():\n@app.post(\"/start_profile\")\nasync def start_profile():\n    \"\"\"Start profiling.\"\"\"\n+    rpd = rpdTracerControl()\n+    rpd.setPythonTrace(True)\n+    rpd.start()\n+    rpd.rangePush(\"\", \"server rpd profile range\", \"\")\n+    logger.info(\"rpd profiling started in server.py!\")\n    tokenizer_manager.start_profile()\n    return Response(\n        content=\"Start profiling.\\n\",\n@@ -164,6 +171,11 @@ async def start_profile():\nasync def stop_profile():\n    \"\"\"Stop profiling.\"\"\"\n    tokenizer_manager.stop_profile()\n+    rpd = rpdTracerControl()\n+    rpd.rangePop()\n+    rpd.stop()\n+    rpd.flush()\n+    logger.info(\"rpd profiling is done in server.py!\")\n    return Response(\n        content=\"Stop profiling. This will take some time.\\n\",\n        status_code=200,\n\n```\n\n----------------------------------------\n\nTITLE: Patching SGLang Scheduler for RPD Profiler\nDESCRIPTION: This patch modifies the sglang/srt/managers/scheduler.py file to integrate with the RPD profiler.  It imports rpdTracerControl, initializes it, and adds start and stop profile functions to enable CPU and Python activity tracing within the scheduler's event loop.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndiff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 62d1ff9..2edb427 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -71,6 +71,8 @@ from sglang.srt.utils import (\n     suppress_other_loggers,\n )\n from sglang.utils import get_exception_traceback\n+from rpdTracerControl import rpdTracerControl\n+rpdTracerControl.skipCreate()\n\nlogger = logging.getLogger(__name__)\n\n@@ -245,6 +247,7 @@ class Scheduler:\n                 ],\n                 with_stack=True,\n             )\n+            self.rpd = rpdTracerControl()\n\n     @torch.inference_mode()\n     def event_loop(self):\n@@ -1027,15 +1030,26 @@ class Scheduler:\n     def start_profile(self) -> None:\n         if self.profiler is None:\n             raise RuntimeError(\"Profiler is not enabled.\")\n-        self.profiler.start()\n+        #self.profiler.start()\n+        logger.info(\"torch profiler is disabled\")\n+        if self.tp_rank == 0 or self.tp_rank == 1:\n+            self.rpd.setPythonTrace(True)\n+            self.rpd.start()\n+            self.rpd.rangePush(\"\", \"scheduler\", \"\")\n+        logger.info(\"rpd is enabled inside scheduler profiling\")\n\n     def stop_profile(self) -> None:\n         if self.profiler is None:\n             raise RuntimeError(\"Profiler is not enabled.\")\n-        self.profiler.stop()\n-        self.profiler.export_chrome_trace(\n-            self.torch_profiler_trace_dir + \"/\" + str(time.time()) + \".trace.json.gz\"\n-        )\n+        #self.profiler.stop()\n+        #self.profiler.export_chrome_trace(\n+        #    self.torch_profiler_trace_dir + \"/\" + str(time.time()) + \".trace.json.gz\"\n+        #)\n+        if self.tp_rank ==0 or self.tp_rank ==1:\n+            self.rpd.rangePop()\n+            self.rpd.stop()\n+            self.rpd.flush()\n+            logger.info(\"rpd is done inside scheduler\")\n+        logger.info(\"Profiler is done\")\n\n```\n\n----------------------------------------\n\nTITLE: Skipping Tests with pytest\nDESCRIPTION: This Python code shows how to skip a pytest test based on a condition using `@pytest.mark.skipif`. This is useful for skipping tests that are not applicable to certain environments or hardware.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.skipif(\n    skip_condition, reason=\"Nvfp4 Requires compute capability of 10 or above.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Run SGLang Client for Nsight Systems Profiling (Bash)\nDESCRIPTION: This command runs a client to generate requests for profiling the SGLang server using Nsight Systems. It specifies the backend, number of prompts, dataset, and input/output lengths.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.bench_serving --backend sglang --num-prompts 1000 --dataset-name random --random-input 1024 --random-output 512\n```\n\n----------------------------------------\n\nTITLE: Extracting Data from JSONL File (Offline)\nDESCRIPTION: This script extracts a specific field from the `offline_trt_70b.jsonl` file using `cat` and `cut`. It extracts the 12th field (delimited by ':') and then further extracts the 1st field (delimited by ','). This command is useful for post-processing the benchmark results to isolate a specific metric or statistic.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncat offline_trt_70b.jsonl | cut -d':' -f12 | cut -d',' -f1\n```\n\n----------------------------------------\n\nTITLE: Launching llama.cpp Server (Python)\nDESCRIPTION: This command launches a llama.cpp server using the downloaded model weights. It specifies the model path, clip model path, chat format, and port number. This server is used to serve requests from the benchmark script.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython3 -m llama_cpp.server --model ~/model_weights/llava-v1.5-7b/ggml-model-f16.gguf --clip_model_path ~/model_weights/llava-v1.5-7b/mmproj-model-f16.gguf --chat_format llava-1-5 --port 23000\n```\n\n----------------------------------------\n\nTITLE: Terminating a Process\nDESCRIPTION: This snippet terminates a process, presumably the SGLang server process, using the `terminate_process` function.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Running Backend Runtime Tests\nDESCRIPTION: These commands demonstrate how to navigate to the backend runtime test directory and execute tests.  It covers running a single file, a single test within a file, and a suite of tests. The commands rely on the 'cd' command for directory navigation and 'python3' to execute python scripts, optionally with the 'unittest' module.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/test/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd sglang/test/srt\n\n# Run a single file\npython3 test_srt_endpoint.py\n\n# Run a single test\npython3 -m unittest test_srt_endpoint.TestSRTEndpoint.test_simple_decode\n\n# Run a suite with multiple files\npython3 run_suite.py --suite per-commit\n```\n\n----------------------------------------\n\nTITLE: Defining Torch Extension with Schema\nDESCRIPTION: This C++ code defines a torch extension with a schema for use with torch.compile. It includes both `m.def` with the schema and `m.impl` to bind the function to the CUDA implementation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// We need def with schema here for torch.compile\nm.def(\n    \"bmm_fp8(Tensor A, Tensor B, Tensor! D, Tensor A_scale, Tensor B_scale, Tensor workspace_buffer, int \"\n    \"cublas_handle, int cuda_stream) -> ()\");\nm.impl(\"bmm_fp8\", torch::kCUDA, &bmm_fp8);\n```\n\n----------------------------------------\n\nTITLE: Wrapping Library Functions with make_pytorch_shim\nDESCRIPTION: This C++ code shows how to wrap a library function with `make_pytorch_shim` to handle data type conversions automatically when integrating third-party libraries with PyTorch bindings.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n/*\n * From flash-attention\n */\n m.impl(\"fwd\", torch::kCUDA, make_pytorch_shim(&mha_fwd));\n```\n\n----------------------------------------\n\nTITLE: Benchmarking vLLM\nDESCRIPTION: Runs the benchmark script against the vLLM backend.  It executes the `bench_other.py` script with specific options to evaluate the performance of vLLM.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_v0/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --num-questions 32 --backend vllm\n```\n\n----------------------------------------\n\nTITLE: Profile SGLang Server with Nsight Systems (Bash)\nDESCRIPTION: This command profiles the SGLang server using Nsight Systems. It sets a delay and duration for profiling, then launches the server with specified options.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nnsys profile --trace-fork-before-exec=true --cuda-graph-trace=node -o sglang.out --delay 60 --duration 70 python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --disable-radix-cache\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT LLM Offline 70B\nDESCRIPTION: This script benchmarks the TensorRT LLM in an offline setting using the `bench_serving.py` script. It runs multiple benchmarks with different numbers of prompts, input sizes, and output sizes, storing the results in `offline_trt_70b.jsonl`. The `--dataset-name` is set to `random`, indicating randomly generated input.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name random --num-prompts 4000 --random-input 1024 --random-output 1024 --output-file offline_trt_70b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name random --num-prompts 5000 --random-input 1024 --random-output 512 --output-file offline_trt_70b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name random --num-prompts 1000 --random-input 4096 --random-output 2048 --output-file offline_trt_70b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name random --num-prompts 2000 --random-input 4096 --random-output 1024 --output-file offline_trt_70b.jsonl\npython3 bench_serving.py --backend trt --dataset-name random --num-prompts 6000 --random-input 256 --random-output 512 --output-file offline_trt_70b.jsonl --model meta-llama/Meta-Llama-3-70B-Instruct\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name sharegpt --num-prompts 3000 --output-file offline_trt_70b.jsonl\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance Llama-7B (short output) (Python)\nDESCRIPTION: This command benchmarks Guidance with Llama-7B using bench_other.py. It specifies the tokenizer, backend, parallelization, context window size, and the path to the GGUF model. A short output is generated.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_turn_chat/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --tokenizer meta-llama/Llama-2-7b-chat-hf --backend guidance --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Setting Optional Features\nDESCRIPTION: Defines options to enable/disable support for different CUDA architectures (SM100A, SM90A) and data types (BF16, FP8, FP4, FA3). These options control which code paths are compiled and linked.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\noption(SGL_KERNEL_ENABLE_SM100A \"Enable SM100A\" OFF)\noption(SGL_KERNEL_ENABLE_SM90A  \"Enable SM90A\"  OFF)\noption(SGL_KERNEL_ENABLE_BF16   \"Enable BF16\"   ON)\noption(SGL_KERNEL_ENABLE_FP8    \"Enable FP8\"    ON)\noption(SGL_KERNEL_ENABLE_FP4    \"Enable FP4\"    OFF)\noption(SGL_KERNEL_ENABLE_FA3    \"Enable FA3\"    OFF)\n```\n\n----------------------------------------\n\nTITLE: Installing SGLang from Source\nDESCRIPTION: This snippet outlines the steps to install SGLang from source, including cloning the repository, navigating to the directory, upgrading pip, installing sgl-kernel, and installing sglang with all_hip extras. It uses pip for package management and git for source code retrieval.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/amd.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\n\npip install --upgrade pip\npip install sgl-kernel --force-reinstall --no-deps\npip install -e \"python[all_hip]\"\n```\n\n----------------------------------------\n\nTITLE: Creating and Entering Development Container\nDESCRIPTION: These commands create and enter a Docker container for SGLang development. The first command creates a detached container with shared memory, GPU access, volume mounting, and inter-process communication. The second command enters the created container using the zsh shell.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -itd --shm-size 32g --gpus all -v $HOME/.cache:/root/.cache --ipc=host --name sglang_zhyncs lmsysorg/sglang:dev /bin/zsh\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it sglang_zhyncs /bin/zsh\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting up LLaVA (Bash)\nDESCRIPTION: This set of commands clones the LLaVA repository, resets it to a specific commit, and installs the necessary dependencies. It prepares the LLaVA environment for running its benchmark script.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:haotian-liu/LLaVA.git\ncd LLaVA\ngit reset --hard 9a26bd1435b4ac42c282757f2c16d34226575e96\npip3 install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing RPD Profiler with Patch\nDESCRIPTION: This bash script automates the installation of the RPD profiler. It updates the package list, installs necessary dependencies (sqlite3, libsqlite3-dev, libfmt-dev), clones the RPD repository, checks out a specific commit, applies the provided patch file (rpd.patch), builds and installs RPD, and installs the rocpd_python and rpd_tracer components.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# download and install RPD\napt update && apt install -y sqlite3 libsqlite3-dev libfmt-dev\n\n# install rpd module\ngit clone https://github.com/ROCmSoftwarePlatform/rocmProfileData\ncd rocmProfileData\ngit checkout 976899e9c6dbc6dd2bccf770818e4e44125590ac\ngit apply rpd.patch\nmake && make install\ncd rocpd_python && python setup.py install && cd ..\ncd rpd_tracer && make clean;make install && python setup.py install && cd ..\n```\n\n----------------------------------------\n\nTITLE: Dependencies List\nDESCRIPTION: This snippet list dependencies needed for the project.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_decode_regex/README.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nllama_cpp_python          0.2.19\nguidance                  0.1.10\nvllm                      0.2.5\noutlines                  0.0.22\n```\n\n----------------------------------------\n\nTITLE: Installing PyPDF2 and Building Dataset\nDESCRIPTION: This command installs the PyPDF2 library and executes the build_dataset.py script, presumably to process data for a specific purpose. PyPDF2 is a dependency for building the dataset.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_document_qa/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install PyPDF2\npython3 build_dataset.py\n```\n\n----------------------------------------\n\nTITLE: Download data using wget\nDESCRIPTION: This command downloads a JSON file containing random words from a specified URL.  It uses `wget`, a command-line utility for retrieving files over HTTP, HTTPS, and FTP.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/line_retrieval/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://raw.githubusercontent.com/merrymercy/merrymercy.github.io/master/files/random_words.json\n```\n\n----------------------------------------\n\nTITLE: Building SGLang Docker Image\nDESCRIPTION: This snippet shows how to build the SGLang Docker image using the `Dockerfile.rocm` Dockerfile. It tags the image as `sglang_image` for subsequent use in containerized environments.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/amd.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t sglang_image -f Dockerfile.rocm .\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Outlines city information retrieval\nDESCRIPTION: This command benchmarks the Outlines framework for city information retrieval. It executes the `bench_other.py` script in city mode with the `--backend outlines` option.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_other.py --mode city --backend outlines\n```\n\n----------------------------------------\n\nTITLE: Install Python Wheel Package with Pip\nDESCRIPTION: This command installs a Python wheel package using `pip`. It takes the path to the wheel file as an argument and installs the package into the current Python environment.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install <path-to-wheel>\n```\n\n----------------------------------------\n\nTITLE: Guidance: Benchmark Execution\nDESCRIPTION: This command executes the bench_other.py script to benchmark Guidance. It uses 10 questions, sets parallelism to 1, context length to 4096, and specifies the path to the GGUF model. Requires the bench_other.py script and necessary dependencies.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_decode_regex/README.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\npython3 bench_other.py --backend guidance --num-questions 10 --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Fused MoE Triton Kernel vs vllm\nDESCRIPTION: This script compares the performance of the fused MoE Triton kernel in sglang against the vllm implementation. It accepts arguments to specify the model, data type (FP8), and tensor parallelism size.  The benchmark results are saved as plots and data files.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/kernels/fused_moe_triton/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Compare with default settings (Mixtral model)\npython benchmark/kernels/fused_moe_triton/benchmark_vllm_vs_sglang_fused_moe_triton.py\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Compare with FP8 mode for Qwen2-57B\npython benchmark/kernels/fused_moe_triton/benchmark_vllm_vs_sglang_fused_moe_triton.py \\\n    --model Qwen/Qwen2-57B-A14B-Instruct \\\n    --use-fp8\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Compare with custom TP size\npython benchmark/kernels/fused_moe_triton/benchmark_vllm_vs_sglang_fused_moe_triton.py \\\n    --tp-size 4\n```\n\n----------------------------------------\n\nTITLE: Finding Torch Package\nDESCRIPTION: Finds the Torch package, which is required for linking the library. The REQUIRED keyword ensures that the build will fail if Torch is not found.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(Torch REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Downloading the Dataset (wget)\nDESCRIPTION: This command uses `wget` to download the `agent_calls.jsonl` dataset from a Google Drive URL. The `-O` option specifies the output filename.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/generative_agents/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget -O agent_calls.jsonl https://drive.google.com/uc?export=download&id=19qLpD45e9JGTKF2cUjJJegwzSUEZEKht\n```\n\n----------------------------------------\n\nTITLE: Launching DeepSeek-R1 on SkyPilot - Bash\nDESCRIPTION: Launches DeepSeek-R1 on multiple nodes using SkyPilot. It clones the SkyPilot repository and executes SkyPilot launch commands using provided YAML configurations. This requires SkyPilot to be installed and configured to access the desired cloud provider or Kubernetes cluster.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/deepseek_v3/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/skypilot-org/skypilot.git\n# Serve on 2 H100/H200x8 nodes\nsky launch -c r1 llm/deepseek-r1/deepseek-r1-671B.yaml --retry-until-up\n# Serve on 4 A100x8 nodes\nsky launch -c r1 llm/deepseek-r1/deepseek-r1-671B-A100.yaml --retry-until-up\n```\n\n----------------------------------------\n\nTITLE: Build Rust Project with Cargo\nDESCRIPTION: This command builds the Rust project using Cargo, the Rust package manager. It compiles the Rust code and creates the necessary executables and libraries. The working directory must contain a `Cargo.toml` file.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-router/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cargo build\n```\n\n----------------------------------------\n\nTITLE: Launching sglang Server in Docker\nDESCRIPTION: This command launches the sglang server inside the Docker container. It sets the model path to `mistralai/Mistral-7B-Instruct-v0.2`, specifies port `30000`, and sets the static memory fraction to `0.9`. It assumes sglang is installed in the container. Requires Python3 and sglang.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/frontend_language/usage/triton/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncd sglang\npython3 -m sglang.launch_server --model-path mistralai/Mistral-7B-Instruct-v0.2 --port 30000 --mem-fraction-static 0.9\n```\n\n----------------------------------------\n\nTITLE: Checking RDMA devices in Container\nDESCRIPTION: This code snippet provides commands to check for RDMA devices within a container.  `ibv_devices` lists the RDMA devices, and `ibv_devinfo` provides detailed information about them. Requires `ibverbs` package to be installed in the container.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n# ibv_devices\n# ibv_devinfo\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Router for Statistic Analysis\nDESCRIPTION: This command launches the SGLang router with a larger data parallelism size for efficient statistic analysis. The `--dp-size` parameter specifies the degree of data parallelism.  Using `sglang_router` requires a separate installation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/reasoning_benchmark/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang_router.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --port 30000 --dp-size 8\n```\n\n----------------------------------------\n\nTITLE: Run SGLang benchmark\nDESCRIPTION: This command executes the `bench_sglang.py` script to benchmark SGLang. The `--src-index` argument sets the starting index to 600. The `--num-q` argument specifies the number of queries to 50, and the `--parallel` argument defines the number of parallel threads or processes, setting it to 1.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/line_retrieval/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_sglang.py --src-index 600 --num-q 50 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Download Data (Bash)\nDESCRIPTION: This script downloads necessary data for running the benchmarks. It assumes that `download_data.sh` is an executable script in the current directory.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbash download_data.sh\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT LLM Online 70B\nDESCRIPTION: This script benchmarks TensorRT LLM in an online setting. It calls `bench_serving.py` with different `--request-rate` values to simulate varying levels of concurrent requests.  The `--dataset-name` is set to `random` indicating a randomly generated input. The results are stored in `online_trt_70b.jsonl`.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/README.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 300 --request-rate 1 --output-file online_trt_70b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 600 --request-rate 2 --output-file online_trt_70b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 1200 --request-rate 4 --output-file online_trt_70b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 2400 --request-rate 8 --output-file online_trt_70b.jsonl\npython3 bench_serving.py --backend trt --model meta-llama/Meta-Llama-3-70B-Instruct --dataset-name random --random-input 1024 --random-output 1024 --num-prompts 3200 --request-rate 16 --output-file online_trt_70b.jsonl\n```\n\n----------------------------------------\n\nTITLE: Installing DeepGEMM and Cutlass Header Files\nDESCRIPTION: This section uses the `install(DIRECTORY)` command to install the DeepGEMM directory and the Cutlass header files. It also excludes certain patterns during installation.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY \"${repo-deepgemm_SOURCE_DIR}/deep_gemm/\"\n        DESTINATION \"deep_gemm\"\n        PATTERN \".git*\" EXCLUDE\n        PATTERN \"__pycache__\" EXCLUDE)\n\ninstall(DIRECTORY \"${repo-cutlass_SOURCE_DIR}/include/cute/\"\n        DESTINATION \"deep_gemm/include/cute\")\n\ninstall(DIRECTORY \"${repo-cutlass_SOURCE_DIR}/include/cutlass/\"\n        DESTINATION \"deep_gemm/include/cutlass\")\n```\n\n----------------------------------------\n\nTITLE: Downloading llama.cpp Weights (Bash)\nDESCRIPTION: This set of commands downloads the required model weights for llama.cpp. It creates a directory and uses wget to download the ggml-model-f16.gguf and mmproj-model-f16.gguf files.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llava_bench/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Download weights\nmkdir -p ~/model_weights/llava-v1.5-7b/\nwget https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/ggml-model-f16.gguf -O ~/model_weights/llava-v1.5-7b/ggml-model-f16.gguf\nwget https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/mmproj-model-f16.gguf -O ~/model_weights/llava-v1.5-7b/mmproj-model-f16.gguf\n```\n\n----------------------------------------\n\nTITLE: vLLM and SGLang LLM Configuration\nDESCRIPTION: This JSON configuration is designed for use with vLLM and SGLang. It specifies the model architecture (LlamaForCausalLM), quantization settings (FP8), and other parameters relevant for efficient inference. Key parameters include vocab_size, hidden_size, num_hidden_layers, rope_scaling configuration and quantization_config details.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/blog_v0_2/config.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"_name_or_path\": \"dummy_fp8\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 16384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 53248,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 128,\n  \"num_hidden_layers\": 126,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"activation_scheme\": \"static\",\n    \"ignored_layers\": [\n      \"lm_head\"\n    ],\n    \"quant_method\": \"fp8\"\n  },\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"low_freq_factor\": 1.0,\n    \"high_freq_factor\": 4.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"max_position_embeddings\": 131072,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.41.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n```\n\n----------------------------------------\n\nTITLE: Patching SGLang Tokenizer Manager for RPD Profiler\nDESCRIPTION: This patch modifies the sglang/srt/managers/tokenizer_manager.py file to integrate with the RPD profiler. It imports rpdTracerControl, skips its creation, and adds start and stop profile functions. These functions enable Python trace, start RPD, push/pop ranges and handle flushing the RPD data for the tokenizer manager.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndiff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py\nindex 2621ccd..181df85 100644\n--- a/python/sglang/srt/managers/tokenizer_manager.py\n+++ b/python/sglang/srt/managers/tokenizer_manager.py\n@@ -58,6 +58,10 @@ from sglang.srt.sampling.sampling_params import SamplingParams\n from sglang.srt.server_args import PortArgs, ServerArgs\n from sglang.srt.utils import is_generation_model, is_multimodal_model\n\n+from rpdTracerControl import rpdTracerControl\n+rpdTracerControl.skipCreate()\n+\n+\nasyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\n\nlogger = logging.getLogger(__name__)\n@@ -514,10 +518,20 @@ class TokenizerManager:\n         self.send_to_scheduler.send_pyobj(req)\n\n     def start_profile(self):\n+        rpd = rpdTracerControl()\n+        rpd.setPythonTrace(True)\n+        rpd.start()\n+        rpd.rangePush(\"\", \"tokenizer_manager\", \"\")\n+        logger.info(\"tokenizer_manager rpd profiling started!\")\n         req = ProfileReq.START_PROFILE\n         self.send_to_scheduler.send_pyobj(req)\n\n     def stop_profile(self):\n+        rpd = rpdTracerControl()\n+        rpd.rangePop()\n+        rpd.stop()\n+        rpd.flush()\n+        logger.info(\"rpd profiling is done inside tokenizer_manager!\")\n         req = ProfileReq.STOP_PROFILE\n         self.send_to_scheduler.send_pyobj(req)\n\n```\n\n----------------------------------------\n\nTITLE: Running Multi-modality Benchmark Client (Python)\nDESCRIPTION: This snippet shows how to run the `bench_serving.py` script for a multi-modality benchmark. It configures the model, backend, dataset path and name, request rate, number of prompts, disables shuffling, specifies the port, enables multi-turn chat, maximum frames, tokenizer, and fixed output length.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/hicache/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_serving.py --model lmms-lab/LLaVA-NeXT-Video-7B --backend sglang  --dataset-path \\\nNExTVideo  --dataset-name nextqa --request-rate 10 --num-prompts 1 --disable-shuffle --port 8001 \\ --enable-multiturn --max-frames 16 --tokenizer llava-hf/llava-1.5-7b-hf --fixed-output-len 2048\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Compiler Flags for Flash Attention Kernel\nDESCRIPTION: This section defines the CUDA compiler flags used for compiling the Flash Attention kernel, including optimization levels, architecture specifications, C++ standard, and various preprocessor definitions related to CUTLASS and CUDA.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_23\n\nLANGUAGE: cmake\nCODE:\n```\nif (SGL_KERNEL_ENABLE_FA3)\n    set(SGL_FLASH_KERNEL_CUDA_FLAGS\n        \"-DNDEBUG\"\n        \"-DOPERATOR_NAMESPACE=sgl-kernel\"\n        \"-O3\"\n        \"-Xcompiler\"\n        \"-fPIC\"\n        \"-gencode=arch=compute_90a,code=sm_90a\"\n        \"-std=c++17\"\n        \"-DCUTE_USE_PACKED_TUPLE=1\"\n        \"-DCUTLASS_ENABLE_TENSOR_CORE_MMA=1\"\n        \"-DCUTLASS_VERSIONS_GENERATED\"\n        \"-DCUTLASS_TEST_LEVEL=0\"\n        \"-DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1\"\n        \"-DCUTLASS_DEBUG_TRACE_LEVEL=0\"\n        \"--expt-relaxed-constexpr\"\n        \"--expt-extended-lambda\"\n        \"--use_fast_math\"\n        \"-Xcompiler=-Wconversion\"\n        \"-Xcompiler=-fno-strict-aliasing\"\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Frontend Language Tests\nDESCRIPTION: These commands demonstrate how to navigate to the frontend language test directory and execute tests.  It covers running a single file, a single test within a file, and a suite of tests.  It also shows setting the OpenAI API key. The commands rely on the 'cd' command for directory navigation, 'export' for environment variables, and 'python3' to execute python scripts, optionally with the 'unittest' module.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/test/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd sglang/test/lang\nexport OPENAI_API_KEY=sk-*****\n\n# Run a single file\npython3 test_openai_backend.py\n\n# Run a single test\npython3 -m unittest test_openai_backend.TestOpenAIServer.test_few_shot_qa\n\n# Run a suite with multiple files\npython3 run_suite.py --suite per-commit\n```\n\n----------------------------------------\n\nTITLE: Creating Flash Ops Library with Python_add_library\nDESCRIPTION: This section defines the `FLASH_SOURCES` and compiles them into the `flash_ops` library using `Python_add_library`. Compiler options and include directories are also set.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\nif (SGL_KERNEL_ENABLE_FA3)\n    set(FA3_GEN_SRCS ${FA3_BF16_GEN_SRCS} ${FA3_FP16_GEN_SRCS} ${FA3_FP8_GEN_SRCS})\n\n    set(FLASH_SOURCES\n        \"csrc/flash_extension.cc\"\n        \"${repo-flash-attention_SOURCE_DIR}/hopper/flash_prepare_scheduler.cu\"\n        \"${repo-flash-attention_SOURCE_DIR}/hopper/flash_api.cpp\"\n        \"${repo-flash-attention_SOURCE_DIR}/hopper/flash_fwd_combine.cu\"\n        \"${FA3_GEN_SRCS}\"\n    )\n\n    Python_add_library(flash_ops MODULE USE_SABI ${SKBUILD_SABI_VERSION} WITH_SOABI ${FLASH_SOURCES})\n\n    target_compile_options(flash_ops PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:${SGL_FLASH_KERNEL_CUDA_FLAGS}>)\n    target_include_directories(flash_ops PRIVATE\n        ${repo-flash-attention_SOURCE_DIR}/hopper\n    )\n    target_link_libraries(flash_ops PRIVATE ${TORCH_LIBRARIES} c10 cuda)\n\n    install(TARGETS flash_ops LIBRARY DESTINATION \"sgl_kernel\")\n\n    target_compile_definitions(flash_ops PRIVATE\n        FLASHATTENTION_DISABLE_SM8x\n        FLASHATTENTION_DISABLE_BACKWARD\n        FLASHATTENTION_DISABLE_DROPOUT\n        FLASHATTENTION_DISABLE_UNEVEN_K\n        FLASHATTENTION_VARLEN_ONLY\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Terminate Server Process\nDESCRIPTION: This snippet demonstrates how to terminate a server process, assumed to be stored in the variable `server_process`.  It likely relies on a custom function `terminate_process` which is not defined in the snippet.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Appending CUDA Flags based on CUDA Version and Options\nDESCRIPTION: Conditionally appends CUDA flags based on the CUDA version and the enabled options. This allows for architecture-specific flags to be added when certain options or CUDA versions are present. This ensures that the code is compiled with the correct flags for the target architecture and data types.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif (\"${CUDA_VERSION}\" VERSION_GREATER_EQUAL \"12.8\" OR SGL_KERNEL_ENABLE_SM100A)\n    list(APPEND SGL_KERNEL_CUDA_FLAGS\n        \"-gencode=arch=compute_100,code=sm_100\"\n        \"-gencode=arch=compute_100a,code=sm_100a\"\n    )\nelse()\n    list(APPEND SGL_KERNEL_CUDA_FLAGS\n        \"-use_fast_math\"\n    )\nendif()\n\nif (\"${CUDA_VERSION}\" VERSION_GREATER_EQUAL \"12.4\" OR SGL_KERNEL_ENABLE_SM90A)\n    set(SGL_KERNEL_ENABLE_FA3 ON)\n    list(APPEND SGL_KERNEL_CUDA_FLAGS\n        \"-gencode=arch=compute_90a,code=sm_90a\"\n    )\nendif()\n\nif (SGL_KERNEL_ENABLE_BF16)\n    list(APPEND SGL_KERNEL_CUDA_FLAGS\n        \"-DFLASHINFER_ENABLE_BF16\"\n    )\nendif()\n\nif (SGL_KERNEL_ENABLE_FP8)\n    list(APPEND SGL_KERNEL_CUDA_FLAGS\n        \"-DFLASHINFER_ENABLE_FP8\"\n        \"-DFLASHINFER_ENABLE_FP8_E4M3\"\n        \"-DFLASHINFER_ENABLE_FP8_E5M2\"\n    )\nendif()\n\nif (\"${CUDA_VERSION}\" VERSION_GREATER_EQUAL \"12.8\" OR SGL_KERNEL_ENABLE_FP4)\n    list(APPEND SGL_KERNEL_CUDA_FLAGS\n        \"-DENABLE_NVFP4=1\"\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Benchmark LightLLM\nDESCRIPTION: This command runs the bench_other.py script to benchmark LightLLM with 32 questions. It specifies lightllm as the backend.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_deep/README.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\npython3 bench_other.py --num-questions 32 --backend lightllm\n```\n\n----------------------------------------\n\nTITLE: SGLang Leader Node Logs\nDESCRIPTION: This text presents example logs from the SGLang leader node. It shows that the server has successfully started, loaded the model, initialized the tensor parallelism (TP), and is ready to accept requests. It logs information about the number of tokens, cache hit rate, running requests, and queue requests. It also includes information about the uvicorn server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n[2025-02-17 05:27:24 TP1] Capture cuda graph end. Time elapsed: 84.89 s\n[2025-02-17 05:27:24 TP6] max_total_num_tokens=712400, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=50, context_len=163840\n[2025-02-17 05:27:24 TP0] max_total_num_tokens=712400, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=50, context_len=163840\n[2025-02-17 05:27:24 TP7] max_total_num_tokens=712400, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=50, context_len=163840\n[2025-02-17 05:27:24 TP3] max_total_num_tokens=712400, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=50, context_len=163840\n[2025-02-17 05:27:24 TP2] max_total_num_tokens=712400, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=50, context_len=163840\n[2025-02-17 05:27:24 TP4] max_total_num_tokens=712400, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=50, context_len=163840\n[2025-02-17 05:27:24 TP1] max_total_num_tokens=712400, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=50, context_len=163840\n[2025-02-17 05:27:24 TP5] max_total_num_tokens=712400, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=50, context_len=163840\n[2025-02-17 05:27:24] INFO:     Started server process [1]\n[2025-02-17 05:27:24] INFO:     Waiting for application startup.\n[2025-02-17 05:27:24] INFO:     Application startup complete.\n[2025-02-17 05:27:24] INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\n[2025-02-17 05:27:25] INFO:     127.0.0.1:48908 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-02-17 05:27:25 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-02-17 05:27:32] INFO:     127.0.0.1:48924 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-02-17 05:27:32] The server is fired up and ready to roll!\n```\n\n----------------------------------------\n\nTITLE: Fetching External Content (cutlass)\nDESCRIPTION: Declares and populates the cutlass repository using FetchContent. This fetches the cutlass library from GitHub at a specific tag. GIT_SHALLOW is set to OFF indicating that the full history should be fetched.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nFetchContent_Declare(\n    repo-cutlass\n    GIT_REPOSITORY https://github.com/NVIDIA/cutlass\n    GIT_TAG        5e497243f7ad13a2aa842143f9b10bbb23d98292\n    GIT_SHALLOW    OFF\n)\nFetchContent_Populate(repo-cutlass)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Runner\nDESCRIPTION: These commands set environment variables required for the runner to function correctly. `HF_HOME` specifies the Hugging Face cache directory, `SGLANG_IS_IN_CI` indicates it's running in a CI environment, `HF_TOKEN` is the Hugging Face API token, `OPENAI_API_KEY` is the OpenAI API key, and `CUDA_VISIBLE_DEVICES` specifies the visible CUDA devices.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/setup_github_runner.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_HOME=/hf_home\nexport SGLANG_IS_IN_CI=true\nexport HF_TOKEN=hf_xxx\nexport OPENAI_API_KEY=sk-xxx\nexport CUDA_VISIBLE_DEVICES=0\n```\n\n----------------------------------------\n\nTITLE: Launch vLLM API server\nDESCRIPTION: Launches the vLLM API server with specified configurations, including the model path and disabling log requests. This command sets up the vLLM server for benchmarking.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/dspy/README.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\npython3 -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-7b-chat-hf --disable-log-requests  --port 21000\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Minimum Version\nDESCRIPTION: Specifies the minimum required CMake version for the project. This ensures that the CMake version used is compatible with the features used in the CMakeLists.txt file. The FATAL_ERROR option will cause CMake to exit if the version is not met.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.26 FATAL_ERROR)\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Container with Nvidia GPU\nDESCRIPTION: This command pulls the specified Nvidia CUDA image and runs a Docker container with GPU support. It mounts a volume for sharing Hugging Face model weights cache. The command uses `/tmp/huggingface` as an example and allows the container to access all GPUs.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/setup_github_runner.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull nvidia/cuda:12.1.1-devel-ubuntu22.04\n# Nvidia\ndocker run --shm-size 128g -it -v /tmp/huggingface:/hf_home --gpus all nvidia/cuda:12.1.1-devel-ubuntu22.04 /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Evaluate SGLang on gsm8k\nDESCRIPTION: This snippet evaluates SGLang on the gsm8k benchmark dataset using a specified number of questions, parallel requests, and shots. It executes the bench_sglang.py script with the appropriate arguments.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/developer/development_guide_using_docker.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# e.g. gsm8k 8 shot\npython3 benchmark/gsm8k/bench_sglang.py --num-questions 2000 --parallel 2000 --num-shots 8\n```\n\n----------------------------------------\n\nTITLE: Building dataset with python\nDESCRIPTION: This command executes the `build_dataset.py` script using Python 3. It is used to generate the dataset needed for benchmarking long document information retrieval.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_dataset.py\n```\n\n----------------------------------------\n\nTITLE: Installing SGLang v0.3.0\nDESCRIPTION: Installs SGLang version 0.3.0 with all dependencies and flashinfer. Requires pip to be upgraded. This snippet prepares the environment for running SGLang benchmarks.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/benchmark_vllm_060/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# install sglang v0.3.0\npip install --upgrade pip\npip install \"sglang[all]\"==0.3.0\npip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/\n```\n\n----------------------------------------\n\nTITLE: Running SGLang Benchmark\nDESCRIPTION: This command runs a benchmark on the SGLang implementation of the new model.  The `--correct` flag ensures that the benchmark checks for correctness. This helps verify the functional equivalence of the SGLang model to the original.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/supported_models/support_new_models.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sglang.bench_one_batch --correct --model [new model]\n```\n\n----------------------------------------\n\nTITLE: Install nvtx (Bash)\nDESCRIPTION: Installs the nvtx library, which is used to annotate code regions for profiling.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npip install nvtx\n```\n\n----------------------------------------\n\nTITLE: Nsight Systems - List Sessions (Bash)\nDESCRIPTION: This command lists the active Nsight Systems sessions, providing session IDs that can be used to stop the profiler manually.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nnsys sessions list\n```\n\n----------------------------------------\n\nTITLE: Profile Multiple Batches with bench_offline_throughput.py (Bash)\nDESCRIPTION: This command profiles multiple batches using `sglang.bench_offline_throughput` with the PyTorch profiler. It specifies the dataset, number of prompts, and memory fraction.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.bench_offline_throughput --model-path meta-llama/Llama-3.1-8B-Instruct --dataset-name random --num-prompts 10 --profile --mem-frac=0.8\n```\n\n----------------------------------------\n\nTITLE: Benchmark with Modified Configs (Bash)\nDESCRIPTION: This command benchmarks a model with modified configurations (e.g., fewer layers) using `--json-model-override-args`. Here's an example for benchmarking a model with 1 layer and 1 kv heads.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/benchmark_and_profiling.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npython -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --batch 32 --input-len 256 --output-len 32 --load-format dummy --json-model-override-args '{\"num_hidden_layers\": 1, \"num_key_value_heads\": 1}'\n```\n\n----------------------------------------\n\nTITLE: Running DeepGemm FP8 Group GEMM Benchmark (bash)\nDESCRIPTION: This command executes the `benchmark_deepgemm_fp8_group_gemm.py` script with correctness verification enabled and sets the tensor parallelism size to 1. It benchmarks FP8 group GEMM operations using DeepGemm and compares its performance with other implementations.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/kernels/deepseek/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark_deepgemm_fp8_group_gemm.py --run_correctness --tp_size 1\n```\n\n----------------------------------------\n\nTITLE: Benchmark SGLang Model\nDESCRIPTION: This command benchmarks an SGLang model using the `bench_sglang.py` script. The `--port` argument specifies the port of the hosted VLM. Optionally, `--mem-fraction-static` can be used to reduce memory usage.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmmu/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython benchmark/mmmu/bench_sglang.py --port 30000\n```\n\n----------------------------------------\n\nTITLE: Check for Running Prometheus/Grafana Containers\nDESCRIPTION: This command uses `docker ps` to list running containers and `grep` to filter the output, searching for containers with names containing 'prometheus' or 'grafana'. This is used to identify potential port conflicts.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/examples/monitoring/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps | grep -E 'prometheus|grafana'\n```\n\n----------------------------------------\n\nTITLE: Building with ccache\nDESCRIPTION: These commands configure and build the SGL kernel using `ccache`. It first installs ccache, then sets environment variables to enable and configure ccache, and finally builds the wheel package with uv.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# or `yum install -y ccache`.\napt-get install -y ccache\n# Building with ccache is enabled when ccache is installed and CCACHE_DIR is set.\nexport CCACHE_DIR=/path/to/your/ccache/dir\nexport CCACHE_BACKEND=\"\"\nexport CCACHE_KEEP_LOCAL_STORAGE=\"TRUE\"\nunset CCACHE_READONLY\npython -m uv build --wheel -Cbuild-dir=build --color=always .\n```\n\n----------------------------------------\n\nTITLE: Installing SGL Kernel with CUDA 11.8\nDESCRIPTION: This command installs the sgl-kernel package from a specific index URL for CUDA 11.8. This ensures compatibility with the CUDA version used in the environment.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install sgl-kernel -i https://docs.sglang.ai/whl/cu118\n```\n\n----------------------------------------\n\nTITLE: Running SGLang with Docker on AMD ROCm (Bash)\nDESCRIPTION: This snippet demonstrates how to build and run SGLang with Docker on AMD ROCm systems. It includes building the Docker image using `Dockerfile.rocm`, creating an alias for the `docker run` command, and then running the SGLang server and a benchmark test.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/start/install.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker build --build-arg SGL_BRANCH=v0.4.5.post3 -t v0.4.5.post3-rocm630 -f Dockerfile.rocm .\n\nalias drun='docker run -it --rm --network=host --device=/dev/kfd --device=/dev/dri --ipc=host \\\n    --shm-size 16G --group-add video --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \\\n    -v $HOME/dockerx:/dockerx -v /data:/data'\n\ndrun -p 30000:30000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --env \"HF_TOKEN=<secret>\" \\\n    v0.4.5.post3-rocm630 \\\n    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000\n\n# Till flashinfer backend available, --attention-backend triton --sampling-backend pytorch are set by default\ndrun v0.4.5.post3-rocm630 python3 -m sglang.bench_one_batch --batch-size 32 --input 1024 --output 128 --model amd/Meta-Llama-3.1-8B-Instruct-FP8-KV --tp 8 --quantization fp8\n```\n\n----------------------------------------\n\nTITLE: Launching SGLang Server with EAGLE-2 Speculative Decoding\nDESCRIPTION: This snippet launches an SGLang server with EAGLE-2 speculative decoding enabled. It configures parameters like the draft model path, number of steps, top-k branching factor, number of draft tokens, and CUDA graph maximum batch size. It then waits for the server to start.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/speculative_decoding.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nserver_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --speculative-algorithm EAGLE \\\n    --speculative-draft-model-path lmsys/sglang-EAGLE-llama2-chat-7B --speculative-num-steps 3 \\\n    --speculative-eagle-topk 4 --speculative-num-draft-tokens 16 --cuda-graph-max-bs 8\n\"\"\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: Specifies the include directories for the project. This includes the project's source directory, the cutlass include directory, and the flashinfer include directory.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(\n    ${PROJECT_SOURCE_DIR}/include\n    ${PROJECT_SOURCE_DIR}/csrc\n    ${repo-cutlass_SOURCE_DIR}/include\n    ${repo-cutlass_SOURCE_DIR}/tools/util/include\n    ${repo-flashinfer_SOURCE_DIR}/include\n    ${repo-flashinfer_SOURCE_DIR}/csrc\n)\n```\n\n----------------------------------------\n\nTITLE: Download Test Data\nDESCRIPTION: This command downloads the test dataset (test.jsonl) from the openai/grade-school-math GitHub repository. The dataset is required for running the benchmarks.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/tree_of_thought_deep/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nwget https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl\n```\n\n----------------------------------------\n\nTITLE: Displaying RDMA Link Status with rdma link show\nDESCRIPTION: This command uses `rdma link show` to display the status of RDMA links. It verifies that the links are active and in the LINK_UP state.  It uses `ibdev2netdev` command to map IB devices to network devices.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/deploy_on_k8s.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n$ rdma link show\n8/1: mlx5_bond_0/1: state ACTIVE physical_state LINK_UP netdev reth0\n9/1: mlx5_bond_1/1: state ACTIVE physical_state LINK_UP netdev reth2\n10/1: mlx5_bond_2/1: state ACTIVE physical_state LINK_UP netdev reth4\n11/1: mlx5_bond_3/1: state ACTIVE physical_state LINK_UP netdev reth6\n\n$ ibdev2netdev\n8/1: mlx5_bond_0/1: state ACTIVE physical_state LINK_UP netdev reth0\n9/1: mlx5_bond_1/1: state ACTIVE physical_state LINK_UP netdev reth2\n10/1: mlx5_bond_2/1: state ACTIVE physical_state LINK_UP netdev reth4\n11/1: mlx5_bond_3/1: state ACTIVE physical_state LINK_UP netdev reth6\n```\n\n----------------------------------------\n\nTITLE: Launch SGLang Server (Python)\nDESCRIPTION: Launches the SGLang server with a specified model path and port. This command uses the `sglang.launch_server` module. The `--model-path` argument specifies the path to the language model, and the `--port` argument sets the port for the server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/mmlu/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n```\n\n----------------------------------------\n\nTITLE: Benchmarking LMQL city information retrieval\nDESCRIPTION: This command benchmarks the LMQL framework for city information retrieval. It executes the `bench_other.py` script with the `--backend lmql` option, specifying parallel requests.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/json_jump_forward/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython3 bench_other.py --mode city --backend lmql --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Extracting and Printing Content from Response\nDESCRIPTION: This snippet extracts the `text` field from the JSON response of an API call, splits it into `reasoing_content` and `content` based on the `</think>` tag, and prints both. The `print_highlight` function is presumably used for formatting the output.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/structured_outputs_for_reasoning_models.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nreasoing_content = response.json()[\"text\"].split(\"</think>\")[0]\ncontent = response.json()[\"text\"].split(\"</think>\")[1]\nprint_highlight(f\"reasoing_content: {reasoing_content}\\n\\ncontent: {content}\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Guidance\nDESCRIPTION: This command executes the benchmark script using Guidance as the backend, with a specified model path, context length, and parallelism.  It is configured to run 25 questions with parallel setting to 1 and context length set to 4096. It requires the model path to be specified.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llm_judge/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_other.py --backend guidance --num-questions 25 --parallel 1 --n-ctx 4096 --model-path path/to/gguf\n```\n\n----------------------------------------\n\nTITLE: Setting Target Compile Options\nDESCRIPTION: Sets the compile options for the `common_ops` target.  Specifically, it applies the `SGL_KERNEL_CUDA_FLAGS` when the compile language is CUDA.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt#_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_compile_options(common_ops PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:${SGL_KERNEL_CUDA_FLAGS}>)\n```\n\n----------------------------------------\n\nTITLE: Compiling Jupyter Notebooks\nDESCRIPTION: This command compiles all Jupyter notebooks in the project. This step is crucial for ensuring that the notebooks are up-to-date and ready for documentation generation or serving. This is part of the documentation build process.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake compile\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang\nDESCRIPTION: These commands execute the SGLang benchmark script with different numbers of questions and parallel requests. The first command runs 25 questions with 8 parallel requests, and the second runs 16 questions with 1 parallel request. The benchmark script measures the performance of SGLang.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/llm_judge/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 25 --parallel 8\n```\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 16 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Terminating Server Process\nDESCRIPTION: Terminates the server process using the `terminate_process` function. This is commonly used to shut down the server after testing.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/native_api.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nterminate_process(server_process)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SGLang\nDESCRIPTION: This command runs the sglang benchmark script with a specified number of questions. It measures the performance of the sglang server.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/benchmark/multi_chain_reasoning/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 64\n```\n\nLANGUAGE: python\nCODE:\n```\npython3 bench_sglang.py --num-questions 32 --parallel 1\n```\n\n----------------------------------------\n\nTITLE: Compiling and Serving Documentation Locally\nDESCRIPTION: This command compiles the documentation and serves it locally, automatically rebuilding when files change. It enables developers to preview the documentation and verify changes before submitting them. The browser can be opened at the displayed port to view the docs. It relies on the serve.sh script.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash serve.sh\n```\n\n----------------------------------------\n\nTITLE: Enabling RPD Profiler in SGLang Scheduler\nDESCRIPTION: This patch modifies the SGLang scheduler to integrate with the RPD profiler. It adds imports for `rpdTracerControl` and disables its creation initially. It then conditionally starts and stops the RPD profiler based on the `tp_rank` (tensor parallel rank), pushing and popping a profiling range.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/3rdparty/amd/profiling/PROFILING.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndiff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 62d1ff9..9021c01 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -71,6 +71,8 @@ from sglang.srt.utils import (\n     suppress_other_loggers,\n ) from sglang.utils import get_exception_traceback\n+from rpdTracerControl import rpdTracerControl\n+rpdTracerControl.skipCreate()\n\nlogger = logging.getLogger(__name__)\n\n@@ -245,6 +247,7 @@ class Scheduler:\n                 ], with_stack=True,\n             )\n+            self.rpd = rpdTracerControl()\n\n    @torch.inference_mode()\n    def event_loop(self):\n@@ -1027,15 +1030,24 @@ class Scheduler:\n    def start_profile(self) -> None:\n        if self.profiler is None:\n            raise RuntimeError(\"Profiler is not enabled.\")\n-        self.profiler.start()\n+        #self.profiler.start() #block pytorch profiler for rpd profiler enabling\n        if self.tp_rank == 0 or self.tp_rank == 1:\n            self.rpd.start()\n            self.rpd.rangePush(\"\", \"rpd profile range\", \"\")\n            logger.info(\"rpd is enabled\")\n\n    def stop_profile(self) -> None:\n        if self.profiler is None:\n            raise RuntimeError(\"Profiler is not enabled.\")\n-        self.profiler.stop()\n-        self.profiler.export_chrome_trace(\n-            self.torch_profiler_trace_dir + \"/\" + str(time.time()) + \".trace.json.gz\"\n-        )\n+        #self.profiler.stop()\n+        #self.profiler.export_chrome_trace(\n+        #    self.torch_profiler_trace_dir + \"/\" + str(time.time()) + \".trace.json.gz\"\n+        #)\n        if self.tp_rank ==0 or self.tp_rank ==1:\n            self.rpd.rangePop()\n            self.rpd.stop()\n            self.rpd.flush()\n            logger.info(\"rpd is done\")\n        logger.info(\"Profiler is done\")\n```\n\n----------------------------------------\n\nTITLE: Cloning SGLang Repository\nDESCRIPTION: This command clones the forked SGLang repository from GitHub to your local machine.  Replace `<your_user_name>` with your GitHub username. This step is essential for setting up a local development environment where you can make and test changes.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/references/contribution_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<your_user_name>/sglang.git\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Offline Engine in Python\nDESCRIPTION: This snippet demonstrates how to shut down the SGLang offline engine.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/backend/separate_reasoning.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nllm.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Cleaning Notebook Outputs\nDESCRIPTION: These commands install `nbstripout` and then remove the outputs from all Jupyter Notebook files in the project.  This helps keep PRs clean by avoiding unnecessary changes in the diff. Requires `nbstripout` to be installed.\nSOURCE: https://github.com/sgl-project/sglang/blob/main/docs/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install nbstripout\n```\n\nLANGUAGE: bash\nCODE:\n```\nfind . -name '*.ipynb' -exec nbstripout {} \\;\n```"
  }
]
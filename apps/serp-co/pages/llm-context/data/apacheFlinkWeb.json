[
  {
    "owner": "apache",
    "repo": "flink-web",
    "content": "TITLE: Registering Google Protobuf Serializer with Kryo in Flink Java\nDESCRIPTION: Code snippet showing how to register Google Protobuf serializer with Kryo. This enables efficient serialization of Protobuf-generated classes using Protobuf's own serialization mechanism.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nenv.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, ProtobufSerializer.class);\n```\n\n----------------------------------------\n\nTITLE: Implementing Windowed Top-K Aggregation in Flink SQL\nDESCRIPTION: This SQL snippet shows how to perform a windowed Top-K aggregation using the new window function capabilities in Flink 1.13. The query uses window start and end time references to select the top 100 records ordered by total_price within each window.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT window_time, ...\n  FROM (\n    SELECT *, ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY total_price DESC) \n      as rank \n    FROM t\n  ) WHERE rank <= 100;\n```\n\n----------------------------------------\n\nTITLE: Removed Methods from StreamExecutionEnvironment Class\nDESCRIPTION: Methods that have been removed from the StreamExecutionEnvironment class. These methods were used for serialization, source configuration, checkpointing, and other execution environment settings.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_24\n\nLANGUAGE: java\nCODE:\n```\nvoid addDefaultKryoSerializer(java.lang.Class<?>, com.esotericsoftware.kryo.Serializer<?>)\nvoid addDefaultKryoSerializer(java.lang.Class<?>, java.lang.Class<? extends com.esotericsoftware.kryo.Serializer<? extends ?>>)\norg.apache.flink.streaming.api.datastream.DataStreamSource<OUT> addSource(org.apache.flink.streaming.api.functions.source.SourceFunction<OUT>)\norg.apache.flink.streaming.api.datastream.DataStreamSource<OUT> addSource(org.apache.flink.streaming.api.functions.source.SourceFunction<OUT>, java.lang.String)\norg.apache.flink.streaming.api.datastream.DataStreamSource<OUT> addSource(org.apache.flink.streaming.api.functions.source.SourceFunction<OUT>, org.apache.flink.api.common.typeinfo.TypeInformation<OUT>)\norg.apache.flink.streaming.api.datastream.DataStreamSource<OUT> addSource(org.apache.flink.streaming.api.functions.source.SourceFunction<OUT>, java.lang.String, org.apache.flink.api.common.typeinfo.TypeInformation<OUT>)\norg.apache.flink.streaming.api.environment.StreamExecutionEnvironment enableCheckpointing(long, org.apache.flink.streaming.api.CheckpointingMode, boolean)\norg.apache.flink.streaming.api.environment.StreamExecutionEnvironment enableCheckpointing()\nint getNumberOfExecutionRetries()\norg.apache.flink.api.common.restartstrategy.RestartStrategies$RestartStrategyConfiguration getRestartStrategy()\norg.apache.flink.runtime.state.StateBackend getStateBackend()\norg.apache.flink.streaming.api.TimeCharacteristic getStreamTimeCharacteristic()\nboolean isForceCheckpointing()\norg.apache.flink.streaming.api.datastream.DataStream<java.lang.String> readFileStream(java.lang.String, long, org.apache.flink.streaming.api.functions.source.FileMonitoringFunction$WatchType)\norg.apache.flink.streaming.api.datastream.DataStreamSource<java.lang.String> readTextFile(java.lang.String)\norg.apache.flink.streaming.api.datastream.DataStreamSource<java.lang.String> readTextFile(java.lang.String, java.lang.String)\nvoid registerType(java.lang.Class<?>)\nvoid registerTypeWithKryoSerializer(java.lang.Class<?>, com.esotericsoftware.kryo.Serializer<?>)\nvoid registerTypeWithKryoSerializer(java.lang.Class<?>, java.lang.Class<? extends com.esotericsoftware.kryo.Serializer>)\nvoid setNumberOfExecutionRetries(int)\nvoid setRestartStrategy(org.apache.flink.api.common.restartstrategy.RestartStrategies$RestartStrategyConfiguration)\norg.apache.flink.streaming.api.environment.StreamExecutionEnvironment setStateBackend(org.apache.flink.runtime.state.StateBackend)\nvoid setStreamTimeCharacteristic(org.apache.flink.streaming.api.TimeCharacteristic)\n```\n\n----------------------------------------\n\nTITLE: Enhanced Email Collection with Multiple Fields in Java\nDESCRIPTION: Implementation of enhanced message collection supporting multiple email fields (subject, sent date, received date).\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_13\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.table.data.GenericRowData;\nimport org.apache.flink.table.data.RowData;\nimport org.apache.flink.table.data.TimestampData;\n\npublic class ImapSource extends RichSourceFunction<RowData> {\n\n    private void collectMessages(SourceFunction.SourceContext<RowData> ctx, Message[] messages) {\n        for (Message message : messages) {\n            try {\n                collectMessage(ctx, message);\n            } catch (MessagingException ignored) {}\n        }\n    }\n\n    private void collectMessage(SourceFunction.SourceContext<RowData> ctx, Message message)\n        throws MessagingException {\n        final GenericRowData row = new GenericRowData(columnNames.size());\n\n        for (int i = 0; i < columnNames.size(); i++) {\n            switch (columnNames.get(i)) {\n                case \"SUBJECT\":\n                    row.setField(i, StringData.fromString(message.getSubject()));\n                    break;\n                case \"SENT\":\n                    row.setField(i, TimestampData.fromInstant(message.getSentDate().toInstant()));\n                    break;\n                case \"RECEIVED\":\n                    row.setField(i, TimestampData.fromInstant(message.getReceivedDate().toInstant()));\n                    break;\n                // ...\n            }\n        }\n\n        ctx.collect(row);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating View for Enriched User Behavior Data in Flink SQL\nDESCRIPTION: This SQL creates a view that joins user behavior data with the category dimension table using a temporal table join. It enriches the data with parent category names.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW rich_user_behavior AS\nSELECT U.user_id, U.item_id, U.behavior, C.parent_category_name as category_name\nFROM user_behavior AS U LEFT JOIN category_dim FOR SYSTEM_TIME AS OF U.proctime AS C\nON U.category_id = C.sub_category_id;\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Keyed ProcessFunction in Apache Flink\nDESCRIPTION: Sample code showing the basic structure of a KeyedProcessFunction in Apache Flink. This demonstrates the contract for implementing custom stream processing logic with access to keyed state and timers.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-30-demo-fraud-detection-3.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic class SomeProcessFunction extends KeyedProcessFunction<KeyType, InputType, OutputType> {\n\n\tpublic void processElement(InputType event, Context ctx, Collector<OutputType> out){}\n\n\tpublic void onTimer(long timestamp, OnTimerContext ctx, Collector<OutputType> out) {}\n\n\tpublic void open(Configuration parameters){}\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Stateless FlatMap Function in Flink\nDESCRIPTION: Java implementation of a simple stateless FlatMap function in Flink. This function prepends 'hello' to the input string and collects the result.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-03-a-guide-for-unit-testing-in-apache-flink.md#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\npublic class MyStatelessFlatMap implements FlatMapFunction<String, String> {\n  @Override\n  public void flatMap(String in, Collector<String> collector) throws Exception {\n    String out = \"hello \" + in;\n    collector.collect(out);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing ResultTypeQueryable for Avro GenericRecord Types in Java\nDESCRIPTION: Example of how to implement ResultTypeQueryable to provide type information for Avro GenericRecord types. This approach creates a source function that specifies the GenericRecordAvroTypeInfo with the required schema.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic static class AvroGenericSource implements SourceFunction<GenericRecord>, ResultTypeQueryable<GenericRecord> {\n  private final GenericRecordAvroTypeInfo producedType;\n\n  public AvroGenericSource(Schema schema) {\n    this.producedType = new GenericRecordAvroTypeInfo(schema);\n  }\n  \n  @Override\n  public TypeInformation<GenericRecord> getProducedType() {\n    return producedType;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Temporal Table Function in Java\nDESCRIPTION: Creates and registers a temporal table function for currency conversion rates using Flink's Table API. Defines a sample dataset with currency rates, creates a table, and registers a temporal table function with versioning field and primary key.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-05-14-temporal-tables.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.table.functions.TemporalTableFunction;\n\n(...)\n\n// Get the stream and table environments.\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nStreamTableEnvironment tEnv = StreamTableEnvironment.getTableEnvironment(env);\n\n// Provide a sample static data set of the rates history table.\nList <Tuple2<String, Long>>ratesHistoryData =new ArrayList<>();\n\nratesHistoryData.add(Tuple2.of(\"USD\", 102L)); \nratesHistoryData.add(Tuple2.of(\"EUR\", 114L)); \nratesHistoryData.add(Tuple2.of(\"YEN\", 1L)); \nratesHistoryData.add(Tuple2.of(\"EUR\", 116L)); \nratesHistoryData.add(Tuple2.of(\"USD\", 105L));\n\n// Create and register an example table using the sample data set.\nDataStream<Tuple2<String, Long>> ratesHistoryStream = env.fromCollection(ratesHistoryData);\n\nTable ratesHistory = tEnv.fromDataStream(ratesHistoryStream, \"r_currency, r_rate, r_proctime.proctime\");\n\ntEnv.registerTable(\"RatesHistory\", ratesHistory);\n\n// Create and register the temporal table function \"rates\".\n// Define \"r_proctime\" as the versioning field and \"r_currency\" as the primary key.\nTemporalTableFunction rates = ratesHistory.createTemporalTableFunction(\"r_proctime\", \"r_currency\");\n\ntEnv.registerFunction(\"Rates\", rates);\n\n(...)\n```\n\n----------------------------------------\n\nTITLE: Implementing State Registration with ValueState in Apache Flink\nDESCRIPTION: Example showing how to register and configure ValueState in a KeyedProcessFunction using state descriptors and TypeInformation. This code demonstrates the basic pattern for state registration with built-in serialization in Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-29-state-unlocked-interacting-with-state-in-apache-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic class MyFunction extends KeyedProcessFunction<Key, Input, Output> {\n\n  private transient ValueState<MyState> valueState;\n\n  public void open(Configuration parameters) {\n    ValueStateDescriptor<MyState> descriptor =\n      new ValueStateDescriptor<>(\"my-state\", TypeInformation.of(MyState.class));\n\n    valueState = getRuntimeContext().getState(descriptor);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Periodic Savepoints in Flink Kubernetes Operator\nDESCRIPTION: Demonstrates how to set up periodic savepoints for Flink applications and session jobs using the Kubernetes Operator configuration. It also shows how to configure the savepoint history cleanup policy.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-07-25-release-kubernetes-operator-1.1.0.md#2025-04-08_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nflinkConfiguration:\n  ...\n  kubernetes.operator.periodic.savepoint.interval: 6h\n\nkubernetes.operator.savepoint.history.max.count: 5\nkubernetes.operator.savepoint.history.max.age: 48h\n```\n\n----------------------------------------\n\nTITLE: Implementing TokenBucketRateLimitingStrategy in Java for Apache Flink\nDESCRIPTION: This code snippet demonstrates the implementation of a TokenBucketRateLimitingStrategy using the Bucket4j library. It creates a token bucket with 10 initial tokens, refilling at a rate of 1 token per second, and checks if there are sufficient tokens for each request.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-11-25-async-sink-rate-limiting-strategy.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic class TokenBucketRateLimitingStrategy implements RateLimitingStrategy {\n    \n    private final Bucket bucket;\n\n    public TokenBucketRateLimitingStrategy() {\n        Refill refill = Refill.intervally(1, Duration.ofSeconds(1));\n        Bandwidth limit = Bandwidth.classic(10, refill);\n        this.bucket = Bucket4j.builder()\n            .addLimit(limit)\n            .build();\n    }\n    \n    // ... (information methods not needed)\n    \n    @Override\n    public boolean shouldBlock(RequestInfo requestInfo) {\n        return bucket.tryConsume(requestInfo.getBatchSize());\n    }\n    \n}\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Source and Elasticsearch Sink Tables with SQL DDL in Apache Flink\nDESCRIPTION: This code demonstrates how to define logical tables in Apache Flink using CREATE TABLE statements. It includes configuration for a Kafka source table with watermarking, an Elasticsearch sink table, and a sample query that processes data from Kafka and writes results to Elasticsearch using SQL window functions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-20-ddl.md#2025-04-08_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Define a table called orders that is backed by a Kafka topic\n-- The definition includes all relevant Kafka properties,\n-- the underlying format (JSON) and even defines a\n-- watermarking algorithm based on one of the fields\n-- so that this table can be used with event time.\nCREATE TABLE orders (\n\tuser_id    BIGINT,\n\tproduct    STRING,\n\torder_time TIMESTAMP(3),\n\tWATERMARK FOR order_time AS order_time - '5' SECONDS\n) WITH (\n\t'connector.type'    \t = 'kafka',\n\t'connector.version' \t = 'universal',\n\t'connector.topic'   \t = 'orders',\n\t'connector.startup-mode' = 'earliest-offset',\n\t'connector.properties.bootstrap.servers' = 'localhost:9092',\n\t'format.type' = 'json' \n);\n\n-- Define a table called product_analysis\n-- on top of ElasticSearch 7 where we \n-- can write the results of our query. \nCREATE TABLE product_analysis (\n\tproduct \tSTRING,\n\ttracking_time \tTIMESTAMP(3),\n\tunits_sold \tBIGINT\n) WITH (\n\t'connector.type'    = 'elasticsearch',\n\t'connector.version' = '7',\n\t'connector.hosts'   = 'localhost:9200',\n\t'connector.index'   = 'ProductAnalysis',\n\t'connector.document.type' = 'analysis' \n);\n\n-- A simple query that analyzes order data\n-- from Kafka and writes results into \n-- ElasticSearch. \nINSERT INTO product_analysis\nSELECT\n\tproduct_id,\n\tTUMBLE_START(order_time, INTERVAL '1' DAY) as tracking_time,\n\tCOUNT(*) as units_sold\nFROM orders\nGROUP BY\n\tproduct_id,\n\tTUMBLE(order_time, INTERVAL '1' DAY);\n```\n\n----------------------------------------\n\nTITLE: Implementing Windowed Aggregates using Flink Table API and SQL\nDESCRIPTION: Example showing equivalent Table API and SQL queries to compute windowed aggregates on temperature sensor measurements. The code demonstrates how to set up the execution environment, define table sources, and perform window operations with both APIs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-30-dynamic-tables.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\nval tEnv = TableEnvironment.getTableEnvironment(env)\n\n// define a table source to read sensor data (sensorId, time, room, temp)\nval sensorTable = ??? // can be a CSV file, Kafka topic, database, or ...\n// register the table source\ntEnv.registerTableSource(\"sensors\", sensorTable)\n\n// Table API\nval tapiResult: Table = tEnv.scan(\"sensors\")   // scan sensors table\n .window(Tumble over 1.hour on 'rowtime as 'w) // define 1-hour window\n .groupBy('w, 'room)                           // group by window and room\n .select('room, 'w.end, 'temp.avg as 'avgTemp) // compute average temperature\n\n// SQL\nval sqlResult: Table = tEnv.sql(\"\"\"\n |SELECT room, TUMBLE_END(rowtime, INTERVAL '1' HOUR), AVG(temp) AS avgTemp\n |FROM sensors\n |GROUP BY TUMBLE(rowtime, INTERVAL '1' HOUR), room\n |\"\"\".stripMargin)\n```\n\n----------------------------------------\n\nTITLE: Inserting Cumulative UV Data into Elasticsearch in Flink SQL\nDESCRIPTION: This SQL query calculates and inserts cumulative unique visitor data into the Elasticsearch table. It uses date formatting, string manipulation, and aggregation functions to generate 10-minute interval data points.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO cumulative_uv\nSELECT date_str, MAX(time_str), COUNT(DISTINCT user_id) as uv\nFROM (\n  SELECT\n    DATE_FORMAT(ts, 'yyyy-MM-dd') as date_str,\n    SUBSTR(DATE_FORMAT(ts, 'HH:mm'),1,4) || '0' as time_str,\n    user_id\n  FROM user_behavior)\nGROUP BY date_str;\n```\n\n----------------------------------------\n\nTITLE: Implementing a Pandas UDF for temperature interpolation\nDESCRIPTION: A Pandas UDF example that takes equipment ID and temperature as input, and interpolates missing temperature values. It uses the pandas.Series as input/output and leverages pandas DataFrame groupby and interpolate functions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-04-pyflink-pandas-udf-support-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@udf(input_types=[DataTypes.STRING(), DataTypes.FLOAT()],\n     result_type=DataTypes.FLOAT(), udf_type='pandas')\ndef interpolate(id, temperature):\n    # takes id: pandas.Series and temperature: pandas.Series as input\n    df = pd.DataFrame({'id': id, 'temperature': temperature})\n\n    # use interpolate() to interpolate the missing temperature\n    interpolated_df = df.groupby('id').apply(\n        lambda group: group.interpolate(limit_direction='both'))\n\n    # output temperature: pandas.Series\n    return interpolated_df['temperature']\n```\n\n----------------------------------------\n\nTITLE: Implementing Row-based Map Operation in PyFlink Table API\nDESCRIPTION: This code snippet shows how to use a row-based map operation in the PyFlink Table API. It defines a user-defined function that increments a column value and applies it to a table using the map() operation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@udf(result_type=DataTypes.ROW(\n  [DataTypes.FIELD(\"c1\", DataTypes.BIGINT()),\n   DataTypes.FIELD(\"c2\", DataTypes.STRING())]))\ndef increment_column(r: Row) -> Row:\n  return Row(r[0] + 1, r[1])\n\ntable = ...  # type: Table\nmapped_result = table.map(increment_column)\n```\n\n----------------------------------------\n\nTITLE: Implementing PatternEvaluator in Flink\nDESCRIPTION: Defines a custom KeyedBroadcastProcessFunction that implements the pattern matching logic. It maintains keyed state for previous actions and uses broadcast state for the current pattern.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-26-broadcast-state.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic static class PatternEvaluator\n    extends KeyedBroadcastProcessFunction<Long, Action, Pattern, Tuple2<Long, Pattern>> {\n \n  // handle for keyed state (per user)\n  ValueState<String> prevActionState;\n  // broadcast state descriptor\n  MapStateDescriptor<Void, Pattern> patternDesc;\n \n  @Override\n  public void open(Configuration conf) {\n    // initialize keyed state\n    prevActionState = getRuntimeContext().getState(\n      new ValueStateDescriptor<>(\"lastAction\", Types.STRING));\n    patternDesc = \n      new MapStateDescriptor<>(\"patterns\", Types.VOID, Types.POJO(Pattern.class));\n  }\n\n  /**\n   * Called for each user action.\n   * Evaluates the current pattern against the previous and\n   * current action of the user.\n   */\n  @Override\n  public void processElement(\n     Action action, \n     ReadOnlyContext ctx, \n     Collector<Tuple2<Long, Pattern>> out) throws Exception {\n   // get current pattern from broadcast state\n   Pattern pattern = ctx\n     .getBroadcastState(this.patternDesc)\n     // access MapState with null as VOID default value\n     .get(null);\n   // get previous action of current user from keyed state\n   String prevAction = prevActionState.value();\n   if (pattern != null && prevAction != null) {\n     // user had an action before, check if pattern matches\n     if (pattern.firstAction.equals(prevAction) && \n         pattern.secondAction.equals(action.action)) {\n       // MATCH\n       out.collect(new Tuple2<>(ctx.getCurrentKey(), pattern));\n     }\n   }\n   // update keyed state and remember action for next pattern evaluation\n   prevActionState.update(action.action);\n }\n\n  /**\n   * Called for each new pattern.\n   * Overwrites the current pattern with the new pattern.\n   */\n  @Override\n  public void processBroadcastElement(\n      Pattern pattern, \n      Context ctx, \n      Collector<Tuple2<Long, Pattern>> out) throws Exception {\n    // store the new pattern by updating the broadcast state\n    BroadcastState<Void, Pattern> bcState = ctx.getBroadcastState(patternDesc);\n    // storing in MapState with null as VOID default value\n    bcState.put(null, pattern);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Source Function Methods in Java\nDESCRIPTION: Implementation of run(), cancel(), and close() methods for the IMAP source connector.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nimport jakarta.mail.*;\nimport org.apache.flink.streaming.api.functions.source.RichSourceFunction;\nimport org.apache.flink.streaming.api.functions.source.SourceFunction;\nimport org.apache.flink.table.data.RowData;\n\npublic class ImapSource extends RichSourceFunction<RowData> {\n    private transient volatile boolean running = false;\n\n    // â€¦\n\n    @Override\n    public void run(SourceFunction.SourceContext<RowData> ctx) throws Exception {\n        connect();\n        running = true;\n\n        // TODO: Listen for new messages\n\n        while (running) {\n            // Trigger some IMAP request to force the server to send a notification\n            folder.getMessageCount();\n            Thread.sleep(250);\n        }\n    }\n\n    @Override\n    public void cancel() {\n        running = false;\n    }\n\n    @Override\n    public void close() throws Exception {\n        if (folder != null) {\n            folder.close();\n        }\n\n        if (store != null) {\n            store.close();\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Two-Input Join in Flink DataStream API\nDESCRIPTION: Example of joining two data streams (orders and customers) on a common key using the DataStream API's join operation with a GlobalWindow. The code includes a ContinuousProcessingTimeTrigger that is used in streaming mode but ignored in batch mode, demonstrating how the same code adapts to different execution modes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-03-11-batch-execution-mode.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nDataStreamSource<String[]> orders = env\n    .fromSource(\n        FileSource.forRecordStreamFormat(new TsvFormat(), ordersPath).build(),\n        WatermarkStrategy.<String[]>noWatermarks()\n            .withTimestampAssigner((record, previous) -> -1),\n        \"Text file\"\n    );\n\nPath customersPath = new Path(config.get(\"customers\"));\nDataStreamSource<String[]> customers = env\n    .fromSource(\n        FileSource.forRecordStreamFormat(new TsvFormat(), customersPath).build(),\n        WatermarkStrategy.<String[]>noWatermarks()\n            .withTimestampAssigner((record, previous) -> -1),\n        \"Text file\"\n    );\n\nDataStream<Tuple2<String, String>> dataStream = orders.join(customers)\n    .where(order -> order[1]).equalTo(customer -> customer[0]) // join on customer id\n    .window(GlobalWindows.create())\n    .trigger(ContinuousProcessingTimeTrigger.of(Time.seconds(5)))\n    .apply(new ProjectFunction());\n```\n\n----------------------------------------\n\nTITLE: Implementing IMAP Source Class in Java\nDESCRIPTION: Implements the ImapSource class extending RichSourceFunction, incorporating the ImapSourceOptions and column names. This class will be responsible for reading data from the IMAP server.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.streaming.api.functions.source.RichSourceFunction;\nimport org.apache.flink.table.data.RowData;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class ImapSource extends RichSourceFunction<RowData> {\n    private final ImapSourceOptions options;\n    private final List<String> columnNames;\n\n    public ImapSource(\n        ImapSourceOptions options,\n        List<String> columnNames\n    ) {\n        this.options = options;\n        this.columnNames = columnNames.stream()\n            .map(String::toUpperCase)\n            .collect(Collectors.toList());\n    }\n\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing Materialized Tables in Flink SQL\nDESCRIPTION: Example demonstrates creating a materialized table with specified freshness interval, managing the data refresh pipeline through suspend/resume operations, and refreshing historical partitions manually.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-08-02-release-1.20.0.md#2025-04-08_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE MATERIALIZED TABLE dwd_orders\n(\n PRIMARY KEY(ds, id) NOT ENFORCED\n)\nPARTITIONED BY (ds)\nFRESHNESS = INTERVAL '3' MINUTE\nAS SELECT \n o.ds\n o.id,\n o.order_number,\n o.user_id,\n...\nFROM \n orders as o\n LEFT JOIN products FOR SYSTEM_TIME AS OF proctime() AS prod\n ON o.product_id = prod.id\n LEFT JOIN order_pay AS pay\n ON o.id = pay.order_id and o.ds = pay.ds;\n\nALTER MATERIALIZED TABLE dwd_orders SUSPEND;\n\nALTER MATERIALIZED TABLE dwd_orders RESUME\nWITH(\n 'sink.parallesim' = '10'\n);\n\nALTER MATERIALIZED TABLE dwd_orders REFRESH PARTITION(ds='20231023');\n```\n\n----------------------------------------\n\nTITLE: Implementing Exponentially Decaying Moving Average in Flink Table API (Scala)\nDESCRIPTION: This advanced example demonstrates a sliding window of one hour with results every second, calculating an exponentially decaying moving average that weighs recent orders more heavily than older ones. It shows future capabilities planned for Flink's Table API.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-29-table-sql-api-update.md#2025-04-08_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\ntable\n  .window(Slide over 1.hour every 1.second as 'w)\n  .groupBy('productId, 'w)\n  .select(\n    'w.end,\n    'productId,\n    ('unitPrice * ('rowtime - 'w.start).exp() / 1.hour).sum / (('rowtime - 'w.start).exp() / 1.hour).sum)\n```\n\n----------------------------------------\n\nTITLE: Enabling Asynchronous State Access in Flink SQL\nDESCRIPTION: Configuration parameter to enable asynchronous state access for SQL operators in Flink 2.0. When activated, all supported SQL operators automatically switch to asynchronous state access mode without requiring code changes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2025-03-24-release-2.0.0.md#2025-04-08_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ntable.exec.async-state.enabled\n```\n\n----------------------------------------\n\nTITLE: Implementing Two-Phase Commit File Sink in Flink\nDESCRIPTION: Implementation example showing the four required methods to extend TwoPhaseCommitSinkFunction for creating an exactly-once file sink. The implementation includes transaction begin, pre-commit, commit, and abort phases.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-02-28-end-to-end-exactly-once-apache-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// Required method implementations for TwoPhaseCommitSinkFunction:\n\n1. beginTransaction() {\n    // Create temporary file in temporary directory\n}\n\n2. preCommit() {\n    // Flush and close file\n    // Start new transaction for next checkpoint\n}\n\n3. commit() {\n    // Atomically move pre-committed file to destination directory\n}\n\n4. abort() {\n    // Delete temporary file\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing DynamicTableSourceFactory in Java\nDESCRIPTION: Factory implementation for making the connector discoverable by Flink. Defines factory identifier, configuration options, and table source creation logic.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part1.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nimport java.util.HashSet;\nimport java.util.Set;\nimport org.apache.flink.configuration.ConfigOption;\nimport org.apache.flink.table.connector.source.DynamicTableSource;\nimport org.apache.flink.table.factories.DynamicTableSourceFactory;\nimport org.apache.flink.table.factories.FactoryUtil;\n\npublic class ImapTableSourceFactory implements DynamicTableSourceFactory {\n  @Override\n  public String factoryIdentifier() {\n    return \"imap\";\n  }\n\n  @Override\n  public Set<ConfigOption<?>> requiredOptions() {\n    return new HashSet<>();\n  }\n\n  @Override\n  public Set<ConfigOption<?>> optionalOptions() {\n    return new HashSet<>();\n  }\n\n  @Override\n  public DynamicTableSource createDynamicTableSource(Context ctx) {\n    final FactoryUtil.TableFactoryHelper factoryHelper = FactoryUtil.createTableFactoryHelper(this, ctx);\n    factoryHelper.validate();\n\n    return new ImapTableSource();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing onTimer Method for Window State Cleanup in Java\nDESCRIPTION: This code snippet shows the implementation of the onTimer method to clean up window state in a Flink application. It removes events that are older than the widest window span from the state.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-30-demo-fraud-detection-3.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic void onTimer(final long timestamp, final OnTimerContext ctx, final Collector<Alert> out)\n    throws Exception {\n\n  Rule widestWindowRule = ctx.getBroadcastState(Descriptors.rulesDescriptor).get(WIDEST_RULE_KEY);\n\n  Optional<Long> cleanupEventTimeWindow =\n      Optional.ofNullable(widestWindowRule).map(Rule::getWindowMillis);\n  Optional<Long> cleanupEventTimeThreshold =\n      cleanupEventTimeWindow.map(window -> timestamp - window);\n  // Remove events that are older than (timestamp - widestWindowSpan)ms\n  cleanupEventTimeThreshold.ifPresent(this::evictOutOfScopeElementsFromWindow);\n}\n\nprivate void evictOutOfScopeElementsFromWindow(Long threshold) {\n  try {\n    Iterator<Long> keys = windowState.keys().iterator();\n    while (keys.hasNext()) {\n      Long stateEventTime = keys.next();\n      if (stateEventTime < threshold) {\n        keys.remove();\n      }\n    }\n  } catch (Exception ex) {\n    throw new RuntimeException(ex);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining FlinkSessionJob Resource in YAML\nDESCRIPTION: Example YAML configuration for creating a FlinkSessionJob resource. It specifies the deployment name, JAR URI, parallelism, and upgrade mode for a Flink session job.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-06-05-release-kubernetes-operator-1.0.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: flink.apache.org/v1beta1\nkind: FlinkSessionJob\nmetadata:\n  name: basic-session-job-example\nspec:\n  deploymentName: basic-session-cluster\n  job:\n    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.15.0/flink-examples-streaming_2.12-1.15.0-TopSpeedWindowing.jar\n    parallelism: 4\n    upgradeMode: stateless\n```\n\n----------------------------------------\n\nTITLE: Using User-defined Table Functions with Join in Flink Table API (Scala)\nDESCRIPTION: This snippet demonstrates how to use a user-defined table function to extract properties from a string column. It creates an instance of a PropertiesExtractor function, joins it with the original table, and selects the relevant columns including the extracted values.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-29-table-sql-api-update.md#2025-04-08_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\n// create an instance of the table function\nval extractPrefs = new PropertiesExtractor()\n\n// derive rows and join them with original row\ntable\n  .join(extractPrefs('prefs) as ('color, 'size))\n  .select('id, 'username, 'color, 'size)\n```\n\n----------------------------------------\n\nTITLE: Creating Elasticsearch Table in Flink SQL\nDESCRIPTION: SQL statement to create a table in Flink SQL that connects to an Elasticsearch index. It defines the schema for storing hourly trading volume data.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE buy_cnt_per_hour (\n    hour_of_day BIGINT,\n    buy_cnt BIGINT\n) WITH (\n    'connector' = 'elasticsearch-7', -- using elasticsearch connector\n    'hosts' = 'http://elasticsearch:9200',  -- elasticsearch address\n    'index' = 'buy_cnt_per_hour'  -- elasticsearch index name, similar to database table name\n);\n```\n\n----------------------------------------\n\nTITLE: Registering Pulsar Topics as Flink Streaming Tables in Java\nDESCRIPTION: This code shows how to register Pulsar topics as streaming tables in Flink. It sets up properties for connecting to Pulsar, creates a StreamTableEnvironment, and registers a Pulsar table source.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-25-query-pulsar-streams-using-apache-flink.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nval tEnv = StreamTableEnvironment.create(env)\n\nval prop = new Properties()\nprop.setProperty(\"service.url\", serviceUrl)\nprop.setProperty(\"admin.url\", adminUrl)\nprop.setProperty(\"flushOnCheckpoint\", \"true\")\nprop.setProperty(\"failOnWrite\", \"true\")\nprops.setProperty(\"topic\", \"test-sink-topic\")\n\ntEnv\n  .connect(new Pulsar().properties(props))\n  .inAppendMode()\n  .registerTableSource(\"sink-table\")\n\nval sql = \"INSERT INTO sink-table .....\"\ntEnv.sqlUpdate(sql)\nenv.execute()\n```\n\n----------------------------------------\n\nTITLE: Implementing Count Windows in Apache Flink with Scala\nDESCRIPTION: This code example shows how to define tumbling and sliding count windows in Flink using the Scala API. The tumbling window collects 100 elements before processing, while the sliding window processes every 10 elements while maintaining a window size of 100 elements.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-04-Introducing-windows.md#2025-04-08_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n// Stream of (sensorId, carCnt)\nval vehicleCnts: DataStream[(Int, Int)] = ...\n\nval tumblingCnts: DataStream[(Int, Int)] = vehicleCnts\n  // key stream by sensorId\n  .keyBy(0)\n  // tumbling count window of 100 elements size\n  .countWindow(100)\n  // compute the carCnt sum \n  .sum(1)\n\nval slidingCnts: DataStream[(Int, Int)] = vehicleCnts\n  .keyBy(0)\n  // sliding count window of 100 elements size and 10 elements trigger interval\n  .countWindow(100, 10)\n  .sum(1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Flink Deployment with Kubernetes YAML\nDESCRIPTION: Minimal YAML configuration for deploying a Flink application using the Kubernetes operator. Defines the deployment specifications including image version, resource allocation, and job configuration.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-04-03-release-kubernetes-operator-0.1.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: flink.apache.org/v1alpha1\nkind: FlinkDeployment\nmetadata:\n  namespace: default\n  name: basic-example\nspec:\n  image: flink:1.14\n  flinkVersion: v1_14\n  flinkConfiguration:\n    taskmanager.numberOfTaskSlots: \"2\"\n  serviceAccount: flink\n  jobManager:\n    replicas: 1\n    resource:\n      memory: \"2048m\"\n      cpu: 1\n  taskManager:\n    resource:\n      memory: \"2048m\"\n      cpu: 1\n  job:\n    jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar\n    parallelism: 2\n    upgradeMode: stateless\n```\n\n----------------------------------------\n\nTITLE: BroadcastProcessFunction Abstract Class Definition\nDESCRIPTION: Shows the structure of BroadcastProcessFunction class which handles both regular stream processing and broadcast stream processing through two distinct methods.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-03-24-demo-fraud-detection-2.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\npublic abstract class BroadcastProcessFunction<IN1, IN2, OUT> {\n\n    public abstract void processElement(IN1 value,\n                                        ReadOnlyContext ctx,\n                                        Collector<OUT> out) throws Exception;\n\n    public abstract void processBroadcastElement(IN2 value,\n                                                 Context ctx,\n                                                 Collector<OUT> out) throws Exception;\n\n}\n```\n\n----------------------------------------\n\nTITLE: Python UDF with Third-party Dependencies\nDESCRIPTION: Example showing how to use third-party libraries (mpmath) in Python UDFs and manage dependencies.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-09-pyflink-udf-support-flink.md#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT())\ndef add(i, j):\n    from mpmath import fadd # add third-party dependency\n    return int(fadd(i, j))\n```\n\n----------------------------------------\n\nTITLE: Time Travel Query in Flink SQL\nDESCRIPTION: Example of using time travel query syntax to access historical data at a specific point in time. This demonstrates the new time traveling feature in Flink 1.18 that allows querying historical versions of data.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-10-24-release-1.18.0.md#2025-04-08_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n-- Query the table `tb` for data on November 11, 2022\nSELECT * FROM tb FOR SYSTEM_TIME AS OF TIMESTAMP '2022-11-11 00:00:00';\n```\n\n----------------------------------------\n\nTITLE: DynamicKeyFunction Implementation\nDESCRIPTION: Implementation of the DynamicKeyFunction class that handles dynamic event routing based on rule definitions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-15-demo-fraud-detection.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\npublic class DynamicKeyFunction\n    extends ProcessFunction<Transaction, Keyed<Transaction, String, Integer>> {\n   ...\n  /* Simplified */\n  List<Rule> rules = /* Rules that are initialized somehow.\n                        Details will be discussed in a future blog post. */;\n\n  @Override\n  public void processElement(\n      Transaction event,\n      Context ctx,\n      Collector<Keyed<Transaction, String, Integer>> out) {\n\n      for (Rule rule :rules) {\n       out.collect(\n           new Keyed<>(\n               event,\n               KeysExtractor.getKey(rule.getGroupingKeyNames(), event),\n               rule.getRuleId()));\n      }\n  }\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Dynamic Table Source in Java\nDESCRIPTION: Implements the createDynamicTableSource method in the ImapTableSourceFactory class. It uses TableFactoryHelper to validate options and create an instance of ImapTableSource with the provided configuration.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nimport java.util.List;\nimport java.util.stream.Collectors;\nimport org.apache.flink.table.factories.DynamicTableSourceFactory;\nimport org.apache.flink.table.factories.FactoryUtil;\nimport org.apache.flink.table.catalog.Column;\n\npublic class ImapTableSourceFactory implements DynamicTableSourceFactory {\n\n    // ...\n\n    @Override\n    public DynamicTableSource createDynamicTableSource(Context ctx) {\n        final FactoryUtil.TableFactoryHelper factoryHelper = FactoryUtil.createTableFactoryHelper(this, ctx);\n        factoryHelper.validate();\n\n        final ImapSourceOptions options = ImapSourceOptions.builder()\n            .host(factoryHelper.getOptions().get(HOST))\n            .port(factoryHelper.getOptions().get(PORT))\n            .user(factoryHelper.getOptions().get(USER))\n            .password(factoryHelper.getOptions().get(PASSWORD))\n            .build();\n\n        final List<String> columnNames = ctx.getCatalogTable().getResolvedSchema().getColumns().stream()\n            .filter(Column::isPhysical)\n            .map(Column::getName)\n            .collect(Collectors.toList());\n\n        return new ImapTableSource(options, columnNames);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Python UDFs - Multiple Approaches\nDESCRIPTION: Demonstrates five different ways to define a Python scalar function that adds two BIGINT columns: using ScalarFunction class, Python function, lambda function, callable object, and partial function.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-09-pyflink-udf-support-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# option 1: extending the base class `ScalarFunction`\nclass Add(ScalarFunction):\n  def eval(self, i, j):\n    return i + j\n\nadd = udf(Add(), [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())\n\n# option 2: Python function\n@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT())\ndef add(i, j):\n  return i + j\n\n# option 3: lambda function\nadd = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())\n\n# option 4: callable function\nclass CallableAdd(object):\n  def __call__(self, i, j):\n    return i + j\n\nadd = udf(CallableAdd(), [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())\n\n# option 5: partial function\ndef partial_add(i, j, k):\n  return i + j + k\n\nadd = udf(functools.partial(partial_add, k=1), [DataTypes.BIGINT(), DataTypes.BIGINT()],\n          DataTypes.BIGINT())\n```\n\n----------------------------------------\n\nTITLE: Complete PyFlink application with Pandas UDF\nDESCRIPTION: A full PyFlink application that demonstrates how to use Pandas UDF. The example creates a table environment, registers the Pandas UDF, defines source and sink tables, and processes data by applying the interpolation function to temperature values.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-04-pyflink-pandas-udf-support-flink.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment, DataTypes\nfrom pyflink.table.udf import udf\nimport pandas as pd\n\nenv = StreamExecutionEnvironment.get_execution_environment()\nenv.set_parallelism(1)\nt_env = StreamTableEnvironment.create(env)\nt_env.get_config().get_configuration().set_boolean(\"python.fn-execution.memory.managed\", True)\n\n@udf(input_types=[DataTypes.STRING(), DataTypes.FLOAT()],\n     result_type=DataTypes.FLOAT(), udf_type='pandas')\ndef interpolate(id, temperature):\n    # takes id: pandas.Series and temperature: pandas.Series as input\n    df = pd.DataFrame({'id': id, 'temperature': temperature})\n\n    # use interpolate() to interpolate the missing temperature\n    interpolated_df = df.groupby('id').apply(\n        lambda group: group.interpolate(limit_direction='both'))\n\n    # output temperature: pandas.Series\n    return interpolated_df['temperature']\n\nt_env.register_function(\"interpolate\", interpolate)\n\nmy_source_ddl = \"\"\"\n    create table mySource (\n        id INT,\n        temperature FLOAT \n    ) with (\n        'connector.type' = 'filesystem',\n        'format.type' = 'csv',\n        'connector.path' = '/tmp/input'\n    )\n\"\"\"\n\nmy_sink_ddl = \"\"\"\n    create table mySink (\n        id INT,\n        temperature FLOAT \n    ) with (\n        'connector.type' = 'filesystem',\n        'format.type' = 'csv',\n        'connector.path' = '/tmp/output'\n    )\n\"\"\"\n\nt_env.execute_sql(my_source_ddl)\nt_env.execute_sql(my_sink_ddl)\n\nt_env.from_path('mySource')\\\n    .select(\"id, interpolate(id, temperature) as temperature\") \\\n    .insert_into('mySink')\n\nt_env.execute(\"pandas_udf_demo\")\n```\n\n----------------------------------------\n\nTITLE: Stock Price Generation Utility in Java\nDESCRIPTION: Java implementation of stock price generation. Includes a StockPrice class and a SourceFunction implementation that continuously generates stock prices with random fluctuations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nprivate static final ArrayList<String> SYMBOLS = new ArrayList<String>(\n    Arrays.asList(\"SPX\", \"FTSE\", \"DJI\", \"DJT\", \"BUX\", \"DAX\", \"GOOG\"));\n\npublic static class StockPrice implements Serializable {\n\n    public String symbol;\n    public Double price;\n\n    public StockPrice() {\n    }\n\n    public StockPrice(String symbol, Double price) {\n        this.symbol = symbol;\n        this.price = price;\n    }\n\n    @Override\n    public String toString() {\n        return \"StockPrice{\" +\n                \"symbol='\" + symbol + '\\'' +\n                \", count=\" + price +\n                '}';\n    }\n}\n\npublic final static class StockSource implements SourceFunction<StockPrice> {\n\n    private Double price;\n    private String symbol;\n    private Integer sigma;\n\n    public StockSource(String symbol, Integer sigma) {\n        this.symbol = symbol;\n        this.sigma = sigma;\n    }\n\n    @Override\n    public void invoke(Collector<StockPrice> collector) throws Exception {\n        price = DEFAULT_PRICE;\n        Random random = new Random();\n\n        while (true) {\n            price = price + random.nextGaussian() * sigma;\n            collector.collect(new StockPrice(symbol, price));\n            Thread.sleep(random.nextInt(200));\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: SQL QUALIFY Clause Enhancement\nDESCRIPTION: New SQL syntax addition that provides a more concise way to filter outputs of window functions. This enhancement is particularly useful for Top-N and Deduplication operations in Flink SQL.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2025-03-24-release-2.0.0.md#2025-04-08_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n-- Table function calls can now be used without TABLE() wrapper in FROM\n-- Example syntax improvement for window table-valued functions\n```\n\n----------------------------------------\n\nTITLE: Complete Processing Pipeline with Dynamic Keying\nDESCRIPTION: Final implementation of the processing pipeline showing the complete flow with dynamic key selection.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-15-demo-fraud-detection.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nDataStream<Alert> alerts =\n    transactions\n        .process(new DynamicKeyFunction())\n        .keyBy((keyed) -> keyed.getKey());\n        .process(new DynamicAlertFunction())\n```\n\n----------------------------------------\n\nTITLE: Configuring Pulsar Source in Apache Flink (Java)\nDESCRIPTION: Creates and configures a Pulsar source to ingest data into a Flink DataStream. Uses PulsarSourceBuilder to set up connection details like service URL, topic, and subscription name.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-25-query-pulsar-streams-using-apache-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// create and configure Pulsar consumer\nPulsarSourceBuilder<String>builder = PulsarSourceBuilder  \n  .builder(new SimpleStringSchema()) \n  .serviceUrl(serviceUrl)\n  .topic(inputTopic)\n  .subsciptionName(subscription);\nSourceFunction<String> src = builder.build();\n// ingest DataStream with Pulsar consumer\nDataStream<String> words = env.addSource(src);\n```\n\n----------------------------------------\n\nTITLE: Converting DataSet to Table and Performing SQL-like Operations in Scala\nDESCRIPTION: This snippet demonstrates how to convert a DataSet to a Table, perform SQL-like operations using the Table API, and convert the result back to a DataSet in Apache Flink. It showcases filtering, selecting, and aggregating operations on temperature data.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-05-24-stream-sql.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval execEnv = ExecutionEnvironment.getExecutionEnvironment\nval tableEnv = TableEnvironment.getTableEnvironment(execEnv)\n\n// obtain a DataSet from somewhere\nval tempData: DataSet[(String, Long, Double)] =\n\n// convert the DataSet to a Table\nval tempTable: Table = tempData.toTable(tableEnv, 'location, 'time, 'tempF)\n// compute your result\nval avgTempCTable: Table = tempTable\n .where('location.like(\"room%\"))\n .select(\n   ('time / (3600 * 24)) as 'day, \n   'Location as 'room, \n   (('tempF - 32) * 0.556) as 'tempC\n  )\n .groupBy('day, 'room)\n .select('day, 'room, 'tempC.avg as 'avgTempC)\n// convert result Table back into a DataSet and print it\navgTempCTable.toDataSet[Row].print()\n```\n\n----------------------------------------\n\nTITLE: Initializing Input Streams in Flink\nDESCRIPTION: Sets up two input DataStreams for actions and patterns. The Action and Pattern classes are POJOs with userId and action fields for Action, and firstAction and secondAction fields for Pattern.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-26-broadcast-state.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nDataStream<Action> actions = ???\nDataStream<Pattern> patterns = ???\n```\n\n----------------------------------------\n\nTITLE: Initializing Dynamic Data Processing Pipeline in Java\nDESCRIPTION: Basic setup of a Flink pipeline with dynamic key function and alert processing. Shows the high-level structure with transaction stream processing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-03-24-demo-fraud-detection-2.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nDataStream<Alert> alerts =\n    transactions\n        .process(new DynamicKeyFunction())\n        .keyBy((keyed) -> keyed.getKey());\n        .process(new DynamicAlertFunction())\n```\n\n----------------------------------------\n\nTITLE: Running Flink Application in Batch Mode\nDESCRIPTION: Command to execute the same Flink application but in batch execution mode by setting the execution.runtime-mode system property to BATCH. This changes how the application processes data, particularly regarding watermarks and late event handling.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-03-11-batch-execution-mode.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar\n```\n\n----------------------------------------\n\nTITLE: Twitter Stream Processing in Java\nDESCRIPTION: Processing of Twitter streams to extract and count stock symbols. Implements tweet generation and symbol counting over 30-second windows.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nDataStream<String> tweetStream = env.addSource(new TweetSource());\n\nDataStream<String> mentionedSymbols = tweetStream.flatMap(\n    new FlatMapFunction<String, String>() {\n    @Override\n    public void flatMap(String value, Collector<String> out) throws Exception {\n        String[] words = value.split(\" \");\n        for (String word : words) {\n            out.collect(word.toUpperCase());\n        }\n    }\n}).filter(new FilterFunction<String>() {\n    @Override\n    public boolean filter(String value) throws Exception {\n        return SYMBOLS.contains(value);\n    }\n});\n\nDataStream<Count> tweetsPerStock = mentionedSymbols.map(new MapFunction<String, Count>() {\n    @Override\n    public Count map(String value) throws Exception {\n        return new Count(value, 1);\n    }\n}).groupBy(\"symbol\").window(Time.of(30, TimeUnit.SECONDS)).sum(\"count\").flatten();\n```\n\n----------------------------------------\n\nTITLE: Window-Based Aggregations in Java\nDESCRIPTION: Java implementation of window-based aggregations on stock price streams. Creates a 10-second window sliding every 5 seconds and computes minimum price, maximum price per stock, and rolling mean.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n//Define the desired time window\nWindowedDataStream<StockPrice> windowedStream = stockStream\n    .window(Time.of(10, TimeUnit.SECONDS))\n    .every(Time.of(5, TimeUnit.SECONDS));\n\n//Compute some simple statistics on a rolling window\nDataStream<StockPrice> lowest = windowedStream.minBy(\"price\").flatten();\nDataStream<StockPrice> maxByStock = windowedStream.groupBy(\"symbol\")\n    .maxBy(\"price\").flatten();\nDataStream<StockPrice> rollingMean = windowedStream.groupBy(\"symbol\")\n```\n\n----------------------------------------\n\nTITLE: Reading and Transforming CSV Data with Flink Table API\nDESCRIPTION: Demonstrates how to set up a CsvTableSource, register it, and perform basic filtering and transformation operations using Flink's Table API. The example shows environment setup, table source configuration, and conversion to DataStream.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-29-table-sql-api-update.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n// set up execution environment\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nval tEnv = TableEnvironment.getTableEnvironment(env)\n\n// configure table source\nval customerSource = CsvTableSource.builder()\n  .path(\"/path/to/customer_data.csv\")\n  .ignoreFirstLine()\n  .fieldDelimiter(\"|\")\n  .field(\"id\", Types.LONG)\n  .field(\"name\", Types.STRING)\n  .field(\"last_update\", Types.TIMESTAMP)\n  .field(\"prefs\", Types.STRING)\n  .build()\n\n// name your table source\ntEnv.registerTableSource(\"customers\", customerSource)\n\n// define your table program\nval table = tEnv\n  .scan(\"customers\")\n  .filter('name.isNotNull && 'last_update > \"2016-01-01 00:00:00\".toTimestamp)\n  .select('id, 'name.lowerCase(), 'prefs)\n\n// convert it to a data stream\nval ds = table.toDataStream[Row]\n\nds.print()\nenv.execute()\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Flink Kubernetes Operator 1.6.1 using Helm\nDESCRIPTION: Commands to add the Flink Kubernetes Operator 1.6.1 Helm chart to a local registry and install it with the webhook creation disabled. This is the recommended way to try out the new features in the official 1.6.1 release.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-10-27-release-kubernetes-operator-1.6.1.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.6.1 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.6.1/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.6.1/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Implementing Control Methods in CongestionControlRateLimitingStrategy\nDESCRIPTION: This code snippet shows the implementation of control methods in CongestionControlRateLimitingStrategy, which determine when to block new requests and adjust the maximum number of in-flight messages.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-11-25-async-sink-rate-limiting-strategy.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic class CongestionControlRateLimitingStrategy implements RateLimitingStrategy {\n    // ...\n    @Override\n    public void registerCompletedRequest(ResultInfo resultInfo) {\n        // ...\n        if (resultInfo.getFailedMessages() > 0) {\n            maxInFlightMessages = scalingStrategy.scaleDown(maxInFlightMessages);\n        } else {\n            maxInFlightMessages = scalingStrategy.scaleUp(maxInFlightMessages);\n        }\n    }\n    \n    public boolean shouldBlock(RequestInfo requestInfo) {\n        return currentInFlightRequests >= maxInFlightRequests\n                || (currentInFlightMessages + requestInfo.getBatchSize() > maxInFlightMessages);\n    }\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring State TTL in Flink Java API\nDESCRIPTION: Demonstrates how to create a StateTtlConfig object and apply it to a state descriptor to enable TTL for application state in Flink's DataStream API.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-05-17-state-ttl.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.api.common.state.StateTtlConfig;\nimport org.apache.flink.api.common.time.Time;\nimport org.apache.flink.api.common.state.ValueStateDescriptor;\n\nStateTtlConfig ttlConfig = StateTtlConfig\n    .newBuilder(Time.days(7))\n    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)\n    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)\n    .build();\n    \nValueStateDescriptor<Long> lastUserLogin = \n    new ValueStateDescriptor<>(\"lastUserLogin\", Long.class);\n\nlastUserLogin.enableTimeToLive(ttlConfig);\n```\n\n----------------------------------------\n\nTITLE: Exposing StateFun Functions via AWS Lambda in Python\nDESCRIPTION: This code snippet demonstrates how to expose and service StateFun functions through AWS Lambda. It shows the integration between Apache Flink StateFun and AWS Lambda for serverless function execution.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-10-13-stateful-serverless-internals.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Code not provided in the given text, but referenced:\n# https://github.com/tzulitai/statefun-aws-demo/blob/master/app/shopping_cart.py#L49\n```\n\n----------------------------------------\n\nTITLE: Complete PyFlink UDF Example\nDESCRIPTION: A complete example showing how to create, register, and use a Python UDF with file system source and sink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-09-pyflink-udf-support-flink.md#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment, DataTypes\nfrom pyflink.table.descriptors import Schema, OldCsv, FileSystem\nfrom pyflink.table.udf import udf\n\nenv = StreamExecutionEnvironment.get_execution_environment()\nenv.set_parallelism(1)\nt_env = StreamTableEnvironment.create(env)\n\nadd = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())\n\nt_env.register_function(\"add\", add)\n\nt_env.connect(FileSystem().path('/tmp/input')) \\\n    .with_format(OldCsv()\n                 .field('a', DataTypes.BIGINT())\n                 .field('b', DataTypes.BIGINT())) \\\n    .with_schema(Schema()\n                 .field('a', DataTypes.BIGINT())\n                 .field('b', DataTypes.BIGINT())) \\\n    .create_temporary_table('mySource')\n\nt_env.connect(FileSystem().path('/tmp/output')) \\\n    .with_format(OldCsv()\n                 .field('sum', DataTypes.BIGINT())) \\\n    .with_schema(Schema()\n                 .field('sum', DataTypes.BIGINT())) \\\n    .create_temporary_table('mySink')\n\nt_env.from_path('mySource')\\\n    .select(\"add(a, b)\") \\\n    .insert_into('mySink')\n\nt_env.execute(\"tutorial_job\")\n```\n\n----------------------------------------\n\nTITLE: Creating Pulsar Source for Flink Streaming Queries in Java\nDESCRIPTION: This snippet demonstrates how to create a Pulsar source for Flink streaming queries. It sets up properties for connecting to Pulsar, creates a FlinkPulsarSource, and adds it to the StreamExecutionEnvironment.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-25-query-pulsar-streams-using-apache-flink.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nval props = new Properties()\nprops.setProperty(\"service.url\", \"pulsar://...\")\nprops.setProperty(\"admin.url\", \"http://...\")\nprops.setProperty(\"partitionDiscoveryIntervalMillis\", \"5000\")\nprops.setProperty(\"startingOffsets\", \"earliest\")\nprops.setProperty(\"topic\", \"test-source-topic\")\nval source = new FlinkPulsarSource(props)\n// you don't need to provide a type information to addSource since FlinkPulsarSource is ResultTypeQueryable\nval dataStream = env.addSource(source)(null)\n\n// chain operations on dataStream of Row and sink the output\n// end method chaining\n\nenv.execute()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Timed Process Function in Flink\nDESCRIPTION: Java implementation of a KeyedProcessFunction in Flink that registers a processing time timer and handles timer events.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-03-a-guide-for-unit-testing-in-apache-flink.md#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\npublic class MyProcessFunction extends KeyedProcessFunction<String, String, String> {\n  @Override\n  public void processElement(String in, Context context, Collector<String> collector) throws Exception {\n    context.timerService().registerProcessingTimeTimer(50);\n    String out = \"hello \" + in;\n    collector.collect(out);\n  }\n\n  @Override\n  public void onTimer(long timestamp, OnTimerContext ctx, Collector<String> out) throws Exception {\n    out.collect(String.format(\"Timer triggered at timestamp %d\", timestamp));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Python DataStream API with MapFunction in PyFlink\nDESCRIPTION: This code demonstrates how to use the newly introduced Python DataStream API in PyFlink 1.12. It shows how to create a custom MapFunction class that adds 1 to each value in a collection, apply it to a data stream, and print the results.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-12-10-release-1.12.0.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyflink.common.typeinfo import Types\nfrom pyflink.datastream import MapFunction, StreamExecutionEnvironment\n\nclass MyMapFunction(MapFunction):\n\n    def map(self, value):\n        return value + 1\n\n\nenv = StreamExecutionEnvironment.get_execution_environment()\ndata_stream = env.from_collection([1, 2, 3, 4, 5], type_info=Types.INT())\nmapped_stream = data_stream.map(MyMapFunction(), output_type=Types.INT())\nmapped_stream.print()\nenv.execute(\"datastream job\")\n```\n\n----------------------------------------\n\nTITLE: DataCenterFun Health Tracking Implementation\nDESCRIPTION: Implementation of DataCenterFun showing how it tracks rack health status and responds to health status queries.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-18-statefun.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nprivate final PersistedValue<Map<RackId, RackAlert>> unhealthyRacks =\n    context.state(\"unhealthy-racks\",\n        TypeName.typeNameOf(new TypeHint<Map<RackId, RackAlert>>(){}));\n```\n\n----------------------------------------\n\nTITLE: Dynamic State Registration with PersistedStateRegistry\nDESCRIPTION: Implementation showing how to use the new PersistedStateRegistry construct for dynamically registering state at runtime. This allows functions to create state constructs when needed rather than defining them eagerly in the class.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-09-28-release-statefun-2.2.0.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic class MyFunction implements StatefulFunction {\n    @Persisted\n    private final PersistedStateRegistry registry = new PersistedStateRegistry();\n    private final PersistedValue<String> myValue;\n\n    public void invoke(Context context, Object input) {\n        if (myValue == null) {\n            myValue = registry.registerValue(PersistedValue.of(\"my-value\", String.class));\n        }\n        ...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Custom Counting Window in Python DataStream API\nDESCRIPTION: This code snippet demonstrates how to implement a custom counting window using state in the Python DataStream API. It uses a FlatMapFunction to maintain a running sum and count, emitting an average when the count reaches the window size.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass CountWindowAverage(FlatMapFunction):\n    def __init__(self, window_size):\n        self.window_size = window_size\n\n    def open(self, runtime_context: RuntimeContext):\n        descriptor = ValueStateDescriptor(\"average\", Types.TUPLE([Types.LONG(), Types.LONG()]))\n        self.sum = runtime_context.get_state(descriptor)\n\n    def flat_map(self, value):\n        current_sum = self.sum.value()\n        if current_sum is None:\n            current_sum = (0, 0)\n        # update the count\n        current_sum = (current_sum[0] + 1, current_sum[1] + value[1])\n        # if the count reaches window_size, emit the average and clear the state\n        if current_sum[0] >= self.window_size:\n            self.sum.clear()\n            yield value[0], current_sum[1] // current_sum[0]\n        else:\n            self.sum.update(current_sum)\n\nds = ...  # type: DataStream\nds.key_by(lambda row: row[0]) \\\n  .flat_map(CountWindowAverage(5))\n```\n\n----------------------------------------\n\nTITLE: ValueState Workload Benchmarks Table - Primary Results\nDESCRIPTION: This table compares key metrics between Changelog Disabled and Enabled configurations, showing improvements in checkpoint duration and frequency at the cost of increased space usage and recovery time.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-30-changelog-state-backend.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<table border=\"1\">\n  <thead>\n    <tr>\n      <th style=\"padding: 5px\">&nbsp;</th>\n      <th style=\"padding: 5px\">Changelog Disabled</th>\n      <th style=\"padding: 5px\">Changelog Enabled</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"padding: 5px\">Records processed</td>\n      <td style=\"padding: 5px\">3,808,629,616</td>\n      <td style=\"padding: 5px\">3,810,508,130</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 5px\">Checkpoints made</td>\n      <td style=\"padding: 5px\">10,023</td>\n      <td style=\"padding: 5px\">108,649</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 5px\">Checkpoint duration, 90%</td>\n      <td style=\"padding: 5px\">6s</td>\n      <td style=\"padding: 5px\">664ms</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 5px\">Checkpoint duration, 99.9%</td>\n      <td style=\"padding: 5px\">10s</td>\n      <td style=\"padding: 5px\">1s</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 5px\">Full checkpoint size *, 99%</td>\n      <td style=\"padding: 5px\">19.6GB</td>\n      <td style=\"padding: 5px\">25.6GB</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 5px\">Recovery time (local recovery disabled)</td>\n      <td style=\"padding: 5px\">20-21s</td>\n      <td style=\"padding: 5px\">35-65s (depending on the checkpoint)</td>\n    </tr>\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Writing Flink Streaming Table to Pulsar in Java\nDESCRIPTION: This code demonstrates how to write a Flink streaming table to Pulsar. It sets up properties for connecting to Pulsar, creates a StreamTableEnvironment, registers a Pulsar table sink, and executes an SQL insert statement.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-25-query-pulsar-streams-using-apache-flink.md#2025-04-08_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nval tEnv = StreamTableEnvironment.create(env)\n\nval prop = new Properties()\nprop.setProperty(\"service.url\", serviceUrl)\nprop.setProperty(\"admin.url\", adminUrl)\nprop.setProperty(\"flushOnCheckpoint\", \"true\")\nprop.setProperty(\"failOnWrite\", \"true\")\nprops.setProperty(\"topic\", \"test-sink-topic\")\n\ntEnv\n  .connect(new Pulsar().properties(props))\n  .inAppendMode()\n  .registerTableSource(\"sink-table\")\n\nval sql = \"INSERT INTO sink-table .....\"\ntEnv.sqlUpdate(sql)\nenv.execute()\n```\n\n----------------------------------------\n\nTITLE: Unit Testing a Stateless Map Function in Flink\nDESCRIPTION: Java unit test for the MyStatelessMap function. It verifies that the function correctly prepends 'hello' to the input string.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-03-a-guide-for-unit-testing-in-apache-flink.md#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n@Test\npublic void testMap() throws Exception {\n  MyStatelessMap statelessMap = new MyStatelessMap();\n  String out = statelessMap.map(\"world\");\n  Assert.assertEquals(\"hello world\", out);\n}\n```\n\n----------------------------------------\n\nTITLE: Inserting Top Category Statistics into Elasticsearch in Flink SQL\nDESCRIPTION: This SQL query calculates and inserts top category statistics into the Elasticsearch table. It groups the enriched user behavior data by category name and counts 'buy' events.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO top_category\nSELECT category_name, COUNT(*) buy_cnt\nFROM rich_user_behavior\nWHERE behavior = 'buy'\nGROUP BY category_name;\n```\n\n----------------------------------------\n\nTITLE: Creating Tables with Watermark Strategy in Flink SQL DDL\nDESCRIPTION: This SQL snippet demonstrates how to create a table with a watermark strategy using Flink SQL DDL. The syntax allows defining time attributes and watermark generation for time-based operations like windowing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-11-release-1.10.0.md#2025-04-08_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE table_name (\n\n  WATERMARK FOR columnName AS <watermark_strategy_expression>\n\n) WITH (\n  ...\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Incremental Checkpointing in Scala\nDESCRIPTION: This code snippet demonstrates how to enable incremental checkpointing in a Scala Flink application using the RocksDBStateBackend. The second parameter in the constructor is set to true to enable incremental checkpointing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-01-30-incremental-checkpointing.md#2025-04-08_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment()\nenv.setStateBackend(new RocksDBStateBackend(filebackend, true))\n```\n\n----------------------------------------\n\nTITLE: Implementing IMAP Table Source Factory in Java\nDESCRIPTION: Sets up the ImapTableSourceFactory class with configuration options for host, port, user, and password. It includes methods for specifying required and optional options, as well as creating the dynamic table source.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.configuration.ConfigOption;\nimport org.apache.flink.configuration.ConfigOptions;\nimport org.apache.flink.table.factories.DynamicTableSourceFactory;\n\nimport java.util.HashSet;\nimport java.util.Set;\n\npublic class ImapTableSourceFactory implements DynamicTableSourceFactory {\n\n    public static final ConfigOption<String> HOST = ConfigOptions.key(\"host\").stringType().noDefaultValue();\n    public static final ConfigOption<Integer> PORT = ConfigOptions.key(\"port\").intType().noDefaultValue();\n    public static final ConfigOption<String> USER = ConfigOptions.key(\"user\").stringType().noDefaultValue();\n    public static final ConfigOption<String> PASSWORD = ConfigOptions.key(\"password\").stringType().noDefaultValue();\n\n    // â€¦\n\n    @Override\n    public Set<ConfigOption<?>> requiredOptions() {\n        final Set<ConfigOption<?>> options = new HashSet<>();\n        options.add(HOST);\n        options.add(USER);\n        options.add(PASSWORD);\n        return options;\n    }\n\n    @Override\n    public Set<ConfigOption<?>> optionalOptions() {\n        final Set<ConfigOption<?>> options = new HashSet<>();\n        options.add(PORT);\n        return options;\n    }\n    // â€¦\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Value Lowercase Function in Python UDF\nDESCRIPTION: Python UDF that takes a JSON string input, converts a specific field 'a' to lowercase, and returns the modified JSON string. Uses the json module for parsing and serialization.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-06-pyflink-1.15-thread-mode.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@udf(result_type=DataTypes.STRING(), func_type=\"general\")\ndef json_value_lower(s: str):\n    import json\n    a = json.loads(s)\n    a['a'] = a['a'].lower()\n    return json.dumps(a)\n```\n\n----------------------------------------\n\nTITLE: Creating Elasticsearch Table for Cumulative UV in Flink SQL\nDESCRIPTION: This SQL snippet creates an Elasticsearch table to store cumulative unique visitor data. It defines columns for date, time, and UV count, with date and time as the primary key for upsert operations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE cumulative_uv (\n    date_str STRING,\n    time_str STRING,\n    uv BIGINT,\n    PRIMARY KEY (date_str, time_str) NOT ENFORCED\n) WITH (\n    'connector' = 'elasticsearch-7',\n    'hosts' = 'http://elasticsearch:9200',\n    'index' = 'cumulative_uv'\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing MNIST Classifier with GPU acceleration in Flink\nDESCRIPTION: Java implementation of a Flink RichMapFunction that uses JCuda to accelerate MNIST image classification on GPU. The code shows how to access GPU information from the external resource framework and perform matrix operations using CUDA.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-06-external-resource.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass MNISTClassifier extends RichMapFunction<List<Float>, Integer> {\n    @Override\n    public void open(Configuration parameters) {\n        // Get the GPU information and select the first GPU.\n        final Set<ExternalResourceInfo> externalResourceInfos = getRuntimeContext().getExternalResourceInfos(resourceName);\n        final Optional<String> firstIndexOptional = externalResourceInfos.iterator().next().getProperty(\"index\");\n\n        // Initialize JCublas with the selected GPU\n        JCuda.cudaSetDevice(Integer.parseInt(firstIndexOptional.get()));\n        JCublas.cublasInit();\n    }\n\n   @Override\n   public Integer map(List<Float> value) {\n       // Performs multiplication using JCublas. The matrixPointer points to our pre-trained model.\n       JCublas.cublasSgemv('n', DIMENSIONS.f1, DIMENSIONS.f0, 1.0f,\n               matrixPointer, DIMENSIONS.f1, inputPointer, 1, 0.0f, outputPointer, 1);\n\n       // Read the result back from GPU.\n       JCublas.cublasGetVector(DIMENSIONS.f1, Sizeof.FLOAT, outputPointer, 1, Pointer.to(output), 1);\n       int result = 0;\n       for (int i = 0; i < DIMENSIONS.f1; ++i) {\n           result = output[i] > output[result] ? i : result;\n       }\n       return result;\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Kafka Stream Processing with Flink SQL in Scala\nDESCRIPTION: Example showing how to read from a JSON-encoded Kafka topic, process the data using SQL queries, and write results back to Kafka. Uses TableEnvironment for SQL execution and demonstrates stream transformation with temperature conversion.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-05-24-stream-sql.md#2025-04-08_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n// get environments\nval execEnv = StreamExecutionEnvironment.getExecutionEnvironment\nval tableEnv = TableEnvironment.getTableEnvironment(execEnv)\n\n// configure Kafka connection\nval kafkaProps = ...\n// define a JSON encoded Kafka topic as external table\nval sensorSource = new KafkaJsonSource[(String, Long, Double)](\n    \"sensorTopic\",\n    kafkaProps,\n    (\"location\", \"time\", \"tempF\"))\n\n// register external table\ntableEnv.registerTableSource(\"sensorData\", sensorSource)\n\n// define query in external table\nval roomSensors: Table = tableEnv.sql(\n    \"SELECT STREAM time, location AS room, (tempF - 32) * 0.556 AS tempC \" +\n    \"FROM sensorData \" +\n    \"WHERE location LIKE 'room%'\"\n  )\n\n// define a JSON encoded Kafka topic as external sink\nval roomSensorSink = new KafkaJsonSink(...)\n\n// define sink for room sensor data and execute query\nroomSensors.toSink(roomSensorSink)\nexecEnv.execute()\n```\n\n----------------------------------------\n\nTITLE: Detecting User Communities with Label Propagation in Flink Gelly\nDESCRIPTION: Uses the Label Propagation algorithm to detect communities in a user-user graph. First initializes each vertex with a unique numeric label, then runs the algorithm to update vertex values based on the most frequent labels among their neighbors.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-08-24-introducing-flink-gelly.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\n// detect user communities using label propagation\n// initialize each vertex with a unique numeric label\nDataSet<Tuple2<String, Long>> idsWithInitialLabels = DataSetUtils\n        .zipWithUniqueId(similarUsersGraph.getVertexIds())\n        .map(new MapFunction<Tuple2<Long, String>, Tuple2<String, Long>>() {\n                @Override\n                public Tuple2<String, Long> map(Tuple2<Long, String> tuple2) throws Exception {\n                    return new Tuple2<String, Long>(tuple2.f1, tuple2.f0);\n                }\n        });\n\n// update the vertex values and run the label propagation algorithm\nDataSet<Vertex> verticesWithCommunity = similarUsersGraph\n        .joinWithVertices(idsWithlLabels, new MapFunction() {\n                public Long map(Tuple2 idWithLabel) {\n                    return idWithLabel.f1;\n                }\n        })\n        .run(new LabelPropagation(numIterations))\n        .getVertices();\n```\n\n----------------------------------------\n\nTITLE: Implementing CongestionControlRateLimitingStrategy in Java\nDESCRIPTION: This code snippet demonstrates the implementation of CongestionControlRateLimitingStrategy, which adjusts the request rate based on the success or failure of previous requests.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-11-25-async-sink-rate-limiting-strategy.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class CongestionControlRateLimitingStrategy implements RateLimitingStrategy {\n    // ...\n    @Override\n    public void registerInFlightRequest(RequestInfo requestInfo) {\n        currentInFlightRequests++;\n        currentInFlightMessages += requestInfo.getBatchSize();\n    }\n    \n    @Override\n    public void registerCompletedRequest(ResultInfo resultInfo) {\n        currentInFlightRequests = Math.max(0, currentInFlightRequests - 1);\n        currentInFlightMessages = Math.max(0, currentInFlightMessages - resultInfo.getBatchSize());\n        \n        if (resultInfo.getFailedMessages() > 0) {\n            maxInFlightMessages = scalingStrategy.scaleDown(maxInFlightMessages);\n        } else {\n            maxInFlightMessages = scalingStrategy.scaleUp(maxInFlightMessages);\n        }\n    }\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Pravega Stream Reader\nDESCRIPTION: Implementation of FlinkPravegaReader as a Flink SourceFunction for parallel reading from Pravega streams, including checkpoint configuration and stream definition.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-20-pravega-connector-101.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n// Enable Flink checkpoint to make state fault tolerant\nenv.enableCheckpointing(60000);\n\n// Define the Pravega configuration\nParameterTool params = ParameterTool.fromArgs(args);\nPravegaConfig config = PravegaConfig.fromParams(params);\n\n// Define the event deserializer\nDeserializationSchema<MyClass> deserializer = ...\n\n// Define the data stream\nFlinkPravegaReader<MyClass> pravegaSource = FlinkPravegaReader.<MyClass>builder()\n    .forStream(...)\n    .withPravegaConfig(config)\n    .withDeserializationSchema(deserializer)\n    .build();\nDataStream<MyClass> stream = env.addSource(pravegaSource)\n    .setParallelism(4)\n    .uid(\"pravega-source\");\n```\n\n----------------------------------------\n\nTITLE: Defining Monitoring Event Classes in Java\nDESCRIPTION: Class definitions for monitoring events including a base MonitoringEvent class and specific event types for temperature and power monitoring.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic abstract class MonitoringEvent {\n    private int rackID;\n    ...\n}\n\npublic class TemperatureEvent extends MonitoringEvent {\n    private double temperature;\n    ...\n}\n\npublic class PowerEvent extends MonitoringEvent {\n    private double voltage;\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing WordCount in Flink using Hadoop Components\nDESCRIPTION: This code snippet demonstrates a Flink WordCount program that uses Hadoop InputFormat, OutputFormat, Mapper, and Reducer functions. It showcases how to wrap Hadoop components within a Flink program using the Hadoop Compatibility package.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2014-11-18-hadoop-compatibility.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Definition of Hadoop Mapper function\npublic class Tokenizer implements Mapper<LongWritable, Text, Text, LongWritable> { ... }\n// Definition of Hadoop Reducer function\npublic class Counter implements Reducer<Text, LongWritable, Text, LongWritable> { ... }\n\npublic static void main(String[] args) {\n  final String inputPath = args[0];\n  final String outputPath = args[1];\n\n  final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n        \n  // Setup Hadoop's TextInputFormat\n  HadoopInputFormat<LongWritable, Text> hadoopInputFormat = \n      new HadoopInputFormat<LongWritable, Text>(\n        new TextInputFormat(), LongWritable.class, Text.class, new JobConf());\n  TextInputFormat.addInputPath(hadoopInputFormat.getJobConf(), new Path(inputPath));\n  \n  // Read a DataSet with the Hadoop InputFormat\n  DataSet<Tuple2<LongWritable, Text>> text = env.createInput(hadoopInputFormat);\n  DataSet<Tuple2<Text, LongWritable>> words = text\n    // Wrap Tokenizer Mapper function\n    .flatMap(new HadoopMapFunction<LongWritable, Text, Text, LongWritable>(new Tokenizer()))\n    .groupBy(0)\n    // Wrap Counter Reducer function (used as Reducer and Combiner)\n    .reduceGroup(new HadoopReduceCombineFunction<Text, LongWritable, Text, LongWritable>(\n      new Counter(), new Counter()));\n        \n  // Setup Hadoop's TextOutputFormat\n  HadoopOutputFormat<Text, LongWritable> hadoopOutputFormat = \n    new HadoopOutputFormat<Text, LongWritable>(\n      new TextOutputFormat<Text, LongWritable>(), new JobConf());\n  hadoopOutputFormat.getJobConf().set(\"mapred.textoutputformat.separator\", \" \");\n  TextOutputFormat.setOutputPath(hadoopOutputFormat.getJobConf(), new Path(outputPath));\n        \n  // Output & Execute\n  words.output(hadoopOutputFormat);\n  env.execute(\"Hadoop Compat WordCount\");\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Complete Temperature Warning Pattern\nDESCRIPTION: Full pattern definition for detecting consecutive high-temperature events within a time window.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nPattern<MonitoringEvent, ?> warningPattern = Pattern.<MonitoringEvent>begin(\"First Event\")\n    .subtype(TemperatureEvent.class)\n    .where(evt -> evt.getTemperature() >= TEMPERATURE_THRESHOLD)\n    .next(\"Second Event\")\n    .subtype(TemperatureEvent.class)\n    .where(evt -> evt.getTemperature() >= TEMPERATURE_THRESHOLD)\n    .within(Time.seconds(10));\n```\n\n----------------------------------------\n\nTITLE: Using Hive SQL Dialect in Flink\nDESCRIPTION: This SQL snippet demonstrates how to enable and use the Hive SQL dialect in Flink 1.13. It shows the setup of a Hive catalog, loading the Hive module, enabling the Hive dialect, and running a Hive-specific CLUSTER BY query.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE CATALOG myhive WITH ('type' = 'hive'); -- setup HiveCatalog\nUSE CATALOG myhive;\nLOAD MODULE hive; -- setup HiveModule\nUSE MODULES hive,core;\nSET table.sql-dialect = hive; -- enable Hive dialect\nSELECT key, value FROM src CLUSTER BY key; -- run some Hive queries\n```\n\n----------------------------------------\n\nTITLE: Stock Price Generation Utility in Scala\nDESCRIPTION: Defines a StockPrice case class and a generator function that creates a continuous stream of simulated stock prices with random fluctuations for a given symbol.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval symbols = List(\"SPX\", \"FTSE\", \"DJI\", \"DJT\", \"BUX\", \"DAX\", \"GOOG\")\n\ncase class StockPrice(symbol: String, price: Double)\n\ndef generateStock(symbol: String)(sigma: Int)(out: Collector[StockPrice]) = {\n  var price = 1000.\n  while (true) {\n    price = price + Random.nextGaussian * sigma\n    out.collect(StockPrice(symbol, price))\n    Thread.sleep(Random.nextInt(200))\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Pattern Stream in Flink\nDESCRIPTION: Applies the broadcast() transformation on the patterns stream using the MapStateDescriptor, creating a BroadcastStream.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-26-broadcast-state.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nBroadcastStream<Pattern> bcedPatterns = patterns.broadcast(bcStateDescriptor);\n```\n\n----------------------------------------\n\nTITLE: Creating Pulsar Sink for Flink Streaming Queries in Java\nDESCRIPTION: This snippet illustrates how to create a Pulsar sink for Flink streaming queries. It sets up properties for connecting to Pulsar and adds a FlinkPulsarSink to a stream.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-25-query-pulsar-streams-using-apache-flink.md#2025-04-08_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nval stream = .....\n\nval prop = new Properties()\nprop.setProperty(\"service.url\", serviceUrl)\nprop.setProperty(\"admin.url\", adminUrl)\nprop.setProperty(\"flushOnCheckpoint\", \"true\")\nprop.setProperty(\"failOnWrite\", \"true\")\nprops.setProperty(\"topic\", \"test-sink-topic\")\n\nstream.addSink(new FlinkPulsarSink(prop, DummyTopicKeyExtractor))\nenv.execute()\n```\n\n----------------------------------------\n\nTITLE: Migrating FlinkKinesisConsumer to KinesisStreamsSource in Java\nDESCRIPTION: Example showing how to migrate from the legacy FlinkKinesisConsumer to the new KinesisStreamsSource implementation. Demonstrates configuration setup, stream initialization, and watermark assignment for both old and new approaches.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-11-25-whats-new-aws-connectors-5.0.0.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n// Old FlinkKinesisConsumer to read from stream test-stream from TRIM_HORIZON\nProperties consumerConfig = new Properties();\nconsumerConfig.put(AWSConfigConstants.AWS_REGION, \"us-east-1\");\nconsumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \"TRIM_HORIZON\");\nFlinkKinesisConsumer<String> oldKinesisConsumer = \n    new FlinkKinesisConsumer<>(\"test-stream\", new SimpleStringSchema(), consumerConfig);\nDataStream<String> kinesisRecordsFromOldKinesisConsumer = env.addSource(oldKinesisConsumer)\n    .uid(\"custom-uid\")\n    .assignTimestampsAndWatermarks(\n        WatermarkStrategy.<String>forMonotonousTimestamps().withIdleness(Duration.ofSeconds(1)))\n\n// New KinesisStreamsSource to read from stream test-stream from TRIM_HORIZON\nConfiguration sourceConfig = new Configuration();\nsourceConfig.set(KinesisSourceConfigOptions.STREAM_INITIAL_POSITION, KinesisSourceConfigOptions.InitialPosition.TRIM_HORIZON); \nKinesisStreamsSource<String> newKdsSource =\n    KinesisStreamsSource.<String>builder()\n        .setStreamArn(\"arn:aws:kinesis:us-east-1:123456789012:stream/test-stream\")\n        .setSourceConfig(sourceConfig)\n        .setDeserializationSchema(new SimpleStringSchema())\n        .build();\nDataStream<String> kinesisRecordsWithEventTimeWatermarks = env.fromSource(\n    kdsSource, \n    WatermarkStrategy.<String>forMonotonousTimestamps().withIdleness(Duration.ofSeconds(1)), \n    \"Kinesis source\")\n        .returns(TypeInformation.of(String.class))\n        .uid(\"custom-uid\");\n```\n\n----------------------------------------\n\nTITLE: Implementing Window Word Count with Event Time in Flink DataStream API\nDESCRIPTION: A Flink application that counts the number of orders by currency in 1-day tumbling windows. It demonstrates how late data handling differs between batch and streaming execution modes. The program uses the unified file source to read input data and applies window aggregation with side output for late arriving records.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-03-11-batch-execution-mode.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic class WindowWordCount {\n\tprivate static final OutputTag<String[]> LATE_DATA = new OutputTag<>(\n\t\t\"late-data\",\n\t\tBasicArrayTypeInfo.STRING_ARRAY_TYPE_INFO);\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tParameterTool config = ParameterTool.fromArgs(args);\n\n\t\tPath path = new Path(config.get(\"path\"));\n\t\tSingleOutputStreamOperator<Tuple4<String, Integer, String, String>> dataStream = env\n\t\t\t.fromSource(\n\t\t\t\tFileSource.forRecordStreamFormat(new TsvFormat(), path).build(),\n\t\t\t\tWatermarkStrategy.<String[]>forBoundedOutOfOrderness(Duration.ofDays(1))\n\t\t\t\t\t.withTimestampAssigner(new OrderTimestampAssigner()),\n\t\t\t\t\"Text file\"\n\t\t\t)\n\t\t\t.keyBy(value -> value[4]) // group by currency\n\t\t\t.window(TumblingEventTimeWindows.of(Time.days(1)))\n\t\t\t.sideOutputLateData(LATE_DATA)\n\t\t\t.aggregate(\n\t\t\t\tnew CountFunction(), // count number of orders in a given currency\n\t\t\t\tnew CombineWindow());\n\n\t\tint i = 0;\n\t\tDataStream<String[]> lateData = dataStream.getSideOutput(LATE_DATA);\n\t\ttry (CloseableIterator<String[]> results = lateData.executeAndCollect()) {\n\t\t\twhile (results.hasNext()) {\n\t\t\t\tString[] late = results.next();\n\t\t\t\tif (i < 100) {\n\t\t\t\t\tSystem.out.println(Arrays.toString(late));\n\t\t\t\t}\n\t\t\t\ti++;\n\t\t\t}\n\t\t}\n\t\tSystem.out.println(\"Number of late records: \" + i);\n\n\t\ttry (CloseableIterator<Tuple4<String, Integer, String, String>> results \n\t\t\t\t= dataStream.executeAndCollect()) {\n\t\t\twhile (results.hasNext()) {\n\t\t\t\tSystem.out.println(results.next());\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing IMAP Table Source in Java\nDESCRIPTION: Creates the ImapTableSource class implementing ScanTableSource, which will be used to create the runtime provider for the IMAP source. It includes methods for creating a copy and getting the scan runtime provider.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.table.connector.source.DynamicTableSource;\nimport org.apache.flink.table.connector.source.ScanTableSource;\nimport java.util.List;\n\npublic class ImapTableSource implements ScanTableSource {\n\n    private final ImapSourceOptions options;\n    private final List<String> columnNames;\n\n    public ImapTableSource(\n        ImapSourceOptions options,\n        List<String> columnNames\n    ) {\n        this.options = options;\n        this.columnNames = columnNames;\n    }\n\n    // â€¦\n\n    @Override\n    public ScanRuntimeProvider getScanRuntimeProvider(ScanContext ctx) {\n        final boolean bounded = false;\n        final ImapSource source = new ImapSource(options, columnNames);\n        return SourceFunctionProvider.of(source, bounded);\n    }\n\n    @Override\n    public DynamicTableSource copy() {\n        return new ImapTableSource(options, columnNames);\n    }\n\n    // â€¦\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Pravega Stream Writer\nDESCRIPTION: Implementation of FlinkPravegaWriter as a Flink SinkFunction for parallel writing to Pravega streams, supporting different writer modes including exactly-once semantics.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-20-pravega-connector-101.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n// Define the Pravega configuration\nPravegaConfig config = PravegaConfig.fromParams(params);\n\n// Define the event serializer\nSerializationSchema<MyClass> serializer = ...\n\n// Define the event router for selecting the Routing Key\nPravegaEventRouter<MyClass> router = ...\n\n// Define the sink function\nFlinkPravegaWriter<MyClass> pravegaSink = FlinkPravegaWriter.<MyClass>builder()\n   .forStream(...)\n   .withPravegaConfig(config)\n   .withSerializationSchema(serializer)\n   .withEventRouter(router)\n   .withWriterMode(EXACTLY_ONCE)\n   .build();\n\nDataStream<MyClass> stream = ...\nstream.addSink(pravegaSink)\n    .setParallelism(4)\n    .uid(\"pravega-sink\");\n```\n\n----------------------------------------\n\nTITLE: Reading Multiple Stock Price Streams in Java\nDESCRIPTION: Java implementation for creating a stock price stream by reading from a socket and generating additional stock sources. Maps the input to StockPrice objects and merges multiple streams.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic static void main(String[] args) throws Exception {\n\n    final StreamExecutionEnvironment env =\n        StreamExecutionEnvironment.getExecutionEnvironment();\n\n    //Read from a socket stream at map it to StockPrice objects\n    DataStream<StockPrice> socketStockStream = env\n            .socketTextStream(\"localhost\", 9999)\n            .map(new MapFunction<String, StockPrice>() {\n                private String[] tokens;\n\n                @Override\n                public StockPrice map(String value) throws Exception {\n                    tokens = value.split(\",\");\n                    return new StockPrice(tokens[0],\n                        Double.parseDouble(tokens[1]));\n                }\n            });\n\n    //Generate other stock streams\n    DataStream<StockPrice> SPX_stream = env.addSource(new StockSource(\"SPX\", 10));\n    DataStream<StockPrice> FTSE_stream = env.addSource(new StockSource(\"FTSE\", 20));\n    DataStream<StockPrice> DJI_stream = env.addSource(new StockSource(\"DJI\", 30));\n    DataStream<StockPrice> BUX_stream = env.addSource(new StockSource(\"BUX\", 40));\n\n    //Merge all stock streams together\n    DataStream<StockPrice> stockStream = socketStockStream\n        .merge(SPX_stream, FTSE_stream, DJI_stream, BUX_stream);\n\n    stockStream.print();\n\n    env.execute(\"Stock stream\");\n    \n```\n\n----------------------------------------\n\nTITLE: Implementing Email Collection Logic in Java\nDESCRIPTION: Implementation of message collection logic with Jakarta Mail's MessageCountListener to process incoming emails.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_10\n\nLANGUAGE: java\nCODE:\n```\nimport jakarta.mail.*;\nimport jakarta.mail.event.MessageCountAdapter;\nimport jakarta.mail.event.MessageCountEvent;\nimport org.apache.flink.streaming.api.functions.source.RichSourceFunction;\nimport org.apache.flink.table.data.GenericRowData;\nimport org.apache.flink.table.data.StringData;\nimport org.apache.flink.table.data.RowData;\n\npublic class ImapSource extends RichSourceFunction<RowData> {\n    @Override\n    public void run(SourceFunction.SourceContext<RowData> ctx) throws Exception {\n        // â€¦\n\n        folder.addMessageCountListener(new MessageCountAdapter() {\n            @Override\n            public void messagesAdded(MessageCountEvent e) {\n                collectMessages(ctx, e.getMessages());\n            }\n        });\n\n        // â€¦\n    }\n\n    private void collectMessages(SourceFunction.SourceContext<RowData> ctx, Message[] messages) {\n        for (Message message : messages) {\n            try {\n                ctx.collect(GenericRowData.of(StringData.fromString(message.getSubject())));\n            } catch (MessagingException ignored) {}\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Unit Testing a Stateful FlatMap Function in Flink\nDESCRIPTION: Java unit test for the StatefulFlatMap function. It uses KeyedOneInputStreamOperatorTestHarness to simulate the runtime environment and test state management.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-03-a-guide-for-unit-testing-in-apache-flink.md#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n@Test\npublic void testFlatMap() throws Exception{\n  StatefulFlatMap statefulFlatMap = new StatefulFlatMap();\n\n  OneInputStreamOperatorTestHarness<String, String> testHarness = \n    new KeyedOneInputStreamOperatorTestHarness<>(\n      new StreamFlatMap<>(statefulFlatMap), x -> \"1\", Types.STRING);\n  testHarness.open();\n\n  testHarness.processElement(\"world\", 10);\n  ValueState<String> previousInput = \n    statefulFlatMap.getRuntimeContext().getState(\n      new ValueStateDescriptor<>(\"previousInput\", Types.STRING));\n  String stateValue = previousInput.value();\n  Assert.assertEquals(\n    Lists.newArrayList(new StreamRecord<>(\"hello world\", 10)), \n    testHarness.extractOutputStreamRecords());\n  Assert.assertEquals(\"world\", stateValue);\n\n  testHarness.processElement(\"parallel\", 20);\n  Assert.assertEquals(\n    Lists.newArrayList(\n      new StreamRecord<>(\"hello world\", 10), \n      new StreamRecord<>(\"hello parallel world\", 20)), \n    testHarness.extractOutputStreamRecords());\n  Assert.assertEquals(\"parallel\", previousInput.value());\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom RateLimitingStrategy in AsyncSinkWriter for Apache Flink\nDESCRIPTION: This code snippet shows how to specify a custom RateLimitingStrategy in the AsyncSinkWriterConfiguration, which is passed into the constructor of the AsyncSinkWriter. It demonstrates the use of the TokenBucketRateLimitingStrategy in a custom sink writer.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-11-25-async-sink-rate-limiting-strategy.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nclass MyCustomSinkWriter<InputT> extends AsyncSinkWriter<InputT, MyCustomRequestEntry> {\n\n    MyCustomSinkWriter(\n        ElementConverter<InputT, MyCustomRequestEntry> elementConverter,\n        Sink.InitContext context,\n        Collection<BufferedRequestState<MyCustomRequestEntry>> states) {\n            super(\n                elementConverter,\n                context,\n                AsyncSinkWriterConfiguration.builder()\n                    // ...\n                    .setRateLimitingStrategy(new TokenBucketRateLimitingStrategy())\n                    .build(),\n                states);\n    }\n    \n}\n```\n\n----------------------------------------\n\nTITLE: Finding Top Song per User with GroupReduceOnEdges in Flink Gelly\nDESCRIPTION: Retrieves the most listened song for each user by using groupReduceOnEdges to aggregate over the first hop neighborhood. It iterates through edges and collects the target song with the maximum weight for each user.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-08-24-introducing-flink-gelly.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n//get the top track (most listened to) for each user\nDataSet<Tuple2> usersWithTopTrack = userSongGraph\n        .groupReduceOnEdges(new GetTopSongPerUser(), EdgeDirection.OUT);\n\nclass GetTopSongPerUser implements EdgesFunctionWithVertexValue {\n    void iterateEdges(Vertex vertex, Iterable<Edge> edges) {\n        int maxPlaycount = 0;\n        String topSong = \"\";\n\n        for (Edge edge : edges) {\n            if (edge.getValue() > maxPlaycount) {\n                maxPlaycount = edge.getValue();\n                topSong = edge.getTarget();\n            }\n        }\n        return new Tuple2(vertex.getId(), topSong);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Flink Application in Streaming Mode\nDESCRIPTION: Command to execute the WindowWordCount application using the default streaming execution mode. This is the standard way to run Flink applications without explicitly specifying the execution mode.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-03-11-batch-execution-mode.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/flink run examples/streaming/WindowWordCount.jar\n```\n\n----------------------------------------\n\nTITLE: Enabling Incremental Checkpointing in Java\nDESCRIPTION: This code snippet shows how to enable incremental checkpointing in a Java Flink application using the RocksDBStateBackend. The second parameter in the constructor is set to true to enable incremental checkpointing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-01-30-incremental-checkpointing.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setStateBackend(new RocksDBStateBackend(filebackend, true));\n```\n\n----------------------------------------\n\nTITLE: Setting RocksDB State Backend in Java Flink Application\nDESCRIPTION: This Java code snippet demonstrates how to set RocksDB as the state backend at the job level in a Flink application.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-18-rocksdb.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n# 'env' is the created StreamExecutionEnvironment\n# 'true' is to enable incremental checkpointing\nenv.setStateBackend(new RocksDBStateBackend(\"hdfs:///fink-checkpoints\", true));\n```\n\n----------------------------------------\n\nTITLE: Using Pulsar as Batch Sink in Flink\nDESCRIPTION: This code example shows how to use Apache Pulsar as a batch sink in Apache Flink. It demonstrates creating a PulsarOutputFormat instance and writing a DataSet to Pulsar after Flink has completed computation on a static data set.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-05-03-pulsar-flink.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// obtain DataSet from arbitrary computation\nDataSet<WordWithCount> wc = ...\n\n// create PulsarOutputFormat instance\nOutputFormat pulsarOutputFormat = new PulsarOutputFormat(\n   serviceUrl, \n   topic, \n   new AuthenticationDisabled(), \n   wordWithCount -> wordWithCount.toString().getBytes());\n// write DataSet to Pulsar\nwc.output(pulsarOutputFormat);\n```\n\n----------------------------------------\n\nTITLE: Processing Flink DataStream (Java)\nDESCRIPTION: Demonstrates processing logic on a Flink DataStream, performing a simple word count operation using flatMap, keyBy, timeWindow, and reduce functions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-25-query-pulsar-streams-using-apache-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// perform computation on DataStream (here a simple WordCount)\nDataStream<WordWithCount> wc = words\n  .flatmap((FlatMapFunction<String, WordWithCount>) (word, collector) -> {\n    collector.collect(new WordWithCount(word, 1));\n  })\n \n  .returns(WordWithCount.class)\n  .keyBy(\"word\")\n  .timeWindow(Time.seconds(5))\n  .reduce((ReduceFunction<WordWithCount>) (c1, c2) ->\n    new WordWithCount(c1.word, c1.count + c2.count));\n```\n\n----------------------------------------\n\nTITLE: Implementing Full Partition Processing with DataStream API in Java\nDESCRIPTION: Example of using the new FullPartitionWindow API to count the number of records in each partition of a non-keyed stream. This approach eliminates the overhead of artificially assigning subtask IDs to create keyed streams for subtask-scope aggregations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-08-02-release-1.20.0.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\ninputStream.fullWindowPartition()\n                .mapPartition(\n         new MapPartitionFunction<Record, Long>() {\n                    @Override\n                    public void mapPartition(\n                            Iterable<Record> values, Collector<Long> out)\n                            throws Exception {\n                        long counter = 0;\n                        for (Record value : values) {\n                            counter++;\n                        }\n                        out.collect(counter));\n                    }\n          })\n```\n\n----------------------------------------\n\nTITLE: Converting between PyFlink Table and Pandas DataFrame in Python\nDESCRIPTION: This code snippet demonstrates how to create a PyFlink Table from a Pandas DataFrame and convert it back to a Pandas DataFrame. It uses StreamExecutionEnvironment and StreamTableEnvironment from PyFlink, and creates a random DataFrame using Pandas and NumPy.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-04-pyflink-pandas-udf-support-flink.md#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment\nimport pandas as pd\nimport numpy as np\n\nenv = StreamExecutionEnvironment.get_execution_environment()\nt_env = StreamTableEnvironment.create(env)\n\n# Create a PyFlink Table\npdf = pd.DataFrame(np.random.rand(1000, 2))\ntable = t_env.from_pandas(pdf, [\"a\", \"b\"]).filter(\"a > 0.5\")\n\n# Convert the PyFlink Table to a Pandas DataFrame\npdf = table.to_pandas()\nprint(pdf)\n```\n\n----------------------------------------\n\nTITLE: Processing Flink DataStream (Java)\nDESCRIPTION: Demonstrates processing logic on a Flink DataStream, performing a simple word count operation using flatMap, keyBy, timeWindow, and reduce functions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-25-query-pulsar-streams-using-apache-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// perform computation on DataStream (here a simple WordCount)\nDataStream<WordWithCount> wc = words\n  .flatmap((FlatMapFunction<String, WordWithCount>) (word, collector) -> {\n    collector.collect(new WordWithCount(word, 1));\n  })\n \n  .returns(WordWithCount.class)\n  .keyBy(\"word\")\n  .timeWindow(Time.seconds(5))\n  .reduce((ReduceFunction<WordWithCount>) (c1, c2) ->\n    new WordWithCount(c1.word, c1.count + c2.count));\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Execution Mode in Flink via Command Line\nDESCRIPTION: Command for submitting a Flink job in BATCH execution mode using the command line interface. This enables the efficient batch processing of bounded streams using the DataStream API.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-12-10-release-1.12.0.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar\n```\n\n----------------------------------------\n\nTITLE: Using Stored Procedures with Apache Paimon in Flink SQL\nDESCRIPTION: Example of creating a table in Apache Paimon catalog and calling a stored procedure to trigger table compaction. This demonstrates the new stored procedure support introduced in Flink 1.18.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-10-24-release-1.18.0.md#2025-04-08_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE `paimon`.`default`.`T` (\nid BIGINT PRIMARY KEY NOT ENFORCED,\ndt STRING, -- format 'yyyy-MM-dd'\nv STRING\n);\n\n-- use catalog before call procedures\nUSE CATALOG `paimon`;\n\n-- compact the whole table using call statement\nCALL sys.compact('default.T');\n```\n\n----------------------------------------\n\nTITLE: Interactive SQL Session with Flink JDBC Driver\nDESCRIPTION: Complete example of an interactive SQL session using Flink's JDBC driver, showing table creation, data insertion, and query execution. This demonstrates the functionality of the new JDBC driver for SQL Gateway introduced in Flink 1.18.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-10-24-release-1.18.0.md#2025-04-08_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsqlline version 1.12.0\nsqlline> !connect jdbc:flink://localhost:8083\nEnter username for jdbc:flink://localhost:8083:\nEnter password for jdbc:flink://localhost:8083:\n0: jdbc:flink://localhost:8083> CREATE TABLE T(\n. . . . . . . . . . . . . . .)>      a INT,\n. . . . . . . . . . . . . . .)>      b VARCHAR(10)\n. . . . . . . . . . . . . . .)>  ) WITH (\n. . . . . . . . . . . . . . .)>      'connector' = 'filesystem',\n. . . . . . . . . . . . . . .)>      'path' = 'file:///tmp/T.csv',\n. . . . . . . . . . . . . . .)>      'format' = 'csv'\n. . . . . . . . . . . . . . .)>  );\nNo rows affected (0.122 seconds)\n0: jdbc:flink://localhost:8083> INSERT INTO T VALUES (1, 'Hi'), (2, 'Hello');\n+----------------------------------+\n|              job id              |\n+----------------------------------+\n| fbade1ab4450fc57ebd5269fdf60dcfd |\n+----------------------------------+\n1 row selected (1.282 seconds)\n0: jdbc:flink://localhost:8083> SELECT * FROM T;\n+---+-------+\n| a |   b   |\n+---+-------+\n| 1 | Hi    |\n| 2 | Hello |\n+---+-------+\n2 rows selected (1.955 seconds)\n0: jdbc:flink://localhost:8083>\n```\n\n----------------------------------------\n\nTITLE: Configuring Execution Mode for Batch Jobs in Java\nDESCRIPTION: This code snippet shows how to set the execution mode for batch jobs using the ExecutionConfig class. It allows tuning of output types and scheduling decisions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-05-flink-network-stack.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nExecutionConfig#setExecutionMode(ExecutionMode)\n```\n\n----------------------------------------\n\nTITLE: Defining SortBuffer Interface in Java for Flink's Sort-Based Shuffle\nDESCRIPTION: This code snippet defines the SortBuffer interface used in Flink's sort-based blocking shuffle implementation. It includes methods for appending data, copying data, and managing the buffer state.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-10-26-sort-shuffle-part2.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic interface SortBuffer {\n\n    /** Appends data of the specified channel to this SortBuffer. */\n    boolean append(ByteBuffer source, int targetChannel, Buffer.DataType dataType) throws IOException;\n\n    /** Copies data in this SortBuffer to the target MemorySegment. */\n    BufferWithChannel copyIntoSegment(MemorySegment target);\n\n    long numRecords();\n\n    long numBytes();\n\n    boolean hasRemaining();\n\n    void finish();\n\n    boolean isFinished();\n\n    void release();\n\n    boolean isReleased();\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling State Cleanup on Full Snapshots in Flink\nDESCRIPTION: Shows how to configure StateTtlConfig to enable automatic eviction of expired state when taking full snapshots for checkpoints or savepoints in Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-05-17-state-ttl.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nStateTtlConfig ttlConfig = StateTtlConfig\n    .newBuilder(Time.days(7))\n    .cleanupFullSnapshot()\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Defining IMAP Source Configuration Options in Java\nDESCRIPTION: Creates a class to encapsulate IMAP source configuration options using Lombok annotations for boilerplate code generation. It includes fields for host, port, user, and password.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport lombok.Data;\nimport lombok.experimental.SuperBuilder;\nimport javax.annotation.Nullable;\nimport java.io.Serializable;\n\n@Data\n@SuperBuilder(toBuilder = true)\npublic class ImapSourceOptions implements Serializable {\n    private static final long serialVersionUID = 1L;\n\n    private final String host;\n    private final @Nullable Integer port;\n    private final @Nullable String user;\n    private final @Nullable String password;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Append-only Table in Flink Table Store\nDESCRIPTION: SQL command for creating an append-only table in Flink Table Store. This configuration improves performance by only accepting INSERT_ONLY data without updating or de-duplicating existing data, suitable for log data synchronization.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-08-29-release-table-store-0.2.0.md#2025-04-08_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE my_table (\n    ...\n) WITH (\n    'write-mode' = 'append-only',\n    ...\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Flink Autotuning in YAML\nDESCRIPTION: Configuration settings to enable Flink Autotuning along with Autoscaling. This feature automatically adjusts memory pools and container sizes based on actual usage patterns.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-21-release-kubernetes-operator-1.8.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Autoscaling needs to be enabled\njob.autoscaler.enabled: true\n# Turn on Autotuning\njob.autoscaler.memory.tuning.enabled: true\n```\n\n----------------------------------------\n\nTITLE: Tumbling Window Aggregation in Flink SQL\nDESCRIPTION: SQL query example demonstrating tumbling window aggregation on sensor data with daily intervals. Includes temperature conversion and averaging functionality.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-05-24-stream-sql.md#2025-04-08_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT STREAM \n  TUMBLE_END(time, INTERVAL '1' DAY) AS day, \n  location AS room, \n  AVG((tempF - 32) * 0.556) AS avgTempC\nFROM sensorData\nWHERE location LIKE 'room%'\nGROUP BY TUMBLE(time, INTERVAL '1' DAY), location\n```\n\n----------------------------------------\n\nTITLE: Configuring Buffer Timeout in Flink StreamExecutionEnvironment\nDESCRIPTION: Sets the buffer timeout for the StreamExecutionEnvironment, which affects how quickly data is flushed from network buffers. This method is used to configure low-latency behavior for scenarios with low-throughput channels.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-05-flink-network-stack.md#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nStreamExecutionEnvironment#setBufferTimeout(long)\n```\n\n----------------------------------------\n\nTITLE: Initializing Kinesis Consumer in Java\nDESCRIPTION: This code shows how to set up a Kinesis consumer in Flink 1.1.0 to read data from Amazon Kinesis Streams.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-08-04-release-1.1.0.md#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nDataStream<String> kinesis = env.addSource(\n  new FlinkKinesisConsumer<>(\"stream-name\", schema, config));\n```\n\n----------------------------------------\n\nTITLE: Integrating StateFun with Flink DataStream API\nDESCRIPTION: Example showing how to use the new DataStream Integration SDK to combine pipelines written with Flink DataStream API with Stateful Functions programming constructs. This enables building complex streaming applications with both APIs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-09-28-release-statefun-2.2.0.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nDataStream<RoutableMessage> namesIngress = ...;\n\nStatefulFunctionEgressStreams egresses =\n    StatefulFunctionDataStreamBuilder.builder(\"example\")\n        .withDataStreamAsIngress(namesIngress)\n        .withRequestReplyRemoteFunction(\n            RequestReplyFunctionBuilder.requestReplyFunctionBuilder(\n                    REMOTE_GREET, URI.create(\"http://...\"))\n                .withPersistedState(\"seen_count\")\n        .withFunctionProvider(GREET, unused -> new MyFunction())\n        .withEgressId(GREETINGS)\n        .build(env);\n\nDataStream<String> responsesEgress = getDataStreamForEgressId(GREETINGS);\n```\n\n----------------------------------------\n\nTITLE: Registering Python UDF\nDESCRIPTION: Shows how to register a Python UDF with the table environment.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-09-pyflink-udf-support-flink.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# register the Python function\ntable_env.register_function(\"add\", add)\n```\n\n----------------------------------------\n\nTITLE: Unit Testing a Timed Process Function in Flink\nDESCRIPTION: Java unit tests for the MyProcessFunction. It demonstrates testing both the processElement method and the timer functionality using KeyedOneInputStreamOperatorTestHarness.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-03-a-guide-for-unit-testing-in-apache-flink.md#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\n@Test\npublic void testProcessElement() throws Exception{\n  MyProcessFunction myProcessFunction = new MyProcessFunction();\n  OneInputStreamOperatorTestHarness<String, String> testHarness = \n    new KeyedOneInputStreamOperatorTestHarness<>(\n      new KeyedProcessOperator<>(myProcessFunction), x -> \"1\", Types.STRING);\n\n  testHarness.open();\n  testHarness.processElement(\"world\", 10);\n\n  Assert.assertEquals(\n    Lists.newArrayList(new StreamRecord<>(\"hello world\", 10)), \n    testHarness.extractOutputStreamRecords());\n}\n\n@Test\npublic void testOnTimer() throws Exception {\n  MyProcessFunction myProcessFunction = new MyProcessFunction();\n  OneInputStreamOperatorTestHarness<String, String> testHarness = \n    new KeyedOneInputStreamOperatorTestHarness<>(\n      new KeyedProcessOperator<>(myProcessFunction), x -> \"1\", Types.STRING);\n\n  testHarness.open();\n  testHarness.processElement(\"world\", 10);\n  Assert.assertEquals(1, testHarness.numProcessingTimeTimers());\n      \n  testHarness.setProcessingTime(50);\n  Assert.assertEquals(\n    Lists.newArrayList(\n      new StreamRecord<>(\"hello world\", 10), \n      new StreamRecord<>(\"Timer triggered at timestamp 50\")), \n    testHarness.extractOutputStreamRecords());\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Reactive Mode locally with Flink 1.13\nDESCRIPTION: These bash commands demonstrate how to set up and run a Flink job in Reactive Mode locally, including starting and stopping TaskManagers to scale the job.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-06-reactive-mode.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# These instructions assume you are in the root directory of a Flink distribution.\n# Put Job into usrlib/ directory\nmkdir usrlib\ncp ./examples/streaming/TopSpeedWindowing.jar usrlib/\n# Submit Job in Reactive Mode\n./bin/standalone-job.sh start -Dscheduler-mode=reactive -Dexecution.checkpointing.interval=\"10s\" -j org.apache.flink.streaming.examples.windowing.TopSpeedWindowing\n# Start first TaskManager\n./bin/taskmanager.sh start\n\n# Start additional TaskManager\n./bin/taskmanager.sh start\n\n# Remove a TaskManager\n./bin/taskmanager.sh stop\n```\n\n----------------------------------------\n\nTITLE: Suspending a Flink Job with Savepoint using CLI\nDESCRIPTION: Command to suspend a Flink job with a savepoint using the command-line interface. This implements the new stop-with-savepoint functionality for consistent job stopping.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-08-22-release-1.9.0.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/flink stop -p [:targetDirectory] :jobId\n```\n\n----------------------------------------\n\nTITLE: Connecting Broadcast Stream to Main Data Stream in Flink\nDESCRIPTION: Shows how to create and connect a broadcast stream containing rules to the main transaction data stream. The code demonstrates setting up processing pipeline with dynamic key function and alert processing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-03-24-demo-fraud-detection-2.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// Streams setup\nDataStream<Transaction> transactions = [...]\nDataStream<Rule> rulesUpdateStream = [...]\n\nBroadcastStream<Rule> rulesStream = rulesUpdateStream.broadcast(RULES_STATE_DESCRIPTOR);\n\n// Processing pipeline setup\n DataStream<Alert> alerts =\n     transactions\n         .connect(rulesStream)\n         .process(new DynamicKeyFunction())\n         .keyBy((keyed) -> keyed.getKey())\n         .connect(rulesStream)\n         .process(new DynamicAlertFunction())\n```\n\n----------------------------------------\n\nTITLE: Creating Source Table in Flink SQL\nDESCRIPTION: SQL statement to create a source table for streaming ETL in Flink. The statement defines the table structure and configures it to use a Kafka source with specific connection parameters.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-06-15-flink-on-zeppelin-part1.md#2025-04-08_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE source_table (\n  id STRING,\n  name STRING,\n  age INT\n) WITH (\n  'connector.type' = 'kafka',\n  'connector.version' = 'universal',\n  'connector.topic' = 'test',\n  'connector.properties.bootstrap.servers' = 'localhost:9092',\n  'format.type' = 'json'\n)\n```\n\n----------------------------------------\n\nTITLE: ServerFun Metric Report Processing\nDESCRIPTION: Implementation of metric report handling logic in ServerFun that evaluates server health and triggers alerts based on metrics analysis.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-18-statefun.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nServerHealthState currentState = serverHealthState.get();\nServerHealthState newState = evaluateServerHealth(\n    metricsHistory.entries(),\n    currentMetricReport,\n    currentState);\n\nif (!newState.isHealthy()) {\n    context.send(AlertsTopic, new Alert(serverId, newState));\n    context.send(RackFun, new ServerIncidents(serverId, newState));\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Metrics in Java\nDESCRIPTION: This code example shows how to add custom metrics (in this case, a counter) to a Flink application using the new metrics system in version 1.1.0.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-08-04-release-1.1.0.md#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nCounter counter = getRuntimeContext()\n  .getMetricGroup()\n  .counter(\"my-counter\");\n```\n\n----------------------------------------\n\nTITLE: Configuring Reactive Scaling Mode in Flink YAML\nDESCRIPTION: This YAML configuration enables the reactive scaling mode in Flink, allowing the application to adjust its parallelism based on the number of available workers.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nscheduler-mode: reactive\n```\n\n----------------------------------------\n\nTITLE: Enabling Avro Reflect Serialization in Flink Java\nDESCRIPTION: Code snippet showing how to enable Avro's reflection-based serializer for POJOs instead of using Flink's default PojoSerializer. This is done by enabling force Avro in the environment configuration.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nenv.getConfig().enableForceAvro();\n```\n\n----------------------------------------\n\nTITLE: Creating an Avro Serializer Snapshot Class in Flink\nDESCRIPTION: Implementation of a serializer snapshot class for Apache Avro in Flink that stores schema information, enabling schema migration between different versions of an application.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-29-state-unlocked-interacting-with-state-in-apache-flink.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic class AvroSerializerSnapshot<T> implements TypeSerializerSnapshot<T> {\n  private Schema runtimeSchema;\n  private Schema previousSchema;\nâ€‹\n  @SuppressWarnings(\"WeakerAccess\")\n  public AvroSerializerSnapshot() { }\nâ€‹\n  AvroSerializerSnapshot(Schema schema) {\n    this.runtimeSchema = schema;\n  }\n```\n\n----------------------------------------\n\nTITLE: Submitting Flink Job to Kubernetes Session\nDESCRIPTION: Command line example showing how to submit a Flink job to an existing Kubernetes session using the unified configuration system.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-11-release-1.10.0.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./bin/flink run -d -e kubernetes-session -Dkubernetes.cluster-id=<ClusterId> examples/streaming/WindowJoin.jar\n```\n\n----------------------------------------\n\nTITLE: Executing Windowed Outer Equi-Join in Flink SQL\nDESCRIPTION: Demonstrates a SQL query for joining two tables (Departures and Arrivals) on a time-bounded condition using Flink's extended join support.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-05-18-release-1.5.0.md#2025-04-08_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT d.rideId, d.departureTime, a.arrivalTime\nFROM Departures d LEFT OUTER JOIN Arrivals a\n  ON d.rideId = a.rideId\n  AND a.arrivalTime BETWEEN \n      d.deptureTime AND d.departureTime + '2' HOURS\n```\n\n----------------------------------------\n\nTITLE: Task Completion Process Flow in Flink\nDESCRIPTION: Detailed sequence of operations that occur during task completion in Flink, including watermark emission, operator cleanup, and partition event handling. This process shows how both source and non-source tasks handle completion.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-07-11-final-checkpoint-part2.md#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n1. Source operators emit MAX_WATERMARK\n2. On received MAX_WATERMARK for non-source operators\n    a. Trigger all the event-time timers\n    b. Emit MAX_WATERMARK\n3. Source tasks finished\n    a. endInput(inputId) for all the operators\n    b. close() for all the operators\n    c. dispose() for all the operators\n    d. Emit EndOfPartitionEvent\n    e. Task cleanup\n4. On received EndOfPartitionEvent for non-source tasks\n    a. endInput(int inputId) for all the operators\n    b. close() for all the operators\n    c. dispose() for all the operators\n    d. Emit EndOfPartitionEvent\n    e. Task cleanup\n```\n\n----------------------------------------\n\nTITLE: Creating Elasticsearch Table for Top Categories in Flink SQL\nDESCRIPTION: This SQL creates an Elasticsearch table to store top category statistics. It defines columns for category name and buy count, with category name as the primary key.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE top_category (\n    category_name STRING PRIMARY KEY NOT ENFORCED,\n    buy_cnt BIGINT\n) WITH (\n    'connector' = 'elasticsearch-7',\n    'hosts' = 'http://elasticsearch:9200',\n    'index' = 'top_category'\n);\n```\n\n----------------------------------------\n\nTITLE: Applying WindowFunction in Apache Flink (Scala)\nDESCRIPTION: This snippet shows how to apply a WindowFunction to a windowed stream in Apache Flink, producing the final output DataStream.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-04-Introducing-windows.md#2025-04-08_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n// apply window function to windowed stream\nval output: DataStream[OUT] = windowed\n  .apply(myWinFunc: WindowFunction[IN, OUT, KEY, WINDOW])\n```\n\n----------------------------------------\n\nTITLE: Implementing Static Data Output in ImapSource for Java\nDESCRIPTION: Enhances the ImapSource class to output static test data. This implementation allows for testing data flow within the Flink system before integrating actual IMAP functionality.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part1.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.streaming.api.functions.source.RichSourceFunction;\nimport org.apache.flink.table.data.GenericRowData;\nimport org.apache.flink.table.data.RowData;\nimport org.apache.flink.table.data.StringData;\n\npublic class ImapSource extends RichSourceFunction<RowData> {\n  @Override\n  public void run(SourceContext<RowData> ctx) throws Exception {\n      ctx.collect(GenericRowData.of(\n          StringData.fromString(\"Subject 1\"),\n          StringData.fromString(\"Hello, World!\")\n      ));\n  }\n\n  @Override\n  public void cancel(){}\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Non-Windowed Inner Join in Flink SQL\nDESCRIPTION: Shows a SQL query for performing a full-history matching join between two streaming tables (Users and Orders) without time bounds.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-05-18-release-1.5.0.md#2025-04-08_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT u.name, u.address, o.productId, o.amount\nFROM Users u JOIN Orders o\n  ON u.userId = o.userId\n```\n\n----------------------------------------\n\nTITLE: Implementing ServerFun State Management\nDESCRIPTION: Definition of ServerFun class showing state management and message handling for server metrics monitoring. Includes state definitions for metrics history and server health tracking.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-18-statefun.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nmetricsHistory = context.state(\"metrics-history\",\n    ValueSpec.of(new TypeHint<LinkedHashMapState<Long, ServerMetricReport>>(){}))\n    .setTtlTime(Duration.ofMinutes(15));\n\nserverHealthState = context.state(\"server-health-state\",\n    TypeName.typeNameof(ServerHealthState.class));\n```\n\n----------------------------------------\n\nTITLE: Registering Apache Thrift Serializer with Kryo in Flink Java\nDESCRIPTION: Example of how to register Apache Thrift serializer with Kryo. This allows Flink to efficiently serialize Thrift-generated data types using Thrift's own serialization mechanism.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nenv.getConfig().addDefaultKryoSerializer(MyCustomType.class, TBaseSerializer.class);\n```\n\n----------------------------------------\n\nTITLE: Specifying Avro GenericRecord Type Information with returns() Method in Java\nDESCRIPTION: Example showing how to provide type information for Avro GenericRecord using the returns() method when building the job graph. This approach explicitly sets the type information on the stream.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nDataStream<GenericRecord> sourceStream =\n    env.addSource(new AvroGenericSource())\n        .returns(new GenericRecordAvroTypeInfo(schema));\n```\n\n----------------------------------------\n\nTITLE: Using Table API in Java\nDESCRIPTION: This code example shows how to use the Table API in Flink 1.1.0 to perform SQL-like operations on datasets.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-08-04-release-1.1.0.md#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nTable custT = tableEnv\n  .toTable(custDs, \"name, zipcode\")\n  .where(\"zipcode = '12345'\")\n  .select(\"name\")\n```\n\n----------------------------------------\n\nTITLE: Generating Temperature Alerts\nDESCRIPTION: Processing matched warning events to generate temperature alerts when temperature is increasing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_10\n\nLANGUAGE: java\nCODE:\n```\nDataStream<TemperatureAlert> alerts = alertPatternStream.flatSelect(\n    (Map<String, TemperatureWarning> pattern, Collector<TemperatureAlert> out) -> {\n        TemperatureWarning first = pattern.get(\"First Event\");\n        TemperatureWarning second = pattern.get(\"Second Event\");\n\n        if (first.getAverageTemperature() < second.getAverageTemperature()) {\n            out.collect(new TemperatureAlert(first.getRackID()));\n        }\n    });\n```\n\n----------------------------------------\n\nTITLE: Configuring RocksDB State TTL with Compaction Filter in Java\nDESCRIPTION: Example showing how to configure TTL for RocksDB state backend with cleanup through compaction filter. Sets a 7-day TTL period and enables the RocksDB compaction filter cleanup strategy.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-05-17-state-ttl.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nStateTtlConfig ttlConfig = StateTtlConfig\n    .newBuilder(Time.days(7))\n    .cleanupInRocksdbCompactFilter()\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Adding Cassandra Sink in Java\nDESCRIPTION: This snippet demonstrates how to add a Cassandra sink to write data from Flink to Apache Cassandra in version 1.1.0.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-08-04-release-1.1.0.md#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nCassandraSink.addSink(input)\n```\n\n----------------------------------------\n\nTITLE: Enabling RocksDB State Backend Latency Tracking in Flink Configuration\nDESCRIPTION: This configuration option enables tracking of latency metrics for the RocksDB state backend, helping to identify performance issues related to state access.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nstate.backend.rocksdb.latency-track-enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Cleanup for Heap State Backends in Flink\nDESCRIPTION: Demonstrates how to enable and configure incremental cleanup of expired state for Heap state backends (FSStateBackend and MemoryStateBackend) in Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-05-17-state-ttl.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nStateTtlConfig ttlConfig = StateTtlConfig\n    .newBuilder(Time.days(7))\n    // check 10 keys for every state access\n    .cleanupIncrementally(10, false)\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Configuring Row Type Information via Stream Returns\nDESCRIPTION: Example demonstrating how to specify Row type information when building the job graph using SingleOutputStreamOperator#returns().\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nDataStream<Row> sourceStream =\n    env.addSource(new RowSource())\n        .returns(Types.ROW(Types.INT, Types.STRING, Types.OBJECT_ARRAY(Types.STRING)));\n```\n\n----------------------------------------\n\nTITLE: Creating Functions with Flink SQL DDL\nDESCRIPTION: This SQL snippet shows the syntax for creating different types of functions in Flink SQL, including temporary, temporary system, and catalog functions. It supports specifying implementation languages like Java or Scala.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-11-release-1.10.0.md#2025-04-08_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION \n\n  [IF NOT EXISTS] [catalog_name.][db_name.]function_name \n\nAS identifier [LANGUAGE JAVA|SCALA]\n```\n\n----------------------------------------\n\nTITLE: Enabling UNIX Domain Sockets for Co-located Functions\nDESCRIPTION: Demonstrates how to configure UNIX Domain Sockets (UDS) for communication between co-located functions in the module.yaml file. This improves performance for functions deployed in the same pod.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-06-09-release-statefun-2.1.0.md#2025-04-08_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nfunctions:\n  - function:\n     spec:\n       - endpoint: http(s)+unix://<socket-file-path>/<serve-url-path>\n```\n\n----------------------------------------\n\nTITLE: Connecting to Flink SQL Gateway via JDBC Using SQLLine\nDESCRIPTION: Example of connecting to the Flink SQL Gateway using the new JDBC driver with SQLLine client. This demonstrates the basic connection syntax for establishing a JDBC connection to a Flink SQL Gateway instance.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-10-24-release-1.18.0.md#2025-04-08_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsqlline> !connect jdbc:flink://localhost:8083\n```\n\n----------------------------------------\n\nTITLE: Static Partition Writing in Hive SQL\nDESCRIPTION: SQL syntax for writing data into static partitions in Hive tables using Flink SQL.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-11-release-1.10.0.md#2025-04-08_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT { INTO | OVERWRITE } TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement;\n```\n\n----------------------------------------\n\nTITLE: Configuring Schema Change Events in YAML\nDESCRIPTION: YAML configuration to specify which schema change events should be included or excluded in the CDC pipeline.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-09-05-release-cdc-3.2.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsink:\n  include.schema.changes: [column]\n  exclude.schema.changes: [alter, drop]\n```\n\n----------------------------------------\n\nTITLE: Configuring Pravega Connection Settings\nDESCRIPTION: Java code demonstrating different ways to create and configure PravegaConfig object for connection settings, including environment defaults, program arguments, and manual specification.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-20-pravega-connector-101.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// From default environment\nPravegaConfig config = PravegaConfig.fromDefaults();\n\n// From program arguments\nParameterTool params = ParameterTool.fromArgs(args);\nPravegaConfig config = PravegaConfig.fromParams(params);\n\n// From user specification\nPravegaConfig config = PravegaConfig.fromDefaults()\n    .withControllerURI(\"tcp://...\")\n    .withDefaultScope(\"SCOPE-NAME\")\n    .withCredentials(credentials)\n    .withHostnameValidation(false);\n```\n\n----------------------------------------\n\nTITLE: Task Deployment Classes - Java\nDESCRIPTION: Core classes involved in Flink's task deployment process, including TaskDeploymentDescriptor and ShuffleDescriptor which contain task execution information and shuffle data transfer details.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-04-scheduler-performance-part-two.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nTaskDeploymentDescriptor\nShuffleDescriptor\n```\n\n----------------------------------------\n\nTITLE: Installing PyFlink via pip\nDESCRIPTION: Command to install PyFlink package using pip package manager. Requires Python 3.5 or higher.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-09-pyflink-udf-support-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m pip install apache-flink\n```\n\n----------------------------------------\n\nTITLE: Embedding Storm Spouts and Bolts in a Flink DataStream Program\nDESCRIPTION: This code example demonstrates how to use Storm Spouts and Bolts within a regular Flink DataStream program. It shows the use of SpoutWrapper and BoltWrapper classes to integrate Storm components into a typed Flink streaming application.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-11-storm-compatibility.md#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// use regular Flink streaming environment\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n// use Spout as source\nDataStream<Tuple1<String>> source = \n  env.addSource(// Flink provided wrapper including original Spout\n                new SpoutWrapper<String>(new FileSpout(localFilePath)), \n                // specify output type manually\n                TypeExtractor.getForObject(new Tuple1<String>(\"\"))); \n// FileSpout cannot be parallelized\nDataStream<Tuple1<String>> text = source.setParallelism(1);\n\n// further processing with Flink\nDataStream<Tuple2<String,Integer> tokens = text.flatMap(new Tokenizer()).keyBy(0);\n\n// use Bolt for counting\nDataStream<Tuple2<String,Integer> counts =\n  tokens.transform(\"Counter\",\n                   // specify output type manually\n                   TypeExtractor.getForObject(new Tuple2<String,Integer>(\"\",0))\n                   // Flink provided wrapper including original Bolt\n                   new BoltWrapper<String,Tuple2<String,Integer>>(new BoltCounter()));\n\n// write result to file via Flink sink\ncounts.writeAsText(outputPath);\n\n// start Flink job\nenv.execute(\"WordCount with Spout source and Bolt counter\");\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic ImapSource Class in Java\nDESCRIPTION: Creates a basic ImapSource class extending RichSourceFunction with empty run() and cancel() methods. This serves as the starting point for implementing the custom source connector.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part1.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.streaming.api.functions.source.RichSourceFunction;\nimport org.apache.flink.table.data.RowData;\n\npublic class ImapSource extends RichSourceFunction<RowData> {\n  @Override\n  public void run(SourceContext<RowData> ctx) throws Exception {}\n\n  @Override\n  public void cancel() {}\n}\n```\n\n----------------------------------------\n\nTITLE: Pulsar Serialization Schema Builder Example\nDESCRIPTION: Example of building a PulsarSerializationSchema using the Builder pattern with SimpleStringSchema and topic extraction configuration.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-07-pulsar-flink-connector-270.md#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nnew PulsarSerializationSchemaWrapper.Builder<>(new SimpleStringSchema())\n    .setTopicExtractor(str -> getTopic(str))\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Markdown List of Apache Flink Contributors\nDESCRIPTION: A markdown formatted list containing the names of all contributors to the Apache Flink project, including both individual developers and organizations\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n# List of Contributors\n\nThe Apache Flink community would like to thank each one of the contributors that have\nmade this release possible:\n\nacqua.csq, AkisAya, Alexander Fedulov, Aljoscha Krettek, Ammar Al-Batool, Andrey Zagrebin, anlen321,\nAnton Kalashnikov, appleyuchi, Arvid Heise, Austin Cawley-Edwards, austin ce, azagrebin, blublinsky,\nBrian Zhou, bytesmithing, caozhen1937, chen qin, Chesnay Schepler, Congxian Qiu, Cristian,\ncxiiiiiii, Danny Chan, Danny Cranmer, David Anderson, Dawid Wysakowicz, dbgp2021, Dian Fu,\nDinoZhang, dixingxing, Dong Lin, Dylan Forciea, est08zw, Etienne Chauchot, fanrui03, Flora Tao,\nFLRNKS, fornaix, fuyli, George, Giacomo Gamba, GitHub, godfrey he, GuoWei Ma, Gyula Fora,\nhackergin, hameizi, Haoyuan Ge, Harshvardhan Chauhan, Haseeb Asif, hehuiyuan, huangxiao, HuangXiao,\nhuangxingbo, HuangXingBo, humengyu2012, huzekang, Hwanju Kim, Ingo BÃ¼rk, I. Raleigh, Ivan, iyupeng,\nJack, Jane, Jark Wu, Jerry Wang, Jiangjie (Becket) Qin, JiangXin, Jiayi Liao, JieFang.He, Jie Wang,\njinfeng, Jingsong Lee, JingsongLi, Jing Zhang, Joao Boto, JohnTeslaa, Jun Qin, kanata163, kevin.cyj,\nKevinyhZou, Kezhu Wang, klion26, Kostas Kloudas, kougazhang, Kurt Young, laughing, legendtkl,\nleiqiang, Leonard Xu, liaojiayi, Lijie Wang, liming.1018, lincoln lee, lincoln-lil, liushouwei,\nliuyufei, LM Kang, lometheus, luyb, Lyn Zhang, Maciej Obuchowski, Maciek PrÃ³chniak, mans2singh,\nMarek Sabo, Matthias Pohl, meijie, Mika Naylor, Miklos Gergely, Mohit Paliwal, Moritz Manner,\nmorsapaes, Mulan, Nico Kruber, openopen2, paul8263, Paul Lam, Peidian li, pengkangjing, Peter Huang,\nPiotr Nowojski, Qinghui Xu, Qingsheng Ren, Raghav Kumar Gautam, Rainie Li, Ricky Burnett, Rion\nWilliams, Robert Metzger, Roc Marshal, Roman, Roman Khachatryan, Ruguo,\nRuguo Yu, Rui Li, Sebastian Liu, Seth Wiesman, sharkdtu, sharkdtu(æ¶‚å°åˆš), Shengkai, shizhengchao,\nshouweikun, Shuo Cheng, simenliuxing, SteNicholas, Stephan Ewen, Suo Lu, sv3ndk, Svend Vanderveken,\ntaox, Terry Wang, Thelgis Kotsos, Thesharing, Thomas Weise, Till Rohrmann, Timo Walther, Ting Sun,\ntotoro, totorooo, TsReaper, Tzu-Li (Gordon) Tai, V1ncentzzZ, vthinkxie, wangfeifan, wangpeibin,\nwangyang0918, wangyemao-github, Wei Zhong, Wenlong Lyu, wineandcheeze, wjc, xiaoHoly, Xintong Song,\nxixingya, xmarker, Xue Wang, Yadong Xie, yangsanity, Yangze Guo, Yao Zhang, Yuan Mei, yulei0824, Yu\nLi, Yun Gao, Yun Tang, yuruguo, yushujun, Yuval Itzchakov, yuzhao.cyz, zck, zhangjunfan,\nzhangzhengqi3, zhao_wei_nan, zhaown, zhaoxing, Zhenghua Gao, Zhenqiu Huang, zhisheng, zhongqishang,\nzhushang, zhuxiaoshang, Zhu Zhu, zjuwangg, zoucao, zoudan, å·¦å…ƒ, æ˜Ÿ, è‚–ä½³æ–‡, é¾™ä¸‰\n```\n\n----------------------------------------\n\nTITLE: Implementing TypeSerializerSnapshot Interface in Java for Flink Schema Evolution\nDESCRIPTION: Definition of the TypeSerializerSnapshot interface that allows Flink to handle schema evolution by storing the configuration of the writer serializer in the snapshot and checking compatibility when restoring.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-29-state-unlocked-interacting-with-state-in-apache-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic interface TypeSerializerSnapshot<T> {\nâ€‹\n  int getCurrentVersion();\nâ€‹\n  void writeSnapshot(DataOutputView out) throws IOException;\nâ€‹\n  void readSnapshot(\n      int readVersion,\n      DataInputView in,\n      ClassLoader userCodeClassLoader) throws IOException;\nâ€‹\n  TypeSerializer<T> restoreSerializer();\nâ€‹\n  TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(\n      TypeSerializer<T> newSerializer);\n}\n```\n\n----------------------------------------\n\nTITLE: Distribution Pattern Analysis - Java Classes\nDESCRIPTION: Key Java classes referenced in the distribution pattern optimization including ExecutionEdge, JobVertices, IntermediateResultPartitions, and ExecutionVertices. These classes are used to store connection information between tasks in Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-04-scheduler-performance-part-two.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nExecutionEdge\nJobVertex\nIntermediateResultPartition\nExecutionVertex\nJobEdge\nConsumerVertexGroup\nConsumedPartitionGroup\n```\n\n----------------------------------------\n\nTITLE: Using CUMULATE Window Function in Flink SQL\nDESCRIPTION: This SQL snippet demonstrates the new CUMULATE window function introduced in Flink 1.13, which assigns windows with an expanding step size until reaching a maximum window size. The query groups and aggregates bid prices within cumulative windows of 2 to 10 minutes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT window_time, window_start, window_end, SUM(price) AS total_price \n  FROM TABLE(CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '2' MINUTES, INTERVAL '10' MINUTES))\nGROUP BY window_start, window_end, window_time;\n```\n\n----------------------------------------\n\nTITLE: Configuring State TTL in Java SDK for Stateful Functions\nDESCRIPTION: Demonstrates how to configure state time-to-live (TTL) in the Java SDK for Apache Flink Stateful Functions. This example creates a persisted integer value that expires one hour after the last write operation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-06-09-release-statefun-2.1.0.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n@Persisted\nPersistedValue<Integer> table = PersistedValue.of(\n    \"my-value\",\n    Integer.class,\n    Expiration.expireAfterWriting(Duration.ofHours(1)));\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Delegation Token Provider in Java\nDESCRIPTION: This snippet outlines the key steps for implementing a custom delegation token provider in Java for the Flink Delegation Token Framework. It includes obtaining custom tokens, receiving tokens on TaskManagers, and using them for authentication.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-01-20-delegation-token-framework.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Check out the example implementation\n// Change FlinkTestJavaDelegationTokenProvider.obtainDelegationTokens to obtain a custom token from any external service\n// Change FlinkTestJavaDelegationTokenReceiver.onNewTokensObtained to receive the previously obtained tokens on all task managers\n// Use the tokens for external service authentication\n// Compile the project and put it into the classpath (adding it inside a plugin also supported)\n// Enjoy that Flink does all the heavy lifting behind the scenes :-)\n```\n\n----------------------------------------\n\nTITLE: Configuring Adaptive Batch Scheduler Settings\nDESCRIPTION: Basic configuration required to enable and use the adaptive batch scheduler in Flink. Sets the scheduler type and ensures batch shuffle mode is compatible.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-06-17-adaptive-batch-scheduler.md#2025-04-08_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\njobmanager.scheduler: AdaptiveBatch\nexecution.batch-shuffle-mode: ALL-EXCHANGES-BLOCKING\n```\n\n----------------------------------------\n\nTITLE: Configuring RocksDB State Backend in Flink YAML Configuration\nDESCRIPTION: This YAML snippet shows how to configure RocksDB as the default state backend at the cluster level in Flink's configuration file.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-18-rocksdb.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nstate.backend: rocksdb\nstate.backend.incremental: true\nstate.checkpoints.dir: hdfs:///flink-checkpoints # location to store checkpoints\n```\n\n----------------------------------------\n\nTITLE: Neighbor Value Reduction in Java\nDESCRIPTION: Example demonstrating how to compute sum of incoming neighbor values for each vertex using the reduceOnNeighbors aggregation method.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-08-24-introducing-flink-gelly.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ngraph.reduceOnNeighbors(new SumValues(), EdgeDirection.IN);\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Functions in Python SDK with StateFun\nDESCRIPTION: Code demonstrating how to register asynchronous Python functions as stateful functions using the new AsyncRequestReplyHandler. This allows integration with Python web frameworks that support asynchronous IO natively, such as aiohttp.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-09-28-release-statefun-2.2.0.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom statefun import StatefulFunctions\nfrom statefun import AsyncRequestReplyHandler\n\nfunctions = StatefulFunctions()\n\n@functions.bind(\"example/greeter\")\nasync def greeter(context, message):\n  html = await fetch(session, 'http://....')\n  context.pack_and_reply(SomeProtobufMessage(html))\n\n# expose this handler via an async web framework\nhandler = AsyncRequestReplyHandler(functions)\n```\n\n----------------------------------------\n\nTITLE: Filtering Music Records with CoGroup in Java\nDESCRIPTION: This code uses a coGroup operation to filter out mismatched songs from user-song-play triplets. It compares song IDs from the triplets with a list of invalid songs, collecting only the valid triplets where no matches are found in the mismatches dataset.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-08-24-introducing-flink-gelly.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// read the user-song-play triplets.\nDataSet<Tuple3<String, String, Integer>> triplets =\n    getUserSongTripletsData(env);\n\n// read the mismatches dataset and extract the songIDs\nDataSet<Tuple3<String, String, Integer>> validTriplets = triplets\n        .coGroup(mismatches).where(1).equalTo(0)\n        .with(new CoGroupFunction() {\n                void coGroup(Iterable triplets, Iterable invalidSongs, Collector out) {\n                        if (!invalidSongs.iterator().hasNext()) {\n                            for (Tuple3 triplet : triplets) { // valid triplet\n                                out.collect(triplet);\n                            }\n                        }\n                    }\n                }\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Pattern Definition in Flink CEP\nDESCRIPTION: Creating a basic pattern for monitoring events using Flink's Pattern API.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nPattern.<MonitoringEvent>begin(\"First Event\");\n```\n\n----------------------------------------\n\nTITLE: Transforming Graph Operations in Java\nDESCRIPTION: Example showing how to chain graph transformations in Gelly by getting an undirected version of a graph and mapping its edges.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-08-24-introducing-flink-gelly.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\ninputGraph.getUndirected().mapEdges(new CustomEdgeMapper());\n```\n\n----------------------------------------\n\nTITLE: Setting Default Parallelism Configuration\nDESCRIPTION: Configuration settings to leave parallelism unset (-1) for adaptive scheduling in different types of Flink jobs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-06-17-adaptive-batch-scheduler.md#2025-04-08_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nparallelism.default: -1\ntable.exec.resource.default-parallelism: -1\n```\n\n----------------------------------------\n\nTITLE: Implementing ResultTypeQueryable for Row Source\nDESCRIPTION: Example showing how to implement ResultTypeQueryable interface to provide type information for Row data types in Flink sources.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic static class RowSource implements SourceFunction<Row>, ResultTypeQueryable<Row> {\n  // ...\n\n  @Override\n  public TypeInformation<Row> getProducedType() {\n    return Types.ROW(Types.INT, Types.STRING, Types.OBJECT_ARRAY(Types.STRING));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Join Operation in Flink DataStream API\nDESCRIPTION: Demonstrating how to perform a join operation in the DataStream API using a custom EndOfStream window. This workaround is necessary because the DataStream join operator doesn't support aggregations in batch mode.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-09-howto-migrate-to-datastream.md#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\njoinedStream = joinedStream\n    .windowAll(new EndOfStream())\n    .apply(\n        new JoinFunction<Row, Row, Row>() {\n            @Override\n            public Row join(Row first, Row second) throws Exception {\n                return Row.join(first, second);\n            }\n        }\n    );\n```\n\n----------------------------------------\n\nTITLE: Implementing a Greeter Function in Python\nDESCRIPTION: Demonstrates how to implement a stateful greeter function using the updated Python SDK in StateFun 3.0.0. It shows state management, message handling, and response sending.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-04-15-release-statefun-3.0.0.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@functions.bind(typename=\"example/greeter\", specs=[ValueSpec(name=\"visits\", type=IntType)])\nasync def greeter(context: Context, message: Message):\n    # update the visit count.\n    visits = context.storage.visits or 0\n    visits += 1\n    context.storage.visits = visits\n\n    # compute a greeting\n    name = message.as_string()\n    greeting = f\"Hello there {name} at the {visits}th time!\"\n\n    caller = context.caller\n\n    context.send(message_builder(target_typename=caller.typename,\n                                 target_id=caller.id,\n                                 str_value=greeting))\n```\n\n----------------------------------------\n\nTITLE: Configuring State TTL in YAML for Remote Functions\nDESCRIPTION: Shows how to configure state time-to-live (TTL) for remote functions in the module.yaml configuration file. This example sets a state named 'xxxx' to expire after 5 minutes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-06-09-release-statefun-2.1.0.md#2025-04-08_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nfunctions:\n  - function:\n     states:\n       - name: xxxx\n         expireAfter: 5min # optional key\n```\n\n----------------------------------------\n\nTITLE: Implementing Pravega Serialization Adapter\nDESCRIPTION: Example of implementing a serialization adapter using PravegaDeserializationSchema with JavaSerializer for custom event types.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-20-pravega-connector-101.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nimport io.pravega.client.stream.impl.JavaSerializer;\n...\nDeserializationSchema<MyEvent> adapter = new PravegaDeserializationSchema<>(\n    MyEvent.class, new JavaSerializer<MyEvent>());\n```\n\n----------------------------------------\n\nTITLE: Configuring Backend Test Environment\nDESCRIPTION: Demonstrates how to set up the backend test environment for integration testing with external systems.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-12-howto-test-batch-source.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n@TestExternalSystem\nMyBackendTestEnvironment backendTestEnvironment = new MyBackendTestEnvironment();\n```\n\n----------------------------------------\n\nTITLE: Enabling Changelog-Based Incremental Checkpoints in Flink Configuration\nDESCRIPTION: This YAML configuration enables changelog-based incremental checkpoints in Apache Flink. It sets the state backend to use changelog, specifies the storage type as filesystem, and sets the base path for distributed state logs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-30-changelog-state-backend.md#2025-04-08_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nstate.backend.changelog.enabled: true\nstate.backend.changelog.storage: filesystem \ndstl.dfs.base-path: <location similar to state.checkpoints.dir>\n```\n\n----------------------------------------\n\nTITLE: Keying Action Stream by User ID in Flink\nDESCRIPTION: Creates a KeyedStream from the actions stream, using the userId as the key. This allows processing actions on a per-user basis.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-26-broadcast-state.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nKeyedStream<Action, Long> actionsByUser = actions\n  .keyBy((KeySelector<Action, Long>) action -> action.userId);\n```\n\n----------------------------------------\n\nTITLE: Implementing HybridMemorySegment for Unified Memory Access\nDESCRIPTION: Implementation of the HybridMemorySegment class that provides transparent access to both heap and off-heap memory using sun.misc.Unsafe. The class handles both memory types by storing either a byte array reference or a direct memory pointer.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-09-16-off-heap-memory.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic class HybridMemorySegment {\n\n  private final byte[] heapMemory;  // non-null in heap case, null in off-heap case\n  private final long address;       // may be absolute, or relative to byte[]\n\n\n  // method of interest\n  public long getLong(int pos) {\n    return UNSAFE.getLong(heapMemory, address + pos);\n  }\n\n\n  // initialize for heap memory\n  public HybridMemorySegment(byte[] heapMemory) {\n    this.heapMemory = heapMemory;\n    this.address = UNSAFE.arrayBaseOffset(byte[].class)\n  }\n  \n  // initialize for off-heap memory\n  public HybridMemorySegment(long offheapPointer) {\n    this.heapMemory = null;\n    this.address = offheapPointer\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Managing Python Dependencies\nDESCRIPTION: Commands to prepare and manage Python dependencies for PyFlink jobs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-09-pyflink-udf-support-flink.md#2025-04-08_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ cd /tmp\n$ echo mpmath==1.1.0 > requirements.txt\n$ pip download -d cached_dir -r requirements.txt --no-binary :all:\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partition Writing in Hive SQL\nDESCRIPTION: SQL syntax for writing data into dynamic partitions in Hive tables using Flink SQL.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-11-release-1.10.0.md#2025-04-08_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT { INTO | OVERWRITE } TABLE tablename1 select_statement1 FROM from_statement;\n```\n\n----------------------------------------\n\nTITLE: Defining Tumbling Window with Grouping in Flink Table API (Scala)\nDESCRIPTION: This snippet demonstrates how to define a tumbling window over a day's duration on event time ('rowtime'), group by customer ID and window, then select the ID, window start/end times, and count of preference updates.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-29-table-sql-api-update.md#2025-04-08_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\ntable\n  .window(Tumble over 1.day on 'rowtime as 'w)\n  .groupBy('id, 'w)\n  .select('id, 'w.start as 'from, 'w.end as 'to, 'prefs.count as 'updates)\n```\n\n----------------------------------------\n\nTITLE: Launching Flink JobManager Container in Docker\nDESCRIPTION: Deploys the Flink JobManager container using Docker run command. The container is connected to the flink-network and exposes port 8081 for the web UI. The FLINK_PROPERTIES environment variable is passed to configure the container.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-20-flink-docker.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n       --rm \\\n       --name=jobmanager \\\n       --network flink-network \\\n       -p 8081:8081 \\\n       --env FLINK_PROPERTIES=\"${FLINK_PROPERTIES}\" \\\n       flink:1.11.1 jobmanager\n```\n\n----------------------------------------\n\nTITLE: Configuring PyFlink Execution Mode\nDESCRIPTION: Example showing how to configure PyFlink's execution mode using Python Table API. Demonstrates setting both process and thread modes via configuration options.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-06-pyflink-1.15-thread-mode.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Specify `process` mode\ntable_env.get_config().set(\"python.execution-mode\", \"process\")\n\n# Specify `thread` mode\ntable_env.get_config().set(\"python.execution-mode\", \"thread\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Python UDF for String Uppercase Conversion in Flink PyFlink\nDESCRIPTION: This code snippet demonstrates how to create a Python UDF named 'PythonUpper' that converts a string to uppercase, and registers it with the PyFlink batch table environment using data type information.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-06-23-flink-on-zeppelin-part2.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass PythonUpper(ScalarFunction):\ndef eval(self, s):\n return s.upper()\n\nbt_env.register_function(\"python_upper\", udf(PythonUpper(), DataTypes.STRING(), DataTypes.STRING()))\n\n```\n\n----------------------------------------\n\nTITLE: Calculating Shortest Paths with Flink Gelly Graph API\nDESCRIPTION: Example showing how to calculate single source shortest paths in a graph using Flink's Gelly Graph API. The code creates a graph from vertices and edges datasets and applies the SingleSourceShortestPaths algorithm.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-03-02-february-2015-in-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nGraph<Long, Double, Double> graph = Graph.fromDataSet(vertices, edges, env);\n\nDataSet<Vertex<Long, Double>> singleSourceShortestPaths = graph\n     .run(new SingleSourceShortestPaths<Long>(srcVertexId,\n           maxIterations)).getVertices();\n```\n\n----------------------------------------\n\nTITLE: Initializing TestContext Factory in Flink Connector Tests\nDESCRIPTION: Creates a TestContextFactory instance that will be used to generate TestContext objects for individual test cases. The factory is associated with a test environment that contains shared test resources.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-12-howto-test-batch-source.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n@TestContext\nTestContextFactory contextFactory = new TestContextFactory(testEnvironment);\n```\n\n----------------------------------------\n\nTITLE: JSON Rule Definition Format\nDESCRIPTION: Defines the JSON structure for specifying fraud detection rules including grouping keys, aggregation fields, and threshold parameters.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-15-demo-fraud-detection.md#2025-04-08_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ruleId\": 1,\n  \"ruleState\": \"ACTIVE\",\n  \"groupingKeyNames\": [\"payerId\", \"beneficiaryId\"],\n  \"aggregateFieldName\": \"paymentAmount\",\n  \"aggregatorFunctionType\": \"SUM\",\n  \"limitOperatorType\": \"GREATER\",\n  \"limit\": 1000000,\n  \"windowMinutes\": 10080\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Session Windows in Java\nDESCRIPTION: This code shows how to set up session windows in Flink 1.1.0's DataStream API, which are useful for data-driven window boundaries.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-08-04-release-1.1.0.md#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\ninput.keyBy(<key selector>)\n    .window(EventTimeSessionWindows.withGap(Time.minutes(10)))\n    .<windowed transformation>(<window function>);\n```\n\n----------------------------------------\n\nTITLE: Launching Flink TaskManager Container in Docker\nDESCRIPTION: Deploys a Flink TaskManager container using Docker. The container is connected to the same flink-network as the JobManager and uses the same FLINK_PROPERTIES environment variable for configuration.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-20-flink-docker.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n      --rm \\\n      --name=taskmanager \\\n      --network flink-network \\\n      --env FLINK_PROPERTIES=\"${FLINK_PROPERTIES}\" \\\n      flink:1.11.1 taskmanager\n```\n\n----------------------------------------\n\nTITLE: Implementing Unordered Result Checking\nDESCRIPTION: Implementation of custom result checking logic for unordered data streams, overriding the default ordered checking behavior.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-12-howto-test-batch-source.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n@Override\nprotected void checkResultWithSemantic(\n  CloseableIterator<Pojo> resultIterator,\n  List<List<Pojo>> testData,\n  CheckpointingMode semantic,\n  Integer limit) {\n    if (limit != null) {\n      Runnable runnable =\n      () -> CollectIteratorAssertions.assertUnordered(resultIterator)\n        .withNumRecordsLimit(limit)\n        .matchesRecordsFromSource(testData, semantic);\n      assertThat(runAsync(runnable)).succeedsWithin(DEFAULT_COLLECT_DATA_TIMEOUT);\n    } else {\n        CollectIteratorAssertions.assertUnordered(resultIterator)\n                .matchesRecordsFromSource(testData, semantic);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Simple Flink Job for Serialization Benchmarking\nDESCRIPTION: A minimal Flink job configuration used in the benchmarks to test serialization performance. This setup creates a source that generates POJO records, redistributes them across worker nodes (forcing serialization), and discards the results.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_10\n\nLANGUAGE: java\nCODE:\n```\nenv.setParallelism(4);\nenv.addSource(new PojoSource(RECORDS_PER_INVOCATION, 10))\n    .rebalance()\n    .addSink(new DiscardingSink<>());\n```\n\n----------------------------------------\n\nTITLE: Keyed Wrapper Class Definition\nDESCRIPTION: Definition of the Keyed wrapper class used to package events with their dynamic keys and rule IDs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-15-demo-fraud-detection.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic class Keyed<IN, KEY, ID> {\n  private IN wrapped;\n  private KEY key;\n  private ID id;\n\n  ...\n  public KEY getKey(){\n      return key;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Alert Pattern\nDESCRIPTION: Defining pattern for detecting consecutive temperature warnings within a time window.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nPattern<TemperatureWarning, ?> alertPattern = Pattern.<TemperatureWarning>begin(\"First Event\")\n    .next(\"Second Event\")\n    .within(Time.seconds(20));\n```\n\n----------------------------------------\n\nTITLE: Applying WindowAssigner in Apache Flink (Scala)\nDESCRIPTION: This code shows how to apply a WindowAssigner to a keyed stream in Apache Flink, creating a windowed stream.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-04-Introducing-windows.md#2025-04-08_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\n// create windowed stream using a WindowAssigner\nvar windowed: WindowedStream[IN, KEY, WINDOW] = keyed\n  .window(myAssigner: WindowAssigner[IN, WINDOW])\n```\n\n----------------------------------------\n\nTITLE: Configuring Checkpointing Semantics\nDESCRIPTION: Shows how to configure exactly-once semantics for source testing using CheckpointingMode.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-12-howto-test-batch-source.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@TestSemantics\nCheckpointingMode[] semantics = new CheckpointingMode[] {CheckpointingMode.EXACTLY_ONCE};\n```\n\n----------------------------------------\n\nTITLE: RackFun Alert Correlation Implementation\nDESCRIPTION: Code showing how RackFun maintains server incident mappings and correlates alerts across servers within a rack.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-18-statefun.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nprivate final PersistedValue<Map<ServerId, Set<Incident>>> serverIncidents =\n    context.state(\"server-incidents\",\n        TypeName.typeNameOf(new TypeHint<Map<ServerId, Set<Incident>>>(){}));\n```\n\n----------------------------------------\n\nTITLE: Querying Temporal Table Function with SQL\nDESCRIPTION: Demonstrates how to query a temporal table function to get conversion rates at a specific point in time using SQL syntax.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-05-14-temporal-tables.md#2025-04-08_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM Rates('11:00');\n```\n\n----------------------------------------\n\nTITLE: Migrating FlinkDynamoDBStreamsConsumer to DynamoDbStreamsSource in Java\nDESCRIPTION: Example demonstrating migration from FlinkDynamoDBStreamsConsumer to DynamoDbStreamsSource. Shows configuration setup, stream initialization, and watermark assignment for both implementations, including custom deserialization schema requirement.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-11-25-whats-new-aws-connectors-5.0.0.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n// Old FlinkDynamoDBStreamsConsumer to read from stream test stream from TRIM_HORIZON\nProperties consumerConfig = new Properties();\nconsumerConfig.put(AWSConfigConstants.AWS_REGION, \"us-east-1\");\nconsumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \"TRIM_HORIZON\");\nFlinkDynamoDBStreamsConsumer<String> oldDynamodbStreamsConsumer = \n    new FlinkDynamoDBStreamsConsumer<>(\"arn:aws:dynamodb:us-east-1:1231231230:table/test/stream/2024-04-11T07:14:19.380\", new SimpleStringSchema(), consumerConfig);\nDataStream<String> dynamodbRecordsFromOldDynamodbStreamsConsumer = env.addSource(oldDynamodbStreamsConsumer)\n    .uid(\"custom-uid\")\n    .assignTimestampsAndWatermarks(\n        WatermarkStrategy.<String>forMonotonousTimestamps().withIdleness(Duration.ofSeconds(1)))\n\n// New DynamoDbStreamsSource to read from stream test stream from TRIM_HORIZON\nConfiguration sourceConfig = new Configuration();\nsourceConfig.set(DynamodbStreamsSourceConfigConstants.STREAM_INITIAL_POSITION, DynamodbStreamsSourceConfigConstants.InitialPosition.TRIM_HORIZON); \nKinesisStreamsSource<String> newDynamoDbStreamsSource =\n    DynamoDbStreamsSource.<String>builder()\n        .setStreamArn(\"arn:aws:dynamodb:us-east-1:1231231230:table/test/stream/2024-04-11T07:14:19.380\")\n        .setSourceConfig(sourceConfig)\n        // User must implement their own deserialization schema to translate change data capture events into custom data types    \n        .setDeserializationSchema(dynamodbDeserializationSchema) \n        .build();\nDataStream<String> dynamodbRecordsWithEventTimeWatermarks = env.fromSource(\n    newDynamoDbStreamsSource, \n    WatermarkStrategy.<String>forMonotonousTimestamps().withIdleness(Duration.ofSeconds(1)), \n    \"DynamoDB Streams source\")\n        .returns(TypeInformation.of(String.class))\n        .uid(\"custom-uid\");\n```\n\n----------------------------------------\n\nTITLE: Disabling Kryo Generic Types Fallback in Flink Java\nDESCRIPTION: Code showing how to disable the Kryo fallback mechanism for serializing generic types. This is useful for finding and replacing fallbacks with better serializers.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nenv.getConfig().disableGenericTypes();\n```\n\n----------------------------------------\n\nTITLE: Configuring PrometheusReporter in Flink\nDESCRIPTION: YAML configuration for enabling the PrometheusReporter in Flink's flink-conf.yaml file. It sets up the reporter class and port.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-03-11-prometheus-monitoring.md#2025-04-08_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics.reporters: prom\nmetrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter\nmetrics.reporter.prom.port: 9999\n```\n\n----------------------------------------\n\nTITLE: Using Pulsar as Streaming Source and Table Sink for Flink SQL\nDESCRIPTION: This snippet illustrates how to use Apache Pulsar as both a streaming source and a streaming table sink for Flink SQL or Table API queries. It shows registering a DataStream as a table, creating a Pulsar TableSink, and executing a SQL query to count words and write results to Pulsar.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-05-03-pulsar-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// obtain a DataStream with words\nDataStream<String> words = ...\n\n// register DataStream as Table \"words\" with two attributes (\"word\", \"ts\"). \n//   \"ts\" is an event-time timestamp.\ntableEnvironment.registerDataStream(\"words\", words, \"word, ts.rowtime\");\n\n// create a TableSink that produces to Pulsar\nTableSink sink = new PulsarJsonTableSink(\n   serviceUrl,\n   outputTopic,\n   new AuthenticationDisabled(),\n   ROUTING_KEY);\n\n// register Pulsar TableSink as table \"wc\"\ntableEnvironment.registerTableSink(\n   \"wc\",\n   sink.configure(\n      new String[]{\"word\", \"cnt\"},\n      new TypeInformation[]{Types.STRING, Types.LONG}));\n\n// count words per 5 seconds and write result to table \"wc\"\ntableEnvironment.sqlUpdate(\n   \"INSERT INTO wc \" +\n   \"SELECT word, COUNT(*) AS cnt \" +\n   \"FROM words \" +\n   \"GROUP BY word, TUMBLE(ts, INTERVAL '5' SECOND)\");\n```\n\n----------------------------------------\n\nTITLE: Implementing Group By and Reduce Operations in Flink DataStream API\nDESCRIPTION: Showing how to perform group by and reduce (sum) operations in the DataStream API. This involves using keyBy() instead of groupBy() and implementing a custom ReduceFunction for incremental aggregation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-09-howto-migrate-to-datastream.md#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nDataStream<Row> summed = joinedStream\n    .keyBy(\n        new KeySelector<Row, Tuple3<Integer, Integer, String>>() {\n            @Override\n            public Tuple3<Integer, Integer, String> getKey(Row row) throws Exception {\n                return new Tuple3<>(\n                    row.getField(0),  // d_year\n                    row.getField(1),  // i_brand_id\n                    row.getField(2)   // i_brand\n                );\n            }\n        }\n    )\n    .window(GlobalWindows.create())\n    .trigger(PurgingTrigger.of(CountTrigger.of(1)))\n    .reduce(\n        new ReduceFunction<Row>() {\n            @Override\n            public Row reduce(Row row1, Row row2) throws Exception {\n                // Implement custom reduce logic here\n            }\n        }\n    );\n```\n\n----------------------------------------\n\nTITLE: Flink Application Mode with Memory Configuration\nDESCRIPTION: Command to launch a Flink application with specific JobManager and TaskManager memory configurations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-14-application-mode.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/flink run-application -t yarn-application \\\n    -Djobmanager.memory.process.size=2048m \\\n    -Dtaskmanager.memory.process.size=4096m \\\n    ./MyApplication.jar\n```\n\n----------------------------------------\n\nTITLE: Creating User-Song Bipartite Graph in Flink Gelly\nDESCRIPTION: Creates a weighted bipartite graph where users are source vertices, songs are target vertices, and edge weights represent play counts. The graph is constructed from a dataset of valid triplets.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-08-24-introducing-flink-gelly.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// create a user -> song weighted bipartite graph where the edge weights\n// correspond to play counts\nGraph<String, NullValue, Integer> userSongGraph = Graph.fromTupleDataSet(validTriplets, env);\n```\n\n----------------------------------------\n\nTITLE: Implementing Checkpointed Kafka Consumer in Flink\nDESCRIPTION: Implementation of a Flink Kafka consumer that demonstrates operator state management using the CheckpointedFunction interface. The code shows how to initialize and snapshot state for Kafka partition offsets, making them redistributable during rescaling operations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-07-04-flink-rescalable-state.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic class FlinkKafkaConsumer<T> extends RichParallelSourceFunction<T> implements CheckpointedFunction {\n\t // ...\n\n   private transient ListState<Tuple2<KafkaTopicPartition, Long>> offsetsOperatorState;\n\n   @Override\n   public void initializeState(FunctionInitializationContext context) throws Exception {\n\n      OperatorStateStore stateStore = context.getOperatorStateStore();\n      // register the state with the backend\n      this.offsetsOperatorState = stateStore.getSerializableListState(\"kafka-offsets\");\n\n      // if the job was restarted, we set the restored offsets\n      if (context.isRestored()) {\n         for (Tuple2<KafkaTopicPartition, Long> kafkaOffset : offsetsOperatorState.get()) {\n            // ... restore logic\n         }\n      }\n   }\n\n   @Override\n   public void snapshotState(FunctionSnapshotContext context) throws Exception {\n\n      this.offsetsOperatorState.clear();\n\n      // write the partition offsets to the list of operator states\n      for (Map.Entry<KafkaTopicPartition, Long> partition : this.subscribedPartitionOffsets.entrySet()) {\n         this.offsetsOperatorState.add(Tuple2.of(partition.getKey(), partition.getValue()));\n      }\n   }\n\n   // ...\n\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Stateless Map Function in Flink\nDESCRIPTION: Java implementation of a simple stateless Map function in Flink. This function prepends 'hello' to the input string.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-03-a-guide-for-unit-testing-in-apache-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic class MyStatelessMap implements MapFunction<String, String> {\n  @Override\n  public String map(String in) throws Exception {\n    String out = \"hello \" + in;\n    return out;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing IMAP Connector with SQL\nDESCRIPTION: SQL commands to create and query the IMAP source table in Flink SQL client.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE T (\n    subject STRING\n) WITH (\n    'connector' = 'imap',\n    'host' = 'greenmail',\n    'port' = '3143',\n    'user' = 'alice',\n    'password' = 'alice'\n);\n\nSELECT * FROM T;\n```\n\n----------------------------------------\n\nTITLE: Determining Avro Schema Compatibility in Flink\nDESCRIPTION: Code that uses Avro's built-in compatibility checking to determine if schemas are compatible between versions, mapping the Avro compatibility result to Flink's compatibility status.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-29-state-unlocked-interacting-with-state-in-apache-flink.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\n  final SchemaPairCompatibility compatibility = SchemaCompatibility\n    .checkReaderWriterCompatibility(previousSchema, runtimeSchema);\nâ€‹\n    return avroCompatibilityToFlinkCompatibility(compatibility);\n  }\n```\n\n----------------------------------------\n\nTITLE: Data-Driven Windowing with Delta Policy in Java\nDESCRIPTION: Implementation of a delta-based window that triggers computation when stock price changes exceed a 5% threshold. Includes warning generation and count aggregation over 30-second windows.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nprivate static final Double DEFAULT_PRICE = 1000.;\nprivate static final StockPrice DEFAULT_STOCK_PRICE = new StockPrice(\"\", DEFAULT_PRICE);\n\nDataStream<String> priceWarnings = stockStream.groupBy(\"symbol\")\n    .window(Delta.of(0.05, new DeltaFunction<StockPrice>() {\n        @Override\n        public double getDelta(StockPrice oldDataPoint, StockPrice newDataPoint) {\n            return Math.abs(oldDataPoint.price - newDataPoint.price);\n        }\n    }, DEFAULT_STOCK_PRICE))\n.mapWindow(new SendWarning()).flatten();\n\nDataStream<Count> warningsPerStock = priceWarnings.map(new MapFunction<String, Count>() {\n    @Override\n    public Count map(String value) throws Exception {\n        return new Count(value, 1);\n    }\n}).groupBy(\"symbol\").window(Time.of(30, TimeUnit.SECONDS)).sum(\"count\").flatten();\n```\n\n----------------------------------------\n\nTITLE: Creating IMAP Table in SQL\nDESCRIPTION: SQL statements to create a table using the IMAP connector and query it. This example will fail due to missing required options, demonstrating the validation functionality.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE T (subject STRING, content STRING) WITH ('connector' = 'imap');\n\nSELECT * FROM T;\n```\n\n----------------------------------------\n\nTITLE: Removed Method in DataStreamUtils (Java)\nDESCRIPTION: Documentation of a removed method in the DataStreamUtils class. This change likely reflects updates to the utility functions for working with DataStreams in Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\njava.util.Iterator<OUT> collect(org.apache.flink.streaming.api.datastream.DataStream<OUT>)\n```\n\n----------------------------------------\n\nTITLE: Applying Pattern to Event Stream\nDESCRIPTION: Creating a pattern stream from input events keyed by rack ID.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nPatternStream<MonitoringEvent> tempPatternStream = CEP.pattern(\n    inputEventStream.keyBy(\"rackID\"),\n    warningPattern);\n```\n\n----------------------------------------\n\nTITLE: Scala Type Information Example\nDESCRIPTION: Demonstrates how to properly handle type information in Scala when working with generic parameters in Flink operations. Shows the usage of context bounds to ensure TypeInformation is available.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/getting-help.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ndef myFunction[T: TypeInformation](input: DataSet[T]): DataSet[Seq[T]] = {\n  input.reduceGroup( i => i.toSeq )\n}\n```\n\n----------------------------------------\n\nTITLE: Window-Based Aggregations in Scala\nDESCRIPTION: Implements rolling window aggregations on stock price streams using time-based windows. Computes minimum price across all stocks, maximum price per stock, and mean price per stock over 10-second windows sliding every 5 seconds.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n//Define the desired time window\nval windowedStream = stockStream\n  .window(Time.of(10, SECONDS)).every(Time.of(5, SECONDS))\n\n//Compute some simple statistics on a rolling window\nval lowest = windowedStream.minBy(\"price\")\nval maxByStock = windowedStream.groupBy(\"symbol\").maxBy(\"price\")\nval rollingMean = windowedStream.groupBy(\"symbol\").mapWindow(mean _)\n\n//Compute the mean of a window\ndef mean(ts: Iterable[StockPrice], out: Collector[StockPrice]) = {\n  if (ts.nonEmpty) {\n    out.collect(StockPrice(ts.head.symbol, ts.foldLeft(0: Double)(_ + _.price) / ts.size))\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Flink Application Mode Launch Command\nDESCRIPTION: Basic command to launch a Flink application in Application Mode using YARN as the target platform.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-14-application-mode.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./bin/flink run-application -t yarn-application ./MyApplication.jar\n```\n\n----------------------------------------\n\nTITLE: Removed Methods from WindowedStream Class\nDESCRIPTION: Methods that have been removed from the WindowedStream class. These methods were used for configuring windowed operations including allowed lateness and applying functions to windows.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_19\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.streaming.api.datastream.WindowedStream<T,K,W> allowedLateness(org.apache.flink.streaming.api.windowing.time.Time)\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<R> apply(org.apache.flink.api.common.functions.ReduceFunction<T>, org.apache.flink.streaming.api.functions.windowing.WindowFunction<T,R,K,W>)\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<R> apply(org.apache.flink.api.common.functions.ReduceFunction<T>, org.apache.flink.streaming.api.functions.windowing.WindowFunction<T,R,K,W>, org.apache.flink.api.common.typeinfo.TypeInformation<R>)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating PLAN_ADVICE for Non-Deterministic Updates in Flink SQL\nDESCRIPTION: Example of Flink 1.17's new PLAN_ADVICE feature that detects potential correctness risks in SQL queries. This example shows a warning for non-deterministic columns that could affect update message processing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-03-23-release-1.17.0.md#2025-04-08_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n== Optimized Physical Plan With Advice ==\n...\n\nadvice[1]: [WARNING] The column(s): day(generated by non-deterministic function: CURRENT_TIMESTAMP ) can not satisfy the determinism requirement for correctly processing update message('UB'/'UA'/'D' in changelogMode, not 'I' only), this usually happens when input node has no upsertKey(upsertKeys=[{}]) or current node outputs non-deterministic update messages. Please consider removing these non-deterministic columns or making them deterministic by using deterministic functions.\n```\n\n----------------------------------------\n\nTITLE: Configuring Retained Checkpoints\nDESCRIPTION: This code snippet shows the configuration flag used to set the number of completed checkpoints retained by Flink. By default, Flink retains 1 completed checkpoint.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-01-30-incremental-checkpointing.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nstate.checkpoints.num-retained\n```\n\n----------------------------------------\n\nTITLE: Defining Transform UDF in YAML Pipeline\nDESCRIPTION: YAML configuration showing how to declare and use custom User Defined Functions (UDFs) in transform operations within a CDC pipeline.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-09-05-release-cdc-3.2.0.md#2025-04-08_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  - source-table: db.tbl\n    projection: \"fmt('id -> %d', id) as fmt_id\"\n    filter: \"inc(id) < 100\"\n\npipeline:\n  user-defined-function:\n    - name: inc\n      classpath: org.apache.flink.cdc.udf.examples.java.AddOneFunctionClass\n    - name: fmt\n      classpath: org.apache.flink.cdc.udf.examples.java.FormatFunctionClass\n```\n\n----------------------------------------\n\nTITLE: Checking KUDO Plan Status for Kafka\nDESCRIPTION: Command to check the status of the Kafka deployment plan, which is a dependency of the financial fraud demo.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl kudo plan status --instance flink-demo-kafka\nPlan(s) for \"flink-demo-kafka\" in namespace \"default\":\n.\nâ””â”€â”€ flink-demo-kafka (Operator-Version: \"kafka-1.2.0\" Active-Plan: \"deploy\")\n\tâ”œâ”€â”€ Plan deploy (serial strategy) [IN_PROGRESS]\n\tâ”‚   â””â”€â”€ Phase deploy-kafka [IN_PROGRESS]\n\tâ”‚   \tâ””â”€â”€ Step deploy (IN_PROGRESS)\n\tâ””â”€â”€ Plan not-allowed (serial strategy) [NOT ACTIVE]\n    \tâ””â”€â”€ Phase not-allowed (serial strategy) [NOT ACTIVE]\n        \tâ””â”€â”€ Step not-allowed (serial strategy) [NOT ACTIVE]\n            \tâ””â”€â”€ not-allowed [NOT ACTIVE]\n```\n\n----------------------------------------\n\nTITLE: Removed Methods from KeyedStream Class\nDESCRIPTION: Time window methods that have been removed from the KeyedStream class. These methods were used for creating windowed streams based on time characteristics.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_17\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.streaming.api.datastream.WindowedStream<T,KEY,org.apache.flink.streaming.api.windowing.windows.TimeWindow> timeWindow(org.apache.flink.streaming.api.windowing.time.Time)\norg.apache.flink.streaming.api.datastream.WindowedStream<T,KEY,org.apache.flink.streaming.api.windowing.windows.TimeWindow> timeWindow(org.apache.flink.streaming.api.windowing.time.Time, org.apache.flink.streaming.api.windowing.time.Time)\n```\n\n----------------------------------------\n\nTITLE: Implementing ScanTableSource for Dynamic Table in Java\nDESCRIPTION: Implementation of a scan table source that defines changelog mode, runtime provider, and basic source functionality. The source is configured to only insert new rows and provides a bounded data stream.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part1.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.table.connector.ChangelogMode;\nimport org.apache.flink.table.connector.source.DynamicTableSource;\nimport org.apache.flink.table.connector.source.ScanTableSource;\nimport org.apache.flink.table.connector.source.SourceFunctionProvider;\n\npublic class ImapTableSource implements ScanTableSource {\n  @Override\n  public ChangelogMode getChangelogMode() {\n    return ChangelogMode.insertOnly();\n  }\n\n  @Override\n  public ScanRuntimeProvider getScanRuntimeProvider(ScanContext ctx) {\n    boolean bounded = true;\n    final ImapSource source = new ImapSource();\n    return SourceFunctionProvider.of(source, bounded);\n  }\n\n  @Override\n  public DynamicTableSource copy() {\n    return new ImapTableSource();\n  }\n\n  @Override\n  public String asSummaryString() {\n    return \"IMAP Table Source\";\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Apache Flink Java Dependencies to Maven POM\nDESCRIPTION: This XML snippet shows how to add Apache Flink Java dependencies to a Maven project's pom.xml file. It includes core Flink libraries for Java API, streaming, and client functionality.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/downloads.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>{{< param FlinkStableVersion >}}</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>{{< param FlinkStableVersion >}}</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>{{< param FlinkStableVersion >}}</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Setting Python Requirements in PyFlink\nDESCRIPTION: Shows how to configure Python dependencies in the table environment.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-09-pyflink-udf-support-flink.md#2025-04-08_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nt_env.set_python_requirements(\"/tmp/requirements.txt\", \"/tmp/cached_dir\")\n```\n\n----------------------------------------\n\nTITLE: Executing Streaming ETL in Flink SQL\nDESCRIPTION: INSERT statement to perform streaming ETL by moving data from source to sink table with transformation logic.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-06-15-flink-on-zeppelin-part1.md#2025-04-08_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO sink_table\nSELECT id, name, age\nFROM source_table\nWHERE age > 18\n```\n\n----------------------------------------\n\nTITLE: Removed Methods in AllWindowedStream, CoGroupedStreams$WithWindow, and DataStream (Java)\nDESCRIPTION: Documentation of removed methods in various stream processing classes. These changes likely reflect updates to the windowing, grouping, and general stream processing APIs in Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.flink.streaming.api.datastream.AllWindowedStream<T,W> allowedLateness(org.apache.flink.streaming.api.windowing.time.Time)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<R> apply(org.apache.flink.api.common.functions.ReduceFunction<T>, org.apache.flink.streaming.api.functions.windowing.AllWindowFunction<T,R,W>)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.flink.streaming.api.datastream.DataStreamSink<T> addSink(org.apache.flink.streaming.api.functions.sink.SinkFunction<T>)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<T> assignTimestampsAndWatermarks(org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks<T>)\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Query on Streams\nDESCRIPTION: This snippet demonstrates how to execute a SQL query on streaming data using Flink's SQL support in version 1.1.0.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-08-04-release-1.1.0.md#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nTable result = tableEnv.sql(\n  \"SELECT STREAM product, amount FROM Orders WHERE product LIKE '%Rubber%'\");\n```\n\n----------------------------------------\n\nTITLE: Operator Lifecycle Method Renaming in Apache Flink\nDESCRIPTION: Definition of renamed operator lifecycle methods in Flink to better reflect their purposes. The close() method was renamed to finish() for normal termination, and dispose() was renamed to close() for resource cleanup.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-07-11-final-checkpoint-part1.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Old methods:\n// close() - called only on normal task finish\n// dispose() - called on both normal finish and failover\n\n// New methods:\nfinish() // Marks operator termination, no more records allowed after this\nclose() // Cleanup and release all held resources\n```\n\n----------------------------------------\n\nTITLE: Establishing IMAP Server Connection in Java\nDESCRIPTION: Implementation of connect() method to establish connection with IMAP server and open the INBOX folder.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nimport jakarta.mail.*;\nimport com.sun.mail.imap.IMAPFolder;\nimport org.apache.flink.streaming.api.functions.source.RichSourceFunction;\nimport org.apache.flink.table.data.RowData;\n\npublic class ImapSource extends RichSourceFunction<RowData> {\n    // â€¦\n\n    private transient Store store;\n    private transient IMAPFolder folder;\n\n    private void connect() throws Exception {\n        final Session session = Session.getInstance(getSessionProperties(), null);\n        store = session.getStore();\n        store.connect(options.getUser(), options.getPassword());\n\n        final Folder genericFolder = store.getFolder(\"INBOX\");\n        folder = (IMAPFolder) genericFolder;\n\n        if (!folder.isOpen()) {\n            folder.open(Folder.READ_ONLY);\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Pattern Detection with Aggregations in Flink 1.8.0\nDESCRIPTION: An example of SQL MATCH_RECOGNIZE clause with aggregation functions for complex CEP (Complex Event Processing) definitions. The query detects patterns where the average price of 'A' events is less than 15, followed by a 'B' event.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-04-09-release-1.8.0.md#2025-04-08_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT *\nFROM Ticker\n    MATCH_RECOGNIZE (\n        ORDER BY rowtime\n        MEASURES\n            AVG(A.price) AS avgPrice\n        ONE ROW PER MATCH\n        AFTER MATCH SKIP TO FIRST B\n        PATTERN (A+ B)\n        DEFINE\n            A AS AVG(A.price) < 15\n    ) MR;\n```\n\n----------------------------------------\n\nTITLE: Removed Interface in Path (Java)\nDESCRIPTION: Documentation of a removed interface implementation in the Path class. The IOReadableWritable interface was likely removed or replaced with a different mechanism for serialization.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.flink.core.io.IOReadableWritable\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing Docker Image\nDESCRIPTION: Bash commands to build the custom Flink Docker image and push it to a remote image repository. This step is necessary before deploying the Flink application on Kubernetes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-02-10-native-k8s-with-ha.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ docker build -t <user-image> .\n\n$ docker push <user-image>\n```\n\n----------------------------------------\n\nTITLE: Beam ParDo Transform Implementation in Flink\nDESCRIPTION: ParDo transform implementation using FlinkDoFnFunction for batch and DoFnOperator for streaming, handling checkpointing, watermarks, and state management through the DoFnRunner interface.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-22-apache-beam-how-beam-runs-on-top-of-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nParDo\n```\n\n----------------------------------------\n\nTITLE: Implementing Window Mean Function in Java\nDESCRIPTION: A WindowMapFunction implementation that computes the mean price of stock values within a time window. Takes StockPrice objects as input and outputs averaged StockPrice objects.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\npublic final static class WindowMean implements WindowMapFunction<StockPrice, StockPrice> {\n    private Double sum = 0.0;\n    private Integer count = 0;\n    private String symbol = \"\";\n\n    @Override\n    public void mapWindow(Iterable<StockPrice> values, Collector<StockPrice> out) throws Exception {\n        if (values.iterator().hasNext()) {\n            for (StockPrice sp : values) {\n                sum += sp.price;\n                symbol = sp.symbol;\n                count++;\n            }\n            out.collect(new StockPrice(symbol, sum / count));\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Building and Running IMAP Connector in Shell\nDESCRIPTION: Shell command to build and run the IMAP connector implementation for testing purposes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n$ cd testing/\n$ ./build_and_run.sh\n```\n\n----------------------------------------\n\nTITLE: Removed Method in FileSystem (Java)\nDESCRIPTION: Documentation of a removed method in the FileSystem class. This method was likely replaced or deprecated in favor of a new implementation for determining the file system kind.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.flink.core.fs.FileSystemKind getKind()\n```\n\n----------------------------------------\n\nTITLE: Creating Alert Pattern Stream\nDESCRIPTION: Applying alert pattern to warning stream keyed by rack ID.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nPatternStream<TemperatureWarning> alertPatternStream = CEP.pattern(\n    warnings.keyBy(\"rackID\"),\n    alertPattern);\n```\n\n----------------------------------------\n\nTITLE: Beam GroupByKey Transform with Flink\nDESCRIPTION: GroupByKey transform for data partitioning using KV<Key, Value> format, similar to Flink's keyBy().window() operation but with Beam-specific window functions and triggers.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-22-apache-beam-how-beam-runs-on-top-of-flink.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nGroupByKey\n```\n\n----------------------------------------\n\nTITLE: Reading Multiple Stock Price Streams in Scala\nDESCRIPTION: Creates a stream of stock prices by reading from a socket stream and generating additional stock streams. Parses text input, creates StockPrice objects, and merges streams from different sources.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ndef main(args: Array[String]) {\n\n  val env = StreamExecutionEnvironment.getExecutionEnvironment\n\n  //Read from a socket stream at map it to StockPrice objects\n  val socketStockStream = env.socketTextStream(\"localhost\", 9999).map(x => {\n    val split = x.split(\",\")\n    StockPrice(split(0), split(1).toDouble)\n  })\n\n  //Generate other stock streams\n  val SPX_Stream = env.addSource(generateStock(\"SPX\")(10) _)\n  val FTSE_Stream = env.addSource(generateStock(\"FTSE\")(20) _)\n  val DJI_Stream = env.addSource(generateStock(\"DJI\")(30) _)\n  val BUX_Stream = env.addSource(generateStock(\"BUX\")(40) _)\n\n  //Merge all stock streams together\n  val stockStream = socketStockStream.merge(SPX_Stream, FTSE_Stream, \n    DJI_Stream, BUX_Stream)\n\n  stockStream.print()\n\n  env.execute(\"Stock stream\")\n}\n```\n\n----------------------------------------\n\nTITLE: Removed Methods in FailureEnricher$Context (Java)\nDESCRIPTION: Documentation of removed methods in the FailureEnricher$Context interface. These methods were likely replaced or deprecated in favor of new implementations for accessing job information.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.flink.api.common.JobID getJobId()\n```\n\nLANGUAGE: Java\nCODE:\n```\njava.lang.String getJobName()\n```\n\n----------------------------------------\n\nTITLE: SQL Table Creation with Pulsar Connector (2.7.0)\nDESCRIPTION: Example of creating a Flink SQL table using the Pulsar connector in version 2.7.0. Shows the new simplified parameter naming convention without connector prefix.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-07-pulsar-flink-connector-270.md#2025-04-08_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table topic1(\n    `rip` VARCHAR,\n    `rtime` VARCHAR,\n    `uid` bigint,\n    `client_ip` VARCHAR,\n    `day` as TO_DATE(rtime),\n    `hour` as date_format(rtime,'HH')\n) with (\n    'connector' ='pulsar',\n    'topic' ='persistent://public/default/test_flink_sql',\n    'service-url' ='pulsar://xxx',\n    'admin-url' ='http://xxx',\n    'scan.startup.mode' ='earliest',\n    'properties.pulsar.reader.readername' = 'testReaderName',\n    'format' ='json'\n);\n```\n\n----------------------------------------\n\nTITLE: Removed Type Serializer Method in Apache Flink API\nDESCRIPTION: Details of a removed serializer creation method that was used to create a legacy Row type serializer.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.api.common.typeutils.TypeSerializer<org.apache.flink.types.Row> createLegacySerializer(org.apache.flink.api.common.serialization.SerializerConfig)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Metrics in Flink MapFunction\nDESCRIPTION: Java code snippet showing how to add custom metrics to a Flink MapFunction. It demonstrates creating a counter metric for events processed.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-03-11-prometheus-monitoring.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nclass FlinkMetricsExposingMapFunction extends RichMapFunction<Integer, Integer> {\n  private transient Counter eventCounter;\n\n  @Override\n  public void open(Configuration parameters) {\n    eventCounter = getRuntimeContext().getMetricGroup().counter(\"events\");\n  }\n\n  @Override\n  public Integer map(Integer value) {\n    eventCounter.inc();\n    return value;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Modified Methods in JoinedStreams.WithWindow Class\nDESCRIPTION: Methods in JoinedStreams.WithWindow class that have been modified. The return type has changed from DataStream to SingleOutputStreamOperator, indicating a refinement in the API design.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_16\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<T><T> (<-org.apache.flink.streaming.api.datastream.DataStream<T><T>) apply(org.apache.flink.api.common.functions.JoinFunction<T1,T2,T><T1,T2,T>)\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<T><T> (<-org.apache.flink.streaming.api.datastream.DataStream<T><T>) apply(org.apache.flink.api.common.functions.FlatJoinFunction<T1,T2,T><T1,T2,T>, org.apache.flink.api.common.typeinfo.TypeInformation<T><T>)\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<T><T> (<-org.apache.flink.streaming.api.datastream.DataStream<T><T>) apply(org.apache.flink.api.common.functions.FlatJoinFunction<T1,T2,T><T1,T2,T>)\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<T><T> (<-org.apache.flink.streaming.api.datastream.DataStream<T><T>) apply(org.apache.flink.api.common.functions.JoinFunction<T1,T2,T><T1,T2,T>, org.apache.flink.api.common.typeinfo.TypeInformation<T><T>)\n```\n\n----------------------------------------\n\nTITLE: Configuring Flink Test Environment in Integration Tests\nDESCRIPTION: Shows how to set up the Flink test environment using MiniClusterTestEnvironment for integration testing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-12-howto-test-batch-source.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n@TestEnv\nMiniClusterTestEnvironment flinkTestEnvironment = new MiniClusterTestEnvironment();\n```\n\n----------------------------------------\n\nTITLE: Setting Batch Execution Mode Programmatically in DataStream API\nDESCRIPTION: Java code snippet demonstrating how to configure the DataStream API to use BATCH execution mode programmatically. This allows for efficient processing of bounded data streams without using the legacy DataSet API.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-12-10-release-1.12.0.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nenv.setRuntimeMode(RuntimeMode.BATCH);\n```\n\n----------------------------------------\n\nTITLE: Removed Method from KeyedStream.IntervalJoin Class\nDESCRIPTION: The between method has been removed from the KeyedStream.IntervalJoin class. This method was used for defining time intervals for joining streams.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_18\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.streaming.api.datastream.KeyedStream$IntervalJoined<T1,T2,KEY> between(org.apache.flink.streaming.api.windowing.time.Time, org.apache.flink.streaming.api.windowing.time.Time)\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Compose Environment\nDESCRIPTION: Command to tear down the Docker Compose setup for Flink and Prometheus after usage.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-03-11-prometheus-monitoring.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew composeDown\n```\n\n----------------------------------------\n\nTITLE: Flink Application Mode with Remote Library Directory\nDESCRIPTION: Launch command that includes configuration for using pre-uploaded Flink distribution from HDFS to optimize bandwidth usage.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-14-application-mode.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/flink run-application -t yarn-application \\\n    -Djobmanager.memory.process.size=2048m \\\n    -Dtaskmanager.memory.process.size=4096m \\\n    -Dyarn.provided.lib.dirs=\"hdfs://myhdfs/remote-flink-dist-dir\" \\\n    ./MyApplication.jar\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Value Lowercase Function in Java UDF\nDESCRIPTION: Java implementation of a Scalar Function UDF that processes JSON strings using Jackson ObjectMapper. Includes initialization logic and proper error handling for JSON processing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-06-pyflink-1.15-thread-mode.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic class JsonValueLower extends ScalarFunction {\n    private transient ObjectMapper mapper;\n    private transient ObjectWriter writer;\n\n    @Override\n    public void open(FunctionContext context) throws Exception {\n        this.mapper = new ObjectMapper();\n        this.writer = mapper.writerWithDefaultPrettyPrinter();\n    }\n\n    public String eval(String s) {\n        try {\n            StringObject object = mapper.readValue(s, StringObject.class);\n            object.setA(object.a.toLowerCase());\n            return writer.writeValueAsString(object);\n        } catch (JsonProcessingException e) {\n            throw new RuntimeException(\"Failed to read json value\", e);\n        }\n    }\n\n    private static class StringObject {\n        private String a;\n\n        public String getA() {\n            return a;\n        }\n\n        public void setA(String a) {\n            this.a = a;\n        }\n\n        @Override\n        public String toString() {\n            return \"StringObject{\" + \"a='\" + a + '\\'' + '}';\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Removed Methods in CommittableMessage, CommittableSummary, and CommittableWithLineage (Java)\nDESCRIPTION: Documentation of removed methods and constructors in committable-related classes. These changes likely reflect updates to the checkpointing and commit mechanisms in Flink's sink API.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\njava.util.OptionalLong getCheckpointId()\n```\n\nLANGUAGE: Java\nCODE:\n```\nCommittableSummary(int, int, java.lang.Long, int, int, int)\n```\n\nLANGUAGE: Java\nCODE:\n```\nCommittableWithLineage(java.lang.Object, java.lang.Long, int)\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Module Name in Flink\nDESCRIPTION: Configuration example showing how to override the default remote module file name in flink-conf.yaml\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-31-release-statefun-3.2.0.md#2025-04-08_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nstatefun.remote.module-name: /flink/usrlib/prod.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining RateLimitingStrategy Interface in Java\nDESCRIPTION: This code snippet shows the interface definition for RateLimitingStrategy, which includes methods for registering requests and controlling the sink's request rate.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-11-25-async-sink-rate-limiting-strategy.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic interface RateLimitingStrategy {\n\n    // Information provided to the RateLimitingStrategy\n    void registerInFlightRequest(RequestInfo requestInfo);\n    void registerCompletedRequest(ResultInfo resultInfo);\n    \n    // Controls offered to the RateLimitingStrategy\n    boolean shouldBlock(RequestInfo requestInfo);\n    int getMaxBatchSize();\n    \n}\n```\n\n----------------------------------------\n\nTITLE: Removed ConfigConstants Fields in Apache Flink\nDESCRIPTION: Extensive list of removed constant fields from the ConfigConstants class, including configuration options for ZooKeeper, Akka, JobManager, TaskManager, Mesos, YARN, and various other system components.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_LEADER_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\ndouble DEFAULT_AKKA_WATCH_THRESHOLD\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_JOB_MANAGER_IPC_PORT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_TMPDIR_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_TASK_MANAGER_MEMORY_SEGMENT_SIZE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_SCOPE_NAMING_TASK\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_NAMESPACE_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_AKKA_DISPATCHER_THROUGHPUT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String RESTART_STRATEGY_FIXED_DELAY_ATTEMPTS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String MESOS_MASTER_URL\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String FLINK_BASE_DIR_PATH_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_SSL_ENABLED\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String YARN_APPLICATION_TAGS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HDFS_SITE_CONFIG\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String EXECUTION_RETRY_DELAY_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_MESOS_ARTIFACT_SERVER_PORT\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_SECURITY_SSL_VERIFY_HOSTNAME\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String CONTAINERIZED_HEAP_CUTOFF_MIN\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String YARN_HEARTBEAT_DELAY_SECONDS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String AKKA_SSL_ENABLED\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_MODE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_MESOS_WORKERS_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_ZOOKEEPER_SASL_DISABLE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_SCOPE_DELIMITER\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String LOCAL_NUMBER_RESOURCE_MANAGER\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String AKKA_TCP_TIMEOUT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_SCOPE_NAMING_OPERATOR\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_RECOVERY_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_ZOOKEEPER_LEADER_PORT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_ZOOKEEPER_LATCH_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_ZOOKEEPER_PEER_PORT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_SCOPE_NAMING_TM_JOB\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_NUM_SAMPLES\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_SESSION_TIMEOUT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String FLINK_JVM_OPTIONS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_CHECKPOINT_COUNTER_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_SCOPE_NAMING_JM\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_YARN_JOB_MANAGER_PORT\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_JOB_MANAGER_WEB_CHECKPOINTS_DISABLE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_QUORUM_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_JOB_MANAGER_WEB_SUBMIT_ENABLED\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_CHECKPOINTS_HISTORY_SIZE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_JOBGRAPHS_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_SASL_SERVICE_NAME\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_AKKA_LOOKUP_TIMEOUT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String RESTART_STRATEGY_FAILURE_RATE_MAX_FAILURES_PER_INTERVAL\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_PORT_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_LATENCY_HISTORY_SIZE\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_BLOB_FETCH_BACKLOG\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_BACK_PRESSURE_REFRESH_INTERVAL\n```\n\nLANGUAGE: java\nCODE:\n```\nfloat DEFAULT_SORT_SPILLING_THRESHOLD\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_AKKA_TRANSPORT_HEARTBEAT_INTERVAL\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String CONTAINERIZED_MASTER_ENV_PREFIX\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_JOB_MANAGER_WEB_ARCHIVE_COUNT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_HOSTNAME_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String AKKA_WATCH_HEARTBEAT_INTERVAL\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_TASK_MANAGER_TMP_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_EXECUTION_RETRIES\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_JOB_MANAGER_WEB_FRONTEND_PORT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_LOG_PATH_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_MEMORY_SIZE_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_NAME\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_DATA_PORT_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_CHECKPOINTS_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_JOB_MANAGER_PORT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_REFUSED_REGISTRATION_PAUSE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String CONTAINERIZED_HEAP_CUTOFF_RATIO\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_SORT_SPILLING_THRESHOLD_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String YARN_CONTAINER_START_COMMAND_TEMPLATE\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_JOB_MANAGER_WEB_SSL_ENABLED\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String LIBRARY_CACHE_MANAGER_CLEANUP_INTERVAL\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_CHECKPOINTS_DISABLE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_ZOOKEEPER_LEADER_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_DELAY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_TASK_MANAGER_MAX_REGISTRATION_PAUSE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_REPORTERS_LIST\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_RECOVERY_MODE\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_METRICS_LATENCY_HISTORY_SIZE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_INITIAL_REGISTRATION_PAUSE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_ROLE\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_JOB_MANAGER_WEB_CHECKPOINTS_HISTORY_SIZE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String YARN_PROPERTIES_FILE_LOCATION\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String RECOVERY_JOB_MANAGER_PORT\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_SECURITY_SSL_ENABLED\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String MESOS_FAILOVER_TIMEOUT_SECONDS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String RUNTIME_HASH_JOIN_BLOOM_FILTERS_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_LEADER_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_MAX_RETRY_ATTEMPTS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_CHECKPOINTS_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String MESOS_RESOURCEMANAGER_FRAMEWORK_ROLE\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_REFRESH_INTERVAL\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_ZOOKEEPER_MESOS_WORKERS_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_IPC_PORT_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String AKKA_WATCH_HEARTBEAT_PAUSE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String MESOS_RESOURCEMANAGER_FRAMEWORK_NAME\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DELIMITED_FORMAT_MAX_SAMPLE_LENGTH_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String STATE_BACKEND\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String MESOS_RESOURCEMANAGER_FRAMEWORK_PRINCIPAL\n```\n\nLANGUAGE: java\nCODE:\n```\nlong DEFAULT_TASK_MANAGER_DEBUG_MEMORY_USAGE_LOG_INTERVAL_MS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_AKKA_CLIENT_TIMEOUT\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_SPILLING_MAX_FAN\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_IPC_PORT_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_MEMORY_OFF_HEAP_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_FILESYSTEM_OVERWRITE\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_USE_LARGE_RECORD_HANDLER\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_JOBGRAPHS_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_BLOB_SERVICE_SSL_ENABLED\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_SESSION_TIMEOUT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_NETWORK_DEFAULT_IO_MODE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String SECURITY_SSL_TRUSTSTORE_PASSWORD\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_ZOOKEEPER_MAX_RETRY_ATTEMPTS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String AKKA_STARTUP_TIMEOUT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_TMP_DIR_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String USE_LARGE_RECORD_HANDLER_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_ZOOKEEPER_DIR_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_YARN_MIN_HEAP_CUTOFF\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_DATA_SSL_ENABLED\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HDFS_DEFAULT_CONFIG\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_TASK_MANAGER_DATA_SSL_ENABLED\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_ZOOKEEPER_JOBGRAPHS_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_MESOS_WORKERS_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String BLOB_STORAGE_DIRECTORY_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_STATE_BACKEND\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_RETRY_WAIT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String AKKA_ASK_TIMEOUT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_SUBMIT_ENABLED_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_ZOOKEEPER_NAMESPACE_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_ZOOKEEPER_CHECKPOINTS_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_LOCAL_NUMBER_JOB_MANAGER\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String AKKA_TRANSPORT_HEARTBEAT_INTERVAL\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_ZOOKEEPER_CHECKPOINT_COUNTER_PATH\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String FS_STREAM_OPENING_TIMEOUT_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String SECURITY_SSL_TRUSTSTORE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_SCOPE_NAMING_JM_JOB\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String MESOS_INITIAL_TASKS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String AKKA_FRAMESIZE\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_ZOOKEEPER_INIT_LIMIT\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_BACK_PRESSURE_CLEAN_UP_INTERVAL\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String SECURITY_SSL_KEYSTORE\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_MESOS_ARTIFACT_SERVER_SSL_ENABLED\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_MAX_RETRY_ATTEMPTS\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_PARALLELISM\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String RECOVERY_MODE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String EXECUTION_RETRIES_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_REPORTER_SCOPE_DELIMITER\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String LOCAL_START_WEBSERVER\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String LOCAL_NUMBER_JOB_MANAGER\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String RESTART_STRATEGY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String ZOOKEEPER_QUORUM_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_MESOS_FAILOVER_TIMEOUT_SECS\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_TASK_MANAGER_MEMORY_PRE_ALLOCATE\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_LOCAL_NUMBER_RESOURCE_MANAGER\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_CLIENT_ACL\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String METRICS_REPORTER_FACTORY_CLASS_SUFFIX\n```\n\nLANGUAGE: java\nCODE:\n```\nboolean DEFAULT_FILESYSTEM_ALWAYS_CREATE_DIRECTORY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String BLOB_FETCH_CONCURRENT_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String FILESYSTEM_DEFAULT_OVERWRITE_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String RESOURCE_MANAGER_IPC_PORT_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_AKKA_ASK_TIMEOUT\n```\n\nLANGUAGE: java\nCODE:\n```\nint DEFAULT_ZOOKEEPER_CLIENT_PORT\n```\n\nLANGUAGE: java\nCODE:\n```\ndouble DEFAULT_AKKA_TRANSPORT_THRESHOLD\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_AKKA_FRAMESIZE\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_NUM_TASK_SLOTS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String YARN_APPLICATION_MASTER_ENV_PREFIX\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String JOB_MANAGER_WEB_BACK_PRESSURE_DELAY\n```\n\nLANGUAGE: java\nCODE:\n```\nlong DEFAULT_TASK_CANCELLATION_INTERVAL_MILLIS\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_MEMORY_PRE_ALLOCATE_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String FILESYSTEM_SCHEME\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String TASK_MANAGER_MAX_REGISTRATION_DURATION\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String HA_ZOOKEEPER_DIR_KEY\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_USER\n```\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_FILESYSTEM_SCHEME\n```\n\n----------------------------------------\n\nTITLE: Starting Flink Application Cluster on Kubernetes\nDESCRIPTION: Flink CLI command to start a Flink application cluster on Kubernetes with High Availability enabled. This command specifies various Kubernetes and Flink configuration options.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-02-10-native-k8s-with-ha.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ ./bin/flink run-application \\\n    --detached \\\n    --parallelism 4 \\\n    --target kubernetes-application \\\n    -Dkubernetes.cluster-id=k8s-ha-app-1 \\\n    -Dkubernetes.container.image=<user-image> \\\n    -Dkubernetes.jobmanager.cpu=0.5 \\\n    -Dkubernetes.taskmanager.cpu=0.5 \\\n    -Dtaskmanager.numberOfTaskSlots=4 \\\n    -Dkubernetes.rest-service.exposed.type=NodePort \\\n    -Dhigh-availability=org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory \\\n    -Dhigh-availability.storageDir=s3://flink-bucket/flink-ha \\\n    -Drestart-strategy=fixed-delay \\\n    -Drestart-strategy.fixed-delay.attempts=10 \\\n    -Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.12.1.jar \\\n    -Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.12.1.jar \\\n    local:///opt/flink/usrlib/my-flink-job.jar\n```\n\n----------------------------------------\n\nTITLE: Creating Dockerfile for Flink Application Image\nDESCRIPTION: Dockerfile to build a custom Flink image with the user's Flink job JAR file included. This image will be used to deploy the Flink application on Kubernetes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-02-10-native-k8s-with-ha.md#2025-04-08_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM flink:1.12.1\nRUN mkdir -p $FLINK_HOME/usrlib\nCOPY /path/of/my-flink-job.jar $FLINK_HOME/usrlib/my-flink-job.jar\n```\n\n----------------------------------------\n\nTITLE: Generating Temperature Warnings\nDESCRIPTION: Processing matched events to generate temperature warnings using pattern select function.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nDataStream<TemperatureWarning> warnings = tempPatternStream.select(\n    (Map<String, MonitoringEvent> pattern) -> {\n        TemperatureEvent first = (TemperatureEvent) pattern.get(\"First Event\");\n        TemperatureEvent second = (TemperatureEvent) pattern.get(\"Second Event\");\n\n        return new TemperatureWarning(\n            first.getRackID(), \n            (first.getTemperature() + second.getTemperature()) / 2);\n    }\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Parallelism in Flink SQL DDL and Query\nDESCRIPTION: Demonstrates how to set custom parallelism for performance tuning using the scan.parallelism option in both table DDL and dynamic table options.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE Orders (\n    order_number BIGINT,\n    price        DECIMAL(32,2),\n    buyer        ROW<first_name STRING, last_name STRING>,\n    order_time   TIMESTAMP(3)\n) WITH (\n    'connector' = 'datagen',\n    'scan.parallelism' = '4'\n);\n\nSELECT * FROM Orders /*+ OPTIONS('scan.parallelism'='4') */;\n```\n\n----------------------------------------\n\nTITLE: Distributed Table Creation with Bucketing in Flink SQL\nDESCRIPTION: Demonstrates different ways to create distributed tables using the DISTRIBUTED BY clause, including hash distribution, custom distribution, and bucket specifications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-08-02-release-1.20.0.md#2025-04-08_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE MyTable (uid BIGINT, name STRING) DISTRIBUTED BY HASH(uid) INTO 4 BUCKETS;\n\nCREATE TABLE MyTable (uid BIGINT, name STRING) DISTRIBUTED BY (uid) INTO 4 BUCKETS;\n\nCREATE TABLE MyTable (uid BIGINT, name STRING) DISTRIBUTED BY (uid);\n\nCREATE TABLE MyTable (uid BIGINT, name STRING) DISTRIBUTED INTO 4 BUCKETS;\n```\n\n----------------------------------------\n\nTITLE: Using sun.misc.Unsafe for Memory Access\nDESCRIPTION: Shows the method signature of sun.misc.Unsafe.getLong that can be used to access both heap and off-heap memory by manipulating object references and offsets.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-09-16-off-heap-memory.md#2025-04-08_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsun.misc.Unsafe.getLong(Object reference, long offset)\n```\n\n----------------------------------------\n\nTITLE: Taking Savepoints with Specific Format in Flink\nDESCRIPTION: Commands to trigger savepoints in either native or canonical format. Native format leverages the state backend's own format (like RocksDB SST files) for better performance, while remaining relocatable and self-contained.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-06-restore-modes.md#2025-04-08_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# take an intermediate savepoint\n$ bin/flink savepoint --type [native/canonical] :jobId [:targetDirectory]\n\n# stop the job with a savepoint\n$ bin/flink stop --type [native/canonical] --savepointPath [:targetDirectory] :jobId\n```\n\n----------------------------------------\n\nTITLE: Configuring Deployment Mode in FlinkDeployment YAML\nDESCRIPTION: This YAML snippet demonstrates how to set the deployment mode (native or standalone) in the FlinkDeployment resource specification. The mode is specified using the 'mode' field in the deployment spec.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-10-07-release-kubernetes-operator-1.2.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: flink.apache.org/v1beta1\nkind: FlinkDeployment\n...\nspec:\n  ...\n  mode: native/standalone\n```\n\n----------------------------------------\n\nTITLE: Initializing Keyed Stream in Apache Flink (Scala)\nDESCRIPTION: This snippet demonstrates how to create a keyed stream from an input DataStream using a key selector function in Apache Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-04-Introducing-windows.md#2025-04-08_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval input: DataStream[IN] = ...\n\n// created a keyed stream using a key selector function\nval keyed: KeyedStream[IN, KEY] = input\n  .keyBy(myKeySel: (IN) => KEY)\n```\n\n----------------------------------------\n\nTITLE: Creating Aggregation Table in Flink SQL\nDESCRIPTION: SQL statement to create an aggregation table with max and sum aggregate functions on price and sales fields respectively, using product_id as primary key.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-01-13-release-table-store-0.3.0.md#2025-04-08_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE MyTable (\n    product_id BIGINT,\n    price DOUBLE,\n    sales BIGINT,\n    PRIMARY KEY (product_id) NOT ENFORCED\n) WITH (\n    'merge-engine' = 'aggregation',\n    'fields.price.aggregate-function' = 'max',\n    'fields.sales.aggregate-function' = 'sum'\n);\n```\n\n----------------------------------------\n\nTITLE: SQL Join Hints Example in Flink\nDESCRIPTION: Join hints allow users to manually specify join strategies in Flink SQL to avoid unreasonable execution plans and optimizer limitations. This feature helps ensure production availability of batch jobs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-10-28-1.16-announcement.md#2025-04-08_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n/*+ LOOKUP('table'='DimTable', 'retry-predicate'='lookup_miss', 'retry-strategy'='fixed_delay', 'fixed-delay'='10s', 'max-attempts'='3') */\n```\n\n----------------------------------------\n\nTITLE: Configuring IMAP Session Properties in Java\nDESCRIPTION: Implementation of getSessionProperties() method to configure IMAP connection properties including host, port and authentication settings.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.flink.streaming.api.functions.source.RichSourceFunction;\nimport org.apache.flink.table.data.RowData;\nimport java.util.Properties;\n\npublic class ImapSource extends RichSourceFunction<RowData> {\n   // â€¦\n\n   private Properties getSessionProperties() {\n        Properties props = new Properties();\n        props.put(\"mail.store.protocol\", \"imap\");\n        props.put(\"mail.imap.auth\", true);\n        props.put(\"mail.imap.host\", options.getHost());\n        if (options.getPort() != null) {\n            props.put(\"mail.imap.port\", options.getPort());\n        }\n\n        return props;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Credit-based Flow Control in Flink Configuration\nDESCRIPTION: This YAML configuration snippet shows how to disable credit-based flow control in Flink by setting the 'taskmanager.network.credit-model' parameter to false in the flink-conf.yaml file. This option is deprecated and will be removed in future versions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-05-flink-network-stack.md#2025-04-08_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntaskmanager.network.credit-model: false\n```\n\n----------------------------------------\n\nTITLE: Pulling Flink Docker Image\nDESCRIPTION: Command to download the latest version of Apache Flink Docker image from Docker Hub.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-05-16-official-docker-image.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull flink\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Kubernetes Operator using Helm\nDESCRIPTION: Bash commands to add the Flink Kubernetes Operator 1.0.0 Helm repository and install the operator. It demonstrates how to use the official release archive as a Helm repository.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-06-05-release-kubernetes-operator-1.0.0.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.0.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.0.0/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.0.0/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Setting up StreamExecutionEnvironment in Flink\nDESCRIPTION: Configuring the StreamExecutionEnvironment for executing a DataStream pipeline. This setup allows the pipeline to run in either streaming or batch mode, with streaming being the default.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-09-howto-migrate-to-datastream.md#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n// Uncomment the following line to set batch mode explicitly\n// env.setRuntimeMode(RuntimeExecutionMode.BATCH);\n```\n\n----------------------------------------\n\nTITLE: Memory Segment Implementation Types Table\nDESCRIPTION: HTML table defining the different memory segment implementations being tested, including heap, hybrid and pure variants with exclusive and mixed loading scenarios.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-09-16-off-heap-memory.md#2025-04-08_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Type</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><code>HeapMemorySegment</code> <em>(exclusive)</em></td>\n      <td>The case where it is the only loaded MemorySegment subclass.</td>\n    </tr>\n    <!-- Additional rows omitted for brevity -->\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Executing TPCDS Query3 in SQL\nDESCRIPTION: SQL representation of TPCDS Query3, which performs an analytic query on store sales data. It includes common operations like filtering, joining, aggregation, grouping, ordering, and limiting results.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-09-howto-migrate-to-datastream.md#2025-04-08_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT dt.d_year, item.i_brand_id brand_id, item.i_brand brand,SUM(ss_ext_sales_price) sum_agg\nFROM  date_dim dt, store_sales, item\nWHERE dt.d_date_sk = store_sales.ss_sold_date_sk\nAND store_sales.ss_item_sk = item.i_item_sk\nAND item.i_manufact_id = 128\nAND dt.d_moy=11\nGROUP BY dt.d_year, item.i_brand, item.i_brand_id\nORDER BY dt.d_year, sum_agg desc, brand_id\nLIMIT 100\n```\n\n----------------------------------------\n\nTITLE: HiveServer2 Endpoint Configuration\nDESCRIPTION: Configuration for HiveServer2 endpoint in SQL Gateway that enables Hive JDBC/Beeline connectivity and ecosystem integration with tools like DBeaver, Superset, DolphinScheduler, and Zeppelin.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-10-28-1.16-announcement.md#2025-04-08_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE CATALOG myhive WITH ('type' = 'hive');\nUSE CATALOG myhive;\nSET table.sql-dialect=hive;\n```\n\n----------------------------------------\n\nTITLE: Revised Job Finishing Process in Apache Flink\nDESCRIPTION: The updated protocol for finishing jobs in Flink that decouples operator logic completion from task termination using a new EndOfData event. This revision allows for better coordination of final checkpoints across the pipeline and proper handling of both bounded and unbounded sources.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-07-11-final-checkpoint-part2.md#2025-04-08_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n1. Source tasks finished due to no more records or stop-with-savepoint. \n    a. if no more records or stop-with-savepoint â€“-drain\n        i. source operators emit MAX_WATERMARK\n        ii. endInput(inputId) for all the operators\n        iii. finish() for all the operators\n        iv. emit EndOfData[isDrain = true] event\n    b. else if stop-with-savepoint\n        i. emit EndOfData[isDrain = false] event\n    c. Wait for the next checkpoint / the savepoint after operator finished complete\n    d. close() for all the operators\n    e. Emit EndOfPartitionEvent\n    f. Task cleanup\n2. On received MAX_WATERMARK for non-source operators\n    a. Trigger all the event times\n    b. Emit MAX_WATERMARK\n3. On received EndOfData for non-source tasks\n    a. If isDrain\n        i. endInput(int inputId) for all the operators\n        ii. finish() for all the operators\n    b. Emit EndOfData[isDrain = the flag value of the received event]\n4. On received EndOfPartitionEvent for non-source tasks\n    a. Wait for the next checkpoint / the savepoint after operator finished complete\n    b. close() for all the operators\n    c. Emit EndOfPartitionEvent\n    d. Task cleanup\n```\n\n----------------------------------------\n\nTITLE: Submitting Dedicated Compaction Job in Flink\nDESCRIPTION: Command to submit a dedicated compaction job for table maintenance using Flink's command line interface. Requires specifying warehouse path, database name and table name.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-01-13-release-table-store-0.3.0.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n<FLINK_HOME>/bin/flink run \\\n    -c org.apache.flink.table.store.connector.action.FlinkActions \\\n    /path/to/flink-table-store-dist-<version>.jar \\\n    compact \\\n    --warehouse <warehouse-path> \\\n    --database <database-name> \\\n    --table <table-name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Latency Tracking Interval in Java\nDESCRIPTION: Shows how to enable and configure latency tracking in Flink using ExecutionConfig. This method sets the interval at which latency markers are emitted by source tasks to track record processing times through the system.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-07-23-flink-network-stack-2.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nExecutionConfig#setLatencyTrackingInterval()\n```\n\n----------------------------------------\n\nTITLE: Implementing Named Parameters for Table Functions in Java\nDESCRIPTION: Example of defining a table function with named parameters including mandatory and optional arguments using Java annotations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic static class NamedArgumentsTableFunction extends TableFunction<Object> {\n\n\t@FunctionHint(\n\t\t\toutput = @DataTypeHint(\"STRING\"),\n\t\t\targuments = {\n\t\t\t\t\t@ArgumentHint(name = \"in1\", isOptional = false, type = @DataTypeHint(\"STRING\")),\n\t\t\t\t\t@ArgumentHint(name = \"in2\", isOptional = true, type = @DataTypeHint(\"STRING\")),\n\t\t\t\t\t@ArgumentHint(name = \"in3\", isOptional = true, type = @DataTypeHint(\"STRING\"))})\n\tpublic void eval(String arg1, String arg2, String arg3) {\n\t\tcollect(arg1 + \", \" + arg2 + \",\" + arg3);\n\t}\n\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Continuous File System Source in Java\nDESCRIPTION: This snippet demonstrates how to set up a continuous file system source in Flink 1.1.0, which monitors a directory and processes files continuously.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-08-04-release-1.1.0.md#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nDataStream<String> stream = env.readFile(\n  textInputFormat,\n  \"hdfs:///file-path\",\n  FileProcessingMode.PROCESS_CONTINUOUSLY,\n  5000, // monitoring interval (millis)\n  FilePathFilter.createDefaultFilter()); // file path filter\n```\n\n----------------------------------------\n\nTITLE: Configuring Flink Operator Job Health Check and Restart Parameters in YAML\nDESCRIPTION: YAML configuration for the Flink Kubernetes Operator that defines health check settings and restart parameters. It disables the job health check, sets a 2-minute duration window for monitoring restarts, and defines a threshold of 64 restarts before triggering recovery.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-12-14-release-kubernetes-operator-1.3.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkubernetes.operator.job.health-check.enabled: false\nkubernetes.operator.job.restart-check.duration-window: 2m\nkubernetes.operator.job.restart-check.threshold: 64\n```\n\n----------------------------------------\n\nTITLE: Defining Broadcast State Descriptor in Flink\nDESCRIPTION: Creates a MapStateDescriptor for rules that will be used in the broadcast state. The descriptor defines the key-value format for storing rules with Integer keys and Rule objects as values.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-03-24-demo-fraud-detection-2.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic static final MapStateDescriptor<Integer, Rule> RULES_STATE_DESCRIPTOR =\n        new MapStateDescriptor<>(\"rules\", Integer.class, Rule.class);\n```\n\n----------------------------------------\n\nTITLE: Enabling Fully Async Mode for RocksDB State Backend in Java\nDESCRIPTION: Java code snippet demonstrating how to enable the fully async mode for the RocksDB state backend, which is recommended for easier upgrades to Flink 1.2.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-10-12-release-1.1.3.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nRocksDBStateBackend backend = new RocksDBStateBackend(\"...\");\nbackend.enableFullyAsyncSnapshots();\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Kubernetes Operator via Helm\nDESCRIPTION: Helm commands to add the Flink Kubernetes Operator repository and install the operator with webhook creation disabled.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-21-release-kubernetes-operator-1.8.0.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.8.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.8.0/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.8.0/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Dropping Table in SQL DDL\nDESCRIPTION: Example of dropping a table using SQL DDL statements, which is now supported in Flink 1.9. This allows removing tables from catalogs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-08-22-release-1.9.0.md#2025-04-08_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nDROP TABLE\n```\n\n----------------------------------------\n\nTITLE: Triggering Checkpoint in ExternallyInducedSource Interface (Java)\nDESCRIPTION: This code snippet shows the method signature for triggering a checkpoint in Flink's ExternallyInducedSource interface. It is called when the FlinkPravegaReader receives a Checkpoint event from Pravega.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-20-pravega-connector-101.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\ntriggerCheckpoint(long checkpointId)\n```\n\n----------------------------------------\n\nTITLE: Creating Broadcast State Descriptor in Flink\nDESCRIPTION: Defines a MapStateDescriptor for the broadcast state, which will store a single Pattern object with a Void key.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-26-broadcast-state.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nMapStateDescriptor<Void, Pattern> bcStateDescriptor = \n  new MapStateDescriptor<>(\"patterns\", Types.VOID, Types.POJO(Pattern.class));\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Kubernetes Operator with Helm\nDESCRIPTION: Helm commands for adding the Flink Kubernetes Operator repository and installing the operator with webhook creation disabled.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-04-03-release-kubernetes-operator-0.1.0.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-0.1.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-0.1.0/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-0.1.0/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Creating Pulsar Producer with Schema (Java)\nDESCRIPTION: Demonstrates creating a Pulsar producer with an AVRO schema for a User class. The producer can then send structured messages conforming to this schema.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-25-query-pulsar-streams-using-apache-flink.md#2025-04-08_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// Create producer with Struct schema and send messages\nProducer<User> producer = client.newProducer(Schema.AVRO(User.class)).create();\nproducer.newMessage()\n  .value(User.builder()\n    .userName(\"pulsar-user\")\n    .userId(1L)\n    .build())\n  .send();\n```\n\n----------------------------------------\n\nTITLE: Executing Storm Topology with Flink's Context-based Execution\nDESCRIPTION: This code demonstrates a shorter Flink-style alternative for submitting a Storm topology to Flink using context-based job execution. This approach replaces the Storm-style submission code.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-11-storm-compatibility.md#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n// transform Storm topology to Flink program (as above)\nFlinkTopology topology = FlinkTopology.createTopology(builder);\n\n// executes locally by default or remotely if submitted with Flink's command-line client\ntopology.execute()\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Kubernetes Operator via Helm\nDESCRIPTION: This code snippet shows how to add the Helm repository for Flink Kubernetes Operator 1.2.0 and install it using Helm. It sets the webhook creation to false during installation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-10-07-release-kubernetes-operator-1.2.0.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.2.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.2.0/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.2.0/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Schema Evolution Examples in SQL\nDESCRIPTION: SQL statements demonstrating schema evolution capabilities including adding columns, renaming columns, and inserting data with different schemas.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-01-13-release-table-store-0.3.0.md#2025-04-08_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE T (i INT, j INT);\n\nINSERT INTO T (1, 1);\n\nALTER TABLE T ADD COLUMN k INT;\n\nALTER TABLE T RENAME COLUMN i to a;\n\nINSERT INTO T (2, 2, 2);\n\nSELECT * FROM T;\n-- outputs (1, 1, NULL) and (2, 2, 2)\n```\n\n----------------------------------------\n\nTITLE: Creating and Joining Tables in SQL for Batch Processing Example\nDESCRIPTION: A SQL example demonstrating a simple batch processing scenario with customer and order tables. The query creates two tables and performs a join operation to enrich orders with customer names, illustrating a common batch processing pattern that would benefit from improved scheduling.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-12-02-pipelined-region-sheduling.md#2025-04-08_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE customers (\n    customerId int,\n    name varchar(255)\n);\n\nCREATE TABLE orders (\n    orderId int,\n    orderCustomerId int\n);\n\n--fill tables with data\n\nSELECT customerId, name\nFROM customers, orders\nWHERE customerId = orderCustomerId\n```\n\n----------------------------------------\n\nTITLE: Querying Data with Flink Table API in Scala\nDESCRIPTION: Demonstrates the new Table API functionality for processing structured data using SQL-like queries. Shows grouping and joining operations with named attributes on click and user tables.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-06-24-announcing-apache-flink-0.9.0-release.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval clickCounts = clicks\n  .groupBy('user).select('userId, 'url.count as 'count)\n\nval activeUsers = users.join(clickCounts)\n  .where('id === 'userId && 'count > 10).select('username, 'count, ...)\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Operator with KUDO\nDESCRIPTION: Command to install the Flink Operator using KUDO without creating an instance. The Flink demo will create and configure the Flink instance.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl kudo install flink --version=0.2.1 --skip-instance\n```\n\n----------------------------------------\n\nTITLE: Configuring Max Parallelism in StreamExecutionEnvironment\nDESCRIPTION: Default configuration values for the max parallelism parameter in StreamExecutionEnvironment. For parallelism <= 128, max parallelism is set to 128. For parallelism > 128, it's set to MIN(nextPowerOfTwo(parallelism + (parallelism / 2)), 2^15).\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-02-06-release-1.2.0.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n128 : for all parallelism <= 128\nMIN(nextPowerOfTwo(parallelism + (parallelism / 2)), 2^15): for all parallelism > 128\n```\n\n----------------------------------------\n\nTITLE: Defining a Person POJO in Java for Flink Serialization\nDESCRIPTION: A simple Java POJO class definition for a Person with id and name fields that demonstrates how Flink serializes composite types within nested structures like Tuple3<Integer, Double, Person>.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-05-11-Juggling-with-Bits-and-Bytes.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic class Person {\n    public int id;\n    public String name;\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading Docker Compose Configuration for Flink SQL Demo\nDESCRIPTION: Commands to create a directory for the demo, download the docker-compose.yml file defining the demo environment.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir flink-sql-demo; cd flink-sql-demo;\nwget https://raw.githubusercontent.com/wuchong/flink-sql-demo/v1.11-EN/docker-compose.yml\n```\n\n----------------------------------------\n\nTITLE: Dependency Table for AWS Kinesis and DynamoDB Sources\nDESCRIPTION: HTML table listing the dependencies, usage, and configuration details for Amazon Kinesis Data Streams and DynamoDB Streams source connectors with Flink, including artifacts required for DataStream API, Table API, and SQL.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-11-25-whats-new-aws-connectors-5.0.0.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<table>\n  <tr>\n    <th>Connector</th>\n    <th>API</th>\n    <th>Dependency</th>\n    <th>Usage</th>\n  </tr>\n  <tr>\n    <td>Amazon Kinesis Data Streams source</td>\n    <td>DataStream<br>Table API</td>\n    <td> Use the <code>flink-connector-aws-kinesis-streams</code> artifact. See <a href=\"https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/datastream/kinesis/\">\n      Flink Kinesis connector documentation</a> for details.\n    </td>\n    <td>\n      Use the fluent \n      <a href=\"https://github.com/apache/flink-connector-aws/blob/v5.0/flink-connector-aws/flink-connector-aws-kinesis-streams/src/main/java/org/apache/flink/connector/kinesis/source/KinesisStreamsSourceBuilder.java\">\n      KinesisStreamsSourceBuilder</a> to create the source. Look at the <a href=\"#example-migrating-flinkkinesisconsumer-to-kinesisstreamssource\">migration guidance section</a> for more details.\n    </td>\n  </tr>\n  <tr>\n    <td>Amazon Kinesis Data Streams source</td>\n    <td>SQL</td>\n    <td> Use the <code>flink-sql-connector-aws-kinesis-streams</code> artifact. See <a href=\"https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/table/kinesis/\">\n      Flink SQL Kinesis connector documentation</a> for details.\n    </td>\n     <td>\n      Use the table identifier <code>kinesis</code>. See the \n      <a href=\"https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/table/kinesis/\">\n      Flink SQL Kinesis connector documentation</a> for configuration and usage details.\n    </td>\n  </tr>\n  <tr>\n    <td>Amazon DynamoDB Streams source</td>\n    <td>DataStream</td>\n    <td> Use the <code>flink-connector-dynamodb</code> artifact. See <a href=\"https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/datastream/dynamodb/\">\n      Flink DynamoDB connector documentation</a> for details.\n    </td>\n    <td>\n      Use the fluent \n      <a href=\"https://github.com/apache/flink-connector-aws/blob/v5.0/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/source/DynamoDbStreamsSourceBuilder.java\">\n      DynamoDbStreamsSourceBuilder</a> to create the source. Look at the <a href=\"#example-migrating-flinkdynamodbstreamsconsumer-to-dynamodbstreamssource\">migration guidance section</a> for more details.\n    </td>\n  </tr>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Executing Storm Topology with FlinkTopology and FlinkSubmitter\nDESCRIPTION: This code shows how to transform a Storm topology to a Flink program and submit it using either FlinkLocalCluster or FlinkSubmitter. This replaces the standard Storm submission code and allows the topology to run on Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-11-storm-compatibility.md#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// transform Storm topology to Flink program\n// replaces: StormTopology topology = builder.createTopology();\nFlinkTopology topology = FlinkTopology.createTopology(builder);\n\nConfig conf = new Config();\nif(runLocal) {\n\t// use FlinkLocalCluster instead of LocalCluster\n\tFlinkLocalCluster cluster = FlinkLocalCluster.getLocalCluster();\n\tcluster.submitTopology(\"WordCount\", conf, topology);\n} else {\n\t// use FlinkSubmitter instead of StormSubmitter\n\tFlinkSubmitter.submitTopology(\"WordCount\", conf, topology);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Pulsar Consumer with Schema (Java)\nDESCRIPTION: Shows how to create a Pulsar consumer with an AVRO schema for a User class. This allows the consumer to receive and automatically deserialize structured messages.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-25-query-pulsar-streams-using-apache-flink.md#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// Create consumer with Struct schema and receive messages\nConsumer<User> consumer = client.newCOnsumer(Schema.AVRO(User.class)).create();\nconsumer.receive();\n```\n\n----------------------------------------\n\nTITLE: Initializing KUDO on Kubernetes Cluster\nDESCRIPTION: Command to initialize KUDO on a Kubernetes cluster. This creates the necessary custom resource definitions, service accounts, and controller instance for KUDO to operate.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl kudo init\n$KUDO_HOME has been configured at /Users/gerred/.kudo\n```\n\n----------------------------------------\n\nTITLE: Using TwoPhaseCommitSinkFunction in Java\nDESCRIPTION: Code reference showing the usage of TwoPhaseCommitSinkFunction class from Flink's Java API, which implements the two-phase commit protocol for exactly-once processing semantics.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-02-28-end-to-end-exactly-once-apache-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nTwoPhaseCommitSinkFunction\n```\n\n----------------------------------------\n\nTITLE: Cloning KUDO Operators Repository\nDESCRIPTION: Command to clone the official KUDO Operators repository which contains the financial fraud demo operator and other examples.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/kudobuilder/operators.git\n```\n\n----------------------------------------\n\nTITLE: Inefficient Lambda Implementation vs Method Reference\nDESCRIPTION: Example showing inefficient inline lambda implementation that can be replaced with a method reference for better readability and potentially better performance.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nmap.computeIfAbsent(key, k-> Loader.load(k));\n```\n\n----------------------------------------\n\nTITLE: Inefficient Lambda Implementation vs Method Reference\nDESCRIPTION: Example showing inefficient inline lambda implementation that can be replaced with a method reference for better readability and potentially better performance.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nmap.computeIfAbsent(key, k-> Loader.load(k));\n```\n\n----------------------------------------\n\nTITLE: Converting Table Data to Custom POJO Objects\nDESCRIPTION: Shows how to transform table data into Customer POJO objects using field mapping and custom parsing function.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-29-table-sql-api-update.md#2025-04-08_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval ds = tEnv\n  .scan(\"customers\")\n  .select('id, 'name, 'last_update as 'update, parseProperties('prefs) as 'prefs)\n  .toDataStream[Customer]\n```\n\n----------------------------------------\n\nTITLE: Efficient Method Reference Implementation\nDESCRIPTION: Example showing the preferred method reference syntax that replaces an inline lambda for better readability and potentially better performance.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nmap.computeIfAbsent(key, Loader::load);\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Flink Kubernetes Operator 1.6.0 with Helm\nDESCRIPTION: Bash commands to add the Flink Kubernetes Operator 1.6.0 Helm chart repository and install the operator with webhooks disabled. This provides a quick way to deploy the operator in a Kubernetes environment.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-08-15-release-kubernetes-operator-1.6.0.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.6.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.6.0/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.6.0/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Checking Schema Compatibility in Avro Serializer for Flink\nDESCRIPTION: Method that determines if the previous schema and runtime schema are compatible, returning the appropriate compatibility status to guide Flink's schema evolution process.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-29-state-unlocked-interacting-with-state-in-apache-flink.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n  @Override\n  public TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(\n      TypeSerializer<T> newSerializer) {\nâ€‹\n    if (!(newSerializer instanceof AvroSerializer)) {\n      return TypeSerializerSchemaCompatibility.incompatible();\n    }\nâ€‹\n    if (Objects.equals(previousSchema, runtimeSchema)) {\n      return TypeSerializerSchemaCompatibility.compatibleAsIs();\n    }\n```\n\n----------------------------------------\n\nTITLE: Examining RocksDB Log Level Setting in Flink\nDESCRIPTION: Code snippet showing how RocksDB logging was disabled in Flink 1.10 by setting the log level to HEADER. This gives context to why users might not see RocksDB logs by default.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-18-rocksdb.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nPredefinedOptions.java#L64\n```\n\n----------------------------------------\n\nTITLE: Efficient Method Reference Implementation\nDESCRIPTION: Example showing the preferred method reference syntax that replaces an inline lambda for better readability and potentially better performance.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nmap.computeIfAbsent(key, Loader::load);\n```\n\n----------------------------------------\n\nTITLE: Table API Window Definition Example\nDESCRIPTION: Example of defining a session window in Table API with a 10-minute gap using rowtime as the time attribute.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-02-06-release-1.2.0.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ntable.window(Session withGap 10.minutes on 'rowtime as 'w)\n```\n\n----------------------------------------\n\nTITLE: Implementing Serial Version UID in Java Classes\nDESCRIPTION: Example of how to define a Serial Version UID in serializable classes. The UID should start at 1 and be bumped on incompatible changes to maintain proper Java serialization compatibility.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nprivate static final long serialVersionUID = 1L;\n```\n\n----------------------------------------\n\nTITLE: Displaying Kubernetes Events for Flink Deployment and Job Changes\nDESCRIPTION: Shows the output of Kubernetes events emitted by the operator for Flink Deployment and Job state changes, including status changes and deployment information.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-07-25-release-kubernetes-operator-1.1.0.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nEvents:\n  Type    Reason         Age   From                  Message\n  ----    ------         ----  ----                  -------\n  Normal  Submit         53m   JobManagerDeployment  Starting deployment\n  Normal  StatusChanged  52m   Job                   Job status changed from RECONCILING to CREATED\n  Normal  StatusChanged  52m   Job                   Job status changed from CREATED to RUNNING\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment Variables and Network for Flink Docker Deployment\nDESCRIPTION: Sets up the necessary environment variables and creates a Docker network for Flink containers to communicate. The FLINK_PROPERTIES variable configures the JobManager RPC address.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-20-flink-docker.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFLINK_PROPERTIES=\"jobmanager.rpc.address: jobmanager\"\ndocker network create flink-network\n```\n\n----------------------------------------\n\nTITLE: Configuring Log4j Security Property in Flink Configuration\nDESCRIPTION: Configuration snippet to set the Log4j2 format message no lookups property in flink-conf.yaml to mitigate CVE-2021-44228 vulnerability. This setting should be added to the environment Java options in Flink's configuration.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-12-10-log4j-cve.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nenv.java.opts: -Dlog4j2.formatMsgNoLookups=true\n```\n\n----------------------------------------\n\nTITLE: Implementing Serial Version UID in Java Classes\nDESCRIPTION: Example of how to define a Serial Version UID in serializable classes. The UID should start at 1 and be bumped on incompatible changes to maintain proper Java serialization compatibility.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nprivate static final long serialVersionUID = 1L;\n```\n\n----------------------------------------\n\nTITLE: Data Transformation using Flink Expressions and Table API\nDESCRIPTION: Demonstration of Flink's Table API and expressions for processing CSV data with logical schemas. Shows filtering, joining, and selecting operations using logical attributes instead of physical data types.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-03-02-february-2015-in-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval customers = getCustomerDataSet(env)\n .as('id, 'mktSegment)\n .filter( 'mktSegment === \"AUTOMOBILE\" )\n\nval orders = getOrdersDataSet(env)\n .filter( o => dateFormat.parse(o.orderDate).before(date) )\n .as('orderId, 'custId, 'orderDate, 'shipPrio)\n\nval items =\n orders.join(customers)\n   .where('custId === 'id)\n   .select('orderId, 'orderDate, 'shipPrio)\n```\n\n----------------------------------------\n\nTITLE: Specifying Restore Mode when Running Flink Jobs\nDESCRIPTION: Command to run a Flink job with a specific restore mode (CLAIM, NO_CLAIM, or LEGACY) when restoring from a savepoint. The restore mode determines ownership of snapshot files after restoration.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-06-restore-modes.md#2025-04-08_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ bin/flink run -s :savepointPath -restoreMode :mode -n [:runArgs]\n```\n\n----------------------------------------\n\nTITLE: C-Style Escape Strings in Flink SQL\nDESCRIPTION: Support for C-style escape strings in Flink SQL queries, providing more flexible string handling capabilities in SQL statements.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2025-03-24-release-2.0.0.md#2025-04-08_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\n-- C-style escape strings now supported in Flink SQL\n```\n\n----------------------------------------\n\nTITLE: Configuring Class Loading Order in Flink\nDESCRIPTION: Sets the configuration to use parent-first class loading instead of the new default child-first order in Flink 1.4.0. This allows reverting to the previous class loading behavior if needed.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-12-12-release-1.4.0.md#2025-04-08_snippet_0\n\nLANGUAGE: Properties\nCODE:\n```\nclassloader.resolve-order: parent-first\n```\n\n----------------------------------------\n\nTITLE: Defining POJO Data Types for Flink Serialization Benchmark\nDESCRIPTION: Sample POJO classes used in the Flink serialization benchmarks. These classes represent typical data structures that might be processed in a Flink job, including nested objects and arrays.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_9\n\nLANGUAGE: java\nCODE:\n```\npublic class MyPojo {\n  public int id;\n  private String name;\n  private String[] operationNames;\n  private MyOperation[] operations;\n  private int otherId1;\n  private int otherId2;\n  private int otherId3;\n  private Object someObject;\n}\npublic class MyOperation {\n  int id;\n  protected String name;\n}\n```\n\n----------------------------------------\n\nTITLE: Overriding Default Trigger in Apache Flink (Scala)\nDESCRIPTION: This snippet shows how to override the default trigger of a WindowAssigner with a custom trigger in Apache Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-04-Introducing-windows.md#2025-04-08_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n// override the default trigger of the WindowAssigner\nwindowed = windowed\n  .trigger(myTrigger: Trigger[IN, WINDOW])\n```\n\n----------------------------------------\n\nTITLE: Adding Apache Flink Kubernetes Operator Dependency to Maven POM\nDESCRIPTION: This XML snippet demonstrates how to include the Apache Flink Kubernetes Operator dependency in a Maven project's pom.xml file for Kubernetes integration.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/downloads.md#2025-04-08_snippet_3\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-kubernetes-operator</artifactId>\n  <version>{{< param FlinkKubernetesOperatorStableVersion >}}</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Viewing Kafka Topic Data in Docker Environment\nDESCRIPTION: Command to view the first 10 data entries generated in the Kafka topic using the kafka-console-consumer.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose exec kafka bash -c 'kafka-console-consumer.sh --topic user_behavior --bootstrap-server kafka:9094 --from-beginning --max-messages 10'\n```\n\n----------------------------------------\n\nTITLE: Setting Configuration Defaults in Apache Flink Kubernetes Operator\nDESCRIPTION: YAML syntax examples for defining configuration defaults specific to Flink versions and namespaces in the Kubernetes Operator. Allows different settings for various Flink versions or separate namespaces.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-08-15-release-kubernetes-operator-1.6.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Version Specific Defaults\nkubernetes.operator.default-configuration.flink-version.v1_17.key: value\n\n# Namespace Specific Defaults\nkubernetes.operator.default-configuration.namespace.ns1.key: value\n```\n\n----------------------------------------\n\nTITLE: Defining Different Window Types in Flink Table API (Scala)\nDESCRIPTION: This snippet shows how to define various types of windows in Flink's Table API, including tumbling windows over rows using processing-time, session windows with a gap interval using event-time, and sliding windows over days with hourly triggers using event-time.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-29-table-sql-api-update.md#2025-04-08_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\n// using processing-time\ntable.window(Tumble over 100.rows as 'manyRowWindow)\n// using event-time\ntable.window(Session withGap 15.minutes on 'rowtime as 'sessionWindow)\ntable.window(Slide over 1.day every 1.hour on 'rowtime as 'dailyWindow)\n```\n\n----------------------------------------\n\nTITLE: Configuring State TTL Using SQL Hints\nDESCRIPTION: Shows how to specify custom TTL values for regular joins and group aggregations using STATE_TTL hints within SQL queries.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT /*+ STATE_TTL('Orders'= '1d', 'Customers' = '20d') */ *\nFROM Orders LEFT OUTER JOIN Customers\n    ON Orders.o_custkey = Customers.c_custkey;\n\nSELECT /*+ STATE_TTL('o' = '1d') */ o_orderkey, SUM(o_totalprice) AS revenue\nFROM Orders AS o\nGROUP BY o_orderkey;\n```\n\n----------------------------------------\n\nTITLE: Adding Pravega Flink Connector Maven Dependency\nDESCRIPTION: Maven dependency configuration for including the Pravega Flink Connector in a project. Specifies the dependency for Flink 1.13 with Scala 2.12.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-20-pravega-connector-101.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>io.pravega</groupId>\n  <artifactId>pravega-connectors-flink-1.13_2.12</artifactId>\n  <version>0.10.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Adding Temperature Event Pattern Constraints\nDESCRIPTION: Extending the pattern with subtype constraints and temperature threshold conditions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nPattern.<MonitoringEvent>begin(\"First Event\")\n    .subtype(TemperatureEvent.class)\n    .where(evt -> evt.getTemperature() >= TEMPERATURE_THRESHOLD);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating PLAN_ADVICE for Aggregation Optimization in Flink SQL\nDESCRIPTION: Example of Flink 1.17's PLAN_ADVICE feature providing optimization suggestions. This example recommends enabling local-global two-phase optimization for GroupAggregate operations to improve query performance.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-03-23-release-1.17.0.md#2025-04-08_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n== Optimized Physical Plan With Advice ==\n...\nadvice[1]: [ADVICE] You might want to enable local-global two-phase optimization by configuring ('table.optimizer.agg-phase-strategy' to 'AUTO').\n```\n\n----------------------------------------\n\nTITLE: Custom POJO Class Definition for Customer Data\nDESCRIPTION: Defines a Customer POJO class with fields for ID, name, update timestamp, and preferences properties.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-29-table-sql-api-update.md#2025-04-08_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nclass Customer {\n  var id: Int = _\n  var name: String = _\n  var update: Long = _\n  var prefs: java.util.Properties = _\n}\n```\n\n----------------------------------------\n\nTITLE: Deprecated Time Class Reference - Java\nDESCRIPTION: Reference to the deprecated Time class that will be removed in Flink 2.0, to be replaced with Java's Duration class.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_6\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.api.common.time.Time\n```\n\n----------------------------------------\n\nTITLE: Triggering Checkpoint in Flink's MasterTriggerRestoreHook (Java)\nDESCRIPTION: This code snippet shows the method signature for triggering a checkpoint in Flink's MasterTriggerRestoreHook interface. It is used to initiate the checkpoint process for the Pravega reader state.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-20-pravega-connector-101.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\ntriggerCheckpoint(long checkpointId, long timestamp, Executor executor)\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose Environment for Flink and Prometheus\nDESCRIPTION: Command to build a Flink job using Gradle and start a local environment with Docker Compose, running a Flink job cluster and Prometheus instance.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-03-11-prometheus-monitoring.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./gradlew composeUp\n```\n\n----------------------------------------\n\nTITLE: Deprecated RestoreMode Usage - Java\nDESCRIPTION: Reference to the deprecated LEGACY restore mode, which should be replaced with either CLAIM or NO_CLAIM modes for clear state file ownership.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_7\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.runtime.jobgraph.RestoreMode#LEGACY\n```\n\n----------------------------------------\n\nTITLE: Referencing Flink's MemorySegment Implementation\nDESCRIPTION: A GitHub link to Flink's MemorySegment class, which is the core component of Flink's memory management system. This class provides efficient read and write access to byte arrays using Java's unsafe methods.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-05-11-Juggling-with-Bits-and-Bytes.md#2025-04-08_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nhttps://github.com/apache/flink/blob/release-0.9.0-milestone-1/flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java\n```\n\n----------------------------------------\n\nTITLE: Defining Temperature Warning Class\nDESCRIPTION: Class definition for temperature warning events with rack ID and average temperature.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic class TemperatureWarning {\n    private int rackID;\n    private double averageTemperature;\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Maven Dependency for DynamoDB Connector in Flink 1.17\nDESCRIPTION: XML snippet showing how to add the DynamoDB connector as a Maven dependency for Flink 1.17. This demonstrates the new connector versioning format (<major>.<minor>.<patch>-<flink-major>.<flink-minor>).\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-08-04-externalized-connectors.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-connector-dynamodb</artifactId>\n    <version>4.1.0-1.17</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Analyzing Buffer Usage Metrics in Flink 1.9+ with Code References\nDESCRIPTION: Code references showing the relationship between floatingBuffersUsage, exclusiveBuffersUsage, and backpressure conditions in Flink 1.9 and above.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-07-23-flink-network-stack-2.md#2025-04-08_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\nfloatingBuffersUsage\nexclusiveBuffersUsage\ninPoolUsage = floatingBuffersUsage + exclusiveBuffersUsage\noutPoolUsage\n```\n\n----------------------------------------\n\nTITLE: Deprecated RichFunction Open Method - Java\nDESCRIPTION: Reference to deprecated open method in RichFunction interface, to be replaced with new OpenContext-based version.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_10\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.api.common.functions.RichFunction#open(Configuration parameters)\n```\n\n----------------------------------------\n\nTITLE: Byte Array Access Performance Results Table\nDESCRIPTION: HTML tables showing performance results for byte array read/write operations across different memory segment implementations and sizes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-09-16-off-heap-memory.md#2025-04-08_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Segment</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><code>HeapMemorySegment</code>, mixed</td>\n      <td>164 msecs</td>\n    </tr>\n    <!-- Additional rows omitted for brevity -->\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Setting Input Dependency Constraint for Batch Jobs in Java\nDESCRIPTION: This code snippet demonstrates how to set the default input dependency constraint for batch jobs using the ExecutionConfig class.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-05-flink-network-stack.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nExecutionConfig#setDefaultInputDependencyConstraint(InputDependencyConstraint)\n```\n\n----------------------------------------\n\nTITLE: Configuration Key Naming Examples\nDESCRIPTION: Examples showing incorrect and correct ways to name configuration keys in Flink, emphasizing proper hierarchical structure.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-components.md#2025-04-08_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n\"taskmanager.detailed.network.metrics\"   # NOT correct\n\"taskmanager.network.detailed-metrics\"   # Correct\n```\n\n----------------------------------------\n\nTITLE: Configuring RocksDB State Backend in Apache Flink 1.1.4\nDESCRIPTION: Java code snippet demonstrating how to configure the RocksDB state backend to use the new default options in Apache Flink 1.1.4. This is relevant for users who want to use the updated RocksDB 4.11.2 defaults.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-12-21-release-1.1.4.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nRocksDBStateBackend backend = new RocksDBStateBackend(\"...\");\n// Use the new default options. Otherwise, the default for RocksDB 4.5.1\n// `PredefinedOptions.DEFAULT_ROCKS_4_5_1` will be used.\nbackend.setPredefinedOptions(PredefinedOptions.DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Removed Constructors in SourceReaderBase (Java)\nDESCRIPTION: Documentation of removed constructors in the SourceReaderBase class. These constructors were likely replaced or deprecated in favor of new implementations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nSourceReaderBase(org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue<org.apache.flink.connector.base.source.reader.RecordsWithSplitIds<E>>, org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager<E,SplitT>, org.apache.flink.connector.base.source.reader.RecordEmitter<E,T,SplitStateT>, org.apache.flink.configuration.Configuration, org.apache.flink.api.connector.source.SourceReaderContext)\n```\n\nLANGUAGE: Java\nCODE:\n```\nSourceReaderBase(org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue<org.apache.flink.connector.base.source.reader.RecordsWithSplitIds<E>>, org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager<E,SplitT>, org.apache.flink.connector.base.source.reader.RecordEmitter<E,T,SplitStateT>, org.apache.flink.connector.base.source.reader.RecordEvaluator<T>, org.apache.flink.configuration.Configuration, org.apache.flink.api.connector.source.SourceReaderContext)\n```\n\n----------------------------------------\n\nTITLE: Performance Benchmark: Heap vs Off-heap Memory Segments\nDESCRIPTION: Code snippet showing performance benchmark results comparing heap and off-heap memory segment implementations in different scenarios. The benchmark writes 100000 x 32768 bytes to memory segments.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-09-16-off-heap-memory.md#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nWriting 100000 x 32768 bytes to 32768 bytes segment:\n\nHeapMemorySegment    (standalone) : 1,441 msecs\nOffHeapMemorySegment (standalone) : 1,628 msecs\nHeapMemorySegment    (subclass)   : 3,841 msecs\nOffHeapMemorySegment (subclass)   : 3,847 msecs\n```\n\n----------------------------------------\n\nTITLE: Specifying Evictor in Apache Flink (Scala)\nDESCRIPTION: This code demonstrates how to specify an optional Evictor for a windowed stream in Apache Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-04-Introducing-windows.md#2025-04-08_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\n// specify an optional evictor\nwindowed = windowed\n  .evictor(myEvictor: Evictor[IN, WINDOW])\n```\n\n----------------------------------------\n\nTITLE: Example of Poor Nested Conditional Logic\nDESCRIPTION: Demonstrates an anti-pattern of deeply nested conditional statements that reduce code readability.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-common.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nif (a) {\n    if (b) {\n        if (c) {\n            the main path\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Checking RocksDB Threads in Flink TaskManager Process\nDESCRIPTION: This bash command demonstrates how to check for RocksDB-related threads in a running Flink TaskManager process.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-18-rocksdb.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ps -T -p 32513 | grep -i rocksdb\n```\n\n----------------------------------------\n\nTITLE: Modified Methods in KeyedPartitionStream and NonKeyedPartitionStream (Java)\nDESCRIPTION: Documentation of modified method signatures in the KeyedPartitionStream and NonKeyedPartitionStream classes. These changes likely reflect updates to the processing API for partitioned streams.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.flink.datastream.api.stream.KeyedPartitionStream$ProcessConfigurableAndTwoKeyedPartitionStreams<K,OUT1,OUT2><K,OUT1,OUT2> (<-org.apache.flink.datastream.api.stream.KeyedPartitionStream$TwoKeyedPartitionStreams<K,OUT1,OUT2><K,OUT1,OUT2>) process(org.apache.flink.datastream.api.function.TwoOutputStreamProcessFunction<T,OUT1,OUT2><T,OUT1,OUT2>, org.apache.flink.api.java.functions.KeySelector<OUT1,K><OUT1,K>, org.apache.flink.api.java.functions.KeySelector<OUT2,K><OUT2,K>)\n```\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.flink.datastream.api.stream.NonKeyedPartitionStream$ProcessConfigurableAndTwoNonKeyedPartitionStream<OUT1,OUT2><OUT1,OUT2> (<-org.apache.flink.datastream.api.stream.NonKeyedPartitionStream$TwoNonKeyedPartitionStreams<OUT1,OUT2><OUT1,OUT2>) process(org.apache.flink.datastream.api.function.TwoOutputStreamProcessFunction<T,OUT1,OUT2><T,OUT1,OUT2>)\n```\n\n----------------------------------------\n\nTITLE: Handling stop-with-savepoint Command Process in Apache Flink\nDESCRIPTION: A detailed workflow of how Flink processes the stop-with-savepoint command, including savepoint triggering, handling MAX_WATERMARK emission, and the task cleanup process. This process is used when users explicitly want to terminate a job with a savepoint.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-07-11-final-checkpoint-part2.md#2025-04-08_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n1. Trigger a savepoint\n2. Sources received savepoint trigger RPC\n    a. If with â€“-drain\n        i. source operators emit MAX_WATERMARK\n    b. Source emits savepoint barrier\n3. On received MAX_WATERMARK for non-source operators\n    a. Trigger all the event times\n    b. Emit MAX_WATERMARK\n4. On received savepoint barrier for non-source operators\n    a. The task blocks till the savepoint succeed\n5. Finish the source tasks actively\n    a. If with â€“-drain\n        ii. endInput(inputId) for all the operators\n    b. close() for all the operators\n    c. dispose() for all the operators\n    d. Emit EndOfPartitionEvent\n    e. Task cleanup\n6. On received EndOfPartitionEvent for non-source tasks\n    a. If with â€“-drain\n        i. endInput(int inputId) for all the operators\n    b. close() for all the operators\n    c. dispose() for all the operators\n    d. Emit EndOfPartitionEvent\n    e. Task cleanup\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Scalar Function for Property Parsing\nDESCRIPTION: Demonstrates implementation of a custom scalar function that parses comma-separated key-value strings into Properties objects.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-29-table-sql-api-update.md#2025-04-08_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nobject parseProperties extends ScalarFunction {\n  def eval(str: String): Properties = {\n    val props = new Properties()\n    str\n      .split(\",\")\n      .map(\\_.split(\"=\"))\n      .foreach(split => props.setProperty(split(0), split(1)))\n    props\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Breaking Chained Method Calls in Java\nDESCRIPTION: Shows the correct way to format long chains of method calls across multiple lines, with proper indentation and dot placement. Each method call is placed on a new line with consistent indentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-formatting.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nvalues\n    .stream()\n    .map(...)\n    .collect(...);\n```\n\n----------------------------------------\n\nTITLE: Starting Minikube Cluster for Flink and Kafka\nDESCRIPTION: Command to start a local Minikube cluster with sufficient resources (CPU, memory, and disk space) to run Apache Flink and Kafka for the financial fraud demo.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nminikube start --cpus=6 --memory=9216 --disk-size=10g\n```\n\n----------------------------------------\n\nTITLE: Removed Fields from CheckpointConfig Class\nDESCRIPTION: Default configuration fields that have been removed from the CheckpointConfig class. These constants defined default values for various checkpoint settings.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_20\n\nLANGUAGE: java\nCODE:\n```\nint UNDEFINED_TOLERABLE_CHECKPOINT_NUMBER\nlong DEFAULT_TIMEOUT\nlong DEFAULT_MIN_PAUSE_BETWEEN_CHECKPOINTS\norg.apache.flink.streaming.api.CheckpointingMode DEFAULT_MODE\nint DEFAULT_MAX_CONCURRENT_CHECKPOINTS\nint DEFAULT_CHECKPOINT_ID_OF_IGNORED_IN_FLIGHT_DATA\n```\n\n----------------------------------------\n\nTITLE: Computing Subpartition Range Formula\nDESCRIPTION: Mathematical formula for calculating the subpartition range for consumer execution vertices in Flink's dynamic graph. Given N consumer execution vertices and P subpartitions, calculates range for kth vertex.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-06-17-adaptive-batch-scheduler.md#2025-04-08_snippet_2\n\nLANGUAGE: mathematical\nCODE:\n```\nfloor(k * P/N) <= i < floor((k+1) * P/N), where i is the subpartition index\n```\n\n----------------------------------------\n\nTITLE: Defining Temperature Alert Class\nDESCRIPTION: Class definition for temperature alert events containing rack ID information.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-04-06-cep-monitoring.md#2025-04-08_snippet_7\n\nLANGUAGE: java\nCODE:\n```\npublic class TemperatureAlert {\n    private int rackID;\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting Command Examples in Flink Documentation\nDESCRIPTION: This bash code block demonstrates the recommended formatting for command examples in Flink documentation, including long parameter names, one parameter per line, proper indentation, and use of $ prefix.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ ./bin/flink run-application \\\n--target kubernetes-application \\\n-Dkubernetes.cluster-id=my-first-application-cluster \\\n-Dkubernetes.container.image=custom-image-name \\\nlocal:///opt/flink/usrlib/my-flink-job.jar\n```\n\n----------------------------------------\n\nTITLE: Beam Read Transform with Flink Integration\nDESCRIPTION: The Read transform allows data ingestion into Beam pipelines, implemented through SourceInputFormat for batch processing and UnboundedSourceWrapper for stream processing in Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-22-apache-beam-how-beam-runs-on-top-of-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nRead\n```\n\n----------------------------------------\n\nTITLE: Enumerating Java Configuration Constants in Apache Flink\nDESCRIPTION: A comprehensive list of configuration constants used in the Apache Flink framework. These constants define configuration parameters for various components including cluster management (YARN, Mesos), high availability (ZooKeeper), security, performance tuning, networking, and resource management.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2025-03-24-release-2.0.0.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String YARN_APPLICATION_TAGS\njava.lang.String HDFS_SITE_CONFIG\njava.lang.String EXECUTION_RETRY_DELAY_KEY\nint DEFAULT_MESOS_ARTIFACT_SERVER_PORT\nboolean DEFAULT_SECURITY_SSL_VERIFY_HOSTNAME\njava.lang.String CONTAINERIZED_HEAP_CUTOFF_MIN\njava.lang.String YARN_HEARTBEAT_DELAY_SECONDS\njava.lang.String AKKA_SSL_ENABLED\njava.lang.String HA_MODE\njava.lang.String ZOOKEEPER_MESOS_WORKERS_PATH\nboolean DEFAULT_ZOOKEEPER_SASL_DISABLE\njava.lang.String METRICS_SCOPE_DELIMITER\njava.lang.String LOCAL_NUMBER_RESOURCE_MANAGER\njava.lang.String AKKA_TCP_TIMEOUT\njava.lang.String METRICS_SCOPE_NAMING_OPERATOR\njava.lang.String ZOOKEEPER_RECOVERY_PATH\nint DEFAULT_ZOOKEEPER_LEADER_PORT\njava.lang.String DEFAULT_ZOOKEEPER_LATCH_PATH\nint DEFAULT_ZOOKEEPER_PEER_PORT\njava.lang.String METRICS_SCOPE_NAMING_TM_JOB\nint DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_NUM_SAMPLES\njava.lang.String HA_ZOOKEEPER_SESSION_TIMEOUT\njava.lang.String FLINK_JVM_OPTIONS\njava.lang.String HA_ZOOKEEPER_CHECKPOINT_COUNTER_PATH\njava.lang.String METRICS_SCOPE_NAMING_JM\njava.lang.String DEFAULT_YARN_JOB_MANAGER_PORT\nboolean DEFAULT_JOB_MANAGER_WEB_CHECKPOINTS_DISABLE\njava.lang.String HA_ZOOKEEPER_QUORUM_KEY\nboolean DEFAULT_JOB_MANAGER_WEB_SUBMIT_ENABLED\njava.lang.String JOB_MANAGER_WEB_CHECKPOINTS_HISTORY_SIZE\njava.lang.String ZOOKEEPER_JOBGRAPHS_PATH\njava.lang.String ZOOKEEPER_SASL_SERVICE_NAME\njava.lang.String DEFAULT_AKKA_LOOKUP_TIMEOUT\njava.lang.String RESTART_STRATEGY_FAILURE_RATE_MAX_FAILURES_PER_INTERVAL\njava.lang.String JOB_MANAGER_WEB_PORT_KEY\njava.lang.String METRICS_LATENCY_HISTORY_SIZE\nint DEFAULT_BLOB_FETCH_BACKLOG\njava.lang.String JOB_MANAGER_WEB_BACK_PRESSURE_REFRESH_INTERVAL\nfloat DEFAULT_SORT_SPILLING_THRESHOLD\njava.lang.String DEFAULT_AKKA_TRANSPORT_HEARTBEAT_INTERVAL\njava.lang.String CONTAINERIZED_MASTER_ENV_PREFIX\nint DEFAULT_JOB_MANAGER_WEB_ARCHIVE_COUNT\njava.lang.String TASK_MANAGER_HOSTNAME_KEY\njava.lang.String AKKA_WATCH_HEARTBEAT_INTERVAL\njava.lang.String DEFAULT_TASK_MANAGER_TMP_PATH\nint DEFAULT_EXECUTION_RETRIES\nint DEFAULT_JOB_MANAGER_WEB_FRONTEND_PORT\njava.lang.String JOB_MANAGER_WEB_LOG_PATH_KEY\njava.lang.String TASK_MANAGER_MEMORY_SIZE_KEY\njava.lang.String DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_NAME\njava.lang.String TASK_MANAGER_DATA_PORT_KEY\njava.lang.String ZOOKEEPER_CHECKPOINTS_PATH\njava.lang.String HA_JOB_MANAGER_PORT\njava.lang.String TASK_MANAGER_REFUSED_REGISTRATION_PAUSE\njava.lang.String CONTAINERIZED_HEAP_CUTOFF_RATIO\njava.lang.String DEFAULT_SORT_SPILLING_THRESHOLD_KEY\njava.lang.String YARN_CONTAINER_START_COMMAND_TEMPLATE\nboolean DEFAULT_JOB_MANAGER_WEB_SSL_ENABLED\njava.lang.String LIBRARY_CACHE_MANAGER_CLEANUP_INTERVAL\njava.lang.String JOB_MANAGER_WEB_CHECKPOINTS_DISABLE\njava.lang.String DEFAULT_ZOOKEEPER_LEADER_PATH\nint DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_DELAY\njava.lang.String DEFAULT_TASK_MANAGER_MAX_REGISTRATION_PAUSE\njava.lang.String METRICS_REPORTERS_LIST\njava.lang.String DEFAULT_RECOVERY_MODE\nint DEFAULT_METRICS_LATENCY_HISTORY_SIZE\njava.lang.String TASK_MANAGER_INITIAL_REGISTRATION_PAUSE\njava.lang.String DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_ROLE\nint DEFAULT_JOB_MANAGER_WEB_CHECKPOINTS_HISTORY_SIZE\njava.lang.String YARN_PROPERTIES_FILE_LOCATION\njava.lang.String RECOVERY_JOB_MANAGER_PORT\nboolean DEFAULT_SECURITY_SSL_ENABLED\njava.lang.String MESOS_FAILOVER_TIMEOUT_SECONDS\njava.lang.String RUNTIME_HASH_JOIN_BLOOM_FILTERS_KEY\njava.lang.String ZOOKEEPER_LEADER_PATH\njava.lang.String ZOOKEEPER_MAX_RETRY_ATTEMPTS\njava.lang.String HA_ZOOKEEPER_CHECKPOINTS_PATH\njava.lang.String MESOS_RESOURCEMANAGER_FRAMEWORK_ROLE\nint DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_REFRESH_INTERVAL\njava.lang.String DEFAULT_ZOOKEEPER_MESOS_WORKERS_PATH\njava.lang.String JOB_MANAGER_IPC_PORT_KEY\njava.lang.String AKKA_WATCH_HEARTBEAT_PAUSE\njava.lang.String MESOS_RESOURCEMANAGER_FRAMEWORK_NAME\njava.lang.String DELIMITED_FORMAT_MAX_SAMPLE_LENGTH_KEY\njava.lang.String STATE_BACKEND\njava.lang.String MESOS_RESOURCEMANAGER_FRAMEWORK_PRINCIPAL\nlong DEFAULT_TASK_MANAGER_DEBUG_MEMORY_USAGE_LOG_INTERVAL_MS\njava.lang.String DEFAULT_AKKA_CLIENT_TIMEOUT\nint DEFAULT_SPILLING_MAX_FAN\njava.lang.String TASK_MANAGER_IPC_PORT_KEY\njava.lang.String TASK_MANAGER_MEMORY_OFF_HEAP_KEY\nboolean DEFAULT_FILESYSTEM_OVERWRITE\nboolean DEFAULT_USE_LARGE_RECORD_HANDLER\njava.lang.String HA_ZOOKEEPER_JOBGRAPHS_PATH\nboolean DEFAULT_BLOB_SERVICE_SSL_ENABLED\njava.lang.String ZOOKEEPER_SESSION_TIMEOUT\njava.lang.String TASK_MANAGER_NETWORK_DEFAULT_IO_MODE\njava.lang.String SECURITY_SSL_TRUSTSTORE_PASSWORD\nint DEFAULT_ZOOKEEPER_MAX_RETRY_ATTEMPTS\njava.lang.String AKKA_STARTUP_TIMEOUT\njava.lang.String TASK_MANAGER_TMP_DIR_KEY\njava.lang.String USE_LARGE_RECORD_HANDLER_KEY\njava.lang.String DEFAULT_ZOOKEEPER_DIR_KEY\nint DEFAULT_YARN_MIN_HEAP_CUTOFF\njava.lang.String TASK_MANAGER_DATA_SSL_ENABLED\njava.lang.String HDFS_DEFAULT_CONFIG\nboolean DEFAULT_TASK_MANAGER_DATA_SSL_ENABLED\njava.lang.String DEFAULT_ZOOKEEPER_JOBGRAPHS_PATH\njava.lang.String HA_ZOOKEEPER_MESOS_WORKERS_PATH\njava.lang.String BLOB_STORAGE_DIRECTORY_KEY\njava.lang.String DEFAULT_STATE_BACKEND\njava.lang.String HA_ZOOKEEPER_RETRY_WAIT\njava.lang.String AKKA_ASK_TIMEOUT\njava.lang.String JOB_MANAGER_WEB_SUBMIT_ENABLED_KEY\njava.lang.String DEFAULT_ZOOKEEPER_NAMESPACE_KEY\njava.lang.String DEFAULT_ZOOKEEPER_CHECKPOINTS_PATH\nint DEFAULT_LOCAL_NUMBER_JOB_MANAGER\njava.lang.String AKKA_TRANSPORT_HEARTBEAT_INTERVAL\njava.lang.String DEFAULT_ZOOKEEPER_CHECKPOINT_COUNTER_PATH\njava.lang.String FS_STREAM_OPENING_TIMEOUT_KEY\njava.lang.String SECURITY_SSL_TRUSTSTORE\njava.lang.String METRICS_SCOPE_NAMING_JM_JOB\njava.lang.String MESOS_INITIAL_TASKS\njava.lang.String AKKA_FRAMESIZE\nint DEFAULT_ZOOKEEPER_INIT_LIMIT\njava.lang.String JOB_MANAGER_WEB_BACK_PRESSURE_CLEAN_UP_INTERVAL\njava.lang.String SECURITY_SSL_KEYSTORE\nboolean DEFAULT_MESOS_ARTIFACT_SERVER_SSL_ENABLED\njava.lang.String HA_ZOOKEEPER_MAX_RETRY_ATTEMPTS\nint DEFAULT_PARALLELISM\njava.lang.String RECOVERY_MODE\njava.lang.String EXECUTION_RETRIES_KEY\njava.lang.String METRICS_REPORTER_SCOPE_DELIMITER\njava.lang.String LOCAL_START_WEBSERVER\njava.lang.String LOCAL_NUMBER_JOB_MANAGER\njava.lang.String RESTART_STRATEGY\njava.lang.String ZOOKEEPER_QUORUM_KEY\nint DEFAULT_MESOS_FAILOVER_TIMEOUT_SECS\nboolean DEFAULT_TASK_MANAGER_MEMORY_PRE_ALLOCATE\nint DEFAULT_LOCAL_NUMBER_RESOURCE_MANAGER\njava.lang.String HA_ZOOKEEPER_CLIENT_ACL\njava.lang.String METRICS_REPORTER_FACTORY_CLASS_SUFFIX\nboolean DEFAULT_FILESYSTEM_ALWAYS_CREATE_DIRECTORY\njava.lang.String BLOB_FETCH_CONCURRENT_KEY\njava.lang.String FILESYSTEM_DEFAULT_OVERWRITE_KEY\njava.lang.String RESOURCE_MANAGER_IPC_PORT_KEY\njava.lang.String DEFAULT_AKKA_ASK_TIMEOUT\nint DEFAULT_ZOOKEEPER_CLIENT_PORT\ndouble DEFAULT_AKKA_TRANSPORT_THRESHOLD\njava.lang.String DEFAULT_AKKA_FRAMESIZE\njava.lang.String TASK_MANAGER_NUM_TASK_SLOTS\njava.lang.String YARN_APPLICATION_MASTER_ENV_PREFIX\njava.lang.String JOB_MANAGER_WEB_BACK_PRESSURE_DELAY\nlong DEFAULT_TASK_CANCELLATION_INTERVAL_MILLIS\njava.lang.String TASK_MANAGER_MEMORY_PRE_ALLOCATE_KEY\njava.lang.String FILESYSTEM_SCHEME\njava.lang.String TASK_MANAGER_MAX_REGISTRATION_DURATION\njava.lang.String HA_ZOOKEEPER_DIR_KEY\njava.lang.String DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_USER\njava.lang.String DEFAULT_FILESYSTEM_SCHEME\njava.lang.String MESOS_RESOURCEMANAGER_FRAMEWORK_SECRET\nint DEFAULT_DELIMITED_FORMAT_MAX_SAMPLE_LEN\njava.lang.String ENV_FLINK_BIN_DIR\nfloat DEFAULT_YARN_HEAP_CUTOFF_RATIO\njava.lang.String SAVEPOINT_FS_DIRECTORY_KEY\njava.lang.String AKKA_JVM_EXIT_ON_FATAL_ERROR\njava.lang.String ZOOKEEPER_RETRY_WAIT\njava.lang.String HA_ZOOKEEPER_NAMESPACE_KEY\njava.lang.String ZOOKEEPER_CONNECTION_TIMEOUT\njava.lang.String TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY\njava.lang.String JOB_MANAGER_WEB_ARCHIVE_COUNT\nint DEFAULT_RESOURCE_MANAGER_IPC_PORT\nint DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_CLEAN_UP_INTERVAL\njava.lang.String YARN_REALLOCATE_FAILED_CONTAINERS\njava.lang.String SECURITY_SSL_KEYSTORE_PASSWORD\njava.lang.String DEFAULT_HA_JOB_MANAGER_PORT\njava.lang.String BLOB_FETCH_RETRIES_KEY\njava.lang.String METRICS_REPORTER_EXCLUDED_VARIABLES\njava.lang.String DEFAULT_SECURITY_SSL_PROTOCOL\njava.lang.String RECOVERY_JOB_DELAY\njava.lang.String TASK_CANCELLATION_INTERVAL_MILLIS\njava.lang.String YARN_APPLICATION_MASTER_PORT\nint DEFAULT_TASK_MANAGER_DATA_PORT\njava.lang.String RESTART_STRATEGY_FAILURE_RATE_FAILURE_RATE_INTERVAL\njava.lang.String YARN_TASK_MANAGER_ENV_PREFIX\nint DEFAULT_DELIMITED_FORMAT_MIN_LINE_SAMPLES\njava.lang.String AKKA_LOG_LIFECYCLE_EVENTS\n```\n\n----------------------------------------\n\nTITLE: Displaying Info Label in Markdown\nDESCRIPTION: Markdown snippet for displaying an info label with an icon and custom styling.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-07-23-flink-network-stack-2.md#2025-04-08_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"alert alert-info\" markdown=\"1\">\n<span class=\"label label-info\" style=\"display: inline-block\"><span class=\"glyphicon glyphicon-info-sign\" aria-hidden=\"true\"></span> Note</span>\n// Note content here\n</div>\n```\n\n----------------------------------------\n\nTITLE: Building and Running Test Environment in Shell\nDESCRIPTION: Shell command to build the connector and start the test environment including Flink cluster, test email server, and SQL client.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part1.md#2025-04-08_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n$ cd testing/\n$ ./build_and_run.sh\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Containers for Flink SQL Demo Environment\nDESCRIPTION: Command to stop all containers after finishing the tutorial.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose down\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Buffer Parameters in Flink\nDESCRIPTION: Reference to network buffer configuration parameters that can be modified to optimize throughput in Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-07-23-flink-network-stack-2.md#2025-04-08_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\nbuffers-per-channel\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Remote-Write Protocol with Protobuf\nDESCRIPTION: Protobuf definition for the Prometheus Remote-Write interface, showing the structure of WriteRequest, TimeSeries, Label and Sample messages used for sending time-series data to Prometheus.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-12-05-introducing-new-prometheus-connector.md#2025-04-08_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\nfunc Send(WriteRequest)\n\nmessage WriteRequest {\n  repeated TimeSeries timeseries = 1;\n  // [...]\n}\n\nmessage TimeSeries {\n  repeated Label labels   = 1;\n  repeated Sample samples = 2;\n}\n\nmessage Label {\n  string name  = 1;\n  string value = 2;\n}\n\nmessage Sample {\n  double value    = 1;\n  int64 timestamp = 2;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML for Fine-grained Batch Recovery in Flink\nDESCRIPTION: Configuration entry required in flink-conf.yaml to enable the new region-based failover strategy for fine-grained batch recovery.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-08-22-release-1.9.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\njobmanager.execution.failover-strategy: region\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.1.1\nDESCRIPTION: Maven dependency configuration showing the recommended artifacts for Flink 1.1.1. Includes core dependencies for Flink Java, Streaming Java, and Clients modules. This update fixes Hadoop dependency issues present in version 1.1.0.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-08-04-release-1.1.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.1.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.10</artifactId>\n  <version>1.1.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.10</artifactId>\n  <version>1.1.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Counting Git Commits in 2017 using Bash\nDESCRIPTION: Command to count the total number of Git commits made to the Flink repository after December 31, 2016\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-12-21-2017-year-in-review.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit log --pretty=oneline --after=12/31/2016 | wc -l\n```\n\n----------------------------------------\n\nTITLE: Removed Method from RemoteStreamEnvironment Class\nDESCRIPTION: The getClientConfiguration method has been removed from the RemoteStreamEnvironment class. This method was used to retrieve the configuration for the client connecting to a remote Flink cluster.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_22\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.configuration.Configuration getClientConfiguration()\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster Resource View Refresh in YAML\nDESCRIPTION: Setting to enable automatic cluster resource capacity checking, which monitors available node metrics and maximum node sizes for autoscaling decisions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-21-release-kubernetes-operator-1.8.0.md#2025-04-08_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nkubernetes.operator.cluster.resource-view.refresh-interval: 5 min\n```\n\n----------------------------------------\n\nTITLE: Configuring Allowed Lateness for Windows in Java\nDESCRIPTION: This snippet demonstrates how to specify allowed lateness for windowed operations in Flink 1.1.0, controlling how late elements are handled.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-08-04-release-1.1.0.md#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\ninput.keyBy(<key selector>).window(<window assigner>)\n    .allowedLateness(<time>)\n    .<windowed transformation>(<window function>);\n```\n\n----------------------------------------\n\nTITLE: Apache Flink API Class References\nDESCRIPTION: Index listing of key Apache Flink API classes organized by functionality, including streaming sources/sinks, table interfaces, serialization utilities, window operations, and factory classes. Each entry represents a fully qualified class name in the Flink codebase.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2025-03-24-release-2.0.0.md#2025-04-08_snippet_4\n\nLANGUAGE: text\nCODE:\n```\norg.apache.flink.streaming.api.*\norg.apache.flink.table.api.*\norg.apache.flink.table.connector.*\norg.apache.flink.table.descriptors.*\norg.apache.flink.table.factories.*\norg.apache.flink.table.planner.*\norg.apache.flink.table.sinks.*\norg.apache.flink.table.sources.*\norg.apache.flink.table.types.*\norg.apache.flink.walkthrough.common.*\n```\n\n----------------------------------------\n\nTITLE: Pushing Documentation Changes to GitHub Fork\nDESCRIPTION: Git command to push committed documentation changes to a fork of the Flink repository on GitHub. This is part of the pull request preparation process.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/contribute-documentation.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit push origin myBranch\n```\n\n----------------------------------------\n\nTITLE: Removed Field from StreamExecutionEnvironment Class\nDESCRIPTION: The DEFAULT_JOB_NAME constant has been removed from the StreamExecutionEnvironment class. This constant defined the default name for Flink jobs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_23\n\nLANGUAGE: java\nCODE:\n```\njava.lang.String DEFAULT_JOB_NAME\n```\n\n----------------------------------------\n\nTITLE: Adding Flink Test Dependencies in Maven\nDESCRIPTION: XML configuration to add necessary Flink test dependencies to a Maven project. These dependencies provide the testing framework for Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-03-a-guide-for-unit-testing-in-apache-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-test-utils_${scala.binary.version}</artifactId>\n  <version>${flink.version}</version>\n  <scope>test</scope>\n</dependency> \n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-runtime_${scala.binary.version}</artifactId>\n  <version>${flink.version}</version>\n  <scope>test</scope>\n  <classifier>tests</classifier>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_${scala.binary.version}</artifactId>\n  <version>${flink.version}</version>\n  <scope>test</scope>\n  <classifier>tests</classifier>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Enabling Dynamic Watched Namespaces in Flink Kubernetes Operator\nDESCRIPTION: Shows the configuration to enable dynamic watched namespaces feature, allowing the operator to watch and manage custom resources in specified namespaces dynamically.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-07-25-release-kubernetes-operator-1.1.0.md#2025-04-08_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nkubernetes.operator.watched.namespaces: ns1,ns2\nkubernetes.operator.dynamic.namespaces.enabled: true\n```\n\n----------------------------------------\n\nTITLE: Defining a Remote Function Endpoint in YAML\nDESCRIPTION: A YAML configuration that declares a remote endpoint and function type for Stateful Functions 2.0, showing how to define modules without writing Flink code.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-07-release-statefun-2.0.0.md#2025-04-08_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n<img src=\"/img/blog/2020-04-07-release-statefun-2.0.0/image3.png\" width=\"600px\" alt=\"Statefun 3\"/>\n```\n\n----------------------------------------\n\nTITLE: Viewing Fraud Transaction Logs\nDESCRIPTION: Command to view the logs of the actor pod which displays detected fraudulent transactions. It selects the pod using a label selector and outputs its logs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl logs $(kubectl get pod -l actor=flink-demo -o jsonpath=\"{.items[0].metadata.name}\")\nBroker:   flink-demo-kafka-kafka-0.flink-demo-kafka-svc:9093\nTopic:   fraud\n\nDetected Fraud:   TransactionAggregate {startTimestamp=0, endTimestamp=1563395831000, totalAmount=19895:\nTransaction{timestamp=1563395778000, origin=1, target='3', amount=8341}\nTransaction{timestamp=1563395813000, origin=1, target='3', amount=8592}\nTransaction{timestamp=1563395817000, origin=1, target='3', amount=2802}\nTransaction{timestamp=1563395831000, origin=1, target='3', amount=160}}\n```\n\n----------------------------------------\n\nTITLE: Cloning Flink Website Repository\nDESCRIPTION: Command to clone the forked Flink website repository to local machine and switch to the asf-site branch where the website content resides.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/improve-website.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<your-user-name>/flink-web.git\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd flink-web\ngit checkout asf-site\n```\n\n----------------------------------------\n\nTITLE: Removed Methods in JobClient (Java)\nDESCRIPTION: Documentation of removed methods in the JobClient interface. These methods were likely replaced or deprecated in favor of new implementations for managing savepoints.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\njava.util.concurrent.CompletableFuture<java.lang.String> stopWithSavepoint(boolean, java.lang.String)\n```\n\nLANGUAGE: Java\nCODE:\n```\njava.util.concurrent.CompletableFuture<java.lang.String> triggerSavepoint(java.lang.String)\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.12.3\nDESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.12.3 for flink-java, flink-streaming-java, and flink-clients artifacts.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-04-29-release-1.12.3.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.12.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.12.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.12.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.16.2\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies are required for building Flink applications using version 1.16.2.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-25-release-1.16.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.16.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.16.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.16.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Defining Time Windows in Apache Flink with Scala\nDESCRIPTION: This code snippet demonstrates how to create tumbling and sliding time windows in Apache Flink using the Scala API. It shows how to key a stream by sensorId and apply time-based windows of 1 minute length, with the sliding window having a 30-second trigger interval.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-04-Introducing-windows.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n// Stream of (sensorId, carCnt)\nval vehicleCnts: DataStream[(Int, Int)] = ...\n\nval tumblingCnts: DataStream[(Int, Int)] = vehicleCnts\n  // key stream by sensorId\n  .keyBy(0) \n  // tumbling time window of 1 minute length\n  .timeWindow(Time.minutes(1))\n  // compute sum over carCnt\n  .sum(1) \n\nval slidingCnts: DataStream[(Int, Int)] = vehicleCnts\n  .keyBy(0) \n  // sliding time window of 1 minute length and 30 secs trigger interval\n  .timeWindow(Time.minutes(1), Time.seconds(30))\n  .sum(1)\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.9.3\nDESCRIPTION: This XML snippet shows how to update Maven dependencies to use Apache Flink 1.9.3. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-24-release-1.9.3.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.9.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.9.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.9.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.14.5\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java and flink-clients. These dependencies are required for building Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-06-22-release-1.14.5.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.14.5</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.14.5</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.14.5</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Building Flink Website Non-Incrementally with Local Hugo\nDESCRIPTION: Command to rebuild the Flink website non-incrementally using a local Hugo installation. This generates static HTML files in the 'content' folder for serving the project website.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/README.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./build.sh build\n```\n\n----------------------------------------\n\nTITLE: Removed Fields from ClusterOptions in Apache Flink\nDESCRIPTION: List of configuration options removed from the ClusterOptions class related to fine-grained shuffle mode and slot distribution strategy.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.configuration.ConfigOption<java.lang.Boolean> FINE_GRAINED_SHUFFLE_MODE_ALL_BLOCKING\n```\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.configuration.ConfigOption<java.lang.Boolean> EVENLY_SPREAD_OUT_SLOTS_STRATEGY\n```\n\n----------------------------------------\n\nTITLE: Declaring Apache Flink 1.14.6 Maven Dependencies\nDESCRIPTION: XML snippet showing how to declare Maven dependencies for Apache Flink 1.14.6 core components including flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-09-28-release-1.14.6.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.14.6</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.14.6</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.14.6</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.5.6\nDESCRIPTION: XML configuration for adding Apache Flink 1.5.6 dependencies to a Maven project. Includes the core Java API, streaming Java API for Scala 2.11, and the client library for Scala 2.11.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-12-26-release-1.5.6.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.5.6</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.5.6</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.5.6</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Querying Tables with Flink's Table API in Scala\nDESCRIPTION: This snippet demonstrates using Flink's new Table API to perform SQL-like queries on distributed datasets. It shows grouping, joining, and filtering operations on tables representing clicks and users data.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-04-13-release-0.9.0-milestone1.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval clickCounts = clicks\n  .groupBy('user).select('userId, 'url.count as 'count)\n\nval activeUsers = users.join(clickCounts)\n  .where('id === 'userId && 'count > 10).select('username, 'count, ...)\n```\n\n----------------------------------------\n\nTITLE: Installing Financial Fraud Demo Operator\nDESCRIPTION: Commands to change directory to the operators repo and install the financial fraud demo operator from the local filesystem, which will deploy Flink, Kafka, and ZooKeeper instances.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ cd operators\n$ kubectl kudo install repository/flink/docs/demo/financial-fraud/demo-operator --instance flink-demo\ninstance.kudo.dev/v1beta1/flink-demo created\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.18.1\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients at version 1.18.1.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-01-19-release-1.18.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.18.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.18.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.18.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Building Flink Website Non-Incrementally with Hugo Docker\nDESCRIPTION: Command to rebuild the Flink website non-incrementally using a Hugo Docker image. This generates static HTML files in the 'content' folder for serving the project website.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/README.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./docker-build.sh build\n```\n\n----------------------------------------\n\nTITLE: Removed Constructors in SingleThreadMultiplexSourceReaderBase (Java)\nDESCRIPTION: Documentation of removed constructors in the SingleThreadMultiplexSourceReaderBase class. These constructors were likely replaced or deprecated in favor of new implementations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nSingleThreadMultiplexSourceReaderBase(org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue<org.apache.flink.connector.base.source.reader.RecordsWithSplitIds<E>>, java.util.function.Supplier<org.apache.flink.connector.base.source.reader.splitreader.SplitReader<E,SplitT>>, org.apache.flink.connector.base.source.reader.RecordEmitter<E,T,SplitStateT>, org.apache.flink.configuration.Configuration, org.apache.flink.api.connector.source.SourceReaderContext)\n```\n\nLANGUAGE: Java\nCODE:\n```\nSingleThreadMultiplexSourceReaderBase(org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue<org.apache.flink.connector.base.source.reader.RecordsWithSplitIds<E>>, org.apache.flink.connector.base.source.reader.fetcher.SingleThreadFetcherManager<E,SplitT>, org.apache.flink.connector.base.source.reader.RecordEmitter<E,T,SplitStateT>, org.apache.flink.configuration.Configuration, org.apache.flink.api.connector.source.SourceReaderContext)\n```\n\nLANGUAGE: Java\nCODE:\n```\nSingleThreadMultiplexSourceReaderBase(org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue<org.apache.flink.connector.base.source.reader.RecordsWithSplitIds<E>>, org.apache.flink.connector.base.source.reader.fetcher.SingleThreadFetcherManager<E,SplitT>, org.apache.flink.connector.base.source.reader.RecordEmitter<E,T,SplitStateT>, org.apache.flink.connector.base.source.reader.RecordEvaluator<T>, org.apache.flink.configuration.Configuration, org.apache.flink.api.connector.source.SourceReaderContext)\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.5.2\nDESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink 1.5.2 for flink-java, flink-streaming-java, and flink-clients modules.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-07-31-release-1.5.2.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.5.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.5.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.5.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.4.1\nDESCRIPTION: XML snippet showing how to update Maven dependencies for Apache Flink 1.4.1. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-02-15-release-1.4.1.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.4.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.4.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.4.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Tumbling Window Implementation using Flink Table API\nDESCRIPTION: Scala implementation of tumbling window aggregation using Flink's Table API, showing how to partition data and compute daily averages of temperature readings.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-05-24-stream-sql.md#2025-04-08_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval avgRoomTemp: Table = tableEnv.ingest(\"sensorData\")\n  .where('location.like(\"room%\"))\n  .partitionBy('location)\n  .window(Tumbling every Days(1) on 'time as 'w)\n  .select('w.end, 'location, , (('tempF - 32) * 0.556).avg as 'avgTempCs)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Table Styles in Markdown\nDESCRIPTION: CSS styles for customizing table appearance in Markdown, including border collapse, padding, and alignment options.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-07-23-flink-network-stack-2.md#2025-04-08_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{padding:10px 10px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n.tg th{padding:10px 10px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;background-color:#eff0f1;}\n.tg .tg-wide{padding:10px 30px;}\n.tg .tg-top{vertical-align:top}\n.tg .tg-topcenter{text-align:center;vertical-align:top}\n.tg .tg-center{text-align:center;vertical-align:center}\n</style>\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.6.3\nDESCRIPTION: XML configuration for adding the core Apache Flink 1.6.3 dependencies to a Maven project. It includes the base Java API, streaming Java API, and clients module for Scala 2.11.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-12-22-release-1.6.3.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.6.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.6.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.6.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Adding Info Alert in Flink Documentation HTML\nDESCRIPTION: This HTML snippet demonstrates how to create an info alert box in Flink documentation to highlight tips or helpful information.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"alert alert-info\"> // Info Message </div>\n```\n\n----------------------------------------\n\nTITLE: Modified Methods in AbstractStreamOperator\nDESCRIPTION: Methods in AbstractStreamOperator where access modifiers have been changed from public to protected. This indicates a tightening of the API to prevent direct access to these methods from outside the class hierarchy.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_26\n\nLANGUAGE: java\nCODE:\n```\nPROTECTED (<- PUBLIC) void setProcessingTimeService(org.apache.flink.streaming.runtime.tasks.ProcessingTimeService)\nPROTECTED (<- PUBLIC) void setup(org.apache.flink.streaming.runtime.tasks.StreamTask<?,?><?,?>, org.apache.flink.streaming.api.graph.StreamConfig, org.apache.flink.streaming.api.operators.Output<org.apache.flink.streaming.runtime.streamrecord.StreamRecord<OUT>><org.apache.flink.streaming.runtime.streamrecord.StreamRecord<OUT>>)\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.8.2\nDESCRIPTION: XML code snippet showing the Maven dependency declarations for core Flink components updated to version 1.8.2. Includes the flink-java core library, flink-streaming-java for streaming applications, and flink-clients for client functionality.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-09-11-release-1.8.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.8.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.8.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.8.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.7.2\nDESCRIPTION: Maven dependency configuration for Apache Flink 1.7.2 core components including Java API, Streaming Java API, and Flink clients. These dependencies are required to develop applications using Flink 1.7.2.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-02-15-release-1.7.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.7.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.7.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.7.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Creating Sink Table in Flink SQL\nDESCRIPTION: SQL statement to create a sink table for streaming ETL in Flink. The statement defines the output table structure and configures it to use Elasticsearch as the destination.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-06-15-flink-on-zeppelin-part1.md#2025-04-08_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE sink_table (\n  id STRING,\n  name STRING,\n  age INT\n) WITH (\n  'connector.type' = 'elasticsearch',\n  'connector.version' = '6',\n  'connector.hosts' = 'http://localhost:9200',\n  'connector.index' = 'test',\n  'connector.document-type' = 'doc',\n  'format.type' = 'json',\n  'update-mode' = 'append'\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.11.4\nDESCRIPTION: Maven dependency configurations for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies should be added to the project's pom.xml file to use Flink 1.11.4.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-08-09-release-1.11.4.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.11.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.11.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.11.4</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.9.1\nDESCRIPTION: XML snippet showing how to configure Maven dependencies for Flink 1.9.1, including the core Java API, streaming Java API, and client modules. These dependencies should be added to a project's pom.xml file.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-10-18-release-1.9.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.9.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.9.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.9.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Liquid Table of Contents Generation\nDESCRIPTION: Markdown/Liquid syntax for automatically generating a table of contents from page headings.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_3\n\nLANGUAGE: liquid\nCODE:\n```\n{{ \"{:toc\" }}}\n```\n\nLANGUAGE: liquid\nCODE:\n```\n{{ \"# Excluded Heading\\n{:.no_toc\" }}}\n```\n\n----------------------------------------\n\nTITLE: Removed Interface from AbstractStreamOperator\nDESCRIPTION: The SetupableStreamOperator interface has been removed from AbstractStreamOperator. This interface was likely used for operator setup operations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_25\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.streaming.api.operators.SetupableStreamOperator\n```\n\n----------------------------------------\n\nTITLE: Declaring Maven Dependencies for Apache Flink 1.1.3\nDESCRIPTION: XML snippet showing how to declare Maven dependencies for Flink Java, Flink Streaming, and Flink Clients artifacts, all version 1.1.3.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-10-12-release-1.1.3.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.1.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.10</artifactId>\n  <version>1.1.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.10</artifactId>\n  <version>1.1.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Adding Apache Flink ML Dependencies to Maven POM\nDESCRIPTION: This XML snippet shows how to add Apache Flink ML dependencies to a Maven project's pom.xml file. It includes core ML libraries, iteration support, and pre-built ML algorithms.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/downloads.md#2025-04-08_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-ml-core</artifactId>\n  <version>{{< param FlinkMLStableVersion >}}</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-ml-iteration</artifactId>\n  <version>{{< param FlinkMLStableVersion >}}</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-ml-lib</artifactId>\n  <version>{{< param FlinkMLStableVersion >}}</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Execution Mode via Command Line in Apache Flink\nDESCRIPTION: This snippet demonstrates how to set the execution runtime mode to BATCH using command line parameters when submitting a Flink job.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-03-11-batch-execution-mode.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar\n```\n\n----------------------------------------\n\nTITLE: Configuring Flink Autoscaler in YAML\nDESCRIPTION: Shows the new configuration keys for the Flink autoscaler after the removal of the 'kubernetes.operator.' prefix. These settings enable the autoscaler and set the metrics window.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-11-22-release-kubernetes-operator-1.7.0.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\njob.autoscaler.enabled\njob.autoscaler.metrics.window\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.1.4\nDESCRIPTION: XML snippet showing how to update Maven dependencies for Apache Flink 1.1.4. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-12-21-release-1.1.4.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.1.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.10</artifactId>\n  <version>1.1.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.10</artifactId>\n  <version>1.1.4</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Liquid Template Variable Usage\nDESCRIPTION: Example of accessing site configuration variables in Liquid templates within Flink documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_2\n\nLANGUAGE: liquid\nCODE:\n```\n{{ \"{{ site.CONFIG_KEY \" }}}}\n```\n\n----------------------------------------\n\nTITLE: Deprecated RuntimeContext Methods - Java\nDESCRIPTION: Reference to deprecated getExecutionConfig method in RuntimeContext, to be replaced with specific getter methods for different functionalities.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_9\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.api.common.functions.RuntimeContext#getExecutionConfig\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.12.4\nDESCRIPTION: XML snippet showing how to update Maven dependencies for Apache Flink 1.12.4. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-21-release-1.12.4.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.12.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.12.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.12.4</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Proper Lambda Implementation Without Capturing Variables\nDESCRIPTION: Example of efficient lambda implementation that uses the parameter provided by the lambda rather than capturing from the outer scope.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nmap.computeIfAbsent(key, k -> k.toLowerCase());\n```\n\n----------------------------------------\n\nTITLE: Using Named Parameters in SQL Function Calls\nDESCRIPTION: Demonstrates how to call functions using named parameters in SQL queries, including optional parameter handling.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM TABLE(myFunction(in1 => 'v1', in3 => 'v3', in2 => 'v2'));\n\nSELECT * FROM TABLE(myFunction(in1 => 'v1'));\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter for Flink Blog Post\nDESCRIPTION: YAML front matter defining metadata for the blog post including authors, date, excerpt, title, and URL aliases.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-18-latency-part1.md#2025-04-08_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nauthors:\n- Jun Qin: null\n  name: Jun Qin\n- Nico Kruber: null\n  name: Nico Kruber\ndate: \"2022-05-18T00:00:00Z\"\nexcerpt: This multi-part series of blog post presents a collection of low-latency\n  techniques in Flink. Part one starts with types of latency in Flink and the way\n  we measure the end-to-end latency, followed by a few techniques that optimize latency\n  directly.\ntitle: Getting into Low-Latency Gears with Apache Flink - Part One\naliases:\n- /2022/05/18/latency-part1.html\n---\n```\n\n----------------------------------------\n\nTITLE: Failure Simulation Code Reference\nDESCRIPTION: Reference to a modified version of TPC-H Query 3 example and failure coordinator for testing fine-grained recovery functionality. The code is used to simulate failures in operators based on exponential distribution.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-11-batch-fine-grained-fault-tolerance.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nhttps://github.com/rmetzger/flip1-bench/blob/master/flip1-bench-jobs/src/main/java/com/ververica/TPCHQuery3.java\n```\n\nLANGUAGE: java\nCODE:\n```\nhttps://github.com/rmetzger/flip1-bench/blob/master/flip1-bench-jobs/src/main/java/com/ververica/utilz/KillerServer.java\n```\n\n----------------------------------------\n\nTITLE: Deprecated Configuration Methods - Java\nDESCRIPTION: Reference to deprecated getter/setter methods in Configuration class, which should be replaced with ConfigOption-based alternatives.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_8\n\nLANGUAGE: java\nCODE:\n```\ngetString(String key, String defaultValue)\nsetString(String key, String value)\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter for Flink 2.0.0 Release Announcement\nDESCRIPTION: YAML front matter defining metadata for the Flink 2.0.0 release announcement, including author information, publication date, title, and URL aliases.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2025-03-24-release-2.0.0.md#2025-04-08_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nauthors:\n- xtsong:\n  name: \"Xintong Song\"\ndate: \"2025-03-24T08:00:00Z\"\ntitle: \"Apache Flink 2.0.0: A new Era of Real-Time Data Processing\"\naliases:\n- /news/2025/03/24/release-2.0.html\n---\n```\n\n----------------------------------------\n\nTITLE: Removed Collection Methods in DataStream API\nDESCRIPTION: List of collection-related methods that have been removed from the DataStream API. These methods allowed collecting data from streams for testing and debugging purposes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_14\n\nLANGUAGE: java\nCODE:\n```\njava.util.Iterator<OUT> collect(org.apache.flink.streaming.api.datastream.DataStream<OUT>, java.lang.String)\njava.util.List<E> collectBoundedStream(org.apache.flink.streaming.api.datastream.DataStream<E>, java.lang.String)\njava.util.List<E> collectRecordsFromUnboundedStream(org.apache.flink.streaming.api.operators.collect.ClientAndIterator<E>, int)\njava.util.List<E> collectUnboundedStream(org.apache.flink.streaming.api.datastream.DataStream<E>, int, java.lang.String)\norg.apache.flink.streaming.api.operators.collect.ClientAndIterator<OUT> collectWithClient(org.apache.flink.streaming.api.datastream.DataStream<OUT>, java.lang.String)\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison Table in Markdown\nDESCRIPTION: A markdown table comparing performance metrics between Flink 1.12 and 1.14, showing improvements in job initialization, task deployment, and failover computation times\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-04-scheduler-performance-part-one.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<center>\nTable 1 - The comparison of time cost between Flink 1.12 and 1.14\n<table width=\"95%\" border=\"1\">\n  <thead>\n    <tr>\n      <th style=\"text-align: center\">Procedure</th>\n      <th style=\"text-align: center\">1.12</th>\n      <th style=\"text-align: center\">1.14</th>\n      <th style=\"text-align: center\">Reduction(%)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center\">Job Initialization</td>\n      <td style=\"text-align: center\">11,431ms</td>\n      <td style=\"text-align: center\">627ms</td>\n      <td style=\"text-align: center\">94.51%</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\">Task Deployment</td>\n      <td style=\"text-align: center\">63,118ms</td>\n      <td style=\"text-align: center\">17,183ms</td>\n      <td style=\"text-align: center\">72.78%</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\">Computing tasks to restart when failover</td>\n      <td style=\"text-align: center\">37,195ms</td>\n      <td style=\"text-align: center\">170ms</td>\n      <td style=\"text-align: center\">99.55%</td>\n    </tr>\n  </tbody>\n</table>\n</center>\n```\n\n----------------------------------------\n\nTITLE: Flink Application Mode with Remote Library and Application Jar\nDESCRIPTION: Complete launch command utilizing both remote Flink distribution and pre-uploaded application jar for maximum bandwidth optimization.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-14-application-mode.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./bin/flink run-application -t yarn-application \\\n    -Djobmanager.memory.process.size=2048m \\\n    -Dtaskmanager.memory.process.size=4096m \\\n    -Dyarn.provided.lib.dirs=\"hdfs://myhdfs/remote-flink-dist-dir\" \\\n    hdfs://myhdfs/jars/MyApplication.jar\n```\n\n----------------------------------------\n\nTITLE: Incorrect Lambda Usage with Capturing Variables\nDESCRIPTION: Example of inefficient lambda implementation that captures variables from the outer scope, requiring a new object instance for each call.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nmap.computeIfAbsent(key, x -> key.toLowerCase())\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata and External Link in Hugo Markdown\nDESCRIPTION: This snippet defines metadata for a Hugo page, including weight, title, and bookHref. It also uses a Hugo shortcode to create an external link to the Flink Kubernetes Operator documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-kubernetes-operator-master.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nweight: 5\ntitle: Kubernetes Operator Main (snapshot)\nbookHref: \"https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main\"\n---\n\n# Flink Kubernetes Operator documentation (latest snapshot)\n\n{{< external_link name=\"You can find the Flink Kubernetes Operator documentation for the latest snapshot here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Creating HTML Table for Flink Resources\nDESCRIPTION: HTML table structure for organizing Flink-related content into categories including blogposts and tutorials, with embedded glyphicon icons and hyperlinks to various resources.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-03-30-community-update.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"table table-bordered\">\n  <thead>\n    <tr>\n      <th>Type</th>\n      <th>Links</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><span class=\"glyphicon glyphicon glyphicon-bookmark\" aria-hidden=\"true\"></span> Blogposts</td>\n      <td><ul>\n\t\t  <li><a href=\"https://medium.com/bird-engineering/replayable-process-functions-in-flink-time-ordering-and-timers-28007a0210e1\">Replayable Process Functions: Time, Ordering, and Timers @Bird</a></li>\n\t\t  <li><a href=\"https://engineering.salesforce.com/application-log-intelligence-performance-insights-at-salesforce-using-flink-92955f30573f\">Application Log Intelligence & Performance Insights at Salesforce Using Flink @Salesforce</a></li>\n\t\t  </ul>\n\t\t  <ul>\n\t\t  <li><a href=\"https://flink.apache.org/news/2020/01/29/state-unlocked-interacting-with-state-in-apache-flink.html\">State Unlocked: Interacting with State in Apache Flink</a></li>\n\t\t  <li><a href=\"https://flink.apache.org/news/2020/01/15/demo-fraud-detection.html\">Advanced Flink Application Patterns Vol.1: Case Study of a Fraud Detection System</a></li>\n\t\t  <li><a href=\"https://flink.apache.org/news/2020/03/24/demo-fraud-detection-2.html\">Advanced Flink Application Patterns Vol.2: Dynamic Updates of Application Logic</a></li>\n\t\t  <li><a href=\"https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html\">Apache Beam: How Beam Runs on Top of Flink</a></li>\n\t\t  <li><a href=\"https://flink.apache.org/features/2020/03/27/flink-for-data-warehouse.html\">Flink as Unified Engine for Modern Data Warehousing: Production-Ready Hive Integration</a></li>\n\t\t</ul>\n\t  </td>\n    </tr>\n    <tr>\n      <td><span class=\"glyphicon glyphicon-console\" aria-hidden=\"true\"></span> Tutorials</td>\n      <td><ul>\n      \t  <li><a href=\"https://medium.com/@zjffdu/flink-on-zeppelin-part-3-streaming-5fca1e16754\">Flink on Zeppelin â€” (Part 3). Streaming</a></li>\n\t\t  <li><a href=\"https://aws.amazon.com/blogs/big-data/streaming-etl-with-apache-flink-and-amazon-kinesis-data-analytics\">Streaming ETL with Apache Flink and Amazon Kinesis Data Analytics</a></li>\n\t\t  <li><a href=\"https://flink.apache.org/news/2020/02/20/ddl.html\">No Java Required: Configuring Sources and Sinks in SQL</a></li>\n\t\t  <li><a href=\"https://flink.apache.org/news/2020/02/07/a-guide-for-unit-testing-in-apache-flink.html\">A Guide for Unit Testing in Apache Flink</a></li>\n\t\t  </ul>\n\t  </td>\n    </tr>\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Displaying Latency Metrics in Markdown Table\nDESCRIPTION: Shows the 'latency' and 'restartingTime' metrics in a markdown table, indicating latency from source to operator and job restart time respectively.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-02-21-monitoring-best-practices.md#2025-04-08_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric | Scope | Description |\n| ------ | ----- | ----------- |\n| `latency` | operator | The latency from the source operator to this operator. |\n| `restartingTime` | job | The time it took to restart the job, or how long the current restart has been in progress. |\n```\n\n----------------------------------------\n\nTITLE: Java I/O Interfaces Used by Flink's Memory Management\nDESCRIPTION: The Java interfaces that Flink implements to provide logical views over multiple MemorySegments, allowing operations on them as if they were a larger chunk of consecutive memory.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-05-11-Juggling-with-Bits-and-Bytes.md#2025-04-08_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\njava.io.DataOutput\n```\n\nLANGUAGE: Java\nCODE:\n```\njava.io.DataInput\n```\n\n----------------------------------------\n\nTITLE: Correct Variable Declaration With Explicit Type - Scala\nDESCRIPTION: Example showing correct variable declaration with explicit type annotation for class members in Scala.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/how-to-contribute/code-style-and-quality-scala.md#2025-04-08_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nvar expressions: java.util.List[String] = new java.util.ArrayList[]()\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Page Parameters for Flink Docs\nDESCRIPTION: YAML front matter configuration for the Hugo documentation page, specifying the page weight, title, and external documentation link.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-master.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 3\ntitle: Flink Master (snapshot)\nbookHref: \"https://nightlies.apache.org/flink/flink-docs-master/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Creating HTML Table Layout for Flink Community Resources\nDESCRIPTION: HTML table structure used to display Apache Flink community resources organized by category, including upcoming events, blog posts, and Flink packages. The table uses Bootstrap styling and glyphicon icons to separate different content sections.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-29-community-update.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"table table-bordered\">\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><span class=\"glyphicon glyphicon glyphicon-console\" aria-hidden=\"true\"></span> Events</td>\n      <td><ul>\n        <b>Virtual Flink Meetup (Jul. 29)</b>\n        <p><a href=\"https://www.meetup.com/seattle-flink/events/271922632/\">What's new in Flink 1.11? + Q&A with Aljoscha Krettek</a></p>\n      </ul>\n      <ul>\n        <b>DC Thursday (Jul. 30)</b>\n        <p><a href=\"https://www.eventbrite.com/e/dc-thurs-apache-flink-w-stephan-ewen-tickets-112137488246?utm_campaign=Events%20%26%20Talks&utm_content=135006406&utm_medium=social&utm_source=twitter&hss_channel=tw-2581958070\">Interview and Community Q&A with Stephan Ewen</a></p>\n      </ul>\n      <ul>\n        <b>KubeCon + CloudNativeCon Europe (Aug. 17-20)</b>\n        <p><a href=\"https://kccnceu20.sched.com/event/ZelA/stateful-serverless-and-the-elephant-in-the-room-stephan-ewen-ververica\">Stateful Serverless and the Elephant in the Room</a></p>\n      </ul>\n      <ul>\n        <b>DataEngBytes (Aug. 20-21)</b>\n        <p><a href=\"https://dataengconf.com.au/\">Change Data Capture with Flink SQL and Debezium</a></p>\n        <p><a href=\"https://dataengconf.com.au/\">Sweet Streams are Made of These: Data Driven Development with Stream Processing</a></p>\n      </ul>\n      <ul>\n        <b>Beam Summit (Aug. 24-29)</b>\n        <p><a href=\"https://2020.beamsummit.org/sessions/streaming-fast-slow/\">Streaming, Fast and Slow</a></p>\n        <p><a href=\"https://2020.beamsummit.org/sessions/building-stateful-streaming-pipelines/\">Building Stateful Streaming Pipelines With Beam</a></p>\n      </ul>\n    </td>\n    </tr>\n    <tr>\n      <td><span class=\"glyphicon glyphicon-fire\" aria-hidden=\"true\"></span> Blogposts</td>\n        <td><ul>\n        <b>Flink 1.11 Series</b>\n        <li><a href=\"https://flink.apache.org/news/2020/07/14/application-mode.html\">Application Deployment in Flink: Current State and the new Application Mode</a></li>\n        <li><a href=\"https://flink.apache.org/2020/07/23/catalogs.html\">Sharing is caring - Catalogs in Flink SQL (Tutorial)</a></li>\n        <li><a href=\"https://flink.apache.org/2020/07/28/flink-sql-demo-building-e2e-streaming-application.html\">Flink SQL Demo: Building an End-to-End Streaming Application (Tutorial)</a></li>\n        <p></p>\n        <b>Other</b>\n        <li><a href=\"https://blogs.oracle.com/javamagazine/streaming-analytics-with-java-and-apache-flink?source=:em:nw:mt::RC_WWMK200429P00043:NSL400072808&elq_mid=167902&sh=162609181316181313222609291604350235&cmid=WWMK200429P00043C0004\">Streaming analytics with Java and Apache Flink (Tutorial)</a></li>\n        <li><a href=\"https://www.ververica.com/blog/flink-for-online-machine-learning-and-real-time-processing-at-weibo\">Flink for online Machine Learning and real-time processing at Weibo</a></li>\n        <li><a href=\"https://www.ververica.com/blog/data-driven-matchmaking-at-azar-with-apache-flink\">Data-driven Matchmaking at Azar with Apache Flink</a></li>\n      </ul>\n      </td>\n    </tr>\n      <td><span class=\"glyphicon glyphicon glyphicon-certificate\" aria-hidden=\"true\"></span> Flink Packages</td>\n      <td><ul><p><a href=\"https://flink-packages.org/\">Flink Packages</a> is a website where you can explore (and contribute to) the Flink <br /> ecosystem of connectors, extensions, APIs, tools and integrations. <b>New in:</b> </p>\n          <li><a href=\"https://flink-packages.org/packages/flink-metrics-signalfx\"> SignalFx Metrics Reporter</a></li>\n          <li><a href=\"https://flink-packages.org/packages/yauaa\">Yauaa: Yet Another UserAgent Analyzer</a></li>\n      </ul>\n    </td>\n    </tr>\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Displaying HTML Issue List Structure\nDESCRIPTION: HTML markup showing JIRA issue links and descriptions organized in an unordered list format with categories for bugs, improvements, and tasks.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-19-release-1.12.1.md#2025-04-08_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-20756'>FLINK-20756</a>] -         PythonCalcSplitConditionRule is not working as expected\n</li>\n```\n\n----------------------------------------\n\nTITLE: Exception Handling for OutOfMemoryError in Java\nDESCRIPTION: A reference to Java's OutOfMemoryError exception that occurs when the JVM cannot allocate more heap memory, which Flink's memory management system aims to prevent.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-05-11-Juggling-with-Bits-and-Bytes.md#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nOutOfMemoryError\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Template for Source Files\nDESCRIPTION: Standard Apache License 2.0 header that must be included at the beginning of each source file in the project.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-common.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter Configuration\nDESCRIPTION: YAML front matter configuration for the roadmap documentation page, specifying title, section collapse behavior, weight and URL aliases.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/roadmap.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Roadmap\nbookCollapseSection: false\nweight: 6\naliases:\n- /roadmap.html\n- /roadmap/index.html\n---\n```\n\n----------------------------------------\n\nTITLE: HTML List of JIRA Issues\nDESCRIPTION: HTML formatted list of JIRA tickets containing issue links and descriptions for various Apache Flink updates and fixes. Each list item includes a JIRA ticket number and description of the change.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-07-12-release-1.5.1.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-8650'>FLINK-8650</a>] -         Add tests and documentation for WINDOW clause\n</li>\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-8654'>FLINK-8654</a>] -         Extend quickstart docs on how to submit jobs\n</li>\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration\nDESCRIPTION: YAML configuration block defining metadata for the blog post including author information, publication date, and page aliases.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-04-17-sod.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nauthors:\n- konstantin: null\n  name: Konstantin Knauf\n  twitter: snntrable\ndate: \"2019-04-17T12:00:00Z\"\ntitle: Apache Flink's Application to Season of Docs\naliases:\n- /news/2019/04/17/sod.html\n```\n\n----------------------------------------\n\nTITLE: Executing Maven Verification for Flink Changes\nDESCRIPTION: Command to run Maven clean and verify to ensure all checks pass, code builds successfully, and tests pass before submitting a pull request.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/contribute-code.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean verify\n```\n\n----------------------------------------\n\nTITLE: Example of Good Comment Documentation\nDESCRIPTION: Demonstrates proper class documentation with meaningful information about purpose and implementation details.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-common.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n/**\n * An expression that wraps a single specific symbol.\n * A symbol could be a unit, an alias, a variable, etc.\n */\npublic class CommonSymbolExpression {}\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Frontmatter for Flink Kubernetes Operator Documentation Page\nDESCRIPTION: YAML frontmatter that defines metadata for the documentation page, including weight for navigation ordering, title, and link to the documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-kubernetes-operator-master.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 5\ntitle: Kubernetes Operator Main (snapshot)\nbookHref: \"https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main\"\n---\n```\n\n----------------------------------------\n\nTITLE: Byte Access Performance Results Table\nDESCRIPTION: HTML tables showing performance results for byte-level read/write operations across different memory segment implementations and sizes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-09-16-off-heap-memory.md#2025-04-08_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"table\">\n  <thead>\n    <tr>\n      <th>Segment</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><code>HeapMemorySegment</code>, exclusive</td>\n      <td>1,441 msecs</td>\n    </tr>\n    <!-- Additional rows omitted for brevity -->\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Removed Methods from CheckpointConfig Class\nDESCRIPTION: Methods that have been removed from the CheckpointConfig class. These methods were used for configuring checkpoint behavior and storage options.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_21\n\nLANGUAGE: java\nCODE:\n```\nvoid enableExternalizedCheckpoints(org.apache.flink.streaming.api.environment.CheckpointConfig$ExternalizedCheckpointCleanup)\njava.time.Duration getAlignmentTimeout()\norg.apache.flink.runtime.state.CheckpointStorage getCheckpointStorage()\norg.apache.flink.streaming.api.environment.CheckpointConfig$ExternalizedCheckpointCleanup getExternalizedCheckpointCleanup()\nboolean isFailOnCheckpointingErrors()\nboolean isForceCheckpointing()\nvoid setAlignmentTimeout(java.time.Duration)\nvoid setCheckpointStorage(org.apache.flink.runtime.state.CheckpointStorage)\nvoid setCheckpointStorage(java.lang.String)\nvoid setCheckpointStorage(java.net.URI)\nvoid setCheckpointStorage(org.apache.flink.core.fs.Path)\nvoid setExternalizedCheckpointCleanup(org.apache.flink.streaming.api.environment.CheckpointConfig$ExternalizedCheckpointCleanup)\nvoid setFailOnCheckpointingErrors(boolean)\nvoid setForceCheckpointing(boolean)\n```\n\n----------------------------------------\n\nTITLE: Building and Previewing Website\nDESCRIPTION: Command to build the website locally and start a preview server\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/improve-website.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build.sh\n```\n\n----------------------------------------\n\nTITLE: Example of Poor Comment Documentation\nDESCRIPTION: Demonstrates an anti-pattern for class documentation that provides redundant or meaningless information.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-common.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n/**\n * The symbol expression.\n */\npublic class CommonSymbolExpression {}\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Front Matter and Table Structure for Flink Users Page\nDESCRIPTION: YAML configuration that defines the page metadata including title, table of contents settings, weight, and aliases. It also structures a 'powered-by' table with multiple columns and rows to showcase organizations using Apache Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/what-is-flink/powered-by.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Flink ç”¨æˆ·\nbookToc: false\nweight: 5\naliases:\n- /zh/powered-by.html\n- /zh/powered-by/index.html\n\ntables:\n    powered-by:\n        name: \"Powered By\"\n        id: \"powered-by-flink\"\n\n        cols: \n          - id: \"column1\"\n          - id: \"column2\"\n          - id: \"column3\"\n          - id: \"column4\"\n\n        rows: \n          - column1: \n             val: \"<img src='/img/poweredby/alibaba-logo.png' alt='Alibaba'>\"\n             html: true\n            column2: \n             val: \"<img src='/img/poweredby/aws-logo.png' alt='AWS'>\"\n             html: true\n            column3: \n             val: \"<img src='/img/poweredby/bettercloud-logo.png' alt='BetterCloud'>\"\n             html: true\n            column4:\n             val: \"<img src='/img/poweredby/bouygues-logo.jpg' alt='Bouygues'>\"\n             html: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Front Matter for the About Section in YAML\nDESCRIPTION: YAML front matter that configures the 'About' section page in Hugo. It sets the page title, enables section collapsing, and defines weight parameters for ordering in the site structure.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/_index.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: About\nbookCollapseSection: true\nweight: 5\nmenu_weight: 1\n---\n```\n\n----------------------------------------\n\nTITLE: Simulating JobManager Failure\nDESCRIPTION: Kubernetes command to kill the JobManager pod, simulating a failure scenario to test the High Availability setup.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-02-10-native-k8s-with-ha.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl exec {jobmanager_pod_name} -- /bin/sh -c \"kill 1\"\n```\n\n----------------------------------------\n\nTITLE: Including HTML Table Shortcodes for Apache Flink Logo Display\nDESCRIPTION: HTML shortcode references used to render tables of Apache Flink logo variants. These Hugo shortcodes refer to the table definitions in the frontmatter to display PNG and SVG logo formats.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/material.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n{{< table \"png\" >}}\n```\n\nLANGUAGE: html\nCODE:\n```\n{{< table \"svg\" >}}\n```\n\n----------------------------------------\n\nTITLE: Adding External Documentation Links\nDESCRIPTION: Markdown syntax for adding external hyperlinks to official Flink documentation\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/improve-website.md#2025-04-08_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n{{</* docs_link file=\"relative_path/\" name=\"Title\"*/>}}\n```\n\nLANGUAGE: markdown\nCODE:\n```\n{{</* docs_link file=\"flink-docs-stable/docs/dev/datastream/side_output/\" name=\"Side Output\"*/>}}\n```\n\n----------------------------------------\n\nTITLE: Cancelling Flink Job on Kubernetes\nDESCRIPTION: Flink CLI command to cancel a running Flink job on the Kubernetes cluster. This command also cleans up all Kubernetes resources created by Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-02-10-native-k8s-with-ha.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ ./bin/flink cancel --target kubernetes-application -Dkubernetes.cluster-id=<ClusterID> <JobID>\n```\n\n----------------------------------------\n\nTITLE: Configuration Hierarchy Example in JSON Style\nDESCRIPTION: Example showing the recommended hierarchical structure for Flink configuration keys, demonstrating proper nesting and organization of configuration parameters.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-components.md#2025-04-08_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  taskmanager: {\n    jvm-exit-on-oom: true,\n    network: {\n      detailed-metrics: false,\n      request-backoff: {\n        initial: 100,\n        max: 10000\n      },\n      memory: {\n        fraction: 0.1,\n        min: 64MB,\n        max: 1GB,\n        buffers-per-channel: 2,\n        floating-buffers-per-gate: 16\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Flink ML Python Package\nDESCRIPTION: Command to install the Apache Flink ML Python package using pip. This allows users to use Flink ML functionality in Python environments.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-07-release-ml-2.0.0.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-flink-ml\n```\n\n----------------------------------------\n\nTITLE: Incorrect Variable Declaration Without Explicit Type - Scala\nDESCRIPTION: Example showing incorrect usage of type inference for class members in Scala, which should be avoided.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/how-to-contribute/code-style-and-quality-scala.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nvar expressions = new java.util.ArrayList[String]()\n```\n\n----------------------------------------\n\nTITLE: Sending Test Email via Shell Command\nDESCRIPTION: Shell command to send a test email to the GreenMail test server using mailx.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part2.md#2025-04-08_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\necho \"This is the email body\" | mailx -Sv15-compat \\\n        -s\"Email Subject\" \\\n        -Smta=\"smtp://alice:alice@localhost:3025\" \\\n        alice@acme.org\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Contribution Areas\nDESCRIPTION: HTML table layout defining different contribution areas and their corresponding information, including bug reporting, code contribution, code reviews, release preparation, documentation, user support, website improvement, and community engagement.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/overview.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"table table-bordered\">\n  <thead>\n    <tr>\n      <th>Area</th>\n      <th>Further information</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><span class=\"glyphicon glyphicon-exclamation-sign\" aria-hidden=\"true\"></span> Report a Bug</td>\n      <td>To report a problem with Flink, open <a href=\"http://issues.apache.org/jira/browse/FLINK\">Flink's Jira</a>, log in if necessary, and click on the red Create button at the top. <br/>\n      Please give detailed information about the problem you encountered and, if possible, add a description that helps to reproduce the problem.</td>\n    </tr>\n    <!-- Additional rows omitted for brevity -->\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Structuring Contribution Process with HTML\nDESCRIPTION: HTML structure for displaying the code contribution process steps in a grid layout. Uses Bootstrap classes and custom CSS classes to create a responsive design with numbered steps.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/contribute-code.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"contribute-grid\">\n  <div class=\"column\">\n    <div class=\"panel panel-default\">\n      <div class=\"panel-body\">\n        <h2><span class=\"number\">1</span><a href=\"#consensus\">Discuss</a></h2>\n        <p>Create a Jira ticket or mailing list discussion and reach consensus</p>\n        <p>Agree on importance, relevance, scope of the ticket, discuss the implementation approach and find a committer willing to review and merge the change.</p>\n        <p><b>Only committers can assign a Jira ticket.</b></p>\n      </div>\n    </div>\n  </div>\n  <div class=\"column\">\n    <div class=\"panel panel-default\">\n      <div class=\"panel-body\">\n        <h2><span class=\"number\">2</span><a href=\"#implement\">Implement</a></h2>\n        <p>Implement the change according to the <a href=\"{{< relref \"how-to-contribute/code-style-and-quality-preamble\" >}}\">Code Style and Quality Guide</a> and the approach agreed upon in the Jira ticket.</p> <br />\n        <p><b>Only start working on the implementation if there is consensus on the approach (e.g. you are assigned to the ticket)</b></p>\n      </div>\n    </div>\n  </div>\n  <div class=\"column\">\n    <div class=\"panel panel-default\">\n      <div class=\"panel-body\">\n        <h2><span class=\"number\">3</span><a href=\"#review\">Review</a></h2>\n        <p>Open a pull request and work with the reviewer.</p>\n        <p><b>Pull requests belonging to unassigned Jira tickets or not authored by assignee will not be reviewed or merged by the community.</b></p>\n      </div>\n    </div>\n  </div>\n  <div class=\"column\">\n    <div class=\"panel panel-default\">\n      <div class=\"panel-body\">\n        <h2><span class=\"number\">4</span><a href=\"#merge\">Merge</a></h2>\n        <p>A committer of Flink checks if the contribution fulfills the requirements and merges the code to the codebase.</p>\n      </div>\n    </div>\n  </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Removing Scala Dependencies from Flink Distribution\nDESCRIPTION: Command to remove Scala-related dependencies from Flink's lib directory to achieve a Scala-free classpath\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-02-22-scala-free.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ rm flink-dist/lib/flink-scala*\n```\n\n----------------------------------------\n\nTITLE: Example Hotfix PR Naming Format\nDESCRIPTION: Demonstrates the naming convention for hotfix pull requests that don't require JIRA tickets.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-pull-requests.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[hotfix][docs] Fix typo in event time introduction\\n[hotfix][javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator\n```\n\n----------------------------------------\n\nTITLE: Pulling Flink Docker Image with Java 17\nDESCRIPTION: Command to pull the official Flink 1.18.0 Docker image that runs on Java 17 from Docker Hub.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-10-24-release-1.18.0.md#2025-04-08_snippet_4\n\nLANGUAGE: docker\nCODE:\n```\ndocker pull flink:1.18.0-java17\n```\n\n----------------------------------------\n\nTITLE: Cloning Apache Flink Git Repository\nDESCRIPTION: This git command clones the Apache Flink repository from GitHub.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-18-a-year-in-review.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:apache/flink.git\n```\n\n----------------------------------------\n\nTITLE: Viewing the output results\nDESCRIPTION: Command to display the output of the PyFlink application, showing the interpolated temperature values where missing data has been filled in.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-04-pyflink-pandas-udf-support-flink.md#2025-04-08_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncat /tmp/output\n```\n\n----------------------------------------\n\nTITLE: Flink Documentation Heading Structure\nDESCRIPTION: Example of the heading structure used in Flink documentation Markdown files. It shows the recommended hierarchy of headings for proper content organization.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/README.md#2025-04-08_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n# Level-1 Heading  <- Used for the title of the page \n## Level-2 Heading <- Start with this one for content\n### Level-3 heading\n#### Level-4 heading\n##### Level-5 heading\n```\n\n----------------------------------------\n\nTITLE: Example Pull Request Naming Format\nDESCRIPTION: Shows the required format for naming pull requests, including JIRA ticket reference and component.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-pull-requests.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[FLINK-XXXX][component] Title of the pull request\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Language Code Tabs in Flink Documentation HTML\nDESCRIPTION: This HTML snippet demonstrates how to create tabbed code blocks for multiple programming languages (Java and Scala) in Flink documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_11\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"codetabs\" markdown=\"1\">\n\n\t  <div data-lang=\"java\" markdown=\"1\"> \n\n\t  ```java\n\t   // Java Code\n\t  ```\n\n\t  </div>\n\n\t  <div data-lang=\"scala\" markdown=\"1\">\n\n\t  ```scala\n\t   // Scala Code\n\t  ```\n\n\t  </div> \n\n  </div>\n```\n\n----------------------------------------\n\nTITLE: Running Local Flink Cluster\nDESCRIPTION: Command to start a local Flink cluster with one TaskManager and Web UI exposed on port 8081.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-05-16-official-docker-image.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -t -p 8081:8081 flink local\n```\n\n----------------------------------------\n\nTITLE: Generating Git Statistics\nDESCRIPTION: A bash command to generate Git statistics for the Flink repository using Gitstats.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-12-19-2016-year-in-review.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngitstats flink/ flink-stats/\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Kubernetes Operator 1.3.1 using Helm\nDESCRIPTION: Commands to add the Flink Kubernetes Operator 1.3.1 Helm chart to a local registry and install it with the webhook creation disabled. This snippet demonstrates how to quickly deploy the operator in a Kubernetes environment.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-01-10-release-kubernetes-operator-1.3.1.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.3.1 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.3.1/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.3.1/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Kubernetes Operator 1.3.0 Using Helm\nDESCRIPTION: Bash commands to add the Flink Kubernetes Operator 1.3.0 Helm chart repository and install the operator with webhooks disabled. These commands allow users to quickly deploy the operator in their Kubernetes environment.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-12-14-release-kubernetes-operator-1.3.0.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.3.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.3.0/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.3.0/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Disabling Table of Contents in Flink Documentation\nDESCRIPTION: Front matter example showing how to disable the automatically generated table of contents for a specific page in the Flink documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/README.md#2025-04-08_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n---\nbookToc: false\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Block Example\nDESCRIPTION: Example of a basic YAML front matter block with redirect layout configuration for a Flink documentation page.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Concepts\nlayout: redirect\n---\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Kubernetes Operator 1.7.0 with Helm\nDESCRIPTION: Bash commands to add the Helm chart repository for Flink Kubernetes Operator 1.7.0 and install it using Helm, with the webhook creation disabled.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-11-22-release-kubernetes-operator-1.7.0.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.7.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.7.0/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.7.0/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Linking to Other Flink Documentation Pages\nDESCRIPTION: This Liquid snippet shows how to create a link to another page within the Flink documentation using the 'link' tag.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_8\n\nLANGUAGE: liquid\nCODE:\n```\n[Link Text]({% link path/to/link-page.md %})\n```\n\n----------------------------------------\n\nTITLE: Opening Generated Git Statistics\nDESCRIPTION: A bash command to open the generated Git statistics HTML page in the default browser.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-12-19-2016-year-in-review.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nopen flink-stats/index.html\n```\n\n----------------------------------------\n\nTITLE: Adding Markdown Separator with Community Mailing List Reference\nDESCRIPTION: Simple markdown horizontal rule separator with a note about subscribing to the Flink community mailing list for staying updated with community activities, events, and announcements.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-29-community-update.md#2025-04-08_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<hr>\n\nIf you'd like to keep a closer eye on what's happening in the community, subscribe to the Flink [@community mailing list](https://flink.apache.org/community.html#mailing-lists) to get fine-grained weekly updates, upcoming event announcements and more.\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Kubernetes Operator 1.9.0 using Helm\nDESCRIPTION: Commands to add the Flink Kubernetes Operator 1.9.0 Helm chart to a local registry and install it with webhook creation disabled.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-07-02-release-kubernetes-operator-1.9.0.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.9.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.9.0/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.9.0/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Demonstrating HTML Alert in Markdown\nDESCRIPTION: This snippet shows how to create an HTML alert box within a Markdown document to highlight known issues in Flink 1.3.0. It includes links to JIRA tickets and uses HTML formatting for emphasis.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-06-01-release-1.3.0.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"alert alert-warning\">\n  There are two <strong>known issues</strong> in Flink 1.3.0. Both will be addressed in the <i>1.3.1</i> release.\n  <br>\n  <ul>\n  \t<li><a href=\"https://issues.apache.org/jira/browse/FLINK-6783\">FLINK-6783</a>: Wrongly extracted TypeInformations for <code>WindowedStream::aggregate</code></li>\n  \t<li><a href=\"https://issues.apache.org/jira/browse/FLINK-6775\">FLINK-6775</a>: StateDescriptor cannot be shared by multiple subtasks</li>\n  </ul> \n</div>\n```\n\n----------------------------------------\n\nTITLE: Adding Danger Alert in Flink Documentation HTML\nDESCRIPTION: This HTML snippet shows how to create a danger alert box in Flink documentation to signal important warnings or crucial information.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_6\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"alert alert-danger\"> // Danger Message </div>\n```\n\n----------------------------------------\n\nTITLE: Inserting Back to Top Link in Flink Documentation\nDESCRIPTION: This snippet shows how to add a 'Back to Top' link placeholder in Flink documentation pages. The placeholder is replaced with a default link when generating the documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_4\n\nLANGUAGE: liquid\nCODE:\n```\n{{ \"{% top \" }}%}\n```\n\n----------------------------------------\n\nTITLE: Generating Git Statistics\nDESCRIPTION: Command to generate Git statistics using gitstats tool for the Flink repository\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-12-21-2017-year-in-review.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngitstats flink/ flink-stats/\n```\n\n----------------------------------------\n\nTITLE: Counting Git Commits in Bash\nDESCRIPTION: This bash command counts the number of git commits in the Flink repository for the year 2015 using git log and wc.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-18-a-year-in-review.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit log --pretty=oneline --after=1/1/2015  | wc -l\n```\n\n----------------------------------------\n\nTITLE: Comparing Test Assertion Styles in Java\nDESCRIPTION: Demonstrates the preferred AssertJ testing style compared to traditional JUnit assertions. Shows how to write more readable and concise assertions using AssertJ's fluent API.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-common.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nassert list.size() == 10;\nfor (String item : list) {\n    assertTrue(item.length() < 10);\n}\n```\n\nLANGUAGE: java\nCODE:\n```\nassertThat(list)\n    .hasSize(10)\n    .allMatch(item -> item.length() < 10);\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.13.6\nDESCRIPTION: XML configuration for adding the core Flink dependencies to a Maven project. Includes flink-java, flink-streaming-java, and flink-clients artifacts with version 1.13.6.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-02-18-release-1.13.6.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.13.6</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.13.6</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.13.6</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Defining Dynamic Function Registration in YAML\nDESCRIPTION: Demonstrates how to configure dynamic function registration using YAML in StateFun 3.0.0. This allows new functions to be added without restarting the StateFun cluster.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-04-15-release-statefun-3.0.0.md#2025-04-08_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n    - endpoint:\n        meta:\n            kind: http\n            spec:\n        functions: example/*\n        urlPathTemplate: https://loadbalancer.svc.cluster.local/{function.name}\n```\n\n----------------------------------------\n\nTITLE: Example of Good Early Return Pattern\nDESCRIPTION: Demonstrates the preferred approach of using early returns to reduce nesting and improve code readability.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-common.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nif (!a) {\n\treturn ..\n}\n\nif (!b) {\n\treturn ...\n}\n\nif (!c) {\n\treturn ...\n}\n\nthe main path\n```\n\n----------------------------------------\n\nTITLE: Cloning the Flink Repository for Documentation Contribution\nDESCRIPTION: Command to clone a forked Flink repository to the local machine. This is the first step after forking the repository on GitHub to obtain the documentation source files located in the docs/ subdirectory.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/contribute-documentation.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<your-user-name>/flink.git\n```\n\n----------------------------------------\n\nTITLE: Installing Gitstats with Homebrew\nDESCRIPTION: This bash command installs the gitstats tool using Homebrew package manager on macOS.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-18-a-year-in-review.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbrew install --HEAD homebrew/head-only/gitstats\n```\n\n----------------------------------------\n\nTITLE: Generating Gitstats for Flink\nDESCRIPTION: This bash command generates statistics for the Flink repository using gitstats tool.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-18-a-year-in-review.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngitstats flink/ flink-stats/\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to GitHub\nDESCRIPTION: Git command to push local changes to a dedicated branch on GitHub fork\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/improve-website.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit push origin myBranch\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.5.1\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies should be added to the project's pom.xml file to use Flink 1.5.1.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-07-12-release-1.5.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.5.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.5.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.5.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Counting Git Commits in Bash\nDESCRIPTION: A bash command to count the number of Git commits in the Flink repository after December 31, 2015.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-12-19-2016-year-in-review.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit log --pretty=oneline --after=12/31/2015 | wc -l\n```\n\n----------------------------------------\n\nTITLE: Building Flink Website with Hugo Docker\nDESCRIPTION: Command to build and serve the Flink website documentation locally using a Hugo Docker image. This method doesn't require a local Hugo installation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/README.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ ./docker-build.sh\n```\n\n----------------------------------------\n\nTITLE: Building and Previewing Flink Documentation\nDESCRIPTION: Command to build the documentation in preview mode. This script compiles Markdown files into static HTML pages and starts a local webserver at http://localhost:1313/ to preview changes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/contribute-documentation.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build_docs.sh -p\n```\n\n----------------------------------------\n\nTITLE: Installing KUDO CLI with Homebrew\nDESCRIPTION: Commands to install the KUDO CLI using Homebrew package manager on macOS. KUDO is a Kubernetes toolkit for building Operators.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ brew tap kudobuilder/tap\n$ brew install kudo-cli\n```\n\n----------------------------------------\n\nTITLE: Installing Gitstats with Homebrew\nDESCRIPTION: A bash command to install the Gitstats tool using Homebrew on macOS.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-12-19-2016-year-in-review.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbrew install --HEAD homebrew/head-only/gitstats\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.6.1\nDESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink 1.6.1 version. It includes dependencies for flink-java, flink-streaming-java, and flink-clients modules.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-09-20-release-1.6.1.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.6.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.6.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.6.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Cloning Apache Flink Git Repository\nDESCRIPTION: A Git command to clone the Apache Flink repository from GitHub.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-12-19-2016-year-in-review.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:apache/flink.git\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: Standard Apache License 2.0 header comment that specifies the terms under which the file is licensed and distributed.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/_index.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Installing ZooKeeper Operator with KUDO\nDESCRIPTION: Command to install the ZooKeeper Operator using KUDO. The --skip-instance flag prevents instance creation since it will be created as a dependency by the Flink demo operator.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl kudo install zookeeper --version=0.3.0 --skip-instance\n```\n\n----------------------------------------\n\nTITLE: External Link Shortcode in Hugo Markdown\nDESCRIPTION: Hugo shortcode that creates an external link to the Flink Stateful Functions documentation for the latest snapshot.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-stateful-functions-master.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{< external_link name=\"You can find the Flink Stateful Functions documentation for the latest snapshot here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.5.5\nDESCRIPTION: XML configuration snippet showing how to update Maven dependencies for Apache Flink 1.5.5. This includes core Flink modules: flink-java, flink-streaming-java, and flink-clients, all updated to version 1.5.5.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-10-29-release-1.5.5.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.5.5</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.5.5</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.5.5</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Displaying Warning Label in Markdown\nDESCRIPTION: Markdown snippet for displaying a warning label with an icon and custom styling.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-07-23-flink-network-stack-2.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"alert alert-warning\" markdown=\"1\">\n<span class=\"label label-warning\" style=\"display: inline-block\"><span class=\"glyphicon glyphicon-warning-sign\" aria-hidden=\"true\"></span> Warning</span>\n// Warning content here\n</div>\n```\n\n----------------------------------------\n\nTITLE: Hugo External Link Shortcode Usage\nDESCRIPTION: Hugo shortcode that creates an external link to the Flink documentation. The shortcode includes descriptive text about finding the latest stable release documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-stable.md#2025-04-08_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n{{< external_link name=\"You can find the Flink documentation for the latest stable release here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.16.3\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients artifacts. These dependencies are required for building Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-11-29-release-1.16.3.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.16.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.16.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.16.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Page Metadata Configuration in YAML\nDESCRIPTION: YAML front matter that defines metadata for the page, including weight for sorting, title, and the URL to the latest Flink Stateful Functions documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-stateful-functions-master.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 11\ntitle: Stateful Functions Master (snapshot)\nbookHref: \"https://nightlies.apache.org/flink/flink-statefun-docs-master\"\n---\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.7.1\nDESCRIPTION: Maven dependency configuration for upgrading to Apache Flink 1.7.1. Includes core dependencies for Java API, Streaming Java API, and Clients with Scala 2.11 compatibility.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-12-21-release-1.7.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.7.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.7.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.7.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: HTML Issue List Elements\nDESCRIPTION: HTML markup for displaying Flink JIRA issues including documentation fixes and technical debt items\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-25-release-1.17.1.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-32112'>FLINK-32112</a>] -         Fix the deprecated state backend sample config in Chinese document\n</li>\n</ul>\n\n<h2>        Technical Debt\n</h2>\n<ul>\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-31704'>FLINK-31704</a>] -         Pulsar docs should be pulled from dedicated branch\n</li>\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-31705'>FLINK-31705</a>] -         Remove Conjars\n</li>\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Defining External Link in Markdown for Flink Documentation\nDESCRIPTION: Creates a markdown shortcode to generate an external link to the latest Flink documentation snapshot.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-master.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{< external_link name=\"You can find the Flink documentation for the latest snapshot here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.19.2\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies are essential for building Flink applications using version 1.19.2.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2025-02-12-release-1.19.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.19.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.19.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.19.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Flink Documentation Front Matter Configuration\nDESCRIPTION: YAML front matter configuration for a documentation page, specifying weight, title and external documentation link.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/getting-started/with-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 1\ntitle: With Flink\nbookHref: \"https://nightlies.apache.org/flink/flink-docs-stable/docs/try-flink/local_installation/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.8.1\nDESCRIPTION: This XML snippet shows how to update Maven dependencies to use Apache Flink 1.8.1. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-07-02-release-1.8.1.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.8.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.8.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.8.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter Definition\nDESCRIPTION: YAML front matter block defining metadata for the blog post including author information, date, excerpt and title.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-05-04-season-of-docs.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nauthors:\n- morsapaes: null\n  name: Marta Paes\n  twitter: morsapaes\ndate: \"2020-05-04T06:00:00Z\"\nexcerpt: The Flink community is thrilled to share that the project is applying again to Google Season of Docs (GSoD) this year! If you're unfamiliar with the program, GSoD is a great initiative organized by Google Open Source to pair technical writers with mentors to work on documentation for open source projects. Does working shoulder to shoulder with the Flink community on documentation sound exciting? We'd love to hear from you!\ntitle: Applying to Google Season of Docs 2020\naliases:\n- /news/2020/05/04/season-of-docs.html\n---\n```\n\n----------------------------------------\n\nTITLE: External Link Shortcode for Flink Documentation\nDESCRIPTION: A Hugo shortcode that creates an external link to the Flink LTS documentation with descriptive text about it being the latest Long-Term Support release.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-lts.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{< external_link name=\"You can find the Flink documentation for the latest Long-Term Support(LTS) release here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.15.1\nDESCRIPTION: This XML snippet shows how to configure the essential Maven dependencies for Apache Flink 1.15.1, including flink-java, flink-streaming-java, and flink-clients artifacts from the org.apache.flink group.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-07-06-release-1.15.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.15.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.15.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.15.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.10.1\nDESCRIPTION: Maven dependency configuration for Apache Flink 1.10.1 core libraries including the Java API, streaming Java API, and clients. These dependencies should be added to your project's pom.xml file.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-05-12-release-1.10.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.10.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.10.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.10.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Viewing Gitstats Results in Chrome\nDESCRIPTION: This bash command opens the generated gitstats HTML report in Google Chrome browser.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-18-a-year-in-review.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nchrome flink-stats/index.html\n```\n\n----------------------------------------\n\nTITLE: Including Recent Posts Shortcode\nDESCRIPTION: Hugo shortcode to include a list of recent blog posts or documentation updates on the homepage.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/_index.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{< recent_posts >}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.4.2\nDESCRIPTION: XML configuration for adding Apache Flink 1.4.2 dependencies to a Maven project. Includes the core Java API, streaming Java API, and clients module dependencies.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-03-08-release-1.4.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.4.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.4.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.4.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.10.2\nDESCRIPTION: XML configuration for adding Flink 1.10.2 dependencies to a Maven project. Includes the core Java API, streaming Java API, and clients module with Scala 2.11 bindings.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-25-release-1.10.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.10.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.10.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.10.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Implementation\nDESCRIPTION: Standard Apache License 2.0 header comment block for source code files\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/powered-by.md#2025-04-08_snippet_2\n\nLANGUAGE: Markdown\nCODE:\n```\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n```\n\n----------------------------------------\n\nTITLE: Including Recent Posts with Hugo Shortcode\nDESCRIPTION: Hugo shortcode that dynamically includes a list of recent posts on the documentation homepage. This code is used to display the most recent blog posts or documentation updates.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/_index.md#2025-04-08_snippet_1\n\nLANGUAGE: hugo\nCODE:\n```\n{{< recent_posts >}}\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.3.2\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java and flink-clients, all updated to version 1.3.2.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-08-05-release-1.3.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.3.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.10</artifactId>\n  <version>1.3.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.10</artifactId>\n  <version>1.3.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.10.3\nDESCRIPTION: XML configuration for Maven dependencies to include the core Flink libraries (flink-java, flink-streaming-java, and flink-clients) at version 1.10.3. These dependencies are essential for building Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-29-release-1.10.3.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.10.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.10.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.10.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Hugo Front Matter Configuration in YAML\nDESCRIPTION: YAML front matter block defining Hugo page configuration parameters including title, section collapsing, weight and menu weight for the About section.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/what-is-flink/_index.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: About\nbookCollapseSection: true\nweight: 5\nmenu_weight: 1\n---\n```\n\n----------------------------------------\n\nTITLE: Defining HTML Link for CDC Documentation\nDESCRIPTION: HTML shortcode that creates an external link to the CDC documentation\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-cdc-master.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n{{< external_link name=\"You can find the Flink CDC documentation for the latest snapshot here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.12.5\nDESCRIPTION: Maven dependency declarations required to upgrade to Apache Flink 1.12.5. Includes core dependencies: flink-java, flink-streaming-java for Scala 2.11, and flink-clients for Scala 2.11.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-08-06-release-1.12.5.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.12.5</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.12.5</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.12.5</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.16.1\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies are required for building Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-01-26-release-1.16.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.16.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.16.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.16.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter Configuration for Flink ML Documentation Page\nDESCRIPTION: YAML configuration block that defines page metadata including weight for ordering (9), title ('ML Master (snapshot)'), and the external documentation URL for the Flink ML master documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flinkml-master.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 9\ntitle: ML Master (snapshot)\nbookHref: \"https://nightlies.apache.org/flink/flink-ml-docs-master\"\n---\n```\n\n----------------------------------------\n\nTITLE: External Link in Hugo Template\nDESCRIPTION: A Hugo shortcode that creates an external link to the Flink Stateful Functions getting started documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/getting-started/with-flink-stateful-functions.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{{< external_link name=\"Read how you can get started with Flink Stateful Functions here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.5.3\nDESCRIPTION: XML configuration for updating Apache Flink Maven dependencies to version 1.5.3. Includes the core modules: flink-java, flink-streaming-java, and flink-clients that are required for most Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-08-21-release-1.5.3.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.5.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.5.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.5.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Defining Maven Dependencies for Apache Flink 1.15.4\nDESCRIPTION: XML snippet showing how to include the core Apache Flink 1.15.4 dependencies in a Maven project. The snippet includes the basic dependencies: flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-03-15-release-1.15.4.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.15.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.15.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.15.4</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Apache License Comment Block\nDESCRIPTION: HTML comment containing the Apache License 2.0 notice for the documentation file.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-master.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Markdown Header and License Comment\nDESCRIPTION: YAML frontmatter defining page metadata and HTML comment containing Apache 2.0 license information.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/what-is-the-flink-kubernetes-operator.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: What is the Flink Kubernetes Operator?\nbookCollapseSection: false\nbookHref: \"https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-stable/\"\n---\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Defining Maven Dependencies for Apache Flink 1.1.2\nDESCRIPTION: XML snippet showing how to define Maven dependencies for Apache Flink 1.1.2. It includes dependencies for flink-java, flink-streaming-java, and flink-clients modules.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2016-09-05-release-1.1.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.1.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.10</artifactId>\n  <version>1.1.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.10</artifactId>\n  <version>1.1.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.17.1\nDESCRIPTION: XML configuration for adding essential Apache Flink 1.17.1 dependencies to a Maven project. Includes the core Java API, streaming Java API, and clients module that are needed for building Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-05-25-release-1.17.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.17.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.17.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.17.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Documentation Section Parameters\nDESCRIPTION: YAML front matter configuration for a Hugo documentation page that sets up collapsible sections, weight, and menu positioning.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/_index.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Documentation\nbookCollapseSection: true\nweight: 15\nmenu_weight: 1\n```\n\n----------------------------------------\n\nTITLE: Sessionizing Clickstream with Flink SQL\nDESCRIPTION: SQL query that groups clickstream data into sessions with a 30-minute interval and counts clicks per user per session. This demonstrates Flink's capability for analyzing time-windowed event data.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/flink-applications.md#2025-04-08_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT userId, COUNT(*)\nFROM clicks\nGROUP BY SESSION(clicktime, INTERVAL '30' MINUTE), userId\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.2.1\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients at version 1.2.1. These dependencies are required for building Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-04-26-release-1.2.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.2.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.10</artifactId>\n  <version>1.2.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.10</artifactId>\n  <version>1.2.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Flink LTS Documentation Front Matter in YAML\nDESCRIPTION: YAML front matter configuration that defines metadata for the Flink LTS documentation page, including weight for ordering, title with version variable, and documentation URL.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-lts.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 2\ntitle: Flink $FlinkLTSShortVersion (LTS)\nbookHref: \"https://nightlies.apache.org/flink/flink-docs-lts/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.17.2\nDESCRIPTION: XML configuration for including the core Apache Flink 1.17.2 dependencies (flink-java, flink-streaming-java, and flink-clients) in a Maven project. These dependencies provide the essential libraries needed for developing Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-11-29-release-1.17.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.17.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.17.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.17.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Frontmatter for Flink Documentation Page\nDESCRIPTION: YAML frontmatter configuration for a Hugo-based documentation site. It defines weight, title, and bookHref properties for the Flink documentation page, with a placeholder variable for the Flink version.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-stable.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 1\ntitle: Flink $FlinkStableShortVersion (stable)\nbookHref: \"https://nightlies.apache.org/flink/flink-docs-stable/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Front Matter for CDC Documentation\nDESCRIPTION: YAML front matter configuration that sets the page weight, title and documentation book reference URL\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-cdc-master.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 7\ntitle: CDC Master (snapshot)\nbookHref: \"https://nightlies.apache.org/flink/flink-cdc-docs-master\"\n---\n```\n\n----------------------------------------\n\nTITLE: Markdown Image Embedding - Bounded vs Unbounded Streams\nDESCRIPTION: Markdown syntax for embedding an image showing the difference between bounded and unbounded data streams in Flink\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/flink-architecture.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{{< img src=\"/img/bounded-unbounded.png\" width=\"600px\" >}}\n```\n\n----------------------------------------\n\nTITLE: External Link Hugo Shortcode\nDESCRIPTION: Hugo shortcode for creating an external link to the Flink getting started documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/getting-started/with-flink.md#2025-04-08_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n{{< external_link name=\"Read how you can get started with Flink here.\">}}\n```\n\n----------------------------------------\n\nTITLE: External Documentation Link Template in HTML\nDESCRIPTION: Hugo template shortcode that creates an external link to the Flink LTS documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-lts.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n{{< external_link name=\"You can find the Flink documentation for the latest Long-Term Support(LTS) release here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Displaying Kinesis Consumer Lag Metric in Markdown Table\nDESCRIPTION: Shows the 'millisBehindLatest' metric for FlinkKinesisConsumer in a markdown table, indicating how far behind the consumer is from the current time.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-02-21-monitoring-best-practices.md#2025-04-08_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric | Scope | Description |\n| ------ | ----- | ----------- |\n| `millisBehindLatest` | user | applies to `FlinkKinesisConsumer`. The number of milliseconds a consumer is behind the head of the stream. For any consumer and Kinesis shard, this indicates how far it is behind the current time. |\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Front Matter for Apache Flink Documentation\nDESCRIPTION: YAML front matter that defines the page configuration for Hugo static site generator. It specifies the title, type, and home status for the Apache Flink documentation landing page.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/_index.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Apache Flink Documentation \ntype: docs\nhome: true\n---\n```\n\n----------------------------------------\n\nTITLE: HTML Security Updates Table\nDESCRIPTION: HTML table displaying historical security vulnerabilities (CVEs) in Apache Flink, including affected versions and mitigation notes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/security.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"table\">\n\t<thead>\n\t\t<tr>\n\t\t\t<th style=\"width: 20%\">CVE ID</th>\n\t\t\t<th style=\"width: 30%\">Affected Flink versions</th>\n\t\t\t<th style=\"width: 50%\">Notes</th>\n\t\t</tr>\n\t</thead>\n\t<tr>\n\t\t<td>\n\t\t\t<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-1960\">CVE-2020-1960</a>\n\t\t</td>\n\t\t<td>\n\t\t\t1.1.0 to 1.1.5, 1.2.0 to 1.2.1, 1.3.0 to 1.3.3, 1.4.0 to 1.4.2, 1.5.0 to 1.5.6, 1.6.0 to 1.6.4, 1.7.0 to 1.7.2, 1.8.0 to 1.8.3, 1.9.0 to 1.9.2, 1.10.0\n\t\t</td>\n\t\t<td>\n\t\t\tUsers are advised to upgrade to Flink 1.9.3 or 1.10.1 or later versions or remove the port parameter from the reporter configuration (see advisory for details).\n\t\t</td>\n\t</tr>\n\t<tr>\n\t\t<td>\n\t\t\t<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17518\">CVE-2020-17518</a>\n\t\t</td>\n\t\t<td>\n\t\t\t1.5.1 to 1.11.2\n\t\t</td>\n\t\t<td>\n\t\t\t<a href=\"https://github.com/apache/flink/commit/a5264a6f41524afe8ceadf1d8ddc8c80f323ebc4\">Fixed in commit a5264a6f41524afe8ceadf1d8ddc8c80f323ebc4</a> <br>\n\t\t\tUsers are advised to upgrade to Flink 1.11.3 or 1.12.0 or later versions.\n\t\t</td>\n\t</tr>\n\t<tr>\n\t\t<td>\n\t\t\t<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17519\">CVE-2020-17519</a>\n\t\t</td>\n\t\t<td>\n\t\t\t1.11.0, 1.11.1, 1.11.2\n\t\t</td>\n\t\t<td>\n\t\t\t<a href=\"https://github.com/apache/flink/commit/b561010b0ee741543c3953306037f00d7a9f0801\">Fixed in commit b561010b0ee741543c3953306037f00d7a9f0801</a> <br>\n\t\t\tUsers are advised to upgrade to Flink 1.11.3 or 1.12.0 or later versions.\n\t\t</td>\n\t</tr>\n\t<tr>\n\t\t<td>\n\t\t\t<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-41834\">CVE-2023-41834</a>\n\t\t</td>\n\t\t<td>\n\t\t\tFlink Stateful Functions 3.1.0, 3.1.1, 3.2.0\n\t\t</td>\n\t\t<td>\n\t\t\t<a href=\"https://github.com/apache/flink-statefun/commit/b06c0a23a5a622d48efc8395699b2e4502bd92be\">Fixed in commit b06c0a23a5a622d48efc8395699b2e4502bd92be</a> <br>\n\t\t\tUsers are advised to upgrade to Flink Stateful Functions 3.3.0 or later versions.\n\t\t</td>\n\t</tr>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Hugo Front Matter Configuration for Documentation Page\nDESCRIPTION: YAML front matter configuration for Hugo static site generator that sets up the documentation section properties including title, collapse behavior, and menu weights.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/_index.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Documentation\nbookCollapseSection: true\nweight: 15\nmenu_weight: 1\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Frontmatter for Flink Master Documentation Page\nDESCRIPTION: Sets up the YAML frontmatter for the Flink master documentation page, including weight, title, and external book reference.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-master.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 3\ntitle: Flink Master (snapshot)\nbookHref: \"https://nightlies.apache.org/flink/flink-docs-master/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.1.5\nDESCRIPTION: Maven dependency configuration for Apache Flink 1.1.5 core modules. Includes the main Java API (flink-java), streaming Java API (flink-streaming-java), and the client module for submitting jobs to Flink clusters.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-23-release-1.1.5.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.1.5</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.10</artifactId>\n  <version>1.1.5</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.10</artifactId>\n  <version>1.1.5</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Flink LTS Documentation Page\nDESCRIPTION: YAML front matter that defines metadata for the Flink LTS documentation page, including its weight in the navigation hierarchy, title with a placeholder variable, and the URL to the actual documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/documentation/flink-lts.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 2\ntitle: Flink $FlinkLTSShortVersion (LTS)\nbookHref: \"https://nightlies.apache.org/flink/flink-docs-lts/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Rendering HTML List of Flink Improvement Issues\nDESCRIPTION: This HTML snippet renders an unordered list of Flink improvement issues. Each list item contains a link to the corresponding JIRA issue and a brief description of the improvement.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-08-05-release-1.3.2.md#2025-04-08_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<h2>        Improvement\n</h2>\n<ul>\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-6365'>FLINK-6365</a>] -         Adapt default values of the Kinesis connector\n</li>\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-6575'>FLINK-6575</a>] -         Disable all tests on Windows that use HDFS\n</li>\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-6682'>FLINK-6682</a>] -         Improve error message in case parallelism exceeds maxParallelism\n</li>\n<!-- ... Additional list items omitted for brevity ... -->\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-7290'>FLINK-7290</a>] -         Make release scripts modular\n</li>\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Block\nDESCRIPTION: Standard Apache License header that must be included in all Flink documentation files immediately after the front matter.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n  http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment in HTML\nDESCRIPTION: HTML comment containing the Apache License 2.0 header that specifies the licensing terms for the file. This is a standard license notice included in Apache Software Foundation projects.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-stable.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.8.3\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java and flink-clients with version 1.8.3.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-12-11-release-1.8.3.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.8.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.8.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.8.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: HTML comment containing the Apache License 2.0 header information for the documentation file.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/getting-started/with-flink.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Cloning Apache Flink Repository\nDESCRIPTION: Command to clone the Apache Flink git repository from GitHub\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-12-21-2017-year-in-review.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:apache/flink.git\n```\n\n----------------------------------------\n\nTITLE: HTML Image Implementation for Company Logos\nDESCRIPTION: HTML code snippets for displaying company logos with alt text in a responsive layout.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/powered-by.md#2025-04-08_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<img src='/img/poweredby/razorpay-logo.png' alt='Razorpay'>\n```\n\nLANGUAGE: HTML\nCODE:\n```\n<img src='/img/poweredby/researchgate-logo.png' alt='ResearchGate'>\n```\n\nLANGUAGE: HTML\nCODE:\n```\n<img src='/img/poweredby/sktelecom-logo.png' alt='SK Telecom'>\n```\n\nLANGUAGE: HTML\nCODE:\n```\n<img src='/img/poweredby/zalando-logo.jpg' alt='Zalando'>\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter Configuration for Flink ML Documentation\nDESCRIPTION: YAML configuration block that sets the weight, title, and bookHref for the Flink ML master snapshot documentation page. This determines how the documentation appears in navigation and where it links to.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flinkml-master.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 9\ntitle: ML Master (snapshot)\nbookHref: \"https://nightlies.apache.org/flink/flink-ml-docs-master\"\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.13.1\nDESCRIPTION: Maven dependency configuration block showing the core Flink artifacts required for Java and streaming applications. Includes flink-java, flink-streaming-java and flink-clients dependencies.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-28-release-1.13.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.13.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.13.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.13.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Rendering HTML Issue List for Flink JIRA Tickets\nDESCRIPTION: HTML markup for displaying a list of JIRA issue tickets with links and descriptions. Each list item contains a JIRA ticket link and summary of the reported issue.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-12-18-release-1.11.3.md#2025-04-08_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-19398'>FLINK-19398</a>] -         Hive connector fails with IllegalAccessError if submitted as usercode</li>\n```\n\n----------------------------------------\n\nTITLE: Opening Git Statistics Report\nDESCRIPTION: Command to open the generated Git statistics HTML report in the default browser\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-12-21-2017-year-in-review.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nopen flink-stats/index.html\n```\n\n----------------------------------------\n\nTITLE: Styling Tables with CSS in HTML\nDESCRIPTION: CSS styling to improve the appearance of tables in the blog post. It sets fixed width, borders, padding, and text alignment for table elements.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-02-21-monitoring-best-practices.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<style>\n  table { border: 0px solid black; table-layout: auto; width: 800px; }\n  th, td { border: 1px solid black; padding: 5px; padding-left: 10px; padding-right: 10px; }\n  th { text-align: center }\n  td { vertical-align: top }\n</style>\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata for Flink ML Documentation Page in Markdown\nDESCRIPTION: This snippet defines metadata for the Flink ML documentation page, including weight, title, and a link to the stable documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flinkml-stable.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nweight: 8\ntitle: ML $FlinkMLStableShortVersion (stable)\nbookHref: \"https://nightlies.apache.org/flink/flink-ml-docs-stable/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.11.1\nDESCRIPTION: This XML snippet shows how to update Maven dependencies to use Apache Flink version 1.11.1. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-21-release-1.11.1.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.11.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.11.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.11.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: HTML Alert Box Definition\nDESCRIPTION: HTML div element defining an alert box with subscription instructions for the Apache Flink mailing list.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-05-04-season-of-docs.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"alert alert-info\">\n\tPlease <a href=\"mailto:dev-subscribe@flink.apache.org\">subscribe</a> to the Apache Flink mailing list before reaching out.\n\tIf you are not subscribed then responses to your message will not go through.\n\tYou can always <a href=\"mailto:dev-unsubscribe@flink.apache.org\">unsubscribe</a> at any time. \n</div>\n```\n\n----------------------------------------\n\nTITLE: Inserting External Link to Flink ML Documentation in Markdown\nDESCRIPTION: This snippet uses a custom shortcode to insert an external link to the Flink ML documentation for the latest stable release.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flinkml-stable.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{< external_link name=\"You can find the Flink ML documentation for the latest stable release here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.11.2\nDESCRIPTION: This XML snippet shows how to update Maven dependencies to use Apache Flink version 1.11.2. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-09-17-release-1.11.2.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.11.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.11.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.11.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: External Link Component in HTML/Markdown\nDESCRIPTION: A shortcode component that renders an external link to the Flink ML documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/getting-started/with-flink-ml.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{{< external_link name=\"Read how you can get started with Flink ML here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.11.3\nDESCRIPTION: Maven dependency configuration for the core Flink modules in version 1.11.3. These dependencies are essential for Java and Scala-based Flink applications, including flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-12-18-release-1.11.3.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.11.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.11.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.11.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Defining Flink Documentation Homepage in Markdown\nDESCRIPTION: Front matter metadata defining the documentation homepage properties including title and type. This establishes the page as the main documentation landing page for Apache Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/_index.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Apache FlinkÂ® â€” Stateful Computations over Data Streams\ntype: docs\nhome: true\n---\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.12.2\nDESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.12.2. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-03-03-release-1.12.2.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.12.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.12.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.12.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.3.3\nDESCRIPTION: XML configuration for updating Maven dependencies to Apache Flink 1.3.3. This snippet shows how to include the core Flink Java dependencies (flink-java, flink-streaming-java, and flink-clients) with the correct version number in a Maven project.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-03-15-release-1.3.3.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.3.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.10</artifactId>\n  <version>1.3.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.10</artifactId>\n  <version>1.3.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.20.1\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies are required for building Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2025-02-12-release-1.20.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.20.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.20.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.20.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.13.2\nDESCRIPTION: XML snippet showing how to configure Maven dependencies for Apache Flink 1.13.2. Includes the core modules: flink-java for basic functionality, flink-streaming-java for stream processing, and flink-clients for client connectivity.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-08-06-release-1.13.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.13.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.13.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.13.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Streaming Join Implementation in Java\nDESCRIPTION: Implementation of a streaming join between tweet counts and price warnings over a 30-second window, computing rolling correlation between the streams.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nDataStream<Tuple2<Integer, Integer>> tweetsAndWarning = warningsPerStock\n    .join(tweetsPerStock)\n    .onWindow(30, TimeUnit.SECONDS)\n    .where(\"symbol\")\n    .equalTo(\"symbol\")\n    .with(new JoinFunction<Count, Count, Tuple2<Integer, Integer>>() {\n        @Override\n        public Tuple2<Integer, Integer> join(Count first, Count second) throws Exception {\n            return new Tuple2<Integer, Integer>(first.count, second.count);\n            }\n    });\n\nDataStream<Double> rollingCorrelation = tweetsAndWarning\n    .window(Time.of(30, TimeUnit.SECONDS))\n    .mapWindow(new WindowCorrelation());\n\nrollingCorrelation.print();\n```\n\n----------------------------------------\n\nTITLE: Styling Contribution Process Grid with CSS\nDESCRIPTION: CSS styles for creating a responsive grid layout to display the code contribution process steps. Includes media queries for different screen sizes and styling for panels and headings.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/contribute-code.md#2025-04-08_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n<style>\n.contribute-grid {\n  margin-bottom: 10px;\n  display: flex;\n  flex-direction: column;\n  margin-left: -2px;\n  margin-right: -2px;\n}\n\n.contribute-grid .column {\n  margin-top: 4px;\n  padding: 0 2px;\n}\n\n@media only screen and (min-width: 480px) {\n  .contribute-grid {\n    flex-direction: row;\n    flex-wrap: wrap;\n  }\n\n  .contribute-grid .column {\n    flex: 0 0 50%;\n  }\n\n  .contribute-grid .column {\n    margin-top: 4px;\n  }\n}\n\n@media only screen and (min-width: 960px) {\n  .contribute-grid {\n    flex-wrap: nowrap;\n  }\n\n  .contribute-grid .column {\n    flex: 0 0 25%;\n  }\n\n}\n\n.contribute-grid .panel {\n  height: 100%;\n  margin: 0;\n}\n\n.contribute-grid .panel-body {\n  padding: 10px;\n}\n\n.contribute-grid h2 {\n  margin: 0 0 10px 0;\n  padding: 0;\n  display: flex;\n  align-items: flex-start;\n  border: none;\n}\n\n.contribute-grid .number {\n  margin-right: 0.25em;\n  font-size: 1.5em;\n  line-height: 0.9;\n}\n</style>\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Frontmatter for Apache Flink Material Page\nDESCRIPTION: YAML frontmatter configuration for a Hugo page displaying Apache Flink logo materials. It defines page properties and table structures for organizing PNG and SVG logo variants with different styling options.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/material.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Material\nbold: true\nbookCollapseSection: false\nbookHidden: true\n\ntables:\n    png:\n        name: \"png\"\n\n        cols: \n          - id: \"Colored\"\n            name: \"Colored logo\"\n          - id: \"WhiteFilled\"\n            name: \"White filled logo\"\n          - id: \"BlackOutline\"\n            name: \"Black outline logo\"\n\n        rows:\n          - Colored: \n             val: \"<img src='/img/logo/png/200/flink_squirrel_200_color.png' alt='Apache Flink Logo' title='Apache Flink Logo' width='200px' />\"\n             html: true\n            WhiteFilled: \n             val: \"<img src='/img/logo/png/200/flink_squirrel_200_white.png' alt='Apache Flink Logo' title='Apache Flink Logo' width='200px' style='background:black;' />\"\n             html: true\n            BlackOutline: \n             val: \"<img src='/img/logo/png/200/flink_squirrel_200_black.png' alt='Apache Flink Logo' title='Apache Flink Logo' width='200px'/>\"\n             html: true\n          - Colored: \"**Sizes (px)** [50x50](/img/logo/png/50/color_50.png), [100x100](/img/logo/png/100/flink_squirrel_100_color.png), [200x200](/img/logo/png/200/flink_squirrel_200_color.png), [500x500](/img/logo/png/500/flink_squirrel_500.png), [1000x1000](/img/logo/png/1000/flink_squirrel_1000.png)\"\n            WhiteFilled: \"**Sizes (px)**: [50x50](/img/logo/png/50/white_50.png), [100x100](/img/logo/png/100/flink_squirrel_100_white.png), [200x200](/img/logo/png/200/flink_squirrel_200_white.png), [500x500](/img/logo/png/500/flink_squirrel_500_white.png), [1000x1000](/img/logo/png/1000/flink_squirrel_white_1000.png)</p>\"\n            BlackOutline: \"**Sizes (px)**: [50x50](/img/logo/png/50/black_50.png), [100x100](/img/logo/png/100/flink_squirrel_100_black.png), [200x200](/img/logo/png/200/flink_squirrel_200_black.png), [500x500](/img/logo/png/500/flink_squirrel_500_black.png), [1000x1000](/img/logo/png/1000/flink_squirrel_black_1000.png)\"\n\n    svg:\n        name: \"svg\"\n\n        cols: \n          - id: \"Colored\"\n            name: \"Colored logo\"\n          - id: \"WhiteFilled\"\n            name: \"White filled logo\"\n          - id: \"BlackOutline\"\n            name: \"Black outline logo\"\n\n        rows:\n          - Colored: \n             val: \"<img src='/img/logo/svg/color_black.svg' alt='Apache Flink Logo' title='Apache Flink Logo' width='200px' />\"\n             html: true\n            WhiteFilled: \n             val: \"<img src='/img/logo/svg/white_filled.svg' alt='Apache Flink Logo' title='Apache Flink Logo' width='200px' style='background:black;' />\"\n             html: true\n            BlackOutline: \n             val: \"<img src='/img/logo/svg/black_outline.svg' alt='Apache Flink Logo' title='Apache Flink Logo' width='200px'/>\"\n             html: true\n          - Colored: \"Colored logo with black text ([color_black.svg](/img/logo/svg/color_black.svg))\"\n            WhiteFilled: \"White filled logo ([white_filled.svg](/img/logo/svg/white_filled.svg))\"\n            BlackOutline: \"Black outline logo ([black_outline.svg](/img/logo/svg/black_outline.svg))\"\n\n\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.3.1\nDESCRIPTION: Maven dependency configurations for core Flink components including flink-java, flink-streaming-java, and flink-clients with version 1.3.1. These dependencies are required for building Flink applications.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-06-23-release-1.3.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.3.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.10</artifactId>\n  <version>1.3.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.10</artifactId>\n  <version>1.3.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: SQL Table Creation with Pulsar Connector (Legacy Version)\nDESCRIPTION: Example of creating a Flink SQL table using the Pulsar connector in versions prior to 2.7.0. Shows the old configuration parameter style with connector prefix.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-07-pulsar-flink-connector-270.md#2025-04-08_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\ncreate table topic1(\n    `rip` VARCHAR,\n    `rtime` VARCHAR,\n    `uid` bigint,\n    `client_ip` VARCHAR,\n    `day` as TO_DATE(rtime),\n    `hour` as date_format(rtime,'HH')\n) with (\n    'connector.type' ='pulsar',\n    'connector.version' = '1',\n    'connector.topic' ='persistent://public/default/test_flink_sql',\n    'connector.service-url' ='pulsar://xxx',\n    'connector.admin-url' ='http://xxx',\n    'connector.startup-mode' ='earliest',\n    'connector.properties.0.key' ='pulsar.reader.readerName',\n    'connector.properties.0.value' ='testReaderName',\n    'format.type' ='json',\n    'update-mode' ='append'\n);\n```\n\n----------------------------------------\n\nTITLE: Markdown Image Embedding - Local State Architecture\nDESCRIPTION: Markdown syntax for embedding an image illustrating Flink's local state management\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/flink-architecture.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{< img src=\"/img/local-state.png\" width=\"600px\" >}}\n```\n\n----------------------------------------\n\nTITLE: Converting Between DataStream API and Table API in Flink\nDESCRIPTION: This Java code demonstrates the improved interoperability between DataStream API and Table API in Flink 1.13. It shows how to convert a DataStream to a Table with schema configuration and watermark handling, and then back to a DataStream with subsequent operations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nTable table = tableEnv.fromDataStream(\n  dataStream,\n  Schema.newBuilder()\n    .columnByMetadata(\"rowtime\", \"TIMESTAMP(3)\")\n    .watermark(\"rowtime\", \"SOURCE_WATERMARK()\")\n    .build());\n\nDataStream<Row> dataStream = tableEnv.toDataStream(table)\n  .keyBy(r -> r.getField(\"user\"))\n  .window(...);\n```\n\n----------------------------------------\n\nTITLE: Installing PyFlink via Pip\nDESCRIPTION: This bash command demonstrates how to install PyFlink through the pip package manager. From Flink 1.10, users can easily install PyFlink using this simple command.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-11-release-1.10.0.md#2025-04-08_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install apache-flink\n```\n\n----------------------------------------\n\nTITLE: Setting Batch Execution Mode Programmatically in Apache Flink\nDESCRIPTION: This Java code snippet shows how to programmatically set the runtime execution mode to BATCH when configuring the StreamExecutionEnvironment in a Flink application.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-03-11-batch-execution-mode.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setRuntimeMode(RuntimeExecutionMode.BATCH);\n```\n\n----------------------------------------\n\nTITLE: Reading Avro Schema Snapshot Configuration in Flink\nDESCRIPTION: Implementation of the readSnapshot method that restores the previous schema definition and attempts to extract the runtime schema from the user code class loader.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-29-state-unlocked-interacting-with-state-in-apache-flink.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n  @Override\n  public void readSnapshot(\n      int readVersion,\n      DataInputView in,\n      ClassLoader userCodeClassLoader) throws IOException {\n\n    assert readVersion == 1;\n    final String previousSchemaDefinition = in.readUTF();\n    this.previousSchema = parseAvroSchema(previousSchemaDefinition);\n    this.runtimeType = findClassOrFallbackToGeneric(\n      userCodeClassLoader,\n      previousSchema.getFullName());\nâ€‹\n    this.runtimeSchema = tryExtractAvroSchema(userCodeClassLoader, runtimeType);\n  }\n```\n\n----------------------------------------\n\nTITLE: HTML Issue List for Flink JIRA Tickets\nDESCRIPTION: HTML markup containing links to Flink JIRA tickets with descriptions of bugs and improvements. Each issue is represented as a list item with a JIRA ticket link and summary.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-08-06-release-1.12.5.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-23135'>FLINK-23135</a>] -         Flink SQL Error while applying rule AggregateReduceGroupingRule</li>\n```\n\n----------------------------------------\n\nTITLE: Entering Flink SQL CLI Client\nDESCRIPTION: Command to start the Flink SQL CLI client in the Docker container.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose exec sql-client ./sql-client.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scrape Targets for Flink\nDESCRIPTION: YAML configuration for Prometheus to scrape metrics from Flink job managers and task managers. It defines static targets in prometheus.yml.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-03-11-prometheus-monitoring.md#2025-04-08_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nscrape_configs:\n- job_name: 'flink'\n  static_configs:\n  - targets: ['job-cluster:9999', 'taskmanager1:9999', 'taskmanager2:9999']\n```\n\n----------------------------------------\n\nTITLE: Displaying Kafka Consumer Lag Metric in Markdown Table\nDESCRIPTION: Shows the 'records-lag-max' metric for FlinkKafkaConsumer in a markdown table, indicating the maximum lag in terms of records for any partition.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-02-21-monitoring-best-practices.md#2025-04-08_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric | Scope | Description |\n| ------ | ----- | ----------- |\n| `records-lag-max` | user | applies to `FlinkKafkaConsumer`. The maximum lag in terms of the number of records for any partition in this window. An increasing value over time is your best indication that the consumer group is not keeping up with the producers. |\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Powered By Page Layout\nDESCRIPTION: YAML frontmatter and table configuration defining the layout and content structure for the Powered By page. Includes page metadata and a complex table structure for displaying company logos and descriptions in a grid format.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/powered-by.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Powered By\nbookToc: false\nweight: 5\naliases:\n- /powered-by.html\n- /powered-by/index.html\n\ntables:\n    powered-by:\n        name: \"Powered By\"\n        id: \"powered-by-flink\"\n    \n        cols: \n          - id: \"column1\"\n          - id: \"column2\"\n          - id: \"column3\"\n          - id: \"column4\"\n\n        rows: \n          - column1: \n             val: \"<img src='/img/poweredby/alibaba-logo.png' alt='Alibaba'>\"\n             html: true\n            column2: \n             val: \"<img src='/img/poweredby/aws-logo.png' alt='AWS'>\"\n             html: true\n            column3: \n             val: \"<img src='/img/poweredby/bettercloud-logo.png' alt='BetterCloud'>\"\n             html: true\n            column4:\n             val: \"<img src='/img/poweredby/bouygues-logo.jpg' alt='Bouygues'>\"\n             html: true\n```\n\n----------------------------------------\n\nTITLE: Implementing Version and Serialization Methods for Avro Schema Snapshot\nDESCRIPTION: Methods to handle versioning and writing the snapshot configuration for Avro serialization, storing the runtime schema information for later compatibility checks.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-29-state-unlocked-interacting-with-state-in-apache-flink.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n  @Override\n  public int getCurrentVersion() {\n    return 1;\n  }\nâ€‹\n  @Override\n  public void writeSnapshot(DataOutputView out) throws IOException {\n    out.writeUTF(runtimeSchema.toString(false));\n  }\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.6.4\nDESCRIPTION: Maven dependency configuration for core Flink components including flink-java, flink-streaming-java, and flink-clients modules. These dependencies should be added to the project's pom.xml file to use Flink 1.6.4.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-02-25-release-1.6.4.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.6.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.6.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.6.4</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Removed Methods from JoinedStreams.WithWindow Class\nDESCRIPTION: Methods that have been removed from the JoinedStreams.WithWindow class. These methods were used for configuring windowed join operations between two streams.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_15\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.streaming.api.datastream.JoinedStreams$WithWindow<T1,T2,KEY,W> allowedLateness(org.apache.flink.streaming.api.windowing.time.Time)\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<T> with(org.apache.flink.api.common.functions.JoinFunction<T1,T2,T>)\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<T> with(org.apache.flink.api.common.functions.FlatJoinFunction<T1,T2,T>, org.apache.flink.api.common.typeinfo.TypeInformation<T>)\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<T> with(org.apache.flink.api.common.functions.FlatJoinFunction<T1,T2,T>)\norg.apache.flink.streaming.api.datastream.SingleOutputStreamOperator<T> with(org.apache.flink.api.common.functions.JoinFunction<T1,T2,T>, org.apache.flink.api.common.typeinfo.TypeInformation<T>)\n```\n\n----------------------------------------\n\nTITLE: Flink Documentation Front Matter Example\nDESCRIPTION: Example of front matter used in Flink documentation Markdown files. It demonstrates how to set the page title, control page ordering, and set up redirects from removed pages.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/README.md#2025-04-08_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"Title of the Page\" <-- Title rendered in the side nave\nweight: 1 <-- Weight controls the ordering of pages in the side nav.\naliases:  <-- Alias to setup redirect from removed page to this one\n  - /alias/to/removed/page.html\n---\n```\n\n----------------------------------------\n\nTITLE: Enabling Restart Time Tracking in YAML\nDESCRIPTION: Configuration to enable automatic tracking of restart times for scaling operations, which helps optimize future scaling decisions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-21-release-kubernetes-operator-1.8.0.md#2025-04-08_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\njob.autoscaler.restart.time-tracking.enabled: true\n```\n\n----------------------------------------\n\nTITLE: Downloading Flink Distribution and Running an Example Job\nDESCRIPTION: Downloads the Flink distribution, unpacks it, and runs an example streaming job (TopSpeedWindowing) on the deployed Flink cluster. This demonstrates how to submit jobs to a running Flink cluster.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-20-flink-docker.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# 1: (optional) Download the Flink distribution, and unpack it\nwget https://archive.apache.org/dist/flink/flink-1.11.1/flink-1.11.1-bin-scala_2.12.tgz\ntar xf flink-1.11.1-bin-scala_2.12.tgz\ncd flink-1.11.1\n\n# 2: Start the Flink job\n./bin/flink run ./examples/streaming/TopSpeedWindowing.jar\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Front Matter for Flink Blog Post\nDESCRIPTION: YAML front matter defining metadata for a blog post about memory management improvements in Apache Flink 1.10. It specifies the author, date, excerpt, title, and aliases for the post.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-21-memory-management-improvements-flink-1.10.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nauthors:\n- andrey: null\n  name: Andrey Zagrebin\ndate: \"2020-04-21T12:00:00Z\"\nexcerpt: This post discusses the recent changes to the memory model of the Task Managers\n  and configuration options for your Flink applications in Flink 1.10.\ntitle: Memory Management Improvements with Apache Flink 1.10\naliases:\n- /news/2020/04/21/memory-management-improvements-flink-1.10.html\n```\n\n----------------------------------------\n\nTITLE: Implementing a Greeter Function in Java\nDESCRIPTION: Shows how to implement the same stateful greeter function using the new Java SDK in StateFun 3.0.0. It demonstrates state management, message handling, and response sending in Java.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-04-15-release-statefun-3.0.0.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nstatic final class Greeter implements StatefulFunction {\n    static final ValueSpec<Integer> VISITS = ValueSpec.named(\"visits\").withIntType();\n\n    @Override\n    public CompletableFuture<Void> apply(Context context, Message message){\n        // update the visits count\n        int visits = context.storage().get(VISITS).orElse(0);\n        visits++;\n        context.storage().set(VISITS, visits);\n\n        // compute a greeting\n        var name = message.asUtf8String();\n        var greeting = String.format(\"Hello there %s at the %d-th time!\\n\", name, visits);\n\n        // reply to the caller with a greeting\n        var caller = context.caller().get();\n        context.send(\n            MessageBuilder.forAddress(caller)\n                .withValue(greeting)\n                .build()\n        );\n\n        return context.done();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Removed Fields from CheckpointingOptions in Apache Flink\nDESCRIPTION: List of configuration options removed from the CheckpointingOptions class, including options for local recovery, state backend, and async snapshots.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.configuration.ConfigOption<java.lang.Boolean> LOCAL_RECOVERY\n```\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.configuration.ConfigOption<java.lang.String> STATE_BACKEND\n```\n\nLANGUAGE: java\nCODE:\n```\norg.apache.flink.configuration.ConfigOption<java.lang.Boolean> ASYNC_SNAPSHOTS\n```\n\n----------------------------------------\n\nTITLE: Session Window TVF Usage in SQL\nDESCRIPTION: Examples of using SESSION Window TVF in streaming mode with partition keys and aggregations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM TABLE(\n   SESSION(TABLE Bid PARTITION BY item, DESCRIPTOR(bidtime), INTERVAL '5' MINUTES));\n\nSELECT window_start, window_end, item, SUM(price) AS total_price\nFROM TABLE(\n    SESSION(TABLE Bid PARTITION BY item, DESCRIPTOR(bidtime), INTERVAL '5' MINUTES))\nGROUP BY item, window_start, window_end;\n```\n\n----------------------------------------\n\nTITLE: Building Flink Website with Local Hugo Installation\nDESCRIPTION: Command to build and serve the Flink website documentation locally using a local Hugo installation. This requires Hugo to be installed on the system.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/README.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ./build.sh\n```\n\n----------------------------------------\n\nTITLE: Creating Internal Page Links in Flink Documentation\nDESCRIPTION: This Liquid snippet demonstrates how to create a link to a section within the same page in Flink documentation. It uses the auto-generated heading ID.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_7\n\nLANGUAGE: liquid\nCODE:\n```\n[Link Text](#heading-title)\n```\n\n----------------------------------------\n\nTITLE: Creating MySQL Dimension Table for Categories in Flink SQL\nDESCRIPTION: This SQL creates a table to access category data from MySQL. It uses the JDBC connector and includes lookup cache settings for efficient dimension table joins.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE category_dim (\n    sub_category_id BIGINT,\n    parent_category_name STRING\n) WITH (\n    'connector' = 'jdbc',\n    'url' = 'jdbc:mysql://mysql:3306/flink',\n    'table-name' = 'category',\n    'username' = 'root',\n    'password' = '123456',\n    'lookup.cache.max-rows' = '5000',\n    'lookup.cache.ttl' = '10min'\n);\n```\n\n----------------------------------------\n\nTITLE: Factory SPI Registration in Java\nDESCRIPTION: Service Provider Interface registration for the factory class to enable discovery by Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-09-07-connector-table-sql-api-part1.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\norg.example.acme.ImapTableSourceFactory\n```\n\n----------------------------------------\n\nTITLE: Running the Fraud Detection Demo with Docker\nDESCRIPTION: Instructions for running the self-contained fraud detection demo application locally using Docker and docker-compose. The demo includes Apache Kafka, Apache Flink, and a web application.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-15-demo-fraud-detection.md#2025-04-08_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.5.4\nDESCRIPTION: XML snippet showing how to update Maven dependencies for Apache Flink 1.5.4. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-09-20-release-1.5.4.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.5.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.5.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.5.4</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.6.2\nDESCRIPTION: XML snippet showing the updated Maven dependencies for Apache Flink 1.6.2. It includes the core Flink modules: flink-java for basic APIs, flink-streaming-java for stream processing, and flink-clients for client connectivity.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-10-29-release-1.6.2.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.6.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.6.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.6.2</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: HTML List of Flink JIRA Issues\nDESCRIPTION: An HTML unordered list showing two JIRA issues for Apache Flink: FLINK-36468 about replacing Parquet preconditions and FLINK-36510 about upgrading Pekko version.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2025-02-12-release-1.20.1.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-36468'>FLINK-36468</a>] -         Use Flink Preconditions util instead of Parquet\n</li>\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-36510'>FLINK-36510</a>] -         Upgrade Pekko from 1.0.1 to 1.1.2\n</li>\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Types with Kryo in Flink Java\nDESCRIPTION: Example of how to register custom types with Kryo serialization. Registering types adds them to an internal map which reduces serialization overhead by using integer tags instead of fully qualified class names.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-15-flink-serialization-tuning-vol-1.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nenv.getConfig().registerKryoType(MyCustomType.class);\n```\n\n----------------------------------------\n\nTITLE: Listing RocksDB Native Library in Flink Distribution\nDESCRIPTION: This bash command shows how to verify the presence of the RocksDB native library in the Flink distribution JAR file.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-18-rocksdb.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ jar -tvf lib/flink-dist_2.12-1.12.0.jar| grep librocksdbjni-linux64\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Flink SQL Script with Statement Sets\nDESCRIPTION: This SQL script demonstrates the enhanced SQL Client capabilities in Flink 1.13, including initialization, configuration settings, and multi-query execution through Statement Sets. It shows how to set up a catalog, create temporary tables, configure execution parameters, and execute multiple INSERT statements as a single job.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-05-03-release-1.13.0.md#2025-04-08_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n-- set up a catalog\nCREATE CATALOG hive_catalog WITH ('type' = 'hive');\nUSE CATALOG hive_catalog;\n\n-- or use temporary objects\nCREATE TEMPORARY TABLE clicks (\n  user_id BIGINT,\n  page_id BIGINT,\n  viewtime TIMESTAMP\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'clicks',\n  'properties.bootstrap.servers' = '...',\n  'format' = 'avro'\n);\n\n-- set the execution mode for jobs\nSET execution.runtime-mode=streaming;\n\n-- set the sync/async mode for INSERT INTOs\nSET table.dml-sync=false;\n\n-- set the job's parallelism\nSET parallism.default=10;\n\n-- set the job name\nSET pipeline.name = my_flink_job;\n\n-- restore state from the specific savepoint path\nSET execution.savepoint.path=/tmp/flink-savepoints/savepoint-bb0dab;\n\nBEGIN STATEMENT SET;\n\nINSERT INTO pageview_pv_sink\nSELECT page_id, count(1) FROM clicks GROUP BY page_id;\n\nINSERT INTO pageview_uv_sink\nSELECT page_id, count(distinct user_id) FROM clicks GROUP BY page_id;\n\nEND;\n```\n\n----------------------------------------\n\nTITLE: Adding Apache Flink Stateful Functions Dependencies to Maven POM\nDESCRIPTION: This XML snippet demonstrates how to include Apache Flink Stateful Functions dependencies in a Maven project's pom.xml file. It includes the SDK for development and a harness for local testing.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/downloads.md#2025-04-08_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>statefun-sdk</artifactId>\n  <version>{{< param StateFunStableVersion >}}</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>statefun-flink-harness</artifactId>\n  <version>{{< param StateFunStableVersion >}}</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Installing Kafka Operator with KUDO\nDESCRIPTION: Command to install the Kafka Operator using KUDO without creating an instance. The Flink demo will create the Kafka instance as a dependency.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl kudo install kafka --version=1.2.0 --skip-instance\n```\n\n----------------------------------------\n\nTITLE: Example Commit Message Format\nDESCRIPTION: Shows proper commit message formatting with JIRA references and components.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/code-style-and-quality-pull-requests.md#2025-04-08_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n[hotfix] Fix update_branch_version.sh to allow version suffixes\\n[hotfix] [table] Remove unused geometry dependency\\n[FLINK-11704][tests] Improve AbstractCheckpointStateOutputStreamTestBase\\n[FLINK-10569][runtime] Remove Instance usage in ExecutionVertexCancelTest\\n[FLINK-11702][table-planner-blink] Introduce a new table type system\n```\n\n----------------------------------------\n\nTITLE: Running the PyFlink application\nDESCRIPTION: Command to execute the Python script containing the PyFlink application with Pandas UDF. This builds and runs the program in a local mini-cluster.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-04-pyflink-pandas-udf-support-flink.md#2025-04-08_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython pandas_udf_demo.py\n```\n\n----------------------------------------\n\nTITLE: Creating HTML Table for Backpressure Interpretation\nDESCRIPTION: HTML snippet for creating a table to interpret backpressure based on inPoolUsage and outPoolUsage metrics.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-07-23-flink-network-stack-2.md#2025-04-08_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<center>\n<table class=\"tg\">\n  <tr>\n    <th></th>\n    <th class=\"tg-center\"><code>outPoolUsage</code> low</th>\n    <th class=\"tg-center\"><code>outPoolUsage</code> high</th>\n  </tr>\n  <tr>\n    <th class=\"tg-top\"><code>inPoolUsage</code> low</th>\n    <td class=\"tg-topcenter\">\n      <span class=\"glyphicon glyphicon-ok-sign\" aria-hidden=\"true\" style=\"color:green;font-size:1.5em;\"></span></td>\n    <td class=\"tg-topcenter\">\n      <span class=\"glyphicon glyphicon-warning-sign\" aria-hidden=\"true\" style=\"color:orange;font-size:1.5em;\"></span><br>\n      (backpressured, temporary situation: upstream is not backpressured yet or not anymore)</td>\n  </tr>\n  <!-- Additional rows omitted for brevity -->\n</table>\n</center>\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Containers for Flink SQL Demo Environment\nDESCRIPTION: Command to start all containers defined in the Docker Compose configuration in detached mode.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Windowing Job Example Reference in Markdown\nDESCRIPTION: A reference to a WindowingJob Java class used in the experiment demonstrating the effect of increasing parallelism on latency reduction. The job is referenced from a GitHub repository for the latency optimization experiments.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-18-latency-part1.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[WindowingJob](https://github.com/ververica/lab-flink-latency/blob/main/src/main/java/com/ververica/lablatency/job/WindowingJob.java)\n```\n\n----------------------------------------\n\nTITLE: Configuring External Resource Framework for GPU in Flink\nDESCRIPTION: Configuration settings for enabling GPU support in Flink's External Resource Framework, including driver factory class specification, resource amount, and platform-specific configuration keys for Yarn and Kubernetes.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-06-external-resource.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexternal-resources: gpu\n# Define the driver factory class of gpu resource.\nexternal-resource.gpu.driver-factory.class: org.apache.flink.externalresource.gpu.GPUDriverFactory\n# Define the amount of gpu resource per TaskManager.\nexternal-resource.gpu.amount: 1\n# Enable the coordination mode if you run it in standalone mode\nexternal-resource.gpu.param.discovery-script.args: --enable-coordination\n\n\n# If you run it on Yarn\nexternal-resource.gpu.yarn.config-key: yarn.io/gpu\n# If you run it on Kubernetes\nexternal-resource.gpu.kubernetes.config-key: nvidia.com/gpu\n```\n\n----------------------------------------\n\nTITLE: Incorrect Lambda Usage with Capturing Variables\nDESCRIPTION: Example of an inefficient lambda that captures a variable from the outer scope, creating a new instance for every call. This should be avoided in performance-critical code.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nmap.computeIfAbsent(key, x -> key.toLowerCase())\n```\n\n----------------------------------------\n\nTITLE: HTML Issue List Markup\nDESCRIPTION: HTML markup showing linked JIRA issues with their respective ticket numbers and descriptions, including bug fixes and test implementations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2018-02-15-release-1.4.1.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-8473'>FLINK-8473</a>] -         JarListHandler may fail with NPE if directory is deleted</li>\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-8571'>FLINK-8571</a>] -         Provide an enhanced KeyedStream implementation to use ForwardPartitioner</li>\n</ul>\n    \n<h2>        Test</h2>\n<ul>\n<li>[<a href='https://issues.apache.org/jira/browse/FLINK-8472'>FLINK-8472</a>] -         Extend migration tests for Flink 1.4</li>\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Displaying JVM Memory Metrics in Markdown Table\nDESCRIPTION: Shows various JVM memory metrics for JobManager and TaskManager in a markdown table, including heap, non-heap, direct, and mapped memory usage.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-02-21-monitoring-best-practices.md#2025-04-08_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric | Scope | Description |\n| ------ | ----- | ----------- |\n| `Status.JVM.Memory.NonHeap.Committed` | job-/taskmanager | The amount of non-heap memory guaranteed to be available to the JVM (in bytes). |\n| `Status.JVM.Memory.Heap.Used` | job-/taskmanager | The amount of heap memory currently used (in bytes). |\n| `Status.JVM.Memory.Heap.Committed` | job-/taskmanager | The amount of heap memory guaranteed to be available to the JVM (in bytes). |\n| `Status.JVM.Memory.Direct.MemoryUsed` | job-/taskmanager | The amount of memory used by the JVM for the direct buffer pool (in bytes). |\n| `Status.JVM.Memory.Mapped.MemoryUsed` | job-/taskmanager | The amount of memory used by the JVM for the mapped buffer pool (in bytes). |\n| `Status.JVM.GarbageCollector.G1 Young Generation.Time` | job-/taskmanager | The total time spent performing G1 Young Generation garbage collection. |\n| `Status.JVM.GarbageCollector.G1 Old Generation.Time` | job-/taskmanager | The total time spent performing G1 Old Generation garbage collection. |\n```\n\n----------------------------------------\n\nTITLE: Creating User-User Similarity Graph Based on Common Songs in Flink Gelly\nDESCRIPTION: Transforms a user-song graph into a user-user similarity graph by connecting users who listen to the same songs. It filters edges by playcount threshold, groups by song, and then creates edges between users who share interests.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-08-24-introducing-flink-gelly.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n// create a user-user similarity graph:\n// two users that listen to the same song are connected\nDataSet<Edge> similarUsers = userSongGraph.getEdges()\n        // filter out user-song edges that are below the playcount threshold\n        .filter(new FilterFunction<Edge<String, Integer>>() {\n            \tpublic boolean filter(Edge<String, Integer> edge) {\n                    return (edge.getValue() > playcountThreshold);\n                }\n        })\n        .groupBy(1)\n        .reduceGroup(new GroupReduceFunction() {\n                void reduce(Iterable<Edge> edges, Collector<Edge> out) {\n                    List users = new ArrayList();\n                    for (Edge edge : edges) {\n                        users.add(edge.getSource());\n                        for (int i = 0; i < users.size() - 1; i++) {\n                            for (int j = i+1; j < users.size() - 1; j++) {\n                                out.collect(new Edge(users.get(i), users.get(j)));\n                            }\n                        }\n                    }\n                }\n        })\n        .distinct();\n\nGraph similarUsersGraph = Graph.fromDataSet(similarUsers).getUndirected();\n```\n\n----------------------------------------\n\nTITLE: External Link Template in Hugo for Flink Training Course\nDESCRIPTION: A Hugo shortcode template that creates an external link to the Flink training course documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/getting-started/training-course.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n{{< external_link name=\"Read all about the Flink Training Course here.\">}}\n```\n\n----------------------------------------\n\nTITLE: Setting Hugo Front Matter Configuration for Flink Documentation\nDESCRIPTION: YAML front matter configuration for a Hugo documentation page. Sets the weight, title, and external book reference URL for the stable version of Apache Flink documentation.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-stable.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 1\ntitle: Flink $FlinkStableShortVersion (stable)\nbookHref: \"https://nightlies.apache.org/flink/flink-docs-stable/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment in Markdown\nDESCRIPTION: Standard Apache License 2.0 header comment block used in Apache Flink documentation files to specify the licensing terms.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/special-thanks.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Creating CDC Table with Debezium JSON Format in Flink SQL\nDESCRIPTION: SQL DDL statement to create a table that consumes change data capture (CDC) events using the Debezium JSON format. The configuration includes options for schema inclusion and parse error handling.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-06-release-1.11.0.md#2025-04-08_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE my_table (\n  ...\n) WITH (\n  'connector'='...', -- e.g. 'kafka'\n  'format'='debezium-json',\n  'debezium-json.schema-include'='true' -- default: false (Debezium can be configured to include or exclude the message schema)\n  'debezium-json.ignore-parse-errors'='true' -- default: false\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Table in Flink SQL\nDESCRIPTION: SQL statement to create a table in Flink SQL that connects to a Kafka topic. It defines the schema, computes a processing-time attribute, and sets a watermark for event time.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE user_behavior (\n    user_id BIGINT,\n    item_id BIGINT,\n    category_id BIGINT,\n    behavior STRING,\n    ts TIMESTAMP(3),\n    proctime AS PROCTIME(),   -- generates processing-time attribute using computed column\n    WATERMARK FOR ts AS ts - INTERVAL '5' SECOND  -- defines watermark on ts column, marks ts as event-time attribute\n) WITH (\n    'connector' = 'kafka',  -- using kafka connector\n    'topic' = 'user_behavior',  -- kafka topic\n    'scan.startup.mode' = 'earliest-offset',  -- reading from the beginning\n    'properties.bootstrap.servers' = 'kafka:9094',  -- kafka broker address\n    'format' = 'json'  -- the data format is json\n);\n```\n\n----------------------------------------\n\nTITLE: ValueState Workload Benchmarks Table - Detailed Percentiles\nDESCRIPTION: This table provides detailed percentile metrics for checkpoint performance, showing the differences in end-to-end duration, checkpointed data size, and full checkpoint data size between Changelog Enabled and Disabled configurations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-30-changelog-state-backend.md#2025-04-08_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<table border=\"1\">\n  <thead>\n    <tr>\n      <th style=\"padding: 5px\">Percentile</th>\n      <th style=\"padding: 5px\">End to End Duration</th>\n      <th style=\"padding: 5px\">Checkpointed Data Size *</th>\n      <th style=\"padding: 5px\">Full Checkpoint Data Size *</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"padding: 5px\">50%</td>\n      <td style=\"padding: 5px\">311ms / 5s</td>\n      <td style=\"padding: 5px\">14.8MB / 3.05GB</td>\n      <td style=\"padding: 5px\">24.2GB / 18.5GB</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 5px\">90%</td>\n      <td style=\"padding: 5px\">664ms / 6s</td>\n      <td style=\"padding: 5px\">23.5MB / 4.52GB</td>\n      <td style=\"padding: 5px\">25.2GB / 19.3GB</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 5px\">99%</td>\n      <td style=\"padding: 5px\">1s / 7s</td>\n      <td style=\"padding: 5px\">36.6MB / 5.19GB</td>\n      <td style=\"padding: 5px\">25.6GB / 19.6GB</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 5px\">99.9%</td>\n      <td style=\"padding: 5px\">1s / 10s</td>\n      <td style=\"padding: 5px\">52.8MB / 6.49GB</td>\n      <td style=\"padding: 5px\">25.7GB / 19.8GB</td>\n    </tr>\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Triggering Checkpoints via Command Line in Apache Flink\nDESCRIPTION: Command line interface for manually triggering checkpoints in Flink jobs. Supports both full and incremental checkpoints based on the provided options. The -full flag triggers a full checkpoint, while omitting it uses incremental checkpointing if configured.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-03-18-release-1.19.0.md#2025-04-08_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./bin/flink checkpoint $JOB_ID [-full]\n```\n\n----------------------------------------\n\nTITLE: Checking KUDO Plan Status for Flink Demo\nDESCRIPTION: Command to check the status of the deployment plan for the financial fraud demo. This shows the progress of each phase and step in the plan.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl kudo plan status --instance flink-demo\nPlan(s) for \"flink-demo\" in namespace \"default\":\n.\nâ””â”€â”€ flink-demo (Operator-Version: \"flink-demo-0.1.4\" Active-Plan: \"deploy\")\n\tâ””â”€â”€ Plan deploy (serial strategy) [IN_PROGRESS]\n    \tâ”œâ”€â”€ Phase dependencies [IN_PROGRESS]\n    \tâ”‚   â”œâ”€â”€ Step zookeeper (COMPLETE)\n    \tâ”‚   â””â”€â”€ Step kafka (IN_PROGRESS)\n    \tâ”œâ”€â”€ Phase flink-cluster [PENDING]\n    \tâ”‚   â””â”€â”€ Step flink (PENDING)\n    \tâ”œâ”€â”€ Phase demo [PENDING]\n    \tâ”‚   â”œâ”€â”€ Step gen (PENDING)\n    \tâ”‚   â””â”€â”€ Step act (PENDING)\n    \tâ””â”€â”€ Phase flink-job [PENDING]\n        \tâ””â”€â”€ Step submit (PENDING)\n```\n\n----------------------------------------\n\nTITLE: Installing PyFlink using pip\nDESCRIPTION: Command line instruction for installing the PyFlink package from PyPI using pip. Python 3.5 or higher is required to install and run PyFlink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-04-pyflink-pandas-udf-support-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install apache-flink\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment in HTML\nDESCRIPTION: Standard Apache License 2.0 header comment that indicates the licensing terms for the content of the file. This is a common requirement for all files in Apache Software Foundation projects.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/what-is-flink/_index.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Connecting Streams and Applying Pattern Evaluator in Flink\nDESCRIPTION: Connects the keyed actions stream with the broadcast patterns stream and applies a custom PatternEvaluator function to process the connected streams.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-06-26-broadcast-state.md#2025-04-08_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nDataStream<Tuple2<Long, Pattern>> matches = actionsByUser\n .connect(bcedPatterns)\n .process(new PatternEvaluator());\n```\n\n----------------------------------------\n\nTITLE: Document Frontmatter in YAML\nDESCRIPTION: YAML frontmatter containing document metadata including weight, title, and external documentation link.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-kubernetes-operator-stable.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 4\ntitle: Kubernetes Operator $FlinkKubernetesOperatorStableShortVersion (latest)\nbookHref: \"https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-stable/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Invoking Python UDF\nDESCRIPTION: Demonstrates how to invoke a registered Python UDF in Table API.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-04-09-pyflink-udf-support-flink.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# use the function in Python Table API\nmy_table.select(\"add(a, b)\")\n```\n\n----------------------------------------\n\nTITLE: Assembling a Storm Topology in Flink\nDESCRIPTION: This code demonstrates how to assemble a Storm topology in the standard Storm way without any code changes to Spouts, Bolts, or the topology itself. This is the first step when running a Storm topology on Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-11-storm-compatibility.md#2025-04-08_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// assemble topology, the Storm way\nTopologyBuilder builder = new TopologyBuilder();\nbuilder.setSpout(\"source\", new StormFileSpout(inputFilePath));\nbuilder.setBolt(\"tokenizer\", new StormBoltTokenizer())\n       .shuffleGrouping(\"source\");\nbuilder.setBolt(\"counter\", new StormBoltCounter())\n       .fieldsGrouping(\"tokenizer\", new Fields(\"word\"));\nbuilder.setBolt(\"sink\", new StormBoltFileSink(outputFilePath))\n       .shuffleGrouping(\"counter\");\n```\n\n----------------------------------------\n\nTITLE: PyFlink UDF Implementation Fragment\nDESCRIPTION: Partial code showing the UDF implementation mentioned in the benchmark section. Note that this appears to be an incomplete code snippet from the original text.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-06-pyflink-1.15-thread-mode.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Table Store Catalog in Flink SQL\nDESCRIPTION: SQL commands for creating a Table Store catalog with HDFS warehouse and optional Hive metastore integration. The example shows how to create the catalog, use it, and prepare to create tables.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-08-29-release-table-store-0.2.0.md#2025-04-08_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE CATALOG tablestore WITH (\n  'type'='table-store',\n  'warehouse'='hdfs://nn:8020/warehouse/path',\n  -- optional hive metastore\n  'metastore'='hive',\n  'uri'='thrift://<hive-metastore-host-name>:<port>'\n);\n\nUSE CATALOG tablestore;\n\nCREATE TABLE my_table ...\n```\n\n----------------------------------------\n\nTITLE: System Requirements for Changelog Storage\nDESCRIPTION: Key requirements for the changelog storage component (DSTL) including durability, workload characteristics, latency targets, and consistency guarantees.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-05-30-changelog-state-backend.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### Durability\nChangelog constitutes a part of a checkpoint, and therefore the same durability guarantees as for checkpoints must be provided. However, the duration for which the changelog is stored is expected to be short (until the changes are materialized).\n\n### Workload\nThe workload is write-heavy: changelog is written continuously, and it is only read in case of failure. Once written, data can not be modified.\n\n### Latency\nWe target checkpoint duration of 1s in the Flink 1.15 MVP for 99% of checkpoints. Therefore, an individual write request must complete within that duration or less (if parallelism is 100, then 99.99% of write requests must complete within 1s).\n\n### Consistency\nOnce a change is persisted (and acknowledged to JM), it must be available for replay to enable recovery (this can be achieved by using a single machine, quorum, or synchronous replication).\n```\n\n----------------------------------------\n\nTITLE: Catalog Management Operations in Flink SQL\nDESCRIPTION: Shows various catalog-related SQL operations including creating a catalog, showing catalog details, describing catalog properties, and altering catalog configurations.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-08-02-release-1.20.0.md#2025-04-08_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE CATALOG `cat` WITH ('type'='generic_in_memory', 'default-database'='db');\n\nSHOW CREATE CATALOG `cat`;\n\nDESCRIBE CATALOG `cat`;\n\nALTER CATALOG `cat` SET ('default-database'='new-db');\n\nSHOW CREATE CATALOG `cat`;\n```\n\n----------------------------------------\n\nTITLE: Accessing Autoscaler Metrics Format\nDESCRIPTION: Example of the metric format used by the Flink Kubernetes Operator to report JobVertex level metrics for the autoscaler. These metrics include utilization, input rate, and target rate that are used in scaling decisions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-02-27-release-kubernetes-operator-1.4.0.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n[resource_prefix].Autoscaler.[jobVertexID].[ScalingMetric].Current/Average\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.14.3\nDESCRIPTION: XML snippet showing how to update Maven dependencies to use Apache Flink version 1.14.3. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-17-release-1.14.3.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.14.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.14.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.14.3</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Beam Window Assignment in Flink\nDESCRIPTION: Window assignment implementation using WindowedValue wrapper to store window information, defaulting to GlobalWindow if no explicit window is assigned.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-22-apache-beam-how-beam-runs-on-top-of-flink.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nWindowedValue\n```\n\n----------------------------------------\n\nTITLE: Unit Testing a Stateless FlatMap Function in Flink\nDESCRIPTION: Java unit test for the MyStatelessFlatMap function. It uses a ListCollector to capture the output and verifies the correct transformation of the input.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-03-a-guide-for-unit-testing-in-apache-flink.md#2025-04-08_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n@Test\npublic void testFlatMap() throws Exception {\n  MyStatelessFlatMap statelessFlatMap = new MyStatelessFlatMap();\n  List<String> out = new ArrayList<>();\n  ListCollector<String> listCollector = new ListCollector<>(out);\n  statelessFlatMap.flatMap(\"world\", listCollector);\n  Assert.assertEquals(Lists.newArrayList(\"hello world\"), out);\n}\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Flink Community Resources\nDESCRIPTION: HTML table markup defining a structured layout for displaying Flink community resources including blogposts, tutorials, and Flink packages with their respective links and descriptions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-05-06-community-update.md#2025-04-08_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<table class=\"table table-bordered\">\n  <thead>\n    <tr>\n      <th>Type</th>\n      <th>Links</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><span class=\"glyphicon glyphicon glyphicon-bookmark\" aria-hidden=\"true\"></span> Blogposts</td>\n      <td><ul>\n\t\t  <li><a href=\"https://medium.com/@abdelkrim.hadjidj/event-driven-supply-chain-for-crisis-with-flinksql-be80cb3ad4f9\">Event-Driven Supply Chain for Crisis with FlinkSQL and Zeppelin</a></li>\n\t\t  </ul>\n\t\t  <ul>\n\t\t  <li><a href=\"https://flink.apache.org/news/2020/04/21/memory-management-improvements-flink-1.10.html\">Memory Management Improvements with Apache Flink 1.10</a></li>\n\t\t  <li><a href=\"https://flink.apache.org/news/2020/04/15/flink-serialization-tuning-vol-1.html\">Flink Serialization Tuning Vol. 1: Choosing your Serializer â€” if you can</a></li>\n\t\t</ul>\n\t  </td>\n    </tr>\n    <tr>\n      <td><span class=\"glyphicon glyphicon-console\" aria-hidden=\"true\"></span> Tutorials</td>\n      <td><ul>\n      \t  <li><a href=\"https://flink.apache.org/2020/04/09/pyflink-udf-support-flink.html\">PyFlink: Introducing Python Support for UDFs in Flink's Table API</a></li>\n      \t  <li><a href=\"https://dev.to/morsapaes/flink-stateful-functions-where-to-start-2j39\">Flink Stateful Functions: where to start?</a></li>\n\t\t  </ul>\n\t  </td>\n    </tr>\n    <tr>\n      <td><span class=\"glyphicon glyphicon glyphicon-certificate\" aria-hidden=\"true\"></span> Flink Packages</td>\n      <td><ul><p><a href=\"https://flink-packages.org/\">Flink Packages</a> is a website where you can explore (and contribute to) the Flink <br /> ecosystem of connectors, extensions, APIs, tools and integrations. <b>New in:</b> </p>\n      \t  <li><a href=\"https://flink-packages.org/packages/spillable-state-backend-for-flink\">Spillable State Backend for Flink</a></li>\n\t\t  <li><a href=\"https://flink-packages.org/packages/flink-memory-calculator\">Flink Memory Calculator</a></li>\n\t\t  <li><a href=\"https://flink-packages.org/packages/ververica-platform-community-edition\">Ververica Platform Community Edition</a></li>\n\t\t  </ul>\n\t  </td>\n    </tr>\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Implementing a User-defined Table Function in Flink (Scala)\nDESCRIPTION: This snippet shows the implementation of a PropertiesExtractor class that extends TableFunction. It parses a preferences string into key-value pairs, extracts color and size properties, and emits a row only when both properties are present, with appropriate type information.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2017-03-29-table-sql-api-update.md#2025-04-08_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nclass PropertiesExtractor extends TableFunction[Row] {\n  def eval(prefs: String): Unit = {\n    // split string into (key, value) pairs\n    val pairs = prefs\n      .split(\",\")\n      .map { kv =>\n        val split = kv.split(\"=\")\n        (split(0), split(1))\n      }\n\n    val color = pairs.find(\\_.\\_.1 == \"color\").map(\\_.\\_.2)\n    val size = pairs.find(\\_.\\_.1 == \"size\").map(\\_.\\_.2)\n\n    // emit a row if color and size are specified\n    (color, size) match {\n      case (Some(c), Some(s)) => collect(Row.of(c, s))\n      case _ => // skip\n    }\n  }\n\n  override def getResultType = new RowTypeInfo(Types.STRING, Types.STRING)\n}\n```\n\n----------------------------------------\n\nTITLE: Dynamic Key Processing Pipeline\nDESCRIPTION: Shows the high-level structure of the processing pipeline using DynamicKeyFunction for flexible data partitioning.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-15-demo-fraud-detection.md#2025-04-08_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nDataStream<Alert> alerts =\n    transactions\n        .process(new DynamicKeyFunction())\n        .keyBy(/* some key selector */);\n        .process(/* actual calculations and alerting */)\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Header Comment\nDESCRIPTION: Standard Apache License 2.0 header comment block specifying the licensing terms and conditions for the file content.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/what-is-flink/_index.md#2025-04-08_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Key Function with Broadcast State\nDESCRIPTION: Implementation of DynamicKeyFunction that extends BroadcastProcessFunction to handle both transaction processing and rule updates. It maintains rules in broadcast state and applies them to incoming transactions.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-03-24-demo-fraud-detection-2.md#2025-04-08_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic class DynamicKeyFunction\n    extends BroadcastProcessFunction<Transaction, Rule, Keyed<Transaction, String, Integer>> {\n\n\n  @Override\n  public void processBroadcastElement(Rule rule,\n                                     Context ctx,\n                                     Collector<Keyed<Transaction, String, Integer>> out) {\n    BroadcastState<Integer, Rule> broadcastState = ctx.getBroadcastState(RULES_STATE_DESCRIPTOR);\n    broadcastState.put(rule.getRuleId(), rule);\n  }\n\n  @Override\n  public void processElement(Transaction event,\n                           ReadOnlyContext ctx,\n                           Collector<Keyed<Transaction, String, Integer>> out){\n    ReadOnlyBroadcastState<Integer, Rule> rulesState =\n                                  ctx.getBroadcastState(RULES_STATE_DESCRIPTOR);\n    for (Map.Entry<Integer, Rule> entry : rulesState.immutableEntries()) {\n        final Rule rule = entry.getValue();\n        out.collect(\n          new Keyed<>(\n            event, KeysExtractor.getKey(rule.getGroupingKeyNames(), event), rule.getRuleId()));\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Preparing input data for PyFlink application\nDESCRIPTION: Bash command to create a sample input file with temperature data, including missing values that will be interpolated by the Pandas UDF.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-08-04-pyflink-pandas-udf-support-flink.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\necho -e  \"1,98.0\\n1,\\n1,100.0\\n2,99.0\" > /tmp/input\n```\n\n----------------------------------------\n\nTITLE: Updating Maven Dependencies for Apache Flink 1.12.1\nDESCRIPTION: Maven dependency declarations for core Flink modules including flink-java, flink-streaming-java and flink-clients, updated to version 1.12.1.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-01-19-release-1.12.1.md#2025-04-08_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.12.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.12.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.12.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Implementing a Stateful FlatMap Function in Flink\nDESCRIPTION: Java implementation of a stateful FlatMap function in Flink. It uses ValueState to store the previous input and combines it with the current input in the output.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-02-03-a-guide-for-unit-testing-in-apache-flink.md#2025-04-08_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\npublic class StatefulFlatMap extends RichFlatMapFunction<String, String> {\n  ValueState<String> previousInput;\n\n  @Override\n  public void open(Configuration parameters) throws Exception {\n    previousInput = getRuntimeContext().getState(\n      new ValueStateDescriptor<String>(\"previousInput\", Types.STRING));\n  }\n\n  @Override\n  public void flatMap(String in, Collector<String> collector) throws Exception {\n    String out = \"hello \" + in;\n    if(previousInput.value() != null){\n      out = out + \" \" + previousInput.value();\n    }\n    previousInput.update(in);\n    collector.collect(out);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing IO Scheduling Algorithm for Sort-Based Blocking Shuffle in Java\nDESCRIPTION: This pseudocode demonstrates the elevator algorithm implementation for IO scheduling in Flink's sort-based blocking shuffle. It processes data regions sequentially while polling readers based on the smallest file offset to maximize sequential reads.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2021-10-26-sort-shuffle-part2.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// let data_regions as the data region list indexed from 0 to n - 1\n// let data_readers as the concurrent downstream data readers queue indexed from 0 to m - 1\nfor (data_region in data_regions) {\n    data_reader = poll_reader_of_the_smallest_file_offset(data_readers);\n    if (data_reader == null)\n        break;\n    reading_buffers = request_reading_buffers();\n    if (reading_buffers.isEmpty())\n        break;\n    read_data(data_region, data_reader, reading_buffers);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.19.1\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies are required for building Flink applications with version 1.19.1.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-06-14-release-1.19.1.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.19.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java</artifactId>\n  <version>1.19.1</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients</artifactId>\n  <version>1.19.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Installing Flink Kubernetes Operator 1.4.0 with Helm\nDESCRIPTION: Commands to add the Flink Kubernetes Operator 1.4.0 Helm chart to a local registry and install it with webhook creation disabled. This allows users to quickly set up the operator in their Kubernetes environment.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2023-02-27-release-kubernetes-operator-1.4.0.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ helm repo add flink-kubernetes-operator-1.4.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.4.0/\n$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.4.0/flink-kubernetes-operator --set webhook.create=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Apache Flink 1.14.4\nDESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java and flink-clients with version 1.14.4\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-03-11-release-1.14.4.md#2025-04-08_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.14.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.14.4</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.14.4</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Calculating Correlation Coefficient in Apache Flink Java API\nDESCRIPTION: This code calculates the Pearson correlation coefficient by computing means, covariance, and standard deviations from a collection of paired integer values. The function collects the result using out.collect() to output the normalized correlation value.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-02-09-streaming-example.md#2025-04-08_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\n            leftSum += pair.f0;\n            rightSum += pair.f1;\n            count++;\n        }\n\n        leftMean = leftSum.doubleValue() / count;\n        rightMean = rightSum.doubleValue() / count;\n\n        //compute covariance & std. deviations\n        for (Tuple2<Integer, Integer> pair : values) {\n            cov += (pair.f0 - leftMean) * (pair.f1 - rightMean) / count;\n        }\n\n        for (Tuple2<Integer, Integer> pair : values) {\n            leftSd += Math.pow(pair.f0 - leftMean, 2) / count;\n            rightSd += Math.pow(pair.f1 - rightMean, 2) / count;\n        }\n        leftSd = Math.sqrt(leftSd);\n        rightSd = Math.sqrt(rightSd);\n\n        out.collect(cov / (leftSd * rightSd));\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Hourly Trading Volume with Flink SQL\nDESCRIPTION: SQL query to calculate hourly trading volume from user behavior data. It uses a tumbling window function to group data by hour and counts 'buy' events.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-28-flink-sql-demo-building-e2e-streaming-application.md#2025-04-08_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO buy_cnt_per_hour\nSELECT HOUR(TUMBLE_START(ts, INTERVAL '1' HOUR)), COUNT(*)\nFROM user_behavior\nWHERE behavior = 'buy'\nGROUP BY TUMBLE(ts, INTERVAL '1' HOUR);\n```\n\n----------------------------------------\n\nTITLE: Submitting Storm Topology to Flink via Command Line\nDESCRIPTION: This command shows how to submit a packaged Storm application to Flink using the Flink command-line client. This allows easy deployment of Storm topologies to a Flink cluster.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-12-11-storm-compatibility.md#2025-04-08_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nbin/flink run StormWordCount.jar\n```\n\n----------------------------------------\n\nTITLE: Release Statistics Markdown Table\nDESCRIPTION: HTML table showing FLIP proposals and details about new Python DataStream API and Temporal Table DDL features\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-07-29-community-update.md#2025-04-08_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"table table-bordered\">\n  <thead>\n    <tr>\n      <th>FLIP</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><a href=\"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=158866298\">FLIP-130</a></td>\n        <td><ul>\n        <li><b>Support Python DataStream API</b></li>\n        <p>Python support in Flink has so far been bounded to the Table API/SQL. These APIs are high-level and convenient, but have some limitations for more complex stream processing use cases. To expand the usability of PyFlink to a broader set of use cases, FLIP-130 proposes to support it also in the DataStream API, starting with stateless operations.</p>\n      </ul>\n      </td>\n    </tr>\n    <tr>\n      <td><a href=\"https://cwiki.apache.org/confluence/display/FLINK/FLIP-132+Temporal+Table+DDL\">FLIP-132</a></td>\n        <td><ul>\n        <li><b>Temporal Table DDL</b></li>\n        <p>Flink SQL users can't currently create temporal tables using SQL DDL, which forces them to change context frequently for use cases that require them. FLIP-132 proposes to extend the DDL syntax to support temporal tables, which in turn will allow to also bring temporal joins with changelog sources to Flink SQL.</p>\n      </ul>\n      </td>\n    </tr>\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Starting Kubernetes Proxy for Flink Dashboard\nDESCRIPTION: Command to start the Kubernetes proxy server, which allows accessing the Flink dashboard from outside the cluster.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-11-06-flink-kubernetes-kudo.md#2025-04-08_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl proxy\n```\n\n----------------------------------------\n\nTITLE: Creating Table in SQL DDL\nDESCRIPTION: Example of creating a table using SQL DDL statements, which is now supported in Flink 1.9. This allows registering tables that can be persisted in catalogs.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-08-22-release-1.9.0.md#2025-04-08_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE\n```\n\n----------------------------------------\n\nTITLE: Implementing SerialVersionUID for Java Serializable Classes\nDESCRIPTION: Example showing the proper implementation of SerialVersionUID for classes that must be serializable. This is required for classes transported via RPC in Flink.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content.zh/how-to-contribute/code-style-and-quality-java.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nprivate static final long serialVersionUID = 1L;\n```\n\n----------------------------------------\n\nTITLE: Creating Code Blocks with Syntax Highlighting in Flink Documentation\nDESCRIPTION: This example shows how to create a code block with syntax highlighting for Java in Flink documentation using Liquid and Markdown.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_10\n\nLANGUAGE: liquid\nCODE:\n```\n```java \n   // Java Code\n```\n```\n\n----------------------------------------\n\nTITLE: Memory Segment Class References\nDESCRIPTION: Reference implementations for memory segments in Apache Flink including HeapMemorySegment, HybridMemorySegment, PureHeapSegment, and PureHybridSegment with both heap and off-heap variants.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2015-09-16-off-heap-memory.md#2025-04-08_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHeapMemorySegment\nHybridMemorySegment\nPureHeapSegment\nPureHybridSegment\n```\n\n----------------------------------------\n\nTITLE: Inserting Images in Flink Documentation\nDESCRIPTION: This Liquid snippet demonstrates how to insert and format images in Flink documentation pages, including specifying source, alt text, and width.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/how-to-contribute/documentation-style-guide.md#2025-04-08_snippet_9\n\nLANGUAGE: liquid\nCODE:\n```\n{{< img src=\"/fig/image_name.png\" alt=\"Picture Text\" width=\"200px\" >}}\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Stateful Functions Documentation\nDESCRIPTION: YAML front matter that configures how the Stateful Functions documentation appears on the Flink website. It sets the page weight, title with version variable, and external documentation link.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/documentation/flink-stateful-functions-stable.md#2025-04-08_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nweight: 10\ntitle: Stateful Functions $StateFunStableShortVersion (stable)\nbookHref: \"https://nightlies.apache.org/flink/flink-statefun-docs-stable/\"\n---\n```\n\n----------------------------------------\n\nTITLE: Restoring a Schema-Compatible Serializer in Flink\nDESCRIPTION: Implementation of the restoreSerializer method that creates a new Avro serializer with either the previous schema as the writer schema (for migration) or the same schema for both roles.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-29-state-unlocked-interacting-with-state-in-apache-flink.md#2025-04-08_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n  @Override\n  public TypeSerializer<T> restoreSerializer() {\n    if (previousSchema != null) {\n      return new AvroSerializer<>(runtimeType, runtimeSchema, previousSchema);\n    } else {\n      return new AvroSerializer<>(runtimeType, runtimeSchema, runtimeSchema);\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating Pulsar as Streaming Source and Sink in Flink DataStream\nDESCRIPTION: This code snippet demonstrates how to use Apache Pulsar as a streaming source and streaming sink in a Flink DataStream application. It includes setting up a Pulsar consumer, performing a word count computation, and emitting results using a Pulsar producer.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2019-05-03-pulsar-flink.md#2025-04-08_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// create and configure Pulsar consumer\nPulsarSourceBuilder<String>builder = PulsarSourceBuilder\n   .builder(new SimpleStringSchema())\n   .serviceUrl(serviceUrl)\n   .topic(inputTopic)\n   .subscriptionName(subscription);\nSourceFunction<String> src = builder.build();\n// ingest DataStream with Pulsar consumer\nDataStream<String> words = env.addSource(src);\n\n// perform computation on DataStream (here a simple WordCount)\nDataStream<WordWithCount> wc = words\n   .flatMap((FlatMapFunction<String, WordWithCount>) (word, collector) -> {\n       collector.collect(new WordWithCount(word, 1));\n   })\n   .returns(WordWithCount.class)\n   .keyBy(\"word\")\n   .timeWindow(Time.seconds(5))\n   .reduce((ReduceFunction<WordWithCount>) (c1, c2) ->\n       new WordWithCount(c1.word, c1.count + c2.count));\n\n// emit result via Pulsar producer\nwc.addSink(new FlinkPulsarProducer<>(\n   serviceUrl,\n   outputTopic,\n   new AuthenticationDisabled(),\n   wordWithCount -> wordWithCount.toString().getBytes(UTF_8),\n   wordWithCount -> wordWithCount.word)\n);\n```\n\n----------------------------------------\n\nTITLE: Basic Stream Keying in Flink DataStream API\nDESCRIPTION: Shows the basic usage of keyBy operation in Flink's DataStream API for grouping transactions by account ID.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-01-15-demo-fraud-detection.md#2025-04-08_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nDataStream<Transaction> input = // [...]\nDataStream<...> windowed = input\n  .keyBy(Transaction::getAccountId)\n  .window(/*window specification*/);\n```\n\n----------------------------------------\n\nTITLE: Implementing Scala UDF for String Uppercase Conversion in Flink\nDESCRIPTION: This code snippet shows how to create a Scala UDF named 'ScalaUpper' that converts a string to uppercase, and registers it with the Flink batch table environment for use in SQL queries.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2020-06-23-flink-on-zeppelin-part2.md#2025-04-08_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nclass ScalaUpper extends ScalarFunction {\ndef eval(str: String) = str.toUpperCase\n}\nbtenv.registerFunction(\"scala_upper\", new ScalaUpper())\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Function with JavaScript SDK\nDESCRIPTION: Example of creating a stateful function using the new JavaScript SDK for NodeJS. The code demonstrates setting up a HTTP server with a greeter function that maintains state of how many times it has seen each name.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2022-01-31-release-statefun-3.2.0.md#2025-04-08_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst http = require(\"http\");\nconst {messageBuilder, StateFun, Context} = require(\"apache-flink-statefun\");\n\nlet statefun = new StateFun();\nstatefun.bind({\n    typename: \"com.example.fns/greeter\",\n    fn(context, message) {\n        const name = message.asString();\n        let seen = context.storage.seen || 0;\n        seen = seen + 1;\n        context.storage.seen = seen;\n\n        context.send(\n            messageBuilder({typename: 'com.example.fns/inbox',\n                            id: name,\n                            value: `\"Hello ${name} for the ${seen}th time!\"`})\n        );\n    },\n    specs: [{\n        name: \"seen\",\n        type: StateFun.intType(),\n    }]\n});\n\nhttp.createServer(statefun.handler()).listen(8000);\n```\n\n----------------------------------------\n\nTITLE: Modified Method in AbstractUdfStreamOperator\nDESCRIPTION: The setup method in AbstractUdfStreamOperator has had its access modifier changed from public to protected. This restricts access to this method to subclasses only.\nSOURCE: https://github.com/apache/flink-web/blob/asf-site/docs/content/posts/2024-10-23-release-2.0-preview.md#2025-04-08_snippet_27\n\nLANGUAGE: java\nCODE:\n```\nPROTECTED (<- PUBLIC) void setup(org.apache.flink.streaming.runtime.tasks.StreamTask<?,?><?,?>, org.apache.flink.streaming.api.graph.StreamConfig, org.apache.flink.streaming.api.operators.Output<org.apache.flink.streaming.runtime.streamrecord.StreamRecord<OUT>><org.apache.flink.streaming.runtime.streamrecord.StreamRecord<OUT>>)\n```"
  }
]
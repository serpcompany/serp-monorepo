[
  {
    "owner": "microsoft",
    "repo": "presidio",
    "content": "TITLE: Importing Presidio Analyzer Modules\nDESCRIPTION: Imports the necessary modules from Presidio Analyzer to perform PII analysis and customization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport pprint\n\nfrom presidio_analyzer import (\n    AnalyzerEngine,\n    PatternRecognizer,\n    EntityRecognizer,\n    Pattern,\n    RecognizerResult,\n)\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n```\n\n----------------------------------------\n\nTITLE: Referencing RecognizerRegistry - Python\nDESCRIPTION: Provides the RecognizerRegistry class from presidio_analyzer.recognizer_registry for managing and discovering available entity recognizers. Essential for dynamic loading, registration, and query of recognizer classes across the engine.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.recognizer_registry.RecognizerRegistry\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: De-identifying Structured Data with Presidio-structured in Python\nDESCRIPTION: This snippet demonstrates how to use Presidio-structured to detect and anonymize PII in a Pandas DataFrame. It initializes the engine, creates a sample DataFrame, generates a tabular analysis, defines anonymization operators, and applies the anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_structured.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom presidio_structured import StructuredEngine, PandasAnalysisBuilder\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom faker import Faker # optionally using faker as an example\n\n# Initialize the engine with a Pandas data processor (default)\npandas_engine = StructuredEngine()\n\n# Create a sample DataFrame\nsample_df = pd.DataFrame({'name': ['John Doe', 'Jane Smith'], 'email': ['john.doe@example.com', 'jane.smith@example.com']})\n\n# Generate a tabular analysis which detects the PII entities in the DataFrame.\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df)\n\n# Define anonymization operators\nfake = Faker()\noperators = {\n    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"REDACTED\"}),\n    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})\n}\n\n# Anonymize DataFrame\nanonymized_df = pandas_engine.anonymize(sample_df, tabular_analysis, operators=operators)\nprint(anonymized_df)\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Analyzer and Anonymizer via Pip (Shell)\nDESCRIPTION: These shell commands use pip to install the core Presidio components (`presidio_analyzer` and `presidio_anonymizer`) and download the large English language model (`en_core_web_lg`) for spaCy, which is often used by Presidio for Named Entity Recognition (NER). Requires pip and Python to be installed.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/00_getting_started.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio_analyzer\npip install presidio_anonymizer\npython -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using AnonymizerEngine - Presidio (Python)\nDESCRIPTION: This Python snippet demonstrates how to initialize Presidio's AnonymizerEngine, define recognized PII entities with RecognizerResult, and anonymize a sample text while specifying custom operator configurations. Dependencies include presidio-anonymizer and its entities module. The anonymize function takes a string, a list of RecognizerResult objects specifying where PII is located, and a dictionary mapping entity types to OperatorConfig for fine-grained anonymization (e.g., replace with a specific string). The function returns and prints the anonymized result. All parameters must be correctly constructed, and recognized spans must not overlap for predictable results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-anonymizer/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n\n# Initialize the engine with logger.\nengine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text, \n# analyzer results (potentially coming from presidio-analyzer) and\n# Operators to get the anonymization output:\nresult = engine.anonymize(\n    text=\"My name is Bond, James Bond\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n        RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n    ],\n    operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})},\n)\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Anonymizing CSV Data with Presidio in Python\nDESCRIPTION: Anonymizes a pandas DataFrame using the StructuredEngine and PandasDataProcessor from presidio_structured. By default, sensitive data is replaced with None.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# anonymized data defaults to be replaced with None, unless operators is specified\n\npandas_engine = StructuredEngine(data_processor=PandasDataProcessor())\ndf_to_be_anonymized = sample_df.copy() # in-place anonymization\nanonymized_df = pandas_engine.anonymize(df_to_be_anonymized, tabular_analysis, operators=None) # explicit None for clarity\nanonymized_df\n```\n\n----------------------------------------\n\nTITLE: Loading Custom RecognizerRegistry from YAML - Presidio Analyzer - Python\nDESCRIPTION: Demonstrates how to create a custom RecognizerRegistry in Presidio Analyzer by loading recognizer definitions from a YAML configuration file. Requires the Presidio Analyzer and the associated YAML file with recognizer definitions. Takes a file path to the YAML configuration, initializes a RecognizerRegistryProvider, creates a registry, and then uses the AnalyzerEngine to process example text. The output is a list of analysis results. Limitations include requiring correct YAML schema and properly configured file paths.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistryProvider\n\nrecognizer_registry_conf_file = \"./analyzer/recognizers-config.yml\"\n\nprovider = RecognizerRegistryProvider(\n                conf_file=recognizer_registry_conf_file\n            )\nregistry = provider.create_recognizer_registry()\nanalyzer = AnalyzerEngine(registry=registry)\n\nresults = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Using Presidio REST APIs\nDESCRIPTION: cURL commands demonstrating how to use the Presidio analyzer and anonymizer REST APIs. The example shows analyzing text to detect phone numbers and then anonymizing the detected PII with a custom replacement value.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_text.md#2025-04-23_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncurl -X POST http://localhost:5002/analyze \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"text\": \"My phone number is 555-123-4567.\",\n  \"language\": \"en\"\n}'\n\n\ncurl -X POST http://localhost:5001/anonymize -H \"Content-Type: application/json\"  -d '\n    {\n        \"text\": \"My phone number is 555-123-4567\",\n        \"anonymizers\": {\n            \"PHONE_NUMBER\": {\n            \"type\": \"replace\",\n            \"new_value\": \"--Redacted phone number--\"\n            }\n        },\n        \"analyzer_results\": [\n        {\n            \"start\": 19,\n            \"end\": 31,\n            \"score\": 0.95,\n            \"entity_type\": \"PHONE_NUMBER\"\n        }\n    ]}'\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio and Dependencies in Python\nDESCRIPTION: This snippet installs the Presidio Analyzer and Anonymizer libraries, as well as the required spaCy language model. It uses pip to install the packages and the spaCy CLI to download the language model.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/presidio_notebook.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Deploying Azure Resources for Presidio Anonymization with Azure CLI\nDESCRIPTION: Azure CLI commands to create a resource group and deploy the ARM template that provisions the required Azure services (App Service, Storage Account, Key Vault) for the Presidio anonymization solution.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/data-factory/presidio-data-factory-template-gallery-http.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nRESOURCE_GROUP=[Name of resource group]\nLOCATION=[location of resources]\n\naz group create --name $RESOURCE_GROUP --location $LOCATION\naz deployment group create -g $RESOURCE_GROUP --template-file ./arm-templates/azure-deploy-adf-template-gallery-http.json\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text for PII using Presidio Analyzer (Python)\nDESCRIPTION: This Python snippet demonstrates the basic usage of Presidio Analyzer. It imports the `AnalyzerEngine`, initializes it, and then calls the `analyze` method with sample text and the language ('en'). The results, containing identified PII entities, are then printed. Requires the `presidio_analyzer` package to be installed.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/00_getting_started.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\n\ntext = \"His name is Mr. Jones and his phone number is 212-555-5555\"\n\nanalyzer = AnalyzerEngine()\nanalyzer_results = analyzer.analyze(text=text, language=\"en\")\n\nprint(analyzer_results)\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio Modules in Python\nDESCRIPTION: This snippet imports the necessary modules from Presidio Analyzer and Anonymizer, as well as additional utility modules for JSON handling and pretty printing. It sets up the environment for PII detection and anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/presidio_notebook.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, PatternRecognizer\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\nimport json\nfrom pprint import pprint\n```\n\n----------------------------------------\n\nTITLE: Analyzing English Text with Custom AnalyzerEngine\nDESCRIPTION: This snippet demonstrates the use of the custom-configured AnalyzerEngine to analyze an English text sample.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/no_code_config.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nen_text = \"Hi, my name is David Johnson and I'm originally from Liverpool. My credit card number is 4095260993934932\"\nanalyzer_engine.analyze(en_text, language=\"en\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Presidio PatternRecognizer with a Deny-List in Python\nDESCRIPTION: This code imports `PatternRecognizer` from `presidio_analyzer` and instantiates it. It configures the recognizer to identify entities of type 'TITLE' using the previously defined `titles_list` as its deny-list. This requires the `presidio-analyzer` library and the `titles_list` variable to be defined.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/01_deny_list.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom presidio_analyzer import PatternRecognizer\n\ntitles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=titles_list)\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Structured via Pip (Shell)\nDESCRIPTION: This snippet provides the command-line instruction to install the `presidio-structured` Python package using `pip`, the standard package installer for Python. Running this command downloads and installs the package and its dependencies.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-structured/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio-structured\n```\n\n----------------------------------------\n\nTITLE: Enhancing ZIP Code Detection Confidence Using Context Words - Presidio Python\nDESCRIPTION: This snippet shows how to define a PatternRecognizer that includes context words (\"zip\", \"zipcode\") to boost detection confidence if these words appear around candidate entities. The recognizer is registered and can provide improved scores when context is present, compared to pattern-only detection. Presidio's AnalyzerEngine will leverage context words automatically if present in the recognizer's definition. Requires Presidio Analyzer, RecognizerRegistry, and PatternRecognizer.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Define the recognizer with the defined pattern and context words\\nzipcode_recognizer = PatternRecognizer(\\n    supported_entity=\\\"US_ZIP_CODE\\\",\\n    patterns=[zipcode_pattern],\\n    context=[\\\"zip\\\", \\\"zipcode\\\"],\\n)\n```\n\n----------------------------------------\n\nTITLE: Text Anonymization Function Implementation in Python\nDESCRIPTION: Implements text anonymization using Presidio Analyzer and Anonymizer engines. Takes session ID, text, language and entity mappings as input and returns anonymized text with updated entity mappings.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/index.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -> Tuple[str, List[OperatorResult], dict] :\n        \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Anonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            results = self.analyzer.analyze(text=text, language=language)\n            logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")\n\n            anonymizer_start_time = timer()\n            anonymizer_entity_mapping = entity_mappings.copy() if entity_mappings is not None else dict()\n            anonymized_result = self.anonymizer.anonymize(\n                text=text,\n                analyzer_results=results,\n                operators={\n                    \"DEFAULT\": OperatorConfig(\n                        \"entity_counter\", {\"entity_mapping\": anonymizer_entity_mapping}\n                    )\n                },\n            )\n            logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n\n            return anonymized_result.text, anonymizer_entity_mapping\n        except Exception as e:\n            logger.exception(f\"Error in anonymize_text for session_id {session_id}\")\n            raise\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Deanonymizer to Restore Original Text\nDESCRIPTION: This snippet demonstrates how to use the custom deanonymizer with the DeanonymizeEngine to restore the original PII in the anonymized text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonymization.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndeanonymizer_engine = DeanonymizeEngine()\ndeanonymizer_engine.add_deanonymizer(InstanceCounterDeanonymizer)\n\ndeanonymized = deanonymizer_engine.deanonymize(\n    anonymized_result.text, \n    anonymized_result.items, \n    {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\", \n                               params={\"entity_mapping\": entity_mapping})}\n)\nprint(\"anonymized text:\")\npprint(anonymized_result.text)\nprint(\"de-anonymized text:\")\npprint(deanonymized.text)\n```\n\n----------------------------------------\n\nTITLE: Performing Default Anonymization with Presidio AnonymizerEngine in Python\nDESCRIPTION: This snippet demonstrates the basic usage of `AnonymizerEngine` from `presidio-anonymizer`. It initializes the engine and calls the `anonymize` method, providing the original text and a list of `RecognizerResult` objects (simulating output from presidio-analyzer). By default, without specific operators defined, it replaces identified PII entities (like 'PERSON') with their entity type names (e.g., '<PERSON>').\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/10_simple_anonymization.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult\n\n# Analyzer output\nanalyzer_results = [\n    RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n    RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n]\n\n# Initialize the engine:\nengine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer) and\n# Operators to get the anonymization output:\nresult = engine.anonymize(\n    text=\"My name is Bond, James Bond\", analyzer_results=analyzer_results\n)\n\nprint(\"De-identified text\")\nprint(result.text)\n```\n\n----------------------------------------\n\nTITLE: Configuring Stanza NLP Engine\nDESCRIPTION: Sets up a Stanza NLP engine with custom entity mappings and executes the analyzer.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define which model to use\nmodel_config = [{\"lang_code\": \"en\", \"model_name\": \"en\"}]\n\n# Define which entities the model returns and how they map to Presidio's\nentity_mapping = dict(\n    PER=\"PERSON\",\n    LOC= \"LOCATION\",\n    GPE=\"LOCATION\",\n    ORG=\"ORGANIZATION\"\n)\n\nner_model_configuration = NerModelConfiguration(model_to_presidio_entity_mapping=entity_mapping)\n\n# Create the Stanza NLP Engine based on this configuration\nstanza_nlp_engine = StanzaNlpEngine(models= model_config, ner_model_configuration=ner_model_configuration)\n\n# Run it as part of Presidio's AnalyzerEngine\ncall_analyzer_and_print_results(stanza_nlp_engine)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text and Tracing Decisions with Presidio Analyzer in Python\nDESCRIPTION: This Python snippet initializes the Presidio `AnalyzerEngine` and uses it to analyze a sample text containing a zip code. It specifically sets the `return_decision_process` parameter to `True` to obtain detailed insights into how the PII entity was identified. The resulting decision process information, stored in the `analysis_explanation` attribute of the first result, is then pretty-printed for examination. This requires the `presidio_analyzer` and `pprint` libraries.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/07_decision_process.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nimport pprint\n\nanalyzer = AnalyzerEngine()\n\nresults = analyzer.analyze(\n    text=\"My zip code is 90210\", language=\"en\", return_decision_process=True\n)\n\ndecision_process = results[0].analysis_explanation\n\npp = pprint.PrettyPrinter()\nprint(\"Decision process output:\\n\")\npp.pprint(decision_process.__dict__)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom PII Recognizers with Presidio in Python\nDESCRIPTION: This snippet demonstrates how to create custom PII recognizers using Presidio's PatternRecognizer. It creates recognizers for titles and pronouns, adds them to the analyzer, and then uses them to analyze the sample text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/presidio_notebook.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntitles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n                                      deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n\npronoun_recognizer = PatternRecognizer(supported_entity=\"PRONOUN\",\n                                       deny_list=[\"he\", \"He\", \"his\", \"His\", \"she\", \"She\", \"hers\", \"Hers\"])\n\nanalyzer.registry.add_recognizer(titles_recognizer)\nanalyzer.registry.add_recognizer(pronoun_recognizer)\n\nanalyzer_results = analyzer.analyze(text=text_to_anonymize,\n                            entities=[\"TITLE\", \"PRONOUN\"],\n                            language=\"en\")\nprint(analyzer_results)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Presidio AnalyzerEngine in Python\nDESCRIPTION: This Python snippet demonstrates the basic usage of the Presidio AnalyzerEngine. It imports the engine, initializes it (which loads the default spaCy NLP model and PII recognizers), and then calls the `analyze` method to find specified PII entities (in this case, 'PHONE_NUMBER') within a given text string in English. The results of the analysis are then printed.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=\"My phone number is 212-555-5555\",\n                           entities=[\"PHONE_NUMBER\"],\n                           language='en')\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Decrypting Encrypted Entities Using Presidio DeanonymizeEngine in Python\nDESCRIPTION: This snippet shows how to use the DeanonymizeEngine to decrypt previously encrypted text and entities. Dependencies: Requires anonymized text, entities list, and crypto key from prior steps. Input: 'anonymized_text', 'anonymized_entities', and an operator configuration for 'decrypt'. Output: 'deanonymized_result', which contains the restored original text. This is essential for reversible entity anonymization workflows.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/encrypt_decrypt.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the engine:\\nengine = DeanonymizeEngine()\\n\\n# Invoke the deanonymize function with the text, anonymizer results\\n# and a 'decrypt' operator to get the original text as output.\\ndeanonymized_result = engine.deanonymize(\\n    text=anonymized_text,\\n    entities=anonymized_entities,\\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": crypto_key})},\\n)\\n\\ndeanonymized_result\n```\n\n----------------------------------------\n\nTITLE: Anonymizing Text Columns in Spark DataFrames with Presidio\nDESCRIPTION: Core implementation for anonymizing PII in a Spark DataFrame column using Presidio's AnalyzerEngine and AnonymizerEngine with pandas UDFs. This pattern broadcasts the Presidio engines to cluster nodes and applies the anonymization transformation to a specified text column.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/spark/index.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nanonymized_column = \"value\" # name of column to anonymize\nanalyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n\n# broadcast the engines to the cluster nodes\nbroadcasted_analyzer = sc.broadcast(analyzer)\nbroadcasted_anonymizer = sc.broadcast(anonymizer)\n\n# define a pandas UDF function and a series function over it.\ndef anonymize_text(text: str) -> str:\n    analyzer = broadcasted_analyzer.value\n    anonymizer = broadcasted_anonymizer.value\n    analyzer_results = analyzer.analyze(text=text, language=\"en\")\n    anonymized_results = anonymizer.anonymize(\n        text=text,\n        analyzer_results=analyzer_results,\n        operators={\n            \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"})\n        },\n    )\n    return anonymized_results.text\n\n\ndef anonymize_series(s: pd.Series) -> pd.Series:\n    return s.apply(anonymize_text)\n\n\n# define a the function as pandas UDF\nanonymize = pandas_udf(anonymize_series, returnType=StringType())\n\n# apply the udf\nanonymized_df = input_df.withColumn(\n    anonymized_column, anonymize(col(anonymized_column))\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Text for PII Analysis in Python\nDESCRIPTION: This snippet defines a sample text containing PII (Personal Identifiable Information) that will be used for demonstration purposes in the subsequent analysis and anonymization steps.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/presidio_notebook.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntext_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\"\n```\n\n----------------------------------------\n\nTITLE: Performing Ad-Hoc Anonymization with Presidio in Python\nDESCRIPTION: This snippet demonstrates how to use ad-hoc recognizers in Presidio to anonymize specific values known only in the context of each record in a dataset.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Go over dataset\nfor person in dataset:\n    \n    # Get the different known values\n    name = person['name']\n    special_val = person['special_value']\n    \n    # Get the free text to anonymize\n    free_text = person['free_text']\n    \n    # Create ad-hoc recognizers\n    ad_hoc_name_recognizer = PatternRecognizer(supported_entity=\"name\", deny_list = [name])\n    ad_hoc_id_recognizer = PatternRecognizer(supported_entity=\"special_value\", deny_list = [special_val])\n    \n    # Run the analyze method with ad_hoc_recognizers:\n    analyzer_results = analyzer.analyze(text=free_text, \n                                        language=\"en\", \n                                        ad_hoc_recognizers=[ad_hoc_name_recognizer, ad_hoc_id_recognizer])\n    \n    # Anonymize results\n    anonymized = anonymizer.anonymize(text=free_text, analyzer_results=analyzer_results)\n    print(anonymized.text)\n    \n    # Store output in original dataset\n    person[\"anonymized_free_text\"] = anonymized.text\n```\n\n----------------------------------------\n\nTITLE: Redacting PII in Standard Images with Presidio Image Redactor - Python\nDESCRIPTION: This Python snippet demonstrates loading an image using the Pillow (PIL) library, initializing the ImageRedactorEngine from the presidio_image_redactor module, and invoking the redact method to conceal detected PII using a specified color fill (RGB tuple for pink in this example). The output is a redacted image saved to a new file. Required dependencies are Pillow, presidio-image-redactor, and its internal requirements (including spaCy and Tesseract OCR as per installation instructions). The input is a standard image file, and the output is a redacted PNG file.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nfrom presidio_image_redactor import ImageRedactorEngine\n\n# Get the image to redact using PIL lib (pillow)\nimage = Image.open(\"presidio-image-redactor/tests/integration/resources/ocr_test.png\")\n\n# Initialize the engine\nengine = ImageRedactorEngine()\n\n# Redact the image with pink color\nredacted_image = engine.redact(image, (255, 192, 203))\n\n# save the redacted image \nredacted_image.save(\"new_image.png\")\n# uncomment to open the image for viewing\n# redacted_image.show()\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Recognizers from File in Presidio Analyzer (Python)\nDESCRIPTION: This code snippet demonstrates how to use RecognizerRegistryProvider to load custom recognizers from a YAML file and create an AnalyzerEngine instance with the custom registry.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/recognizer_registry_provider.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistryProvider\n\nrecognizer_registry_conf_file = \"./analyzer/recognizers-config.yml\"\n\nprovider = RecognizerRegistryProvider(\n                conf_file=recognizer_registry_conf_file\n            )\nregistry = provider.create_recognizer_registry()\nanalyzer = AnalyzerEngine(registry=registry)\n\nresults = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Referencing AnalyzerEngine - Python\nDESCRIPTION: Imports the main AnalyzerEngine object from the presidio_analyzer package, forming the foundation for text analysis API usage. This class supports text analysis and PII recognition workflows and is a mandatory dependency for instantiating analyzers in applications. The typical inputs are text and analyzer configuration, and it outputs analysis results with detected entities.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.AnalyzerEngine\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Anonymization Operators for Presidio in Python\nDESCRIPTION: This snippet shows how to define a dictionary of custom anonymization operators using `OperatorConfig` from `presidio_anonymizer.entities`. It specifies different actions for different entity types: 'PHONE_NUMBER' entities will be masked (last 12 chars replaced with '*'), 'TITLE' entities will be redacted (removed completely), and a 'DEFAULT' operator replaces any other identified entity with the string '<ANONYMIZED>'.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/10_simple_anonymization.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define anonymization operators\noperators = {\n    \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"}),\n    \"PHONE_NUMBER\": OperatorConfig(\n        \"mask\",\n        {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 12,\n            \"from_end\": True,\n        },\n    ),\n    \"TITLE\": OperatorConfig(\"redact\", {}),\n}\n```\n\n----------------------------------------\n\nTITLE: Anonymizing PII in Text Data with Presidio\nDESCRIPTION: Implements a user-defined function that detects and anonymizes PII in text data using the Presidio Analyzer and Anonymizer. The function replaces identified PII with empty strings while preserving the structure of the text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\ndef anonymize_text(text: str) -> str:\n    \"\"\"\n    Detect PII in the given text using the large model and replace it with an empty string.\n    \"\"\"\n    if text is None:\n        return None\n\n    analyzer = broadcasted_analyzer.value\n    anonymizer = broadcasted_anonymizer.value\n    analyze_results = analyzer.analyze(text=text, language=\"en\")\n    anonymized_result = anonymizer.anonymize(\n        text=text,\n        analyzer_results=analyze_results,\n        operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"\"})}\n)\n    return anonymized_result.text\n\n# Registering as a regular PySpark UDF\nanon_udf = udf(anonymize_text, StringType())\n\ndf_final = df_with_pii_summary.withColumn(\"anon_user_query\", anon_udf(col(\"user_query\")))\n\ndisplay(df_final)\n# df_final.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Presidio Engines Configuration in Python\nDESCRIPTION: Sets up the NLP engine provider and initializes analyzer, anonymizer and deanonymizer engines with support for multiple languages. Configures custom anonymizer instances for handling PII data.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/index.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n        configuration = {\n            \"nlp_engine_name\": \"spacy\",\n            \"models\": [\n                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n                {\"lang_code\": \"nl\", \"model_name\": \"nl_core_news_sm\"},\n                {\"lang_code\": \"es\", \"model_name\": \"es_core_news_sm\"},\n            ],\n        }\n        provider = NlpEngineProvider(nlp_configuration=configuration)\n        nlp_engine = provider.create_engine()\n\n        self.analyzer = AnalyzerEngine(\n            nlp_engine=nlp_engine,\n            supported_languages=[\"en\", \"nl\", \"es\"]\n        )\n        self.anonymizer = AnonymizerEngine()\n        self.anonymizer.add_anonymizer(InstanceCounterAnonymizer)\n        self.deanonymizer = DeanonymizeEngine()\n        self.deanonymizer.add_deanonymizer(InstanceCounterDeanonymizer)\n```\n\n----------------------------------------\n\nTITLE: Using Python to Get Decision Process in Presidio-analyzer Results\nDESCRIPTION: Python code example that sets up the AnalyzerEngine from the presidio_analyzer package and calls analyze with return_decision_process set to True. The sample analyzes text containing a phone number and prints the analysis explanation from the first result.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/decision_process.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default)\n# and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text='My phone number is 212-555-5555', \n                        entities=['PHONE_NUMBER'], \n                        language='en', \n                        return_decision_process=True)\n\n# Get the decision process results for the first result\nprint(results[0].analysis_explanation)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Entity Recognizer Class with Presidio in Python\nDESCRIPTION: This snippet defines the skeleton for a custom entity recognizer by subclassing Presidio's EntityRecognizer abstract class in Python. It includes required load and analyze methods; the former is a stub when no resources need preloading, and the latter is intended to contain logic for identifying specific PII in given text using spaCy results from NlpArtifacts. Dependencies: presidio_analyzer package and spaCy NLP artifacts. Expected input: text string and NlpArtifacts; output: a list of RecognizerResult objects.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/03_rule_based.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom presidio_analyzer import EntityRecognizer, RecognizerResult\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n\n\nclass MyRecognizer(EntityRecognizer):\n    def load(self) -> None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -> List[RecognizerResult]:\n        \"\"\"\n        Logic for detecting a specific PII\n        \"\"\"\n        pass\n\n```\n\n----------------------------------------\n\nTITLE: Instantiating AnalyzerEngine from Unified YAML File - Python\nDESCRIPTION: Creates an AnalyzerEngine instance using the unified YAML configuration file from disk, leveraging `AnalyzerEngineProvider`. This enables the engine to use all custom and multilingual settings specified earlier. Requires the Presidio engine provider and correct YAML schema.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/08_no_code.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nanalyzer_engine = AnalyzerEngineProvider(analyzer_engine_conf_file=temp_file_path).create_engine()\n\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text with Context-Enhanced ZIP Code Recognizer - Presidio Python\nDESCRIPTION: This snippet applies the previously registered context-enhanced ZIP code recognizer to sample text using the AnalyzerEngine. It performs entity detection and prints the analyzer's results, demonstrating the influence of context words on detection confidence. Dependencies include the AnalyzerEngine, RecognizerRegistry, and required context configuration. Outputs detection results and highlights the adjusted confidence scores.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Test\\nresults = analyzer.analyze(text=\\\"My zip code is 90210\\\", language=\\\"en\\\")\\nprint(\\\"Result:\\\")\\nprint_analyzer_results(results, text=text)\n```\n\n----------------------------------------\n\nTITLE: Referencing EntityRecognizer - Python\nDESCRIPTION: Provides the EntityRecognizer class from presidio_analyzer.entity_recognizer for implementing custom entity recognition logic. Entities, types, and recognition rules can be defined and extended via this base class, and it forms the scaffolding for custom recognizers used within the engine.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.entity_recognizer.EntityRecognizer\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: De-identifying Text using Presidio Analyzer and Anonymizer in Python\nDESCRIPTION: Initializes Presidio's `AnalyzerEngine` and `AnonymizerEngine`. It analyzes a sample string (`sample`) containing various PII types using `analyzer.analyze()` for English. The results are passed to `anonymizer.anonymize()` to replace detected PII with default placeholders (e.g., `<PERSON>`, `<CREDIT_CARD_NUMBER>`). The resulting anonymized text is then printed.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\nanalyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n\nsample = \"\"\"\nHello, my name is David Johnson and I live in Maine.\nMy credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.\n\nOn September 18 I visited microsoft.com and sent an email to test@presidio.site,  from the IP 192.168.0.1.\n\nMy passport: 191280342 and my phone number: (212) 555-1234.\n\nThis is a valid International Bank Account Number: IL150120690000003111111 . Can you please check the status on bank account 954567876544?\n\nKate's social security number is 078-05-1126.  Her driver license? it is 1234567A.\n\"\"\"\n\nresults = analyzer.analyze(sample, language=\"en\")\nanonymized = anonymizer.anonymize(text=sample, analyzer_results=results)\nanonymized_text = anonymized.text\nprint(anonymized_text)\n```\n\n----------------------------------------\n\nTITLE: Anonymizing Simple JSON Data with Presidio in Python\nDESCRIPTION: Uses StructuredEngine with JsonDataProcessor to anonymize simple JSON data based on the generated analysis and custom operators.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# anonymizing simple data\njson_engine = StructuredEngine(data_processor=JsonDataProcessor())\nanonymized_json = json_engine.anonymize(sample_json, json_analysis, operators=operators)\nanonymized_json\n```\n\n----------------------------------------\n\nTITLE: Registering and Using a Custom Recognizer in Presidio AnalyzerEngine (Python)\nDESCRIPTION: This code demonstrates how to add the custom NumbersRecognizer to the Presidio AnalyzerEngine, analyze sample text, and print recognized entities. It assumes new_numbers_recognizer is defined and spaCy/analyzer dependencies are met. The analyzer detects both numeric and alphabetic numbers and supports multi-entity recognition. Inputs: 'text3' string to analyze; outputs: printed RecognizerResults for each match. To run, ensure Presidio and all prerequisites are installed.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/03_rule_based.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\n\ntext3 = \"Roberto lives in Five 10 Broad st.\"\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(new_numbers_recognizer)\n\nnumbers_results2 = analyzer.analyze(text=text3, language=\"en\")\nprint(\"Results:\")\nprint(\"\\n\".join([str(res) for res in numbers_results2]))\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Presidio Analyzer and Anonymizer Engines\nDESCRIPTION: This code initializes the Presidio Analyzer and Anonymizer engines for batch processing of structured data.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nanalyzer = AnalyzerEngine()\nbatch_analyzer = BatchAnalyzerEngine(analyzer_engine=analyzer)\nbatch_anonymizer = BatchAnonymizerEngine()\n```\n\n----------------------------------------\n\nTITLE: Configuring Presidio Analyzer with TransformersRecognizer in Python\nDESCRIPTION: This Python code demonstrates setting up the Presidio `AnalyzerEngine` to use a `TransformersRecognizer` with the 'obi/deid_roberta_i2b2' model from Hugging Face. It involves initializing the `TransformersRecognizer`, loading the model configuration, adding it to the `RecognizerRegistry`, removing the default `SpacyRecognizer`, configuring a lightweight SpaCy model ('en_core_web_sm') via `NlpEngineProvider` for basic NLP tasks, initializing the `AnalyzerEngine`, and finally analyzing sample text to identify entities. Dependencies include `presidio_analyzer`, `spacy`, the specified Transformers model, and the SpaCy model.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/transformers_recognizer/index.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nimport spacy\n\nmodel_path = \"obi/deid_roberta_i2b2\"\nsupported_entities = BERT_DEID_CONFIGURATION.get(\n    \"PRESIDIO_SUPPORTED_ENTITIES\")\ntransformers_recognizer = TransformersRecognizer(model_path=model_path,\n                                                 supported_entities=supported_entities)\n\n# This would download a large (~500Mb) model on the first run\ntransformers_recognizer.load_transformer(**BERT_DEID_CONFIGURATION)\n\n# Add transformers model to the registry\nregistry = RecognizerRegistry()\nregistry.add_recognizer(transformers_recognizer)\nregistry.remove_recognizer(\"SpacyRecognizer\")\n\n# Use small spacy model, for faster inference.\nif not spacy.util.is_package(\"en_core_web_sm\"):\n    spacy.cli.download(\"en_core_web_sm\")\n\nnlp_configuration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n}\n\nnlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n\nanalyzer = AnalyzerEngine(registry=registry, nlp_engine=nlp_engine)\n\nsample = \"My name is John and I live in NY\"\nresults = analyzer.analyze(sample, language=\"en\",\n                           return_decision_process=True,\n                           )\nprint(\"Found the following entities:\")\nfor result in results:\n    print(result, '----', sample[result.start:result.end])\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom PatternRecognizer in Python\nDESCRIPTION: This snippet creates a custom PatternRecognizer using the deny-list of known names to identify a specific entity type.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create a PatternRecognizer for the deny list\ndeny_list_recognizer = PatternRecognizer(supported_entity=\"PRESIDENT_FIRST_NAME\", deny_list=known_names_list)\n```\n\n----------------------------------------\n\nTITLE: Excluding Words from Detection Using Allow List Parameter - Presidio Python\nDESCRIPTION: This snippet demonstrates the use of the allow_list parameter in analyze to specify words or phrases (e.g., website domains) that should not be detected as PII by Presidio. Only entities not on the allow_list will be recognized, while others are ignored. A detailed decision process trace is produced for transparency. Useful for tailoring detection to business needs where certain items are known-safe. Requires AnalyzerEngine and appropriate recognizer setup.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nresults = analyzer.analyze(\\n    text=text1,\\n    language=\\\"en\\\",\\n    allow_list=[\\\"bing.com\\\", \\\"google.com\\\"],\\n    return_decision_process=True,\\n)\\nprint_analyzer_results(results, text=text1)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Operator for PII Entity Extraction with Presidio in Python\nDESCRIPTION: This code uses a custom operator with an identity function to extract PII entities, handling overlaps automatically.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nanonymized_results = anonymizer.anonymize(\n        text=text_to_analyze,\n        analyzer_results=analyzer_results,            \n        operators={\"DEFAULT\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x})}        \n    )\n```\n\n----------------------------------------\n\nTITLE: Analyzing and Anonymizing PII with spaCy Model\nDESCRIPTION: Python code demonstrating how to use Presidio to detect and anonymize phone numbers in text using the default spaCy NLP engine. The code shows the complete workflow from initializing the analyzer to outputting anonymized text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_text.md#2025-04-23_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\ntext=\"My phone number is 212-555-5555\"\n\n# Set up the engine, loads the NLP module (spaCy model by default) \n# and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=text,\n                           entities=[\"PHONE_NUMBER\"],\n                           language='en')\nprint(results)\n\n# Analyzer results are passed to the AnonymizerEngine for anonymization\n\nanonymizer = AnonymizerEngine()\n\nanonymized_text = anonymizer.anonymize(text=text,analyzer_results=results)\n\nprint(anonymized_text)\n```\n\n----------------------------------------\n\nTITLE: Text De-anonymization Implementation\nDESCRIPTION: Implements text de-anonymization using Presidio engines. Restores original sensitive information from anonymized text using stored entity mappings.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -> str:\n        \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Deanonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            entities = self.get_entities(entity_mappings, text)\n\n            deanonymized = self.deanonymizer.deanonymize(\n                text=text,\n                entities=entities,\n                operators=\n                {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\", \n                                        params={\"entity_mapping\": entity_mappings})}\n            )\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n            \n            return deanonymized.text\n        except Exception as e:\n            logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")\n            raise\n\n    def get_entities(self, entity_mappings: dict, text: str) -> List[OperatorResult]:\n        \"\"\" Get the entities from the entity mappings \"\"\"\n\n        entities = []\n        for entity_type, entity_mapping in entity_mappings.items():\n            for entity_value, entity_id in entity_mapping.items():\n                start_index = 0\n                while True:\n                    start_index = text.find(entity_id, start_index)\n                    if start_index == -1:\n                        break\n                    end_index = start_index + len(entity_id)\n                    entities.append(OperatorResult(start_index, end_index, entity_type, entity_value, entity_id))\n                    start_index += len(entity_id)\n        return entities\n```\n\n----------------------------------------\n\nTITLE: Tracing the Decision Process in Presidio Analyzer - Python\nDESCRIPTION: This snippet invokes the analyze function with return_decision_process=True to extract detailed information about how an entity was detected, including which recognizer, pattern, context, and scoring details were involved. The explanation is printed using Python's pprint for improved readability. Useful for debugging, audit, and model transparency. Requires the analyzer to be initialized and context/decision process support in Presidio.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresults = analyzer.analyze(\\n    text=\\\"My zip code is 90210\\\", language=\\\"en\\\", return_decision_process=True\\n)\\ndecision_process = results[0].analysis_explanation\\n\\npp = pprint.PrettyPrinter()\\nprint(\\\"Decision process output:\\n\\\")\\npp.pprint(decision_process.__dict__)\n```\n\n----------------------------------------\n\nTITLE: Analyzing and Anonymizing Text with Presidio in Python\nDESCRIPTION: This snippet demonstrates how to use the Presidio Analyzer and Anonymizer to process a sample text, identify entities, and anonymize the content.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntext=\"George Washington was the first US president\"\n\nresults = analyzer.analyze(text=text, language=\"en\")\n\nprint(\"Identified entities:\")\nprint(results)\nprint(\"\")\nanonymized = anonymizer.anonymize(text=text, analyzer_results=results)\nprint(f\"Anonymized text:\\n{anonymized.text}\")\n```\n\n----------------------------------------\n\nTITLE: Enhancing Zip Code Recognition with Context Words using Presidio Analyzer in Python\nDESCRIPTION: Defines a PatternRecognizer for US zip codes with additional context words (e.g., 'zip', 'zipcode') to improve recognition when these words are present near the entity. The recognizer still uses the prior weak pattern but is now context-aware, designed to boost detection confidence. Requires presidio_analyzer. Key parameters are supported_entity, patterns, and context. The recognizer itself needs to be added to an AnalyzerEngine to be used.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/06_context.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import PatternRecognizer\n\n# Define the recognizer with the defined pattern and context words\nzipcode_recognizer_w_context = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\",\n    patterns=[zipcode_pattern],\n    context=[\"zip\", \"zipcode\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Anonymizing Pandas DataFrame with Presidio Structured (Python)\nDESCRIPTION: This Python code demonstrates the basic usage of the `presidio-structured` package to anonymize sensitive data within a Pandas DataFrame. It initializes a `StructuredEngine`, creates a sample DataFrame, uses `PandasAnalysisBuilder` to identify PII entities, defines anonymization rules using `OperatorConfig` from `presidio-anonymizer` (including redaction and custom replacement with Faker), and finally applies the anonymization. Key dependencies include `pandas`, `presidio_structured`, `presidio_anonymizer.entities`, and optionally `faker`. The output is the anonymized DataFrame printed to the console.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-structured/README.md#2025-04-23_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nimport pandas as pd\nfrom presidio_structured import StructuredEngine, PandasAnalysisBuilder\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom faker import Faker # optionally using faker as an example\n\n# Initialize the engine with a Pandas data processor (default)\npandas_engine = StructuredEngine()\n\n# Create a sample DataFrame\nsample_df = pd.DataFrame({'name': ['John Doe', 'Jane Smith'], 'email': ['john.doe@example.com', 'jane.smith@example.com']})\n\n# Generate a tabular analysis which describes PII entities in the DataFrame.\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df)\n\n# Define anonymization operators\nfake = Faker()\noperators = {\n    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"REDACTED\"}),\n    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})\n}\n\n# Anonymize DataFrame\nanonymized_df = pandas_engine.anonymize(sample_df, tabular_analysis, operators=operators)\nprint(anonymized_df)\n```\n\n----------------------------------------\n\nTITLE: Providing Outer Context to Analyzer for Improved Detection Scores - Presidio Python\nDESCRIPTION: This snippet demonstrates passing an explicit list of context words (e.g., [\"zip\"]) to the AnalyzerEngine's analyze function, thereby improving detection confidence even when the context does not appear in the actual text. It illustrates how context provided externally at analysis time contributes to the score calculation. Useful for scenarios such as column-based, field-based, or domain-specific context enrichment. Requires prior recognizer and analyzer setup.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Define the recognizer with the defined pattern and context words\\nzipcode_recognizer = PatternRecognizer(\\n    supported_entity=\\\"US_ZIP_CODE\\\",\\n    patterns=[zipcode_pattern],\\n    context=[\\\"zip\\\", \\\"zipcode\\\"],\\n)\\n\\nregistry = RecognizerRegistry()\\nregistry.add_recognizer(zipcode_recognizer)\\nanalyzer = AnalyzerEngine(registry=registry)\\n\\n# Test\\ntext = \\\"My code is 90210\\\"\\nresult = analyzer.analyze(text=text, language=\\\"en\\\", context=[\\\"zip\\\"])\\nprint(\\\"Result:\\\")\\nprint_analyzer_results(result, text=text)\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio Modules in Python\nDESCRIPTION: This code imports the necessary classes from Presidio Analyzer and Anonymizer to perform PII detection and anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\n```\n\n----------------------------------------\n\nTITLE: Configuring Presidio Analyzer for Multiple Languages\nDESCRIPTION: Demonstrates how to configure the AnalyzerEngine to support multiple languages (English and Spanish) using spaCy models.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\n# Create configuration containing engine name and models\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [\n        {\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n    ],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, supported_languages=[\"en\", \"es\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text for PII Entities using Presidio Analyzer in Python\nDESCRIPTION: This snippet demonstrates how to set up the AnalyzerEngine and use it to analyze text for specific PII entities. It loads the NLP module and PII recognizers, then calls the analyze method with a sample text and entity type.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/index.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=\"My phone number is 212-555-5555\",\n                           entities=[\"PHONE_NUMBER\"],\n                           language='en')\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Text De-anonymization Functions Implementation in Python\nDESCRIPTION: Implements text de-anonymization using Presidio engine. Includes functions for de-anonymizing text and extracting entities from mappings to restore original PII data.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/index.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -> str:\n        \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Deanonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            entities = self.get_entities(entity_mappings, text)\n\n            deanonymized = self.deanonymizer.deanonymize(\n                text=text,\n                entities=entities,\n                operators=\n                {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\", \n                                        params={\"entity_mapping\": entity_mappings})}\n            )\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n            \n            return deanonymized.text\n        except Exception as e:\n            logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")\n            raise\n\n    def get_entities(self, entity_mappings: dict, text: str) -> List[OperatorResult]:\n        \"\"\" Get the entities from the entity mappings \"\"\"\n\n        entities = []\n        for entity_type, entity_mapping in entity_mappings.items():\n            for entity_value, entity_id in entity_mapping.items():\n                start_index = 0\n                while True:\n                    start_index = text.find(entity_id, start_index)\n                    if start_index == -1:\n                        break\n                    end_index = start_index + len(entity_id)\n                    entities.append(OperatorResult(start_index, end_index, entity_type, entity_value, entity_id))\n                    start_index += len(entity_id)\n        return entities\n```\n\n----------------------------------------\n\nTITLE: Anonymizing Identified PII Entities with Presidio in Python\nDESCRIPTION: This snippet demonstrates how to use the Presidio Anonymizer to anonymize the identified PII entities in the text. It sets up different anonymization operators for different entity types and prints both the anonymized text and detailed results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/presidio_notebook.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nanonymizer = AnonymizerEngine()\n\nanonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize,\n    analyzer_results=analyzer_results,    \n    operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"}), \n                        \"PHONE_NUMBER\": OperatorConfig(\"mask\", {\"type\": \"mask\", \"masking_char\" : \"*\", \"chars_to_mask\" : 12, \"from_end\" : True}),\n                        \"TITLE\": OperatorConfig(\"redact\", {})}\n)\n\nprint(f\"text: {anonymized_results.text}\")\nprint(\"detailed response:\")\n\npprint(json.loads(anonymized_results.to_json()))\n```\n\n----------------------------------------\n\nTITLE: Injecting Request-level Context Words for Zip Code Recognition using Presidio Analyzer in Python\nDESCRIPTION: Demonstrates passing extra context words at analysis time (e.g., from metadata like column names) to further enhance recognition confidence, even if those words do not appear in the text. Defines and registers a PatternRecognizer with context, then supplies context words at the analyze() call. Requires presidio_analyzer. Key parameters: supplied context list, supported_entity, and regex pattern. Input is a record dictionary with text and column name; output is a confidence-boosted analysis result.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/06_context.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry, PatternRecognizer\n\n# Define the recognizer with the defined pattern and context words\nzipcode_recognizer = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\",\n    patterns=[zipcode_pattern],\n    context=[\"zip\", \"zipcode\"],\n)\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test with an example record having a column name which could be injected as context\nrecord = {\"column_name\": \"zip\", \"text\": \"My code is 90210\"}\n\nresult = analyzer.analyze(\n    text=record[\"text\"], language=\"en\", context=[record[\"column_name\"]]\n)\n\nprint(\"Result:\")\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Defining a Weak US Zip Code Recognizer with Presidio Analyzer in Python\nDESCRIPTION: Creates and registers a PatternRecognizer for US zip codes using a basic five-digit regex pattern with a low confidence score (0.01), using Presidio Analyzer components. Demonstrates basic text analysis with the recognizer. Dependencies: presidio_analyzer library. Key inputs are the regex pattern and input text ('My zip code is 90210'). Outputs are analysis results with low confidence. This recognizer is intentionally weak and will match any five digits.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/06_context.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import (\n    Pattern,\n    PatternRecognizer,\n    RecognizerRegistry,\n    AnalyzerEngine,\n)\n\n# Define the regex pattern\nregex = r\"(\\b\\d{5}(?:\\-\\d{4})?\\b)\"  # very weak regex pattern\nzipcode_pattern = Pattern(name=\"zip code (weak)\", regex=regex, score=0.01)\n\n# Define the recognizer with the defined pattern\nzipcode_recognizer = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\", patterns=[zipcode_pattern]\n)\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\n\nprint(f\"Result:\\n {results}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiprocessing for Presidio Batch Analysis\nDESCRIPTION: This code demonstrates how to use multiprocessing with Presidio's BatchAnalyzerEngine for improved performance when processing large datasets.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport multiprocessing\nimport psutil\nimport time\n\ndef analyze_batch_multiprocess(n_process=12, batch_size=4):\n    \"\"\"Run BatchAnalyzer with `n_process` processes and batch size of `batch_size`.\"\"\"\n    list_of_texts = [\"My name is mike\"]*1000\n\n    results = batch_analyzer.analyze_iterator(\n            texts=list_of_texts, \n            language=\"en\",\n            n_process=n_process, \n            batch_size=batch_size\n        )\n\n    return list(results)\n\n\n\ndef monitor_processes():\n    \"\"\"Monitor all Python processes dynamically.\"\"\"\n    while True:\n        processes = [p for p in psutil.process_iter(attrs=['pid', 'name']) if \"python\" in p.info['name']]\n        print(f\"[Monitor] Active Python processes: {len(processes)} - {[p.info['pid'] for p in processes]}\")\n        time.sleep(1)\n\n\n# Run interactive monitoring\nmonitor_proc = multiprocessing.Process(target=monitor_processes, daemon=True)\nmonitor_proc.start()\n\n# Run the batch analyzer process\nanalyze_batch_multiprocess(n_process=4, batch_size=2)\n\n# Wait for everything to conclude\ntime.sleep(1)  \n\n# Clean up (not needed if daemon=True, but useful if stopping manually)\nmonitor_proc.terminate()\n```\n\n----------------------------------------\n\nTITLE: Example Data for PHONE_NUMBER Entity (Space Separated)\nDESCRIPTION: Provides sample text containing a US phone number ('425 8829090') labeled as PHONE_NUMBER, including context words 'my phone number is'. Tests recognition of space-separated phone number format.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nPHONE_NUMBER\nmy phone number is 425 8829090\n```\n\n----------------------------------------\n\nTITLE: Importing Transformers NLP Engine\nDESCRIPTION: Imports the TransformersNlpEngine for using transformers models as the NER engine.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer.nlp_engine import TransformersNlpEngine, NerModelConfiguration\n```\n\n----------------------------------------\n\nTITLE: Defining Weak Pattern Recognizer for ZIP Code Detection - Presidio Python\nDESCRIPTION: This snippet defines a weak regular expression pattern for ZIP code detection using only five digits and sets a low initial score. It creates a PatternRecognizer for the 'US_ZIP_CODE' entity with the pattern, registers it, and analyzes sample text. The pattern is intentionally relaxed to demonstrate low initial confidence in detection which can later be improved with context. Requires Presidio Analyzer, RecognizerRegistry, Pattern, and PatternRecognizer classes.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define the regex pattern\\nregex = r\\\"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\\\"  # very weak regex pattern\\nzipcode_pattern = Pattern(name=\\\"zip code (weak)\\\", regex=regex, score=0.01)\\n\\n# Define the recognizer with the defined pattern\\nzipcode_recognizer = PatternRecognizer(\\n    supported_entity=\\\"US_ZIP_CODE\\\", patterns=[zipcode_pattern]\\n)\\n\\nregistry = RecognizerRegistry()\\nregistry.add_recognizer(zipcode_recognizer)\\nanalyzer = AnalyzerEngine(registry=registry)\\n\\n# Test\\ntext = \\\"My zip code is 90210\\\"\\nresults = analyzer.analyze(text=text, language=\\\"en\\\")\\nprint_analyzer_results(results, text=text)\n```\n\n----------------------------------------\n\nTITLE: Creating a Deny-List Based PII Recognizer for Titles\nDESCRIPTION: Demonstrates how to create a custom PatternRecognizer for detecting titles using a deny-list approach.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntitles_list = [\n    \"Sir\",\n    \"Ma'am\",\n    \"Madam\",\n    \"Mr.\",\n    \"Mrs.\",\n    \"Ms.\",\n    \"Miss\",\n    \"Dr.\",\n    \"Professor\",\n]\n\ntitles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=titles_list)\n```\n\n----------------------------------------\n\nTITLE: Restricting Network Access to Presidio App Service using Azure CLI in Bash\nDESCRIPTION: This Bash script uses the `az webapp config access-restriction add` command to configure an access rule for the Azure Web App. It allows traffic only from a specified IP address range (`$FRONT_END_IP_RANGE`), enhancing security by limiting who can reach the Presidio service endpoint. The `$RESOURCE_GROUP` and `$APP_SERVICE_NAME` variables must be set.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/app-service/index.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nFRONT_END_IP_RANGE=[front end ip range]\naz webapp config access-restriction add --resource-group $RESOURCE_GROUP --name $APP_SERVICE_NAME \\\n  --rule-name 'Front-end allow rule' --action Allow --ip-address $FRONT_END_IP_RANGE --priority 100\n```\n\n----------------------------------------\n\nTITLE: Analyzing and Anonymizing PII with Transformers Model\nDESCRIPTION: Python code showing how to use Presidio with a transformers-based NLP engine to detect and anonymize PII in text. The example configures a custom NLP engine using both a small spaCy model and a BERT-based NER model from transformers.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_text.md#2025-04-23_snippet_3\n\nLANGUAGE: py\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import TransformersNlpEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\ntext = \"My name is Don and my phone number is 212-555-5555\"\n\n# Define which transformers model to use\nmodel_config = [{\"lang_code\": \"en\", \"model_name\": {\n    \"spacy\": \"en_core_web_sm\",  # use a small spaCy model for lemmas, tokens etc.\n    \"transformers\": \"dslim/bert-base-NER\"\n    }\n}]\n\nnlp_engine = TransformersNlpEngine(models=model_config)\n\n# Set up the engine, loads the NLP module (spaCy model by default) \n# and other PII recognizers\nanalyzer = AnalyzerEngine(nlp_engine=nlp_engine)\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=text, language='en')\nprint(results)\n\n# Analyzer results are passed to the AnonymizerEngine for anonymization\n\nanonymizer = AnonymizerEngine()\n\nanonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n\nprint(anonymized_text)\n```\n\n----------------------------------------\n\nTITLE: Registering a Context-aware Recognizer and Running Analysis with Presidio Analyzer in Python\nDESCRIPTION: Registers the previously defined context-aware zip code recognizer with the Presidio RecognizerRegistry and initializes an AnalyzerEngine. Demonstrates analyzing text to see the enhanced confidence score due to context words. Dependencies are presidio_analyzer and the prior recognizer definition. The test shows a confidence increase when context words are present. Key inputs are the registry, the recognizer, and the analyzed text. Outputs are the enhanced results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/06_context.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer_w_context)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\nprint(\"Result:\")\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Anonymization Operators with Presidio AnonymizerEngine in Python\nDESCRIPTION: This complete example demonstrates initializing `AnonymizerEngine`, defining custom operators using `OperatorConfig` (including default, masking for phone numbers, and redacting for titles), and then applying these operators during anonymization. It takes sample text and simulated analyzer results, passes them along with the custom operators dictionary to the `anonymize` method, and prints the resulting anonymized text and the detailed JSON output.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/10_simple_anonymization.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\nimport json\n\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig, RecognizerResult\n\n\n# Analyzer output\nanalyzer_results = [\n    RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n    RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n]\n\ntext_to_anonymize = \"My name is Bond, James Bond\"\n\nanonymizer = AnonymizerEngine()\n\n# Define anonymization operators\noperators = {\n    \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"}),\n    \"PHONE_NUMBER\": OperatorConfig(\n        \"mask\",\n        {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 12,\n            \"from_end\": True,\n        },\n    ),\n    \"TITLE\": OperatorConfig(\"redact\", {}),\n}\n\nanonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n)\n\nprint(f\"text: {anonymized_results.text}\")\nprint(\"detailed result:\")\n\npprint(json.loads(anonymized_results.to_json()))\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using GLiNERRecognizer in Presidio\nDESCRIPTION: This Python script demonstrates how to set up and use GLiNERRecognizer within Presidio's AnalyzerEngine. It includes configuring the NLP engine, creating a custom GLiNERRecognizer, adding it to the registry, and analyzing text for PII entities.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/gliner.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nfrom presidio_analyzer.predefined_recognizers import GLiNERRecognizer\n\n\n# Load a small spaCy model as we don't need spaCy's NER\nnlp_engine = NlpEngineProvider(\n    nlp_configuration={\n        \"nlp_engine_name\": \"spacy\",\n        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n    }\n)\n\n# Create an analyzer engine \nanalyzer_engine = AnalyzerEngine()\n\n# Define and create the GLiNER recognizer\nentity_mapping = {\n    \"person\": \"PERSON\",\n    \"name\": \"PERSON\",\n    \"organization\": \"ORGANIZATION\",\n    \"location\": \"LOCATION\"\n}\n\ngliner_recognizer = GLiNERRecognizer(\n    model_name=\"urchade/gliner_multi_pii-v1\",\n    entity_mapping=entity_mapping,\n    flat_ner=False,\n    multi_label=True,\n    map_location=\"cpu\",\n)\n\n# Add the GLiNER recognizer to the registry\nanalyzer_engine.registry.add_recognizer(gliner_recognizer)\n\n# Remove the spaCy recognizer to avoid NER coming from spaCy\nanalyzer_engine.registry.remove_recognizer(\"SpacyRecognizer\")\n\n# Analyze text\nresults = analyzer_engine.analyze(\n    text=\"Hello, my name is Rafi Mor, I'm from Binyamina and I work at Microsoft. \", language=\"en\"\n)\n\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Defining Recognizer Registry Configuration with Multilingual Support - Python\nDESCRIPTION: Specifies a YAML string configuring the recognizer registry, listing predefined recognizers for credit card, date, email, and phone entities, each with context words for both English and Spanish. It also defines custom deny-list recognizers (Titles) tailored to each language. Used as part of the main configuration for Presidio's recognizer registry.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/08_no_code.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrecognizer_registry_config_yaml = \"\"\"\nrecognizer_registry:\n  supported_languages: \n  - en\n  - es\n  global_regex_flags: 26\n\n  recognizers:\n  - name: CreditCardRecognizer\n    supported_languages:\n    - language: en\n      context: [credit, card, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    - language: es\n      context: [tarjeta, credito, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    type: predefined\n    \n  - name: DateRecognizer\n    supported_languages:\n    - language: en\n      context: [date, time, birthday, birthdate, dob]\n    - language: es\n      context: [fecha, tiempo, hora, nacimiento, dob]\n    type: predefined\n\n  - name: EmailRecognizer\n    supported_languages:\n    - language: en\n      context: [email, mail, address]\n    - language: es\n      context: [correo, electrnico, email]\n    type: predefined\n    \n  - name: PhoneRecognizer\n    type: predefined\n    supported_languages:\n    - language: en\n      context: [phone, number, telephone, fax]\n    - language: es\n      context: [telfono, nmero, fax]\n    \n  - name: \\\"Titles recognizer (en)\\\"\n    supported_language: \\\"en\\\"\n    supported_entity: \\\"TITLE\\\"\n    deny_list:\n      - Mr.\n      - Mrs.\n      - Ms.\n      - Miss\n      - Dr.\n      - Prof.\n      - Doctor\n      - Professor\n  - name: \\\"Titles recognizer (es)\\\"\n    supported_language: \\\"es\\\"\n    supported_entity: \\\"TITLE\\\"\n    deny_list:\n      - Sr.\n      - Seor\n      - Sra.\n      - Seora\n      - Srta.\n      - Seorita\n      - Dr.\n      - Doctor\n      - Doctora\n      - Prof.\n      - Profesor\n      - Profesora\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Allow List of URLs for PII Exclusion with Presidio - Python\nDESCRIPTION: Defines a list of URLs that should not be marked as PII by recognizers. The `websites_list` variable contains the example domains set for exclusion, serving as the allow list in subsequent analysis steps. This snippet is foundational for excluding specific tokens during PII detection. No dependencies required at this point, but assumes later usage with Presidio's AnalyzerEngine.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/13_allow_list.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwebsites_list = [\n    \"bing.com\",\n    \"microsoft.com\"\n]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text for Phone Numbers using Presidio in Python\nDESCRIPTION: This snippet initializes the Presidio Analyzer engine and uses it to analyze the sample text for phone numbers. It demonstrates how to specify a particular entity type for analysis and prints the results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/presidio_notebook.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nanalyzer = AnalyzerEngine()\nanalyzer_results = analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')\n\nprint(analyzer_results)\n```\n\n----------------------------------------\n\nTITLE: Deanonymizing Encrypted Text with Presidio Deanonymizer in Python\nDESCRIPTION: This example shows how to use the DeanonymizeEngine to decrypt previously anonymized text. It initializes the engine, defines the encrypted entity, and applies a decrypt operator to restore the original text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/anonymizer/index.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer import DeanonymizeEngine\nfrom presidio_anonymizer.entities import OperatorResult, OperatorConfig\n\n# Initialize the engine:\nengine = DeanonymizeEngine()\n\n# Invoke the deanonymize function with the text, anonymizer results and\n# Operators to define the deanonymization type.\nresult = engine.deanonymize(\n    text=\"My name is S184CMt9Drj7QaKQ21JTrpYzghnboTF9pn/neN8JME0=\",\n    entities=[\n        OperatorResult(start=11, end=55, entity_type=\"PERSON\"),\n    ],\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": \"WmZq4t7w!z%C&F)J\"})},\n)\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Generating Analysis for Simple JSON Data with Presidio in Python\nDESCRIPTION: Uses JsonAnalysisBuilder to automatically generate analysis for simple JSON data structures.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\njson_analysis = JsonAnalysisBuilder().generate_analysis(sample_json)\njson_analysis\n```\n\n----------------------------------------\n\nTITLE: Defining Regex-Based PII Recognizer with Presidio in Python\nDESCRIPTION: This code defines a regular expression-based PII recognizer using Presidio's Pattern and PatternRecognizer classes in Python. It sets up a pattern to match any sequence of digits (\\d+) and assigns it a detection score. Dependencies include the presidio_analyzer package. The recognizer supports the entity type 'NUMBER' and requires initialization of Presidio components.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/02_regex.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import Pattern, PatternRecognizer\n\n# Define the regex pattern in a Presidio `Pattern` object:\nnumbers_pattern = Pattern(name=\"numbers_pattern\", regex=\"\\d+\", score=0.5)\n\n# Define the recognizer with one or more patterns\nnumber_recognizer = PatternRecognizer(\n    supported_entity=\"NUMBER\", patterns=[numbers_pattern]\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Analyzer with Customized Context Similarity and Printing Results - Presidio Python\nDESCRIPTION: This snippet analyzes the ZIP code in text 'My zip code is 90210' using the analyzer configured with custom context similarity settings. It prints results that showcase the impact of tweaked context scoring enhancements. This is useful for experimentation and for observing how context factors alter final recognition outcomes. Requires the previous setup of custom context enhancer.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Test\\nresults = analyzer.analyze(text=\\\"My zip code is 90210\\\", language=\\\"en\\\")\\nprint(\\\"Result:\\\")\\nprint_analyzer_results(results, text=text)\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Recognizer with AnalyzerEngine (Presidio, Python)\nDESCRIPTION: This multi-step example demonstrates how to add a custom recognizer (such as a PatternRecognizer) to Presidio's RecognizerRegistry, then use it with the AnalyzerEngine to analyze text for entities. Dependencies: presidio_analyzer library and an existing recognizer object. Inputs include an instance of RecognizerRegistry, the recognizer to add, and the text to analyze; output is the recognizer's findings printed to stdout.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nregistry = RecognizerRegistry()\nregistry.load_predefined_recognizers()\n\n# Add the recognizer to the existing list of recognizers\nregistry.add_recognizer(titles_recognizer)\n\n# Set up analyzer with our updated recognizer registry\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Run with input text\ntext=\"His name is Mr. Jones\"\nresults = analyzer.analyze(text=text, language=\"en\")\nprint(results)\n\n```\n\n----------------------------------------\n\nTITLE: Customizing Context Similarity Factors for LemmaContextAwareEnhancer - Presidio Python\nDESCRIPTION: This snippet sets up the AnalyzerEngine with custom values for context_similarity_factor and min_score_with_context_similarity by explicitly providing a LemmaContextAwareEnhancer. This allows tuning the scoring mechanism that boosts entity confidence when context words are present. Demonstrates advanced customization for nuanced control over detection scores. Requires Presidio's LemmaContextAwareEnhancer class and a registered recognizer.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nregistry = RecognizerRegistry()\\nregistry.add_recognizer(zipcode_recognizer)\\nanalyzer = AnalyzerEngine(\\n    registry=registry,\\n    context_aware_enhancer=LemmaContextAwareEnhancer(\\n        context_similarity_factor=0.45, min_score_with_context_similarity=0.4\\n    ),\\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing and Anonymizing Dictionary Data with Presidio\nDESCRIPTION: This snippet demonstrates how to use Presidio's batch analyzer and anonymizer to process dictionary data, identifying and anonymizing personal information.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nanalyzer_results = batch_analyzer.analyze_dict(df_dict, language=\"en\")\nanalyzer_results = list(analyzer_results)\nanalyzer_results\n```\n\nLANGUAGE: python\nCODE:\n```\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results)\n```\n\nLANGUAGE: python\nCODE:\n```\nscrubbed_df = pd.DataFrame(anonymizer_results)\n```\n\nLANGUAGE: python\nCODE:\n```\nscrubbed_df\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Anonymization Operator with Faker in Python\nDESCRIPTION: This snippet demonstrates how to create a custom anonymization operator for Presidio. It initializes the `Faker` library, defines a function `fake_name` that ignores its input and returns a fake name, and then creates an `OperatorConfig` mapping the 'PERSON' entity type to this custom function using the 'custom' operator type.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/11_custom_anonymization.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom faker import Faker\nfrom presidio_anonymizer.entities import OperatorConfig\n\nfake = Faker()\n\n# Create faker function (note that it has to receive a value)\ndef fake_name(x):\n    return fake.name()\n\n\n# Create custom operator for the PERSON entity\noperators = {\"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name})}\n```\n\n----------------------------------------\n\nTITLE: Referencing Pattern - Python\nDESCRIPTION: Provides the Pattern class from presidio_analyzer.pattern, representing a regular expression or sequence rule for entity detection. Utilized within custom recognizers to define the expressions and context windows for text matching operations.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.pattern.Pattern\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Service Usage Examples in Python\nDESCRIPTION: Demonstrates how to call the anonymization and de-anonymization services with the required parameters including session ID, text, language and entity mappings.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/index.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    # anonymize example\n    anonymized_text, new_entity_mappings = presidio_service.anonymize_text(\n            request.session_id, request.text, request.language, entity_mappings\n        )\n\n    # deanonymize example\n    deanonymized_text = presidio_service.deanonymize_text(\n            request.session_id, request.text, entity_mappings\n        )\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text in Multiple Languages with Presidio Analyzer - Python\nDESCRIPTION: This snippet demonstrates using the Presidio Analyzer's analyze function to detect entities in input text for both Spanish (\"es\") and English (\"en\") languages. The example shows back-to-back analysis requests and prints the detection results. Usage assumes an initialized analyzer object. Requires the Presidio Analyzer module and any model dependencies for multilingual support. Inputs include text and a language code, outputs detection results for each analyzed language.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Analyze in different languages\\nresults_spanish = analyzer.analyze(text=\\\"Mi nombre es Morris\\\", language=\\\"es\\\")\\nprint(\\\"Results from Spanish request:\\\")\\nprint(results_spanish)\\n\\nresults_english = analyzer.analyze(text=\\\"My name is Morris\\\", language=\\\"en\\\")\\nprint(\\\"Results from English request:\\\")\\nprint(results_english)\n```\n\n----------------------------------------\n\nTITLE: Creating a Regex-Based PII Recognizer for Numbers\nDESCRIPTION: Shows how to create a PatternRecognizer that uses a regular expression to detect numbers within text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Define the regex pattern in a Presidio `Pattern` object:\nnumbers_pattern = Pattern(name=\"numbers_pattern\", regex=\"\\d+\", score=0.5)\n\n# Define the recognizer with one or more patterns\nnumber_recognizer = PatternRecognizer(\n    supported_entity=\"NUMBER\", patterns=[numbers_pattern]\n)\n```\n\n----------------------------------------\n\nTITLE: Encrypting Entities in Text Using Presidio AnonymizerEngine (Python)\nDESCRIPTION: Instantiates the AnonymizerEngine and encrypts entities within a provided text using the 'encrypt' operator and a specified key. Requires a RecognizerResult for each entity to encrypt and an OperatorConfig specifying operation type and key. The output includes anonymized text and positions or values of all encrypted items. Input parameters include original text, detected entity spans, and the cryptographic key.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/12_encryption.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nengine = AnonymizerEngine()\\n\\n# Invoke the anonymize function with the text,\\n# analyzer results (potentially coming from presidio-analyzer)\\n# and an 'encrypt' operator to get an encrypted anonymization output:\\nanonymize_result = engine.anonymize(\\n    text=\"My name is James Bond\",\\n    analyzer_results=[\\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),\\n    ],\\n    operators={\"PERSON\": OperatorConfig(\"encrypt\", {\"key\": crypto_key})},\\n)\\n\\nanonymize_result\n```\n\n----------------------------------------\n\nTITLE: DICOM Image Redaction with Python\nDESCRIPTION: Python script showing how to use DicomImageRedactorEngine for redacting PII from DICOM medical images with various options.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/index.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pydicom\nfrom presidio_image_redactor import DicomImageRedactorEngine\n\n# Set input and output paths\ninput_path = \"path/to/your/dicom/file.dcm\"\noutput_dir = \"./output\"\n\n# Initialize the engine\nengine = DicomImageRedactorEngine()\n\n# Option 1: Redact from a loaded DICOM image\ndicom_image = pydicom.dcmread(input_path)\nredacted_dicom_image = engine.redact(dicom_image, fill=\"contrast\")\n\n# Option 2: Redact from a loaded DICOM image and return redacted regions\nredacted_dicom_image, bboxes = engine.redact_and_return_bbox(dicom_image, fill=\"contrast\")\n\n# Option 3: Redact from DICOM file and save redacted regions as json file\nengine.redact_from_file(input_path, output_dir, padding_width=25, fill=\"contrast\", save_bboxes=True)\n\n# Option 4: Redact from directory and save redacted regions as json files\nocr_kwargs = {\"ocr_threshold\": 50}\nengine.redact_from_directory(\"path/to/your/dicom\", output_dir, fill=\"background\", save_bboxes=True, ocr_kwargs=ocr_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text with Presidio AnalyzerEngine and Custom Recognizer in Python\nDESCRIPTION: This snippet calls the `analyze` method on the configured `AnalyzerEngine` instance (`analyzer`), passing the sample text (`text1`) and specifying the language as English ('en'). This triggers the analysis using all registered recognizers, including the custom titles recognizer previously added. It depends on the `analyzer` object and `text1` variable.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/01_deny_list.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\nresults = analyzer.analyze(text=text1, language=\"en\")\n```\n```\n\n----------------------------------------\n\nTITLE: Registering Context-Enhanced Recognizer with Analyzer Engine - Presidio Python\nDESCRIPTION: This snippet demonstrates registering a PatternRecognizer (with patterns and context words) into a RecognizerRegistry and passing it to AnalyzerEngine. This enables the analyzer to use the enhanced recognizer configuration when analyzing text for PII. Requires that the recognizer, registry, and analyzer objects are properly initialized. Ensures context-aware enhancements are active during analysis.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nregistry = RecognizerRegistry()\\nregistry.add_recognizer(zipcode_recognizer)\\nanalyzer = AnalyzerEngine(registry=registry)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text with All Configured Recognizers in Presidio\nDESCRIPTION: This snippet demonstrates how to use the Presidio Analyzer with all configured recognizers, including both default and custom ones. It analyzes the sample text and returns the results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/presidio_notebook.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nanalyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')\n\nanalyzer_results\n```\n\n----------------------------------------\n\nTITLE: Configuring Transformers NLP Engine\nDESCRIPTION: Sets up a Transformers NLP engine with custom model configuration and entity mappings.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define which model to use\nmodel_config = [{\n   \"lang_code\":\"en\",\n   \"model_name\":{\n      \"spacy\":\"en_core_web_sm\",\n      \"transformers\":\"obi/deid_roberta_i2b2\"\n   }\n}]\n\n# Map transformers model labels to Presidio's\nmodel_to_presidio_entity_mapping = dict(\n    PER=\"PERSON\",\n    PERSON=\"PERSON\",\n    LOC= \"LOCATION\",\n    LOCATION= \"LOCATION\",\n    GPE=\"LOCATION\",\n    ORG=\"ORGANIZATION\",\n    ORGANIZATION=\"ORGANIZATION\",\n    NORP=\"NRP\",\n    AGE=\"AGE\",\n    ID=\"ID\",\n    EMAIL=\"EMAIL\",\n    PATIENT=\"PERSON\",\n    STAFF=\"PERSON\",\n    HOSP=\"ORGANIZATION\",\n    PATORG=\"ORGANIZATION\",\n    DATE=\"DATE_TIME\",\n    TIME=\"DATE_TIME\",\n    PHONE=\"PHONE_NUMBER\",\n    HCW=\"PERSON\",\n    HOSPITAL=\"ORGANIZATION\",\n    FACILITY=\"LOCATION\",\n)\n\nner_model_configuration = NerModelConfiguration(model_to_presidio_entity_mapping=model_to_presidio_entity_mapping, \n                                                aggregation_strategy=\"simple\",\n                                                stride=14)\n\ntransformers_nlp_engine = TransformersNlpEngine(models=model_config,\n                                                ner_model_configuration=ner_model_configuration)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using DeanonymizeEngine - Presidio (Python)\nDESCRIPTION: This Python snippet shows how to initialize Presidio's DeanonymizeEngine and use it to decrypt (deanonymize) previously encrypted text based on entity spans. It depends on the presidio-anonymizer package and requires the correct cryptographic key and proper configuration of OperatorResult and OperatorConfig. The engine.deanonymize method takes an encrypted text, a list of OperatorResult objects indicating the PII locations, and operators dict mapping entity types (or \"DEFAULT\") to OperatorConfig for decryption. This use case specifically demonstrates restoring an anonymized PERSON entity in the sample string.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-anonymizer/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer import DeanonymizeEngine\nfrom presidio_anonymizer.entities import OperatorResult, OperatorConfig\n\n# Initialize the engine with logger.\nengine = DeanonymizeEngine()\n\n# Invoke the deanonymize function with the text, anonymizer results and\n# Operators to define the deanonymization type.\nresult = engine.deanonymize(\n    text=\"My name is S184CMt9Drj7QaKQ21JTrpYzghnboTF9pn/neN8JME0=\",\n    entities=[\n        OperatorResult(start=11, end=55, entity_type=\"PERSON\"),\n    ],\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": \"WmZq4t7w!z%C&F)J\"})},\n)\n\nprint(result)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Anonymizer for Unique Identifier Replacement\nDESCRIPTION: This snippet defines a custom Anonymizer class that replaces PII with unique identifiers based on entity type and instance count.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonymization.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass InstanceCounterAnonymizer(Operator):\n    \"\"\"\n    Anonymizer which replaces the entity value\n    with an instance counter per entity.\n    \"\"\"\n\n    REPLACING_FORMAT = \"<{entity_type}_{index}>\"\n\n    def operate(self, text: str, params: Dict = None) -> str:\n        \"\"\"Anonymize the input text.\"\"\"\n\n        entity_type: str = params[\"entity_type\"]\n\n        # entity_mapping is a dict of dicts containing mappings per entity type\n        entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]\n\n        entity_mapping_for_type = entity_mapping.get(entity_type)\n        if not entity_mapping_for_type:\n            new_text = self.REPLACING_FORMAT.format(\n                entity_type=entity_type, index=0\n            )\n            entity_mapping[entity_type] = {}\n\n        else:\n            if text in entity_mapping_for_type:\n                return entity_mapping_for_type[text]\n\n            previous_index = self._get_last_index(entity_mapping_for_type)\n            new_text = self.REPLACING_FORMAT.format(\n                entity_type=entity_type, index=previous_index + 1\n            )\n\n        entity_mapping[entity_type][text] = new_text\n        return new_text\n\n    @staticmethod\n    def _get_last_index(entity_mapping_for_type: Dict) -> int:\n        \"\"\"Get the last index for a given entity type.\"\"\"\n\n        def get_index(value: str) -> int:\n            return int(value.split(\"_\")[-1][:-1])\n\n        indices = [get_index(v) for v in entity_mapping_for_type.values()]\n        return max(indices)\n\n    def validate(self, params: Dict = None) -> None:\n        \"\"\"Validate operator parameters.\"\"\"\n\n        if \"entity_mapping\" not in params:\n            raise ValueError(\"An input Dict called `entity_mapping` is required.\")\n        if \"entity_type\" not in params:\n            raise ValueError(\"An entity_type param is required.\")\n\n    def operator_name(self) -> str:\n        return \"entity_counter\"\n\n    def operator_type(self) -> OperatorType:\n        return OperatorType.Anonymize\n```\n\n----------------------------------------\n\nTITLE: Using 'Keep' Operator for PII Entity Extraction with Presidio in Python\nDESCRIPTION: This code demonstrates an alternative approach using the 'keep' operator to extract PII entities without modifying them.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nanonymized_results_with_keep = anonymizer.anonymize(\n        text=text_to_analyze,\n        analyzer_results=analyzer_results,            \n        operators={\"DEFAULT\": OperatorConfig(\"keep\")}        \n    )\n[(item.text, item.start, item.end) for item in anonymized_results_with_keep.items]\n```\n\n----------------------------------------\n\nTITLE: Decrypting Entities in Text Using Presidio DeanonymizeEngine (Python)\nDESCRIPTION: Instantiates the DeanonymizeEngine and decrypts entities in the anonymized text using the 'decrypt' operator and the same cryptographic key used for encryption. The method takes as input the anonymized text, the list of encrypted entity descriptions, and the operator configuration. The output is the original text with all entities restored.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/12_encryption.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the engine:\\nengine = DeanonymizeEngine()\\n\\n# Invoke the deanonymize function with the text, anonymizer results\\n# and a 'decrypt' operator to get the original text as output.\\ndeanonymized_result = engine.deanonymize(\\n    text=anonymized_text,\\n    entities=anonymized_entities,\\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": crypto_key})},\\n)\\n\\ndeanonymized_result\n```\n\n----------------------------------------\n\nTITLE: Example Single Presidio Analyzer Configuration File in YAML\nDESCRIPTION: This YAML snippet illustrates the structure of a comprehensive configuration file for the Presidio Analyzer Engine. It defines key parameters like `supported_languages`, `default_score_threshold`, `nlp_configuration` (specifying the NLP engine, models, and entity mappings), and the `recognizer_registry` which lists the PII recognizers to be used. This file format is used when configuring the engine via a single file path.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/analyzer_engine_provider.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsupported_languages: \n- en\ndefault_score_threshold: 0\n\nnlp_configuration:\n  nlp_engine_name: spacy\n  models:\n  -\n    lang_code: en\n    model_name: en_core_web_lg\n  -\n    lang_code: es\n    model_name: es_core_news_md\n  ner_model_configuration:\n    model_to_presidio_entity_mapping:\n      PER: PERSON\n      PERSON: PERSON\n      LOC: LOCATION\n      LOCATION: LOCATION\n      GPE: LOCATION\n      ORG: ORGANIZATION\n      DATE: DATE_TIME\n      TIME: DATE_TIME\n      NORP: NRP\n\n    low_confidence_score_multiplier: 0.4\n    low_score_entity_names:\n    - ORGANIZATION\n    - ORG\n    default_score: 0.85\n\nrecognizer_registry:\n  global_regex_flags: 26\n  recognizers: \n  - name: CreditCardRecognizer\n    supported_languages: \n      - en\n    supported_entity: IT_FISCAL_CODE\n    type: predefined\n\n  - name: ItFiscalCodeRecognizer\n    type: predefined\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio, OpenAI, and Dependencies using pip and SpaCy\nDESCRIPTION: Installs the `presidio_analyzer`, `presidio_anonymizer`, `openai`, and `pandas` Python packages using pip. It also downloads the `en_core_web_lg` English language model for SpaCy, which is likely used by Presidio for Named Entity Recognition.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!pip install openai pandas\n!python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Enabling Decision Process Logging in Presidio-analyzer\nDESCRIPTION: Python code example showing how to enable decision process logging by creating the AnalyzerEngine with log_decision_process set to True. It also demonstrates how to provide a correlation_id for tracing purposes when analyzing text containing a phone number.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/decision_process.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default)\n# and other PII recognizers\nanalyzer = AnalyzerEngine(log_decision_process=True)\n\n# Call analyzer to get results\nresults = analyzer.analyze(text='My phone number is 212-555-5555', \n                       entities=['PHONE_NUMBER'], \n                       language='en', \n                       correlation_id=\"xyz\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text in PDF for PII using Presidio in Python\nDESCRIPTION: This snippet extracts text from a PDF using pdfminer and analyzes it for PII using Presidio Analyzer. It processes the PDF at the text container level and stores the analyzed character sets.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_pdf_annotation.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nanalyzer = AnalyzerEngine()\n\nanalyzed_character_sets = []\n\nfor page_layout in extract_pages(\"./sample_data/sample.pdf\"):\n    for text_container in page_layout:\n        if isinstance(text_container, LTTextContainer):\n\n            # The element is a LTTextContainer, containing a paragraph of text.\n            text_to_anonymize = text_container.get_text()\n\n            # Analyze the text using the analyzer engine\n            analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')\n \n            if text_to_anonymize.isspace() == False:\n                print(text_to_anonymize)\n                print(analyzer_results)\n\n            characters = list([])\n\n            # Grab the characters from the PDF\n            for text_line in filter(lambda t: isinstance(t, LTTextLine), text_container):\n                    for character in filter(lambda t: isinstance(t, LTChar), text_line):\n                            characters.append(character)\n\n\n            # Slice out the characters that match the analyzer results.\n            for result in analyzer_results:\n                start = result.start\n                end = result.end\n                analyzed_character_sets.append({\"characters\": characters[start:end], \"result\": result})\n```\n\n----------------------------------------\n\nTITLE: Extracting PII Entity Values Using List Comprehension in Python\nDESCRIPTION: This snippet demonstrates a naive approach to extract the text values of identified PII entities using list comprehension.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[(text_to_analyze[res.start:res.end], res.start, res.end) for res in analyzer_results]\n```\n\n----------------------------------------\n\nTITLE: Initializing Presidio AnalyzerEngine with Default Configuration in Python\nDESCRIPTION: This Python snippet illustrates how to create a Presidio `AnalyzerEngine` using its default built-in configuration. An `AnalyzerEngineProvider` is instantiated without any arguments, and `create_engine()` is called to get an engine with predefined settings. This is the simplest way to get started if the default behavior is sufficient and depends only on the `presidio_analyzer` library.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/analyzer_engine_provider.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, AnalyzerEngineProvider\n\nprovider = AnalyzerEngineProvider().create_engine()\n\nresults = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom NLP Engine via Python Code\nDESCRIPTION: Example showing how to create and configure a custom NLP engine using the NlpEngineProvider class to support multiple languages (English and Spanish) in Presidio Analyzer.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/customizing_nlp_models.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\n# Create configuration containing engine name and models\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, \n    supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results_english)\n```\n\n----------------------------------------\n\nTITLE: Extracting Anonymized Text and Entity Results with Presidio in Python\nDESCRIPTION: This snippet demonstrates how to access the encrypted/anonymized text and the list of anonymized entities from the result object returned by Presidio. It assumes 'anonymize_result' has been previously computed. Output: 'anonymized_text' (encrypted/scrambled string) and 'anonymized_entities' (metadata about each anonymized entity). These are necessary for later decryption steps.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/encrypt_decrypt.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Fetch the anonymized text from the result.\\nanonymized_text = anonymize_result.text\\n\\n# Fetch the anonynized entities from the result.\\nanonymized_entities = anonymize_result.items\n```\n\n----------------------------------------\n\nTITLE: Directly Decrypting an Encrypted Entity Using Presidio Decrypt Operator (Python)\nDESCRIPTION: Demonstrates direct usage of the Decrypt operator to manually decrypt a single entity value from the list of anonymized results. Requires importing the Decrypt class and providing the encrypted text and cryptographic key. Returns the original decrypted value of the entity. This is useful for partial or selective decryption outside the main engine workflow.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/12_encryption.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer.operators import Decrypt\\n\\n# Fetch the encrypted entity value from the previous stage\\nencrypted_entity_value = anonymize_result.items[0].text\\n\\n# Restore the original entity value\\nDecrypt().operate(text=encrypted_entity_value, params={\"key\": crypto_key})\n```\n\n----------------------------------------\n\nTITLE: Executing Multi-Language Analyzer (English)\nDESCRIPTION: Runs the AnalyzerEngine with the multi-language Transformers NLP engine for English text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Call in English\ncall_analyzer_and_print_results(transformers_nlp_engine, \n                                language=\"en\", \n                                text = \"Bill Clinton was the president of the United States\")\n```\n\n----------------------------------------\n\nTITLE: Text Anonymization Implementation\nDESCRIPTION: Implements text anonymization using Presidio Analyzer and Anonymizer engines. Processes input text to identify and replace sensitive information while maintaining entity mappings for later de-anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -> Tuple[str, List[OperatorResult], dict] :\n        \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Anonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            results = self.analyzer.analyze(text=text, language=language)\n            logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")\n\n            anonymizer_start_time = timer()\n            anonymizer_entity_mapping = entity_mappings.copy() if entity_mappings is not None else dict()\n            anonymized_result = self.anonymizer.anonymize(\n                text=text,\n                analyzer_results=results,\n                operators={\n                    \"DEFAULT\": OperatorConfig(\n                        \"entity_counter\", {\"entity_mapping\": anonymizer_entity_mapping}\n                    )\n                },\n            )\n            logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n\n            return anonymized_result.text, anonymizer_entity_mapping\n        except Exception as e:\n            logger.exception(f\"Error in anonymize_text for session_id {session_id}\")\n            raise\n```\n\n----------------------------------------\n\nTITLE: Initializing Presidio Analyzer and Anonymizer in Python\nDESCRIPTION: This snippet creates instances of the AnalyzerEngine and AnonymizerEngine classes for later use in PII detection and anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nanalyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n```\n\n----------------------------------------\n\nTITLE: Encrypting Identified Entities with Presidio Anonymizer in Python\nDESCRIPTION: This snippet invokes the Presidio AnonymizerEngine to encrypt PERSON entities detected in a text using the specified cryptographic key. Dependencies: Presidio packages and spaCy model must be installed, and previous imports and key definition are required. Inputs: A sample text, recognized entities, operator configurations with 'encrypt' action. Output: An anonymize_result object containing encrypted text and entity details. Limitation: Only 'PERSON' entity types are processed as per configured operator.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/encrypt_decrypt.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nengine = AnonymizerEngine()\\n\\n# Invoke the anonymize function with the text,\\n# analyzer results (potentially coming from presidio-analyzer)\\n# and an 'encrypt' operator to get an encrypted anonymization output:\\nanonymize_result = engine.anonymize(\\n    text=\"My name is James Bond\",\\n    analyzer_results=[\\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),\\n    ],\\n    operators={\"PERSON\": OperatorConfig(\"encrypt\", {\"key\": crypto_key})},\\n)\\n\\nanonymize_result\n```\n\n----------------------------------------\n\nTITLE: Executing Multi-Language Analyzer (Spanish)\nDESCRIPTION: Runs the AnalyzerEngine with the multi-language Transformers NLP engine for Spanish text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Call in Spanish\ncall_analyzer_and_print_results(transformers_nlp_engine, \n                                language=\"es\", \n                                text = \"Bill Clinton sola ser el presidente de los Estados Unidos.\")\n```\n\n----------------------------------------\n\nTITLE: Initializing a Simple PatternRecognizer with Deny-List (Presidio, Python)\nDESCRIPTION: This Python snippet demonstrates initializing a PatternRecognizer to detect titles (e.g., \"Mr.\", \"Mrs.\", \"Miss\") using a deny-list within the Presidio Analyzer ecosystem. The PatternRecognizer is set up to flag occurrences of the specified words in analyzed texts as the entity \"TITLE\". Requires the presidio_analyzer library. Inputs include the entity name and matching deny-list; output is a recognizer instance.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import PatternRecognizer\ntitles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n                                      deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Broadcasting Analyzer Objects in Spark\nDESCRIPTION: Loads data from a CSV file and broadcasts the analyzer and anonymizer objects to all Spark worker nodes for improved performance in distributed PII detection and anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nanonymizer = AnonymizerEngine()\nbroadcasted_analyzer = spark.sparkContext.broadcast(analyzer)\nbroadcasted_anonymizer = spark.sparkContext.broadcast(anonymizer)\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_path)\ndisplay(df)\n# df.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Deanonymizer for Restoring Original PII\nDESCRIPTION: This snippet defines a custom Deanonymizer class that replaces the unique identifiers with the original PII values using the entity mapping.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonymization.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass InstanceCounterDeanonymizer(Operator):\n    \"\"\"\n    Deanonymizer which replaces the unique identifier \n    with the original text.\n    \"\"\"\n\n    def operate(self, text: str, params: Dict = None) -> str:\n        \"\"\"Anonymize the input text.\"\"\"\n\n        entity_type: str = params[\"entity_type\"]\n\n        # entity_mapping is a dict of dicts containing mappings per entity type\n        entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]\n\n        if entity_type not in entity_mapping:\n            raise ValueError(f\"Entity type {entity_type} not found in entity mapping!\")\n        if text not in entity_mapping[entity_type].values():\n            raise ValueError(f\"Text {text} not found in entity mapping for entity type {entity_type}!\")\n\n        return self._find_key_by_value(entity_mapping[entity_type], text)\n\n    @staticmethod\n    def _find_key_by_value(entity_mapping, value):\n        for key, val in entity_mapping.items():\n            if val == value:\n                return key\n        return None\n    \n    def validate(self, params: Dict = None) -> None:\n        \"\"\"Validate operator parameters.\"\"\"\n\n        if \"entity_mapping\" not in params:\n            raise ValueError(\"An input Dict called `entity_mapping` is required.\")\n        if \"entity_type\" not in params:\n            raise ValueError(\"An entity_type param is required.\")\n\n    def operator_name(self) -> str:\n        return \"entity_counter_deanonymizer\"\n\n    def operator_type(self) -> OperatorType:\n        return OperatorType.Deanonymize\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running Presidio Anonymizer with Selective Keeping in Python\nDESCRIPTION: This code demonstrates how to configure the Presidio Anonymizer to keep person names but replace location names. It uses predefined analyzer results and custom operator configurations.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/keep_entities.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nengine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer)\n# and 'keep' operator on <PERSON> PIIs\nanonymize_result = engine.anonymize(\n    text=\"My name is James Bond, I live in London\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),\n        RecognizerResult(entity_type=\"LOCATION\", start=33, end=39, score=0.8),\n    ],\n    operators={\n        \"PERSON\": OperatorConfig(\"keep\"),\n        \"DEFAULT\": OperatorConfig(\"replace\"),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Referencing AnalyzerEngineProvider - Python\nDESCRIPTION: Imports the AnalyzerEngineProvider from presidio_analyzer.analyzer_engine_provider, used for supplying and managing AnalyzerEngine instances. It is a factory/provider pattern class facilitating dependency injection and reusability of AnalyzerEngine configurations. Must be used when multiple analyzers with different settings are required.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.analyzer_engine_provider.AnalyzerEngineProvider\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Anonymizing Text with Presidio Anonymizer in Python\nDESCRIPTION: This snippet demonstrates how to use the AnonymizerEngine to anonymize personal names in a text string. It initializes the engine, defines analyzer results, and applies a replace operator to anonymize the detected entities.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/anonymizer/index.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n\n# Initialize the engine:\nengine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text, \n# analyzer results (potentially coming from presidio-analyzer) and\n# Operators to get the anonymization output:\nresult = engine.anonymize(\n    text=\"My name is Bond, James Bond\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n        RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n    ],\n    operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})},\n)\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Anonymization Operators with Presidio in Python\nDESCRIPTION: Demonstrates how to define custom anonymization operators using OperatorConfig and Faker for replacing sensitive data in structured datasets.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer.entities.engine import OperatorConfig\nfrom faker import Faker\nfake = Faker()\n\noperators = {\n    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"person...\"}),\n    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})\n    # etc...\n    }\nanonymized_df = pandas_engine.anonymize(sample_df, tabular_analysis, operators=operators)\nanonymized_df\n```\n\n----------------------------------------\n\nTITLE: Anonymizing JSON Data with Presidio Structured - Python\nDESCRIPTION: Illustrates anonymizing PII in JSON-structured data using Presidio Structured's JSON data processor, with both simple and complex nested data. Requires presidio-structured, presidio-anonymizer, and faker libraries. Major steps include generating an analysis from JSON, defining operator configurations for entity types, and anonymizing using StructuredEngine. For complex nested lists, manual entity mapping is used since automatic analysis is not yet supported. The anonymize() method expects raw JSON, the analysis object, and designated operators, and returns anonymized JSON data.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/structured/index.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_structured import StructuredEngine, JsonAnalysisBuilder, StructuredAnalysis, JsonDataProcessor\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom faker import Faker # optionally using faker as an example\n\n# Initialize the engine with a JSON data processor\njson_engine = StructuredEngine(data_processor=JsonDataProcessor())\n\n\n# Sample JSON data\nsample_json = {\n    \"user\": {\n        \"name\": \"John Doe\",\n        \"email\": \"john.doe@example.com\"\n    }\n}\n\n# Generate analysis for simple JSON data\njson_analysis = JsonAnalysisBuilder().generate_analysis(sample_json)\n\n# Define anonymization operators\nfake = Faker() # using faker for email generation.\noperators = {\n    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"REDACTED\"}),\n    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})\n}\n\n# Anonymize JSON data\nanonymized_json = json_engine.anonymize(sample_json, json_analysis, operators=operators)\nprint(anonymized_json)\n\n# Handling Json Data with nested objects in lists\nsample_complex_json = {\n    \"users\": [\n        {\"name\": \"John Doe\", \"email\": \"john.doe@example.com\"},\n        {\"name\": \"Jane Smith\", \"email\": \"jane.smith@example.com\"}\n    ]\n}\n\n# Nesting objects in lists is not supported in JsonAnalysisBuilder for now,\n# Manually defining the analysis for complex JSON data\njson_complex_analysis = StructuredAnalysis(entity_mapping={\n    \"users.name\": \"PERSON\",\n    \"users.email\": \"EMAIL_ADDRESS\"\n})\n\n# Anonymize complex JSON data\nanonymized_complex_json = json_engine.anonymize(sample_complex_json, json_complex_analysis, operators=operators)\nprint(anonymized_complex_json)\n\n```\n\n----------------------------------------\n\nTITLE: Anonymizing Text via HTTP POST Request\nDESCRIPTION: This curl command demonstrates how to send an anonymization request to the Presidio Anonymizer HTTP server. It includes a JSON payload with the text to anonymize, anonymizer configurations, and analyzer results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/anonymizer/index.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XPOST http://localhost:3000/anonymize -H \"Content-Type: application/json\" -d @payload\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n\"text\": \"hello world, my name is Jane Doe. My number is: 034453334\",\n\"anonymizers\": {\n    \"PHONE_NUMBER\": {\n        \"type\": \"mask\",\n        \"masking_char\": \"*\",\n        \"chars_to_mask\": 4,\n        \"from_end\": true\n    }\n},\n\"analyzer_results\": [\n    {\n        \"start\": 24,\n        \"end\": 32,\n        \"score\": 0.8,\n        \"entity_type\": \"NAME\"\n    },\n    {\n        \"start\": 24,\n        \"end\": 28,\n        \"score\": 0.8,\n        \"entity_type\": \"FIRST_NAME\"\n    },\n    {\n        \"start\": 29,\n        \"end\": 32,\n        \"score\": 0.6,\n        \"entity_type\": \"LAST_NAME\"\n    },\n    {\n        \"start\": 48,\n        \"end\": 57,\n        \"score\": 0.95,\n        \"entity_type\": \"PHONE_NUMBER\"\n    }\n]}\n```\n\n----------------------------------------\n\nTITLE: Configuring Presidio Analyzer for English and Spanish Email Detection in Python\nDESCRIPTION: This Python code demonstrates initializing the Presidio AnalyzerEngine to support both English and Spanish. It involves creating an NlpEngine from a configuration file ('languages-config.yml'), setting up separate EmailRecognizer instances for English ('en') and Spanish ('es') with language-specific context words, adding these recognizers to a RecognizerRegistry, and finally configuring the AnalyzerEngine with the registry, supported languages, and the NLP engine. An example analysis call on English text is included. Dependencies include the presidio_analyzer library and a valid language configuration file.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/languages.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.predefined_recognizers import EmailRecognizer\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nLANGUAGES_CONFIG_FILE = \"./docs/analyzer/languages-config.yml\"\n\n# Create NLP engine based on configuration file\nprovider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Setting up an English Email recognizer:\nemail_recognizer_en = EmailRecognizer(supported_language=\"en\", context=[\"email\", \"mail\"])\n\n# Setting up a Spanish Email recognizer\nemail_recognizer_es = EmailRecognizer(supported_language=\"es\", context=[\"correo\", \"electrnico\"])\n\nregistry = RecognizerRegistry()\n\n# Add recognizers to registry\nregistry.add_recognizer(email_recognizer_en)\nregistry.add_recognizer(email_recognizer_es)\n\n# Set up analyzer with our updated recognizer registry\nanalyzer = AnalyzerEngine(\n    registry=registry,\n    supported_languages=[\"en\",\"es\"],\n    nlp_engine=nlp_engine_with_spanish)\n\nanalyzer.analyze(text=\"My name is David\", language=\"en\")\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Packages\nDESCRIPTION: Installs the Presidio Analyzer and Anonymizer packages using pip.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\n!pip install presidio_analyzer presidio_anonymizer\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text with Allow List in Presidio AnalyzerEngine - Python\nDESCRIPTION: Illustrates using the `allow_list` parameter in Presidio's AnalyzerEngine to exclude specified tokens (e.g., 'bing.com') from PII results. Relies on prior initialization of the AnalyzerEngine and definition of `text1`. The analysis omits allowed entries from PII matches, modifying the output accordingly. Requires the `presidio_analyzer` package. The input is the original text and a specified allow list; the output shows reduced PII recognition corresponding to the allow-list exclusion.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/13_allow_list.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\nresult = analyzer.analyze(text = text1, language = 'en', allow_list = [\"bing.com\"] )\nprint(f\"Result:\\n {result}\")\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Anonymizer to Text\nDESCRIPTION: This snippet demonstrates how to use the custom anonymizer with the AnonymizerEngine to anonymize the identified PII in the text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonymization.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create Anonymizer engine and add the custom anonymizer\nanonymizer_engine = AnonymizerEngine()\nanonymizer_engine.add_anonymizer(InstanceCounterAnonymizer)\n\n# Create a mapping between entity types and counters\nentity_mapping = dict()\n\n# Anonymize the text\n\nanonymized_result = anonymizer_engine.anonymize(\n    text,\n    analyzer_results,\n    {\n        \"DEFAULT\": OperatorConfig(\n            \"entity_counter\", {\"entity_mapping\": entity_mapping}\n        )\n    },\n)\n\nprint(anonymized_result.text)\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving Unified YAML Configuration\nDESCRIPTION: This snippet combines all YAML configurations into a single string and saves it as a temporary file for use with the AnalyzerEngineProvider.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/no_code_config.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfull_config = f\"{analyzer_config_yaml}\\n{recognizer_registry_config_yaml}\\n{nlp_engine_yaml}\"\n\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.yaml') as temp_file:\n    # Write the YAML string to the temp file\n    temp_file.write(full_config)\n    temp_file_path = temp_file.name\n```\n\n----------------------------------------\n\nTITLE: Handling Complex Nested JSON Analysis with Presidio in Python\nDESCRIPTION: Demonstrates error handling for complex nested JSON and shows how to manually define a StructuredAnalysis for such cases.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Currently does not support nested objects in lists\ntry:\n    json_complex_analysis = JsonAnalysisBuilder().generate_analysis(sample_complex_json)\nexcept ValueError as e:\n    print(e)\n\n# however, we can define it manually:\njson_complex_analysis = StructuredAnalysis(entity_mapping={\n    \"users.name\":\"PERSON\",\n    \"users.address.street\":\"LOCATION\",\n    \"users.address.city\":\"LOCATION\",\n    \"users.address.state\":\"LOCATION\",\n    \"users.email\": \"EMAIL_ADDRESS\",\n})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text with Combined Recognizers in Presidio\nDESCRIPTION: This snippet shows how to use the analyzer with both custom and predefined recognizers to identify entities in a sample text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nanalyzer.analyze(\"George Washington was the first president of the United States\", language=\"en\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text for URLs as PII with Presidio AnalyzerEngine - Python\nDESCRIPTION: Uses the Presidio AnalyzerEngine to detect PII in a sample text containing URLs. This snippet demonstrates the default recognition capabilities of Presidio, using built-in recognizers without specifying an allow list. Requires the `presidio_analyzer` package. The `analyze()` method is called with default parameters on the sample text. Outputs the detection result, including all detected PII entities.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/13_allow_list.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\ntext1 = \"My favorite website is bing.com, his is microsoft.com\"\nanalyzer = AnalyzerEngine()\nresult = analyzer.analyze(text = text1, language = 'en')\nprint(f\"Result: \\n {result}\")\n```\n\n----------------------------------------\n\nTITLE: Instantiating Text Analytics Recognizer in Python\nDESCRIPTION: This code creates an instance of the TextAnalyticsRecognizer with the specified Azure Text Analytics key, endpoint, and entity categories.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/integrating_with_external_services.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntext_analytics_recognizer = TextAnalyticsRecognizer(\n        text_analytics_key=\"<YOUR_TEXT_ANALYTICS_KEY>\",\n        text_analytics_endpoint=\"<YOUR_TEXT_ANALYTICS_ENDPOINT>\",\n        text_analytics_categories = ta_entities)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Large SpaCy Model for Enhanced PII Detection\nDESCRIPTION: Configures and initializes the Presidio Analyzer with a large SpaCy model (en_core_web_lg) for improved PII detection accuracy, and tests it on a sample text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [\n        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n    ]\n}\n\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine = provider.create_engine()\n\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine, supported_languages=[\"en\"]\n)\n\ntext_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\"\nanalyzer_results = analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')\n\nprint(analyzer_results)\n```\n\n----------------------------------------\n\nTITLE: Standard Image Redaction with Python\nDESCRIPTION: Python script demonstrating how to use the ImageRedactorEngine to redact PII from a standard image file.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/index.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nfrom presidio_image_redactor import ImageRedactorEngine\n\n# Get the image to redact using PIL lib (pillow)\nimage = Image.open(\"./docs/image-redactor/ocr_text.png\")\n\n# Initialize the engine\nengine = ImageRedactorEngine()\n\n# Redact the image with pink color\nredacted_image = engine.redact(image, (255, 192, 203))\n\n# save the redacted image \nredacted_image.save(\"new_image.png\")\n# uncomment to open the image for viewing\n# redacted_image.show()\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to Dictionary for Presidio Processing\nDESCRIPTION: This snippet demonstrates how to convert a Pandas DataFrame to a dictionary format suitable for Presidio batch processing.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create Pandas DataFrame\ndf  = pd.DataFrame(sample_data,columns=columns)\n\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\n# DataFrame to dict\ndf_dict = df.to_dict(orient=\"list\")\n```\n\nLANGUAGE: python\nCODE:\n```\npprint.pprint(df_dict)\n```\n\n----------------------------------------\n\nTITLE: Redacting PII from DICOM Images in Python\nDESCRIPTION: This Python snippet shows how to redact PII specifically from DICOM image files using `DicomImageRedactorEngine`. It demonstrates three options: redacting from a pre-loaded DICOM object using `pydicom`, redacting directly from a specified DICOM file, and redacting all DICOM files within a directory. Key parameters like `fill` (e.g., 'contrast') and `padding_width` control the appearance of the redacted areas.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_images.md#2025-04-23_snippet_3\n\nLANGUAGE: py\nCODE:\n```\nimport pydicom\nfrom presidio_image_redactor import DicomImageRedactorEngine\n\n# Set input and output paths\ninput_path = \"path/to/your/dicom/file.dcm\"\noutput_dir = \"./output\"\n\n# Initialize the engine\nengine = DicomImageRedactorEngine()\n\n# Option 1: Redact from a loaded DICOM image\ndicom_image = pydicom.dcmread(input_path)\nredacted_dicom_image = engine.redact(dicom_image, fill=\"contrast\")\n\n# Option 2: Redact from DICOM file\nengine.redact_from_file(input_path, output_dir, padding_width=25, fill=\"contrast\")\n\n# Option 3: Redact from directory\nengine.redact_from_directory(\"path/to/your/dicom\", output_dir, padding_width=25, fill=\"contrast\")\n```\n\n----------------------------------------\n\nTITLE: Anonymizing PII in Text with Presidio Anonymizer API\nDESCRIPTION: A curl command to send a POST request to the Presidio Anonymizer service to redact identified PII entities. This example applies different anonymization techniques: replacing the NAME entity with \"ANONYMIZED\" and masking the last 4 digits of the PHONE_NUMBER.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/docker/index.md#2025-04-23_snippet_1\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X POST http://localhost:5001/anonymize -H \"Content-type: application/json\" --data \"{\\\"text\\\": \\\"hello world, my name is Jane Doe. My number is: 034453334\\\", \\\"analyzer_results\\\": [{\\\"start\\\": 24, \\\"end\\\": 32, \\\"score\\\": 0.8, \\\"entity_type\\\": \\\"NAME\\\"}, { \\\"start\\\": 48, \\\"end\\\": 57,  \\\"score\\\": 0.95,\\\"entity_type\\\": \\\"PHONE_NUMBER\\\" }],  \\\"anonymizers\\\": {\\\"DEFAULT\\\": { \\\"type\\\": \\\"replace\\\", \\\"new_value\\\": \\\"ANONYMIZED\\\" },\\\"PHONE_NUMBER\\\": { \\\"type\\\": \\\"mask\\\", \\\"masking_char\\\": \\\"*\\\", \\\"chars_to_mask\\\": 4, \\\"from_end\\\": true }}}\"\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Analyzer and Anonymizer in Python\nDESCRIPTION: This snippet installs the Presidio Analyzer and Anonymizer packages, as well as the 'en_core_web_lg' spaCy language model, which is required for text analysis. Running this code is a prerequisite for using Presidio's functionalities in subsequent steps. Input: None. Output: Installs necessary packages in the Python environment.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/encrypt_decrypt.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\\n!pip install presidio_analyzer presidio_anonymizer\\n!python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Text Analysis with Transformers\nDESCRIPTION: Installs Presidio packages with Transformers NLP engine support and minimal spaCy model\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/installation.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"presidio_analyzer[transformers]\"\npip install presidio_anonymizer\npython -m spacy download en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Configuring OpenAI Client in Python\nDESCRIPTION: Imports necessary modules (`pprint`, `dotenv`, `os`, `pandas`, `openai`). It loads environment variables using `load_dotenv()` and initializes the OpenAI client (`OpenAI`) by retrieving the API key from the environment variable `OPENAI_API_KEY`. Requires a `.env` file or the environment variable to be set.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pprint\nfrom dotenv import load_dotenv\nimport os\nimport pandas as pd\nfrom openai import OpenAI\n\nload_dotenv()\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n#Or put explicitly in notebook. Find out more here: https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key\n```\n\n----------------------------------------\n\nTITLE: Ad-hoc Regex Recognizer JSON for /analyze API (Presidio, JSON)\nDESCRIPTION: This JSON payload defines an ad-hoc recognizer for the /analyze API in the Presidio Analyzer service. The recognizer is based on a regex pattern and is injected inline for a single analysis request. The entry includes the text, language, and an array of ad_hoc_recognizers specifying the pattern and detection context. Useful for one-off or temporary PII detection rules.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"John Smith drivers license is AC432223. Zip code: 10023\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Zip code Recognizer\",\n        \"supported_language\": \"en\",\n        \"patterns\": [\n            {\n            \"name\": \"zip code (weak)\", \n            \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\", \n            \"score\": 0.01\n            }\n        ],\n        \"context\": [\"zip\", \"code\"],\n        \"supported_entity\":\"ZIP\"\n        }\n    ]\n}\n\n```\n\n----------------------------------------\n\nTITLE: Adding 'keep' No-Op Anonymizer for PII Preservation (Python)\nDESCRIPTION: Introduces a new anonymizer operator named `keep` (#1062) added in version 2.2.33. This operator acts as a no-operation anonymizer, allowing users to explicitly preserve certain types of detected PII while still acknowledging their presence and location in the anonymized result.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nkeep\n```\n\n----------------------------------------\n\nTITLE: Defining Recognizer Registry Configuration in YAML\nDESCRIPTION: This snippet defines the Recognizer Registry parameters in YAML, including supported languages, global regex flags, and custom recognizers with context words for English and Spanish.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/no_code_config.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrecognizer_registry_config_yaml = \"\"\"\nrecognizer_registry:\n  supported_languages: \n  - en\n  - es\n  global_regex_flags: 26\n\n  recognizers:\n  - name: CreditCardRecognizer\n    supported_languages:\n    - language: en\n      context: [credit, card, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    - language: es\n      context: [tarjeta, credito, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    type: predefined\n    \n  - name: DateRecognizer\n    supported_languages:\n    - language: en\n      context: [date, time, birthday, birthdate, dob]\n    - language: es\n      context: [fecha, tiempo, hora, nacimiento, dob]\n    type: predefined\n\n  - name: EmailRecognizer\n    supported_languages:\n    - language: en\n      context: [email, mail, address]\n    - language: es\n      context: [correo, electrnico, email]\n    type: predefined\n    \n  - name: PhoneRecognizer\n    type: predefined\n    supported_languages:\n    - language: en\n      context: [phone, number, telephone, fax]\n    - language: es\n      context: [telfono, nmero, fax]\n    \n  - name: \"Titles recognizer (en)\"\n    supported_language: \"en\"\n    supported_entity: \"TITLE\"\n    deny_list:\n      - Mr.\n      - Mrs.\n      - Ms.\n      - Miss\n      - Dr.\n      - Prof.\n      - Doctor\n      - Professor\n  - name: \"Titles recognizer (es)\"\n    supported_language: \"es\"\n    supported_entity: \"TITLE\"\n    deny_list:\n      - Sr.\n      - Seor\n      - Sra.\n      - Seora\n      - Srta.\n      - Seorita\n      - Dr.\n      - Doctor\n      - Doctora\n      - Prof.\n      - Profesor\n      - Profesora\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting Regex Flags for PatternRecognizer and Registry (Presidio, Python)\nDESCRIPTION: This code example configures Python regex flags globally for recognizer logic in Presidio. By passing the global_regex_flags parameter to RecognizerRegistry, all recognizers added will inherit these regex matching flags (e.g., DOTALL, MULTILINE, IGNORECASE). Requires both presidio_analyzer and the regex package. It is suitable for advanced use-cases where custom pattern matching is needed for all entities.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nimport regex as re\n\nregistry = RecognizerRegistry(global_regex_flags=re.DOTALL | re.MULTILINE | re.IGNORECASE)\nengine = AnalyzerEngine(registry=registry)\nengine.analyze(...)\n\n```\n\n----------------------------------------\n\nTITLE: Integrating Text Analytics with Presidio Analyzer in Python\nDESCRIPTION: This snippet demonstrates how to add the Text Analytics recognizer to Presidio's AnalyzerEngine and use it to analyze text for PII entities.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/integrating_with_external_services.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(text_analytics_recognizer)\n\nresults = analyzer.analyze(\n    text=\"David is 30 years old. His IBAN: IL150120690000003111111\", language=\"en\"\n)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Applying a Custom Faker-based Anonymizer in Presidio (Python)\nDESCRIPTION: This snippet provides a full example of using a custom anonymization operator in Presidio. It defines the custom `fake_name` function and operator configuration (as in the previous snippet), sets up sample analyzer results identifying a 'PERSON' entity, initializes the `AnonymizerEngine`, and calls the `anonymize` method with the text, results, and the custom operators. The result demonstrates pseudonymization where the identified name is replaced by a fake name generated by Faker.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/11_custom_anonymization.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig, EngineResult, RecognizerResult\nfrom faker import Faker\n\n\nfake = Faker()\n\n# Create faker function (note that it has to receive a value)\ndef fake_name(x):\n    return fake.name()\n\n\n# Create custom operator for the PERSON entity\noperators = {\"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name})}\n\n# Analyzer output\nanalyzer_results = [RecognizerResult(entity_type=\"PERSON\", start=11, end=18, score=0.8)]\n\ntext_to_anonymize = \"My name is Raphael and I like to fish.\"\n\nanonymizer = AnonymizerEngine()\n\nanonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n)\n\nprint(anonymized_results.text)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for PII Annotation in PDFs using Python\nDESCRIPTION: This code imports the required libraries for PII annotation in PDFs, including Presidio components, PDF extraction and manipulation tools, and utility functions.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_pdf_annotation.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# For Presidio\nfrom presidio_analyzer import AnalyzerEngine, PatternRecognizer\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\n\n# For console output\nfrom pprint import pprint\n\n# For extracting text\nfrom pdfminer.high_level import extract_text, extract_pages\nfrom pdfminer.layout import LTTextContainer, LTChar, LTTextLine\n\n# For updating the PDF\nfrom pikepdf import Pdf, AttachedFileSpec, Name, Dictionary, Array\n```\n\n----------------------------------------\n\nTITLE: Creating Phrase Bounding Boxes for PII in PDF using Python\nDESCRIPTION: This code creates phrase bounding boxes for identified PII in the PDF. It combines individual character bounding boxes into a single bounding box for each PII entity.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_pdf_annotation.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Combine the bounding boxes into a single bounding box.\ndef combine_rect(rectA, rectB):\n    a, b = rectA, rectB\n    startX = min( a[0], b[0] )\n    startY = min( a[1], b[1] )\n    endX = max( a[2], b[2] )\n    endY = max( a[3], b[3] )\n    return (startX, startY, endX, endY)\n\nanalyzed_bounding_boxes = []\n\n# For each character set, combine the bounding boxes into a single bounding box.\nfor analyzed_character_set in analyzed_character_sets:\n    completeBoundingBox = analyzed_character_set[\"characters\"][0].bbox\n    \n    for character in analyzed_character_set[\"characters\"]:\n        completeBoundingBox = combine_rect(completeBoundingBox, character.bbox)\n    \n    analyzed_bounding_boxes.append({\"boundingBox\": completeBoundingBox, \"result\": analyzed_character_set[\"result\"]})\n```\n\n----------------------------------------\n\nTITLE: Configuring SpaCy NLP Engine with Custom Parameters\nDESCRIPTION: Sets up a SpaCy NLP engine with custom confidence scores and entity mappings.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define which model to use\nmodel_config = [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}]\n\n# Define which entities the model returns and how they map to Presidio's\nentity_mapping = dict(\n    PER=\"PERSON\",\n    LOC= \"LOCATION\",\n    GPE=\"LOCATION\",\n    ORG=\"ORGANIZATION\"\n)\n\nner_model_configuration = NerModelConfiguration(default_score = 0.6, \n                                                model_to_presidio_entity_mapping=entity_mapping)\n\n# Create the NLP Engine based on this configuration\nspacy_nlp_engine = SpacyNlpEngine(models= model_config, ner_model_configuration=ner_model_configuration)\n```\n\n----------------------------------------\n\nTITLE: Defining Cryptographic Key for Encryption/Decryption (Python)\nDESCRIPTION: Defines the symmetric cryptographic key used for both encryption and decryption with Presidio's AES implementation in CBC mode. The value assigned must be kept secret and secure, and is passed as a parameter when configuring encryption and decryption operators.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/12_encryption.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncrypto_key = \"WmZq4t7w!z%C&F)J\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Allow-List Functionality in Presidio AnalyzerEngine (Python)\nDESCRIPTION: Highlights the addition of allow-list functionality (version 2.2.29) within the `AnalyzerEngine` class (`presidio-analyzer/presidio_analyzer/analyzer_engine.py`, specifically around line 135 in commit 4cbfc1a). This feature enables users to provide a list of strings that Presidio should explicitly ignore during PII analysis.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\npresidio-analyzer/presidio_analyzer/analyzer_engine.py#L135\n```\n\n----------------------------------------\n\nTITLE: Directly Adding Recognizer to AnalyzerEngine's Registry (Presidio, Python)\nDESCRIPTION: This Python snippet shows an alternative approach: adding a recognizer directly to the AnalyzerEngine's registry after initialization. Useful for ad-hoc extension without customizing the RecognizerRegistry during setup. Assumes presidio_analyzer is installed and titles_recognizer is defined. The analyzed results are printed. Constraints: registry must exist as analyzer.registry.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\n\nanalyzer = AnalyzerEngine()\n\nanalyzer.registry.add_recognizer(titles_recognizer)\n\nresults = analyzer.analyze(text=text, language=\"en\")\nprint(results)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a NumbersRecognizer Using spaCy and Presidio in Python\nDESCRIPTION: This snippet defines NumbersRecognizer, a subclass of EntityRecognizer, to detect numbers (numeric or alphabetic words like 'One', 'Two') in text. It leverages spaCy's token.like_num property from NlpArtifacts to identify such tokens. A confidence level is set for result scoring. Dependencies: presidio_analyzer and spaCy for language processing. Inputs: text, entities, and NlpArtifacts; outputs: RecognizerResult list for detected numbers. It concludes by instantiating the recognizer to handle the 'NUMBER' entity.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/03_rule_based.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom presidio_analyzer import EntityRecognizer, RecognizerResult\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n\n\nclass NumbersRecognizer(EntityRecognizer):\n\n    expected_confidence_level = 0.7  # expected confidence level for this recognizer\n\n    def load(self) -> None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -> List[RecognizerResult]:\n        \"\"\"\n        Analyzes test to find tokens which represent numbers (either 123 or One Two Three).\n        \"\"\"\n        results = []\n\n        # iterate over the spaCy tokens, and call `token.like_num`\n        for token in nlp_artifacts.tokens:\n            if token.like_num:\n                result = RecognizerResult(\n                    entity_type=\"NUMBER\",\n                    start=token.idx,\n                    end=token.idx + len(token),\n                    score=self.expected_confidence_level,\n                )\n                results.append(result)\n        return results\n\n\n# Instantiate the new NumbersRecognizer:\nnew_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n\n```\n\n----------------------------------------\n\nTITLE: Generating Tabular Analysis for CSV Data with Presidio in Python\nDESCRIPTION: Automatically detects entities for columns in a pandas DataFrame using the PandasAnalysisBuilder from presidio_structured.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Automatically detect the entity for the columns\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df)\ntabular_analysis\n```\n\n----------------------------------------\n\nTITLE: Anonymizing Complex Nested JSON Data with Presidio in Python\nDESCRIPTION: Applies StructuredEngine with JsonDataProcessor to anonymize complex nested JSON data using manually defined analysis and custom operators.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nanonymized_complex_json = json_engine.anonymize(sample_complex_json, json_complex_analysis, operators=operators)\nanonymized_complex_json\n```\n\n----------------------------------------\n\nTITLE: Redacting Burnt-in Text in DICOM Images using Presidio DicomImageRedactorEngine - Python\nDESCRIPTION: This Python code illustrates the use of the DicomImageRedactorEngine from the presidio_image_redactor module to redact PII from DICOM medical images. It offers several options: redacting a loaded DICOM image, obtaining bounding boxes, mass file handling, and batch processing with OCR thresholds. Dependencies include pydicom, presidio-image-redactor, spaCy, and Tesseract OCR, and input DICOM files. Outputs include redacted DICOM files and optionally JSON with bounding box details.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pydicom\nfrom presidio_image_redactor import DicomImageRedactorEngine\n\n# Set input and output paths\ninput_path = \"path/to/your/dicom/file.dcm\"\noutput_dir = \"./output\"\n\n# Initialize the engine\nengine = DicomImageRedactorEngine()\n\n# Option 1: Redact from a loaded DICOM image\ndicom_image = pydicom.dcmread(input_path)\nredacted_dicom_image = engine.redact(dicom_image, fill=\"contrast\")\n\n# Option 2: Redact from a loaded DICOM image and return redacted regions\nredacted_dicom_image, bboxes = engine.redact_and_return_bbox(dicom_image, fill=\"contrast\")\n\n# Option 3: Redact from DICOM file and save redacted regions as json file\nengine.redact_from_file(input_path, output_dir, padding_width=25, fill=\"contrast\", save_bboxes=True)\n\n# Option 4: Redact from directory and save redacted regions as json files\nocr_kwargs = {\"ocr_threshold\": 50}\nengine.redact_from_directory(\"path/to/your/dicom\", output_dir, fill=\"background\", save_bboxes=True, ocr_kwargs=ocr_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Small SpaCy Model for Presidio Analyzer\nDESCRIPTION: Configures and initializes the Presidio Analyzer with a medium-sized SpaCy model (en_core_web_md) for PII detection, and demonstrates basic usage with a sample text analysis.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [\n        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_md\"},\n    ]\n}\n\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine = provider.create_engine()\n\nsmall_analyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine, supported_languages=[\"en\"]\n)\n\ntext_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\"\nanalyzer_results = small_analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')\nprint(analyzer_results)\n```\n\n----------------------------------------\n\nTITLE: Adding a Custom Recognizer to Presidio AnalyzerEngine in Python\nDESCRIPTION: This code imports and initializes the `AnalyzerEngine` from `presidio_analyzer`. It then uses the `add_recognizer` method of the engine's registry to include the custom `titles_recognizer` (created earlier with the deny-list) in the analysis process. This prepares the engine for analyzing text using both default and custom recognizers.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/01_deny_list.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom presidio_analyzer import AnalyzerEngine\n\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(titles_recognizer)\n```\n```\n\n----------------------------------------\n\nTITLE: Introducing BatchAnonymizerEngine for Batch Anonymization (Python)\nDESCRIPTION: Introduces the `BatchAnonymizerEngine` Python class (#993), added in version 2.2.33. This engine is designed to perform anonymization on multiple inputs (lists, dicts) in batches, complementing the `BatchAnalyzerEngine` for efficient bulk processing.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nBatchAnonymizerEngine\n```\n\n----------------------------------------\n\nTITLE: NLP Engine Configuration via YAML\nDESCRIPTION: YAML configuration example for setting up NLP models with custom entity mappings and confidence settings in Presidio Analyzer.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/customizing_nlp_models.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nnlp_engine_name: spacy\nmodels:\n    -\n    lang_code: en\n    model_name: en_core_web_lg\n    -\n    lang_code: es\n    model_name: es_core_news_md \nner_model_configuration:\nlabels_to_ignore:\n- O\nmodel_to_presidio_entity_mapping:\n    PER: PERSON\n    LOC: LOCATION\n    ORG: ORGANIZATION\n    AGE: AGE\n    ID: ID\n    DATE: DATE_TIME\nlow_confidence_score_multiplier: 0.4\nlow_score_entity_names:\n- ID\n- ORG\n```\n\n----------------------------------------\n\nTITLE: Analyzing Spanish Text with Custom AnalyzerEngine\nDESCRIPTION: This snippet demonstrates the use of the custom-configured AnalyzerEngine to analyze a Spanish text sample.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/no_code_config.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nes_text = \"Hola, me llamo David Johnson y soy originalmente de Liverpool. Mi nmero de tarjeta de crdito es 4095260993934932\"\nanalyzer_engine.analyze(es_text, language=\"es\")\n```\n\n----------------------------------------\n\nTITLE: Redacting PII from Standard Images in Python\nDESCRIPTION: This Python snippet demonstrates how to redact PII from a standard image file. It imports necessary classes (`ImageRedactorEngine` from `presidio_image_redactor` and `Image` from `PIL`), loads an image, initializes the redaction engine, and then calls the `redact` method to process the image in place or return a redacted version.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_images.md#2025-04-23_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nfrom presidio_image_redactor import ImageRedactorEngine\nfrom PIL import Image\n\nimage = Image.open(path_to_image_file)\n\nredactor = ImageRedactorEngine()\nredactor.redact(image=image)\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Call OpenAI Chat Completion API in Python\nDESCRIPTION: Defines a function `call_completion_model` that takes a prompt string, an optional model name (defaulting to 'gpt-3.5-turbo'), and max tokens (defaulting to 512). It uses the pre-configured `client` object to call the OpenAI Chat Completions API (`client.chat.completions.create`) and returns the generated text content from the model's response.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef call_completion_model(prompt:str, model:str=\"gpt-3.5-turbo\", max_tokens:int=512) ->str:\n    \"\"\"Creates a request for the OpenAI Completion service and returns the response.\n    \n    :param prompt: The prompt for the completion model\n    :param model: OpenAI model name\n    :param max_tokens: Model's max tokens parameter\n    \"\"\"\n\n    completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n        }\n    ],\n    model=model,\n)\n\n    return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Processing Nested JSON Data with Presidio\nDESCRIPTION: This code demonstrates how to use Presidio to analyze and anonymize nested JSON data, including options for ignoring specific keys.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnested_dict = {\n    \"key_a\": {\"key_a1\": \"My phone number is 212-121-1424\"},\n    \"key_b\": {\"www.abc.com\"},\n    \"key_c\": 3,\n    \"names\": [\"James Bond\", \"Clark Kent\", \"Hakeem Olajuwon\", \"No name here!\"]\n}\n\npprint.pprint(nested_dict)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Analyze dict\nanalyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\")\n\n# Anonymize dict\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\npprint.pprint(anonymizer_results)\n```\n\nLANGUAGE: python\nCODE:\n```\nkeys_to_skip=[\"key_a1\", \"names\"]\nanalyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\", keys_to_skip=keys_to_skip)\n\n# Anonymize dict\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\npprint.pprint(anonymizer_results)\n```\n\nLANGUAGE: python\nCODE:\n```\nkeys_to_skip = [\"key_a.key_a1\"]\n\nanalyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\", keys_to_skip=keys_to_skip)\n\n# Anonymize dict\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\npprint.pprint(anonymizer_results)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Rule-Based Logic Recognizer for Numbers\nDESCRIPTION: Creates a custom EntityRecognizer that uses spaCy's token attributes to detect numbers in both numerical and alphabetic forms.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass NumbersRecognizer(EntityRecognizer):\n\n    expected_confidence_level = 0.7  # expected confidence level for this recognizer\n\n    def load(self) -> None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -> List[RecognizerResult]:\n        \"\"\"\n        Analyzes test to find tokens which represent numbers (either 123 or One Two Three).\n        \"\"\"\n        results = []\n\n        # iterate over the spaCy tokens, and call `token.like_num`\n        for token in nlp_artifacts.tokens:\n            if token.like_num:\n                result = RecognizerResult(\n                    entity_type=\"NUMBER\",\n                    start=token.idx,\n                    end=token.idx + len(token),\n                    score=self.expected_confidence_level,\n                )\n                results.append(result)\n        return results\n```\n\n----------------------------------------\n\nTITLE: Defining Regex Ad-hoc Recognizer - Presidio Analyzer API - JSON\nDESCRIPTION: Specifies a sample JSON payload to define an ad-hoc recognizer for ZIP codes using a regular expression within the Presidio Analyzer API. This snippet requires Presidio to be already deployed and demonstrates use of the \"ad_hoc_recognizers\" field to include a pattern with a custom entity and language context. The input includes the main text, language, recognizer definition (with a score and regex), and is expected as part of a request to the /analyze endpoint. The primary output is enhanced detection logic for the supplied request only.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/09_ad_hoc.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"John Smith drivers license is AC432223. Zip code: 10023\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Zip code Recognizer\",\n        \"supported_language\": \"en\",\n        \"patterns\": [\n            {\n            \"name\": \"zip code (weak)\", \n            \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\", \n            \"score\": 0.01\n            }\n        ],\n        \"context\": [\"zip\", \"code\"],\n        \"supported_entity\":\"ZIP\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Custom Number Recognizer with Presidio in Python\nDESCRIPTION: This snippet tests the previously defined regular expression recognizer by analyzing an example text for numeric entities. The recognize object analyzes the input and prints detected results. Prerequisites involve the presence of a fully initialized PatternRecognizer as previously described, and the presidio_analyzer package.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/02_regex.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntext2 = \"I live in 510 Broad st.\"\n\nnumbers_result = number_recognizer.analyze(text=text2, entities=[\"NUMBER\"])\n\nprint(\"Result:\")\nprint(numbers_result)\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Analyzer with GLiNER Support\nDESCRIPTION: This command installs the Presidio Analyzer package with the GLiNER extra, which is required to use GLiNER within Presidio. Note that GLiNER requires Python 3.10 or above.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/gliner.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'presidio-analyzer[gliner]'\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Text Analysis with spaCy\nDESCRIPTION: Installs core Presidio packages for text analysis with spaCy NLP engine\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/installation.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install presidio_analyzer\npip install presidio_anonymizer\npython -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Configuration from File\nDESCRIPTION: Example demonstrating how to load a custom NLP engine configuration from a YAML file and initialize the Analyzer with multiple language support.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/customizing_nlp_models.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nLANGUAGES_CONFIG_FILE = \"./docs/analyzer/languages-config.yml\"\n\n# Create NLP engine based on configuration file\nprovider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, \n    supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es David\", language=\"es\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is David\", language=\"en\")\nprint(results_english)\n```\n\n----------------------------------------\n\nTITLE: Anonymizing Pandas DataFrames with Presidio Structured - Python\nDESCRIPTION: Shows how to use Presidio Structured to anonymize PII in pandas DataFrames by generating analyses and applying operator configurations. Requires pandas, presidio-structured, presidio-anonymizer, and faker as dependencies. Key steps include creating a DataFrame, generating an analysis with PandasAnalysisBuilder, setting up anonymization operators for specific PII entities, and transforming the data. The anonymize() function takes the raw data, analysis object, and operator mappings, returning a new DataFrame with de-identified values. Limitations include detection coverage and the need for correct operator definitions.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/structured/index.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom presidio_structured import StructuredEngine, PandasAnalysisBuilder\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom faker import Faker # optionally using faker as an example\n\n# Initialize the engine with a Pandas data processor (default)\npandas_engine = StructuredEngine()\n\n# Create a sample DataFrame\nsample_df = pd.DataFrame({'name': ['John Doe', 'Jane Smith'], 'email': ['john.doe@example.com', 'jane.smith@example.com']})\n\n# Generate a tabular analysis which describes PII entities in the DataFrame.\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df)\n\n# Define anonymization operators\nfake = Faker()\noperators = {\n    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"REDACTED\"}),\n    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})\n}\n\n# Anonymize DataFrame\nanonymized_df = pandas_engine.anonymize(sample_df, tabular_analysis, operators=operators)\nprint(anonymized_df)\n\n```\n\n----------------------------------------\n\nTITLE: Generating and Printing the OpenAI Prompt in Python\nDESCRIPTION: Demonstrates using the `create_prompt` function. It passes the `anonymized_text` (generated previously with Presidio) to `create_prompt` and prints the resulting detailed prompt string intended for the OpenAI API.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(\"This is the prompt with de-identified values:\")\nprint(create_prompt(anonymized_text))\n```\n\n----------------------------------------\n\nTITLE: Outputting Extracted PII Entities with Presidio in Python\nDESCRIPTION: This snippet extracts the text, start, and end positions of each detected PII entity from the anonymized results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[(item.text, item.start, item.end) for item in anonymized_results.items]\n```\n\n----------------------------------------\n\nTITLE: Defining load Method for Custom LocalRecognizer (Presidio, Python)\nDESCRIPTION: This declaration outlines the required signature for a load method in a Python class inheriting from LocalRecognizer. The function is meant to initialize or load external resources or ML models necessary for the recognizer's operation. Typically, no parameters are expected and initialization state is internal.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef load(self)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Presidio Analyzer and Anonymizer in Python\nDESCRIPTION: This snippet initializes the Presidio Analyzer with a custom registry and creates an Anonymizer engine for text processing.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nregistry = RecognizerRegistry()\nregistry.add_recognizer(deny_list_recognizer)\n\nanalyzer = AnalyzerEngine(registry=registry)\n\nanonymizer = AnonymizerEngine()\n```\n\n----------------------------------------\n\nTITLE: Adding Highlight Annotations for PII in PDF using Python\nDESCRIPTION: This snippet adds highlight annotations to the PDF for each identified PII entity. It creates a highlight annotation for each bounding box and saves the annotated PDF.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_pdf_annotation.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npdf = Pdf.open(\"./sample_data/sample.pdf\")\n\nannotations = []\n\n# Create a highlight annotation for each bounding box.\nfor analyzed_bounding_box in analyzed_bounding_boxes:\n\n    boundingBox = analyzed_bounding_box[\"boundingBox\"]\n\n    # Create the annotation. \n    # We could also create a redaction annotation if the ongoing workflows supports them.\n    highlight = Dictionary(\n        Type=Name.Annot,\n        Subtype=Name.Highlight,\n        QuadPoints=[boundingBox[0], boundingBox[3],\n                    boundingBox[2], boundingBox[3],\n                    boundingBox[0], boundingBox[1],\n                    boundingBox[2], boundingBox[1]],\n        Rect=[boundingBox[0], boundingBox[1], boundingBox[2], boundingBox[3]],\n        C=[1, 0, 0],\n        CA=0.5,\n        T=analyzed_bounding_box[\"result\"].entity_type,\n    )\n    \n    annotations.append(highlight)\n\n# Add the annotations to the PDF.\npdf.pages[0].Annots = pdf.make_indirect(annotations)\n\n# And save.\npdf.save(\"./sample_data/sample_annotated.pdf\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text for PII with Presidio Analyzer API\nDESCRIPTION: A curl command to send a POST request to the Presidio Analyzer service to detect PII entities in text. This example identifies a person's name and drivers license in English text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/docker/index.md#2025-04-23_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X POST http://localhost:5002/analyze -H \"Content-type: application/json\" --data \"{ \\\"text\\\": \\\"John Smith drivers license is AC432223\\\", \\\"language\\\" : \\\"en\\\"}\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Deny-List for Known Values in Python\nDESCRIPTION: This snippet creates a list of known names to be used as a deny-list for anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Get known values as a deny-list\nknown_names_list = [\"George\", \"Abraham\", \"Theodore\", \"Bill\", \"Barack\", \"Donald\", \"Joe\"]\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained Transformers Model in Python\nDESCRIPTION: This snippet demonstrates how to download a pre-trained NER model from HuggingFace and instantiate the necessary components.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/nlp_engines/transformers.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\nfrom huggingface_hub import snapshot_download\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntransformers_model = <PATH_TO_MODEL> # e.g. \"obi/deid_roberta_i2b2\"\n\nsnapshot_download(repo_id=transformers_model)\n\n# Instantiate to make sure it's downloaded during installation and not runtime\nAutoTokenizer.from_pretrained(transformers_model)\nAutoModelForTokenClassification.from_pretrained(transformers_model)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using AzureAILanguageRecognizer in Presidio (Python)\nDESCRIPTION: This Python code demonstrates how to integrate the Azure AI Language service with Presidio. It imports the `AnalyzerEngine` and `AzureAILanguageRecognizer`, instantiates the recognizer, adds it to the analyzer's registry, and then calls the `analyze` method to process text. This setup requires the `AZURE_AI_KEY` and `AZURE_AI_ENDPOINT` environment variables to be set for authentication with the Azure service.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/text_analytics/index.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.predefined_recognizers import AzureAILanguageRecognizer\n  \nazure_ai_language = AzureAILanguageRecognizer()\n\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(azure_ai_language)\n\nanalyzer.analyze(text=\"My email is email@email.com\", language=\"en\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Imports for Presidio Anonymizer (Python)\nDESCRIPTION: Imports all necessary Presidio Anonymizer and entity classes required for encryption and decryption operations. This includes core engine classes as well as entity wrappers and operator configurations. All subsequent code depends on these imports being available in the Python environment.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/12_encryption.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine\\nfrom presidio_anonymizer.entities import (\\n    RecognizerResult,\\n    OperatorResult,\\n    OperatorConfig,\\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Presidio Analyzer with Multi-Language spaCy Models\nDESCRIPTION: This Python snippet demonstrates configuring the Presidio Analyzer to support both English and Spanish using spaCy models. It initializes an `NlpEngineProvider` with specific spaCy models (`es_core_news_md` for Spanish, `en_core_web_lg` for English), creates an `NlpEngine`, and then instantiates an `AnalyzerEngine` with this custom engine and the list of supported languages. Finally, it shows how to call the `analyze` method for text in both Spanish and English, printing the respective results. It requires the `presidio-analyzer` library and the specified spaCy models to be downloaded (`python -m spacy download es_core_news_md`, `python -m spacy download en_core_web_lg`).\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/05_languages.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\n# import spacy\n# spacy.cli.download(\"es_core_news_md\")\n\n# Create configuration containing engine name and models\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [\n        {\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n    ],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\")\nprint(\"Results from Spanish request:\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(\"Results from English request:\")\nprint(results_english)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Presidio and PySpark\nDESCRIPTION: Imports necessary modules and functions from PySpark, Presidio Analyzer, and Presidio Anonymizer to support PII detection and anonymization operations on Spark DataFrames.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import (\n    array, lit, explode, col, monotonically_increasing_id, concat\n)\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nfrom presidio_anonymizer import AnonymizerEngine\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom presidio_anonymizer.entities import OperatorConfig\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Referencing AnalyzerRequest - Python\nDESCRIPTION: Imports AnalyzerRequest from presidio_analyzer.analyzer_request, the canonical structure for formulating and transmitting analyzer input requests. Used for submitting text or batch data for analysis, with schema validation and serialization considerations.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.analyzer_request.AnalyzerRequest\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Language Transformers NLP Engine\nDESCRIPTION: Sets up a Transformers NLP engine supporting multiple languages (English and Spanish).\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Define which model to use\nmodel_config = [{\n   \"lang_code\":\"en\",\n   \"model_name\":{\n      \"spacy\":\"en_core_web_sm\",\n      \"transformers\":\"obi/deid_roberta_i2b2\"\n   }\n},\n{\n    \"lang_code\":\"es\",\n    \"model_name\":{\n      \"spacy\":\"es_core_news_sm\",\n      \"transformers\":\"PlanTL-GOB-ES/roberta-large-bne-capitel-ner\"\n   }\n}]\n\ntransformers_nlp_engine = TransformersNlpEngine(models=model_config,\n                                                ner_model_configuration=ner_model_configuration)\n```\n\n----------------------------------------\n\nTITLE: Presidio Analyzer Configuration File Structure (YAML)\nDESCRIPTION: This YAML snippet shows the basic structure of the configuration file used to define custom recognizers in Presidio Analyzer. It includes global regex flags and supported languages.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/recognizer_registry_provider.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nglobal_regex_flags: 26\n\nsupported_languages: \n  - en\n\nrecognizers: \n...\n```\n\n----------------------------------------\n\nTITLE: Implementing PII Detection across DataFrame Columns\nDESCRIPTION: Creates a user-defined function to detect PII in each column of the DataFrame and generates a summary of identified entities. The function analyzes each column separately to maintain context of which PII belongs to which column.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef detect_pii_in_row(*cols):\n    \"\"\"\n    Analyze each column separately so we know which substring (entity text) \n    belongs to which column. Return a dict {col_name: [ 'ENTITY_TYPE: substring', ... ] }.\n    \"\"\"\n    analyzer = broadcasted_analyzer.value\n    col_names = detect_pii_in_row.col_names\n    entities_found = {}\n\n    for idx, val in enumerate(cols):\n        if val is None:\n            continue\n        column_text = str(val)\n        results = analyzer.analyze(text=column_text, language=\"en\")\n\n        if results:\n            # Example: [\"PERSON: John Doe\", \"PHONE_NUMBER: 212-555-1111\", ...]\n            found_entities = []\n            for res in results:\n                substring = column_text[res.start:res.end]  # The actual text recognized\n                entity_str = f\"{res.entity_type}: {substring}\"\n                found_entities.append(entity_str)\n            \n            entities_found[col_names[idx]] = found_entities\n\n    # If no PII was detected at all\n    if not entities_found:\n        return \"No PII\"\n    return str(entities_found)\n\ndetect_pii_in_row.col_names = df.columns\ndetect_pii_udf = udf(detect_pii_in_row, StringType())\n\ndf_with_pii_summary = df.withColumn(\n    \"pii_summary\",\n    detect_pii_udf(*[col(c) for c in df.columns])\n)\n\ndisplay(df_with_pii_summary)\n# df_with_pii_summary.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Analyzer Configuration in YAML\nDESCRIPTION: This snippet defines the general Analyzer parameters in YAML format, including supported languages and default score threshold.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/no_code_config.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nanalyzer_config_yaml = \"\"\"\nsupported_languages: \n  - en\n  - es\ndefault_score_threshold: 0.4\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Adding YAML Recognizers to Predefined Set - Presidio Analyzer - Python\nDESCRIPTION: Shows how to augment Presidio Analyzer's built-in recognizer registry by loading user-defined recognizers from a YAML file. Depends on Presidio Analyzer and requires a YAML file containing recognizer definitions. Starts with the standard registry and uses load_predefined_recognizers() for defaults, followed by add_recognizers_from_yaml() to add custom recognizers. The AnalyzerEngine is used to analyze text with the expanded set of recognizers. Input YAML must conform to expected schemas.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nyaml_file = \"recognizers.yaml\"\nregistry = RecognizerRegistry()\nregistry.load_predefined_recognizers()\n\nregistry.add_recognizers_from_yaml(yaml_file)\n\nanalyzer = AnalyzerEngine()\nanalyzer.analyze(text=\"Mr. and Mrs. Smith\", language=\"en\")\n```\n\n----------------------------------------\n\nTITLE: Defining NLP Engine Configuration in YAML\nDESCRIPTION: This snippet defines the NLP Engine parameters in YAML, including the engine name, language models, and entity mapping configurations for both English and Spanish.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/no_code_config.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnlp_engine_yaml = \"\"\"\nnlp_configuration:\n    nlp_engine_name: transformers\n    models:\n      -\n        lang_code: en\n        model_name:\n          spacy: en_core_web_sm\n          transformers: StanfordAIMI/stanford-deidentifier-base\n      -\n        lang_code: es\n        model_name:\n          spacy: es_core_news_sm\n          transformers: MMG/xlm-roberta-large-ner-spanish  \n    ner_model_configuration:\n      labels_to_ignore:\n      - O\n      aggregation_strategy: first # \"simple\", \"first\", \"average\", \"max\"\n      stride: 16\n      alignment_mode: expand # \"strict\", \"contract\", \"expand\"\n      model_to_presidio_entity_mapping:\n        PER: PERSON\n        PERSON: PERSON\n        LOC: LOCATION\n        LOCATION: LOCATION\n        GPE: LOCATION\n        ORG: ORGANIZATION\n        ORGANIZATION: ORGANIZATION\n        NORP: NRP\n        AGE: AGE\n        ID: ID\n        EMAIL: EMAIL\n        PATIENT: PERSON\n        STAFF: PERSON\n        HOSP: ORGANIZATION\n        PATORG: ORGANIZATION\n        DATE: DATE_TIME\n        TIME: DATE_TIME\n        PHONE: PHONE_NUMBER\n        HCW: PERSON\n        HOSPITAL: LOCATION\n        FACILITY: LOCATION\n        VENDOR: ORGANIZATION\n        MISC: ID\n    \n      low_confidence_score_multiplier: 0.4\n      low_score_entity_names:\n      - ID\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Referencing PatternRecognizer - Python\nDESCRIPTION: Imports the PatternRecognizer class from presidio_analyzer.pattern_recognizer, a key class for building new recognizers based on regex or pattern logic. Essential for quickly assembling new entity recognizers using reusable and extensible regular expressions.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.pattern_recognizer.PatternRecognizer\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Comparing Analyzer API Requests in Legacy gRPC (V1) and HTTP-based (V2) formats\nDESCRIPTION: This snippet illustrates the difference between the legacy gRPC-based request format and the new HTTP-based JSON request format for the Presidio Analyzer service. The V2 format is flatter, uses snake_case naming, and removes template structures.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/presidio_V2.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"My phone number is 212-555-5555\",\n    \"AnalyzeTemplateId\": \"1234\",\n    \"AnalyzeTemplate\": {\n        \"Fields\": [\n            {\n                \"Name\": \"PHONE_NUMBER\",\n                \"MinScore\": \"0.5\"\n            }\n        ],\n        \"AllFields\": true,\n        \"Description\": \"template description\",\n        \"CreateTime\": \"template creation time\",\n        \"ModifiedTime\": \"template modification time\",\n        \"Language\": \"fr\",\n        \"ResultsScoreThreshold\": 0.5\n    }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"My phone number is 212-555-5555\",\n    \"entities\": [\"PHONE_NUMBER\"],\n    \"language\": \"en\",\n    \"correlation_id\": \"213\",\n    \"score_threshold\": 0.5,\n    \"trace\": true,\n    \"return_decision_process\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Simple JSON Data with Presidio in Python\nDESCRIPTION: Reads a simple JSON file containing structured data using the JsonReader class from presidio_structured.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsample_json = JsonReader().read(\"./sample_data/test_structured.json\")\nsample_json\n```\n\n----------------------------------------\n\nTITLE: Defining Text Analytics Entities in Python\nDESCRIPTION: This snippet defines a list of Text Analytics entity categories to be used for PII recognition, including Person, Age, and IBAN.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/integrating_with_external_services.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nta_entities = [\n    TextAnalyticsEntityCategory(name=\"Person\",\n                                entity_type=\"NAME\",\n                                supported_languages=[\"en\"]),\n    TextAnalyticsEntityCategory(name=\"Age\",\n                                entity_type=\"AGE\",\n                                subcategory = \"Age\", \n                                supported_languages=[\"en\"]),\n    TextAnalyticsEntityCategory(name=\"InternationlBankingAccountNumber\",\n                                entity_type=\"IBAN\",\n                                supported_languages=[\"en\"])]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text for PII using AnalyzerEngine\nDESCRIPTION: This snippet demonstrates how to use the AnalyzerEngine to identify PII in a given text sample.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonymization.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntext = \"Peter gave his book to Heidi which later gave it to Nicole. Peter lives in London and Nicole lives in Tashkent.\"\nprint(\"original text:\")\npprint(text)\nanalyzer = AnalyzerEngine()\nanalyzer_results = analyzer.analyze(text=text, language=\"en\")\nprint(\"analyzer results:\")\npprint(analyzer_results)\n```\n\n----------------------------------------\n\nTITLE: Referencing predefined_recognizers Module - Python\nDESCRIPTION: Lists the presidio_analyzer.predefined_recognizers module which includes a suite of out-of-the-box entity recognizers. These are generally ready-to-use and require minimal configuration for standard PII types.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.predefined_recognizers\n```\n\n----------------------------------------\n\nTITLE: Configuring NER Pipeline in Python\nDESCRIPTION: This snippet shows how to configure the NER pipeline using Python, including model configuration, entity mappings, and analyzer setup.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/nlp_engines/transformers.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Transformer model config\nmodel_config = [\n    {\"lang_code\": \"en\",\n     \"model_name\": {\n         \"spacy\": \"en_core_web_sm\", # for tokenization, lemmatization\n         \"transformers\": \"StanfordAIMI/stanford-deidentifier-base\" # for NER\n    }\n}]\n\n# Entity mappings between the model's and Presidio's\nmapping = dict(\n    PER=\"PERSON\",\n    LOC=\"LOCATION\",\n    ORG=\"ORGANIZATION\",\n    AGE=\"AGE\",\n    ID=\"ID\",\n    EMAIL=\"EMAIL\",\n    DATE=\"DATE_TIME\",\n    PHONE=\"PHONE_NUMBER\",\n    PERSON=\"PERSON\",\n    LOCATION=\"LOCATION\",\n    GPE=\"LOCATION\",\n    ORGANIZATION=\"ORGANIZATION\",\n    NORP=\"NRP\",\n    PATIENT=\"PERSON\",\n    STAFF=\"PERSON\",\n    HOSP=\"LOCATION\",\n    PATORG=\"ORGANIZATION\",\n    TIME=\"DATE_TIME\",\n    HCW=\"PERSON\",\n    HOSPITAL=\"LOCATION\",\n    FACILITY=\"LOCATION\",\n    VENDOR=\"ORGANIZATION\",\n)\n\nlabels_to_ignore = [\"O\"]\n\nner_model_configuration = NerModelConfiguration(\n    model_to_presidio_entity_mapping=mapping,\n    alignment_mode=\"expand\", # \"strict\", \"contract\", \"expand\"\n    aggregation_strategy=\"max\", # \"simple\", \"first\", \"average\", \"max\"\n    labels_to_ignore = labels_to_ignore)\n\ntransformers_nlp_engine = TransformersNlpEngine(\n    models=model_config,\n    ner_model_configuration=ner_model_configuration)\n\n# Transformer-based analyzer\nanalyzer = AnalyzerEngine(\n    nlp_engine=transformers_nlp_engine, \n    supported_languages=[\"en\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Installing SpaCy Large Model from Lakehouse\nDESCRIPTION: Installs the large SpaCy language model (en_core_web_lg) from a wheel file stored in the Lakehouse. This model provides enhanced language processing capabilities for more accurate PII detection.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%pip install /lakehouse/default/Files/presidio/models/en_core_web_lg-3.8.0-py3-none-any.whl  \n# Installing the large model from the lakehouse as it exceeds the size limit for custom libraries in the Fabric environment.\n```\n\n----------------------------------------\n\nTITLE: Calling the OpenAI Model with the Generated Prompt in Python\nDESCRIPTION: Executes the synthetic data generation step. It calls `create_prompt` with `anonymized_text` to get the prompt, then passes this prompt to `call_completion_model` to invoke the OpenAI API. The LLM's response (synthetic text) is stored in `gpt_res`.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ngpt_res = call_completion_model(create_prompt(anonymized_text))\n```\n\n----------------------------------------\n\nTITLE: Introducing ContextAwareEnhancer for Contextual Detection Logic (Python)\nDESCRIPTION: Introduces the `ContextAwareEnhancer` base class (`presidio-analyzer/presidio_analyzer/context_aware_enhancers/context_aware_enhancer.py`), added in version 2.2.25. This class provides a foundation for implementing custom logic that leverages surrounding text (context) to improve the accuracy and confidence score of PII detection.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\npresidio-analyzer/presidio_analyzer/context_aware_enhancers/context_aware_enhancer.py\n```\n\n----------------------------------------\n\nTITLE: Extracting Anonymized Data from Presidio Result (Python)\nDESCRIPTION: Fetches the anonymized text and list of anonymized entities from the result returned by the anonymizer engine. This code is used to access output fields required for subsequent decryption or entity mapping steps.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/12_encryption.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Fetch the anonymized text from the result.\\nanonymized_text = anonymize_result.text\\n\\n# Fetch the anonynized entities from the result.\\nanonymized_entities = anonymize_result.items\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Docker Containers\nDESCRIPTION: Shell commands to run the Presidio analyzer and anonymizer containers as detached services, mapping their default ports to the host machine.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_text.md#2025-04-23_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ndocker run -d -p 5002:3000 mcr.microsoft.com/presidio-analyzer:latest\n\ndocker run -d -p 5001:3000 mcr.microsoft.com/presidio-anonymizer:latest\n```\n\n----------------------------------------\n\nTITLE: Defining analyze Method for Custom LocalRecognizer (Presidio, Python)\nDESCRIPTION: This function signature specifies the interface for the analyze method in a Presidio LocalRecognizer. It processes input text, receives a list of target entity types, and NLP artifacts (tokens, lemmas, etc.), and returns recognized entities. Return value should be a list of RecognizerResult instances. Prerequisites: must subclass LocalRecognizer, have loaded necessary NLP assets, and implement this function.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef analyze(self, text, entities, nlp_artifacts)\n\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Function for Analyzer Execution and Result Printing\nDESCRIPTION: Creates a function to instantiate AnalyzerEngine, process text, and print results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef call_analyzer_and_print_results(nlp_engine: NlpEngine,\n                                    language: str = \"en\",\n                                    text: str = \"Bill Clinton used to be the president of the United States\") -> None:\n    \"\"\"\n    Instantiate the AnalyzerEngine with the provided nlp_engine and return output.\n\n    This method creates an AnalyzerEngine instance with the provided NlpEngine, and three supported languages (en, es, de)\n    Then, it calls the analyze method to return identified PII.\n\n    :param nlp_engine: The NlpEngine instance as configured by the user\n    :param language: the language the request should support (in contrast to the AnalyzerEngine which can support multiple)\n    :param text: The text to look for PII entities in.\n\n    \"\"\"\n    \n    print(f\"Input text:\\n\\t{text}\\n\")\n    \n    # Initialize the AnalyzerEngine with the configured Nlp Engine:\n    analyzer = AnalyzerEngine(nlp_engine=nlp_engine, \n                              supported_languages=[\"en\", \"de\", \"es\"])\n\n    # Print the NLP Engine's configuration\n    print(f\"NLP Engine configuration:\\n\\tLoaded NLP engine: {analyzer.nlp_engine.__class__.__name__}\")\n    print(f\"\\tSupported entities: {analyzer.nlp_engine.get_supported_entities()}\")\n    print(f\"\\tSupported languages: {analyzer.nlp_engine.get_supported_languages()}\")\n    print()\n    \n    # Call the analyzer.analyze to detect PII entities (from the NLP engine + all other recognizers)\n    results = analyzer.analyze(text=text, \n                               language=language, \n                               return_decision_process=True)\n\n    # sort results\n    results = sorted(results, key= lambda x: x.start)\n    \n    # Print results\n    print(\"Returning full results, including the decision process:\")\n    for i, result in enumerate(results):\n        print(f\"\\tResult {i}: {result}\")\n        print(f\"\\tDetected text: {text[result.start: result.end]}\")\n        print(f\"\\t{result.analysis_explanation.textual_explanation}\")\n        print(\"\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Presidio Batch Processing\nDESCRIPTION: This snippet imports necessary modules and classes from Presidio Analyzer, Anonymizer, and other required libraries for batch processing of structured data.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional, Dict, Union, Iterator, Iterable\nimport collections\nfrom dataclasses import dataclass\nimport pprint\n\nimport pandas as pd\n\nfrom presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult\nfrom presidio_anonymizer import AnonymizerEngine, BatchAnonymizerEngine\nfrom presidio_anonymizer.entities import EngineResult\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio with spaCy\nDESCRIPTION: Commands to install Presidio's analyzer and anonymizer packages along with the required spaCy language model for English.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_text.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio-analyzer\npip install presidio-anonymizer\npython -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Deploying Presidio using Helm\nDESCRIPTION: Complete Helm deployment commands including namespace setup, version tag selection, and installation with optional registry configuration\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/k8s/index.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Choose a namespace and ensure it is created\nNAMESPACE=presidio\n\n# Choose the tag, from mcr.microsoft.com, e.g. `latest`\nTAG=latest\n\n# Choose a name for the deployment\nNAME=<name>\n\n# Use Helm to install all required components\nhelm install $NAME . --set tag=$PRESIDIO_LABEL --namespace $NAMESPACE\n\n# If you have your own images in a separate ACR, run\nDOCKER_REGISTRY=<your_registry>\nhelm install $NAME . --set registry=$DOCKER_REGISTRY,tag=$PRESIDIO_LABEL . --namespace $NAMESPACE\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text with Default Recognizers and Decision Trace - Presidio Python\nDESCRIPTION: This snippet uses the AnalyzerEngine with default recognizers (including URLRecognizer and EntityRecognizer) to process a list of website domains in a sentence. It passes return_decision_process=True to obtain detailed detection traces. Useful for baseline evaluation and to observe default entity detection in Presidio. Required dependencies: default recognizers and AnalyzerEngine initialization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nwebsites_list = [\\\"bing.com\\\", \\\"microsoft.com\\\"]\\ntext1 = \\\"Bill's favorite website is bing.com, David's is microsoft.com\\\"\\nanalyzer = AnalyzerEngine()\\nresults = analyzer.analyze(text=text1, language=\\\"en\\\", return_decision_process=True)\\nprint_analyzer_results(results, text=text1)\n```\n\n----------------------------------------\n\nTITLE: Referencing context_aware_enhancers Module - Python\nDESCRIPTION: Lists the presidio_analyzer.context_aware_enhancers module, designed for pluggable context enhancement of entity recognition, allowing developers to leverage additional logic for improving accuracy in localized contexts. No explicit handler tag implies broader module usage.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.context_aware_enhancers\n```\n\n----------------------------------------\n\nTITLE: Configuring App Service Logging to Log Analytics Workspace using Azure CLI in Bash\nDESCRIPTION: This Bash script configures diagnostic settings for an Azure App Service to stream logs and metrics to a Log Analytics Workspace. It first creates the workspace (if needed), retrieves the resource IDs for both the workspace and the App Service, and then uses `az monitor diagnostic-settings create` to link them, enabling platform logs, console logs, and all metrics. Variables for Log Analytics resource group/name and App Service resource group/name must be set.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/app-service/index.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nLOG_ANALYTICS_WORKSPACE_RESROUCE_GROUP=<resource group of log analytics>\nLOG_ANALYTICS_WORKSPACE_NAME=<log analytics name>\n\n# create a log analytics workspace\naz monitor log-analytics workspace create --resource-group $LOG_ANALYTICS_WORKSPACE_RESROUCE_GROUP --workspace-name $LOG_ANALYTICS_WORKSPACE_NAME\n\n# query the log analytics workspace id\nLOG_ANALYTICS_WORKSPACE_ID=$(az monitor log-analytics workspace show --resource-group $LOG_ANALYTICS_WORKSPACE_RESROUCE_GROUP --workspace-name $LOG_ANALYTICS_WORKSPACE_NAME --query id -o tsv)\n# query the app service id\nAPP_SERVICE_ID=$(az monitor log-analytics workspace show --resource-group $RESOURCE_GROUP --name $APP_SERVICE_NAME --query id -o tsv)\n\n# create the diagnostics settings\naz monitor diagnostic-settings create --name $APP_SERVICE_NAME-diagnostics --resource /\n$APP_SERVICE_ID --logs   '[{\"category\": \"AppServicePlatformLogs\",\"enabled\": true}, {\"category\": \"AppServiceConsoleLogs\", \"enabled\": true}]' --metrics '[{\"category\": \"AllMetrics\",\"enabled\": true}]' --workspace $LOG_ANALYTICS_WORKSPACE_ID\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Image Redactor Docker Container (Shell)\nDESCRIPTION: This command runs the previously downloaded Presidio image redactor Docker container. The `-d` flag runs it in detached mode (background), and `-p 5003:3000` maps port 5003 on the host machine to port 3000 inside the container, making the service accessible via `localhost:5003`.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_images.md#2025-04-23_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ndocker run -d -p 5003:3000 mcr.microsoft.com/presidio-image-redactor\n```\n\n----------------------------------------\n\nTITLE: Analyzing Example Spanish Text with Customized Analyzer - Python\nDESCRIPTION: Sends a sample Spanish sentence containing a name and credit card number to the analyzer engine for PII extraction. Demonstrates the engine's multilingual capability and configuration-driven recognizer behavior. Requires the analyzer to be trained for Spanish with corresponding recognizers.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/08_no_code.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nes_text = \"Hola, me llamo David Johnson y soy originalmente de Liverpool. Mi n\\u00famero de tarjeta de cr\\u00e9dito es 4095260993934932\"\nanalyzer_engine.analyze(es_text, language=\"es\")\n```\n\n----------------------------------------\n\nTITLE: Querying PatternRecognizer for Entities (Presidio, Python)\nDESCRIPTION: This snippet runs the analyze method of a PatternRecognizer instance to process a given text for the entity \"TITLE\". It accepts the input string and desired entity label, returning locations and details of detected entities. Dependencies: a previously-initialized PatternRecognizer object (from presidio_analyzer).\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntitles_recognizer.analyze(text=\"Mr. Schmidt\", entities=\"TITLE\")\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Presidio Anonymizer Imports in Python\nDESCRIPTION: This snippet imports essential modules from the Presidio Anonymizer package required for encryption, decryption, and entity recognition. Dependencies: Presidio Anonymizer must be installed. Imports include main engine classes, result entities, operator configurations, and the Decrypt operator. Input: None. Output: Imports are made available for subsequent code.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/encrypt_decrypt.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine\\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorResult, OperatorConfig\\nfrom presidio_anonymizer.operators import Decrypt\n```\n\n----------------------------------------\n\nTITLE: Presidio Analyzer Recognizer List Configuration (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define both predefined and custom recognizers in the Presidio Analyzer configuration file. It includes examples of various recognizer types and their parameters.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/recognizer_registry_provider.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  - name: CreditCardRecognizer\n    supported_languages:\n    - language: en\n      context: [credit, card, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    - language: es\n      context: [tarjeta, credito, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    - language: it\n    - language: pl\n    type: predefined\n\n  - name: UsBankRecognizer\n    supported_languages: \n    - en\n    type: predefined\n\n  - name: MedicalLicenseRecognizer\n    type: predefined\n\n  - name: ExampleCustomRecognizer\n    patterns:\n    - name: \"zip code (weak)\"\n      regex: \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\"\n      score: 0.01\n    - name: \"zip code (weak)\"\n      regex: \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\"\n      score: 0.01\n    supported_languages:\n    - language: en\n      context: [zip, code]\n    - language: es\n      context: [cdigo, postal]\n    supported_entity: \"ZIP\"\n    type: custom\n    enabled: true\n\n  - name: \"TitlesRecognizer\"\n    supported_language: \"en\"\n    supported_entity: \"TITLE\"\n    deny_list: [Mr., Mrs., Ms., Miss, Dr., Prof.]\n    deny_list_score: 1\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio and SpaCy Models\nDESCRIPTION: Installs the Presidio Analyzer and Anonymizer packages from PyPI, along with the required SpaCy language model.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Initializing Presidio AnalyzerEngine from Multiple Config Files in Python\nDESCRIPTION: This Python snippet demonstrates configuring the Presidio `AnalyzerEngine` using separate YAML files for the analyzer settings, NLP engine, and recognizer registry. Paths to these files are passed to the `AnalyzerEngineProvider`, which then creates the engine. This approach allows for modular configuration and relies on the `presidio_analyzer` library and the existence of the specified configuration files.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/analyzer_engine_provider.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, AnalyzerEngineProvider\n\nanalyzer_conf_file = \"./analyzer/analyzer-config.yml\"\nnlp_engine_conf_file = \"./analyzer/nlp-config.yml\"\nrecognizer_registry_conf_file = \"./analyzer/recognizers-config.yml\"\n\nprovider = AnalyzerEngineProvider(\n    analyzer_engine_conf_file=analyzer_conf_file,\n    nlp_engine_conf_file=nlp_engine_conf_file,\n    recognizer_registry_conf_file=recognizer_registry_conf_file,\n    )\nanalyzer = provider.create_engine()\n\nresults = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Referencing RecognizerRegistryProvider - Python\nDESCRIPTION: Enables the RecognizerRegistryProvider class from presidio_analyzer.recognizer_registry, offering supplier and manager functionality to orchestrate recognizer registry lifecycles, typically in larger pipelines or container-based deployments.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.recognizer_registry.RecognizerRegistryProvider\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Running Presidio CLI with Various Configuration Sources - Shell\nDESCRIPTION: This shell block shows multiple ways to execute presidio-cli, either with default, file-based, or specific file configurations via the -c parameter. The commands expect presidio-cli and requisite dependencies installed. Inputs include configuration file paths and target directories or files. Outputs are CLI results indicating detected PII entities.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# run with default configuration (file `.presidiocli`) in the current directory\\npresidio .\\n\\n# run with configuration limited.yaml in the \\\"tests\\\" directory\\npresidio -c presidio_cli/conf/limited.yaml tests/\\n\\n# run with configuration limited.yaml in single file only tests/test_analyzer.py\\npresidio -c presidio_cli/conf/limited.yaml tests/test_analyzer.py\n```\n\n----------------------------------------\n\nTITLE: Referencing DictAnalyzerResult - Python\nDESCRIPTION: Imports DictAnalyzerResult from presidio_analyzer.dict_analyzer_result, enabling results to be marshaled into dictionary format for serialization or further processing. Useful for converting analysis output for storage or transport.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.dict_analyzer_result.DictAnalyzerResult\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Presidio Analyzer Class Diagram in Mermaid\nDESCRIPTION: This Mermaid diagram illustrates the main classes in Presidio Analyzer, including RecognizerResult, EntityRecognizer, RecognizerRegistry, NlpEngine, ContextAwareEnhancer, and AnalyzerEngine. It shows their relationships and key methods.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/index.md#2025-04-23_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\nclassDiagram\n    direction LR\n    class RecognizerResult {\n        +str entity_type\n        +float score\n        +int start\n        +int end\n    }\n\n    class EntityRecognizer {\n        +str name\n        +int version\n        +List[str] supported_entities\n        +analyze(text, entities) List[RecognizerResult]\n    }\n\n\n    class RecognizerRegistry {\n        +add_recognizer(recognizer) None\n        +remove_recognizer(recognizer) None\n        +load_predefined_recognizers() None\n        +get_recognizers() List[EntityRecognizer]\n\n\n    }\n\n    class NlpEngine {\n        +process_text(text, language) NlpArtifacts\n        +process_batch(texts, language) Iterator[NlpArtifacts]\n    }\n\n    class ContextAwareEnhancer {\n        +enhance_using_context(text, recognizer_results) List[RecognizerResult]\n    }\n\n\n    class AnalyzerEngine {\n        +NlpEngine nlp_engine\n        +RecognizerRegistry registry\n        +ContextAwareEnhancer context_aware_enhancer\n        +analyze(text: str, language) List[RecognizerResult]\n\n    }\n\n    NlpEngine <|-- SpacyNlpEngine\n    NlpEngine <|-- TransformersNlpEngine\n    NlpEngine <|-- StanzaNlpEngine\n    AnalyzerEngine *-- RecognizerRegistry\n    AnalyzerEngine *-- NlpEngine\n    AnalyzerEngine *-- ContextAwareEnhancer\n    RecognizerRegistry o-- \"0..*\" EntityRecognizer\n    ContextAwareEnhancer <|-- LemmaContextAwareEnhancer\n\n    %% Defining styles\n    style RecognizerRegistry fill:#E6F7FF,stroke:#005BAC,stroke-width:2px\n    style NlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style SpacyNlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style YourNlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style TransformersNlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style StanzaNlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style ContextAwareEnhancer fill:#E6FFE6,stroke:#008000,stroke-width:2px\n    style LemmaContextAwareEnhancer fill:#E6FFE6,stroke:#008000,stroke-width:2px\n    style EntityRecognizer fill:#F5F5DC,stroke:#8B4513,stroke-width:2px\n    style YourEntityRecognizer fill:#F5F5DC,stroke:#8B4513,stroke-width:2px\n    style RecognizerResult fill:#FFF0F5,stroke:#FF69B4,stroke-width:2px\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Sentences from Templates using OpenAI in Python\nDESCRIPTION: Imports `time` and sets up `PrettyPrinter`. It processes the first five templates. For each template, it creates a prompt, calls the OpenAI API via `call_completion_model`, stores the original and synthetic text in a dictionary, appends it to the `sentences` list, prints the dictionary, and pauses for 3 seconds using `time.sleep()` to respect potential rate limits.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntemplates_to_use = templates[:5]\n\n\nimport time\npp = pprint.PrettyPrinter(indent=2, width=110)\nsentences = []\nfor template in templates_to_use:\n    synth_sentence = call_completion_model(create_prompt(template))\n    sentence_dict = {\"original\": template, \"synthetic\":synth_sentence.strip()}\n    sentences.append(sentence_dict)\n    pp.pprint(sentence_dict)\n    time.sleep(3) # wait to not get blocked by service (only applicable for the free tier)\n    print(\"--------------\")\n```\n\n----------------------------------------\n\nTITLE: Defining General Analyzer Configuration as YAML String - Python\nDESCRIPTION: Defines the analyzer configuration, specifying support for English ('en') and Spanish ('es') languages and setting the default score threshold to 0.4. This YAML content will later be written to a configuration file consumed by Presidio. `analyzer_config_yaml` is used as part of the full analyzer config string.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/08_no_code.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nanalyzer_config_yaml = \"\"\"\nsupported_languages: \n  - en\n  - es\ndefault_score_threshold: 0.4\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio Modules in Python\nDESCRIPTION: This snippet imports the necessary modules from Presidio Analyzer and Anonymizer to create custom recognizers and perform anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry, PatternRecognizer\nfrom presidio_anonymizer import AnonymizerEngine\n```\n\n----------------------------------------\n\nTITLE: Pulling Presidio Image Redactor Docker Image (Shell)\nDESCRIPTION: This command uses Docker to download the official Presidio image redactor container image from the Microsoft Container Registry (mcr.microsoft.com). This image provides a ready-to-run environment with the Presidio image redaction service.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_images.md#2025-04-23_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndocker pull mcr.microsoft.com/presidio-image-redactor\n```\n\n----------------------------------------\n\nTITLE: Creating AnalyzerEngine with Custom Configuration\nDESCRIPTION: This snippet uses the AnalyzerEngineProvider to create an AnalyzerEngine instance with the custom YAML configuration.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/no_code_config.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nanalyzer_engine = AnalyzerEngineProvider(analyzer_engine_conf_file=temp_file_path).create_engine()\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio and Dependencies in Python\nDESCRIPTION: This snippet installs the Presidio Analyzer and Anonymizer libraries, as well as the required spaCy English language model.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Building Presidio Cluster with Docker Compose\nDESCRIPTION: Command to build and start the Presidio cluster with all services in HTTP mode using Docker Compose.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up --build -d\n```\n\n----------------------------------------\n\nTITLE: Printing Raw Analysis Results from Presidio AnalyzerEngine in Python\nDESCRIPTION: This code simply prints the raw `results` object obtained from the `AnalyzerEngine.analyze` call in the previous step. This shows the complete output structure returned by Presidio, which is a list of `RecognizerResult` objects. It depends on the `results` variable from the preceding analysis.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/01_deny_list.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```python\nprint(\"Results:\")\nprint(results)\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Image Redactor with pip - Shell\nDESCRIPTION: This shell command uses pip to install the Presidio Image Redactor package from the Python Package Index. pip resolves the required dependencies including the Presidio core modules. It's assumed that Python and pip are already installed. This is the fundamental step before using the library in Python scripts or as a Docker service.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio-image-redactor\n```\n\n----------------------------------------\n\nTITLE: Customizing Context Enhancement Logic with LemmaContextAwareEnhancer in Presidio Analyzer (Python)\nDESCRIPTION: Shows how to override the default context enhancement logic in Presidio Analyzer by instantiating a LemmaContextAwareEnhancer with custom parameters (context_similarity_factor and min_score_with_context_similarity). Registers a context-aware recognizer and uses the custom enhancer during analysis. Requires presidio_analyzer and presidio_analyzer.context_aware_enhancers. Key inputs are custom factor values and text to analyze. Output is analysis results with further-tuned confidence score.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/06_context.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n\ncontext_aware_enhancer = LemmaContextAwareEnhancer(\n    context_similarity_factor=0.45, min_score_with_context_similarity=0.4\n)\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer_w_context)\nanalyzer = AnalyzerEngine(\n    registry=registry, context_aware_enhancer=context_aware_enhancer\n)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\nprint(\"Result:\")\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Parameters for Presidio PII Detection\nDESCRIPTION: Sets configuration parameters for the demonstration, including the number of rows to duplicate for scale testing, input data path, and Delta table settings for storing anonymized results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnum_duplicates = 5000 # for the scale part\ncsv_path = \"Files/presidio/fabric_sample_data.csv\"\nis_write_to_delta = True\ntable_namne = \"presidio_demo_table\"\npartitions_number = 100\n```\n\n----------------------------------------\n\nTITLE: Analyzing Example English Text with Customized Analyzer - Python\nDESCRIPTION: Sends a sample English sentence with PII to the analyzer for entity recognition. Showcases use of custom YAML configuration and confirms correct recognition of credit card numbers per earlier setup. Engine must be initialized and configured before running.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/08_no_code.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nen_text = \"Hi, my name is David Johnson and I'm originally from Liverpool. My credit card number is 4095260993934932\"\nanalyzer_engine.analyze(en_text, language=\"en\")\n```\n\n----------------------------------------\n\nTITLE: Uploading Test Data to Azure Blob Storage\nDESCRIPTION: Bash command to upload a test file to the input folder in Azure Blob Storage using Azure CLI. This data will be processed by the Presidio anonymization notebook.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/spark/index.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naz storage blob upload --account-name $STORAGE_ACCOUNT_NAME  --container $STORAGE_CONTAINER_NAME --file ./[file name] --name input/[file name]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Decision Process Explanations for Recognizers\nDESCRIPTION: Python code example from the spacy_recognizer.py file that demonstrates how to implement custom decision process explanations for a PII recognizer. It creates an AnalysisExplanation object with recognizer name, original score, and a textual explanation of the entity recognition process.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/decision_process.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nSPACY_DEFAULT_EXPLANATION = \"Identified as {} by Spacy's Named Entity Recognition\"\n\ndef build_spacy_explanation(recognizer_name, original_score, entity):\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        textual_explanation=SPACY_DEFAULT_EXPLANATION.format(entity))\n    return explanation\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Anonymizer as an HTTP Server using Docker\nDESCRIPTION: This bash command demonstrates how to run the Presidio Anonymizer as an HTTP server using a Docker container. It maps port 5001 on the host to port 3000 in the container.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/anonymizer/index.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd presidio-anonymizer\ndocker run -p 5001:3000 presidio-anonymizer\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Configuration for Anonymizer API Demo Client\nDESCRIPTION: This snippet demonstrates how to create a .env file by copying the sample file. The user is then instructed to edit the configuration, including OpenAI API settings and anonymizer/deanonymizer API endpoints.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.sample .env\n```\n\n----------------------------------------\n\nTITLE: Iterating and Printing Identified PII Entities from Presidio Results in Python\nDESCRIPTION: This snippet iterates through the `results` list obtained from the `AnalyzerEngine`. For each identified PII entity (`result`), it extracts the corresponding text substring from the original `text1` using the start and end indices provided by the result object and prints it along with its identified entity type. It depends on the `results` and `text1` variables.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/01_deny_list.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```python\nprint(\"Identified these PII entities:\")\nfor result in results:\n    print(f\"- {text1[result.start:result.end]} as {result.entity_type}\")\n```\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text for PII Entities with Presidio in Python\nDESCRIPTION: This code defines a sample text and uses the Presidio Analyzer to detect PII entities within it.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntext_to_analyze = \"Hi my name is Charles Darwin and my email is cdarwin@hmsbeagle.org\"\nanalyzer_results = analyzer.analyze(text_to_analyze, language=\"en\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Create an OpenAI Prompt for Synthetic Data Generation in Python\nDESCRIPTION: Defines `create_prompt` function taking anonymized text. It constructs a detailed prompt for an OpenAI model, instructing it to replace PII placeholders with fake values following specific rules (random numbers, diverse names, format preservation). The prompt includes examples and wraps the input `anonymized_text` within markers.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef create_prompt(anonymized_text: str) -> str:\n    \"\"\"\n    Create the prompt with instructions to GPT-3.\n    \n    :param anonymized_text: Text with placeholders instead of PII values, e.g. My name is <PERSON>.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Your role is to create synthetic text based on de-identified text with placeholders instead of Personally Identifiable Information (PII).\n    Replace the placeholders (e.g. ,<PERSON>, {{DATE}}, {{ip_address}}) with fake values.\n    Instructions:\n    a. Use completely random numbers, so every digit is drawn between 0 and 9.\n    b. Use realistic names that come from diverse genders, ethnicities and countries.\n    c. If there are no placeholders, return the text as is.\n    d. Keep the formatting as close to the original as possible.\n    e. If PII exists in the input, replace it with fake values in the output.\n    f. Remove whitespace before and after the generated text\n    \n    input: [[TEXT STARTS]] How do I change the limit on my credit card {{credit_card_number}}?[[TEXT ENDS]]\n    output: How do I change the limit on my credit card 2539 3519 2345 1555?\n    input: [[TEXT STARTS]]<PERSON> was the chief science officer at <ORGANIZATION>.[[TEXT ENDS]]\n    output: Katherine Buckjov was the chief science officer at NASA.\n    input: [[TEXT STARTS]]Cameroon lives in <LOCATION>.[[TEXT ENDS]]\n    output: Vladimir lives in Moscow.\n    \n    input: [[TEXT STARTS]]{anonymized_text}[[TEXT ENDS]]\n    output:\"\"\"\n    return prompt\n```\n\n----------------------------------------\n\nTITLE: Configuring Detection Strategy for Tabular Analysis in Presidio Structured - Python\nDESCRIPTION: Demonstrates selecting entity detection strategies (most common, highest confidence, or mixed) in Presidio Structured's DataFrame analysis step. Requires presidio-structured and pandas. The generate_analysis() method of PandasAnalysisBuilder takes selection_strategy and mixed_strategy_threshold arguments to control entity mapping logic; the output analysis affects subsequent anonymization and may have different sensitivity and accuracy. Values: 'most_common' is default, 'highest_confidence' for best-scoring, or 'mixed' with a threshold parameter for hybrid behavior.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/structured/index.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Generate a tabular analysis using the most common strategy\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df)\n\n# Generate a tabular analysis using the highest confidence strategy\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df, selection_strategy=\"highest_confidence\")\n\n# Generate a tabular analysis using the mixed strategy\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df, selection_strategy=\"mixed\", mixed_strategy_threshold=0.75)\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating DICOM De-identification Performance with Python\nDESCRIPTION: Python code demonstrating how to evaluate DICOM de-identification performance using ground truth data and DicomImagePiiVerifyEngine.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/evaluating_dicom_redaction.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Load ground truth for one file\nwith open(gt_path) as json_file:\n    all_ground_truth = json.load(json_file)\nground_truth = all_ground_truth[file_of_interest]\n\n# Select your DICOM instance\ninstance = pydicom.dcmread(file_of_interest)\n\n# Evaluate the DICOM de-identification performance\n_, eval_results = dicom_engine.eval_dicom_instance(instance, ground_truth)\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for NER Pipeline\nDESCRIPTION: This YAML configuration file example shows how to set up the NER pipeline, including model specifications and entity mappings.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/nlp_engines/transformers.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nnlp_engine_name: transformers\nmodels:\n  -\n    lang_code: en\n    model_name:\n      spacy: en_core_web_sm\n      transformers: StanfordAIMI/stanford-deidentifier-base\n\nner_model_configuration:\n  labels_to_ignore:\n  - O\n  aggregation_strategy: max # \"simple\", \"first\", \"average\", \"max\"\n  stride: 16\n  alignment_mode: expand # \"strict\", \"contract\", \"expand\"\n  model_to_presidio_entity_mapping:\n    PER: PERSON\n    LOC: LOCATION\n    ORG: ORGANIZATION\n    AGE: AGE\n    ID: ID\n    EMAIL: EMAIL\n    PATIENT: PERSON\n    STAFF: PERSON\n    HOSP: ORGANIZATION\n    PATORG: ORGANIZATION\n    DATE: DATE_TIME\n    PHONE: PHONE_NUMBER\n    HCW: PERSON\n    HOSPITAL: LOCATION\n    VENDOR: ORGANIZATION\n\n  low_confidence_score_multiplier: 0.4\n  low_score_entity_names:\n  - ID\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for E2E Tests - Shell\nDESCRIPTION: This shell snippet installs all Python dependencies needed for Presidio end-to-end testing using pip. It expects a requirements.txt file listing the required packages, and it's advised to run it within a virtual environment. This step ensures all necessary testing frameworks and libraries are available before running the tests.\nSOURCE: https://github.com/microsoft/presidio/blob/main/e2e-tests/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Implementing PII Controls in OpenAI Client\nDESCRIPTION: Python code showing how to control PII masking at request level using OpenAI client\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/docker/litellm.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n        base_url=\"http://0.0.0.0:4000\"\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"My name is Jane Doe, my number is 8382043839\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    extra_body={\n        \"content_safety\": {\"output_parse_pii\": False} \n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Deanonymizing Text via HTTP POST Request\nDESCRIPTION: This curl command shows how to send a deanonymization request to the Presidio Anonymizer HTTP server. It includes a JSON payload with the anonymized text, deanonymizer configurations, and anonymizer results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/anonymizer/index.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XPOST http://localhost:3000/deanonymize -H \"Content-Type: application/json\" -d @payload\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n\"text\": \"My name is S184CMt9Drj7QaKQ21JTrpYzghnboTF9pn/neN8JME0=\",\n\"deanonymizers\": {\n    \"PERSON\": {\n        \"type\": \"decrypt\",\n        \"key\": \"WmZq4t7w!z%C&F)J\"\n    }\n},\n\"anonymizer_results\": [\n    {\n        \"start\": 11,\n        \"end\": 55,\n        \"entity_type\": \"PERSON\"\n    }\n]}\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: This snippet lists required Python packages and their versions. It includes packages for argument parsing, OpenAI API integration, environment variable management, HTTP requests, and text-based user interfaces.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nConfigArgParse==1.7\nopenai==1.37.1\npython-dotenv==1.0.0\nrequests==2.32.3\ntextual[syntax]==0.79.1\ntextual-serve==1.1.1\n```\n\n----------------------------------------\n\nTITLE: Deploying ADF/App Service Resources via Azure CLI (Bash)\nDESCRIPTION: This Bash script uses the Azure CLI to deploy Azure resources necessary for the Presidio-as-HTTP-endpoint scenario. It first creates a resource group (if it doesn't exist) and then deploys an ARM template (`azure-deploy-adf-app-service.json`) which provisions Azure Data Factory, Azure App Service, Azure Key Vault, and Azure Storage, configured to work together.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/data-factory/presidio-data-factory.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nRESOURCE_GROUP=[Name of resource group]\nLOCATION=[location of resources]\n\naz group create --name $RESOURCE_GROUP --location $LOCATION\naz deployment group create -g $RESOURCE_GROUP --template-file ./arm-templates/azure-deploy-adf-app-service.json\n```\n\n----------------------------------------\n\nTITLE: Executing Analyzer with Transformers NLP Engine\nDESCRIPTION: Runs the AnalyzerEngine with the configured Transformers NLP engine and prints results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Run it as part of Presidio's AnalyzerEngine\ncall_analyzer_and_print_results(transformers_nlp_engine)\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio and Text Analytics Modules in Python\nDESCRIPTION: This code imports the necessary classes from Presidio Analyzer and the custom Text Analytics recognizer implementation.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/integrating_with_external_services.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom text_analytics.example_text_analytics_recognizer import TextAnalyticsEntityCategory, TextAnalyticsRecognizer\n```\n\n----------------------------------------\n\nTITLE: Generating DICOM Verification Data with Python\nDESCRIPTION: Python code to initialize DicomImagePiiVerifyEngine and generate verification image, OCR results, and NER results for ground truth creation.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/evaluating_dicom_redaction.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pydicom\nfrom presidio_image_redactor import DicomImagePiiVerifyEngine\n\n# Initialize engine\ndicom_engine = DicomImagePiiVerifyEngine()\n\n# Choose your file to create ground truth for\nfilename = \"path/to/your/file.dcm\"\ninstance = pydicom.dcmread(filename)\npadding_width = 25\n\n# Get OCR and NER results\nverification_image, ocr_results, analyzer_results = dicom_engine.verify_dicom_instance(instance, padding_width)\n```\n\n----------------------------------------\n\nTITLE: Referencing nlp_engine Module - Python\nDESCRIPTION: Exposes the presidio_analyzer.nlp_engine module responsible for underlying NLP tasks, tokenization, and preprocessing functions. Required for initializing and configuring NLP frameworks backing entity recognizers. Marked with a python handler.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.nlp_engine\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio CLI from PyPI - Shell\nDESCRIPTION: This shell command installs the presidio-cli package from Python Package Index (PyPi) into the current Python environment. It requires Python 3.9-3.11 and pip. The command ensures presidio-cli is accessible as a shell command for further use.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install presidio-cli\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Anonymizer using pip - Shell\nDESCRIPTION: This snippet provides the shell command to install the Presidio Anonymizer Python package using pip. This requires Python and pip to be installed on the system, and it's recommended to run in a virtual environment. The command will download and install the entire presidio-anonymizer module and its dependencies.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-anonymizer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio-anonymizer\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Anonymizer as an HTTP Server using Python\nDESCRIPTION: This bash command shows how to run the Presidio Anonymizer as an HTTP server using Python runtime. It requires the Presidio Github repository to be cloned.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/anonymizer/index.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd presidio-anonymizer\npython app.py\n```\n\n----------------------------------------\n\nTITLE: Illustrating Formatted OCR Results from DicomImagePiiVerifyEngine in JSON\nDESCRIPTION: Provides an example of the JSON structure for formatted OCR results obtained from the `DicomImagePiiVerifyEngine`. Each object in the array represents a detected text element, including its bounding box coordinates (left, top, width, height), OCR confidence score ('conf'), and the detected text ('label').\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/Evaluation_Approach.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n// OCR Results (formatted)\n[\n    {\n        \"left\": 25,\n        \"top\": 25,\n        \"width\": 241,\n        \"height\": 37,\n        \"conf\": 95.833916,\n        \"label\": \"DAVIDSON\"\n    },\n    {\n        \"left\": 287,\n        \"top\": 25,\n        \"width\": 230,\n        \"height\": 36,\n        \"conf\": 93.292221,\n        \"label\": \"DOUGLAS\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Deploying ADF/Databricks Resources via Azure CLI (Bash)\nDESCRIPTION: This Bash script uses the Azure CLI to deploy Azure resources for the Presidio-on-Databricks scenario. It deploys an ARM template (`azure-deploy-adf-databricks.json`) into a specified resource group, passing parameters like Databricks access token, cluster ID, notebook location, workspace URL, and Azure Storage details. This template sets up ADF to orchestrate a Databricks job for anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/data-factory/presidio-data-factory.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRESOURCE_GROUP=[Name of resource group]\nLOCATION=[location of resources]\nDATABRICKS_HOST=https://$DATABRICKS_WORKSPACE_URL\nDATABRICKS_CLUSTER_ID=$(databricks clusters get --cluster-name presidio_cluster | jq -r .cluster_id)\nDATABRICKS_NOTEBOOK_LOCATION=\"/notebooks/01_transform_presidio\"\n\naz deployment group create -g $RESOURCE_GROUP --template-file ./arm-templates/azure-deploy-adf-databricks.json --parameters Databricks_accessToken=$DATABRICKS_TOKEN Databricks_clusterId=$DATABRICKS_CLUSTER_ID Databricks_notebookLocation=$DATABRICKS_NOTEBOOK_LOCATION Databricks_workSpaceUrl=$DATABRICKS_HOST AzureBlobStorage_accountName=$STORAGE_ACCOUNT_NAME AzureBlobStorage_cotainerName=$STORAGE_CONTAINER_NAME\n```\n\n----------------------------------------\n\nTITLE: Initializing Presidio AnalyzerEngine from a Single Config File in Python\nDESCRIPTION: This Python snippet shows how to configure the Presidio `AnalyzerEngine` by providing the path to a single YAML configuration file to the `AnalyzerEngineProvider`. The provider then instantiates the engine based on this configuration. It depends on the `presidio_analyzer` library and requires a valid configuration file (`analyzer-config-all.yml` in this example).\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/analyzer_engine_provider.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, AnalyzerEngineProvider\n\nanalyzer_conf_file = \"./analyzer/analyzer-config-all.yml\"\n\nprovider = AnalyzerEngineProvider(\n    analyzer_engine_conf_file=analyzer_conf_file\n    )\nanalyzer = provider.create_engine()\n\nresults = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables (.env)\nDESCRIPTION: Defines optional environment variables in a `.env` file for customizing the demo application. These include API keys and endpoints for Azure Text Analytics and OpenAI, model selection options, and specifying the OpenAI service type (Azure or OpenAI).\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/streamlit/index.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nTA_KEY=YOUR_TEXT_ANALYTICS_KEY\nTA_ENDPOINT=YOUR_TEXT_ANALYTICS_ENDPOINT\nOPENAI_TYPE=\"Azure\" #or \"openai\"\nOPENAI_KEY=YOUR_OPENAI_KEY\nOPENAI_API_VERSION = \"2023-05-15\"\nAZURE_OPENAI_ENDPOINT=YOUR_AZURE_OPENAI_AZURE_OPENAI_ENDPOINT\nAZURE_OPENAI_DEPLOYMENT=text-davinci-003\nALLOW_OTHER_MODELS=true #true if the user could download new models\n```\n\n----------------------------------------\n\nTITLE: Redacting PII in Images via HTTP API with curl - Shell\nDESCRIPTION: This curl command demonstrates a multipart HTTP POST request to the /redact endpoint of a locally running Presidio Image Redactor API (port 3000). An image file (ocr_test.png) and a color fill parameter (255) are submitted; the redacted output is saved as out.png. Requires a running API instance and the referenced files present on disk. Returns a binary image file as the response.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/README.md#2025-04-23_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncurl -XPOST \"http://localhost:3000/redact\" -H \"content-type: multipart/form-data\" -F \"image=@ocr_test.png\" -F \"data=\\\"{'color_fill':'255'}\\\"\" > out.png\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Function for Printing Analyzer Results\nDESCRIPTION: Creates a function to print the analyzer results in a human-readable format, including the detected entities and their explanations.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef print_analyzer_results(results: List[RecognizerResult], text: str):\n    \"\"\"Print the results in a human readable way.\"\"\"\n\n    for i, result in enumerate(results):\n        print(f\"Result {i}:\")\n        print(f\" {result}, text: {text[result.start:result.end]}\")\n\n        if result.analysis_explanation is not None:\n            print(f\" {result.analysis_explanation.textual_explanation}\")\n```\n\n----------------------------------------\n\nTITLE: Integrating Transformer Models via TransformersNlpEngine in spaCy Pipelines (Python)\nDESCRIPTION: Introduces the `TransformersNlpEngine` Python class (added in version 2.2.30), enabling the use of transformer-based Named Entity Recognition (NER) models within spaCy pipelines used by Presidio Analyzer. This allows leveraging models like those from Hugging Face for PII detection.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nTransformersNlpEngine\n```\n\n----------------------------------------\n\nTITLE: Building Presidio Image Redactor from Source\nDESCRIPTION: Commands to build the Presidio image redactor container from source code.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/index.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncd presidio-image-redactor\ndocker build . -t presidio/presidio-image-redactor\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio with Transformers Support\nDESCRIPTION: Commands to install Presidio's analyzer with transformers support, the anonymizer package, and the required spaCy language model for English.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_text.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install \"presidio-analyzer[transformers]\"\npip install presidio-anonymizer\npython -m spacy download en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Analyzer as HTTP Server using Docker\nDESCRIPTION: This snippet shows how to run the Presidio Analyzer as an HTTP server using a Docker container. It maps port 5002 on the host to port 3000 in the container.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/index.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd presidio-analyzer\ndocker run -p 5002:3000 presidio-analyzer\n```\n\n----------------------------------------\n\nTITLE: Setting Up Databricks Environment for Presidio\nDESCRIPTION: Bash script to configure a Databricks cluster for running Presidio anonymization jobs. This script is referenced in the documentation but not provided in full.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/spark/index.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsh ./scripts/configure_databricks.sh\n```\n\n----------------------------------------\n\nTITLE: Disabling/Enabling Specific Recognizers in Presidio YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to selectively enable or disable specific recognizers within the Presidio configuration. By setting `enabled: false` for a recognizer (like `SpacyRecognizer` here) in the `recognizer_registry`, it prevents that recognizer from being loaded and used by the Analyzer Engine. Conversely, recognizers like `CreditCardRecognizer` remain active (enabled by default or explicitly set to `true`).\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/analyzer_engine_provider.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nrecognizer_registry:\n  global_regex_flags: 26\n  recognizers:\n    - name: SpacyRecognizer\n      type: predefined\n      enabled: false\n    - name: CreditCardRecognizer\n      type: predefined\n      enabled: true\n\nsupported_languages:\n  - en\ndefault_score_threshold: 0.7\n\nnlp_configuration:\n  nlp_engine_name: spacy\n  models:\n    -\n      lang_code: en\n      model_name: en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for PII Annotation in PDFs using Python\nDESCRIPTION: This snippet installs the necessary Python packages for PII annotation in PDFs, including Presidio components, spaCy language model, and PDF manipulation libraries.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_pdf_annotation.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install presidio_analyzer\n!pip install presidio_anonymizer\n!python -m spacy download en_core_web_lg\n!pip install pdfminer.six\n!pip install pikepdf\n```\n\n----------------------------------------\n\nTITLE: Defining Cryptographic Key for Presidio Encryption in Python\nDESCRIPTION: This snippet defines a cryptographic key string used for both encryption and decryption in the Presidio Anonymizer. The key must be kept secure and is mandatory for enabling AES encryption in CBC mode. Input: None. Output: A cryptographic key string assigned to the variable 'crypto_key'. Changing this will alter the encryption/decryption results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/encrypt_decrypt.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncrypto_key = \"WmZq4t7w!z%C&F)J\"\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio and YAML Dependencies - Python\nDESCRIPTION: This snippet imports the essential libraries for creating YAML configurations, handling temporary files, pretty-printing, and working with Presidio's analyzer engine provider. Dependencies include `yaml`, `json`, `tempfile`, `pprint`, and `presidio_analyzer`. These imports are required for subsequent configuration setup and analysis.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/08_no_code.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\nimport json\nimport tempfile\nfrom pprint import pprint\nfrom presidio_analyzer import AnalyzerEngineProvider\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Image Redactor via pip\nDESCRIPTION: Commands to install the Presidio image redactor package and required spaCy model using pip package manager.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/index.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio-image-redactor\npython -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Running the Streamlit Application (Shell)\nDESCRIPTION: Starts the Presidio demo website using the Streamlit framework by executing the `presidio_streamlit.py` Python script.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/streamlit/index.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nstreamlit run presidio_streamlit.py\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio and Dependencies\nDESCRIPTION: This code snippet shows the commands to install Presidio Analyzer, Anonymizer, and required dependencies including spaCy and pandas.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\n#!pip install presidio_analyzer presidio_anonymizer\n#!python -m spacy download en_core_web_lg\n#!pip install pandas\n```\n\n----------------------------------------\n\nTITLE: Displaying Presidio Anonymizer Results in Python\nDESCRIPTION: This snippet shows how to display the results of the Presidio Anonymizer operation, which will show the person name preserved but the location replaced.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/keep_entities.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nanonymize_result\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Presidio Project\nDESCRIPTION: This list specifies the necessary Python packages for the Presidio project, typically found in a requirements file. It includes dependencies for documentation generation using MkDocs and its extensions (`mkdocs`, `mkdocs-material`, `mkdocs-jupyter`, `pymdown-extensions`, `markdown`, `mkdocstrings-python`), core Presidio libraries (`presidio_analyzer`, `presidio_anonymizer`, `presidio_image_redactor`, `presidio_structured`), syntax highlighting (`pygments>=2.10`), and code formatting (`black`). This file is commonly used by package managers like `pip` (`pip install -r requirements.txt`) to install the required environment.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/requirements-docs.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmkdocs\nmkdocs-material\nmkdocs-jupyter\npymdown-extensions\nmarkdown\nmkdocstrings-python\npresidio_analyzer\npresidio_anonymizer\npresidio_image_redactor\npresidio_structured\npygments>=2.10\nblack\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing Presidio CLI from Source - Shell\nDESCRIPTION: This shell block details the steps for cloning the Presidio project repository from GitHub and installing presidio-cli from its source code using Poetry. It requires git, Poetry, and an appropriate Python version installed. The commands sequentially clone the repository, navigate to the CLI directory, and install dependencies and the CLI in the local environment.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# clone from git\\ngit clone https://github.com/microsoft/presidio\\ncd presidio/presidio-cli\\n# install required apps and presidio-cli\\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio and Dependencies in Python\nDESCRIPTION: This snippet installs the Presidio Analyzer and Anonymizer packages, as well as the required spaCy language model.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/integrating_with_external_services.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Directly Decrypting Encrypted Entity with Presidio Decrypt Operator in Python\nDESCRIPTION: This snippet illustrates invoking the Decrypt operator directly on an encrypted entity value, bypassing the full engine workflow. Input: An encrypted entity string from the anonymizer result and the cryptographic key. Output: Returns the original decrypted entity string. This method is suitable for cases where only direct entity decryption is needed.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/encrypt_decrypt.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Alternatively, call the Decrypt operator directly:\\n\\n# Fetch the encrypted entitiy value from the previous stage\\nencrypted_entity_value = anonymize_result.items[0].text\\n\\n# Restore the original entity value\\nDecrypt().operate(text=encrypted_entity_value, params={\"key\": crypto_key})\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio Anonymizer Python Package\nDESCRIPTION: Python directive for importing and documenting the presidio_anonymizer package. Uses a handler to process Python documentation.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/anonymizer_python.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_anonymizer\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Making HTTP API Request with Decision Process in Presidio-analyzer\nDESCRIPTION: A curl command example that demonstrates how to make an HTTP POST request to the Presidio-analyzer API with the decision process enabled. The request includes a JSON payload with sample text containing PII (a driver's license) and sets the return_decision_process parameter to true.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/decision_process.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncurl -d '{\n    \"text\": \"John Smith drivers license is AC432223\", \n    \"language\": \"en\", \n    \"return_decision_process\": true}' -H \"Content-Type: application/json\" -X POST http://localhost:3000/analyze\n```\n\n----------------------------------------\n\nTITLE: Running Presidio with KIND\nDESCRIPTION: Script execution command to deploy Presidio using Kubernetes IN Docker (KIND)\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/k8s/index.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docs/samples/deployments/k8s/deployment/\n./run-with-kind.sh\n```\n\n----------------------------------------\n\nTITLE: Defining NLP Engine Configuration for Multilingual NER - Python\nDESCRIPTION: Sets up a YAML string specifying NLP engine settings for English and Spanish. This includes model names for spaCy and transformer backends, entity label mappings, and NER aggregation and alignment strategies. These parameters control how Presidio performs multilingual entity recognition.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/08_no_code.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnlp_engine_yaml = \"\"\"\nnlp_configuration:\n    nlp_engine_name: transformers\n    models:\n      -\n        lang_code: en\n        model_name:\n          spacy: en_core_web_sm\n          transformers: StanfordAIMI/stanford-deidentifier-base\n      -\n        lang_code: es\n        model_name:\n          spacy: es_core_news_sm\n          transformers: MMG/xlm-roberta-large-ner-spanish  \n    ner_model_configuration:\n      labels_to_ignore:\n      - O\n      aggregation_strategy: first # \\\"simple\\\", \\\"first\\\", \\\"average\\\", \\\"max\\\"\n      stride: 16\n      alignment_mode: expand # \\\"strict\\\", \\\"contract\\\", \\\"expand\\\"\n      model_to_presidio_entity_mapping:\n        PER: PERSON\n        PERSON: PERSON\n        LOC: LOCATION\n        LOCATION: LOCATION\n        GPE: LOCATION\n        ORG: ORGANIZATION\n        ORGANIZATION: ORGANIZATION\n        NORP: NRP\n        AGE: AGE\n        ID: ID\n        EMAIL: EMAIL\n        PATIENT: PERSON\n        STAFF: PERSON\n        HOSP: ORGANIZATION\n        PATORG: ORGANIZATION\n        DATE: DATE_TIME\n        TIME: DATE_TIME\n        PHONE: PHONE_NUMBER\n        HCW: PERSON\n        HOSPITAL: LOCATION\n        FACILITY: LOCATION\n        VENDOR: ORGANIZATION\n        MISC: ID\n    \n      low_confidence_score_multiplier: 0.4\n      low_score_entity_names:\n      - ID\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Deny-list Ad-hoc Recognizers - Presidio Analyzer API - JSON\nDESCRIPTION: Demonstrates a JSON payload for supplying deny-list based ad-hoc recognizers to the Presidio Analyzer API. This example includes two recognizers, targeting honorifics (Mr and Ms) using separate deny-lists. It requires the API to support the \"ad_hoc_recognizers\" property and is used to identify specific known values within the analysis request. Key parameters are the lists of words/phrases for matching, the supported entity tag, and the language; output modifies entity detection for the request.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/09_ad_hoc.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"Mr. John Smith's drivers license is AC432223\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Mr. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Mr\", \"Mr.\", \"Mister\"],\n        \"supported_entity\":\"MR_TITLE\"\n        },\n        {\n        \"name\": \"Ms. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Ms\", \"Ms.\", \"Miss\", \"Mrs\", \"Mrs.\"],\n        \"supported_entity\":\"MS_TITLE\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Uploading Cluster Initialization Scripts to Databricks\nDESCRIPTION: Bash command to upload the cluster initialization script to Databricks File System (DBFS). This script installs Presidio libraries during cluster startup.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/spark/index.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndatabricks fs cp \"./setup/startup.sh\" \"dbfs:/FileStore/dependencies/startup.sh\"\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Text Analysis with Stanza\nDESCRIPTION: Installs Presidio packages with Stanza NLP engine support\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/installation.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install \"presidio_analyzer[stanza]\"\npip install presidio_anonymizer\n```\n\n----------------------------------------\n\nTITLE: Deploying Azure Infrastructure for Presidio-Spark Integration\nDESCRIPTION: Bash script for provisioning the necessary Azure resources including resource group, storage account, and Databricks workspace. Uses Azure CLI to deploy an ARM template and exports essential environment variables for subsequent configuration steps.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/spark/index.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport RESOURCE_GROUP=[resource group name]\nexport STORAGE_ACCOUNT_NAME=[storage account name]\nexport STORAGE_CONTAINER_NAME=[blob container name]\nexport DATABRICKS_WORKSPACE_NAME=[databricks workspace name]\nexport DATABRICKS_SKU=[basic/standard/premium]\nexport LOCATION=[location]\n\n# Create the resource group\naz group create --name $RESOURCE_GROUP --location $LOCATION\n\n# Use ARM template to build the resources and get back the workspace URL\ndeployment_response=$(az deployment group create -g $RESOURCE_GROUP --template-file ./docs/samples/deployments/spark/arm-template/databricks.json  --parameters location=$LOCATION workspaceName=$DATABRICKS_WORKSPACE_NAME storageAccountName=$STORAGE_ACCOUNT_NAME containerName=$STORAGE_CONTAINER_NAME)\n\nexport DATABRICKS_WORKSPACE_URL=$(echo $deployment_response | jq -r \".properties.outputs.workspaceUrl.value\")\nexport DATABRICKS_WORKSPACE_ID=$(echo $deployment_response | jq -r \".properties.outputs.workspaceId.value\")\n```\n\n----------------------------------------\n\nTITLE: Loading Complex Nested JSON Data with Presidio in Python\nDESCRIPTION: Reads a complex JSON file containing nested objects in lists using the JsonReader class from presidio_structured.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# contains nested objects in lists\nsample_complex_json = JsonReader().read(\"./sample_data/test_structured_complex.json\")\nsample_complex_json\n```\n\n----------------------------------------\n\nTITLE: Document Intelligence OCR Integration\nDESCRIPTION: Python code examples for setting up and using the Azure Document Intelligence OCR engine with Presidio.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/index.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndiOCR = DocumentIntelligenceOCR(endpoint=\"<your_endpoint>\", key=\"<your_key>\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndiOCR = DocumentIntelligenceOCR()\nia_engine = ImageAnalyzerEngine(ocr=di_ocr)\nmy_engine = ImageRedactorEngine(image_analyzer_engine=ia_engine)\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Text Analysis with Docker\nDESCRIPTION: Downloads and runs Docker containers for Presidio Analyzer and Anonymizer services\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/installation.md#2025-04-23_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# Download Docker images\ndocker pull mcr.microsoft.com/presidio-analyzer\ndocker pull mcr.microsoft.com/presidio-anonymizer\n\n# Run containers with default ports\ndocker run -d -p 5002:3000 mcr.microsoft.com/presidio-analyzer:latest\n\ndocker run -d -p 5001:3000 mcr.microsoft.com/presidio-anonymizer:latest\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PII Pseudonymization\nDESCRIPTION: This snippet imports necessary classes and functions from Presidio and other required libraries for PII analysis and anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonymization.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine, OperatorConfig\nfrom presidio_anonymizer.operators import Operator, OperatorType\n\nfrom typing import Dict\nfrom pprint import pprint\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Analyzer as HTTP Server using Python Runtime\nDESCRIPTION: This snippet demonstrates how to run the Presidio Analyzer as an HTTP server using the Python runtime. It includes starting the server and making a curl request to analyze text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/index.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncd presidio-analyzer\npython app.py\ncurl -d '{\"text\":\"John Smith drivers license is AC432223\", \"language\":\"en\"}' -H \"Content-Type: application/json\" -X POST http://localhost:3000/analyze\n```\n\n----------------------------------------\n\nTITLE: Calling the /analyze Endpoint with Ad-hoc Recognizers - Presidio Analyzer API - JSON\nDESCRIPTION: Provides a full example of a JSON request to the /analyze endpoint in Presidio, including an ad-hoc recognizer for ZIP code pattern detection. Dependencies include an active Presidio Analyzer API and prior definition of supported entities. Mandatory fields include input text, language, entities to search for, score threshold, and the ad-hoc recognizers field with regex pattern logic. The request returns detection results scoped to the supplied recognizers and configuration.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/09_ad_hoc.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"text\": \"John Smith drivers license is AC432223 and the zip code is 12345\",\n  \"language\": \"en\",\n  \"return_decision_process\": false,\n  \"correlation_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"score_threshold\": 0.6,\n  \"entities\": [\n    \"US_DRIVER_LICENSE\",\n    \"ZIP\"\n  ],\n  \"trace\": false,\n  \"ad_hoc_recognizers\": [\n    {\n      \"name\": \"Zip code Recognizer\",\n      \"supported_language\": \"en\",\n      \"patterns\": [\n        {\n          \"name\": \"zip code (weak)\",\n          \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\",\n          \"score\": 0.01\n        }\n      ],\n      \"context\": [\n        \"zip\",\n        \"code\"\n      ],\n      \"supported_entity\": \"ZIP\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Output PII Parsing\nDESCRIPTION: YAML configuration for enabling PII output parsing in LiteLLM\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/docker/litellm.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlitellm_settings:\n    output_parse_pii: true\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running E2E Tests on Windows\nDESCRIPTION: Commands to set up a virtual environment, install requirements, and run E2E tests for Presidio on Windows CMD/Powershell.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npy -m venv presidio-e2e\npresidio-e2e\\Scripts\\activate\npip install -r requirements.txt\npytest\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Downloading spaCy Model for Presidio Analyzer - Shell\nDESCRIPTION: This command uses Python's module syntax to download the 'en_core_web_lg' spaCy model, required by the Presidio Analyzer for language processing and PII detection. The spaCy library must be installed in the environment before executing this command. It should be run after installing the Presidio Image Redactor package to enable English NLP support.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/README.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Using Presidio Image Redactor API via cURL (Shell)\nDESCRIPTION: This command demonstrates how to interact with the running Presidio image redactor Docker container's API using `curl`. It sends a POST request to the `/redact` endpoint, uploading an image file (`img.png`) via multipart form data. It also includes redaction parameters (like `color_fill`) in JSON format within the `data` field. The redacted image output is saved to `out.png`.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_images.md#2025-04-23_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncurl -XPOST \"http://localhost:5003/redact\" -H \"content-type: multipart/form-data\" -F \"image=@img.png\" -F \"data=\\\"{\\'color_fill\\':\\'255\\'}\\\"\" > out.png\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio and Required Dependencies\nDESCRIPTION: This snippet installs the Presidio Analyzer and Anonymizer libraries, as well as the required spaCy language model.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonymization.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Initializing Presidio Engines Configuration\nDESCRIPTION: Sets up the core Presidio engines for analysis, anonymization, and de-anonymization with support for multiple languages. Configures NLP engine with specific language models and creates analyzer and anonymizer instances.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n        configuration = {\n            \"nlp_engine_name\": \"spacy\",\n            \"models\": [\n                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n                {\"lang_code\": \"nl\", \"model_name\": \"nl_core_news_sm\"},\n                {\"lang_code\": \"es\", \"model_name\": \"es_core_news_sm\"},\n            ],\n        }\n        provider = NlpEngineProvider(nlp_configuration=configuration)\n        nlp_engine = provider.create_engine()\n\n        self.analyzer = AnalyzerEngine(\n            nlp_engine=nlp_engine,\n            supported_languages=[\"en\", \"nl\", \"es\"]\n        )\n        self.anonymizer = AnonymizerEngine()\n        self.anonymizer.add_anonymizer(InstanceCounterAnonymizer)\n        self.deanonymizer = DeanonymizeEngine()\n        self.deanonymizer.add_deanonymizer(InstanceCounterDeanonymizer)\n```\n\n----------------------------------------\n\nTITLE: Adding DICOM Image Redaction with DicomImageRedactorEngine (Python)\nDESCRIPTION: Introduces the `DicomImageRedactorEngine` Python class, added in version 2.2.31, specifically designed to handle the redaction of sensitive information within DICOM image files. This includes handling pixel data and potentially metadata. Requires dependencies listed in `setup.py`.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nDicomImageRedactorEngine\n```\n\n----------------------------------------\n\nTITLE: Refactoring NlpEngine and NER Recognizers for Transformers Integration (Python)\nDESCRIPTION: Describes a major refactoring (#1159) of core Presidio Analyzer classes: `NlpEngine`, `SpacyRecognizer`, `TransformersRecognizer`, and `StanzaRecognizer`. This refactoring simplifies the integration of Hugging Face Transformers models, changes how NER results flow, introduces configuration via files or `NerModelConfiguration`, integrates `spacy-huggingface-pipelines`, and deprecates some fields in `SpacyRecognizer`. Requires understanding of Presidio's architecture and potentially updating model configurations.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nNlpEngine\n```\n\nLANGUAGE: Python\nCODE:\n```\nSpacyRecognizer\n```\n\nLANGUAGE: Python\nCODE:\n```\nTransformersRecognizer\n```\n\nLANGUAGE: Python\nCODE:\n```\nStanzaRecognizer\n```\n\n----------------------------------------\n\nTITLE: Loading Recognizers from YAML Configuration (YAML/Python)\nDESCRIPTION: Introduced in version 2.2.28, this feature allows Presidio recognizers to be defined and loaded from YAML files. This provides a declarative way to configure custom or predefined recognizers for the AnalyzerEngine.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_19\n\nLANGUAGE: YAML\nCODE:\n```\nRecognizers can be loaded from YAML\n```\n\n----------------------------------------\n\nTITLE: Using Parameter-Based Configuration for Presidio CLI - Shell\nDESCRIPTION: This shell snippet demonstrates how to supply configuration data directly to the presidio-cli via the -d parameter. It allows on-the-fly definition of ignore paths or entity lists, and can reference external files with command substitution. Dependencies are the same as for presidio-cli, and output is PII detection results for specified directories or files.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n# ignore paths .git and *.cfg\\npresidio -d \\\"ignore: |\\n  .git\\n  *.cfg\\\" tests/\\n\\n# limit list of entities to CREDIT_CARD\\npresidio -d \\\"entities:\\n  - CREDIT_CARD\\\" tests/\\n\\n# equivalent to use -c parameter\\npresidio -d \\\"$(cat presidio_cli/conf/limited.yaml)\\\" tests/\n```\n\n----------------------------------------\n\nTITLE: Configuring Presidio Callback in LiteLLM\nDESCRIPTION: YAML configuration for setting up Presidio as a callback in LiteLLM proxy\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/docker/litellm.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_list:\n  - model_name: my-openai-model ### RECEIVED MODEL NAME ###\n    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input\n      model: gpt-3.5-turbo ### MODEL NAME sent to `litellm.completion()` ###\n\nlitellm_settings: \n    callbacks = [\"presidio\"]\n```\n\n----------------------------------------\n\nTITLE: Running Scripts with Poetry\nDESCRIPTION: Examples of running various scripts within the Poetry virtual environment.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry run ruff check\npoetry run pip freeze\npoetry run python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Printing the Synthetic Text Generated by OpenAI in Python\nDESCRIPTION: Prints the value stored in the `gpt_res` variable, which contains the synthetic text returned by the OpenAI completion model in the preceding step.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(gpt_res)\n```\n\n----------------------------------------\n\nTITLE: Downloading Presidio Docker Images\nDESCRIPTION: Shell commands to pull the Presidio analyzer and anonymizer Docker images from Microsoft Container Registry.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_text.md#2025-04-23_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndocker pull mcr.microsoft.com/presidio-analyzer\ndocker pull mcr.microsoft.com/presidio-anonymizer\n```\n\n----------------------------------------\n\nTITLE: Enhancing ImagePiiVerifyEngine for Customization and Verification (Python)\nDESCRIPTION: Details updates to the `ImagePiiVerifyEngine` Python class across versions 2.2.31 and 2.2.32. Modifications allow passing keyword arguments (kwargs) for greater flexibility and enable the use of custom Presidio Analyzer engines during the image PII verification process.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nImagePiiVerifyEngine\n```\n\n----------------------------------------\n\nTITLE: Introducing BatchAnalyzerEngine in Presidio Analyzer (Python)\nDESCRIPTION: Introduces the `BatchAnalyzerEngine` class located in `presidio-analyzer/presidio_analyzer/batch_analyzer_engine.py` (added in version 2.2.29). This engine is designed to process multiple text inputs in a batch for improved performance when analyzing large datasets.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\npresidio-analyzer/presidio_analyzer/batch_analyzer_engine.py\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Tabular Data for Presidio Processing\nDESCRIPTION: This code creates a sample dataset with personal information to be used for demonstrating Presidio's batch processing capabilities on tabular data.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncolumns = [\"name phrase\", \"phone number phrase\", \"integer\", \"boolean\" ]\nsample_data = [\n        ('Charlie likes this', 'Please call 212-555-1234 after 2pm', 1, True),\n        ('You should talk to Mike', 'his number is 978-428-7111', 2, False),\n        ('Mary had a little startup', 'Phone number: 202-342-1234', 3, False)\n]\n```\n\n----------------------------------------\n\nTITLE: Presidio CLI YAML Configuration Example - YAML\nDESCRIPTION: This YAML snippet provides an example configuration for presidio-cli, specifying the language, patterns to ignore, entities to detect, and allowed tokens. The file structure supports configuring PII detection via keys: language, ignore, entities, and allow. It must reside in the project root or be supplied explicitly on the CLI; at least one parameter must be set.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n---\\nlanguage: en\\nignore: |\\n  .git\\n  *.cfg\\nentities:\\n  - PERSON\\n  - CREDIT_CARD\\n  - EMAIL_ADDRESS\\nallow:\\n  - \\\"allowed token 1\\\"\\n  - \\\"allowed token 2\\\"\\n\n```\n\n----------------------------------------\n\nTITLE: Defining US Passport Recognizer Pattern in Presidio Analyzer (Python)\nDESCRIPTION: Specifies the location of the Python file (`presidio-analyzer/presidio_analyzer/predefined_recognizers/us_passport_recognizer.py`) containing the predefined recognizer patterns for US passport numbers. This file was updated in version 2.2.30 to include patterns for next-generation US passports.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\npresidio-analyzer/presidio_analyzer/predefined_recognizers/us_passport_recognizer.py\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio and Dependencies in Python\nDESCRIPTION: This snippet installs the Presidio Analyzer and Anonymizer libraries, as well as the required spaCy language model for English.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\n!pip install presidio_analyzer presidio_anonymizer\n\n!python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data with Presidio in Python\nDESCRIPTION: Reads a CSV file containing structured data using the CsvReader class from presidio_structured.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsample_df = CsvReader().read(\"./csv_sample_data/test_structured.csv\")\nsample_df\n```\n\n----------------------------------------\n\nTITLE: Scaling PII Detection and Anonymization with Data Duplication\nDESCRIPTION: Demonstrates how to scale up PII detection and anonymization by duplicating rows to create a larger dataset. This is useful for testing performance on large datasets and evaluating how well the solution scales with increasing data volume.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf_expanded = df.withColumn(\n    \"duplication_array\",\n    array([lit(i) for i in range(num_duplicates)])\n)\n\ndf_test = df_expanded.withColumn(\"duplicate_id\", explode(col(\"duplication_array\")))\n\ndf_test = df_test.withColumn(\"id\", monotonically_increasing_id())\n\ndf_test = df_test.withColumn(\n    \"user_query\",\n    concat(col(\"user_query\"), lit(\" - ID: \"), col(\"id\"))\n)\n\ndf_test = df_test.drop(\"duplication_array\", \"duplicate_id\")\ndf_test = df_test.repartition(partitions_number) # repartition to show parrallel processing -should be remove/modify to allow high scales.\ndf_test = df_test.withColumn(\"anon_user_query\", anon_udf(col(\"user_query\")))\nprint(f\"total row number {df_test.count()}\") # Number of duplicates X number of rows in the DF\ndisplay(df_test.limit(partitions_number))\n# (df_test.limit(partitions_number)).show()\n```\n\n----------------------------------------\n\nTITLE: Combining and Saving YAML Configurations to Temporary File - Python\nDESCRIPTION: Assembles all configuration YAML strings into one unified string, writes it to a temporary file, and stores the file path. This temporary file serves as the configuration input to the Presidio Analyzer Engine. Relies on `tempfile.NamedTemporaryFile` for safe, transient file handling.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/08_no_code.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfull_config = f\"{analyzer_config_yaml}\\n{recognizer_registry_config_yaml}\\n{nlp_engine_yaml}\"\n\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.yaml') as temp_file:\n    # Write the YAML string to the temp file\n    temp_file.write(full_config)\n    temp_file_path = temp_file.name\n\n\n```\n\n----------------------------------------\n\nTITLE: Ground Truth JSON Format for DICOM De-identification\nDESCRIPTION: Example structure of a ground truth JSON file containing PHI locations in DICOM images. Each file entry contains an array of objects with label, position (left, top), and size (width, height) information.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/evaluating_dicom_redaction.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"your/dicom/dir/file_0.dcm\": [\n        {\n            \"label\": \"DAVIDSON\",\n            \"left\": 25,\n            \"top\": 25,\n            \"width\": 241,\n            \"height\": 37\n        },\n        {\n            \"label\": \"DOUGLAS\",\n            \"left\": 287,\n            \"top\": 25,\n            \"width\": 230,\n            \"height\": 36\n        },\n        {\n            \"label\": \"[M]\",\n            \"left\": 535,\n            \"top\": 25,\n            \"width\": 60,\n            \"height\": 45\n        },\n        {\n            \"label\": \"01.09.2012\",\n            \"left\": 613,\n            \"top\": 26,\n            \"width\": 226,\n            \"height\": 35\n        },\n        {\n            \"label\": \"06.16.1976\",\n            \"left\": 170,\n            \"top\": 72,\n            \"width\": 218,\n            \"height\": 35\n        }\n    ],\n    \"your/dicom/dir/file_1.dcm\": [\n        ...\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Presidio Services via ARM Template using Azure CLI in Bash\nDESCRIPTION: This Bash script uses the `az deployment group create` command to initiate a deployment within a specified resource group (`$RESOURCE_GROUP`). It deploys resources defined in the `presidio-services.json` ARM template file, using parameter values supplied in the `values.json` file. This provides an alternative, declarative way to deploy the required Azure infrastructure for Presidio.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/app-service/index.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naz deployment group create --resource-group $RESOURCE_GROUP --template-file presidio-services.json --parameters @values.json\n```\n\n----------------------------------------\n\nTITLE: Integrating Transformers Models with Presidio Analyzer (Python Sample)\nDESCRIPTION: Points to a Python sample file (`docs/samples/python/transformers_recognizer.py`), added in version 2.2.29. This sample demonstrates the integration of Hugging Face `transformers` based NER models with the Presidio Analyzer for PII detection tasks.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ndocs/samples/python/transformers_recognizer.py\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Presidio Configuration\nDESCRIPTION: This snippet imports necessary Python libraries for working with YAML, JSON, temporary files, and the Presidio Analyzer.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/no_code_config.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\nimport json\nimport tempfile\nimport warnings\nfrom pprint import pprint\nfrom presidio_analyzer import AnalyzerEngineProvider\n\nwarnings.filterwarnings(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Installing Poetry Dependencies for Presidio Analyzer\nDESCRIPTION: Command to install all dependencies, including dev requirements, for the Presidio Analyzer using Poetry.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --all-extras\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Image Redactor as a Docker Service - Shell\nDESCRIPTION: This shell command starts the Presidio Image Redactor as a background Docker service using docker-compose. The expected context is the 'presidio/presidio-image-redactor' directory containing a properly configured 'docker-compose.yml' file. Docker and docker-compose must be installed. The service exposes API endpoints for programmatic redaction of PII from images.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Displaying Anonymized Dataset in Python\nDESCRIPTION: This snippet prints the dataset after anonymization to show the results of the ad-hoc anonymization process.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Dataset now contains the anonymiezd version as well\ndataset\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for Presidio Application\nDESCRIPTION: Lists required Python packages and their specific versions for a Presidio deployment. Includes FastAPI for the web framework, Uvicorn for ASGI server, Presidio analyzer and anonymizer components, Pydantic for data validation, and Redis for caching.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastapi[standard]==0.114.2\nuvicorn[standard]==0.30.6\npresidio-analyzer==2.2.355\npresidio-anonymizer==2.2.355\npydantic==2.9.2\nredis==5.0.8\n```\n\n----------------------------------------\n\nTITLE: Printing Loaded Configuration for Both Languages\nDESCRIPTION: This snippet prints the supported entities and loaded recognizers for both English and Spanish, as well as the loaded NER models.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/no_code_config.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor lang in (\"en\", \"es\"):\n    pprint(f\"Supported entities for {lang}:\")\n    print(\"\\n\")\n    pprint(analyzer_engine.get_supported_entities(lang), compact=True)\n    \n    print(f\"\\nLoaded recognizers for {lang}:\")\n    pprint([rec.name for rec in analyzer_engine.registry.get_recognizers(lang, all_fields=True)], compact=True)\n    print(\"\\n\")\n   \nprint(f\"\\nLoaded NER models:\")\npprint(analyzer_engine.nlp_engine.models)\n```\n\n----------------------------------------\n\nTITLE: Example Data for PHONE_NUMBER Entity (Parentheses Format)\nDESCRIPTION: Provides sample text containing a US phone number ('(425) 882-9090') labeled as PHONE_NUMBER, including context words 'my phone number is'. Tests recognition of standard US phone format with parentheses.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nPHONE_NUMBER\nmy phone number is (425) 882-9090\n```\n\n----------------------------------------\n\nTITLE: Downloading Stanza Language Model\nDESCRIPTION: Python code to download a Stanza language model using the stanza.download() function.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/nlp_engines/spacy_stanza.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport stanza\nstanza.download(\"en\") # where en is the language code of the model.\n```\n\n----------------------------------------\n\nTITLE: Installing Poetry using Pip in Bash\nDESCRIPTION: Command to install Poetry, a Python package manager, using Pip in a Bash shell.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install poetry\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Anonymizer as a Docker Service - Shell\nDESCRIPTION: This shell snippet provides the command to start the Presidio Anonymizer service as a background Docker Compose application. This usage is intended for those wishing to run Presidio Anonymizer as an HTTP API server. It requires Docker and Docker Compose to be installed, and the working directory should be presidio/presidio-anonymizer where the relevant docker-compose.yml file is located.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-anonymizer/README.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Running Build and E2E Tests Script on Mac/Linux/WSL\nDESCRIPTION: Commands to make the run script executable and execute it for building and running E2E tests on Mac/Linux/WSL.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nchmod +x run.bat\n./run.bat\n```\n\n----------------------------------------\n\nTITLE: Custom spaCy Pipeline Integration with Presidio\nDESCRIPTION: Implementation showing how to reuse an existing spaCy pipeline in Presidio by extending the SpacyNlpEngine class and creating a custom analyzer.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/nlp_engines/spacy_stanza.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import SpacyNlpEngine\nimport spacy\n\n# Create a class inheriting from SpacyNlpEngine\nclass LoadedSpacyNlpEngine(SpacyNlpEngine):\n    def __init__(self, loaded_spacy_model):\n        super().__init__()\n        self.nlp = {\"en\": loaded_spacy_model}\n\n# Load a model a-priori\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Pass the loaded model to the new LoadedSpacyNlpEngine\nloaded_nlp_engine = LoadedSpacyNlpEngine(loaded_spacy_model = nlp)\n\n# Pass the engine to the analyzer\nanalyzer = AnalyzerEngine(nlp_engine = loaded_nlp_engine)\n\n# Analyze text\nanalyzer.analyze(text=\"My name is Bob\", language=\"en\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Anonymizer API Demo Client\nDESCRIPTION: This snippet shows how to navigate to the client app directory and install the required Python packages using pip.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd src/client_app\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Image Redactor via Docker\nDESCRIPTION: Docker commands to pull and run the Presidio image redactor container from Microsoft Container Registry.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/index.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# Download image from Dockerhub\ndocker pull mcr.microsoft.com/presidio-image-redactor\n\n# Run the container with the default port\ndocker run -d -p 5003:3000 mcr.microsoft.com/presidio-image-redactor:latest\n```\n\n----------------------------------------\n\nTITLE: Printing Supported Entities and Recognizers per Language - Python\nDESCRIPTION: Iterates over both English and Spanish, printing the supported entities and recognizers loaded by the analyzer engine per language, and displaying loaded NER models. Uses `pprint` for clear, formatted output. Useful for verifying that multilingual and custom recognizers are properly configured.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/08_no_code.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor lang in (\"en\", \"es\"):\n    pprint(f\"Supported entities for {lang}:\")\n    print(\"\\n\")\n    pprint(analyzer_engine.get_supported_entities(lang), compact=True)\n    \n    print(f\"\\nLoaded recognizers for {lang}:\")\n    pprint([rec.name for rec in analyzer_engine.registry.get_recognizers(lang, all_fields=True)], compact=True)\n    print(\"\\n\")\n   \nprint(f\"\\nLoaded NER models:\")\npprint(analyzer_engine.nlp_engine.models)\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_BANK_NUMBER Entity (Alt Context)\nDESCRIPTION: Provides sample text containing a US Bank Account Number ('912803456') labeled as US_BANK_NUMBER, using alternative context words 'my banking account number is'. Tests recognition with varied but related context.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_22\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_BANK_NUMBER\nmy banking account number is 912803456\n```\n\n----------------------------------------\n\nTITLE: Installing presidio-image-redactor for DICOM (Shell)\nDESCRIPTION: This command installs the `presidio-image-redactor` Python package using pip. While identical to the previous installation command, this step is listed specifically in the context of preparing for DICOM image redaction.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_images.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio-image-redactor\n```\n\n----------------------------------------\n\nTITLE: Installing Poetry using Homebrew on MacOS\nDESCRIPTION: Command to install Poetry, a Python package manager, using Homebrew on MacOS.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbrew install poetry\n```\n\n----------------------------------------\n\nTITLE: Using OperatorResult for Anonymization/Deanonymization Results (Python)\nDESCRIPTION: Highlights the use of the `OperatorResult` Python class, which replaced `AnonymizerResult` in version 2.2.23. This class now returns an `operator_name` field (string) instead of the `operator` object itself, simplifying serialization and usage of anonymization results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nOperatorResult\n```\n\n----------------------------------------\n\nTITLE: Starting Specific Presidio Service with Docker Compose\nDESCRIPTION: Command to start a specific Presidio service (e.g., analyzer) using Docker Compose.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up --build -d presidio-analyzer\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio Notebooks to Databricks Workspace\nDESCRIPTION: Bash command to import Presidio notebooks to the Databricks workspace. These notebooks contain the code for PII analysis and anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/spark/index.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndatabricks workspace import_dir \"./notebooks\" \"/notebooks\" --overwrite\n```\n\n----------------------------------------\n\nTITLE: Running Anonymizer API Demo Client with LLM in Terminal\nDESCRIPTION: This command runs the client application in the terminal, allowing the user to chat with an LLM using the Anonymizer API.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython client.py\n```\n\n----------------------------------------\n\nTITLE: Adding Presidio CLI with Poetry - Shell\nDESCRIPTION: This shell snippet demonstrates how to add presidio-cli as a dependency using Poetry within a Python virtual environment. The command modifies pyproject.toml to include presidio-cli and installs its dependencies. Poetry and the correct Python version must be installed prior to use.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npoetry add presidio-cli\n```\n\n----------------------------------------\n\nTITLE: Example PII Detection: Date, URL, Email, IP Address\nDESCRIPTION: An example sentence showcasing Presidio's ability to detect and mask Date/Time information, Uniform Resource Locators (URLs), Email Addresses, and IP Addresses within a text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/e2e-tests/resources/demo_anonymized.txt#2025-04-23_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nOn <DATE_TIME> I visited <URL> and sent an email to <EMAIL_ADDRESS>,  from IP <IP_ADDRESS>.\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_DRIVER_LICENSE Entity (Alphanumeric with Mask)\nDESCRIPTION: Provides sample text containing a US Driver License number ('AA1B2**9ABA7') with masked characters, labeled as US_DRIVER_LICENSE, including context words 'my driver license number:'. Tests recognition of partially masked or specific alphanumeric patterns.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_DRIVER_LICENSE\nmy driver license number: AA1B2**9ABA7\n```\n\n----------------------------------------\n\nTITLE: Ad-hoc Deny-List Recognizers JSON for /analyze API (Presidio, JSON)\nDESCRIPTION: This JSON provides two deny-list based ad-hoc recognizers for the Presidio Analyzer API. Each recognizer flags the occurrence of specified words (e.g., title honorifics) as a specific entity in the analyzed text. Meant for inline, request-scoped custom PII recognition without prior code deployment. Each recognizer specifies the name, language, deny_list, and supported_entity fields.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/adding_recognizers.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"Mr. John Smith's drivers license is AC432223\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Mr. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Mr\", \"Mr.\", \"Mister\"],\n        \"supported_entity\":\"MR_TITLE\"\n        },\n        {\n        \"name\": \"Ms. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Ms\", \"Ms.\", \"Miss\", \"Mrs\", \"Mrs.\"],\n        \"supported_entity\":\"MS_TITLE\"\n        }\n    ]\n}\n\n```\n\n----------------------------------------\n\nTITLE: Example PII Detection: Credit Card and Crypto Wallet\nDESCRIPTION: An example sentence illustrating the detection of financial identifiers like Credit Card numbers and Cryptocurrency wallet IDs, represented by placeholders. It shows Presidio's capability in handling sensitive financial data.\nSOURCE: https://github.com/microsoft/presidio/blob/main/e2e-tests/resources/demo_anonymized.txt#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nMy credit card number is <CREDIT_CARD> and my crypto wallet id is <CRYPTO>.\n```\n\n----------------------------------------\n\nTITLE: Downloading spaCy Pipeline in Shell\nDESCRIPTION: This command downloads a spaCy pipeline/model, which is required alongside the Transformers model.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/nlp_engines/transformers.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m spacy download en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Example PII Detection: Person, US SSN, US Driver License\nDESCRIPTION: An example sentence showing the detection of a Person's name alongside highly sensitive US-specific identifiers like Social Security Numbers (SSN) and Driver's License numbers.\nSOURCE: https://github.com/microsoft/presidio/blob/main/e2e-tests/resources/demo_anonymized.txt#2025-04-23_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n<PERSON>'s social security number is <US_SSN>.  Her driver license? it is <US_DRIVER_LICENSE>.\n```\n\n----------------------------------------\n\nTITLE: Installing presidio-image-redactor Package using pip (Shell)\nDESCRIPTION: This command installs the `presidio-image-redactor` Python package using pip, the Python package installer. This package is required for performing PII redaction on images using Presidio's Python interface.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/getting_started/getting_started_images.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio-image-redactor\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio Structured Libraries in Python\nDESCRIPTION: Imports necessary classes from presidio_structured for data analysis and processing of structured and semi-structured data.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_structured import StructuredEngine, JsonAnalysisBuilder, PandasAnalysisBuilder, StructuredAnalysis, CsvReader, JsonReader, JsonDataProcessor, PandasDataProcessor\n```\n\n----------------------------------------\n\nTITLE: Presidio Service Usage Examples\nDESCRIPTION: Demonstrates how to use the anonymization and de-anonymization services with sample requests. Shows the required parameters and expected usage pattern.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    # anonymize example\n    anonymized_text, new_entity_mappings = presidio_service.anonymize_text(\n            request.session_id, request.text, request.language, entity_mappings\n        )\n\n    # deanonymize example\n    deanonymized_text = presidio_service.deanonymize_text(\n            request.session_id, request.text, entity_mappings\n        )\n```\n\n----------------------------------------\n\nTITLE: Evaluating DICOM De-identification Performance using Python\nDESCRIPTION: Illustrates how to load a ground truth JSON file and utilize the `eval_dicom_instance` method of the `DicomImagePiiVerifyEngine` to assess the de-identification performance for a given DICOM instance against the loaded ground truth. Requires `json` for loading, `pydicom` for DICOM reading, and `presidio_image_redactor`. Evaluation parameters like padding, tolerance, and OCR threshold can be customized.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/Evaluation_Approach.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Load ground truth for one file\nwith open(gt_path) as json_file:\n    all_ground_truth = json.load(json_file)\nground_truth = all_ground_truth[file_of_interest]\n\n# Select your DICOM instance\ninstance = pydicom.dcmread(file_of_interest)\n\n# Evaluate the DICOM de-identification performance\n_, eval_results = dicom_engine.eval_dicom_instance(instance, ground_truth)\n```\n\n----------------------------------------\n\nTITLE: Example PII Detection: IBAN and US Bank Account\nDESCRIPTION: An example sentence demonstrating the identification of International Bank Account Numbers (IBAN) and US-specific Bank Account numbers, indicated by placeholders.\nSOURCE: https://github.com/microsoft/presidio/blob/main/e2e-tests/resources/demo_anonymized.txt#2025-04-23_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nThis is a valid International Bank Account Number: <IBAN_CODE> . Can you please check the status on bank account <US_BANK_NUMBER>?\n```\n\n----------------------------------------\n\nTITLE: Example Analyzer Configuration Section for Multi-File Setup in YAML\nDESCRIPTION: This YAML snippet shows an example structure for the analyzer-specific configuration file (`analyzer-config.yml`) when using the multi-file setup for Presidio. It contains parameters directly related to the analyzer engine itself, such as `supported_languages` and `default_score_threshold`. The NLP and recognizer configurations are provided in separate files.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/analyzer_engine_provider.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsupported_languages: \n- en\ndefault_score_threshold: 0\n```\n\n----------------------------------------\n\nTITLE: Example PII Detection: US Passport and Phone Number\nDESCRIPTION: An example sentence illustrating the detection of specific identification numbers like US Passport numbers and general Phone Numbers, which are replaced by corresponding placeholders.\nSOURCE: https://github.com/microsoft/presidio/blob/main/e2e-tests/resources/demo_anonymized.txt#2025-04-23_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nMy passport: <US_PASSPORT> and my phone number: <PHONE_NUMBER>.\n```\n\n----------------------------------------\n\nTITLE: Referencing LocalRecognizer - Python\nDESCRIPTION: Supplies the LocalRecognizer class from presidio_analyzer.local_recognizer, designed for recognition tasks with local (non-remote) resources or models. Used to define recognizers operating purely within the executing environment, with customizable patterns and validation logic.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.local_recognizer.LocalRecognizer\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy Language Model via Command Line\nDESCRIPTION: Command to download a pre-trained spaCy model, specifically the medium-sized Spanish language model.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/nlp_engines/spacy_stanza.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython -m spacy download es_core_news_md\n```\n\n----------------------------------------\n\nTITLE: Configuring NER Models with NerModelConfiguration Object (Python)\nDESCRIPTION: Introduces the `NerModelConfiguration` Python object, used alongside configuration files as part of the refactoring (#1159). This object allows developers to define and pass NER model configurations programmatically when setting up Presidio's NLP engine, offering an alternative to conf files.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nNerModelConfiguration\n```\n\n----------------------------------------\n\nTITLE: Generating API Reference for Presidio Image Redactor in Python\nDESCRIPTION: This code snippet configures the documentation generator to process the 'presidio_image_redactor' module. It specifies Python as the handler and sets the docstring style to Sphinx format.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/image_redactor_python.md#2025-04-23_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n::: presidio_image_redactor\n    handler: python\n    options:\n      docstring_style: sphinx\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Image Redactor with Docker\nDESCRIPTION: Downloads and runs Docker container for Presidio Image Redactor service\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/installation.md#2025-04-23_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# Download Docker image\ndocker pull mcr.microsoft.com/presidio-image-redactor\n\n# Run container with the default port\ndocker run -d -p 5003:3000 mcr.microsoft.com/presidio-image-redactor:latest\n```\n\n----------------------------------------\n\nTITLE: HTTP API Redact Endpoint Example Payload - JSON\nDESCRIPTION: This JSON snippet shows the payload structure for an HTTP POST request to the Presidio Image Redactor API's /redact endpoint. The 'data' field contains the desired color fill in string format (e.g., '0,0,0' for black). The API expects multipart-form submission including this payload and an image file. The color_fill can be a single value or an RGB triplet.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/README.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": \"{'color_fill':'0,0,0'}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching Text Templates from a URL in Python\nDESCRIPTION: Imports the `urllib` module. It defines a URL pointing to a raw text file on GitHub containing templates. It iterates through the fetched lines, decodes each from UTF-8, and appends it to the `templates` list.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport urllib\n\ntemplates = []\n\nurl = \"https://raw.githubusercontent.com/microsoft/presidio-research/master/presidio_evaluator/data_generator/raw_data/templates.txt\"\nfor line in urllib.request.urlopen(url):\n    templates.append(line.decode('utf-8')) \n```\n\n----------------------------------------\n\nTITLE: Comparing Anonymizer API Requests in Legacy gRPC (V1) and HTTP-based (V2) formats\nDESCRIPTION: This snippet shows the request format changes for the Presidio Anonymizer service between V1 and V2. The V2 format is simplified with a flatter structure, uses snake_case instead of camelCase, and removes deprecated fields from the template.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/presidio_V2.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"text\": \"hello world, my name is Jane Doe. My number is: 034453334\",\n  \"template\": {\n    \"description\": \"DEPRECATED\",\n    \"create_time\": \"DEPRECATED\",\n    \"modified_time\": \"DEPRECATED\",\n    \"default_transformation\": {\n      \"replace_value\": {...},\n      \"redact_value\": {...},\n      \"hash_value\": {...},\n      \"mask_value\": {...},\n      \"fpe_value\": {...}\n    },\n    \"field_type_transformations\": [\n      {\n        \"fields\": [\n          {\n            \"name\": \"FIRST_NAME\",\n            \"min_score\": \"0.2\"\n          }\n        ],\n        \"transfomarion\": {\n          \"replace_value\": {...},\n          \"redact_value\": {...},\n          \"hash_value\": {...},\n          \"mask_value\": {...},\n          \"fpe_value\": {...}\n        }\n      }\n    ],\n    \"analyze_results\": [\n      {\n        \"text\": \"Jane\",\n        \"field\": {\n          \"name\": \"FIRST_NAME\",\n          \"min_score\": \"0.5\"\n        },\n        \"location\": {\n          \"start\": 24,\n          \"end\": 32,\n          \"length\": 6\n        },\n        \"score\": 0.8\n      }\n    ]\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"hello world, my name is Jane Doe. My number is: 034453334\",\n    \"anonymizers\": {\n        \"DEFAULT\": {\n            \"type\": \"replace\",\n            \"new_value\": \"val\"\n        },\n        \"PHONE_NUMBER\": {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 4,\n            \"from_end\": true\n        }\n    },\n    \"analyzer_results\": [\n        {\n            \"start\": 24,\n            \"end\": 32,\n            \"score\": 0.8,\n            \"entity_type\": \"NAME\"\n        },\n        {\n            \"start\": 24,\n            \"end\": 28,\n            \"score\": 0.8,\n            \"entity_type\": \"FIRST_NAME\"\n        },\n        {\n            \"start\": 29,\n            \"end\": 32,\n            \"score\": 0.6,\n            \"entity_type\": \"LAST_NAME\"\n        },\n        {\n            \"start\": 48,\n            \"end\": 57,\n            \"score\": 0.95,\n            \"entity_type\": \"PHONE_NUMBER\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Referencing RecognizerResult - Python\nDESCRIPTION: Imports the RecognizerResult class from presidio_analyzer.recognizer_result, which structures the output for recognized entities during text analysis. Essential for standardized access to recognized entity information, such as type, score, location, and analysis details.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.recognizer_result.RecognizerResult\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Referencing BatchAnalyzerEngine - Python\nDESCRIPTION: Imports BatchAnalyzerEngine from presidio_analyzer.batch_analyzer_engine for bulk or batch processing of text items. Required when performing analysis over large datasets efficiently. Accepts lists or batches of documents as input, and outputs structured results per item.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.batch_analyzer_engine.BatchAnalyzerEngine\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Configuration for Version 2.2.357\nDESCRIPTION: YAML structure defining version 2.2.357 of the project, including updates to Analyzer and General components.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n## [2.2.357] - 2025-01-13\n\n### Analyzer\n- Example GLiNER integration (#1504)\n\n### General\n- Docs revamp and docstring bug fixes (#1500)\n- Minor updates to the mkdocstrings config (#1503)\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Image Redactor\nDESCRIPTION: Installs Presidio package for image redaction with required spaCy model\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/installation.md#2025-04-23_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio_image_redactor\n\n# Presidio image redactor uses the presidio-analyzer\n# which requires a spaCy language model:\npython -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Presidio Design Structure Markdown\nDESCRIPTION: Markdown structure defining the main components of Presidio's architecture with links to detailed documentation and design diagrams for each component.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/design.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Presidio design\n\n## Analyzer\n\n[![Analyzer Design](assets/analyzer-design.png)](analyzer/index.md)\n\n## Anonymizer\n\n[![Anonymizer Design](assets/anonymizer-design.png)](anonymizer/index.md)\n\n## Image Redactor\n\n### Standard Image Types\n\n[![Image Redactor Design](assets/image-redactor-design.png)](image-redactor/index.md)\n\n### DICOM Images\n\n[![DICOM Image Redactor Design](assets/dicom-image-redactor-design.png)](image-redactor/index.md)\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio Anonymizer Components in Python\nDESCRIPTION: This snippet imports the necessary classes from the Presidio Anonymizer package to perform selective anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/keep_entities.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n```\n\n----------------------------------------\n\nTITLE: Implementing Lemma-Based Context Enhancement with LemmaContextAwareEnhancer (Python)\nDESCRIPTION: Presents the `LemmaContextAwareEnhancer` class (`presidio-analyzer/presidio_analyzer/context_aware_enhancers/lemma_context_aware_enhancer.py`), added in version 2.2.25. This is a specific implementation of `ContextAwareEnhancer` that utilizes word lemmas (base forms of words) to match context words provided during analysis, enhancing detection based on semantic proximity.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\npresidio-analyzer/presidio_analyzer/context_aware_enhancers/lemma_context_aware_enhancer.py\n```\n\n----------------------------------------\n\nTITLE: Downloading spaCy English Language Model - Shell\nDESCRIPTION: This shell command downloads the large English language model required by spaCy for PII detection through Presidio Analyzer. It requires Python, spaCy installed, and an active internet connection. The command makes the model available for Presidio's entity recognition tasks.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Checking Presidio Installation Status with Helm\nDESCRIPTION: Command to display information about the newly configured Presidio system using Helm.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/k8s/charts/presidio/templates/NOTES.txt#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ helm status {{ .Release.Name }}\n```\n\n----------------------------------------\n\nTITLE: Introducing BboxProcessor for Bounding Box Operations (Python)\nDESCRIPTION: Introduces the new `BboxProcessor` Python class (version 2.2.32), created to consolidate common operations related to bounding boxes (bboxes), such as calculation or manipulation, primarily used within the image redaction modules.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nBboxProcessor\n```\n\n----------------------------------------\n\nTITLE: Referencing RemoteRecognizer - Python\nDESCRIPTION: Exposes the RemoteRecognizer class from presidio_analyzer.remote_recognizer, facilitating entity recognition by delegating processing to remote services or APIs. Used when local resources are insufficient or when integrating with external ML or NER services.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.remote_recognizer.RemoteRecognizer\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Referencing AnalysisExplanation - Python\nDESCRIPTION: Provides the AnalysisExplanation class from presidio_analyzer.analysis_explanation, utilized to access reasoning and metadata behind the analyzer's decision making. Critical for auditability and debugging of entity recognition results, it is typically used for explainability in results reporting.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/analyzer_python.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_analyzer.analysis_explanation.AnalysisExplanation\n    handler: python\n```\n\n----------------------------------------\n\nTITLE: Example PII Detection: Person and Location\nDESCRIPTION: An example sentence demonstrating how Presidio identifies and replaces Person names and Locations with placeholders. This illustrates the detection of common PII types in natural language text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/e2e-tests/resources/demo_anonymized.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nHello, my name is <PERSON> and I live in <LOCATION>.\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running E2E Tests on Mac/Linux/WSL\nDESCRIPTION: Commands to set up a virtual environment, install requirements, and run E2E tests for Presidio on Mac/Linux/WSL.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npython -m venv presidio-e2e\nsource presidio-e2e/bin/activate\npip install -r requirements.txt\npytest\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Defining a Deny-List of Titles in Python\nDESCRIPTION: This snippet initializes a Python list named `titles_list` containing common titles. This list will be used as a deny-list for identifying specific terms as Personally Identifiable Information (PII) in subsequent steps.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/01_deny_list.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\ntitles_list = [\n    \"Sir\",\n    \"Ma'am\",\n    \"Madam\",\n    \"Mr.\",\n    \"Mrs.\",\n    \"Ms.\",\n    \"Miss\",\n    \"Dr.\",\n    \"Professor\",\n]\n```\n```\n\n----------------------------------------\n\nTITLE: Running Presidio Services with Docker Compose - Shell\nDESCRIPTION: This shell snippet demonstrates how to start Presidio's required services as detached background processes using Docker Compose. The command builds necessary Docker images and launches all services defined in the docker-compose.yml file. Docker and docker-compose must be installed and the appropriate configuration files present. After execution, the HTTP services should be available for subsequent testing steps.\nSOURCE: https://github.com/microsoft/presidio/blob/main/e2e-tests/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker-compose up --build -d \n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies using pip (Shell)\nDESCRIPTION: Installs the required Python packages listed in the `requirements.txt` file using pip. Note that this may install optional packages like `transformers` and `flair` not strictly necessary for core Presidio functionality.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/streamlit/index.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -r requirements\n```\n\n----------------------------------------\n\nTITLE: Illustrating Formatted Analyzer (NER) Results from DicomImagePiiVerifyEngine in JSON\nDESCRIPTION: Shows an example JSON structure for formatted NER (Named Entity Recognition) or analyzer results from the `DicomImagePiiVerifyEngine`. Each object corresponds to a detected entity, specifying its type ('entity_type'), confidence score ('score'), and bounding box coordinates (left, top, width, height).\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/Evaluation_Approach.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n// Analyzer Results (formatted)\n[\n    {\n        \"entity_type\": \"PERSON\",\n        \"score\": 1.0,\n        \"left\": 25,\n        \"top\": 25,\n        \"width\": 241,\n        \"height\": 37\n    },\n    {\n        \"entity_type\": \"PERSON\",\n        \"score\": 1.0,\n        \"left\": 287,\n        \"top\": 25,\n        \"width\": 230,\n        \"height\": 36\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Writing Anonymized Data to Delta Table in Fabric\nDESCRIPTION: Saves the anonymized data to a Delta table in Microsoft Fabric for persistence and further analysis. This step is optional and executes only if the is_write_to_delta flag is set to True.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nif is_write_to_delta:\n    df_test.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_namne)\n```\n\n----------------------------------------\n\nTITLE: Listing Deployment Options in Markdown\nDESCRIPTION: This snippet uses Markdown to create a list of links to documentation for different deployment options for Microsoft Presidio. It covers various Azure services and Kubernetes.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/index.md#2025-04-23_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# Example deployments\n\n- [Azure App Service](app-service/index.md)\n- [Kubernetes](k8s/index.md)\n- [Spark/Azure Databricks](spark/index.md)\n- [Azure Data Factory](data-factory/index.md)\n- [Data Protection toolkit for OpenAI](openai-anonymaztion-and-deanonymaztion-best-practices/index.md)\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Requirements for Presidio Analyzer and Anonymizer\nDESCRIPTION: This snippet specifies all required and optional dependencies for running the Presidio Analyzer and Presidio Anonymizer, accompanied by additional libraries for streaming UI (streamlit), tagging, data manipulation (pandas), environment configuration, and integration with various NLP and AI services. It covers both mandatory packages and optional extras (e.g., stanza/transformers) that enhance natural language processing capabilities. These entries are typically placed in a requirements.txt file and require pip for installation; the listed packages must be available from the Python Package Index or relevant sources.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/streamlit/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\npresidio-analyzer[transformers]\npresidio-analyzer[stanza]\npresidio-anonymizer\nstreamlit\nstreamlit-tags\npandas\npython-dotenv\nst-annotated-text\ntorch\nflair\nopenai\nazure-ai-textanalytics\n```\n\n----------------------------------------\n\nTITLE: Example Data for IP_ADDRESS Entity\nDESCRIPTION: Provides sample text containing an IP address ('192.168.0.1') labeled as IP_ADDRESS, including context words 'my ip:'. This data is used for testing or training entity recognition models.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nIP_ADDRESS\nmy ip: 192.168.0.1\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Structured via pip - Shell\nDESCRIPTION: Demonstrates how to install the presidio-structured package using pip, enabling access to its structured data anonymization capabilities. This command must be executed in a supported Python environment. No additional inputs are required for installation; internet access is needed.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/structured/index.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install presidio-structured\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text Directly with a Custom Presidio Recognizer in Python\nDESCRIPTION: This snippet demonstrates using the `analyze` method of the custom `titles_recognizer` (a `PatternRecognizer` instance) to scan a sample text (`text1`) specifically for entities of type 'TITLE'. The identified results are then printed. It depends on the `titles_recognizer` object created previously.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/tutorial/01_deny_list.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom presidio_analyzer import PatternRecognizer\n\ntext1 = \"I suspect Professor Plum, in the Dining Room, with the candlestick\"\nresult = titles_recognizer.analyze(text1, entities=[\"TITLE\"])\nprint(f\"Result:\\n {result}\")\n```\n```\n\n----------------------------------------\n\nTITLE: Importing Presidio Analyzer Components\nDESCRIPTION: Imports necessary classes from Presidio Analyzer for NER model configuration.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NerModelConfiguration\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Configuration for Version 2.2.356\nDESCRIPTION: YAML structure defining version 2.2.356 of the project, including updates to Analyzer, Presidio-Structured, Image Redactor, and General components.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n## [2.2.356] - 2024-12-15\n\n### Analyzer\n- Added logic to handle phone numbers with country code (#1426) (Thanks @kauabh)\n- Added UK National Insurance Number Recognizer (#1446) (Thanks @hhobson)\n- Fixed regex match_time output (#1488) (Thanks @andrewisplinghoff)\n- Added fix to ensure configuration files are closed properly when loading them (#1423) (Thanks @saulbein)\n- Closing handles for YAML file (#1424) (Thanks @roeybc)\n- Reduce memory usage of Analyzer test suite (#1429) (Thanks @hhobson)\n- Added `batch_size` parameter to `BatchAnalyzerEngine` (#1449) (Thanks @roeybc)\n- Remove ignored labels from supported entities (#1454) (Thanks @omri374)\n- Update US_SSN CONTEXT and unit test (#1455) (Thanks @claesmk)\n- Fixed bug with Azure AI language context (#1458) (Thanks @omri374)\n- Add support for allow_list, allow_list_match, regex_flags in REST API (#1484) (Thanks @hdw868)\n- Add a link to model classes to simplify configuration (#1472) (Thanks @omri374)\n- Restricting spacy.cli for version 3.7.0 (#1495) (Thanks @kshitijcode)\n\n### Anonymizer\n- No changes specified for Anonymizer in this release.\n\n\n### Presidio-Structured\n- Fix presidio-structured build - lock numpy version (#1465) (Thanks @SharonHart)\n\n\n### Image Redactor\n- Fix bug with image conversion (#1445) (Thanks @omri374)\n\n### General\n- Removed Python 3.8 support (EOL) and added 3.12 (#1479) (Thanks @omri374)\n- Update Docker build to use gunicorn for containers (#1497) (Thanks @RKapadia01)\n- New Dev containers for analyzer, analyzer+transformers, anonymizer (#1459) (Thanks @roeybc)\n- Added dev containers for: analyzer, analyzer+transformers, anonymizer, and image redaction (#1450) (Thanks @roeybc)\n- Added support for allow_list, allow_list_match, regex_flags in REST API (#1488) (Thanks @hdw868)\n- Typo fix in if condition (#1419) (Thanks @omri374)\n- Minor notebook changes (#1420) (Thanks @omri374)\n- Do not release `presidio-cli` as part of the release pipeline (#1422) (Thanks @SharonHart)\n- (Docs) Use Presidio across Anthropic, Bedrock, VertexAI, Azure OpenAI, etc. with LiteLLM Proxy (#1421) (Thanks @krrishdholakia)\n- Update CI due to DockerCompose project name issue (#1428) (Thanks @omri374)\n- Update docker-compose installation docs (#1439) (Thanks @MWest2020)\n- Fix space typo in docs (#1459) (Thanks @artfuldev)\n- Unlock numpy after dropping 3.8 (#1480) (Thanks @SharonHart)\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_SSN Entity (Numeric)\nDESCRIPTION: Provides sample text containing a US Social Security Number ('078051120') labeled as US_SSN, including context words 'my social security number is'. This data is used for testing or training entity recognition models, specifically for numeric-only SSN formats.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_SSN\nmy social security number is 078051120\n```\n\n----------------------------------------\n\nTITLE: Generating Verification Data for DICOM Ground Truth Creation using Python\nDESCRIPTION: Demonstrates initializing the `DicomImagePiiVerifyEngine` and using its `verify_dicom_instance` method to obtain a verification image, OCR results, and NER (analyzer) results for a specific DICOM file. It also shows formatting these results using helper methods (`get_bboxes_from_ocr_results`, `get_bboxes_from_analyzer_results`) to facilitate ground truth JSON creation. Dependencies include `pydicom` and `presidio_image_redactor`.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/Evaluation_Approach.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pydicom\nfrom presidio_image_redactor import DicomImagePiiVerifyEngine\n\n# Initialize engine\ndicom_engine = DicomImagePiiVerifyEngine()\n\n# Choose your file to create ground truth for\nfilename = \"path/to/your/file.dcm\"\ninstance = pydicom.dcmread(filename)\npadding_width = 25\n\n# Get OCR and NER results\nverification_image, ocr_results, analyzer_results = dicom_engine.verify_dicom_instance(instance, padding_width)\n\n# Format results for more direct comparison\nocr_results_formatted = dicom_engine.bbox_processor.get_bboxes_from_ocr_results(ocr_results)\nanalyzer_results_formatted = dicom_engine.bbox_processor.get_bboxes_from_analyzer_results(analyzer_results)\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample Dataset for Ad-Hoc Anonymization in Python\nDESCRIPTION: This snippet creates a sample dataset containing personal information to demonstrate ad-hoc anonymization techniques.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nperson1 = {\"name\": \"Martin Smith\", \n           \"special_value\":\"145A\", \n           \"free_text\": \"Martin Smith, id 145A, likes playing basketball\"}\nperson2 = {\"name\":\"Deb Schmidt\", \n           \"special_value\":\"256B\", \n           \"free_text\": \"Deb Schmidt, id 256B likes playing soccer\"}\nperson3 = {\"name\":\"R2D2\", \n           \"special_value\":\"X1T2\", \n           \"free_text\": \"X1T2 is R2D2's special value\"}\n\ndataset = [person1, person2, person3]\ndataset\n```\n\n----------------------------------------\n\nTITLE: Displaying Example Text Templates in Python\nDESCRIPTION: Prints a header and then displays the first five items (indices 0-4) from the `templates` list, which was populated by fetching data from a URL in the previous snippet.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Example templates:\")\ntemplates[:5]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Basic Azure Resources for Presidio App Service using Azure CLI in Bash\nDESCRIPTION: This Bash script uses Azure CLI commands to create a Resource Group, an App Service Plan (Linux-based), and a Web App. It deploys a Presidio container image (defaulting to `presidio-analyzer` from Microsoft Container Registry) or allows specifying a private container registry with credentials. Key parameters like Resource Group name, App Service name, location, and SKU need to be defined beforehand.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/app-service/index.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nRESOURCE_GROUP=<resource group name>\nAPP_SERVICE_NAME=<name of app service>\nLOCATION=<location>\nAPP_SERVICE_SKU=<sku>\n\nIMAGE_NAME=mcr.microsoft.com/presidio-analyzer\n# the following parameters are only required if you build and deploy your own containers from a private registry\nACR_USER_NAME=<user name>\nACR_USER_PASSWORD=<password>\n\n# create the resource group\naz group create --name $RESOURCE_GROUP\n# create the app service plan\naz appservice plan create --name $APP_SERVICE_NAME-plan --resource-group $RESOURCE_GROUP  \\\n--is-linux --location $LOCATION --sku $APP_SERVICE_SKU\n# create the web app using the official presidio images\naz webapp create --name $APP_SERVICE_NAME --plan $APP_SERVICE_NAME-plan \\\n--resource-group $RESOURCE_GROUP -i $IMAGE_NAME\n\n# or alternatively, if building presidio and deploying from a private container registry\naz webapp create --name $APP_SERVICE_NAME --plan $APP_SERVICE_NAME-plan \\\n--resource-group $RESOURCE_GROUP -i $IMAGE_NAME -s $ACR_USER_NAME -w $ACR_USER_PASSWORD\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_PASSPORT Entity (Adjacent Context)\nDESCRIPTION: Provides sample text containing a US Passport Number ('912803456') labeled as US_PASSPORT, immediately adjacent to context words with no spaces ('mypassportnumberis912803456'). Tests recognition when context and entity are directly concatenated.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_27\n\nLANGUAGE: plaintext\nCODE:\n```\n# Verify adjacent context words\nUS_PASSPORT\nmypassportnumberis912803456\n```\n\n----------------------------------------\n\nTITLE: Removing Deprecated AnonymizerResult Class (Python)\nDESCRIPTION: Notes the removal of the `AnonymizerResult` Python class in version 2.2.23. It has been replaced by `OperatorResult` to provide a more consistent structure for results from both anonymization and deanonymization operations.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nAnonymizerResult\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_DRIVER_LICENSE Entity (Numeric, Uppercase Context)\nDESCRIPTION: Provides sample text containing a US Driver License number ('7774567901234') labeled as US_DRIVER_LICENSE, with uppercase context words 'my DRIVER LICENSE is:'. Tests case-insensitivity of context word matching.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\n# Verify Upper case works\nUS_DRIVER_LICENSE\nmy DRIVER LICENSE is: 7774567901234\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Presidio Analyzer\nDESCRIPTION: Command to set up port forwarding from local port 8080 to the Presidio analyzer pod\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/k8s/index.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward <presidio-analyzer-pod-name> 8080:8080 -n presidio\n```\n\n----------------------------------------\n\nTITLE: Checking and Installing Poetry - Shell\nDESCRIPTION: This shell snippet demonstrates how to check if the Poetry dependency manager is installed and how to install it via pip if not present. Poetry is necessary for managing and installing Python project dependencies, including presidio-cli. Required: Python 3.9 through 3.11 and pip must be installed. Output is the installed Poetry version or installs Poetry system-wide if missing.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# check if app is installed\\npoetry --version\\n\\n# install, if not available\\npip install poetry\n```\n\n----------------------------------------\n\nTITLE: Running Dockerized Anonymizer API Demo Client\nDESCRIPTION: This command runs the Docker container for the client application, mapping port 8081 and using the .env file for configuration. Note that the anonymizer API URL should not point to localhost for Docker compatibility.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 8081:8081 --env-file .env client\n```\n\n----------------------------------------\n\nTITLE: Initializing Spark Session for Presidio with PySpark\nDESCRIPTION: Creates a Spark session with the application name 'PresidioInFabric' that will be used for distributed PII detection and anonymization.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/artifacts/presidio_and_spark.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"PresidioInFabric\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Example Data for PHONE_NUMBER Entity (Multiple Numbers, Mixed Formats)\nDESCRIPTION: Provides sample text containing multiple phone numbers ('052 5552606', '074-7111234') in different formats, labeled as PHONE_NUMBER, with context 'try one of these phones'. Tests recognition of multiple entities and varied formats within one sentence.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nPHONE_NUMBER\ntry one of these phones 052 5552606 074-7111234\n```\n\n----------------------------------------\n\nTITLE: Installing Large SpaCy Model in Fabric Notebook\nDESCRIPTION: Command to install a large SpaCy language model (en_core_web_lg) from a Lakehouse location within a Fabric notebook using pip. This approach is necessary when the model size exceeds the environment's direct installation limits.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/fabric/env_setup.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install /lakehouse/default/Files/presidio/models/en_core_web_lg-3.8.0-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_SSN Entity (Hyphenated)\nDESCRIPTION: Provides sample text containing a US Social Security Number ('078-051121') labeled as US_SSN, including context words 'my ssn is'. This data is used for testing or training entity recognition models, specifically for hyphenated SSN formats.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_SSN\nmy ssn is 078-051121\n```\n\n----------------------------------------\n\nTITLE: Building Presidio from Source\nDESCRIPTION: Instructions for building and running Presidio containers from source code\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/installation.md#2025-04-23_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ndocker-compose up --build\n```\n\nLANGUAGE: sh\nCODE:\n```\ndocker build ./presidio-anonymizer -t presidio/presidio-anonymizer\n```\n\nLANGUAGE: sh\nCODE:\n```\ndocker run -d -p 5001:5001 presidio/presidio-anonymizer\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_SSN Entity (Hyphenated, Alt Context)\nDESCRIPTION: Provides sample text containing a US Social Security Number ('078-05-1121') labeled as US_SSN, using context words 'my ssns is'. This tests recognition with hyphenated format and alternate context ('ssns').\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_SSN\nmy ssns is 078-05-1121\n```\n\n----------------------------------------\n\nTITLE: Running Pytest for Presidio Services\nDESCRIPTION: Command to run Pytest for Presidio services using Poetry.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npoetry run pytest\n```\n\n----------------------------------------\n\nTITLE: Running Anonymizer API Demo Client in Manual Mode\nDESCRIPTION: This command runs the client application in manual mode, allowing the user to chat with themselves in the terminal using the Anonymizer API.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython client.py --mode manual\n```\n\n----------------------------------------\n\nTITLE: Example OCR and Analyzer Results JSON\nDESCRIPTION: Sample output showing the format of OCR and analyzer results, including position coordinates, confidence scores, and entity types.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/evaluating_dicom_redaction.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n// OCR Results\n[\n    {\n        \"left\": 25,\n        \"top\": 25,\n        \"width\": 241,\n        \"height\": 37,\n        \"conf\": 95.833916,\n        \"label\": \"DAVIDSON\"\n    },\n    {\n        \"left\": 287,\n        \"top\": 25,\n        \"width\": 230,\n        \"height\": 36,\n        \"conf\": 93.292221,\n        \"label\": \"DOUGLAS\"\n    }\n]\n\n// Analyzer Results\n[\n    {\n        \"entity_type\": \"PERSON\",\n        \"score\": 1.0,\n        \"left\": 25,\n        \"top\": 25,\n        \"width\": 241,\n        \"height\": 37\n    },\n    {\n        \"entity_type\": \"PERSON\",\n        \"score\": 1.0,\n        \"left\": 287,\n        \"top\": 25,\n        \"width\": 230,\n        \"height\": 36\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_SSN Entity (Numeric, Alt Context)\nDESCRIPTION: Provides sample text containing a US Social Security Number ('078051121') labeled as US_SSN, using context words 'my social security number is'. This tests recognition with numeric format and standard context.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_SSN\nmy social security number is 078051121\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Anonymization of Known Values in Presidio (Jupyter Notebook)\nDESCRIPTION: Provides a link to a Jupyter Notebook sample (`docs/samples/python/Anonymizing%20known%20values.ipynb`), added in version 2.2.29. This notebook illustrates how to configure Presidio to anonymize predefined or known strings within text using specific anonymizers or configurations.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_17\n\nLANGUAGE: Jupyter Notebook\nCODE:\n```\ndocs/samples/python/Anonymizing%20known%20values.ipynb\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_SSN Entity (Spaced Hyphens)\nDESCRIPTION: Provides sample text containing a US Social Security Number ('078-05-1121') labeled as US_SSN, including context words 'my social security number is'. This tests recognition with space-separated hyphenated parts.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_SSN\nmy social security number is 078-05-1121\n```\n\n----------------------------------------\n\nTITLE: Defining Presidio Structured API Documentation\nDESCRIPTION: This code snippet defines a custom documentation block for the Presidio Structured module. It specifies that the documentation should be generated for Python code.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/api/structured_python.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: presidio_structured\nhandler: python\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_SSN Entity (Hyphenated, Misleading Context)\nDESCRIPTION: Provides sample text containing a US Social Security Number ('078-05-1121') labeled as US_SSN, using potentially misleading context words 'my ssid is'. This tests robustness against similar but incorrect acronyms.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_SSN\nmy ssid is 078-05-1121\n```\n\n----------------------------------------\n\nTITLE: Building Docker Container for Anonymizer API Demo Client\nDESCRIPTION: This command builds a Docker container for the client application, packaging all dependencies and the application code.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t client .\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Cluster Communication\nDESCRIPTION: Command to check the kubectl version and verify cluster connectivity\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/k8s/index.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl version\n```\n\n----------------------------------------\n\nTITLE: Configuring Storage Access Secrets in Databricks\nDESCRIPTION: Bash commands to create a secret scope and add storage account access key to Databricks secrets. This allows secure access to the Azure Storage account from Databricks notebooks.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/spark/index.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nSTORAGE_PRIMARY_KEY=[Primary key of storage account]\n\ndatabricks secrets create-scope --scope storage_scope --initial-manage-principal users\ndatabricks secrets put --scope storage_scope --key storage_account_access_key --string-value \"$STORAGE_PRIMARY_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_DRIVER_LICENSE Entity (Numeric)\nDESCRIPTION: Provides sample text containing a US Driver License number ('1234567901234') labeled as US_DRIVER_LICENSE, including context words 'my driver license is:'. Tests recognition of a purely numeric format.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_DRIVER_LICENSE\nmy driver license is: 1234567901234\n```\n\n----------------------------------------\n\nTITLE: Running Presidio API Locally with Uvicorn\nDESCRIPTION: Command to run the API locally using Uvicorn web server, exposing it on all network interfaces (0.0.0.0) and port 8080.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn main:app --host 0.0.0.0 --port 8080\n```\n\n----------------------------------------\n\nTITLE: Example Data for PHONE_NUMBER Entity (Numeric Only)\nDESCRIPTION: Provides sample text containing a US phone number ('4258829090') written as a single numeric string, labeled as PHONE_NUMBER, including context words 'my phone number is'. Tests recognition of numeric-only phone format.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nPHONE_NUMBER\nmy phone number is 4258829090\n```\n\n----------------------------------------\n\nTITLE: Example Data for SG FIN Entity (NRIC Context)\nDESCRIPTION: Provides sample text containing a Singapore NRIC/FIN number ('S0000001I') labeled as FIN, within a sentence discussing special NRIC numbers. Tests recognition using 'NRIC' as context for the FIN entity type.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\n# Verify SG NRIC/FIN context words\nFIN\nSpecial NRIC numbers e.g. S0000001I that are numerically significant have been issued to notable people Yusof bin Ishak, first President of Singapore\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_PASSPORT Entity (Substring Context)\nDESCRIPTION: Provides sample text containing a US Passport Number ('912803456') labeled as US_PASSPORT, with context words 'my passportnumber is' having no space. Tests recognition when context words are run together with the entity identifier.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_24\n\nLANGUAGE: plaintext\nCODE:\n```\n# Verify that substring is also found (e.g. forgotten spaces)\nUS_PASSPORT\nmy passportnumber is 912803456\n```\n\n----------------------------------------\n\nTITLE: Starting LiteLLM Proxy\nDESCRIPTION: Command to start the LiteLLM proxy with a configuration file\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/docker/litellm.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nlitellm --config /path/to/config.yaml\n```\n\n----------------------------------------\n\nTITLE: Example Data for ABA_ROUTING_NUMBER Entity\nDESCRIPTION: Provides sample text containing an ABA Routing Number ('101205681') labeled as ABA_ROUTING_NUMBER, including context words 'routing number is:'. Tests recognition of US bank routing numbers.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_20\n\nLANGUAGE: plaintext\nCODE:\n```\nABA_ROUTING_NUMBER\nrouting number is: 101205681\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Presidio and OpenAI\nDESCRIPTION: Required environment variables setup for Presidio analyzer/anonymizer endpoints and OpenAI API key\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/docker/litellm.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PRESIDIO_ANALYZER_API_BASE=\"http://localhost:5002\"\nexport PRESIDIO_ANONYMIZER_API_BASE=\"http://localhost:5001\"\nexport OPENAI_API_KEY=\"sk-...\"\n```\n\n----------------------------------------\n\nTITLE: Configuring App Service File System Logging using Azure CLI in Bash\nDESCRIPTION: This Bash script uses the `az webapp log config` command to enable logging for the specified App Service (`$APP_SERVICE_NAME` in `$RESOURCE_GROUP`). It configures both application logging and Docker container logging to write to the filesystem, enables detailed error messages, and sets the logging level to 'information'.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/app-service/index.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naz webapp log config --name $APP_SERVICE_NAME --resource-group $RESOURCE_GROUP \\\n--application-logging filesystem --detailed-error-messages true \\\n--docker-container-logging filesystem --level information\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_DRIVER_LICENSE Entity (Numeric, Mixed Case Context)\nDESCRIPTION: Provides sample text containing a US Driver License number ('7774567901234') labeled as US_DRIVER_LICENSE, with mixed-case context words 'my DrIvEr LiCeNsE is:'. Further tests case-insensitivity of context word matching.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\n# Verify Mixed case works\nUS_DRIVER_LICENSE\nmy DrIvEr LiCeNsE is: 7774567901234\n```\n\n----------------------------------------\n\nTITLE: Ground Truth JSON Example\nDESCRIPTION: Example of a properly formatted ground truth JSON file after processing OCR and analyzer results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/image-redactor/evaluating_dicom_redaction.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"path/to/your/file.dcm\": [\n        {\n            \"label\": \"DAVIDSON\",\n            \"left\": 25,\n            \"top\": 25,\n            \"width\": 241,\n            \"height\": 37\n        },\n        {\n            \"label\": \"DOUGLAS\",\n            \"left\": 287,\n            \"top\": 25,\n            \"width\": 230,\n            \"height\": 36\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Data for IN_PASSPORT Entity\nDESCRIPTION: Provides sample text containing an Indian Passport number ('T1234567') labeled as IN_PASSPORT, along with descriptive text and context words 'my passport number is'. Tests recognition of Indian passport format and associated context.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_33\n\nLANGUAGE: plaintext\nCODE:\n```\n#Verify IN PASSPORT context words\nIN_PASSPORT\nmy passport number is T1234567. Indian Passport number is of 8 characters long, always starting with a capital letter.\n```\n\n----------------------------------------\n\nTITLE: Example Data for IN_PAN Entity (Mixed Case Context)\nDESCRIPTION: Provides sample text containing an Indian PAN number ('DJPMS1234Z') labeled as IN_PAN, with mixed-case context words 'my PAN number is'. Tests case-insensitivity for context word ('PAN') matching.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_32\n\nLANGUAGE: plaintext\nCODE:\n```\n#Verify IN PAN mixed case\nIN_PAN\nmy PAN number is DJPMS1234Z\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_PASSPORT Entity (Context Window Test)\nDESCRIPTION: Provides sample text containing a US Passport Number ('912803456') labeled as US_PASSPORT, separated from the context words 'my passport number is' by several other words ('hi bye hello'). Tests the effectiveness of the context detection window (mentioned as potentially 5 words in comments).\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_26\n\nLANGUAGE: plaintext\nCODE:\n```\n# Verify that within the max window size (currently 5) the context is picked up\nUS_PASSPORT\nmy passport number is hi bye hello 912803456\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio and Dependencies in Python\nDESCRIPTION: This code snippet installs the Presidio Analyzer and Anonymizer packages, along with the required spaCy language model for English.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/keep_entities.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_DRIVER_LICENSE Entity (Alphanumeric)\nDESCRIPTION: Provides sample text containing a US Driver License number ('H12234567') labeled as US_DRIVER_LICENSE, including context words 'my driver license is'. Tests recognition of a common alphanumeric format.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_DRIVER_LICENSE\nmy driver license is H12234567\n```\n\n----------------------------------------\n\nTITLE: Configuring Ad-hoc Recognizers\nDESCRIPTION: YAML configuration for setting up custom Presidio recognizers\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/docker/litellm.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nlitellm_settings: \n  callbacks: [\"presidio\"]\n  presidio_ad_hoc_recognizers: \"./hooks/example_presidio_ad_hoc_recognizer.json\"\n```\n\n----------------------------------------\n\nTITLE: Installing API Dependencies with pip\nDESCRIPTION: Commands to navigate to the API directory and install the required Python dependencies from the requirements.txt file.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd src/api\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Fixing Documentation Warning for OperatorConfig Class (Python)\nDESCRIPTION: Mentions a fix (#1143) for a Sphinx documentation generation warning related to the `OperatorConfig` Python class. This class is used for configuring anonymization operators within the Presidio Anonymizer.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperatorConfig\n```\n\n----------------------------------------\n\nTITLE: Formatting Presidio CLI Output - Shell\nDESCRIPTION: This collection of shell commands illustrates invoking presidio-cli with the -f parameter to control reporting format (standard, github, parsable) when detecting PERSON entities. Supported output styles are displayed, and sample outputs are included as comments. Prerequisites are correct installation of presidio-cli and valid test file paths. Output varies according to the format option used.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npresidio -d \\\"entities:\\n  - PERSON\\\" -f standard tests/conftest.py\\n# result\\ntests/conftest.py\\n  34:58     0.85     PERSON\\n  37:33     0.85     PERSON\\n\\npresidio -d \\\"entities:\\n  - PERSON\\\" -f github tests/conftest.py\\n# result\\n::group::tests/conftest.py\\n::0.85 file=tests/conftest.py,line=34,col=58::34:58 [PERSON]\\n::0.85 file=tests/conftest.py,line=37,col=33::37:33 [PERSON]\\n::endgroup::\\n\\npresidio -d \\\"entities:\\n  - PERSON\\\" -f parsable tests/conftest.py\\n# result\\n{\\\"entity_type\\\": \\\"PERSON\\\", \\\"start\\\": 57, \\\"end\\\": 62, \\\"score\\\": 0.85, \\\"analysis_explanation\\\": null}\\n{\\\"entity_type\\\": \\\"PERSON\\\", \\\"start\\\": 32, \\\"end\\\": 37, \\\"score\\\": 0.85, \\\"analysis_explanation\\\": null}\n```\n\n----------------------------------------\n\nTITLE: Example Data for SG FIN Entity (Lowercase)\nDESCRIPTION: Provides sample text containing a Singapore FIN number ('g3300299L') labeled as FIN, using lowercase letters. Tests case-insensitivity for FIN recognition and context 'my fin is'.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_29\n\nLANGUAGE: plaintext\nCODE:\n```\n# Verify SG NRIC/FIN mixed case (e.g. lower case )\nFIN\nmy fin is g3300299L\n```\n\n----------------------------------------\n\nTITLE: Using Configured NER Model in Python\nDESCRIPTION: This snippet demonstrates how to use the configured NER model with the AnalyzerEngine in Presidio.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/analyzer/nlp_engines/transformers.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\n# Create configuration containing engine name and models\nconf_file = PATH_TO_CONF_FILE\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(conf_file=conf_file)\nnlp_engine = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine, \n    supported_languages=[\"en\"]\n)\n\nresults_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results_english)\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_ITIN Entity (Hyphenated)\nDESCRIPTION: Provides sample text containing a US Individual Taxpayer Identification Number ('911-70-1234') labeled as US_ITIN, including context words 'my itin is'. Tests recognition of the standard hyphenated ITIN format.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_ITIN\nmy itin is 911-70-1234\n```\n\n----------------------------------------\n\nTITLE: Cloning Presidio Repository\nDESCRIPTION: Clone Presidio repository using HTTPS or SSH\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/installation.md#2025-04-23_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/microsoft/presidio.git\n```\n\nLANGUAGE: sh\nCODE:\n```\ngit clone git@github.com:microsoft/presidio.git\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Configuration File\nDESCRIPTION: Command to create a .env file by copying the sample environment file. This configuration file needs to be edited according to the user's setup.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.sample .env\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Configuration for Version 2.2.358\nDESCRIPTION: YAML structure defining version 2.2.358 of the project, including updates to Analyzer, Anonymizer, Image Redactor, and General components.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n## [2.2.358] - 2025-03-18\n\n### Analyzer\n- Fixed: Updated URL regex pattern to correctly exclude trailing single (') and double (\") quotes from matched URLs.\n- Drop dependency of spacy_stanza package, and add supporting code to stanza_nlp_engine, to support recent stanza versions\n- Add parameters to allow users to define the number of processes and batch size when running BatchAnalyzerEngine.\n- Fix InPassportRecognizer regex recognizer\n  \n### Anonymizer\n- Changed: Deprecate `MD5` hash type option, defaulting into `sha256`.\n- Replace crypto package dependency from pycryptodom to cryptography \n- Remove azure-core dependency from anonymizer\n  \n### Image Redactor\n- Changed: Updated the return type annotation of `ocr_bboxes` in `verify_dicom_instance()` from `dict` to `list`.  \n\n### Presidio Structured\n\n### General\n- Updated the `Evaluating DICOM Redaction` documentation to reflect changes in verify_dicom_instance() within the DicomImagePiiVerifyEngine class.\n```\n\n----------------------------------------\n\nTITLE: Getting CLI Parameter Help for Presidio - Shell\nDESCRIPTION: This shell command invokes presidio-cli with the --help parameter to display a list of all available command-line options and usage documentation. It does not require any arguments beyond a working installation of presidio-cli, and outputs usage info directly to standard output.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-cli/README.md#2025-04-23_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npresidio --help\n```\n\n----------------------------------------\n\nTITLE: Example Data for IN_PAN Entity (Adjacent Context)\nDESCRIPTION: Provides sample text containing an Indian PAN number ('DJPMS1234Z') labeled as IN_PAN, adjacent to context words 'my pan is' within a larger sentence. Tests recognition with adjacent context and surrounding text.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_30\n\nLANGUAGE: plaintext\nCODE:\n```\n#Verify IN PAN in adjacent context words\nIN_PAN\nmy pan is DJPMS1234Z amongst so many other things\n```\n\n----------------------------------------\n\nTITLE: Updating Package Dependencies in setup.py for DICOM Support (Python)\nDESCRIPTION: Mentions updates to the `setup.py` file in version 2.2.31. The changes incorporate new dependencies (e.g., for DICOM handling) required for the added DICOM image redaction features.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nsetup.py\n```\n\n----------------------------------------\n\nTITLE: Running Pytest with Poetry\nDESCRIPTION: Command to run all tests using Pytest within the Poetry virtual environment.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pytest\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_PASSPORT Entity (Extended Sentence)\nDESCRIPTION: Provides sample text containing a US Passport Number ('912803456') labeled as US_PASSPORT, followed by many unrelated words. Tests recognition within a longer sentence structure.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_25\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_PASSPORT\nmy passport number is 912803456 and now there are a lot a lot a lot a lot a lot a lot a lot a lot a lot a lot a lot a lot a lot a lot a lot a lot a lot of other words\n```\n\n----------------------------------------\n\nTITLE: Example Data for IN_PAN Entity (Descriptive Context)\nDESCRIPTION: Provides sample text describing the Indian PAN ('permanent account number') labeled as IN_PAN, without an explicit PAN number present. Tests context word recognition ('PAN', 'permanent account number') even without the entity instance.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_31\n\nLANGUAGE: plaintext\nCODE:\n```\n#Verify IN PAN context words\nIN_PAN\nTypical tax filing identifier is known as PAN in India also known as permanent account number\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Configuration for Unreleased Version\nDESCRIPTION: YAML structure defining the unreleased version of the project, with placeholders for Analyzer, Anonymizer, Image Redactor, Presidio Structured, and General updates.\nSOURCE: https://github.com/microsoft/presidio/blob/main/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n## [unreleased]\n\n### Analyzer\n  \n### Anonymizer\n  \n### Image Redactor\n\n### Presidio Structured\n\n### General\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_PASSPORT Entity\nDESCRIPTION: Provides sample text containing a US Passport Number ('912803456') labeled as US_PASSPORT, including context words 'my passport number is'. Tests recognition of passport numbers.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_PASSPORT\nmy passport number is 912803456\n```\n\n----------------------------------------\n\nTITLE: Checking Presidio Pod Status\nDESCRIPTION: Command to verify the status of Presidio pods in the designated namespace\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/k8s/index.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pod -n presidio\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Overlapping PII Anonymization Scenarios - Markdown - Plaintext\nDESCRIPTION: These code snippets illustrate, via Markdown code blocks, the outputs of the Presidio Anonymizer when applied to sample text where PII entities overlap in different scenarios (no overlap, full overlap, containment, partial intersection). No external dependencies or programming execution context is required, as they serve as illustrative output rather than runnable code. Each snippet demonstrates how the input string would look after anonymization procedures, based on entity recognition and operator rules.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/anonymizer/index.md#2025-04-23_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nI'm George Washington Square Park.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nI'm <PERSON><LOCATION>.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nMy name is Inigo Montoya. You Killed my Father. Prepare to die. BTW my number is:\n03-232323.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nMy name is <NAME> Montoya. You Killed my Father. Prepare to die. BTW my number is:\n03-232323.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nMy name is Inigo Montoya. You Killed my Father. Prepare to die. BTW my number is: <\nPHONE_NUMBER>.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nMy name is <NAME>. You Killed my Father. Prepare to die. BTW my number is: 03-232323.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nMy name is Inigo Montoya. You Killed my Father. Prepare to die. BTW my number is: <\nPHONE_NUMBER><SSN>.\n```\n\n----------------------------------------\n\nTITLE: Constructing a Ground Truth JSON File Based on OCR/Analyzer Results\nDESCRIPTION: Presents an example of a ground truth JSON file derived from correlating formatted OCR and analyzer results. The file path acts as the key, mapping to an array where each object represents a ground truth PHI instance, manually assigned the correct text ('label') based on visual inspection and coordinate matching, along with its bounding box.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/Evaluation_Approach.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n// Ground truth json\n{\n    \"path/to/your/file.dcm\": [\n        {\n            \"label\": \"DAVIDSON\",\n            \"left\": 25,\n            \"top\": 25,\n            \"width\": 241,\n            \"height\": 37\n        },\n        {\n            \"label\": \"DOUGLAS\",\n            \"left\": 287,\n            \"top\": 25,\n            \"width\": 230,\n            \"height\": 36\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Ground Truth Format for DICOM PHI in JSON\nDESCRIPTION: Specifies the JSON structure for storing ground truth labels for DICOM images. File paths serve as top-level keys, mapping to arrays of objects. Each object represents a distinct piece of Personal Health Information (PHI), containing its textual label and bounding box coordinates (left, top, width, height).\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-image-redactor/Evaluation_Approach.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"your/dicom/dir/file_0.dcm\": [\n        {\n            \"label\": \"DAVIDSON\",\n            \"left\": 25,\n            \"top\": 25,\n            \"width\": 241,\n            \"height\": 37\n        },\n        {\n            \"label\": \"DOUGLAS\",\n            \"left\": 287,\n            \"top\": 25,\n            \"width\": 230,\n            \"height\": 36\n        },\n        {\n            \"label\": \"[M]\",\n            \"left\": 535,\n            \"top\": 25,\n            \"width\": 60,\n            \"height\": 45\n        },\n        {\n            \"label\": \"01.09.2012\",\n            \"left\": 613,\n            \"top\": 26,\n            \"width\": 226,\n            \"height\": 35\n        },\n        {\n            \"label\": \"06.16.1976\",\n            \"left\": 170,\n            \"top\": 72,\n            \"width\": 218,\n            \"height\": 35\n        }\n    ],\n    \"your/dicom/dir/file_1.dcm\": [\n        ...\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Serving Anonymizer API Demo Client as Web Application\nDESCRIPTION: This command starts a web server to run the chat application, allowing users to interact with the LLM through a web interface.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython serve.py\n```\n\n----------------------------------------\n\nTITLE: Running Build and E2E Tests Script on Windows\nDESCRIPTION: Command to execute the run script for building and running E2E tests on Windows CMD/Powershell.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/development.md#2025-04-23_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nrun.bat\n```\n\n----------------------------------------\n\nTITLE: Running E2E Tests with Pytest - Shell\nDESCRIPTION: This shell snippet runs all defined Python end-to-end tests using the pytest testing framework. It should be executed after installing dependencies and starting Presidio services. The command will discover and execute all test files matching pytest's conventions, reporting results directly to the console.\nSOURCE: https://github.com/microsoft/presidio/blob/main/e2e-tests/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Presidio Analyzer Service\nDESCRIPTION: Command to forward the Presidio Analyzer service port (8080) to the local machine for testing. This uses kubectl to identify the appropriate pod and set up port forwarding.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/k8s/charts/presidio/templates/NOTES.txt#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl port-forward -n presidio $(kubectl get pod -n presidio -l app=demo-presidio-analyzer -o jsonpath='{.items[0].metadata.name}') 8080:8080\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_ITIN Entity (Mixed Hyphenation)\nDESCRIPTION: Provides sample text containing multiple US Individual Taxpayer Identification Numbers ('911-701234', '91170-1234') with non-standard hyphenation, labeled as US_ITIN, including context words 'my taxpayer id is'. Tests recognition robustness to varied hyphenation patterns.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_ITIN\nmy taxpayer id is 911-701234 91170-1234\n```\n\n----------------------------------------\n\nTITLE: Example Data for PHONE_NUMBER Entity (Parentheses Format, No Space)\nDESCRIPTION: Provides sample text containing a US phone number ('(425) 882-9090') labeled as PHONE_NUMBER, including context words 'my phone number is:' with no space after the colon. Tests recognition robustness to minor spacing variations.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nPHONE_NUMBER\nmy phone number is:(425) 882-9090\n```\n\n----------------------------------------\n\nTITLE: Installing Presidio Analyzer with Azure AI Language Support (Shell)\nDESCRIPTION: This shell command uses pip to install the `presidio-analyzer` Python package along with the necessary optional dependencies required for Azure AI Language integration. The `[azure-ai-language]` extra ensures that libraries needed to communicate with the Azure AI Language service are included.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/text_analytics/index.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install \"presidio-analyzer[azure-ai-language]\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Presidio API as Docker Container\nDESCRIPTION: Commands to build a Docker image for the API and run it as a container with the environment variables loaded from the .env file. The container exposes port 80, which is mapped to port 8080 on the host.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t api .\ndocker run -d -p 8080:80 --env-file .env api\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_ITIN Entity (Numeric)\nDESCRIPTION: Provides sample text containing a US Individual Taxpayer Identification Number ('911701234') labeled as US_ITIN, including context words 'my itin:'. Tests recognition of the numeric ITIN format.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_ITIN\nmy itin: 911701234\n```\n\n----------------------------------------\n\nTITLE: Example Data for US_BANK_NUMBER Entity\nDESCRIPTION: Provides sample text containing a US Bank Account Number ('912803456') labeled as US_BANK_NUMBER, including context words 'my bank account number is'. Tests recognition of bank account numbers.\nSOURCE: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/tests/data/context_sentences_tests.txt#2025-04-23_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\nUS_BANK_NUMBER\nmy bank account number is 912803456\n```\n\n----------------------------------------\n\nTITLE: Combining Custom and Predefined Recognizers in Presidio\nDESCRIPTION: This snippet demonstrates how to combine custom recognizers with Presidio's predefined recognizers for more comprehensive entity recognition.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing known values.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nregistry = RecognizerRegistry()\n\n# Load existing recognizer\nregistry.load_predefined_recognizers()\n\n# Add our custom one\nregistry.add_recognizer(deny_list_recognizer)\n\n# Initialize AnalyzerEngine\nanalyzer = AnalyzerEngine(registry=registry)\n```\n\n----------------------------------------\n\nTITLE: Executing Analyzer with SpaCy NLP Engine\nDESCRIPTION: Runs the AnalyzerEngine with the configured SpaCy NLP engine and prints results.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Run it as part of Presidio's AnalyzerEngine\ncall_analyzer_and_print_results(spacy_nlp_engine)\n```\n\n----------------------------------------\n\nTITLE: Importing Stanza NLP Engine\nDESCRIPTION: Imports the StanzaNlpEngine for using Stanza as an alternative NER engine.\nSOURCE: https://github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom presidio_analyzer.nlp_engine import StanzaNlpEngine, NerModelConfiguration\n```"
  }
]
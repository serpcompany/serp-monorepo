[
  {
    "owner": "openstack",
    "repo": "openstack-helm",
    "content": "TITLE: Checking Ceph Deployment Status using kubectl and ceph commands\nDESCRIPTION: This snippet demonstrates how to check the status of a Ceph cluster by executing commands on a Ceph monitor pod. It shows cluster health, services, and data usage.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/troubleshooting/persistent-storage.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nMON_POD=$(kubectl get --no-headers pods -n=ceph -l=\"application=ceph,component=mon\" | awk '{ print $1; exit }')\nkubectl exec -n ceph ${MON_POD} -- ceph -s\n```\n\n----------------------------------------\n\nTITLE: Deploying Tenant Ceph OpenStack Configuration with Helm\nDESCRIPTION: Bash commands to deploy the Tenant Ceph OpenStack configuration using Helm. The script includes upgrading and installing the ceph-provisioners chart with custom values, followed by validation commands.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install tenant-ceph-openstack-config ./ceph-provisioners \\\n  --namespace=openstack \\\n  --values=/tmp/tenant-ceph-openstack-config.yaml \\\n  ${OSH_EXTRA_HELM_ARGS} \\\n  ${OSH_EXTRA_HELM_ARGS_CEPH_NS_ACTIVATE}\n\n#NOTE: Wait for deploy\n./tools/deployment/common/wait-for-pods.sh openstack\n\n#NOTE: Validate Deployment info\nhelm status tenant-ceph-openstack-config\n```\n\n----------------------------------------\n\nTITLE: Creating OpenStack Namespace in Kubernetes\nDESCRIPTION: This snippet creates a Kubernetes namespace called 'openstack' for deploying OpenStack workloads. It uses a YAML configuration applied with kubectl.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntee > /tmp/openstack_namespace.yaml <<EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: openstack\nEOF\nkubectl apply -f /tmp/openstack_namespace.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenStack-Helm Deployment\nDESCRIPTION: Sets up environment variables for OpenStack release version, feature sets, and overrides directory location.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENSTACK_RELEASE=2025.1\nexport FEATURES=\"${OPENSTACK_RELEASE} ubuntu_noble\"\nexport OVERRIDES_DIR=$(pwd)/overrides\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph MON Status in Kubernetes\nDESCRIPTION: This command retrieves the status of Ceph monitors in a Kubernetes-deployed cluster. It executes the 'ceph mon_status' command within a Ceph monitor pod with JSON output formatting.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_22\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph mon_status -f json-pretty\n```\n\n----------------------------------------\n\nTITLE: Installing OpenStack-Helm Plugin\nDESCRIPTION: This command installs the OpenStack-Helm plugin which provides helper commands that simplify working with OpenStack-Helm charts. The plugin is essential for many operations in the OpenStack-Helm workflow.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/before_starting.rst#2025-04-20_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm plugin install https://opendev.org/openstack/openstack-helm-plugin\n```\n\n----------------------------------------\n\nTITLE: Downloading Values Overrides for OpenStack Charts\nDESCRIPTION: Downloads values override files for each OpenStack component from the repository using helm osh plugin.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOVERRIDES_URL=https://opendev.org/openstack/openstack-helm/raw/branch/master/values_overrides\nfor chart in rabbitmq mariadb memcached openvswitch libvirt keystone heat glance cinder placement nova neutron horizon; do\n    helm osh get-values-overrides -d -u ${OVERRIDES_URL} -p ${OVERRIDES_DIR} -c ${chart} ${FEATURES}\ndone\n```\n\n----------------------------------------\n\nTITLE: Configuring Cinder with Ceph Failure Domains\nDESCRIPTION: Configuration YAML for the Cinder component to ensure it uses the correct CRUSH rule with rack-level failure domains. This sets up the backup and volume pools with the appropriate replication settings.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\npod:\n  replicas:\n    api: 2\n    volume: 1\n    scheduler: 1\n    backup: 1\nconf:\n  cinder:\n    DEFAULT:\n      backup_driver: cinder.backup.drivers.swift\n  ceph:\n    pools:\n      backup:\n        replicated: 3\n        crush_rule: rack_replicated_rule\n        chunk_size: 8\n      volume:\n        replicated: 3\n        crush_rule: rack_replicated_rule\n        chunk_size: 8\n```\n\n----------------------------------------\n\nTITLE: Monitoring Ceph Cluster Status During OSD Pod Deletion\nDESCRIPTION: This snippet shows the Ceph cluster status during the process of an OSD pod deletion. It displays degraded data redundancy and a down OSD, illustrating the immediate impact of pod deletion.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/osd-failure.rst#2025-04-20_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nroot@voyager3:/# ceph -s\n  cluster:\n    id:     fd366aef-b356-4fe7-9ca5-1c313fe2e324\n    health: HEALTH_WARN\n            1 osds down\n            Degraded data redundancy: 43/945 objects degraded (4.550%), 35 pgs degraded, 109 pgs undersized\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager4(active)\n    osd: 24 osds: 23 up, 24 in\n```\n\n----------------------------------------\n\nTITLE: Labeling Kubernetes Nodes for OpenStack\nDESCRIPTION: This snippet labels all Kubernetes nodes for OpenStack component placement. It sets labels for control plane, compute nodes, and network plugins.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl label --overwrite nodes --all openstack-control-plane=enabled\nkubectl label --overwrite nodes --all openstack-compute-node=enabled\nkubectl label --overwrite nodes --all openvswitch=enabled\nkubectl label --overwrite nodes --all linuxbridge=enabled\n```\n\n----------------------------------------\n\nTITLE: Configuring Neutron Network Service with Helm\nDESCRIPTION: Helm deployment for OpenStack Neutron networking service with configuration for L3 HA, provider interfaces, and ML2 plugin settings.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nPROVIDER_INTERFACE=<provider_interface_name>\ntee ${OVERRIDES_DIR}/neutron/values_overrides/neutron_simple.yaml << EOF\nconf:\n  neutron:\n    DEFAULT:\n      l3_ha: False\n      max_l3_agents_per_router: 1\n  auto_bridge_add:\n    br-ex: ${PROVIDER_INTERFACE}\n  plugins:\n    ml2_conf:\n      ml2_type_flat:\n        flat_networks: public\n    openvswitch_agent:\n      ovs:\n        bridge_mappings: public:br-ex\nEOF\n\nhelm upgrade --install neutron openstack-helm/neutron \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c neutron neutron_simple ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Deploying Ingress-Nginx Controller for OpenStack\nDESCRIPTION: This snippet installs the ingress-nginx controller in the 'openstack' namespace using Helm. It configures various settings for the controller, including disabling admission webhooks and setting the ingress class.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \\\n    --version=\"4.8.3\" \\\n    --namespace=openstack \\\n    --set controller.kind=Deployment \\\n    --set controller.admissionWebhooks.enabled=\"false\" \\\n    --set controller.scope.enabled=\"true\" \\\n    --set controller.service.enabled=\"false\" \\\n    --set controller.ingressClassResource.name=nginx \\\n    --set controller.ingressClassResource.controllerValue=\"k8s.io/ingress-nginx\" \\\n    --set controller.ingressClassResource.default=\"false\" \\\n    --set controller.ingressClass=nginx \\\n    --set controller.labels.app=ingress-api\n```\n\n----------------------------------------\n\nTITLE: Running Ansible Playbook for Kubernetes Deployment\nDESCRIPTION: Command to execute the Ansible playbook for deploying the Kubernetes cluster using the prepared inventory.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/kubernetes.rst#2025-04-20_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/osh\nansible-playbook -i inventory.yaml deploy-env.yaml\n```\n\n----------------------------------------\n\nTITLE: Analyzing Ceph Monitor Status After Node Failure\nDESCRIPTION: Provides detailed Ceph monitor status in JSON format, showing mnode1 as the leader and mnode3 out of the quorum.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"mnode1\",\n    \"rank\": 0,\n    \"state\": \"leader\",\n    \"election_epoch\": 96,\n    \"quorum\": [\n        0,\n        1\n    ],\n    \"features\": {\n        \"required_con\": \"153140804152475648\",\n        \"required_mon\": [\n            \"kraken\",\n            \"luminous\"\n        ],\n        \"quorum_con\": \"2305244844532236283\",\n        \"quorum_mon\": [\n            \"kraken\",\n            \"luminous\"\n        ]\n    },\n    \"outside_quorum\": [],\n    \"extra_probe_peers\": [],\n    \"sync_provider\": [],\n    \"monmap\": {\n        \"epoch\": 1,\n        \"fsid\": \"54d9af7e-da6d-4980-9075-96bb145db65c\",\n        \"modified\": \"2018-08-14 21:02:24.330403\",\n        \"created\": \"2018-08-14 21:02:24.330403\",\n        \"features\": {\n            \"persistent\": [\n                \"kraken\",\n                \"luminous\"\n            ],\n            \"optional\": []\n        },\n        \"mons\": [\n            {\n                \"rank\": 0,\n                \"name\": \"mnode1\",\n                \"addr\": \"192.168.10.246:6789/0\",\n                \"public_addr\": \"192.168.10.246:6789/0\"\n            },\n            {\n                \"rank\": 1,\n                \"name\": \"mnode2\",\n                \"addr\": \"192.168.10.247:6789/0\",\n                \"public_addr\": \"192.168.10.247:6789/0\"\n            },\n            {\n                \"rank\": 2,\n                \"name\": \"mnode3\",\n                \"addr\": \"192.168.10.248:6789/0\",\n                \"public_addr\": \"192.168.10.248:6789/0\"\n            }\n        ]\n    },\n    \"feature_map\": {\n        \"mon\": {\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        },\n        \"mds\": {\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        },\n        \"osd\": {\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        },\n        \"client\": {\n            \"group\": {\n\n\n\n```\n\n----------------------------------------\n\nTITLE: Untainting Kubernetes Control Plane Nodes with kubectl\nDESCRIPTION: This command removes the 'node-role.kubernetes.io/control-plane' taint from all nodes labeled as control plane nodes, allowing pods to be scheduled on them. By default, Kubernetes taints control plane nodes to prevent workload pods from being scheduled there.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl taint nodes -l 'node-role.kubernetes.io/control-plane' node-role.kubernetes.io/control-plane-\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status in OpenStack Helm\nDESCRIPTION: Command output showing the health status of a Ceph cluster, including monitors, OSDs, pools, and IO statistics. Demonstrates a healthy cluster with 3 MONs and 3 OSDs.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/validate-object-replication.rst#2025-04-20_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-5qn68 -- ceph -s\n  cluster:\n    id:     54d9af7e-da6d-4980-9075-96bb145db65c\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum mnode1,mnode2,mnode3\n    mgr: mnode2(active), standbys: mnode3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-6f66956547-c25cx=up:active}, 1 up:standby\n    osd: 3 osds: 3 up, 3 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   19 pools, 101 pgs\n    objects: 354 objects, 260 MB\n    usage:   77807 MB used, 70106 MB / 144 GB avail\n    pgs:     101 active+clean\n\n  io:\n    client:   48769 B/s wr, 0 op/s rd, 12 op/s wr\n```\n\n----------------------------------------\n\nTITLE: Deploying Nova Service\nDESCRIPTION: Installs OpenStack Nova compute service using Helm chart.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install nova openstack-helm/nova \\\n    --namespace=openstack \\\n```\n\n----------------------------------------\n\nTITLE: Creating Ansible Playbook for Kubernetes Deployment\nDESCRIPTION: YAML configuration for Ansible playbook defining roles to deploy the Kubernetes environment.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/kubernetes.rst#2025-04-20_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncat > ~/osh/deploy-env.yaml <<EOF\n---\n- hosts: all\n  become: true\n  gather_facts: true\n  roles:\n    - ensure-python\n    - ensure-pip\n    - clear-firewall\n    - deploy-env\nEOF\n```\n\n----------------------------------------\n\nTITLE: Examining Ceph Quorum Status JSON Output\nDESCRIPTION: The JSON output from the Ceph quorum status command, showing election epoch, quorum members by index and name, the current quorum leader, and monitor map details for the cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"election_epoch\": 92,\n    \"quorum\": [\n        0,\n        1,\n        2\n    ],\n    \"quorum_names\": [\n        \"mnode1\",\n        \"mnode2\",\n        \"mnode3\"\n    ],\n    \"quorum_leader_name\": \"mnode1\",\n    \"monmap\": {\n        \"epoch\": 1,\n        \"fsid\": \"54d9af7e-da6d-4980-9075-96bb145db65c\",\n        \"modified\": \"2018-08-14 21:02:24.330403\",\n        \"created\": \"2018-08-14 21:02:24.330403\",\n        \"features\": {\n            \"persistent\": [\n                \"kraken\",\n                \"luminous\"\n            ],\n            \"optional\": []\n        },\n        \"mons\": [\n            {\n                \"rank\": 0,\n                \"name\": \"mnode1\",\n                \"addr\": \"192.168.10.246:6789/0\",\n                \"public_addr\": \"192.168.10.246:6789/0\"\n            },\n            {\n                \"rank\": 1,\n                \"name\": \"mnode2\",\n                \"addr\": \"192.168.10.247:6789/0\",\n                \"public_addr\": \"192.168.10.247:6789/0\"\n            },\n            {\n                \"rank\": 2,\n                \"name\": \"mnode3\",\n                \"addr\": \"192.168.10.248:6789/0\",\n                \"public_addr\": \"192.168.10.248:6789/0\"\n            }\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring MetalLB IP Address Pool\nDESCRIPTION: This snippet creates a MetalLB IPAddressPool custom resource to define the IP address range for load balancing. It uses a YAML configuration applied with kubectl.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntee > /tmp/metallb_ipaddresspool.yaml <<EOF\n---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n    name: public\n    namespace: metallb-system\nspec:\n    addresses:\n    - \"172.24.128.0/24\"\nEOF\n\nkubectl apply -f /tmp/metallb_ipaddresspool.yaml\n```\n\n----------------------------------------\n\nTITLE: Verifying Ceph Component Versions\nDESCRIPTION: Series of commands to check the version of different Ceph components (monitor, OSD, MGR, MDS, RGW) running in the Kubernetes cluster. All components show version 12.2.4, confirming the initial deployment is at the expected version.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-4c8xs -- ceph -v\nceph version 12.2.4 (52085d5249a80c5f5121a76d6288429f35e4e77b) luminous (stable)\n\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-osd-default-83945928-7jz4s -- ceph -v\nceph version 12.2.4 (52085d5249a80c5f5121a76d6288429f35e4e77b) luminous (stable)\n\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mgr-86bdc7c64b-7ptr4 -- ceph -v\nceph version 12.2.4 (52085d5249a80c5f5121a76d6288429f35e4e77b) luminous (stable)\n\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mds-745576757f-4vdn4 -- ceph -v\nceph version 12.2.4 (52085d5249a80c5f5121a76d6288429f35e4e77b) luminous (stable)\n\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-rgw-74559877-h56xs -- ceph -v\nceph version 12.2.4 (52085d5249a80c5f5121a76d6288429f35e4e77b) luminous (stable)\n```\n\n----------------------------------------\n\nTITLE: Configuring OCI Image Registry Authentication in values.yaml\nDESCRIPTION: Example YAML configuration showing the structure for OCI image registry authentication settings in a chart's values.yaml file. This includes credential storage, endpoint configuration, and manifest controls.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/support-OCI-image-registry-with-authentication-turned-on.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsecrets:\n  oci_image_registry:\n    nova: nova-oci-image-registry-key\n\nendpoints:\n  ...\n  oci_image_registry:\n    name: oci-image-registry\n    namespace: oci-image-registry\n    auth:\n      enabled: false\n      nova:\n        username: nova\n        password: password\n    hosts:\n      default: localhost\n    host_fqdn_override:\n      default: null\n    port:\n      registry:\n        default: 5000\n\nmanifests:\n  secret_registry: true\n```\n\n----------------------------------------\n\nTITLE: Deploying Memcached Service\nDESCRIPTION: Installs Memcached caching service using Helm chart with specified configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install memcached openstack-helm/memcached \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c memcached ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Deploying MariaDB Service\nDESCRIPTION: Installs MariaDB database service using Helm chart with specified configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install mariadb openstack-helm/mariadb \\\n    --namespace=openstack \\\n    --set pod.replicas.server=1 \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c mariadb ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Querying Ceph Cluster Status via kubectl\nDESCRIPTION: Executes 'ceph -s' command inside a Ceph monitor pod to display cluster health, services status, data usage, and I/O statistics. The command provides a comprehensive overview of the Ceph cluster's current state including monitor quorum, OSD status, and pool information.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec -n tenant-ceph ceph-mon-2g6km -- ceph -s\n```\n\nLANGUAGE: yaml\nCODE:\n```\ncluster:\n  id:     38339a5a-d976-49dd-88a0-2ac092c271c7\n  health: HEALTH_OK\n\nservices:\n  mon: 3 daemons, quorum node3,node2,node1\n  mgr: node2(active), standbys: node1\n  osd: 3 osds: 3 up, 3 in\n  rgw: 2 daemons active\n\ndata:\n  pools:   19 pools, 101 pgs\n  objects: 233 objects, 52644 bytes\n  usage:   33404 MB used, 199 GB / 232 GB avail\n  pgs:     101 active+clean\n\nio:\n  client:   27544 B/s rd, 0 B/s wr, 26 op/s rd, 17 op/s wr\n```\n\n----------------------------------------\n\nTITLE: Nova vCPU Configuration Override Example in OpenStack Helm\nDESCRIPTION: A practical example showing how to implement node-specific vCPU pinning configurations for Nova compute in OpenStack using the override mechanism.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nova:\n    DEFAULT:\n      vcpu_pin_set: \"0-31\"\n      cpu_allocation_ratio: 3.0\n  overrides:\n    nova_compute:\n      labels:\n      - label:\n          key: compute-type\n          values:\n```\n\n----------------------------------------\n\nTITLE: Listing Ceph Pods in Kubernetes\nDESCRIPTION: Command to check all Ceph-related pods running in the Kubernetes cluster. The output shows various Ceph components including monitors, OSDs, MDS, MGR, and RGW pods in running or completed states.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl get pods -n ceph\nNAME                                       READY     STATUS      RESTARTS   AGE\nceph-bootstrap-s4jkx                       0/1       Completed   0          10m\nceph-cephfs-client-key-generator-6bmzz     0/1       Completed   0          3m\nceph-mds-745576757f-4vdn4                  1/1       Running     0          6m\nceph-mds-745576757f-bxdcs                  1/1       Running     0          6m\nceph-mds-keyring-generator-f5lxf           0/1       Completed   0          10m\nceph-mgr-86bdc7c64b-7ptr4                  1/1       Running     0          6m\nceph-mgr-86bdc7c64b-xgplj                  1/1       Running     0          6m\nceph-mgr-keyring-generator-w7nxq           0/1       Completed   0          10m\nceph-mon-4c8xs                             1/1       Running     0          10m\nceph-mon-check-d85994946-zzwb4             1/1       Running     0          10m\nceph-mon-keyring-generator-jdgfw           0/1       Completed   0          10m\nceph-mon-kht8d                             1/1       Running     0          10m\nceph-mon-mkpmm                             1/1       Running     0          10m\nceph-osd-default-83945928-7jz4s            1/1       Running     0          8m\nceph-osd-default-83945928-bh82j            1/1       Running     0          8m\nceph-osd-default-83945928-t9szk            1/1       Running     0          8m\nceph-osd-keyring-generator-6rg65           0/1       Completed   0          10m\nceph-rbd-pool-z8vlc                        0/1       Completed   0          6m\nceph-rbd-provisioner-84665cb84f-6s55r      1/1       Running     0          3m\nceph-rbd-provisioner-84665cb84f-chwhd      1/1       Running     0          3m\nceph-rgw-74559877-h56xs                    1/1       Running     0          6m\nceph-rgw-74559877-pfjr5                    1/1       Running     0          6m\nceph-rgw-keyring-generator-6rwct           0/1       Completed   0          10m\nceph-storage-keys-generator-bgj2t          0/1       Completed   0          10m\ningress-796d8cf8d6-nzrd2                   1/1       Running     0          11m\ningress-796d8cf8d6-qqvq9                   1/1       Running     0          11m\ningress-error-pages-54454dc79b-d5r5w       1/1       Running     0          11m\ningress-error-pages-54454dc79b-gfpqv       1/1       Running     0          11m\n```\n\n----------------------------------------\n\nTITLE: Defining CRUSH Map with Rack-Level Failure Domains\nDESCRIPTION: Example of CRUSH map configuration with rack-level failure domains defined. The configuration shows the structure for hosts within racks and the replicated_rack rule for placement.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nid -20 class hdd        # do not change unnecessarily\n# weight 26.160\nalg straw2\nhash 0  # rjenkins1\nitem host1 weight 13.080\nitem host2 weight 13.080\n}\n. . .\nroot default {\n      id -1          # do not change unnecessarily\n      id -4 class hdd        # do not change unnecessarily\n      # weight 78.480\n      alg straw2\n      hash 0  # rjenkins1\n      item rack1 weight 26.160\n      item rack2 weight 26.160\n      item rack3 weight 26.160\n}\n\n# rules\nrule replicated_rack {\n      id 2\n      type replicated\n      min_size 1\n      max_size 10\n      step take default\n      step chooseleaf firstn 0 type rack\n      step emit\n}\n# end crush map\n```\n\n----------------------------------------\n\nTITLE: Deploying Libvirt Service\nDESCRIPTION: Installs Libvirt virtualization service using Helm chart with Ceph enabled.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install libvirt openstack-helm/libvirt \\\n    --namespace=openstack \\\n    --set conf.ceph.enabled=true \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c libvirt ${FEATURES})\n```\n\n----------------------------------------\n\nTITLE: Deploying Horizon Dashboard with Helm\nDESCRIPTION: Installation of OpenStack Horizon web interface using Helm charts.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install horizon openstack-helm/horizon \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c horizon ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Using utils-checkPGs.py to Validate Placement Groups\nDESCRIPTION: Examples of using the utils-checkPGs.py script to check if PGs have OSDs allocated from the same failure domain. The output shows both failed and passed checks for different pool configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nroot@host5:/# /tmp/utils-checkPGs.py cmTestPool\nChecking PGs in pool cmTestPool ... Failed\nOSDs [44, 32, 53] in PG 20.a failed check in rack [u'rack2', u'rack2', u'rack2']\nOSDs [61, 5, 12] in PG 20.19 failed check in rack [u'rack1', u'rack1', u'rack1']\nOSDs [69, 9, 15] in PG 20.2a failed check in rack [u'rack1', u'rack1', u'rack1']\n. . .\n```\n\n----------------------------------------\n\nTITLE: Configuring Neutron Core and Service Plugins in neutron.conf\nDESCRIPTION: Configuration example for neutron.conf showing how to set the core_plugin to ML2 and configure service_plugins for router functionality. These settings determine which plugins handle L2 connectivity, IP address assignments, and additional services.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[DEFAULT]\n...\n# core_plugin - plugin responsible for L2 connectivity and IP address\n#               assignments.\n# ML2 (Modular Layer 2) is the core plugin provided by Neutron ref arch\n# If other SDN implements its own logic for L2, it should replace the\n# ml2 here\ncore_plugin = ml2\n\n# service_plugins - a list of extra services exposed by Neutron API.\n# Example: router, qos, trunk, metering.\n# If other SDN implement L3 or other services, it should be configured\n# here\nservice_plugins = router\n```\n\n----------------------------------------\n\nTITLE: Describing Kubernetes StorageClass for Ceph RBD\nDESCRIPTION: This command retrieves detailed information about the 'general' StorageClass, including its provisioner and parameters for Ceph RBD integration.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/troubleshooting/persistent-storage.rst#2025-04-20_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe storageclass/general\n```\n\n----------------------------------------\n\nTITLE: Configuring Nova Compute Node Overrides with Labels and Host-Specific Settings in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up Nova compute node overrides using labels and host-specific settings. It shows the precedence of different override types and how they are applied to various host configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- label:\n    key: compute-type\n    values:\n    - \"dpdk\"\n    - \"sriov\"\n  conf:\n    nova:\n      DEFAULT:\n        vcpu_pin_set: \"0-15\"\n- label:\n    key: another-label\n    values:\n    - \"another-value\"\n  conf:\n    nova:\n      DEFAULT:\n        vcpu_pin_set: \"16-31\"\nhosts:\n- name: host1.fqdn\n  conf:\n    nova:\n      DEFAULT:\n        vcpu_pin_set: \"8-15\"\n- name: host2.fqdn\n  conf:\n    nova:\n      DEFAULT:\n        vcpu_pin_set: \"16-23\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ceph Adapter for Rook\nDESCRIPTION: This snippet installs the ceph-adapter-rook chart in the 'openstack' namespace using Helm. It prepares Kubernetes secrets for interfacing with the Ceph cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install ceph-adapter-rook openstack-helm/ceph-adapter-rook \\\n    --namespace=openstack\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Deploying Tenant RadosGW for OpenStack with Helm\nDESCRIPTION: Bash commands to deploy the Tenant Ceph RadosGW for OpenStack using Helm. The script includes upgrading and installing the ceph-rgw chart with custom values, followed by validation commands.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install tenant-radosgw-openstack ./ceph-rgw \\\n  --namespace=openstack \\\n  --values=/tmp/tenant-radosgw-openstack.yaml \\\n  ${OSH_EXTRA_HELM_ARGS} \\\n  ${OSH_EXTRA_HELM_ARGS_HEAT}\n\n#NOTE: Wait for deploy\n./tools/deployment/common/wait-for-pods.sh openstack\n\n#NOTE: Validate Deployment info\nhelm status tenant-radosgw-openstack\n```\n\n----------------------------------------\n\nTITLE: Managing Ceph Cluster Flags\nDESCRIPTION: Commands for setting and unsetting Ceph cluster flags to prevent rebalancing during the migration process.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/upgrade/multiple-osd-releases.rst#2025-04-20_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nubuntu@k8smaster:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-5qn68 -- ceph osd set noout\n\nubuntu@k8smaster:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-5qn68 -- ceph osd set nobackfill\n\nubuntu@k8smaster:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-5qn68 -- ceph osd set norecover\n\nubuntu@k8smaster:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-5qn68 -- ceph osd set pause\n```\n\n----------------------------------------\n\nTITLE: Removing Failed OSD Pod from Kubernetes\nDESCRIPTION: These commands label nodes and patch the DaemonSet to remove the failed OSD pod from the Kubernetes cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/disk-failure.rst#2025-04-20_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl label nodes --all ceph_maintenance_window=inactive\n$ kubectl label nodes voyager4 --overwrite ceph_maintenance_window=active\n$ kubectl patch -n ceph ds ceph-osd-default-64779b8c -p='{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"ceph-osd\":\"enabled\",\"ceph_maintenance_window\":\"inactive\"}}}}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Fluentbit Parsers\nDESCRIPTION: Parser configuration for Fluentbit to process Docker log formats with timestamp handling and JSON formatting.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/fluent-logging.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  parsers:\n    - docker:\n        header: parser\n        Name: docker\n        Format: json\n        Time_Key: time\n        Time_Format: \"%Y-%m-%dT%H:%M:%S.%L\"\n        Time_Keep: On\n```\n\n----------------------------------------\n\nTITLE: Installing MetalLB using Helm\nDESCRIPTION: This snippet installs MetalLB in the 'metallb-system' namespace using Helm. It adds the MetalLB Helm repository and then installs the chart.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add metallb https://metallb.github.io/metallb\nhelm install metallb metallb/metallb -n metallb-system\n```\n\n----------------------------------------\n\nTITLE: Configuring Tenant Ceph RadosGW for OpenStack Integration\nDESCRIPTION: YAML configuration for deploying Rados Gateway with OpenStack integration. This defines endpoints, networking, and keyring secrets to connect Tenant Ceph object storage with Keystone authentication.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\ntee /tmp/tenant-radosgw-openstack.yaml <<EOF\nendpoints:\n  identity:\n    namespace: openstack\n  object_store:\n    namespace: openstack\n  ceph_mon:\n    namespace: tenant-ceph\n    port:\n      mon:\n        default: 6790\nnetwork:\n  public: ${CEPH_PUBLIC_NETWORK}\n  cluster: ${CEPH_CLUSTER_NETWORK}\ndeployment:\n  storage_secrets: false\n  ceph: true\n  rbd_provisioner: false\n  csi_rbd_provisioner: false\n  cephfs_provisioner: false\n  client_secrets: false\nbootstrap:\n  enabled: false\nconf:\n  rgw_ks:\n    enabled: true\nsecrets:\n  keyrings:\n    admin: pvc-tenant-ceph-client-key\n    rgw: os-ceph-bootstrap-rgw-keyring\n  identity:\n    admin: ceph-keystone-admin\n    swift: ceph-keystone-user\n    user_rgw: ceph-keystone-user-rgw\nceph_client:\n  configmap: tenant-ceph-etc\nEOF\n```\n\n----------------------------------------\n\nTITLE: Examining Tenant Ceph RGW Configuration\nDESCRIPTION: YAML output showing the Ceph RadosGW configuration in OpenStack namespace. This shows custom monitor port (6790) and network settings for the tenant Ceph cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_27\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\ndata:\n  ceph.conf: |\n    [global]\n    cephx = true\n    cephx_cluster_require_signatures = true\n    cephx_require_signatures = false\n    cephx_service_require_signatures = false\n    mon_addr = :6790\n    mon_host = ceph-mon.tenant-ceph.svc.cluster.local:6790\n    [osd]\n    cluster_network = 10.0.0.0/24\n    ms_bind_port_max = 7100\n    ms_bind_port_min = 6800\n    osd_max_object_name_len = 256\n    osd_mkfs_options_xfs = -f -i size=2048\n    osd_mkfs_type = xfs\n    public_network = 10.0.0.0/24\nkind: ConfigMap\nmetadata:\n  creationTimestamp: 2018-08-27T07:47:59Z\n  name: ceph-rgw-etc\n  namespace: openstack\n  resourceVersion: \"30058\"\n  selfLink: /api/v1/namespaces/openstack/configmaps/ceph-rgw-etc\n  uid: 848df05c-a9cd-11e8-bb1d-fa163ec12213\n```\n\n----------------------------------------\n\nTITLE: Displaying CRUSH Hierarchy in Ceph\nDESCRIPTION: This snippet shows how to use the 'ceph osd crush tree' command to display the CRUSH hierarchy of a Ceph cluster, including racks, hosts, and OSDs.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nroot@host5:/# ceph osd crush tree\nID  CLASS WEIGHT   TYPE NAME\n -1       78.47974 root default\n-15       26.15991     rack rack1\n -2       13.07996         host host1\n  0   hdd  1.09000             osd.0\n  1   hdd  1.09000             osd.1\n  2   hdd  1.09000             osd.2\n  3   hdd  1.09000             osd.3\n  4   hdd  1.09000             osd.4\n  5   hdd  1.09000             osd.5\n  6   hdd  1.09000             osd.6\n  7   hdd  1.09000             osd.7\n  8   hdd  1.09000             osd.8\n  9   hdd  1.09000             osd.9\n 10   hdd  1.09000             osd.10\n 11   hdd  1.09000             osd.11\n -5       13.07996         host host2\n 12   hdd  1.09000             osd.12\n 13   hdd  1.09000             osd.13\n 14   hdd  1.09000             osd.14\n 15   hdd  1.09000             osd.15\n 16   hdd  1.09000             osd.16\n 17   hdd  1.09000             osd.17\n 18   hdd  1.09000             osd.18\n 19   hdd  1.09000             osd.19\n 20   hdd  1.09000             osd.20\n 21   hdd  1.09000             osd.21\n 22   hdd  1.09000             osd.22\n 23   hdd  1.09000             osd.23\n-16       26.15991     rack rack2\n-13       13.07996         host host3\n 53   hdd  1.09000             osd.53\n 54   hdd  1.09000             osd.54\n 58   hdd  1.09000             osd.58\n 59   hdd  1.09000             osd.59\n 64   hdd  1.09000             osd.64\n 65   hdd  1.09000             osd.65\n 66   hdd  1.09000             osd.66\n 67   hdd  1.09000             osd.67\n 68   hdd  1.09000             osd.68\n 69   hdd  1.09000             osd.69\n 70   hdd  1.09000             osd.70\n 71   hdd  1.09000             osd.71\n -9       13.07996         host host4\n 36   hdd  1.09000             osd.36\n 37   hdd  1.09000             osd.37\n 38   hdd  1.09000             osd.38\n 39   hdd  1.09000             osd.39\n 40   hdd  1.09000             osd.40\n 41   hdd  1.09000             osd.41\n 42   hdd  1.09000             osd.42\n 43   hdd  1.09000             osd.43\n 44   hdd  1.09000             osd.44\n 45   hdd  1.09000             osd.45\n 46   hdd  1.09000             osd.46\n 47   hdd  1.09000             osd.47\n-17       26.15991     rack rack3\n-11       13.07996         host host5\n 48   hdd  1.09000             osd.48\n 49   hdd  1.09000             osd.49\n 50   hdd  1.09000             osd.50\n 51   hdd  1.09000             osd.51\n 52   hdd  1.09000             osd.52\n 55   hdd  1.09000             osd.55\n 56   hdd  1.09000             osd.56\n 57   hdd  1.09000             osd.57\n 60   hdd  1.09000             osd.60\n 61   hdd  1.09000             osd.61\n 62   hdd  1.09000             osd.62\n 63   hdd  1.09000             osd.63\n -7       13.07996         host host6\n 24   hdd  1.09000             osd.24\n 25   hdd  1.09000             osd.25\n 26   hdd  1.09000             osd.26\n 27   hdd  1.09000             osd.27\n 28   hdd  1.09000             osd.28\n 29   hdd  1.09000             osd.29\n 30   hdd  1.09000             osd.30\n 31   hdd  1.09000             osd.31\n 32   hdd  1.09000             osd.32\n 33   hdd  1.09000             osd.33\n 34   hdd  1.09000             osd.34\n 35   hdd  1.09000             osd.35\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Status After Complete Recovery\nDESCRIPTION: Shows the Ceph cluster status after the failed node has been recovered, with all monitors in quorum and all 24 OSDs up and running.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_8\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n    health: HEALTH_WARN\n            too few PGs per OSD (22 < min 30)\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager1(active), standbys: voyager2\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:standby\n    osd: 24 osds: 24 up, 24 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 240 objects, 3359 bytes\n    usage:   2699 MB used, 44675 GB / 44678 GB avail\n    pgs:     182 active+clean\n```\n\n----------------------------------------\n\nTITLE: Sample Ansible Inventory for Multi-Node Kubernetes Deployment\nDESCRIPTION: Example Ansible inventory YAML file that defines a multi-node Kubernetes cluster deployment. It sets up SSH connection parameters and organizes hosts into inventory groups including primary (deployer node), k8s_cluster (all K8s nodes), k8s_control_plane (master node), and k8s_nodes (worker nodes).\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/roles/deploy-env/README.md#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nall:\n  vars:\n    ansible_port: 22\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: /home/ubuntu/.ssh/id_rsa\n    ansible_ssh_extra_args: -o StrictHostKeyChecking=no\n  hosts:\n    primary:\n      ansible_host: 10.10.10.10\n    node-1:\n      ansible_host: 10.10.10.11\n    node-2:\n      ansible_host: 10.10.10.12\n    node-3:\n      ansible_host: 10.10.10.13\n  children:\n    primary:\n      hosts:\n        primary:\n    k8s_cluster:\n      hosts:\n        node-1:\n        node-2:\n        node-3:\n    k8s_control_plane:\n      hosts:\n        node-1:\n    k8s_nodes:\n      hosts:\n        node-2:\n        node-3:\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Chart with Helm\nDESCRIPTION: Bash command to install the Prometheus chart in the OpenStack namespace. This creates a Prometheus deployment configured to discover services with appropriate annotations for scraping metrics.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/prometheus.rst#2025-04-20_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --namespace=openstack local/prometheus --name=prometheus\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status in Kubernetes\nDESCRIPTION: This snippet shows the output of the 'ceph -s' command executed within a Ceph monitor pod. It displays the cluster ID, health status, running services, and data usage statistics.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl exec -n ceph ceph-mon-b2d9w -- ceph -s\n  cluster:\n    id:     3e53e3b7-e5d9-4bab-9701-134687f4954e\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum node3,node2,node1\n    mgr: node3(active), standbys: node2\n    osd: 3 osds: 3 up, 3 in\n\n  data:\n    pools:   18 pools, 93 pgs\n    objects: 127 objects, 218 MB\n    usage:   46820 MB used, 186 GB / 232 GB avail\n    pgs:     93 active+clean\n```\n\n----------------------------------------\n\nTITLE: Verifying Placement Groups in Ceph Pool\nDESCRIPTION: This snippet demonstrates how to use a custom script to check if there are any placement groups (PGs) with OSDs allocated from the same rack in a Ceph pool.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# /tmp/utils-checkPGs.py rbd\nChecking PGs in pool rbd ... Passed\n```\n\n----------------------------------------\n\nTITLE: Verifying Ceph Cluster Status\nDESCRIPTION: Command to check the status of the Ceph cluster using the kubectl exec command. The output shows a healthy Ceph cluster with 3 monitors, 3 OSDs, and 2 RGW daemons all in an active state.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n+ kubectl exec -n ceph ceph-mon-4c8xs -- ceph -s\n  cluster:\n    id:     39061799-d25e-4f3b-8c1a-a350e4c6d06c\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum mnode1,mnode2,mnode3\n    mgr: mnode2(active), standbys: mnode3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-745576757f-4vdn4=up:active}, 1 up:standby\n    osd: 3 osds: 3 up, 3 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 93 pgs\n    objects: 208 objects, 3359 bytes\n    usage:   72175 MB used, 75739 MB / 144 GB avail\n    pgs:     93 active+clean\n```\n\n----------------------------------------\n\nTITLE: Verifying Ceph Cluster Recovery After OSD Failure\nDESCRIPTION: This snippet demonstrates checking the Ceph cluster status after recovery from OSD failures. It shows all OSDs are back up and running, indicating successful automatic recovery by Kubernetes.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/osd-failure.rst#2025-04-20_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     fd366aef-b356-4fe7-9ca5-1c313fe2e324\n    health: HEALTH_WARN\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager4(active)\n    osd: 24 osds: 24 up, 24 in\n```\n\n----------------------------------------\n\nTITLE: Configuring Neutron Agents to Use LinuxBridge\nDESCRIPTION: Sample YAML configuration overrides for adjusting L3/DHCP/metadata agents to use LinuxBridge instead of OVS. This changes the interface driver to linuxbridge and sets LinuxBridge as a mechanism driver in ML2.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/support-linux-bridge-on-neutron.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  neutron:\n    default:\n      agent:\n        interface_driver: linuxbridge\n  ml2_conf:\n    ml2_type_flat:\n      neutron:\n        ml2:\n          mechanism_drivers: linuxbridge, l2population\n  dhcp_agent:\n    default:\n      neutron:\n        base:\n          agent:\n            interface_driver: linuxbridge\n  l3_agent:\n    default:\n      neutron:\n        base:\n          agent:\n            interface_driver: linuxbridge\n```\n\n----------------------------------------\n\nTITLE: Example Node Exporter Monitoring Configuration\nDESCRIPTION: Sample YAML configuration for Prometheus monitoring of a node exporter. This snippet shows the structure needed in a chart's values.yaml to enable Prometheus scraping.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/prometheus.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmonitoring:\n  prometheus:\n    enabled: false\n    node_exporter:\n      scrape: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Tenant Ceph Storage Classes with YAML\nDESCRIPTION: YAML configuration for setting up Tenant Ceph storage classes with custom namespace and secret configurations. This defines both RBD and CephFS provision settings to be applied to the OpenStack environment.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nadmin_secret_name: pvc-tenant-ceph-conf-combined-storageclass\nadmin_secret_namespace: tenant-ceph\nuser_secret_name: pvc-tenant-ceph-client-key\ncephfs:\n  provision_storage_class: false\n  name: cephfs\n  admin_secret_name: pvc-tenant-ceph-conf-combined-storageclass\n  admin_secret_namespace: tenant-ceph\n  user_secret_name: pvc-tenant-ceph-cephfs-client-key\n```\n\n----------------------------------------\n\nTITLE: Ceph Client Update Status - Console Output\nDESCRIPTION: Console output showing the rolling update process of Ceph client components including MDS, MGR, and RGW pods.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_15\n\nLANGUAGE: console\nCODE:\n```\nceph-mds-5fdcb5c64c-t9nmb                  0/1       Init:0/2      0          11s\nceph-mds-745576757f-4vdn4                  1/1       Running       0          2h\nceph-mds-745576757f-bxdcs                  1/1       Running       0          2h\nceph-mgr-86bdc7c64b-7ptr4                  1/1       Terminating   0          2h\nceph-mgr-86bdc7c64b-xgplj                  0/1       Terminating   0          2h\nceph-rgw-57c68b7cd5-vxcc5                  0/1       Init:1/3      0          11s\nceph-rgw-74559877-h56xs                    1/1       Running       0          2h\nceph-rgw-74559877-pfjr5                    1/1       Running       0          2h\n```\n\nLANGUAGE: console\nCODE:\n```\nceph-mds-5fdcb5c64c-c52xq                  1/1       Running     0          2m\nceph-mds-5fdcb5c64c-t9nmb                  1/1       Running     0          2m\nceph-mgr-654f97cbfd-9kcvb                  1/1       Running     0          1m\nceph-mgr-654f97cbfd-gzb7k                  1/1       Running     0          1m\nceph-rgw-57c68b7cd5-vxcc5                  1/1       Running     0          2m\nceph-rgw-57c68b7cd5-zmdqb                  1/1       Running     0          2m\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status After Monitor Failure\nDESCRIPTION: Commands to check the status of the Ceph cluster after a monitor failure. Shows the cluster health, quorum status, and service information.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/monitor-failure.rst#2025-04-20_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     fd366aef-b356-4fe7-9ca5-1c313fe2e324\n    health: HEALTH_WARN\n            mon voyager1 is low on available space\n            1/3 mons down, quorum voyager1,voyager3\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager3, out of quorum: voyager2\n    mgr: voyager4(active)\n    osd: 24 osds: 24 up, 24 in\n```\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     fd366aef-b356-4fe7-9ca5-1c313fe2e324\n    health: HEALTH_WARN\n            mon voyager1 is low on available space\n            1/3 mons down, quorum voyager1,voyager2\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager4(active)\n    osd: 24 osds: 24 up, 24 in\n```\n\n----------------------------------------\n\nTITLE: Displaying OSD Tree with Status in Ceph\nDESCRIPTION: This snippet demonstrates how to use the 'ceph osd tree' command to display the OSD tree, including the status, weight, and affinity of each OSD in the Ceph cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nroot@host5:/# ceph osd tree\nID  CLASS WEIGHT   TYPE NAME                 STATUS REWEIGHT PRI-AFF\n -1       78.47974 root default\n-15       26.15991     rack rack1\n -2       13.07996         host host1\n  0   hdd  1.09000             osd.0             up  1.00000 1.00000\n  1   hdd  1.09000             osd.1             up  1.00000 1.00000\n  2   hdd  1.09000             osd.2             up  1.00000 1.00000\n  3   hdd  1.09000             osd.3             up  1.00000 1.00000\n  4   hdd  1.09000             osd.4             up  1.00000 1.00000\n  5   hdd  1.09000             osd.5             up  1.00000 1.00000\n  6   hdd  1.09000             osd.6             up  1.00000 1.00000\n  7   hdd  1.09000             osd.7             up  1.00000 1.00000\n  8   hdd  1.09000             osd.8             up  1.00000 1.00000\n  9   hdd  1.09000             osd.9             up  1.00000 1.00000\n 10   hdd  1.09000             osd.10            up  1.00000 1.00000\n 11   hdd  1.09000             osd.11            up  1.00000 1.00000\n -5       13.07996         host host2\n 12   hdd  1.09000             osd.12            up  1.00000 1.00000\n 13   hdd  1.09000             osd.13            up  1.00000 1.00000\n 14   hdd  1.09000             osd.14            up  1.00000 1.00000\n 15   hdd  1.09000             osd.15            up  1.00000 1.00000\n 16   hdd  1.09000             osd.16            up  1.00000 1.00000\n 17   hdd  1.09000             osd.17            up  1.00000 1.00000\n 18   hdd  1.09000             osd.18            up  1.00000 1.00000\n 19   hdd  1.09000             osd.19            up  1.00000 1.00000\n 20   hdd  1.09000             osd.20            up  1.00000 1.00000\n 21   hdd  1.09000             osd.21            up  1.00000 1.00000\n 22   hdd  1.09000             osd.22            up  1.00000 1.00000\n 23   hdd  1.09000             osd.23            up  1.00000 1.00000\n```\n\n----------------------------------------\n\nTITLE: Creating OpenStack Endpoint Service\nDESCRIPTION: This snippet creates a Kubernetes LoadBalancer service as the public endpoint for OpenStack services. It uses a YAML configuration applied with kubectl, specifying the IP address and ports.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntee > /tmp/openstack_endpoint_service.yaml <<EOF\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: public-openstack\n  namespace: openstack\n  annotations:\n    metallb.universe.tf/loadBalancerIPs: \"172.24.128.100\"\nspec:\n  externalTrafficPolicy: Cluster\n  type: LoadBalancer\n  selector:\n    app: ingress-api\n  ports:\n    - name: http\n      port: 80\n    - name: https\n      port: 443\nEOF\n\nkubectl apply -f /tmp/openstack_endpoint_service.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying Prometheus Annotations to a Kubernetes Service\nDESCRIPTION: A Helm template example showing how to conditionally apply Prometheus annotations to a Kubernetes service. The annotations are only applied if Prometheus monitoring is enabled in the chart values.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/prometheus.rst#2025-04-20_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $prometheus_annotations := $envAll.Values.monitoring.prometheus.node_exporter }}\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: {{ tuple \"node_metrics\" \"internal\" . | include \"helm-toolkit.endpoints.hostname_short_endpoint_lookup\" }}\nlabels:\n{{ tuple $envAll \"node_exporter\" \"metrics\" | include \"helm-toolkit.snippets.kubernetes_metadata_labels\" | indent 4 }}\nannotations:\n{{- if .Values.monitoring.prometheus.enabled }}\n{{ tuple $prometheus_annotations | include \"helm-toolkit.snippets.prometheus_service_annotations\" | indent 4 }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Status After Rebalancing\nDESCRIPTION: Shows the Ceph cluster status after it has rebalanced itself with the failed node still down, having reduced the OSDs in use from 24 to 18.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n    health: HEALTH_WARN\n            mon voyager1 is low on available space\n            1/3 mons down, quorum voyager1,voyager2\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2, out of quorum: voyager3\n    mgr: voyager1(active), standbys: voyager2\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:standby\n    osd: 24 osds: 18 up, 18 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 240 objects, 3359 bytes\n    usage:   2025 MB used, 33506 GB / 33508 GB avail\n    pgs:     182 active+clean\n```\n\n----------------------------------------\n\nTITLE: Example Nova Configuration for host4.fqdn in YAML\nDESCRIPTION: This YAML snippet shows the resulting Nova configuration for host4.fqdn with specific labels. It illustrates how label-based overrides are applied when only some labels are present.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nova:\n    DEFAULT:\n      vcpu_pin_set: \"0-15\"\n      cpu_allocation_ratio: 3.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Nagios Service Checks in OpenStack-Helm\nDESCRIPTION: This configuration defines the service checks that Nagios will execute. It includes service definitions for monitoring CEPH health, host nodes status, and Prometheus replica count. Each service check specifies parameters like target host groups, check intervals, and notification settings.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/nagios.rst#2025-04-20_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nagios:\n    services:\n      - notifying_service:\n          name: notifying_service\n          use: generic-service\n          flap_detection_enabled: 0\n          process_perf_data: 0\n          contact_groups: snmp_and_http_notifying_contact_group\n          check_interval: 60\n          notification_interval: 120\n          retry_interval: 30\n          register: 0\n      - check_ceph_health:\n          use: notifying_service\n          hostgroup_name: base-os\n          service_description: \"CEPH_health\"\n          check_command: check_ceph_health\n          check_interval: 300\n      - check_hosts_health:\n          use: generic-service\n          hostgroup_name: prometheus-hosts\n          service_description: \"Nodes_health\"\n          check_command: check_prom_alert!K8SNodesNotReady!CRITICAL- One or more nodes are not ready.\n          check_interval: 60\n      - check_prometheus_replicas:\n          use: notifying_service\n          hostgroup_name: prometheus-hosts\n          service_description: \"Prometheus_replica-count\"\n          check_command: check_prom_alert_with_labels!replicas_unavailable_statefulset!statefulset=\"prometheus\"!statefulset {statefulset} has lesser than configured replicas\n          check_interval: 60\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status\nDESCRIPTION: Console command and output showing the health status of the Ceph cluster including monitor, manager, and OSD information.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/upgrade/multiple-osd-releases.rst#2025-04-20_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nubuntu@k8smaster:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-5qn68 -- ceph -s\n    cluster:\n      id:     61a4e07f-8b4a-4c47-8fc7-a0e7345ac0b0\n      health: HEALTH_OK\n\n    services:\n       mon: 3 daemons, quorum k8smaster,k8sslave1,k8sslave2\n       mgr: k8sslave2(active), standbys: k8sslave1\n       mds: cephfs-1/1/1 up  {0=mds-ceph-mds-5bf9fdfc6b-8nq4p=up:active}, 1 up:standby\n       osd: 6 osds: 6 up, 6 in\n    data:\n       pools:   18 pools, 186 pgs\n       objects: 377  objects, 1.2 GiB\n       usage:   4.2 GiB used, 116 GiB / 120 GiB avail\n       pgs:     186 active+clean\n```\n\n----------------------------------------\n\nTITLE: Glance Service Configuration\nDESCRIPTION: YAML configuration for Glance deployment showing endpoint and Ceph client settings\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_31\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  object_store:\n    namespace: tenant-ceph\n  ceph_object_store:\n    namespace: tenant-ceph\nceph_client:\n  configmap: tenant-ceph-etc\n  user_secret_name: tenant-pvc-ceph-client-key\n```\n\n----------------------------------------\n\nTITLE: Setting up Grafana dashboards in values.YAML\nDESCRIPTION: Structure for defining dashboards in YAML that will be transformed to JSON and added to Grafana's configuration. Custom dashboards can be added under this configuration section.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/grafana.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  dashboards:\n```\n\n----------------------------------------\n\nTITLE: Checking Node Status After Complete Host Failure\nDESCRIPTION: Shows the kubectl get nodes output after a host has completely failed (not just rebooted), with its status showing as NotReady.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get nodes\nNAME       STATUS     ROLES     AGE       VERSION\nvoyager1   Ready      master    14d       v1.10.5\nvoyager2   Ready      <none>    14d       v1.10.5\nvoyager3   NotReady   <none>    14d       v1.10.5\nvoyager4   Ready      <none>    14d       v1.10.5\n```\n\n----------------------------------------\n\nTITLE: Monitoring Ceph Monitor Pod Status\nDESCRIPTION: Commands to check the status of Ceph monitor pods using kubectl. Shows the transition of pod states during failure and recovery.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/monitor-failure.rst#2025-04-20_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n ceph -o wide | grep ceph-mon\nNAME                                       READY     STATUS    RESTARTS   AGE\nceph-mon-8tml7                             0/1       Error     4          10d\nceph-mon-kstf8                             0/1       Error     4          10d\nceph-mon-z4sl9                             0/1       Error     7          10d\n```\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n ceph -o wide | grep ceph-mon\nNAME                                       READY     STATUS               RESTARTS   AGE\nceph-mon-8tml7                             0/1       CrashLoopBackOff     4          10d\nceph-mon-kstf8                             0/1       Error                4          10d\nceph-mon-z4sl9                             0/1       CrashLoopBackOff     7          10d\n```\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n ceph -o wide | grep ceph-mon\nNAME                                       READY     STATUS    RESTARTS   AGE\nceph-mon-8tml7                             1/1       Running   5          10d\nceph-mon-kstf8                             1/1       Running   5          10d\nceph-mon-z4sl9                             1/1       Running   8          10d\n```\n\n----------------------------------------\n\nTITLE: Restoring a Ceph-based PVC in Shell\nDESCRIPTION: This snippet shows the process for restoring a previously backed up PVC. It requires stopping the workload that consumes the device, removing the raw RBD device, uncompressing the backup file, and importing it back to create a new Ceph RBD device.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/troubleshooting/ceph.rst#2025-04-20_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd /backup\nunxz -k ${RBD_NAME}.img.xz\nrbd import /backup/${RBD_NAME}.img rbd/${RBD_NAME} -m ${CEPH_MON_NAME}\n```\n\n----------------------------------------\n\nTITLE: Replacing Ceph Key Secrets in Kubernetes\nDESCRIPTION: This snippet demonstrates how to extract Ceph authentication keys from a monitor pod and replace the corresponding Kubernetes secrets.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/namespace-deletion.rst#2025-04-20_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ntee /tmp/ceph/ceph-templates/mon <<EOF\n[mon.]\n  key = $(kubectl --namespace ${CEPH_NAMESPACE} exec ${MON_POD} -- bash -c \"ceph-authtool -l \\\"/var/lib/ceph/mon/ceph-\\$(hostname)/keyring\\\"\" | awk '/key =/ {print $NF}')\n  caps mon = \"allow *\"\nEOF\n\nfor KEY in mds osd rgw; do\n  tee /tmp/ceph/ceph-templates/${KEY} <<EOF\n    [client.bootstrap-${KEY}]\n      key = $(kubectl --namespace ${CEPH_NAMESPACE} exec ${MON_POD} -- ceph auth get-key client.bootstrap-${KEY})\n      caps mon = \"allow profile bootstrap-${KEY}\"\n  EOF\ndone\n\ntee /tmp/ceph/ceph-templates/admin <<EOF\n[client.admin]\n  key = $(kubectl --namespace ${CEPH_NAMESPACE} exec ${MON_POD} -- ceph auth get-key client.admin)\n  auid = 0\n  caps mds = \"allow\"\n  caps mon = \"allow *\"\n  caps osd = \"allow *\"\n  caps mgr = \"allow *\"\nEOF\n```\n\nLANGUAGE: console\nCODE:\n```\ntee /tmp/ceph/ceph-key-relationships <<EOF\nmon ceph-mon-keyring ceph.mon.keyring mon.\nmds ceph-bootstrap-mds-keyring ceph.keyring client.bootstrap-mds\nosd ceph-bootstrap-osd-keyring ceph.keyring client.bootstrap-osd\nrgw ceph-bootstrap-rgw-keyring ceph.keyring client.bootstrap-rgw\nadmin ceph-client-admin-keyring ceph.client.admin.keyring client.admin\nEOF\n```\n\nLANGUAGE: console\nCODE:\n```\nwhile read CEPH_KEY_RELATIONS; do\n  KEY_RELATIONS=($(echo ${CEPH_KEY_RELATIONS}))\n  COMPONENT=${KEY_RELATIONS[0]}\n  KUBE_SECRET_NAME=${KEY_RELATIONS[1]}\n  KUBE_SECRET_DATA_KEY=${KEY_RELATIONS[2]}\n  KEYRING_NAME=${KEY_RELATIONS[3]}\n  DATA_PATCH=$(cat /tmp/ceph/ceph-templates/${COMPONENT} | envsubst | base64 -w0)\n  kubectl --namespace ${CEPH_NAMESPACE} patch secret ${KUBE_SECRET_NAME} -p \"{\\\"data\\\":{\\\"${KUBE_SECRET_DATA_KEY}\\\": \\\"${DATA_PATCH}\\\"}}\"\ndone < /tmp/ceph/ceph-key-relationships\n```\n\n----------------------------------------\n\nTITLE: Implementing Hash Annotations for ConfigMap-Triggered Rolling Updates in Helm Templates\nDESCRIPTION: This snippet shows how annotations are used to trigger rolling updates when ConfigMap content changes. The helm-toolkit.utils.hash function generates a hash of the ConfigMap contents, which changes whenever the referenced files are modified.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/upgrades.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nannotations:\n  configmap-bin-hash: {{ tuple \"configmap-bin.yaml\" . | include \"helm-toolkit.utils.hash\" }}\n  configmap-etc-hash: {{ tuple \"configmap-etc.yaml\" . | include \"helm-toolkit.utils.hash\" }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Fluentbit Service Settings\nDESCRIPTION: YAML configuration for Fluentbit service including input, filter and output settings. Defines log collection parameters, Kubernetes metadata filtering, and forwarding to Fluentd.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/fluent-logging.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  fluentbit:\n    - service:\n        header: service\n        Flush: 1\n        Daemon: Off\n        Log_Level: info\n        Parsers_File: parsers.conf\n    - containers_tail:\n        header: input\n        Name: tail\n        Tag: kube.*\n        Path: /var/log/containers/*.log\n        Parser: docker\n        DB: /var/log/flb_kube.db\n        Mem_Buf_Limit: 5MB\n    - kube_filter:\n        header: filter\n        Name: kubernetes\n        Match: kube.*\n        Merge_JSON_Log: On\n    - fluentd_output:\n        header: output\n        Name: forward\n        Match: \"*\"\n        Host: ${FLUENTD_HOST}\n        Port: ${FLUENTD_PORT}\n```\n\n----------------------------------------\n\nTITLE: Listing Ceph Pods in Kubernetes\nDESCRIPTION: This command lists all Ceph-related pods in the 'ceph' namespace, showing their status, IP addresses, and the nodes they are running on. It's useful for monitoring the health of the Ceph cluster components.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_18\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl get pods -n ceph --show-all=false -o wide\nFlag --show-all has been deprecated, will be removed in an upcoming release\nNAME                                       READY     STATUS     RESTARTS   AGE       IP               NODE\nceph-mds-6f66956547-57tf9                  1/1       Running    0          10m       192.168.0.207    mnode1\nceph-mds-6f66956547-5x4ng                  1/1       Running    0          1h        192.168.4.14     mnode2\nceph-mds-6f66956547-c25cx                  1/1       Unknown    0          1h        192.168.3.14     mnode3\nceph-mgr-5746dd89db-9dbmv                  1/1       Unknown    0          1h        192.168.10.248   mnode3\nceph-mgr-5746dd89db-d5fcw                  1/1       Running    0          10m       192.168.10.246   mnode1\nceph-mgr-5746dd89db-qq4nl                  1/1       Running    0          1h        192.168.10.247   mnode2\nceph-mon-5krkd                             1/1       Running    0          4m        192.168.10.249   mnode4\nceph-mon-5qn68                             1/1       NodeLost   0          1h        192.168.10.248   mnode3\nceph-mon-check-d85994946-4g5xc             1/1       Running    0          1h        192.168.4.8      mnode2\nceph-mon-mwkj9                             1/1       Running    0          1h        192.168.10.247   mnode2\nceph-mon-ql9zp                             1/1       Running    0          1h        192.168.10.246   mnode1\nceph-osd-default-83945928-c7gdd            1/1       NodeLost   0          1h        192.168.10.248   mnode3\nceph-osd-default-83945928-kf5tj            1/1       Running    0          4m        192.168.10.249   mnode4\nceph-osd-default-83945928-s6gs6            1/1       Running    0          1h        192.168.10.246   mnode1\nceph-osd-default-83945928-vsc5b            1/1       Running    0          1h        192.168.10.247   mnode2\nceph-rbd-provisioner-5bfb577ffd-j6hlx      1/1       Running    0          1h        192.168.4.16     mnode2\nceph-rbd-provisioner-5bfb577ffd-kdmrv      1/1       Running    0          10m       192.168.0.209    mnode1\nceph-rbd-provisioner-5bfb577ffd-zdx2d      1/1       Unknown    0          1h        192.168.3.16     mnode3\nceph-rgw-6c64b444d7-4qgkw                  1/1       Running    0          10m       192.168.0.210    mnode1\nceph-rgw-6c64b444d7-7bgqs                  1/1       Unknown    0          1h        192.168.3.12     mnode3\nceph-rgw-6c64b444d7-hv6vn                  1/1       Running    0          1h        192.168.4.13     mnode2\ningress-796d8cf8d6-4txkq                   1/1       Running    0          1h        192.168.2.6      mnode5\ningress-796d8cf8d6-9t7m8                   1/1       Running    0          1h        192.168.5.4      mnode4\ningress-error-pages-54454dc79b-hhb4f       1/1       Running    0          1h        192.168.2.5      mnode5\ningress-error-pages-54454dc79b-twpgc       1/1       Running    0          1h        192.168.4.4      mnode2\n```\n\n----------------------------------------\n\nTITLE: Finding Failed OSD DaemonSet\nDESCRIPTION: These commands help identify the DaemonSet associated with the failed OSD by checking processes and Kubernetes resources.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/disk-failure.rst#2025-04-20_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n(voyager4)$ ps -ef|grep /usr/bin/ceph-osd\n(voyager1)$ kubectl get ds -n ceph\n(voyager1)$ kubectl get ds <daemonset-name> -n ceph -o yaml\n```\n\n----------------------------------------\n\nTITLE: Querying Ceph Quorum Status in JSON Format\nDESCRIPTION: This snippet shows how to execute a command in a Ceph monitor pod to get the quorum status in JSON format. It provides detailed information about the election epoch, quorum members, and monitor map.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_17\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph quorum_status -f json-pretty\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"election_epoch\": 100,\n    \"quorum\": [\n        0,\n        1,\n        3\n    ],\n    \"quorum_names\": [\n        \"mnode1\",\n        \"mnode2\",\n        \"mnode4\"\n    ],\n    \"quorum_leader_name\": \"mnode1\",\n    \"monmap\": {\n        \"epoch\": 2,\n        \"fsid\": \"54d9af7e-da6d-4980-9075-96bb145db65c\",\n        \"modified\": \"2018-08-14 22:43:31.517568\",\n        \"created\": \"2018-08-14 21:02:24.330403\",\n        \"features\": {\n            \"persistent\": [\n                \"kraken\",\n                \"luminous\"\n            ],\n            \"optional\": []\n        },\n        \"mons\": [\n            {\n                \"rank\": 0,\n                \"name\": \"mnode1\",\n                \"addr\": \"192.168.10.246:6789/0\",\n                \"public_addr\": \"192.168.10.246:6789/0\"\n            },\n            {\n                \"rank\": 1,\n                \"name\": \"mnode2\",\n                \"addr\": \"192.168.10.247:6789/0\",\n                \"public_addr\": \"192.168.10.247:6789/0\"\n            },\n            {\n                \"rank\": 2,\n                \"name\": \"mnode3\",\n                \"addr\": \"192.168.10.248:6789/0\",\n                \"public_addr\": \"192.168.10.248:6789/0\"\n            },\n            {\n                \"rank\": 3,\n                \"name\": \"mnode4\",\n                \"addr\": \"192.168.10.249:6789/0\",\n                \"public_addr\": \"192.168.10.249:6789/0\"\n            }\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Ceph Object Replication Validation Script\nDESCRIPTION: Example of executing a Python validation script to check object replication across different hosts. The script takes a pool name as an argument and shows the OSDs and hosts where objects are replicated.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/validate-object-replication.rst#2025-04-20_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ /tmp/checkObjectReplication.py rbd\nTest object got replicated on these osds: [1, 0, 2]\nTest object got replicated on these hosts: [u'mnode1', u'mnode2', u'mnode3']\nHosts hosting multiple copies of a placement groups are:[]\n```\n\n----------------------------------------\n\nTITLE: Listing Tenant Ceph Kubernetes Services\nDESCRIPTION: Console output showing the Kubernetes services created in the tenant-ceph namespace. This includes monitor, manager, and ingress services with their respective IP addresses and ports.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_29\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1: kubectl get svc -n tenant-ceph\nNAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nceph-mgr              ClusterIP   10.107.183.4     <none>        7001/TCP,9284/TCP   2h\nceph-mon              ClusterIP   None             <none>        6790/TCP            2h\nceph-mon-discovery    ClusterIP   None             <none>        6790/TCP            2h\ningress               ClusterIP   10.109.105.140   <none>        80/TCP,443/TCP      3h\ningress-error-pages   ClusterIP   None             <none>        80/TCP              3h\ningress-exporter      ClusterIP   10.102.110.153   <none>        10254/TCP           3h\n```\n\n----------------------------------------\n\nTITLE: Creating a Ceph Pool with Rack-based CRUSH Rule\nDESCRIPTION: This snippet shows how to create a Ceph pool using a CRUSH rule that is based on rack failure domain, and how to verify the pool's configuration.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# ceph osd pool create rbd 2048 2048 replicated rack_replicated_rule\npool 'rbd' created\n# ceph osd pool get rbd crush_rule\ncrush_rule: rack_replicated_rule\n# ceph osd pool get rbd size\nsize: 3\n# ceph osd pool get rbd pg_num\npg_num: 2048\n```\n\n----------------------------------------\n\nTITLE: Configuring DHCP Agent Dependencies on L2 Agent\nDESCRIPTION: YAML configuration defining dynamic dependencies for the DHCP agent. This ensures that the DHCP agent is scheduled on the same node as the corresponding L2 agent (OpenVSwitch agent in this case).\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndependencies:\n  dynamic:\n    targeted:\n      openvswitch:\n        dhcp:\n          pod:\n            # this should be set to corresponding neutron L2 agent\n            - requireSameNode: true\n              labels:\n                application: neutron\n                component: neutron-ovs-agent\n```\n\n----------------------------------------\n\nTITLE: Viewing Ceph OSD Tree in Kubernetes\nDESCRIPTION: Command to retrieve and display the Ceph OSD tree structure, showing the hierarchy of OSDs and their status. This helps identify which OSDs are up or down in the cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_26\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph osd tree\nID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF\n-1       0.19995 root default\n-7       0.04999     host mnode1\n 2   hdd 0.04999         osd.2       up  1.00000 1.00000\n-2       0.04999     host mnode2\n 0   hdd 0.04999         osd.0       up  1.00000 1.00000\n-3       0.04999     host mnode3\n 1   hdd 0.04999         osd.1     down        0 1.00000\n-9       0.04999     host mnode4\n 3   hdd 0.04999         osd.3       up  1.00000 1.00000\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Status After Complete Host Failure\nDESCRIPTION: Displays the Ceph cluster status after a host has completely failed, showing degraded state with monitor out of quorum and OSDs down.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n      id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n      health: HEALTH_WARN\n              6 osds down\n              1 host (6 osds) down\n              Degraded data redundancy: 227/720 objects degraded (31.528%), 8 pgs\n  degraded\n              too few PGs per OSD (17 < min 30)\n              mon voyager1 is low on available space\n              1/3 mons down, quorum voyager1,voyager2\n\n    services:\n      mon: 3 daemons, quorum voyager1,voyager2, out of quorum: voyager3\n      mgr: voyager1(active), standbys: voyager3\n      mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:stan\n  dby\n      osd: 24 osds: 18 up, 24 in\n      rgw: 2 daemons active\n\n    data:\n      pools:   18 pools, 182 pgs\n      objects: 240 objects, 3359 bytes\n      usage:   2695 MB used, 44675 GB / 44678 GB avail\n      pgs:     227/720 objects degraded (31.528%)\n               126 active+undersized\n               48  active+clean\n               8   active+undersized+degraded\n```\n\n----------------------------------------\n\nTITLE: Installing Kube-State-Metrics Chart with Helm\nDESCRIPTION: Bash command to install the prometheus-kube-state-metrics chart in the kube-system namespace. This provides additional Kubernetes metrics for objects and components.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/prometheus.rst#2025-04-20_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --namespace=kube-system local/prometheus-kube-state-metrics --name=prometheus-kube-state-metrics\n```\n\n----------------------------------------\n\nTITLE: Removing Out-of-Quorum Ceph Monitor\nDESCRIPTION: This command removes a Ceph monitor (mnode3) that is out of quorum from the Ceph cluster. It's part of the cluster recovery process after a node failure.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_20\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph mon remove mnode3\nremoving mon.mnode3 at 192.168.10.248:6789/0, there will be 3 monitors\n```\n\n----------------------------------------\n\nTITLE: Configuring Fluentd Settings\nDESCRIPTION: Configuration for Fluentd service including forward input and Elasticsearch output settings with buffer and retry parameters.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/fluent-logging.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  fluentd:\n    - fluentbit_forward:\n        header: source\n        type: forward\n        port: \"#{ENV['FLUENTD_PORT']}\"\n        bind: 0.0.0.0\n    - elasticsearch:\n        header: match\n        type: elasticsearch\n        expression: \"**\"\n        include_tag_key: true\n        host: \"#{ENV['ELASTICSEARCH_HOST']}\"\n        port: \"#{ENV['ELASTICSEARCH_PORT']}\"\n        logstash_format: true\n        buffer_chunk_limit: 10M\n        buffer_queue_limit: 32\n        flush_interval: \"20\"\n        max_retry_wait: 300\n        disable_retry_limit: \"\"\n```\n\n----------------------------------------\n\nTITLE: DHCP Agent Startup Script with Conditional OVS Configuration\nDESCRIPTION: Bash script template for starting the neutron-dhcp-agent process. The script includes conditional inclusion of the OpenVSwitch agent configuration file when OpenVSwitch is selected as the backend.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexec neutron-dhcp-agent \\\n      --config-file /etc/neutron/neutron.conf \\\n      --config-file /etc/neutron/dhcp_agent.ini \\\n      --config-file /etc/neutron/metadata_agent.ini \\\n      --config-file /etc/neutron/plugins/ml2/ml2_conf.ini\n{{- if ( has \"openvswitch\" .Values.network.backend ) }} \\\n      --config-file /etc/neutron/plugins/ml2/openvswitch_agent.ini\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Validating Ceph Status After OSD Addition\nDESCRIPTION: This command checks the Ceph cluster status after adding the new OSD, showing 24 OSDs total.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/disk-failure.rst#2025-04-20_snippet_8\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n    health: HEALTH_WARN\n            too few PGs per OSD (22 < min 30)\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager1(active), standbys: voyager3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:standby\n    osd: 24 osds: 24 up, 24 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 240 objects, 3359 bytes\n    usage:   2665 MB used, 44675 GB / 44678 GB avail\n    pgs:     182 active+clean\n```\n\n----------------------------------------\n\nTITLE: Listing Ceph Pods in Kubernetes with Node Allocation\nDESCRIPTION: Command to list all running Ceph pods in the Kubernetes environment, showing their status, IP addresses, and the nodes they are running on, which helps verify the deployment distribution across the cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl get pods -n ceph --show-all=false -o wide\nNAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE\nceph-mds-6f66956547-5x4ng                  1/1       Running   0          1h        192.168.4.14     mnode2\nceph-mds-6f66956547-c25cx                  1/1       Running   0          1h        192.168.3.14     mnode3\nceph-mgr-5746dd89db-9dbmv                  1/1       Running   0          1h        192.168.10.248   mnode3\nceph-mgr-5746dd89db-qq4nl                  1/1       Running   0          1h        192.168.10.247   mnode2\nceph-mon-5qn68                             1/1       Running   0          1h        192.168.10.248   mnode3\nceph-mon-check-d85994946-4g5xc             1/1       Running   0          1h        192.168.4.8      mnode2\nceph-mon-mwkj9                             1/1       Running   0          1h        192.168.10.247   mnode2\nceph-mon-ql9zp                             1/1       Running   0          1h        192.168.10.246   mnode1\nceph-osd-default-83945928-c7gdd            1/1       Running   0          1h        192.168.10.248   mnode3\nceph-osd-default-83945928-s6gs6            1/1       Running   0          1h        192.168.10.246   mnode1\nceph-osd-default-83945928-vsc5b            1/1       Running   0          1h        192.168.10.247   mnode2\nceph-rbd-provisioner-5bfb577ffd-j6hlx      1/1       Running   0          1h        192.168.4.16     mnode2\nceph-rbd-provisioner-5bfb577ffd-zdx2d      1/1       Running   0          1h        192.168.3.16     mnode3\nceph-rgw-6c64b444d7-7bgqs                  1/1       Running   0          1h        192.168.3.12     mnode3\nceph-rgw-6c64b444d7-hv6vn                  1/1       Running   0          1h        192.168.4.13     mnode2\ningress-796d8cf8d6-4txkq                   1/1       Running   0          1h        192.168.2.6      mnode5\ningress-796d8cf8d6-9t7m8                   1/1       Running   0          1h        192.168.5.4      mnode4\ningress-error-pages-54454dc79b-hhb4f       1/1       Running   0          1h        192.168.2.5      mnode5\ningress-error-pages-54454dc79b-twpgc       1/1       Running   0          1h        192.168.4.4      mnode2\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status After Monitor Removal\nDESCRIPTION: This command checks the status of the Ceph cluster after removing an out-of-quorum monitor. It shows that the cluster health has improved to HEALTH_OK with 3 active monitor daemons.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_21\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph -s\n  cluster:\n    id:     54d9af7e-da6d-4980-9075-96bb145db65c\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum mnode1,mnode2,mnode4\n    mgr: mnode2(active), standbys: mnode1\n```\n\n----------------------------------------\n\nTITLE: Extracting and Viewing CRUSH Map in Ceph\nDESCRIPTION: This snippet shows how to extract the CRUSH map from a running Ceph cluster, convert it to ASCII format, and view its contents, focusing on the host and rack structure.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# ceph osd getcrushmap -o /tmp/cm.bin\n100\n# crushtool -d /tmp/cm.bin -o /tmp/cm.rack.ascii\n# cat /tmp/cm.rack.ascii\n. . .\n# buckets\nhost host1 {\n      id -2           # do not change unnecessarily\n      id -3 class hdd         # do not change unnecessarily\n      # weight 13.080\n      alg straw2\n      hash 0  # rjenkins1\n      item osd.0 weight 1.090\n      item osd.1 weight 1.090\n      item osd.2 weight 1.090\n      item osd.3 weight 1.090\n      item osd.4 weight 1.090\n      item osd.5 weight 1.090\n      item osd.6 weight 1.090\n      item osd.7 weight 1.090\n      item osd.8 weight 1.090\n      item osd.9 weight 1.090\n      item osd.10 weight 1.090\n      item osd.11 weight 1.090\n}\nhost host2 {\n      id -5           # do not change unnecessarily\n      id -6 class hdd         # do not change unnecessarily\n      # weight 13.080\n      alg straw2\n      hash 0  # rjenkins1\n      item osd.12 weight 1.090\n      item osd.13 weight 1.090\n      item osd.14 weight 1.090\n      item osd.15 weight 1.090\n      item osd.16 weight 1.090\n      item osd.18 weight 1.090\n      item osd.19 weight 1.090\n      item osd.17 weight 1.090\n      item osd.20 weight 1.090\n      item osd.21 weight 1.090\n      item osd.22 weight 1.090\n      item osd.23 weight 1.090\n}\nrack rack1 {\n      id -15          # do not change unnecessarily\n```\n\n----------------------------------------\n\nTITLE: Configuring MetalLB L2 Advertisement\nDESCRIPTION: This snippet creates a MetalLB L2Advertisement custom resource to advertise the IP address pool. It uses a YAML configuration applied with kubectl.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntee > /tmp/metallb_l2advertisement.yaml <<EOF\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n    name: public\n    namespace: metallb-system\nspec:\n    ipAddressPools:\n    - public\nEOF\n\nkubectl apply -f /tmp/metallb_l2advertisement.yaml\n```\n\n----------------------------------------\n\nTITLE: Linuxbridge Configuration Override for OpenStack-Helm Neutron\nDESCRIPTION: YAML configuration override for implementing Linuxbridge as the network backend in OpenStack-Helm. This snippet shows how to properly configure the network backend, dependencies, and all necessary interface drivers to switch from OpenVSwitch to Linuxbridge.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nnetwork:\n  backend: linuxbridge\ndependencies:\n  dynamic:\n    targeted:\n      linuxbridge:\n        dhcp:\n          pod:\n            - requireSameNode: true\n              labels:\n                application: neutron\n                component: neutron-lb-agent\n        l3:\n          pod:\n            - requireSameNode: true\n              labels:\n                application: neutron\n                component: neutron-lb-agent\n        metadata:\n          pod:\n            - requireSameNode: true\n              labels:\n                application: neutron\n                component: neutron-lb-agent\n        lb_agent:\n          pod: null\nconf:\n  neutron:\n    DEFAULT\n      interface_driver: linuxbridge\n  dhcp_agent:\n    DEFAULT:\n      interface_driver: linuxbridge\n  l3_agent:\n    DEFAULT:\n      interface_driver: linuxbridge\n```\n\n----------------------------------------\n\nTITLE: Creating Release Notes with Reno for OpenStack-Helm Charts\nDESCRIPTION: Commands for generating release notes using the reno tool. The first command creates a release note for a specific chart, while the second creates a common release note for multiple chart updates in a single commit.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/README.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nreno new <chart_name>\n```\n\nLANGUAGE: bash\nCODE:\n```\nreno new common\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status After Node Expansion\nDESCRIPTION: This console command shows the current Ceph cluster status after adding mnode4 to replace the failed node. The cluster is in HEALTH_WARN state with 1 out of 4 monitors down (mnode3), but the quorum is maintained by the other three nodes.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph -s\n  cluster:\n    id:     54d9af7e-da6d-4980-9075-96bb145db65c\n    health: HEALTH_WARN\n            1/4 mons down, quorum mnode1,mnode2,mnode4\n\n  services:\n    mon: 4 daemons, quorum mnode1,mnode2,mnode4, out of quorum: mnode3\n    mgr: mnode2(active), standbys: mnode1\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-6f66956547-5x4ng=up:active}, 1 up:standby\n    osd: 4 osds: 3 up, 3 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   19 pools, 101 pgs\n    objects: 354 objects, 260 MB\n    usage:   74684 MB used, 73229 MB / 144 GB avail\n    pgs:     101 active+clean\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus data source in Grafana\nDESCRIPTION: Configuration structure for setting up Prometheus as a data source in Grafana. The key (monitoring) should map to an entry in the endpoints section of values.yaml to populate the URL and authentication credentials.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/grafana.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  provisioning:\n    datasources;\n      monitoring:\n        name: prometheus\n        type: prometheus\n        access: proxy\n        orgId: 1\n        editable: true\n        basicAuth: true\n```\n\n----------------------------------------\n\nTITLE: Listing OpenStack Pods in Kubernetes\nDESCRIPTION: This command lists all OpenStack-related pods in the 'openstack' namespace, showing their status, IP addresses, and the nodes they are running on. It's useful for monitoring the health of OpenStack components.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_19\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl get pods -n openstack --show-all=false -o wide\nFlag --show-all has been deprecated, will be removed in an upcoming release\nNAME                                           READY     STATUS    RESTARTS   AGE       IP              NODE\ncinder-api-66f4f9678-2lgwk                     1/1       Unknown   0          32m       192.168.3.41    mnode3\ncinder-api-66f4f9678-flvr5                     1/1       Running   0          32m       192.168.0.202   mnode1\ncinder-api-66f4f9678-w5xhd                     1/1       Running   0          11m       192.168.4.45    mnode2\ncinder-backup-659b68b474-582kr                 1/1       Running   0          32m       192.168.4.39    mnode2\ncinder-scheduler-6778f6f88c-mm9mt              1/1       Running   0          32m       192.168.0.201   mnode1\ncinder-volume-79b9bd8bb9-qsxdk                 1/1       Running   0          32m       192.168.4.40    mnode2\nglance-api-676fd49d4d-4tnm6                    1/1       Running   0          11m       192.168.0.212   mnode1\nglance-api-676fd49d4d-j4bdb                    1/1       Unknown   0          36m       192.168.3.37    mnode3\nglance-api-676fd49d4d-wtxqt                    1/1       Running   0          36m       192.168.4.31    mnode2\ningress-7b4bc84cdd-9fs78                       1/1       Running   0          1h        192.168.5.3     mnode4\ningress-7b4bc84cdd-wztz7                       1/1       Running   0          1h        192.168.1.4     mnode6\ningress-error-pages-586c7f86d6-2jl5q           1/1       Running   0          1h        192.168.2.4     mnode5\ningress-error-pages-586c7f86d6-455j5           1/1       Unknown   0          1h        192.168.3.3     mnode3\ningress-error-pages-586c7f86d6-55j4x           1/1       Running   0          11m       192.168.4.47    mnode2\nkeystone-api-5bcc7cb698-dzm8q                  1/1       Running   0          45m       192.168.4.24    mnode2\nkeystone-api-5bcc7cb698-vvwwr                  1/1       Unknown   0          45m       192.168.3.25    mnode3\nkeystone-api-5bcc7cb698-wx5l6                  1/1       Running   0          11m       192.168.0.213   mnode1\nmariadb-ingress-84894687fd-9lmpx               1/1       Running   0          11m       192.168.4.48    mnode2\nmariadb-ingress-84894687fd-dfnkm               1/1       Unknown   2          1h        192.168.3.20    mnode3\nmariadb-ingress-error-pages-78fb865f84-p8lpg   1/1       Running   0          1h        192.168.4.17    mnode2\nmariadb-server-0                               1/1       Running   0          1h        192.168.4.18    mnode2\nmemcached-memcached-5db74ddfd5-926ln           1/1       Running   0          11m       192.168.4.49    mnode2\nmemcached-memcached-5db74ddfd5-wfr9q           1/1       Unknown   0          48m       192.168.3.23    mnode3\nrabbitmq-rabbitmq-0                            1/1       Unknown   0          1h        192.168.3.21    mnode3\nrabbitmq-rabbitmq-1                            1/1       Running   0          1h        192.168.4.19    mnode2\nrabbitmq-rabbitmq-2                            1/1       Running   0          1h        192.168.0.195   mnode1\n```\n\n----------------------------------------\n\nTITLE: Configuring OVS-vswitchd with Unix Socket in OpenStack-Helm\nDESCRIPTION: This script shows how OpenVSwitch vswitchd is configured to use Unix sockets rather than the default loopback mechanism. The command sets up the OVS virtual switch daemon with specified console logging levels and process management options.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexec /usr/sbin/ovs-vswitchd unix:${OVS_SOCKET} \\\n        -vconsole:emer \\\n        -vconsole:err \\\n        -vconsole:info \\\n        --pidfile=${OVS_PID} \\\n        --mlockall\n```\n\n----------------------------------------\n\nTITLE: ConfigMap Template with Value Injection in OpenStack-Helm\nDESCRIPTION: Template code showing how additional values are injected into the configuration file before conversion. It demonstrates the use of helper functions like authenticated_endpoint_uri_lookup to dynamically set database connections, messaging URLs, and cache server addresses.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/oslo-config.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n{{- if empty .Values.conf.keystone.database.connection -}}\n{{- $_ := tuple \"oslo_db\" \"internal\" \"user\" \"mysql\" . | include \"helm-toolkit.endpoints.authenticated_endpoint_uri_lookup\"| set .Values.conf.keystone.database \"connection\" -}}\n{{- end -}}\n\n{{- if empty .Values.conf.keystone.DEFAULT.transport_url -}}\n{{- $_ := tuple \"oslo_messaging\" \"internal\" \"user\" \"amqp\" . | include \"helm-toolkit.endpoints.authenticated_endpoint_uri_lookup\" | set .Values.conf.keystone.DEFAULT \"transport_url\" -}}\n{{- end -}}\n\n{{- if empty .Values.conf.keystone.cache.memcache_servers -}}\n{{- $_ := tuple \"oslo_cache\" \"internal\" \"memcache\" . | include \"helm-toolkit.endpoints.host_and_port_endpoint_uri_lookup\" | set .Values.conf.keystone.cache \"memcache_servers\" -}}\n{{- end -}}\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: keystone-etc\ndata:\n  keystone.conf: |\n{{ include \"helm-toolkit.utils.to_oslo_conf\" .Values.conf.keystone | indent 4 }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Scaling Applications\nDESCRIPTION: Commands for scaling down and up applications that use Ceph PVCs during the migration process.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/upgrade/multiple-osd-releases.rst#2025-04-20_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nubuntu@k8smaster:/opt/openstack-helm$  sudo kubectl scale statefulsets -n openstack mariadb-server --replicas=0\n\nubuntu@k8smaster:/opt/openstack-helm$  sudo kubectl scale statefulsets -n openstack rabbitmq-rabbitmq --replicas=0\n```\n\n----------------------------------------\n\nTITLE: Listing OpenStack Secrets in Kubernetes\nDESCRIPTION: This snippet shows the output of 'kubectl get secrets' command in the OpenStack namespace, displaying various OpenStack-related secrets such as tokens and service account credentials.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl get secrets -n openstack\nNAME                                                      TYPE                                  DATA      AGE\nceph-openstack-config-ceph-ns-key-cleaner-token-jj7n6     kubernetes.io/service-account-token   3         17m\nceph-openstack-config-ceph-ns-key-generator-token-5sqfw   kubernetes.io/service-account-token   3         17m\ndefault-token-r5knr                                       kubernetes.io/service-account-token   3         35m\ningress-error-pages-token-xxjxt                           kubernetes.io/service-account-token   3         35m\ningress-openstack-ingress-token-hrvv8                     kubernetes.io/service-account-token   3         35m\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Pods Status After Deployment\nDESCRIPTION: Command showing the status of Ceph pods after deployment but before the upgrade. All pods are either in Running or Completed state, indicating a healthy deployment that is ready for the upgrade process.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nNAME                                       READY     STATUS      RESTARTS   AGE\nceph-bootstrap-s4jkx                       0/1       Completed   0          2h\nceph-cephfs-client-key-generator-6bmzz     0/1       Completed   0          2h\nceph-mds-745576757f-4vdn4                  1/1       Running     0          2h\nceph-mds-745576757f-bxdcs                  1/1       Running     0          2h\nceph-mds-keyring-generator-f5lxf           0/1       Completed   0          2h\nceph-mgr-86bdc7c64b-7ptr4                  1/1       Running     0          2h\nceph-mgr-86bdc7c64b-xgplj                  1/1       Running     0          2h\nceph-mgr-keyring-generator-w7nxq           0/1       Completed   0          2h\nceph-mon-4c8xs                             1/1       Running     0          2h\nceph-mon-check-d85994946-zzwb4             1/1       Running     0          2h\nceph-mon-keyring-generator-jdgfw           0/1       Completed   0          2h\nceph-mon-kht8d                             1/1       Running     0          2h\nceph-mon-mkpmm                             1/1       Running     0          2h\nceph-osd-default-83945928-7jz4s            1/1       Running     0          2h\nceph-osd-default-83945928-bh82j            1/1       Running     0          2h\nceph-osd-default-83945928-t9szk            1/1       Running     0          2h\nceph-osd-keyring-generator-6rg65           0/1       Completed   0          2h\nceph-rbd-pool-z8vlc                        0/1       Completed   0          2h\n```\n\n----------------------------------------\n\nTITLE: Ceph Component Version Check\nDESCRIPTION: Version verification of various Ceph components showing consistent version across all components\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_19\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-fsrv4 -- ceph -v\nceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)\n\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-osd-default-83945928-l84tl -- ceph -v\nceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)\n\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-rgw-57c68b7cd5-vxcc5 -- ceph -v\nceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)\n\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mgr-654f97cbfd-gzb7k -- ceph -v\nceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)\n\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mds-5fdcb5c64c-c52xq -- ceph -v\nceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)\n```\n\n----------------------------------------\n\nTITLE: Tenant Ceph Namespace Activation Configuration\nDESCRIPTION: Configuration for enabling OpenStack namespace to use tenant Ceph storage, including endpoint configuration and storage class settings.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  identity:\n    namespace: openstack\n  object_store:\n    namespace: openstack\n  ceph_mon:\n    namespace: tenant-ceph\n    port:\n      mon:\n        default: 6790\nnetwork:\n  public: ${CEPH_PUBLIC_NETWORK}\n  cluster: ${CEPH_CLUSTER_NETWORK}\ndeployment:\n  storage_secrets: false\n  ceph: false\n  rbd_provisioner: false\n  csi_rbd_provisioner: false\n  cephfs_provisioner: false\n  client_secrets: true\nbootstrap:\n  enabled: false\nconf:\n  rgw_ks:\n    enabled: true\nstorageclass:\n  rbd:\n    ceph_configmap_name: tenant-ceph-etc\n    provision_storage_class: false\n    name: tenant-rbd\n```\n\n----------------------------------------\n\nTITLE: Configuring Image Settings in OpenStack-Helm Heat Chart (YAML)\nDESCRIPTION: This YAML snippet demonstrates the standard image configuration structure used in OpenStack-Helm charts, specifically for the Heat service. It defines image sources for various components and operations, including database management, Keystone integration, and service-specific containers.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/images.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nimages:\n  tags:\n    bootstrap: docker.io/openstackhelm/heat:ocata\n    db_init: docker.io/openstackhelm/heat:ocata\n    db_sync: docker.io/kolla/ubuntu-source-heat-api:ocata\n    db_drop: docker.io/openstackhelm/heat:ocata\n    ks_user: docker.io/openstackhelm/heat:ocata\n    ks_service: docker.io/openstackhelm/heat:ocata\n    ks_endpoints: docker.io/openstackhelm/heat:ocata\n    api: docker.io/kolla/ubuntu-source-heat-api:ocata\n    cfn: docker.io/kolla/ubuntu-source-heat-api:ocata\n    cloudwatch: docker.io/kolla/ubuntu-source-heat-api:ocata\n    engine: docker.io/openstackhelm/heat:ocata\n    dep_check: quay.io/airshipit/kubernetes-entrypoint:latest-ubuntu_focal\n  pull_policy: \"IfNotPresent\"\n```\n\n----------------------------------------\n\nTITLE: Listing OpenStack ConfigMaps for Ceph Integration\nDESCRIPTION: Console output showing the ConfigMaps created in the OpenStack namespace for Ceph integration. This demonstrates the configuration resources created for both primary and tenant Ceph clusters.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_26\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1: kubectl get cm -n openstack\nNAME                                                 DATA      AGE\nceph-etc                                             1         2h\nceph-openstack-config-ceph-prov-bin-clients          2         2h\nceph-rgw-bin                                         5         3m\nceph-rgw-bin-ks                                      3         3m\nceph-rgw-etc                                         1         3m\ntenant-ceph-etc                                      1         1h\ntenant-ceph-openstack-config-ceph-prov-bin-clients   2         1h\ntenant-radosgw-openstack-ceph-templates              1         3m\n...\n```\n\n----------------------------------------\n\nTITLE: Listing OpenStack Pods in Kubernetes\nDESCRIPTION: Command to list all pods in the OpenStack namespace showing their status and node distribution. This provides visibility into the OpenStack components that integrate with the Ceph storage backend.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_30\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl get pods -n openstack --show-all=false -o wide\nFlag --show-all has been deprecated, will be removed in an upcoming release\nNAME                                           READY     STATUS    RESTARTS   AGE       IP              NODE\ncinder-api-66f4f9678-2lgwk                     1/1       Unknown   0          47m       192.168.3.41    mnode3\ncinder-api-66f4f9678-flvr5                     1/1       Running   0          47m       192.168.0.202   mnode1\ncinder-api-66f4f9678-w5xhd                     1/1       Running   0          26m       192.168.4.45    mnode2\ncinder-backup-659b68b474-582kr                 1/1       Running   0          47m       192.168.4.39    mnode2\n```\n\n----------------------------------------\n\nTITLE: Neutron Server Component Configuration\nDESCRIPTION: YAML configuration showing which manifests should be enabled for the core Neutron server functionality, which is common across different SDN implementations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/neutron-multiple-sdns.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmanifests:\n  # neutron-server components:\n  configmap_bin: true\n  configmap_etc: true\n  deployment_server: true\n  deployment_rpc_server: true\n  ingress_server: true\n  job_db_init: true\n  job_db_sync: true\n  job_ks_endpoints: true\n  job_ks_service: true\n  job_ks_user: true\n  pdb_server: true\n  secret_db: true\n  secret_keystone: true\n  service_ingress_server: true\n  service_server: true\n```\n\n----------------------------------------\n\nTITLE: Listing OpenStack Pods Status in Kubernetes\nDESCRIPTION: This command displays the status of all pods in the OpenStack namespace, showing which components are affected by the node failure. Similar to the Ceph namespace, pods on mnode3 are shown as Unknown.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl get pods -n openstack --show-all=false -o wide\nNAME                                           READY     STATUS    RESTARTS   AGE       IP              NODE\ncinder-api-66f4f9678-2lgwk                     1/1       Unknown   0          22m       192.168.3.41    mnode3\ncinder-api-66f4f9678-flvr5                     1/1       Running   0          22m       192.168.0.202   mnode1\ncinder-api-66f4f9678-w5xhd                     1/1       Running   0          1m        192.168.4.45    mnode2\ncinder-backup-659b68b474-582kr                 1/1       Running   0          22m       192.168.4.39    mnode2\ncinder-scheduler-6778f6f88c-mm9mt              1/1       Running   0          22m       192.168.0.201   mnode1\ncinder-volume-79b9bd8bb9-qsxdk                 1/1       Running   0          22m       192.168.4.40    mnode2\ncinder-volume-usage-audit-1534286100-mm8r7     1/1       Running   0          4m        192.168.4.44    mnode2\nglance-api-676fd49d4d-4tnm6                    1/1       Running   0          1m        192.168.0.212   mnode1\nglance-api-676fd49d4d-j4bdb                    1/1       Unknown   0          26m       192.168.3.37    mnode3\nglance-api-676fd49d4d-wtxqt                    1/1       Running   0          26m       192.168.4.31    mnode2\ningress-7b4bc84cdd-9fs78                       1/1       Running   0          1h        192.168.5.3     mnode4\ningress-7b4bc84cdd-wztz7                       1/1       Running   0          1h        192.168.1.4     mnode6\ningress-error-pages-586c7f86d6-2jl5q           1/1       Running   0          1h        192.168.2.4     mnode5\ningress-error-pages-586c7f86d6-455j5           1/1       Unknown   0          1h        192.168.3.3     mnode3\ningress-error-pages-586c7f86d6-55j4x           1/1       Running   0          1m        192.168.4.47    mnode2\nkeystone-api-5bcc7cb698-dzm8q                  1/1       Running   0          35m       192.168.4.24    mnode2\nkeystone-api-5bcc7cb698-vvwwr                  1/1       Unknown   0          35m       192.168.3.25    mnode3\nkeystone-api-5bcc7cb698-wx5l6                  1/1       Running   0          1m        192.168.0.213   mnode1\nmariadb-ingress-84894687fd-9lmpx               1/1       Running   0          1m        192.168.4.48    mnode2\nmariadb-ingress-84894687fd-dfnkm               1/1       Unknown   2          1h        192.168.3.20    mnode3\nmariadb-ingress-error-pages-78fb865f84-p8lpg   1/1       Running   0          1h        192.168.4.17    mnode2\nmariadb-server-0                               1/1       Running   0          1h        192.168.4.18    mnode2\nmemcached-memcached-5db74ddfd5-926ln           1/1       Running   0          1m        192.168.4.49    mnode2\nmemcached-memcached-5db74ddfd5-wfr9q           1/1       Unknown   0          38m       192.168.3.23    mnode3\nrabbitmq-rabbitmq-0                            1/1       Unknown   0          1h        192.168.3.21    mnode3\nrabbitmq-rabbitmq-1                            1/1       Running   0          1h        192.168.4.19    mnode2\nrabbitmq-rabbitmq-2                            1/1       Running   0          1h        192.168.0.195   mnode1\n```\n\n----------------------------------------\n\nTITLE: Deploying Glance Service with PVC Storage\nDESCRIPTION: Installs OpenStack Glance image service using Helm chart with PVC storage configuration.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntee ${OVERRIDES_DIR}/glance/values_overrides/glance_pvc_storage.yaml <<EOF\nstorage: pvc\nvolume:\n  class_name: general\n  size: 10Gi\nEOF\n\nhelm upgrade --install glance openstack-helm/glance \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c glance glance_pvc_storage ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Defining OpenStack Service Endpoints in YAML\nDESCRIPTION: This snippet demonstrates how to define endpoints for various OpenStack services (image, compute, identity, network) in the values.yaml file. It includes host, path, type, scheme, and port information for each service.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/endpoints.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  image:\n    hosts:\n      default: glance-api\n    type: image\n    path: null\n    scheme: 'http'\n    port:\n      api: 9292\n  compute:\n    hosts:\n      default: nova-api\n    path: \"/v2/%(tenant_id)s\"\n    type: compute\n    scheme: 'http'\n    port:\n      api: 8774\n      metadata: 8775\n      novncproxy: 6080\n  identity:\n    hosts:\n      default: keystone-api\n    path: /v3\n    type: identity\n    scheme: 'http'\n    port:\n      admin: 35357\n      public: 5000\n  network:\n    hosts:\n      default: neutron-server\n    path: null\n    type: network\n    scheme: 'http'\n    port:\n      api: 9696\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Endpoints and Services in OpenStack Namespace\nDESCRIPTION: Shows the Kubernetes endpoints and services configuration for OpenStack components including Ceph, Keystone, MariaDB, RabbitMQ and others\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_30\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1: kubectl get endpoints -n openstack\nNAME                          ENDPOINTS                                                               AGE\nceph-rgw                      192.168.2.42:8088,192.168.5.44:8088                                     20m\ningress                       192.168.2.4:80,192.168.5.5:80,192.168.2.4:443 + 1 more...               3h\ningress-error-pages           192.168.2.3:8080,192.168.5.6:8080                                       3h\n```\n\n----------------------------------------\n\nTITLE: Listing OpenStack Persistent Volumes\nDESCRIPTION: Shows the current persistent volumes in the OpenStack namespace, including their capacity, access modes, and bound claims.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl get pv -n openstack\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                         STORAGECLASS   REASON    AGE\npvc-348f4c52-a9b8-11e8-bb1d-fa163ec12213   256Mi      RWO            Delete           Bound     openstack/rabbitmq-data-rabbitmq-rabbitmq-0   general                  15m\npvc-4418c745-a9b8-11e8-bb1d-fa163ec12213   256Mi      RWO            Delete           Bound     openstack/rabbitmq-data-rabbitmq-rabbitmq-1   general                  14m\npvc-524d4213-a9b8-11e8-bb1d-fa163ec12213   256Mi      RWO            Delete           Bound     openstack/rabbitmq-data-rabbitmq-rabbitmq-2   general                  14m\npvc-da9c9dd2-a9b7-11e8-bb1d-fa163ec12213   5Gi        RWO            Delete           Bound     openstack/mysql-data-mariadb-server-0         general                  17m\n```\n\n----------------------------------------\n\nTITLE: Identifying and Killing Ceph OSD Processes\nDESCRIPTION: This snippet demonstrates how to identify running Ceph OSD processes and forcefully terminate them using the kill command. It's part of a test to simulate OSD failures.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/osd-failure.rst#2025-04-20_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ ps -ef|grep /usr/bin/ceph-osd\nceph     44587 43680  1 18:12 ?        00:00:01 /usr/bin/ceph-osd --cluster ceph --osd-journal /dev/sdb5 -f -i 4 --setuser ceph --setgroup disk\nceph     44627 43744  1 18:12 ?        00:00:01 /usr/bin/ceph-osd --cluster ceph --osd-journal /dev/sdb2 -f -i 6 --setuser ceph --setgroup disk\nceph     44720 43927  2 18:12 ?        00:00:01 /usr/bin/ceph-osd --cluster ceph --osd-journal /dev/sdb6 -f -i 3 --setuser ceph --setgroup disk\nceph     44735 43868  1 18:12 ?        00:00:01 /usr/bin/ceph-osd --cluster ceph --osd-journal /dev/sdb1 -f -i 9 --setuser ceph --setgroup disk\nceph     44806 43855  1 18:12 ?        00:00:01 /usr/bin/ceph-osd --cluster ceph --osd-journal /dev/sdb4 -f -i 0 --setuser ceph --setgroup disk\nceph     44896 44011  2 18:12 ?        00:00:01 /usr/bin/ceph-osd --cluster ceph --osd-journal /dev/sdb3 -f -i 1 --setuser ceph --setgroup disk\nroot     46144 45998  0 18:13 pts/10   00:00:00 grep --color=auto /usr/bin/ceph-osd\n\n$ sudo kill -9 44587 44627 44720 44735 44806 44896\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple Ceph OSD Charts\nDESCRIPTION: Commands for creating multiple Ceph OSD charts by copying and configuring separate chart directories.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/upgrade/multiple-osd-releases.rst#2025-04-20_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nubuntu@k8smaster:/opt/openstack-helm-infra$  cp -r ceph-osd ceph-osd-vdb\nubuntu@k8smaster:/opt/openstack-helm-infra$  cp -r ceph-osd ceph-osd-vdc\n```\n\n----------------------------------------\n\nTITLE: Listing Ceph ConfigMaps in Kubernetes\nDESCRIPTION: This snippet shows the output of 'kubectl get cm' command in the Ceph namespace, listing all ConfigMaps related to Ceph components such as monitors, OSDs, and provisioners.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl get cm -n ceph\nNAME                                      DATA      AGE\nceph-client-bin                           7         25m\nceph-client-etc                           1         25m\nceph-etc                                  1         23m\nceph-mon-bin                              10        29m\nceph-mon-etc                              1         29m\nceph-osd-bin                              7         27m\nceph-osd-default                          1         27m\nceph-osd-etc                              1         27m\nceph-provisioners-ceph-provisioners-bin   4         23m\nceph-templates                            6         29m\ningress-bin                               2         30m\ningress-ceph-nginx                        0         30m\ningress-conf                              3         30m\ningress-services-tcp                      0         30m\ningress-services-udp                      0         30m\n```\n\n----------------------------------------\n\nTITLE: Deploying RabbitMQ Service\nDESCRIPTION: Installs RabbitMQ message broker using Helm chart with custom configurations and replicas.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install rabbitmq openstack-helm/rabbitmq \\\n    --namespace=openstack \\\n    --set pod.replicas.server=1 \\\n    --timeout=600s \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c rabbitmq ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Tenant Ceph Configuration YAML\nDESCRIPTION: Configuration for deploying tenant Ceph storage, including endpoint settings, network configuration, deployment options, and storage class definitions.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  identity:\n    namespace: openstack\n  object_store:\n    namespace: openstack\n  ceph_mon:\n    namespace: tenant-ceph\n    port:\n      mon:\n        default: 6790\n  ceph_mgr:\n    namespace: tenant-ceph\n    port:\n      mgr:\n        default: 7001\n      metrics:\n        default: 9284\nnetwork:\n  public: ${CEPH_PUBLIC_NETWORK}\n  cluster: ${CEPH_CLUSTER_NETWORK}\ndeployment:\n  storage_secrets: true\n  ceph: true\n  rbd_provisioner: false\n  csi_rbd_provisioner: false\n  cephfs_provisioner: false\n  client_secrets: false\nlabels:\n  mon:\n    node_selector_key: ceph-mon-tenant\n  osd:\n    node_selector_key: ceph-osd-tenant\n  rgw:\n    node_selector_key: ceph-rgw-tenant\n  mgr:\n    node_selector_key: ceph-mgr-tenant\n  job:\n    node_selector_key: tenant-ceph-control-plane\nstorageclass:\n  rbd:\n    ceph_configmap_name: tenant-ceph-etc\n    provision_storage_class: false\n    name: tenant-rbd\n    admin_secret_name: pvc-tenant-ceph-conf-combined-storageclass\n    admin_secret_namespace: tenant-ceph\n    user_secret_name: pvc-tenant-ceph-client-key\n  cephfs:\n    provision_storage_class: false\n    name: cephfs\n    user_secret_name: pvc-tenant-ceph-cephfs-client-key\n    admin_secret_name: pvc-tenant-ceph-conf-combined-storageclass\n    admin_secret_namespace: tenant-ceph\n```\n\n----------------------------------------\n\nTITLE: Conditional Kubernetes Manifest Template\nDESCRIPTION: Example of how to conditionally include a Kubernetes manifest based on the configuration values. Shows implementation for the OVS agent daemonset.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/neutron-multiple-sdns.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{{- if .Values.manifests.daemonset_ovs_agent }}\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n\n...\n\n            - name: libmodules\n          hostPath:\n            path: /lib/modules\n        - name: run\n          hostPath:\n            path: /run\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Checking Pod Status After Recovery\nDESCRIPTION: Shows the status of Ceph pods on the recovered node, with all pods now in Running status after previously being in NodeLost status.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n ceph -o wide|grep voyager3\nceph-mon-6bbs6                             1/1       Running     9          8d        135.207.240.43   voyager3\nceph-osd-default-64779b8c-lbkcd            1/1       Running     2          7d        135.207.240.43   voyager3\nceph-osd-default-6ea9de2c-gp7zm            1/1       Running     3          8d        135.207.240.43   voyager3\nceph-osd-default-7544b6da-7mfdc            1/1       Running     3          8d        135.207.240.43   voyager3\nceph-osd-default-7cfc44c1-hhk8v            1/1       Running     3          8d        135.207.240.43   voyager3\nceph-osd-default-83945928-b95qs            1/1       Running     3          8d        135.207.240.43   voyager3\nceph-osd-default-f9249fa9-n7p4v            1/1       Running     4          8d        135.207.240.43   voyager3\n```\n\n----------------------------------------\n\nTITLE: Re-enabling CephX Authentication in Kubernetes\nDESCRIPTION: This snippet shows how to re-enable CephX authentication by restoring the original Ceph configuration.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/namespace-deletion.rst#2025-04-20_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl --namespace ${CEPH_NAMESPACE} delete configmap ceph-mon-etc\nkubectl --namespace ${CEPH_NAMESPACE} create configmap ceph-mon-etc --from-file=ceph.conf=/tmp/ceph/ceph-mon.conf\n```\n\n----------------------------------------\n\nTITLE: Displaying Ceph Monitor Configuration in YAML\nDESCRIPTION: This snippet shows the YAML output of the 'ceph-mon-etc' ConfigMap, which contains the Ceph configuration file. It includes global settings, monitor addresses, and OSD network configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\ndata:\n  ceph.conf: |\n    [global]\n    cephx = true\n    cephx_cluster_require_signatures = true\n    cephx_require_signatures = false\n    cephx_service_require_signatures = false\n    fsid = 3e53e3b7-e5d9-4bab-9701-134687f4954e\n    mon_addr = :6789\n    mon_host = ceph-mon-discovery.ceph.svc.cluster.local:6789\n    [osd]\n    cluster_network = 10.0.0.0/24\n    ms_bind_port_max = 7100\n    ms_bind_port_min = 6800\n    osd_max_object_name_len = 256\n    osd_mkfs_options_xfs = -f -i size=2048\n    osd_mkfs_type = xfs\n    public_network = 10.0.0.0/24\nkind: ConfigMap\nmetadata:\n  creationTimestamp: 2018-08-27T04:55:32Z\n  name: ceph-mon-etc\n  namespace: ceph\n  resourceVersion: \"3218\"\n  selfLink: /api/v1/namespaces/ceph/configmaps/ceph-mon-etc\n  uid: 6d9fdcba-a9b5-11e8-bb1d-fa163ec12213\n```\n\n----------------------------------------\n\nTITLE: Enabling Prometheus Monitoring for Elasticsearch\nDESCRIPTION: This snippet shows how to enable Prometheus monitoring for Elasticsearch in the values.yaml file. This configuration allows Prometheus to collect metrics from the Elasticsearch exporter.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/elasticsearch.rst#2025-04-20_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmonitoring:\n  prometheus:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Checking Tenant Ceph Cluster Status\nDESCRIPTION: Console output showing the status of the tenant Ceph cluster after deployment. This verifies the health and configuration of the Ceph monitors, managers, OSDs, and Rados Gateway services.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_25\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1: kubectl exec -n tenant-ceph ceph-mon-2g6km -- ceph -s\n  cluster:\n    id:     38339a5a-d976-49dd-88a0-2ac092c271c7\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum node3,node2,node1\n    mgr: node2(active), standbys: node1\n    osd: 3 osds: 3 up, 3 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 93 pgs\n    objects: 193 objects, 37421 bytes\n    usage:   33394 MB used, 199 GB / 232 GB avail\n    pgs:     93 active+clean\n```\n\n----------------------------------------\n\nTITLE: Updating ServiceAccount Template with ImagePullSecrets\nDESCRIPTION: YAML modifications to the Helm-Toolkit ServiceAccount template that conditionally add the imagePullSecrets field when registry authentication is enabled. This allows Kubernetes to use the registry secrets when pulling images.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/support-OCI-image-registry-with-authentication-turned-on.rst#2025-04-20_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: v1\nkind: ServiceAccount\n...\n{{- if $envAll.Values.endpoints.oci_image_registry.auth.enabled }}\nimagePullSecrets:\n  - name: {{ index $envAll.Values.secrets.oci_image_registry $envAll.Chart.Name }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Verifying Tenant Ceph Storage Directory Structure\nDESCRIPTION: Console output showing the storage directory structure on different nodes. This shows the separation between primary Ceph and tenant-ceph directories.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_23\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ ls -l /var/lib/openstack-helm/\ntotal 8\ndrwxr-xr-x 4 root root 4096 Aug 27 04:57 ceph\ndrwxr-xr-x 3 root root 4096 Aug 27 05:47 tenant-ceph\n```\n\n----------------------------------------\n\nTITLE: Network Backend Configuration in YAML\nDESCRIPTION: YAML configuration specifying the network backend type to use. This setting determines which networking technology (OpenVSwitch, LinuxBridge, SR-IOV) will be used for the deployment.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nnetwork:\n  # provide what type of network wiring will be used\n  # possible options: openvswitch, linuxbridge, sriov\n  backend:\n    - openvswitch\n```\n\n----------------------------------------\n\nTITLE: Reinstalling Ceph Charts in OpenStack Helm\nDESCRIPTION: This snippet demonstrates how to reinstall Ceph charts in a multinode OpenStack Helm environment. It includes steps to delete existing charts and run the deployment script.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/namespace-deletion.rst#2025-04-20_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nhelm delete --purge ceph-openstack-config\n\nfor chart in $(helm list --namespace ${CEPH_NAMESPACE} | awk '/ceph-/{print $1}'); do\n  helm delete ${chart} --purge;\ndone\n```\n\nLANGUAGE: console\nCODE:\n```\ncd /opt/openstack-helm-infra/\n./tools/deployment/multinode/030-ceph.sh\n```\n\n----------------------------------------\n\nTITLE: Deploying Heat Service\nDESCRIPTION: Installs OpenStack Heat orchestration service using Helm chart with specified configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install heat openstack-helm/heat \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c heat ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Listing OpenStack Secrets for Ceph Integration\nDESCRIPTION: Console output showing the Kubernetes secrets created for Ceph integration in OpenStack namespace. This includes keyring secrets, service account tokens, and client authentication keys.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_28\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1: kubectl get secrets -n openstack\nNAME                                                             TYPE                                  DATA      AGE\nceph-keystone-admin                                              Opaque                                8         4m\nceph-keystone-user                                               Opaque                                8         4m\nceph-keystone-user-rgw                                           Opaque                                8         4m\nceph-ks-endpoints-token-crnrr                                    kubernetes.io/service-account-token   3         4m\nceph-ks-service-token-9bnr8                                      kubernetes.io/service-account-token   3         4m\nceph-openstack-config-ceph-ns-key-cleaner-token-jj7n6            kubernetes.io/service-account-token   3         2h\nceph-openstack-config-ceph-ns-key-generator-token-5sqfw          kubernetes.io/service-account-token   3         2h\nceph-rgw-storage-init-token-mhqdw                                kubernetes.io/service-account-token   3         4m\nceph-rgw-token-9s6nd                                             kubernetes.io/service-account-token   3         4m\nos-ceph-bootstrap-rgw-keyring                                    Opaque                                1         36m\npvc-ceph-client-key                                              kubernetes.io/rbd                     1         2h\npvc-tenant-ceph-client-key                                       kubernetes.io/rbd                     1         1h\nswift-ks-user-token-9slvc                                        kubernetes.io/service-account-token   3         4m\ntenant-ceph-openstack-config-ceph-ns-key-cleaner-token-r6v9v     kubernetes.io/service-account-token   3         1h\ntenant-ceph-openstack-config-ceph-ns-key-generator-token-dt472   kubernetes.io/service-account-token   3         1h\n...\n```\n\n----------------------------------------\n\nTITLE: Defining OpenStack-Helm Values YAML Structure\nDESCRIPTION: This snippet references a specification in progress for cementing conventions around the format and ordering of charts' values.yaml files. These conventions will be used to support reusable Helm-Toolkit functions and ease developer onboarding.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/osh-1.0-requirements.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nvalues.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking Pod Status on Failed Node\nDESCRIPTION: Shows the status of Ceph pods on the failed node, with all pods in NodeLost status.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n ceph -o wide|grep voyager3\nceph-mgr-55f68d44b8-hncrq                  1/1       Unknown     6          8d        135.207.240.43   voyager3\nceph-mon-6bbs6                             1/1       NodeLost    8          8d        135.207.240.43   voyager3\nceph-osd-default-64779b8c-lbkcd            1/1       NodeLost    1          6d        135.207.240.43   voyager3\nceph-osd-default-6ea9de2c-gp7zm            1/1       NodeLost    2          8d        135.207.240.43   voyager3\nceph-osd-default-7544b6da-7mfdc            1/1       NodeLost    2          8d        135.207.240.43   voyager3\nceph-osd-default-7cfc44c1-hhk8v            1/1       NodeLost    2          8d        135.207.240.43   voyager3\nceph-osd-default-83945928-b95qs            1/1       NodeLost    2          8d        135.207.240.43   voyager3\nceph-osd-default-f9249fa9-n7p4v            1/1       NodeLost    3          8d        135.207.240.43   voyager3\n```\n\n----------------------------------------\n\nTITLE: Viewing Ceph OSD Tree\nDESCRIPTION: This command displays the hierarchical structure of OSDs in the Ceph cluster, showing their status and weight.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/disk-failure.rst#2025-04-20_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph osd tree\nID CLASS WEIGHT   TYPE NAME         STATUS REWEIGHT PRI-AFF\n-1       43.67981 root default\n-9       10.91995     host voyager1\n 5   hdd  1.81999         osd.5         up  1.00000 1.00000\n 6   hdd  1.81999         osd.6         up  1.00000 1.00000\n10   hdd  1.81999         osd.10        up  1.00000 1.00000\n17   hdd  1.81999         osd.17        up  1.00000 1.00000\n19   hdd  1.81999         osd.19        up  1.00000 1.00000\n21   hdd  1.81999         osd.21        up  1.00000 1.00000\n-3       10.91995     host voyager2\n 1   hdd  1.81999         osd.1         up  1.00000 1.00000\n 4   hdd  1.81999         osd.4         up  1.00000 1.00000\n11   hdd  1.81999         osd.11        up  1.00000 1.00000\n13   hdd  1.81999         osd.13        up  1.00000 1.00000\n16   hdd  1.81999         osd.16        up  1.00000 1.00000\n18   hdd  1.81999         osd.18        up  1.00000 1.00000\n-2       10.91995     host voyager3\n 0   hdd  1.81999         osd.0         up  1.00000 1.00000\n 3   hdd  1.81999         osd.3         up  1.00000 1.00000\n12   hdd  1.81999         osd.12        up  1.00000 1.00000\n20   hdd  1.81999         osd.20        up  1.00000 1.00000\n22   hdd  1.81999         osd.22        up  1.00000 1.00000\n23   hdd  1.81999         osd.23        up  1.00000 1.00000\n-4       10.91995     host voyager4\n 2   hdd  1.81999         osd.2       down        0 1.00000\n 7   hdd  1.81999         osd.7         up  1.00000 1.00000\n 8   hdd  1.81999         osd.8         up  1.00000 1.00000\n 9   hdd  1.81999         osd.9         up  1.00000 1.00000\n14   hdd  1.81999         osd.14        up  1.00000 1.00000\n15   hdd  1.81999         osd.15        up  1.00000 1.00000\n```\n\n----------------------------------------\n\nTITLE: Initial Ceph OSD Configuration\nDESCRIPTION: YAML configuration showing the disk layout for the initial single ceph-osd chart setup with multiple OSDs and their journal locations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/upgrade/multiple-osd-releases.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nosd:\n  - data:\n      type: block-logical\n      location: /dev/vdb\n    journal:\n      type: block-logical\n      location:  /dev/vda1\n  - data:\n      type: block-logical\n      location: /dev/vdc\n    journal:\n      type: block-logical\n      location:  /dev/vda2\n```\n\n----------------------------------------\n\nTITLE: CRUSH Rule Configuration Example\nDESCRIPTION: Shows the CRUSH rule configuration using host as the failure domain, including rule listing and detailed rule structure.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# ceph osd crush rule ls\nreplicated_host\n# ceph osd crush rule dump replicated_host\n```\n\n----------------------------------------\n\nTITLE: Listing Ceph Services in Kubernetes\nDESCRIPTION: This snippet shows the output of 'kubectl get svc' command in the Ceph namespace, displaying the Ceph-related services running in the Kubernetes cluster, including monitor, manager, and ingress services.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl get svc -n ceph\nNAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nceph-mgr              ClusterIP   10.111.185.73    <none>        7000/TCP,9283/TCP   27m\nceph-mon              ClusterIP   None             <none>        6789/TCP            31m\nceph-mon-discovery    ClusterIP   None             <none>        6789/TCP            31m\ningress               ClusterIP   10.100.23.32     <none>        80/TCP,443/TCP      32m\ningress-error-pages   ClusterIP   None             <none>        80/TCP              32m\ningress-exporter      ClusterIP   10.109.196.155   <none>        10254/TCP           32m\n```\n\n----------------------------------------\n\nTITLE: Defining Indices and Tables in reStructuredText for OpenStack-Helm Documentation\nDESCRIPTION: This snippet defines the indices and tables section of the documentation using reStructuredText. It includes references to the general index and search functionality.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/index.rst#2025-04-20_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n* :ref:`genindex`\n* :ref:`search`\n```\n\n----------------------------------------\n\nTITLE: Reinstalling Ceph Charts and Verifying Cluster Health\nDESCRIPTION: This snippet demonstrates how to reinstall Ceph charts, activate the namespace, and verify the cluster health after recovery.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/namespace-deletion.rst#2025-04-20_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nfor chart in $(helm list --namespace ${CEPH_NAMESPACE} | awk '/ceph-/{print $1}'); do\n  helm delete ${chart} --purge;\ndone\n```\n\nLANGUAGE: console\nCODE:\n```\ncd /opt/openstack-helm-infra/\n./tools/deployment/multinode/030-ceph.sh\n./tools/deployment/multinode/040-ceph-ns-activate.sh\n```\n\nLANGUAGE: console\nCODE:\n```\nMON_POD=$(kubectl get pods --namespace=${CEPH_NAMESPACE} \\\n--selector=\"application=ceph\" --selector=\"component=mon\" \\\n--no-headers | awk '{ print $1; exit }')\n\nkubectl exec --namespace=${CEPH_NAMESPACE} ${MON_POD} -- ceph status\n```\n\nLANGUAGE: console\nCODE:\n```\nkubectl exec --namespace=${CEPH_NAMESPACE} ${MON_POD} -- ceph fs set cephfs standby_count_wanted 0\n```\n\n----------------------------------------\n\nTITLE: Querying MariaDB Database in OpenStack Helm\nDESCRIPTION: Example command to test MariaDB connectivity and list available databases within a Kubernetes pod. Executes a MySQL show databases command through kubectl exec into the mariadb-0 pod in the openstack namespace.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/troubleshooting/database.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec mariadb-0 -it -n openstack -- mysql -h mariadb.openstack -uroot -ppassword -e 'show databases;'\n```\n\n----------------------------------------\n\nTITLE: Creating Ansible Inventory for Kubernetes Cluster\nDESCRIPTION: YAML configuration for Ansible inventory defining nodes and variables for Kubernetes cluster setup.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/kubernetes.rst#2025-04-20_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncat > ~/osh/inventory.yaml <<EOF\n---\nall:\n  vars:\n    ansible_port: 22\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: /home/ubuntu/.ssh/id_rsa\n    ansible_ssh_extra_args: -o StrictHostKeyChecking=no\n    kubectl:\n      user: ubuntu\n      group: ubuntu\n    docker_users:\n      - ununtu\n    client_ssh_user: ubuntu\n    cluster_ssh_user: ubuntu\n    metallb_setup: true\n    loopback_setup: true\n    loopback_device: /dev/loop100\n    loopback_image: /var/lib/openstack-helm/ceph-loop.img\n    loopback_image_size: 12G\n  children:\n    primary:\n      hosts:\n        primary:\n          ansible_host: 10.10.10.10\n    k8s_cluster:\n      hosts:\n        node-1:\n          ansible_host: 10.10.10.11\n        node-2:\n          ansible_host: 10.10.10.12\n        node-3:\n          ansible_host: 10.10.10.13\n    k8s_control_plane:\n      hosts:\n        node-1:\n          ansible_host: 10.10.10.11\n    k8s_nodes:\n      hosts:\n        node-2:\n          ansible_host: 10.10.10.12\n        node-3:\n          ansible_host: 10.10.10.13\nEOF\n```\n\n----------------------------------------\n\nTITLE: Retrieving Ceph Monitor Status in JSON Format\nDESCRIPTION: Command to retrieve the detailed status of a Ceph monitor in JSON format, showing the monitor's name, rank, state, election epoch, quorum information, and feature compatibility details.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-5qn68 -- ceph mon_status -f json-pretty\n```\n\n----------------------------------------\n\nTITLE: Listing Kubernetes Storage Classes\nDESCRIPTION: This snippet shows the output of 'kubectl get storageclasses' command, displaying the available storage classes in the Kubernetes cluster. It shows a 'general' storage class using the Ceph RBD provisioner.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl get storageclasses\nNAME      PROVISIONER    AGE\ngeneral   ceph.com/rbd   14m\n```\n\n----------------------------------------\n\nTITLE: Installing Rally Chart in OpenStack-Helm\nDESCRIPTION: Command to install the Rally chart into a Kubernetes cluster. The command deploys Rally to the 'openstack' namespace and enables benchmarking of OpenStack services specified in the values.yaml file.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/rally/README.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name=rally ./rally --namespace=openstack\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status After OSD Failure\nDESCRIPTION: This snippet shows how to check the Ceph cluster status using the 'ceph -s' command after artificially inducing OSD failures. It displays the cluster health, including down OSDs and degraded data.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/osd-failure.rst#2025-04-20_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     fd366aef-b356-4fe7-9ca5-1c313fe2e324\n    health: HEALTH_WARN\n            6 osds down\n            1 host (6 osds) down\n            Reduced data availability: 8 pgs inactive, 58 pgs peering\n            Degraded data redundancy: 141/1002 objects degraded (14.072%), 133 pgs degraded\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager4(active)\n    osd: 24 osds: 18 up, 24 in\n```\n\n----------------------------------------\n\nTITLE: Labeling Kubernetes Nodes for MariaDB Deployment\nDESCRIPTION: Command to label Kubernetes nodes that should host MariaDB instances. Nodes must be labeled with 'openstack-control-plane=enabled' (or as configured in values.yaml) to receive MariaDB instances.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/mariadb/README.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl label nodes openstack-control-plane=enabled --all\n```\n\n----------------------------------------\n\nTITLE: Preparing Repaired Disk\nDESCRIPTION: This command creates a new partition table on the repaired disk if it's not replaced.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/disk-failure.rst#2025-04-20_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n(voyager4)$ parted /dev/sdh mklabel msdos\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status in Kubernetes Environment\nDESCRIPTION: Command to check the overall health status of the Ceph cluster by executing the 'ceph -s' command inside a Ceph monitor pod, showing cluster ID, health status, services, data usage, and I/O metrics.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-5qn68 -- ceph -s\n  cluster:\n    id:     54d9af7e-da6d-4980-9075-96bb145db65c\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum mnode1,mnode2,mnode3\n    mgr: mnode2(active), standbys: mnode3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-6f66956547-c25cx=up:active}, 1 up:standby\n    osd: 3 osds: 3 up, 3 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   19 pools, 101 pgs\n    objects: 354 objects, 260 MB\n    usage:   77807 MB used, 70106 MB / 144 GB avail\n    pgs:     101 active+clean\n\n  io:\n    client:   48769 B/s wr, 0 op/s rd, 12 op/s wr\n```\n\n----------------------------------------\n\nTITLE: Updating Nova Compute Dependencies for LinuxBridge\nDESCRIPTION: YAML configuration to update Nova compute dependencies to use LinuxBridge agent instead of OVS agent. This change is required in the nova/values.yaml file.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/support-linux-bridge-on-neutron.rst#2025-04-20_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndependencies:\n  compute:\n    daemonset:\n    - lb-agent\n```\n\n----------------------------------------\n\nTITLE: Configuring Neutron Manifest Options in YAML\nDESCRIPTION: Configuration section in values.yaml that defines which Kubernetes resources should be enabled in the Neutron deployment. Each manifest can be individually toggled using boolean values.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/neutron-multiple-sdns.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmanifests:\n  configmap_bin: true\n  configmap_etc: true\n  daemonset_dhcp_agent: true\n  daemonset_l3_agent: true\n  daemonset_metadata_agent: true\n  daemonset_ovs_agent: true\n  daemonset_ovs_db: true\n  daemonset_ovs_vswitchd: true\n  deployment_server: true\n  deployment_rpc_server: true\n  ingress_server: true\n  job_db_init: true\n  job_db_sync: true\n  job_ks_endpoints: true\n  job_ks_service: true\n  job_ks_user: true\n  pdb_server: true\n  secret_db: true\n  secret_keystone: true\n  service_ingress_server: true\n  service_server: true\n```\n\n----------------------------------------\n\nTITLE: OpenStack Pods Status Display\nDESCRIPTION: Console output showing the status of OpenStack-related pods in the OpenStack namespace.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl get pods -n openstack -o wide\nNAME                                                READY     STATUS      RESTARTS   AGE       IP              NODE\nceph-openstack-config-ceph-ns-key-generator-mcxrs   0/1       Completed   0          11m       192.168.2.16    node2\n[...]\n```\n\n----------------------------------------\n\nTITLE: Help Output for utils-checkPGs.py Script\nDESCRIPTION: The help output for the utils-checkPGs.py utility, showing command line arguments and usage information. This explains how to validate PGs and OSDs mapping across different pools.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nroot@host5:/# /tmp/utils-checkPGs.py --help\nusage: utils-checkPGs.py [-h] PoolName [PoolName ...]\n\nCross-check the OSDs assigned to the Placement Groups (PGs) of a ceph pool\nwith the CRUSH topology.  The cross-check compares the OSDs in a PG and\nverifies the OSDs reside in separate failure domains.  PGs with OSDs in\nthe same failure domain are flagged as violation.  The offending PGs are\nprinted to stdout.\n\nThis CLI is executed on-demand on a ceph-mon pod.  To invoke the CLI, you\ncan specify one pool or list of pools to check.  The special pool name\nAll (or all) checks all the pools in the ceph cluster.\n\npositional arguments:\n  PoolName    List of pools (or All) to validate the PGs and OSDs mapping\n\noptional arguments:\n  -h, --help  show this help message and exit\nroot@host5:/#\n```\n\n----------------------------------------\n\nTITLE: Validating OpenStack Service Integration for Tenant Ceph\nDESCRIPTION: Console output showing OpenStack service list after Tenant Ceph deployment. This verifies that the swift object-store service has been properly registered with Keystone identity service.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_21\n\nLANGUAGE: console\nCODE:\n```\n+ openstack service list\n+----------------------------------+----------+--------------+\n| ID                               | Name     | Type         |\n+----------------------------------+----------+--------------+\n| 0eddeb6af4fd43ea8f73f63a1ae01438 | swift    | object-store |\n| 67cc6b945e934246b25d31a9374a64af | keystone | identity     |\n+----------------------------------+----------+--------------+\n```\n\n----------------------------------------\n\nTITLE: Removing Failed OSD from Ceph Cluster\nDESCRIPTION: These commands remove the failed OSD (OSD ID 2 in this example) from the Ceph cluster configuration.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/disk-failure.rst#2025-04-20_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph osd lost 2\n(mon-pod):/# ceph osd crush remove osd.2\n(mon-pod):/# ceph auth del osd.2\n(mon-pod):/# ceph osd rm 2\n```\n\n----------------------------------------\n\nTITLE: Proposed Configuration Extension for Software-Specific Settings\nDESCRIPTION: Shows the proposed extension to the configuration schema that adds generic software information for Apache2, including binary location, startup arguments, and directory configurations to support multi-OS deployments.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/multi-os.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  software:\n    apache2:\n      #the apache2 binary location\n      binary: apache2\n      start_args: -DFOREGROUND\n      stop_args: -k graceful-stop\n      #directory where to drop the config files for apache vhosts\n      conf_dir: /etc/apache2/conf-enabled\n      sites_dir: /etc/apache2/sites-enabled\n```\n\n----------------------------------------\n\nTITLE: Successful PG Validation with utils-checkPGs.py\nDESCRIPTION: Example of utils-checkPGs.py output when no failure domain violations are found in the Ceph cluster configuration, showing a passed check for the rbd pool.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nroot@host5:/# /tmp/utils-checkPGs.py rbd\nChecking PGs in pool rbd ... Passed\n```\n\n----------------------------------------\n\nTITLE: Configuring Nagios Commands in OpenStack-Helm\nDESCRIPTION: This configuration defines the command definitions that Nagios will use when executing service checks. It includes commands for sending SNMP traps, HTTP posts, and checking Prometheus host status. These commands are essential for alerting and monitoring system health.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/nagios.rst#2025-04-20_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nagios:\n    commands:\n      - send_service_snmp_trap:\n          command_name: send_service_snmp_trap\n          command_line: \"$USER1$/send_service_trap.sh '$USER8$' '$HOSTNAME$' '$SERVICEDESC$' $SERVICESTATEID$ '$SERVICEOUTPUT$' '$USER4$' '$USER5$'\"\n      - send_host_snmp_trap:\n          command_name: send_host_snmp_trap\n          command_line: \"$USER1$/send_host_trap.sh '$USER8$' '$HOSTNAME$' $HOSTSTATEID$ '$HOSTOUTPUT$' '$USER4$' '$USER5$'\"\n      - send_service_http_post:\n          command_name: send_service_http_post\n          command_line: \"$USER1$/send_http_post_event.py --type service --hostname '$HOSTNAME$' --servicedesc '$SERVICEDESC$' --state_id $SERVICESTATEID$ --output '$SERVICEOUTPUT$' --monitoring_hostname '$HOSTNAME$' --primary_url '$USER6$' --secondary_url '$USER7$'\"\n      - send_host_http_post:\n          command_name: send_host_http_post\n          command_line: \"$USER1$/send_http_post_event.py --type host --hostname '$HOSTNAME$' --state_id $HOSTSTATEID$ --output '$HOSTOUTPUT$' --monitoring_hostname '$HOSTNAME$' --primary_url '$USER6$' --secondary_url '$USER7$'\"\n      - check_prometheus_host_alive:\n          command_name: check-prometheus-host-alive\n          command_line: \"$USER1$/check_rest_get_api.py --url $USER2$ --warning_response_seconds 5 --critical_response_seconds 10\"\n```\n\n----------------------------------------\n\nTITLE: Adding OpenStack-Helm Repository to Helm\nDESCRIPTION: This command adds the official OpenStack-Helm repository to your Helm configuration. This repository contains all the published OpenStack-Helm charts that will be needed for deployment.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/before_starting.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add openstack-helm https://tarballs.opendev.org/openstack/openstack-helm\n```\n\n----------------------------------------\n\nTITLE: Listing OpenStack Pods in Kubernetes with Node Allocation\nDESCRIPTION: Command to list all running OpenStack pods in the Kubernetes environment, showing their status, IP addresses, and node allocation, which helps verify the current state of OpenStack services using Ceph.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl get pods -n openstack --show-all=false -o wide\nNAME                                           READY     STATUS    RESTARTS   AGE       IP              NODE\ncinder-api-66f4f9678-2lgwk                     1/1       Running   0          12m       192.168.3.41    mnode3\ncinder-api-66f4f9678-flvr5                     1/1       Running   0          12m       192.168.0.202   mnode1\ncinder-backup-659b68b474-582kr                 1/1       Running   0          12m       192.168.4.39    mnode2\n```\n\n----------------------------------------\n\nTITLE: Disabling CephX Authentication in Kubernetes\nDESCRIPTION: This snippet shows how to disable CephX authentication by modifying the Ceph configuration and restarting the monitor pods.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/namespace-deletion.rst#2025-04-20_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nmkdir -p /tmp/ceph/ceph-templates /tmp/ceph/extracted-keys\n\nkubectl get -n ${CEPH_NAMESPACE} configmaps ceph-mon-etc -o=jsonpath='{.data.ceph\\.conf}' > /tmp/ceph/ceph-mon.conf\nsed '/\\[global\\]/a auth_client_required = none' /tmp/ceph/ceph-mon.conf | \\\nsed '/\\[global\\]/a auth_service_required = none' | \\\nsed '/\\[global\\]/a auth_cluster_required = none' > /tmp/ceph/ceph-mon-noauth.conf\n\nkubectl --namespace ${CEPH_NAMESPACE} delete configmap ceph-mon-etc\nkubectl --namespace ${CEPH_NAMESPACE} create configmap ceph-mon-etc --from-file=ceph.conf=/tmp/ceph/ceph-mon-noauth.conf\n\nkubectl delete pod --namespace ${CEPH_NAMESPACE} -l application=ceph,component=mon\n```\n\nLANGUAGE: console\nCODE:\n```\nMON_POD=$(kubectl get pods --namespace=${CEPH_NAMESPACE} \\\n--selector=\"application=ceph\" --selector=\"component=mon\" \\\n--no-headers | awk '{ print $1; exit }')\n\nkubectl exec --namespace=${CEPH_NAMESPACE} ${MON_POD} -- ceph status\n```\n\n----------------------------------------\n\nTITLE: Calculating Chart Version and Package with Build Metadata in Bash\nDESCRIPTION: Script for calculating the version number for chart tarballs by counting commits after the latest tag and adding build metadata with commit SHAs from both OpenStack-Helm repositories.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/2025.1/chart_versioning.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ PATCH=$(git log --oneline <tag>.. <chart_directory> | wc -l)\n$ OSH_COMMIT_SHA=$(cd ${SRC}/openstack-helm; git rev-parse --short HEAD)\n$ OSH_INFRA_COMMIT_SHA=$(cd ${SRC}/openstack-helm-infra; git rev-parse --short HEAD)\n$ helm package <chart> --version 2024.2.${PATCH}+${OSH_COMMIT_SHA}_${OSH_INFRA_COMMIT_SHA}\n```\n\n----------------------------------------\n\nTITLE: Ceph Monitor Update Status - Console Output\nDESCRIPTION: Console output showing the rolling update process of Ceph monitor pods during the upgrade.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nceph-mon-4c8xs                             0/1       Terminating   0          2h\nceph-mon-check-d85994946-zzwb4             1/1       Running       0          2h\nceph-mon-keyring-generator-jdgfw           0/1       Completed     0          2h\nceph-mon-kht8d                             1/1       Running       0          2h\nceph-mon-mkpmm                             1/1       Running       0          2h\n```\n\nLANGUAGE: console\nCODE:\n```\nceph-mon-7zxjs                             1/1       Running     1          4m\nceph-mon-84xt2                             1/1       Running     1          2m\nceph-mon-check-d85994946-zzwb4             1/1       Running     0          2h\nceph-mon-fsrv4                             1/1       Running     1          6m\nceph-mon-keyring-generator-jdgfw           0/1       Completed   0          2h\n```\n\n----------------------------------------\n\nTITLE: Configuring DHCP Agent Interface Driver in YAML\nDESCRIPTION: YAML configuration specifying the interface driver for the DHCP agent. This setting determines how the tap interface created for serving DHCP requests is wired, using OpenVSwitch in this example.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  dhcp_agent:\n    DEFAULT:\n      # we can define here, which driver we are using:\n      # openvswitch or linuxbridge\n      interface_driver: openvswitch\n```\n\n----------------------------------------\n\nTITLE: Enabling LinuxBridge and Disabling OVS in Manifests\nDESCRIPTION: YAML configuration that enables the LinuxBridge agent daemonset while disabling OVS-related daemonsets. This is the manifest configuration needed to switch from OVS to LinuxBridge.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/support-linux-bridge-on-neutron.rst#2025-04-20_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmanifests:\n  daemonset_lb_agent: true\n  daemonset_ovs_agent: false\n  daemonset_ovs_db: false\n  daemonset_ovs_vswitchd: false\n```\n\n----------------------------------------\n\nTITLE: Ceph RBD Cluster Configuration\nDESCRIPTION: YAML configuration for the primary Ceph cluster used for Kubernetes RBD storage, including endpoint configurations and storage class settings.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndeployment:\n  storage_secrets: true\n  ceph: true\n  rbd_provisioner: false\n  csi_rbd_provisioner: true\n  cephfs_provisioner: false\n  client_secrets: false\nendpoints:\n  ceph_mon:\n    namespace: ceph\n    port:\n      mon:\n        default: 6789\n  ceph_mgr:\n    namespace: ceph\n    port:\n      mgr:\n        default: 7000\n      metrics:\n        default: 9283\nmanifests:\n  deployment_mds: false\nbootstrap:\n  enabled: true\nconf:\n  pool:\n    target:\n      osd: 3\nstorageclass:\n  rbd:\n    ceph_configmap_name: ceph-etc\n  cephfs:\n    provision_storage_class: false\nceph_mgr_modules_config:\n  prometheus:\n    server_port: 9283\nmonitoring:\n  prometheus:\n    enabled: true\n    ceph_mgr:\n      port: 9283\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status\nDESCRIPTION: This command displays the current status of the Ceph cluster, showing cluster health, services, and data usage.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/disk-failure.rst#2025-04-20_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n    health: HEALTH_WARN\n            too few PGs per OSD (23 < min 30)\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager1(active), standbys: voyager3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:standby\n    osd: 24 osds: 23 up, 23 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 240 objects, 3359 bytes\n    usage:   2548 MB used, 42814 GB / 42816 GB avail\n    pgs:     182 active+clean\n```\n\n----------------------------------------\n\nTITLE: Example CHANGELOG.md with 2024.2.0 Tag in Markdown\nDESCRIPTION: Specific example of how the CHANGELOG.md will look with the new versioning scheme, showing entries under the 2024.2.0 tag and for commits after that tag.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/2025.1/chart_versioning.rst#2025-04-20_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## 2024.2.0-<num_commits_after_2024.2.0>\n\n- Some new update\n\n## 2024.2.0\n\n- Some update\n- Previous update\n```\n\n----------------------------------------\n\nTITLE: Including README Document with RST Directive\nDESCRIPTION: Uses the RST include directive to import the contents of the main README.rst file from two directory levels up in the project structure. This is a common pattern in documentation to avoid duplication.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/readme.rst#2025-04-20_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../../README.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Prometheus Exporter\nDESCRIPTION: This snippet shows the configuration options for the Elasticsearch Prometheus exporter. It includes settings for gathering metrics from all nodes and setting the timeout for metrics queries.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/elasticsearch.rst#2025-04-20_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconf:\n  prometheus_elasticsearch_exporter:\n    es:\n      all: true\n      timeout: 20s\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Current Image Schema Structure in OpenStack Helm\nDESCRIPTION: Shows the existing schema used for defining images in OpenStack Helm charts, including image tags, pull policy, and local registry configuration.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/multi-os.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nimages:\n  tags:\n    imagename1: imagelocation:version-distro\n    imagename2: imagelocation:version-distro\n  pull_policy:\n  local_registry:\n```\n\n----------------------------------------\n\nTITLE: Ceph Image Version Update Configuration - YAML\nDESCRIPTION: YAML configuration showing updated image versions for various Ceph components including bootstrap, config helper, monitors, OSDs, and provisioners.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nimages:\n  tags:\n    ceph_bootstrap: 'docker.io/ceph/daemon:master-0351083-luminous-ubuntu-16.04-x86_64'\n    ceph_config_helper: 'docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal'\n    ceph_rbd_pool: 'docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal'\n    ceph_mon_check: 'docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal'\n    ceph_mon: 'docker.io/ceph/daemon:master-a8d20ed-luminous-ubuntu-16.04-x86_64'\n    ceph_osd: 'docker.io/ceph/daemon:master-a8d20ed-luminous-ubuntu-16.04-x86_64'\n    ceph_mds: 'docker.io/ceph/daemon:master-a8d20ed-luminous-ubuntu-16.04-x86_64'\n    ceph_mgr: 'docker.io/ceph/daemon:master-a8d20ed-luminous-ubuntu-16.04-x86_64'\n    ceph_rgw: 'docker.io/ceph/daemon:master-a8d20ed-luminous-ubuntu-16.04-x86_64'\n    ceph_cephfs_provisioner: 'quay.io/external_storage/cephfs-provisioner:v0.1.2'\n    ceph_rbd_provisioner: 'quay.io/external_storage/rbd-provisioner:v0.1.1'\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Quorum Status in Kubernetes\nDESCRIPTION: This command retrieves the quorum status of the Ceph cluster in a Kubernetes deployment. It executes within a Ceph monitor pod and outputs the result in JSON format.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_24\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph quorum_status -f json-pretty\n```\n\n----------------------------------------\n\nTITLE: Automated Environment Test Script Items\nDESCRIPTION: A list of sequential operations that the automated test scripts will perform to validate the OpenStack deployment functionality. These operations include network creation, VM deployment, and connectivity testing.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/developer-environment.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n* Create external network\n* Setup access to the external network from the development machine\n* Create tenant network\n* Create tenant router to link tenant network and external\n* Create SSH Key in nova\n* Create VM on tenant network\n* Assign Floating IP to VM\n* SSH into VM and check it can access the internet\n```\n\n----------------------------------------\n\nTITLE: Labeling Kubernetes Nodes for MariaDB Deployment\nDESCRIPTION: Command to label Kubernetes nodes that should receive MariaDB instances. This label is required for the StatefulSet to schedule MariaDB pods on appropriate control plane nodes.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/prometheus-mysql-exporter/README.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl label nodes openstack-control-plane=enabled --all\n```\n\n----------------------------------------\n\nTITLE: OVS Component Configuration\nDESCRIPTION: YAML configuration for OpenVSwitch components, showing how to disable the default Neutron L2 agent while keeping OVS infrastructure enabled.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/neutron-multiple-sdns.rst#2025-04-20_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmanifests:\n  # Neutron L2 agent:\n  daemonset_ovs_agent: false\n  # OVS tool:\n  daemonset_ovs_db: true\n  daemonset_ovs_vswitchd: true\n```\n\n----------------------------------------\n\nTITLE: Listing Ceph Pods Status in Kubernetes\nDESCRIPTION: This console command shows the status of all pods in the Ceph namespace, revealing which pods are running normally and which ones are in Unknown/NodeLost state due to the failure of mnode3 node.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl get pods -n ceph --show-all=false -o wide\nNAME                                       READY     STATUS     RESTARTS   AGE       IP               NODE\nceph-mds-6f66956547-57tf9                  1/1       Running    0          1m        192.168.0.207    mnode1\nceph-mds-6f66956547-5x4ng                  1/1       Running    0          1h        192.168.4.14     mnode2\nceph-mds-6f66956547-c25cx                  1/1       Unknown    0          1h        192.168.3.14     mnode3\nceph-mgr-5746dd89db-9dbmv                  1/1       Unknown    0          1h        192.168.10.248   mnode3\nceph-mgr-5746dd89db-d5fcw                  1/1       Running    0          1m        192.168.10.246   mnode1\nceph-mgr-5746dd89db-qq4nl                  1/1       Running    0          1h        192.168.10.247   mnode2\nceph-mon-5qn68                             1/1       NodeLost   0          1h        192.168.10.248   mnode3\nceph-mon-check-d85994946-4g5xc             1/1       Running    0          1h        192.168.4.8      mnode2\nceph-mon-mwkj9                             1/1       Running    0          1h        192.168.10.247   mnode2\nceph-mon-ql9zp                             1/1       Running    0          1h        192.168.10.246   mnode1\nceph-osd-default-83945928-c7gdd            1/1       NodeLost   0          1h        192.168.10.248   mnode3\nceph-osd-default-83945928-s6gs6            1/1       Running    0          1h        192.168.10.246   mnode1\nceph-osd-default-83945928-vsc5b            1/1       Running    0          1h        192.168.10.247   mnode2\nceph-rbd-provisioner-5bfb577ffd-j6hlx      1/1       Running    0          1h        192.168.4.16     mnode2\nceph-rbd-provisioner-5bfb577ffd-kdmrv      1/1       Running    0          1m        192.168.0.209    mnode1\nceph-rbd-provisioner-5bfb577ffd-zdx2d      1/1       Unknown    0          1h        192.168.3.16     mnode3\nceph-rgw-6c64b444d7-4qgkw                  1/1       Running    0          1m        192.168.0.210    mnode1\nceph-rgw-6c64b444d7-7bgqs                  1/1       Unknown    0          1h        192.168.3.12     mnode3\nceph-rgw-6c64b444d7-hv6vn                  1/1       Running    0          1h        192.168.4.13     mnode2\ningress-796d8cf8d6-4txkq                   1/1       Running    0          1h        192.168.2.6      mnode5\ningress-796d8cf8d6-9t7m8                   1/1       Running    0          1h        192.168.5.4      mnode4\ningress-error-pages-54454dc79b-hhb4f       1/1       Running    0          1h        192.168.2.5      mnode5\ningress-error-pages-54454dc79b-twpgc       1/1       Running    0          1h        192.168.4.4      mnode2\n```\n\n----------------------------------------\n\nTITLE: Cinder Service Configuration\nDESCRIPTION: YAML configuration for Cinder deployment specifying backup and Ceph client settings\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_32\n\nLANGUAGE: yaml\nCODE:\n```\nbackup:\n  posix:\n    volume:\n      class_name: rbd-tenant\nceph_client:\n  configmap: tenant-ceph-etc\n  user_secret_name: pvc-tenant-ceph-client-key\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Status After Node Recovery\nDESCRIPTION: Displays the Ceph cluster status after the rebooted node has returned to service, showing all monitors in quorum and all OSDs up.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n    health: HEALTH_WARN\n            too few PGs per OSD (22 < min 30)\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager1(active), standbys: voyager3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:standby\n    osd: 24 osds: 24 up, 24 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 208 objects, 3359 bytes\n    usage:   2635 MB used, 44675 GB / 44678 GB avail\n    pgs:     182 active+clean\n```\n\n----------------------------------------\n\nTITLE: Specifying Ceph Docker Images for Installation\nDESCRIPTION: Docker image references for Ceph Luminous point releases that will be used during the upgrade process. These images are specific versions tagged for Ceph 12.2.4 and 12.2.5 on Ubuntu 16.04.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nCeph 12.2.4: ceph/daemon:master-0351083-luminous-ubuntu-16.04-x86_64\nCeph 12.2.5: ceph/daemon:master-a8d20ed-luminous-ubuntu-16.04-x86_64\n```\n\n----------------------------------------\n\nTITLE: Configuring Neutron Core and Plugin Options in YAML\nDESCRIPTION: YAML configuration in values.yaml for setting Neutron's core_plugin, service_plugins, and mechanism_drivers. This shows how the INI file settings are represented in the Helm chart's values file.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  neutron:\n    DEFAULT:\n        ...\n      # core_plugin can be: ml2, calico\n      core_plugin: ml2\n      # service_plugin can be: router, odl-router, empty for calico,\n      # networking_ovn.l3.l3_ovn.OVNL3RouterPlugin for OVN\n      service_plugins: router\n\n  plugins:\n    ml2_conf:\n      ml2:\n        # mechnism_drivers can be: openvswitch, linuxbridge,\n        # opendaylight, ovn\n        mechanism_drivers: openvswitch,l2population\n        type_drivers: flat,vlan,vxlan\n```\n\n----------------------------------------\n\nTITLE: Implementing Node/Nodelabel Overrides for Daemonsets in Helm\nDESCRIPTION: A complete example showing how to wrap existing secret and daemonset configurations to support node-specific and node-label-specific overrides using helm-toolkit.utils.daemonset_overrides.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Simplified secret definition needed for node/nodelabel overrides\n# -------------------------------------------------------------------\n# Wrap secret definition\n{{- define \"mychart.secret.etc\" }}\n{{- $secretName := index . 0 }}\n{{- $envAll := index . 1 }}\n# Set to the same env context as was available to the caller, so we can\n# access any env data needed to build the template (e.g., envAll.Values...)\n{{- with $envAll }}\n---\napiVersion: v1\nkind: Secret\n# Note ref to $secretName for dynamically generated secrets\nmetadata:\n  name: {{ $secretName }}\ndata:\n  myConf: {{ include \"helm-toolkit.utils.template\" | b64enc }}\n{{- end }}\n{{- end }}\n\n# Simplified daemonset definition needed for node/nodelabel overrides\n# -------------------------------------------------------------------\n# Wrap daemonset definition\n{{- define \"mychart.daemonset\" }}\n{{- $daemonset := index . 0 }}\n{{- $secretName := index . 1 }}\n{{- $envAll := index . 2 }}\n# Set to the same env context as was available to the caller, so we can\n# access any env data needed to build the template (e.g., envAll.Values...)\n{{- with $envAll }}\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: {{ $daemonset }}\nspec:\n  template:\n    spec:\n      containers:\n      - name: {{ $daemonset }}\n      volumes:\n        # Note refs to $secretName for dynamically generated secrets\n      - name: {{ $secretName }}\n        secret:\n          name: {{ $secretName }}\n          defaultMode: 0444\n{{- end }}\n{{- end }}\n# Desired daemonset name/prefix that helm will register with kubernetes\n# Note that this needs to be a valid dns-1123 name for a k8s resource\n{{- $daemonset := \"mydaemonset\" }}\n# Desired secret name/prefix that helm will register with kubernetes\n# Note that this needs to be a valid dns-1123 name for a k8s resource\n{{- $secretName := \"mychart-etc\" }}\n# Generate the daemonset YAML with a matching/consistent secretName (so\n# daemonset_overrides knows which volumes to dynamically substitute with the\n# auto-generated secrets). You may include in this list any other vars\n# which you need to reference or substitute into the daemonset YAML above.\n{{- $daemonset_yaml := list $secretName . | include \"mychart.daemonset\" | toString | fromYaml }}\n# Namespace to the secret definition which will be used/manipulated\n{{- $secret_include := \"mychart.secret.etc\" }}\n# Pass all these elements to daemonset_overrides to generate secret/daemonset\n# pairings for each set of overrides (plus one with no overrides)\n{{- list $daemonset $daemonset_yaml $secret_include $secretName . | include \"helm-toolkit.utils.daemonset_overrides\" }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nagios Service in YAML\nDESCRIPTION: This YAML snippet shows the configuration for the Nagios service, including log files, object caches, user settings, and various operational parameters.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/nagios.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nagios:\n    nagios:\n      log_file: /opt/nagios/var/log/nagios.log\n      cfg_file:\n        - /opt/nagios/etc/nagios_objects.cfg\n        - /opt/nagios/etc/objects/commands.cfg\n        - /opt/nagios/etc/objects/contacts.cfg\n        - /opt/nagios/etc/objects/timeperiods.cfg\n        - /opt/nagios/etc/objects/templates.cfg\n        - /opt/nagios/etc/objects/prometheus_discovery_objects.cfg\n      object_cache_file: /opt/nagios/var/objects.cache\n      precached_object_file: /opt/nagios/var/objects.precache\n      resource_file: /opt/nagios/etc/resource.cfg\n      status_file: /opt/nagios/var/status.dat\n      status_update_interval: 10\n      nagios_user: nagios\n      nagios_group: nagios\n      check_external_commands: 1\n      command_file: /opt/nagios/var/rw/nagios.cmd\n      lock_file: /var/run/nagios.lock\n      temp_file: /opt/nagios/var/nagios.tmp\n      temp_path: /tmp\n      event_broker_options: -1\n      log_rotation_method: d\n      log_archive_path: /opt/nagios/var/log/archives\n      use_syslog: 1\n      log_service_retries: 1\n      log_host_retries: 1\n      log_event_handlers: 1\n      log_initial_states: 0\n      log_current_states: 1\n      log_external_commands: 1\n      log_passive_checks: 1\n      service_inter_check_delay_method: s\n      max_service_check_spread: 30\n      service_interleave_factor: s\n      host_inter_check_delay_method: s\n      max_host_check_spread: 30\n      max_concurrent_checks: 60\n      check_result_reaper_frequency: 10\n      max_check_result_reaper_time: 30\n      check_result_path: /opt/nagios/var/spool/checkresults\n      max_check_result_file_age: 3600\n      cached_host_check_horizon: 15\n      cached_service_check_horizon: 15\n      enable_predictive_host_dependency_checks: 1\n      enable_predictive_service_dependency_checks: 1\n      soft_state_dependencies: 0\n      auto_reschedule_checks: 0\n      auto_rescheduling_interval: 30\n      auto_rescheduling_window: 180\n      service_check_timeout: 60\n      host_check_timeout: 60\n      event_handler_timeout: 60\n      notification_timeout: 60\n      ocsp_timeout: 5\n      perfdata_timeout: 5\n      retain_state_information: 1\n      state_retention_file: /opt/nagios/var/retention.dat\n      retention_update_interval: 60\n      use_retained_program_state: 1\n      use_retained_scheduling_info: 1\n      retained_host_attribute_mask: 0\n      retained_service_attribute_mask: 0\n      retained_process_host_attribute_mask: 0\n      retained_process_service_attribute_mask: 0\n      retained_contact_host_attribute_mask: 0\n      retained_contact_service_attribute_mask: 0\n      interval_length: 1\n      check_workers: 4\n      check_for_updates: 1\n      bare_update_check: 0\n      use_aggressive_host_checking: 0\n      execute_service_checks: 1\n      accept_passive_service_checks: 1\n      execute_host_checks: 1\n      accept_passive_host_checks: 1\n      enable_notifications: 1\n      enable_event_handlers: 1\n      process_performance_data: 0\n      obsess_over_services: 0\n      obsess_over_hosts: 0\n      translate_passive_host_checks: 0\n      passive_host_checks_are_soft: 0\n      check_for_orphaned_services: 1\n      check_for_orphaned_hosts: 1\n      check_service_freshness: 1\n      service_freshness_check_interval: 60\n      check_host_freshness: 0\n      host_freshness_check_interval: 60\n      additional_freshness_latency: 15\n      enable_flap_detection: 1\n      low_service_flap_threshold: 5.0\n      high_service_flap_threshold: 20.0\n      low_host_flap_threshold: 5.0\n      high_host_flap_threshold: 20.0\n      date_format: us\n      use_regexp_matching: 1\n      use_true_regexp_matching: 0\n      daemon_dumps_core: 0\n      use_large_installation_tweaks: 0\n      enable_environment_macros: 0\n      debug_level: 0\n      debug_verbosity: 1\n      debug_file: /opt/nagios/var/nagios.debug\n      max_debug_file_size: 1000000\n      allow_empty_hostgroup_assignment: 1\n      illegal_macro_output_chars: \"`~$&|'<>\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Checking Node Status After Reboot\nDESCRIPTION: Shows the output of the kubectl get nodes command after a node (voyager3) has been rebooted, with its status showing as NotReady.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get nodes\nNAME       STATUS     ROLES     AGE       VERSION\nvoyager1   Ready      master    6d        v1.10.5\nvoyager2   Ready      <none>    6d        v1.10.5\nvoyager3   NotReady   <none>    6d        v1.10.5\nvoyager4   Ready      <none>    6d        v1.10.5\n```\n\n----------------------------------------\n\nTITLE: Killing a Ceph Monitor Process in Console\nDESCRIPTION: Commands to identify and kill a Ceph monitor process on a host machine. This simulates a single monitor failure scenario.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/monitor-failure.rst#2025-04-20_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ ps -ef | grep ceph-mon\nceph     16112 16095  1 14:58 ?        00:00:03 /usr/bin/ceph-mon --cluster ceph --setuser ceph --setgroup ceph -d -i voyager2 --mon-data /var/lib/ceph/mon/ceph-voyager2 --public-addr 135.207.240.42:6789\n$ sudo kill -9 16112\n```\n\n----------------------------------------\n\nTITLE: Configuring Ceph Deployment with YAML Overrides\nDESCRIPTION: YAML configuration for the Ceph deployment showing how to override image tags to use specific versions. This configuration sets up Ceph 12.2.4 images for all components and specifies the Ceph network and filesystem settings.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntee /tmp/ceph.yaml << EOF\n  ...\n\tnetwork:\n\t  public: ${CEPH_PUBLIC_NETWORK}\n\t  cluster: ${CEPH_CLUSTER_NETWORK}\n  images:\n    tags:\n      ceph_bootstrap: 'docker.io/ceph/daemon:master-0351083-luminous-ubuntu-16.04-x86_64'\n      ceph_config_helper: 'docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal'\n      ceph_rbd_pool: 'docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal'\n      ceph_mon_check: 'docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal'\n      ceph_mon: 'docker.io/ceph/daemon:master-0351083-luminous-ubuntu-16.04-x86_64'\n      ceph_osd: 'docker.io/ceph/daemon:master-0351083-luminous-ubuntu-16.04-x86_64'\n      ceph_mds: 'docker.io/ceph/daemon:master-0351083-luminous-ubuntu-16.04-x86_64'\n      ceph_mgr: 'docker.io/ceph/daemon:master-0351083-luminous-ubuntu-16.04-x86_64'\n      ceph_rgw: 'docker.io/ceph/daemon:master-0351083-luminous-ubuntu-16.04-x86_64'\n      ceph_cephfs_provisioner: 'quay.io/external_storage/cephfs-provisioner:v0.1.1'\n      ceph_rbd_provisioner: 'quay.io/external_storage/rbd-provisioner:v0.1.0'\n\tconf:\n\t  ceph:\n\t    global:\n\t      fsid: ${CEPH_FS_ID}\n\t  rgw_ks:\n\t    enabled: true\n\t  pool:\n\t    crush:\n\t      tunables: ${CRUSH_TUNABLES}\n\t    target:\n        # NOTE(portdirect): 5 nodes, with one osd per node\n\t      osd: 5\n\t      pg_per_osd: 100\n  ...\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Curator in values.yaml\nDESCRIPTION: This snippet shows the structure for configuring the Elastic Curator service in the values.yaml file. It includes connection settings for Elasticsearch such as hosts, port, SSL options, and authentication.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/elasticsearch.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconf:\n  curator:\n    config:\n      client:\n        hosts:\n          - elasticsearch-logging\n        port: 9200\n        url_prefix:\n        use_ssl: False\n        certificate:\n        client_cert:\n        client_key:\n        ssl_no_validate: False\n        http_auth:\n        timeout: 30\n        master_only: False\n      logging:\n        loglevel: INFO\n        logfile:\n        logformat: default\n        blacklist: ['elasticsearch', 'urllib3']\n```\n\n----------------------------------------\n\nTITLE: Using Endpoint Lookup in Go Templates\nDESCRIPTION: This snippet shows how to use the helm-toolkit.endpoints.keystone_endpoint_uri_lookup macro in a Go template to dynamically generate the glance_api_servers configuration for Cinder. It demonstrates the usage of tuple and include functions to pass parameters to the macro.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/endpoints.rst#2025-04-20_snippet_1\n\nLANGUAGE: go-template\nCODE:\n```\n{{- if empty .Values.conf.cinder.DEFAULT.glance_api_servers -}}\n{{- $_ := tuple \"image\" \"internal\" \"api\" . | include \"helm-toolkit.endpoints.keystone_endpoint_uri_lookup\"| set .Values.conf.cinder.DEFAULT \"glance_api_servers\" -}}\n{{- end -}}\n```\n\n----------------------------------------\n\nTITLE: Fluentd Exporter Configuration\nDESCRIPTION: Configuration for Fluentd metrics exporter, defining logging format and level for Prometheus monitoring.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/fluent-logging.rst#2025-04-20_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  fluentd_exporter:\n    log:\n      format: \"logger:stdout?json=true\"\n      level: \"info\"\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenStack Client Environment\nDESCRIPTION: Instructions for installing OpenStack client in a Python virtual environment and configuring client credentials.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv ~/openstack-client\nsource ~/openstack-client/bin/activate\npip install python-openstackclient\n```\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p ~/.config/openstack\ntee ~/.config/openstack/clouds.yaml << EOF\nclouds:\n  openstack_helm:\n    region_name: RegionOne\n    identity_api_version: 3\n    auth:\n      username: 'admin'\n      password: 'password'\n      project_name: 'admin'\n      project_domain_name: 'default'\n      user_domain_name: 'default'\n      auth_url: 'http://keystone.openstack.svc.cluster.local/v3'\n```\n\nLANGUAGE: bash\nCODE:\n```\nopenstack --os-cloud openstack_helm endpoint list\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --rm --network host \\\n    -v ~/.config/openstack/clouds.yaml:/etc/openstack/clouds.yaml \\\n    -e OS_CLOUD=openstack_helm \\\n    quay.io/airshipit/openstack-client:${OPENSTACK_RELEASE}-ubuntu_jammy \\\n    openstack endpoint list\n```\n\n----------------------------------------\n\nTITLE: Removing a Down OSD from Ceph Cluster in Kubernetes\nDESCRIPTION: Command to purge a down OSD (osd.1) from the Ceph cluster. The --yes-i-really-mean-it flag confirms the operation which removes the OSD permanently from the cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_27\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph osd purge osd.1 --yes-i-really-mean-it\npurged osd.1\n```\n\n----------------------------------------\n\nTITLE: Checking RBD Provisioner Pod Image Version\nDESCRIPTION: Command to check which Docker image is being used by the RBD provisioner pod. The output confirms it's using quay.io/external_storage/rbd-provisioner:v0.1.0, which is the initial version before the upgrade.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl describe pod -n ceph ceph-rbd-provisioner-84665cb84f-6s55r\n\nContainers:\n  ceph-rbd-provisioner:\n    Container ID:  docker://383be3d653cecf4cbf0c3c7509774d39dce54102309f1f0bdb07cdc2441e5e47\n    Image:         quay.io/external_storage/rbd-provisioner:v0.1.0\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenStack Endpoints for Tenant Ceph Integration\nDESCRIPTION: Console output showing OpenStack endpoint list after Tenant Ceph deployment. This shows the swift endpoint URLs configured to use the RadosGW service for object storage.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_22\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1: openstack endpoint list\n+----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------------------------------------------------------+\n| ID                               | Region    | Service Name | Service Type | Enabled | Interface | URL                                                                         |\n+----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------------------------------------------------------+\n| 265212a5856e4a0aba8eb294508279c7 | RegionOne | swift        | object-store | True    | admin     | http://ceph-rgw.openstack.svc.cluster.local:8088/swift/v1/KEY_$(tenant_id)s |\n| 430174e280444598b676d503c5ed9799 | RegionOne | swift        | object-store | True    | internal  | http://ceph-rgw.openstack.svc.cluster.local:8088/swift/v1/KEY_$(tenant_id)s |\n| 480cc7360752498e822cbbc7211d213a | RegionOne | keystone     | identity     | True    | internal  | http://keystone-api.openstack.svc.cluster.local:5000/v3                     |\n| 8dfe4e4725b84e51a5eda564dee0960c | RegionOne | keystone     | identity     | True    | public    | http://keystone.openstack.svc.cluster.local:80/v3                           |\n| 948552a0d90940f7944f8c2eba7ef462 | RegionOne | swift        | object-store | True    | public    | http://radosgw.openstack.svc.cluster.local:80/swift/v1/KEY_$(tenant_id)s    |\n| 9b3526e36307400b9accfc7cc834cf99 | RegionOne | keystone     | identity     | True    | admin     | http://keystone.openstack.svc.cluster.local:80/v3                           |\n+----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Rules Structure in YAML\nDESCRIPTION: YAML configuration structure showing the available rule categories for Prometheus monitoring in OpenStack-Helm. Includes sections for various infrastructure components like alertmanager, etcd3, kubernetes, and OpenStack services.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/prometheus.rst#2025-04-20_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nvalues:\n  conf:\n    rules:\n      alertmanager:\n      etcd3:\n      kube_apiserver:\n      kube_controller_manager:\n      kubelet:\n      kubernetes:\n      rabbitmq:\n      mysql:\n      ceph:\n      openstack:\n      custom:\n```\n\n----------------------------------------\n\nTITLE: Examining Ceph Quorum Status JSON Output\nDESCRIPTION: JSON output showing the Ceph quorum status with details about the election epoch, quorum members, quorum leader, and monitor map configuration.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"election_epoch\": 106,\n    \"quorum\": [\n        0,\n        1,\n        2\n    ],\n    \"quorum_names\": [\n        \"mnode1\",\n        \"mnode2\",\n        \"mnode4\"\n    ],\n    \"quorum_leader_name\": \"mnode1\",\n    \"monmap\": {\n        \"epoch\": 3,\n        \"fsid\": \"54d9af7e-da6d-4980-9075-96bb145db65c\",\n        \"modified\": \"2018-08-14 22:55:41.256612\",\n        \"created\": \"2018-08-14 21:02:24.330403\",\n        \"features\": {\n            \"persistent\": [\n                \"kraken\",\n                \"luminous\"\n            ],\n            \"optional\": []\n        },\n        \"mons\": [\n            {\n                \"rank\": 0,\n                \"name\": \"mnode1\",\n                \"addr\": \"192.168.10.246:6789/0\",\n                \"public_addr\": \"192.168.10.246:6789/0\"\n            },\n            {\n                \"rank\": 1,\n                \"name\": \"mnode2\",\n                \"addr\": \"192.168.10.247:6789/0\",\n                \"public_addr\": \"192.168.10.247:6789/0\"\n            },\n            {\n                \"rank\": 2,\n                \"name\": \"mnode4\",\n                \"addr\": \"192.168.10.249:6789/0\",\n                \"public_addr\": \"192.168.10.249:6789/0\"\n            }\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Keystone Service\nDESCRIPTION: Installs OpenStack Keystone identity service using Helm chart with specified configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install keystone openstack-helm/keystone \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c keystone ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Listing OpenStack Endpoints\nDESCRIPTION: Displays the configured OpenStack service endpoints including their regions, service types, and URLs.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ openstack endpoint list\n+----------------------------------+-----------+--------------+--------------+---------+-----------+---------------------------------------------------------+\n| ID                               | Region    | Service Name | Service Type | Enabled | Interface | URL                                                     |\n+----------------------------------+-----------+--------------+--------------+---------+-----------+---------------------------------------------------------+\n| 480cc7360752498e822cbbc7211d213a | RegionOne | keystone     | identity     | True    | internal  | http://keystone-api.openstack.svc.cluster.local:5000/v3 |\n| 8dfe4e4725b84e51a5eda564dee0960c | RegionOne | keystone     | identity     | True    | public    | http://keystone.openstack.svc.cluster.local:80/v3       |\n| 9b3526e36307400b9accfc7cc834cf99 | RegionOne | keystone     | identity     | True    | admin     | http://keystone.openstack.svc.cluster.local:80/v3       |\n+----------------------------------+-----------+--------------+--------------+---------+-----------+---------------------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Verifying CRUSH Rule for Rack Failure Domain in Ceph\nDESCRIPTION: This snippet demonstrates how to list and inspect CRUSH rules in a Ceph cluster, specifically looking for a rule that uses rack as the failure domain.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# ceph osd crush rule ls\nrack_replicated_rule\n# ceph osd crush rule dump rack_replicated_rule\n{\n    \"rule_id\": 2,\n    \"rule_name\": \"rack_replicated_rule\",\n    \"ruleset\": 2,\n    \"type\": 1,\n    \"min_size\": 1,\n    \"max_size\": 10,\n    \"steps\": [\n        {\n            \"op\": \"take\",\n            \"item\": -1,\n            \"item_name\": \"default\"\n        },\n        {\n            \"op\": \"chooseleaf_firstn\",\n            \"num\": 0,\n            \"type\": \"rack\"\n        },\n        {\n            \"op\": \"emit\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status After Recovery\nDESCRIPTION: Output of 'ceph status' command showing a healthy cluster (HEALTH_OK) after rebalancing is complete. All placement groups are in active+clean state with no warnings or degraded objects. The output provides details about services, data distribution, and I/O operations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n  # ceph status\n    cluster:\n      id:     07c31d0f-bcc6-4db4-aadf-2d2a0f13edb8\n      health: HEALTH_OK\n\n    services:\n      mon: 5 daemons, quorum host1,host2,host3,host4,host5\n      mgr: host6(active), standbys: host1\n      mds: cephfs-1/1/1 up  {0=mds-ceph-mds-7cc55c9695-lj22d=up:active}, 1 up:standby\n      osd: 72 osds: 72 up, 72 in\n      rgw: 2 daemons active\n\n    data:\n      pools:   19 pools, 2920 pgs\n      objects: 134k objects, 519 GB\n      usage:   1933 GB used, 78494 GB / 80428 GB avail\n      pgs:     2920 active+clean\n\n    io:\n      client:   1179 B/s rd, 971 kB/s wr, 1 op/s rd, 41 op/s wr\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Configuration for OpenStack Services in OpenStack-Helm\nDESCRIPTION: Example YAML structure showing how to define configuration values for OpenStack services (Keystone) in OpenStack-Helm. It demonstrates various configuration options including section headings, simple values, multistring options, and list values.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/oslo-config.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  keystone:\n    DEFAULT: # Keys at this level are used for section headings\n      max_token_size: 255\n    token:\n      provider: fernet\n    fernet_tokens:\n      key_repository: /etc/keystone/fernet-keys/\n    credential:\n      key_repository: /etc/keystone/credential-keys/\n    database:\n      max_retries: -1\n    cache:\n      enabled: true\n      backend: dogpile.cache.memcached\n    oslo_messaging_notifications:\n      driver: # An example of a multistring option's syntax\n        type: multistring\n        values:\n          - messagingv2\n          - log\n    security_compliance:\n      password_expires_ignore_user_ids:\n      # Values in a list will be converted to a comma separated key\n        - \"123\"\n        - \"456\"\n```\n\n----------------------------------------\n\nTITLE: Ceph Monitor Status JSON Output\nDESCRIPTION: The JSON output of the Ceph monitor status command showing detailed information about the quorum status. It indicates that monitors on mnode1, mnode2, and mnode4 are in the quorum (ranks 0, 1, and 3), while mnode3 (rank 2) is out of the quorum.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"mnode2\",\n    \"rank\": 1,\n    \"state\": \"peon\",\n    \"election_epoch\": 100,\n    \"quorum\": [\n        0,\n        1,\n        3\n    ],\n    \"features\": {\n        \"required_con\": \"153140804152475648\",\n        \"required_mon\": [\n            \"kraken\",\n            \"luminous\"\n        ],\n        \"quorum_con\": \"2305244844532236283\",\n        \"quorum_mon\": [\n            \"kraken\",\n            \"luminous\"\n        ]\n    },\n    \"outside_quorum\": [],\n    \"extra_probe_peers\": [\n        \"192.168.10.249:6789/0\"\n    ],\n    \"sync_provider\": [],\n    \"monmap\": {\n        \"epoch\": 2,\n        \"fsid\": \"54d9af7e-da6d-4980-9075-96bb145db65c\",\n        \"modified\": \"2018-08-14 22:43:31.517568\",\n        \"created\": \"2018-08-14 21:02:24.330403\",\n        \"features\": {\n            \"persistent\": [\n                \"kraken\",\n                \"luminous\"\n            ],\n            \"optional\": []\n        },\n        \"mons\": [\n            {\n                \"rank\": 0,\n                \"name\": \"mnode1\",\n                \"addr\": \"192.168.10.246:6789/0\",\n                \"public_addr\": \"192.168.10.246:6789/0\"\n            },\n            {\n                \"rank\": 1,\n                \"name\": \"mnode2\",\n                \"addr\": \"192.168.10.247:6789/0\",\n                \"public_addr\": \"192.168.10.247:6789/0\"\n            },\n            {\n                \"rank\": 2,\n                \"name\": \"mnode3\",\n                \"addr\": \"192.168.10.248:6789/0\",\n                \"public_addr\": \"192.168.10.248:6789/0\"\n            },\n            {\n                \"rank\": 3,\n                \"name\": \"mnode4\",\n                \"addr\": \"192.168.10.249:6789/0\",\n                \"public_addr\": \"192.168.10.249:6789/0\"\n            }\n        ]\n    },\n    \"feature_map\": {\n        \"mon\": {\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        },\n        \"mds\": {\n\n```\n\n----------------------------------------\n\nTITLE: Deploying Cinder Service\nDESCRIPTION: Installs OpenStack Cinder block storage service using Helm chart with specified configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install cinder openstack-helm/cinder \\\n    --namespace=openstack \\\n    --timeout=600s \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c cinder ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Deleting Ceph Namespace in Kubernetes\nDESCRIPTION: This snippet shows how to delete the Ceph namespace in Kubernetes. It includes steps to store the Ceph filesystem UUID and verify that all resources in the namespace are removed.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/namespace-deletion.rst#2025-04-20_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nCEPH_NAMESPACE=\"ceph\"\nMON_POD=$(kubectl get pods --namespace=${CEPH_NAMESPACE} \\\n--selector=\"application=ceph\" --selector=\"component=mon\" \\\n--no-headers | awk '{ print $1; exit }')\n\nkubectl exec --namespace=${CEPH_NAMESPACE} ${MON_POD} -- ceph status \\\n| awk '/id:/{print $2}' | tee /tmp/ceph-fs-uuid.txt\n```\n\nLANGUAGE: console\nCODE:\n```\nkubectl delete namespace ${CEPH_NAMESPACE}\n```\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods --namespace ${CEPH_NAMESPACE} -o wide\nNo resources found.\n\nkubectl get secrets --namespace ${CEPH_NAMESPACE}\nNo resources found.\n```\n\n----------------------------------------\n\nTITLE: Configuring Ceph with Rack-Level Failure Domain in YAML\nDESCRIPTION: YAML configuration example for setting up rack-level failure domains in Ceph. This demonstrates how to specify failure domains, network parameters, and storage configuration in OpenStack Helm deployments.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  identity:\n    namespace: openstack\n  object_store:\n    namespace: ceph\n  ceph_mon:\n    namespace: ceph\nnetwork:\n  public: 10.0.0.0/24\n  cluster: 10.0.0.0/24\ndeployment:\n  storage_secrets: true\n  ceph: true\n  csi_rbd_provisioner: true\n  rbd_provisioner: false\n  cephfs_provisioner: false\n  client_secrets: false\n  rgw_keystone_user_and_endpoints: false\nbootstrap:\n  enabled: true\nconf:\n  ceph:\n    global:\n      fsid: 6c12a986-148d-45a7-9120-0cf0522ca5e0\n  rgw_ks:\n    enabled: true\n  pool:\n    default:\n      crush_rule: rack_replicated_rule\n    crush:\n      tunables: null\n    target:\n      # NOTE(portdirect): 5 nodes, with one osd per node\n      osd: 18\n      pg_per_osd: 100\n  storage:\n    osd:\n      - data:\n          type: block-logical\n          location: /dev/vdb\n        journal:\n          type: block-logical\n          location: /dev/vde1\n      - data:\n          type: block-logical\n          location: /dev/vdc\n        journal:\n          type: block-logical\n          location: /dev/vde2\n      - data:\n          type: block-logical\n          location: /dev/vdd\n        journal:\n          type: block-logical\n          location: /dev/vde3\n  overrides:\n    ceph_osd:\n      hosts:\n        - name: osh-1\n          conf:\n            storage:\n              failure_domain: \"rack\"\n              failure_domain_name: \"rack1\"\n        - name: osh-2\n          conf:\n            storage:\n              failure_domain: \"rack\"\n              failure_domain_name: \"rack1\"\n        - name: osh-3\n          conf:\n            storage:\n              failure_domain: \"rack\"\n              failure_domain_name: \"rack2\"\n        - name: osh-4\n          conf:\n            storage:\n              failure_domain: \"rack\"\n              failure_domain_name: \"rack2\"\n        - name: osh-5\n          conf:\n            storage:\n              failure_domain: \"rack\"\n              failure_domain_name: \"rack3\"\n        - name: osh-6\n          conf:\n            storage:\n              failure_domain: \"rack\"\n              failure_domain_name: \"rack3\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Neutron Services\nDESCRIPTION: Configuration to disable default Neutron services when using custom SDN implementations that provide their own L3, DHCP, and metadata services.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/neutron-multiple-sdns.rst#2025-04-20_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmanifests:\n  daemonset_dhcp_agent: false\n  daemonset_l3_agent: false\n  daemonset_metadata_agent: false\n```\n\n----------------------------------------\n\nTITLE: Implementing Helm-Toolkit Registry Secret Function\nDESCRIPTION: Definition of a new Helm-Toolkit function that creates a Kubernetes secret containing Docker registry credentials formatted as a dockerconfigjson. This generates the appropriate authentication token and registry URL.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/support-OCI-image-registry-with-authentication-turned-on.rst#2025-04-20_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n{{- define \"helm-toolkit.manifests.secret_registry\" -}}\n{{- $envAll := index . \"envAll\" }}\n{{- $registryUser := index . \"registryUser\" }}\n{{- $secretName := index $envAll.Values.secrets.oci_image_registry $registryUser }}\n{{- $registryHost := tuple \"oci_image_registry\" \"internal\" $envAll | include \"helm-toolkit.endpoints.endpoint_host_lookup\" }}\n{{- $registryPort := tuple \"oci_image_registry\" \"internal\" \"registry\" $envAll | include \"helm-toolkit.endpoints.endpoint_port_lookup\" }}\n{{- $imageCredentials := index $envAll.Values.endpoints.oci_image_registry.auth $registryUser }}\n{{- $dockerAuthToken := printf \"%s:%s\" $imageCredentials.username $imageCredentials.password | b64enc }}\n{{- $dockerAuth := printf \"{\\\"auths\\\": {\\\"%s:%s\\\": {\\\"auth\\\": \\\"%s\\\"}}}\" $registryHost $registryPort $dockerAuthToken | b64enc }}\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ $secretName }}\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: {{ $dockerAuth }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Status After Node Reboot\nDESCRIPTION: Displays the Ceph cluster status after a node running ceph-mon has been rebooted, showing degraded state with the monitor out of quorum and OSDs down.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n    health: HEALTH_WARN\n            6 osds down\n            1 host (6 osds) down\n            Degraded data redundancy: 195/624 objects degraded (31.250%), 8 pgs degraded\n            too few PGs per OSD (17 < min 30)\n            mon voyager1 is low on available space\n            1/3 mons down, quorum voyager1,voyager2\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2, out of quorum: voyager3\n    mgr: voyager1(active), standbys: voyager3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:standby\n    osd: 24 osds: 18 up, 24 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 208 objects, 3359 bytes\n    usage:   2630 MB used, 44675 GB / 44678 GB avail\n    pgs:     195/624 objects degraded (31.250%)\n             126 active+undersized\n             48  active+clean\n             8   active+undersized+degraded\n```\n\n----------------------------------------\n\nTITLE: Generated ConfigMap with Oslo Config in Kubernetes\nDESCRIPTION: The resulting Kubernetes ConfigMap with oslo-config formatted configuration after the YAML values are processed. This shows how the YAML structure is transformed into the standard oslo.conf format with sections and key-value pairs.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/oslo-config.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\n# Source: keystone/templates/configmap-etc.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: keystone-etc\ndata:\n  keystone.conf: |\n    [DEFAULT]\n    max_token_size = 255\n    transport_url = rabbit://keystone:password@rabbitmq.default.svc.cluster.local:5672/openstack\n    [cache]\n    backend = dogpile.cache.memcached\n    enabled = true\n    memcache_servers = memcached.default.svc.cluster.local:11211\n    [credential]\n    key_repository = /etc/keystone/credential-keys/\n    [database]\n    connection = mysql+pymysql://keystone:password@mariadb.default.svc.cluster.local:3306/keystone\n    max_retries = -1\n    [fernet_tokens]\n    key_repository = /etc/keystone/fernet-keys/\n    [oslo_messaging_notifications]\n    driver = messagingv2\n    driver = log\n    [security_compliance]\n    password_expires_ignore_user_ids = 123,456\n    [token]\n    provider = fernet\n```\n\n----------------------------------------\n\nTITLE: Starting New OSD Pod\nDESCRIPTION: This command removes the maintenance label from the node, allowing a new OSD pod to start.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/disk-failure.rst#2025-04-20_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl label nodes voyager4 --overwrite ceph_maintenance_window=inactive\n```\n\n----------------------------------------\n\nTITLE: Monitoring Ceph Data Movement Progress\nDESCRIPTION: Command to monitor the progress of data movement during Ceph rebalancing after CRUSH map changes. This shows how to watch the cluster status in real-time.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n# watch ceph status\n  cluster:\n    id:     07c31d0f-bcc6-4db4-aadf-2d2a0f13edb8\n```\n\n----------------------------------------\n\nTITLE: Viewing OpenStack Services and Endpoints\nDESCRIPTION: Console output showing configured OpenStack services and their endpoints including Swift, Keystone, Glance and Cinder\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_33\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1: openstack service list\n+----------------------------------+----------+--------------+\n| ID                               | Name     | Type         |\n+----------------------------------+----------+--------------+\n| 0eddeb6af4fd43ea8f73f63a1ae01438 | swift    | object-store |\n| 66bd0179eada4ab8899a58356fd4d508 | cinder   | volume       |\n| 67cc6b945e934246b25d31a9374a64af | keystone | identity     |\n| 81a61ec8eff74070bb3c2f0118c1bcd5 | glance   | image        |\n| c126046fc5ec4c52acfc8fee0e2f4dda | cinderv2 | volumev2     |\n| f89b99a31a124b7790e3bb60387380b1 | cinderv3 | volumev3     |\n+----------------------------------+----------+--------------+\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Pod Annotations in Helm Templates\nDESCRIPTION: A Helm toolkit snippet that defines Prometheus pod annotations for monitoring. It configures annotations for scrape configuration, path, and port based on provided configuration values.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/prometheus.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{{- define \"helm-toolkit.snippets.prometheus_pod_annotations\" -}}\n{{- $config := index . 0 -}}\n{{- if $config.scrape }}\nprometheus.io/scrape: {{ $config.scrape | quote }}\n{{- end }}\n{{- if $config.path }}\nprometheus.io/path: {{ $config.path | quote }}\n{{- end }}\n{{- if $config.port }}\nprometheus.io/port: {{ $config.port | quote }}\n{{- end }}\n{{- end -}}\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Status After Monitor Database Destruction\nDESCRIPTION: Command to check Ceph cluster status after intentionally destroying a monitor database. Shows the cluster health and quorum status.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/monitor-failure.rst#2025-04-20_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n    health: HEALTH_WARN\n            too few PGs per OSD (22 < min 30)\n            mon voyager1 is low on available space\n            1/3 mons down, quorum voyager1,voyager2\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2, out of quorum: voyager3\n    mgr: voyager1(active), standbys: voyager3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:standby\n    osd: 24 osds: 24 up, 24 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 240 objects, 3359 bytes\n    usage:   2675 MB used, 44675 GB / 44678 GB avail\n    pgs:     182 active+clean\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenvSwitch Service\nDESCRIPTION: Installs OpenvSwitch networking service using Helm chart with specified configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install openvswitch openstack-helm/openvswitch \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c openvswitch ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Ceph OSD Update Status - Console Output\nDESCRIPTION: Console output showing the rolling update process of Ceph OSD pods during the upgrade.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nceph-osd-default-83945928-7jz4s            0/1       Terminating   0          2h\nceph-osd-default-83945928-bh82j            1/1       Running       0          2h\nceph-osd-default-83945928-t9szk            1/1       Running       0          2h\nceph-osd-keyring-generator-6rg65           0/1       Completed     0          2h\n```\n\nLANGUAGE: console\nCODE:\n```\nceph-osd-default-83945928-l84tl            1/1       Running     0          9m\nceph-osd-default-83945928-twzmk            1/1       Running     0          6m\nceph-osd-default-83945928-wxpmh            1/1       Running     0          11m\nceph-osd-keyring-generator-6rg65           0/1       Completed   0          2h\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Status After OSD Removal\nDESCRIPTION: This command checks the Ceph cluster status after removing the failed OSD, showing 23 OSDs total.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/disk-failure.rst#2025-04-20_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n    health: HEALTH_WARN\n            too few PGs per OSD (23 < min 30)\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager1(active), standbys: voyager3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:standby\n    osd: 23 osds: 23 up, 23 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 240 objects, 3359 bytes\n    usage:   2551 MB used, 42814 GB / 42816 GB avail\n    pgs:     182 active+clean\n```\n\n----------------------------------------\n\nTITLE: Examining Ceph Monitor Status JSON Output\nDESCRIPTION: Detailed JSON output showing the Ceph monitor status, including the monitor rank, state, quorum information, features, and the monitor map with node addresses.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"mnode4\",\n    \"rank\": 2,\n    \"state\": \"peon\",\n    \"election_epoch\": 106,\n    \"quorum\": [\n        0,\n        1,\n        2\n    ],\n    \"features\": {\n        \"required_con\": \"153140804152475648\",\n        \"required_mon\": [\n            \"kraken\",\n            \"luminous\"\n        ],\n        \"quorum_con\": \"2305244844532236283\",\n        \"quorum_mon\": [\n            \"kraken\",\n            \"luminous\"\n        ]\n    },\n    \"outside_quorum\": [],\n    \"extra_probe_peers\": [],\n    \"sync_provider\": [],\n    \"monmap\": {\n        \"epoch\": 3,\n        \"fsid\": \"54d9af7e-da6d-4980-9075-96bb145db65c\",\n        \"modified\": \"2018-08-14 22:55:41.256612\",\n        \"created\": \"2018-08-14 21:02:24.330403\",\n        \"features\": {\n            \"persistent\": [\n                \"kraken\",\n                \"luminous\"\n            ],\n            \"optional\": []\n        },\n        \"mons\": [\n            {\n                \"rank\": 0,\n                \"name\": \"mnode1\",\n                \"addr\": \"192.168.10.246:6789/0\",\n                \"public_addr\": \"192.168.10.246:6789/0\"\n            },\n            {\n                \"rank\": 1,\n                \"name\": \"mnode2\",\n                \"addr\": \"192.168.10.247:6789/0\",\n                \"public_addr\": \"192.168.10.247:6789/0\"\n            },\n            {\n                \"rank\": 2,\n                \"name\": \"mnode4\",\n                \"addr\": \"192.168.10.249:6789/0\",\n                \"public_addr\": \"192.168.10.249:6789/0\"\n            }\n        ]\n    },\n    \"feature_map\": {\n        \"mon\": {\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        },\n        \"client\": {\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Rolling Update Settings in values.yaml\nDESCRIPTION: This snippet from values.yaml shows the default configuration for rolling updates that is supplied in every chart. These settings define how many pods can be unavailable or created during an update and how many revisions to keep in history.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/upgrades.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npod:\n  lifecycle:\n    upgrades:\n      deployments:\n        revision_history: 3\n        pod_replacement_strategy: RollingUpdate\n        rolling_update:\n          max_unavailable: 1\n          max_surge: 3\n```\n\n----------------------------------------\n\nTITLE: Examining Ceph Quorum Status After Node Failure\nDESCRIPTION: Displays detailed Ceph quorum status in JSON format, showing mnode3 is out of the quorum.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"election_epoch\": 96,\n    \"quorum\": [\n        0,\n        1\n    ],\n    \"quorum_names\": [\n        \"mnode1\",\n        \"mnode2\"\n    ],\n    \"quorum_leader_name\": \"mnode1\",\n    \"monmap\": {\n        \"epoch\": 1,\n        \"fsid\": \"54d9af7e-da6d-4980-9075-96bb145db65c\",\n        \"modified\": \"2018-08-14 21:02:24.330403\",\n        \"created\": \"2018-08-14 21:02:24.330403\",\n        \"features\": {\n            \"persistent\": [\n                \"kraken\",\n                \"luminous\"\n            ],\n            \"optional\": []\n        },\n        \"mons\": [\n            {\n                \"rank\": 0,\n                \"name\": \"mnode1\",\n                \"addr\": \"192.168.10.246:6789/0\",\n                \"public_addr\": \"192.168.10.246:6789/0\"\n            },\n            {\n                \"rank\": 1,\n                \"name\": \"mnode2\",\n                \"addr\": \"192.168.10.247:6789/0\",\n                \"public_addr\": \"192.168.10.247:6789/0\"\n            },\n            {\n                \"rank\": 2,\n                \"name\": \"mnode3\",\n                \"addr\": \"192.168.10.248:6789/0\",\n                \"public_addr\": \"192.168.10.248:6789/0\"\n            }\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Placement Service\nDESCRIPTION: Installs OpenStack Placement resource tracking service using Helm chart.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install placement openstack-helm/placement \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c placement ${FEATURES})\n```\n\n----------------------------------------\n\nTITLE: Initial Ceph Pod Status Check - Console Output\nDESCRIPTION: Console output showing the status of running Ceph pods including RBD provisioners, RGW pods, and ingress components.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nceph-rbd-provisioner-84665cb84f-6s55r      1/1       Running     0          2h\nceph-rbd-provisioner-84665cb84f-chwhd      1/1       Running     0          2h\nceph-rgw-74559877-h56xs                    1/1       Running     0          2h\nceph-rgw-74559877-pfjr5                    1/1       Running     0          2h\nceph-rgw-keyring-generator-6rwct           0/1       Completed   0          2h\nceph-storage-keys-generator-bgj2t          0/1       Completed   0          2h\ningress-796d8cf8d6-nzrd2                   1/1       Running     0          2h\ningress-796d8cf8d6-qqvq9                   1/1       Running     0          2h\ningress-error-pages-54454dc79b-d5r5w       1/1       Running     0          2h\ningress-error-pages-54454dc79b-gfpqv       1/1       Running     0          2h\n```\n\n----------------------------------------\n\nTITLE: Viewing Ceph Monitor Pod Status After Database Destruction\nDESCRIPTION: Command to check the status of Ceph monitor pods after database destruction. Shows the pod states, including the CrashLoopBackOff state for the affected monitor.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/monitor-failure.rst#2025-04-20_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n ceph -o wide|grep ceph-mon\nceph-mon-4gzzw                             1/1       Running            0          6d        135.207.240.42    voyager2\nceph-mon-6bbs6                             0/1       CrashLoopBackOff   5          6d        135.207.240.43    voyager3\nceph-mon-qgc7p                             1/1       Running            0          6d        135.207.240.41    voyager1\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Quorum Status Command\nDESCRIPTION: Command to verify the quorum status of the Ceph cluster in JSON format, which provides information about monitor election state, quorum participation, and leadership.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-5qn68 -- ceph quorum_status -f json-pretty\n```\n\n----------------------------------------\n\nTITLE: Configuring ML2 Plugin in ml2_conf.ini\nDESCRIPTION: Configuration for the ML2 plugin showing type_drivers and mechanism_drivers settings. This specifies which layer 2 technologies (flat, VLAN, VXLAN) and implementation mechanisms (OpenVSwitch, L2 population) are supported.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[ml2]\n# type_drivers - layer 2 technologies that ML2 plugin supports.\n# Those are local,flat,vlan,gre,vxlan,geneve\ntype_drivers = flat,vlan,vxlan\n\n# mech_drivers - implementation of above L2 technologies. This option is\n# pointing to the engines like linux bridge or OpenVSwitch in ref arch.\n# This is the place where SDN implementing ML2 driver should be configured\nmech_drivers = openvswitch, l2population\n```\n\n----------------------------------------\n\nTITLE: Specifying RBD Provisioner Docker Images\nDESCRIPTION: Docker image references for the Ceph RBD provisioner which handles Ceph block storage provisioning in Kubernetes. Shows the version currently in use (v0.1.0) and the target upgrade version (v0.1.1).\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nquay.io/external_storage/rbd-provisioner:v0.1.0\nquay.io/external_storage/rbd-provisioner:v0.1.1\n```\n\n----------------------------------------\n\nTITLE: Installing Node Exporter Chart with Helm\nDESCRIPTION: Bash command to install the prometheus-node-exporter chart in the kube-system namespace. This provides hardware and operating system metrics from Linux kernels.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/prometheus.rst#2025-04-20_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --namespace=kube-system local/prometheus-node-exporter --name=prometheus-node-exporter\n```\n\n----------------------------------------\n\nTITLE: OpenStack Pod Status Check\nDESCRIPTION: Status of all OpenStack component pods showing healthy running state\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_18\n\nLANGUAGE: console\nCODE:\n```\ncinder-api-67495cdffc-24fhs                    1/1       Running   0          1h\ncinder-api-67495cdffc-kz5fn                    1/1       Running   0          1h\ncinder-backup-65b7bd9b79-8n9pb                 1/1       Running   0          1h\ncinder-scheduler-9ddbb7878-rbt4l               1/1       Running   0          1h\ncinder-volume-75bf4cc9bd-6298x                 1/1       Running   0          1h\nglance-api-68f6df4d5d-q84hs                    1/1       Running   0          2h\nglance-api-68f6df4d5d-qbfwb                    1/1       Running   0          2h\ningress-7b4bc84cdd-84dtj                       1/1       Running   0          2h\ningress-7b4bc84cdd-ws45r                       1/1       Running   0          2h\ningress-error-pages-586c7f86d6-dlpm2           1/1       Running   0          2h\ningress-error-pages-586c7f86d6-w7cj2           1/1       Running   0          2h\nkeystone-api-7d9759db58-dz6kt                  1/1       Running   0          2h\nkeystone-api-7d9759db58-pvsc2                  1/1       Running   0          2h\nlibvirt-f7ngc                                  1/1       Running   0          1h\nlibvirt-gtjc7                                  1/1       Running   0          1h\nlibvirt-qmwf5                                  1/1       Running   0          1h\n```\n\n----------------------------------------\n\nTITLE: Recovering Ceph Monitor After Database Destruction\nDESCRIPTION: Commands to recover a Ceph monitor after database destruction. Involves removing the entire ceph-mon directory and checking the cluster status after recovery.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/monitor-failure.rst#2025-04-20_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ sudo rm -rf /var/lib/openstack-helm/ceph/mon/mon/ceph-voyager3\n```\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     9d4d8c61-cf87-4129-9cef-8fbf301210ad\n    health: HEALTH_WARN\n            too few PGs per OSD (22 < min 30)\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager1(active), standbys: voyager3\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-65bb45dffc-cslr6=up:active}, 1 up:standby\n    osd: 24 osds: 24 up, 24 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 240 objects, 3359 bytes\n    usage:   2675 MB used, 44675 GB / 44678 GB avail\n    pgs:     182 active+clean\n```\n\n----------------------------------------\n\nTITLE: Updating Ceph Configuration Post-Deployment\nDESCRIPTION: Bash commands for applying CRUSH map changes to an existing OpenStack Helm deployment. These commands upgrade the Ceph components and recreate the necessary resources to apply the new configuration.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncd /opt/openstack-helm\nhelm upgrade --install ceph-osd ../openstack-helm-infra/ceph-osd --namespace=ceph --values=/tmp/ceph.yaml\nkubectl delete jobs/ceph-rbd-pool -n ceph\nhelm upgrade --install ceph-client ../openstack-helm-infra/ceph-client --namespace=ceph --values=/tmp/ceph.yaml\nhelm delete cinder --purge\nhelm upgrade --install cinder ./cinder --namespace=openstack --values=/tmp/cinder.yaml\n```\n\n----------------------------------------\n\nTITLE: Specifying Overrides in Helm Values.yaml for Daemonsets\nDESCRIPTION: An example of how to define node and node-label overrides in the values.yaml file that will be applied to the daemonset configurations.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  values:\n    conf:\n      mychart:\n        foo: 1\n      # \"overrides\" keyword to invoke override behavior\n      overrides:\n        # To match these overrides to the right daemonset, the following key\n        # needs to follow the pattern:\n        # Chart.Name + '_' + $daemonset\n        # where $daemonset is the value set for $daemonset in the daemonset\n        # config above.\n        mychart_mydaemonset:\n          # labels dict contains a list of labels which overrides apply to. Dict may be excluded\n          # if there are no labels to override.\n          # Note - if a host satisfies more than one label in this list, then whichever matching\n          # label is furtherest down on the list will be the one applied to the node. E.g., if\n          # a host matched both label criteria below, then the overrides for \"another_label\"\n          # would be applied.\n          labels:\n            # node label key and values to match against to apply these config overrides.\n            # The values are ORed, so the daemonset will spawn to all nodes to node_type\n            # set to \"foo\" and to all nodes with node_type set to \"bar\".\n          - label:\n              key: node_type\n              values:\n              - \"foo\"\n              - \"bar\"\n            # The setting overrides that will be applied for hosts with this host label\n            conf:\n              mychart:\n                foo: 2\n            # another label/key to match against to apply different overrides\n          - label:\n              key: another_label\n              values:\n              - \"another_value\"\n            # The setting overrides that will be applied for hosts with this host label\n            conf:\n              mychart:\n                foo: 3\n          # hosts dict contains a list of hosts which overrides apply to. Dict may be excluded\n          # if there are no hosts to override.\n          hosts:\n            # FQDN of the host to override settings on\n          - name: superhost\n            # The setting overrides that will be applied for this host\n            conf:\n              mychart:\n                foo: 4\n            # FQDN of another host to override settings on\n          - name: superhost2\n            # The setting overrides that will be applied for this host\n            conf:\n              mychart:\n                foo: 5\n```\n\n----------------------------------------\n\nTITLE: Specifying CephFS Provisioner Docker Images\nDESCRIPTION: Docker image references for the CephFS provisioner which handles Ceph filesystem storage provisioning in Kubernetes. Shows the version currently in use (v0.1.1) and the target upgrade version (v0.1.2).\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nquay.io/external_storage/cephfs-provisioner:v0.1.1\nquay.io/external_storage/cephfs-provisioner:v0.1.2\n```\n\n----------------------------------------\n\nTITLE: Neutron Manifests Configuration in YAML\nDESCRIPTION: YAML configuration controlling which Kubernetes resources are deployed for Neutron. This allows for selective enabling/disabling of various Neutron components to support different networking solutions.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/networking.rst#2025-04-20_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmanifests:\n  configmap_bin: true\n  configmap_etc: true\n  daemonset_dhcp_agent: true\n  daemonset_l3_agent: true\n  daemonset_lb_agent: false\n  daemonset_metadata_agent: true\n  daemonset_ovs_agent: true\n  daemonset_sriov_agent: true\n  deployment_server: true\n  deployment_rpc_server: true\n  ingress_server: true\n  job_bootstrap: true\n  job_db_init: true\n  job_db_sync: true\n  job_db_drop: false\n  job_image_repo_sync: true\n  job_ks_endpoints: true\n  job_ks_service: true\n  job_ks_user: true\n  job_rabbit_init: true\n  pdb_server: true\n  pod_rally_test: true\n  secret_db: true\n  secret_keystone: true\n  secret_rabbitmq: true\n  service_ingress_server: true\n  service_server: true\n```\n\n----------------------------------------\n\nTITLE: Checking Node Status After mnode3 Shutdown\nDESCRIPTION: Displays the status of all nodes in the cluster after mnode3 was shut down, showing mnode3 as NotReady.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl get nodes\nNAME      STATUS     ROLES     AGE       VERSION\nmnode1    Ready      <none>    1h        v1.10.6\nmnode2    Ready      <none>    1h        v1.10.6\nmnode3    NotReady   <none>    1h        v1.10.6\nmnode4    Ready      <none>    1h        v1.10.6\nmnode5    Ready      <none>    1h        v1.10.6\nmnode6    Ready      <none>    1h        v1.10.6\n```\n\n----------------------------------------\n\nTITLE: Examining Ceph Monitor Status JSON Output\nDESCRIPTION: The JSON output from the Ceph monitor status command, containing detailed information about monitor configuration, election state, quorum membership, and feature compatibility matrix for the Ceph cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"mnode2\",\n    \"rank\": 1,\n    \"state\": \"peon\",\n    \"election_epoch\": 92,\n    \"quorum\": [\n        0,\n        1,\n        2\n    ],\n    \"features\": {\n        \"required_con\": \"153140804152475648\",\n        \"required_mon\": [\n            \"kraken\",\n            \"luminous\"\n        ],\n        \"quorum_con\": \"2305244844532236283\",\n        \"quorum_mon\": [\n            \"kraken\",\n            \"luminous\"\n        ]\n    },\n    \"outside_quorum\": [],\n    \"extra_probe_peers\": [],\n    \"sync_provider\": [],\n    \"monmap\": {\n        \"epoch\": 1,\n        \"fsid\": \"54d9af7e-da6d-4980-9075-96bb145db65c\",\n        \"modified\": \"2018-08-14 21:02:24.330403\",\n        \"created\": \"2018-08-14 21:02:24.330403\",\n        \"features\": {\n            \"persistent\": [\n                \"kraken\",\n                \"luminous\"\n            ],\n            \"optional\": []\n        },\n        \"mons\": [\n            {\n                \"rank\": 0,\n                \"name\": \"mnode1\",\n                \"addr\": \"192.168.10.246:6789/0\",\n                \"public_addr\": \"192.168.10.246:6789/0\"\n            },\n            {\n                \"rank\": 1,\n                \"name\": \"mnode2\",\n                \"addr\": \"192.168.10.247:6789/0\",\n                \"public_addr\": \"192.168.10.247:6789/0\"\n            },\n            {\n                \"rank\": 2,\n                \"name\": \"mnode3\",\n                \"addr\": \"192.168.10.248:6789/0\",\n                \"public_addr\": \"192.168.10.248:6789/0\"\n            }\n        ]\n    },\n    \"feature_map\": {\n        \"mon\": {\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        },\n        \"mds\": {\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        },\n        \"osd\": {\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        },\n        \"client\": {\n            \"group\": {\n                \"features\": \"0x7010fb86aa42ada\",\n                \"release\": \"jewel\",\n                \"num\": 1\n            },\n            \"group\": {\n                \"features\": \"0x1ffddff8eea4fffb\",\n                \"release\": \"luminous\",\n                \"num\": 1\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Examining Ceph Monitor Status in JSON Format\nDESCRIPTION: This command retrieves detailed monitor status information in JSON format, showing the quorum members, cluster features, and monitor map. It confirms that mnode4 has successfully joined the quorum while mnode3 remains outside of it.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_15\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph mon_status -f json-pretty\n```\n\n----------------------------------------\n\nTITLE: Basic Secret and Daemonset YAML Configuration in Helm\nDESCRIPTION: This shows a simplified example of a typical Helm secret and daemonset pair before implementing node/node-label overrides.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Simplified secret definition\n# ===============================\n---\napiVersion: v1\nkind: Secret\n# Note ref to $secretName for dynamically generated secrets\nmetadata:\n  name: mychart-etc\ndata:\n  myConf: {{ include \"helm-toolkit.utils.template\" | b64enc }}\n\n# Simplified daemonset definition\n# ===============================\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: mychart-name\nspec:\n  template:\n    spec:\n      containers:\n      - name: my-container\n      volumes:\n      - name: mychart-etc\n        secret:\n          name: mychart-etc\n          defaultMode: 0444\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status After Node Failure\nDESCRIPTION: Shows the Ceph cluster status after mnode3 failure, indicating health warnings and degraded data redundancy.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph -s\n  cluster:\n    id:     54d9af7e-da6d-4980-9075-96bb145db65c\n    health: HEALTH_WARN\n            insufficient standby MDS daemons available\n            1 osds down\n            1 host (1 osds) down\n            Degraded data redundancy: 354/1062 objects degraded (33.333%), 46 pgs degraded, 101 pgs undersized\n            1/3 mons down, quorum mnode1,mnode2\n\n  services:\n    mon: 3 daemons, quorum mnode1,mnode2, out of quorum: mnode3\n    mgr: mnode2(active)\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-6f66956547-5x4ng=up:active}\n    osd: 3 osds: 2 up, 3 in\n    rgw: 1 daemon active\n\n  data:\n    pools:   19 pools, 101 pgs\n    objects: 354 objects, 260 MB\n    usage:   77845 MB used, 70068 MB / 144 GB avail\n    pgs:     354/1062 objects degraded (33.333%)\n             55 active+undersized\n             46 active+undersized+degraded\n```\n\n----------------------------------------\n\nTITLE: Viewing Helm Test Results in OpenStack-Helm\nDESCRIPTION: Command to view the logs of the pod created by Helm tests. This allows inspection of test output for debugging and verification purposes.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/helm-tests.rst#2025-04-20_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs <test-pod-name> -n <namespace>\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Status After OSD Removal\nDESCRIPTION: Command to verify the Ceph cluster status after removing a down OSD. The output shows HEALTH_OK status with 3 OSDs up and running, confirming successful cluster maintenance.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_28\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl exec -n ceph ceph-mon-ql9zp -- ceph -s\n  cluster:\n    id:     54d9af7e-da6d-4980-9075-96bb145db65c\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum mnode1,mnode2,mnode4\n    mgr: mnode2(active), standbys: mnode1\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-6f66956547-5x4ng=up:active}, 1 up:standby\n    osd: 3 osds: 3 up, 3 in\n    rgw: 2 daemons active\n\n  data:\n    pools:   19 pools, 101 pgs\n    objects: 354 objects, 260 MB\n    usage:   74681 MB used, 73232 MB / 144 GB avail\n    pgs:     101 active+clean\n\n  io:\n    client:   57936 B/s wr, 0 op/s rd, 14 op/s wr\n```\n\n----------------------------------------\n\nTITLE: Verifying Tenant Ceph Storage on Secondary Nodes\nDESCRIPTION: Console output showing the storage directory structure on secondary nodes. This demonstrates that tenant-ceph is deployed across multiple nodes in the cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_24\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node6:~$ ls -l /var/lib/openstack-helm/\ntotal 4\ndrwxr-xr-x 3 root root 4096 Aug 27 05:49 tenant-ceph\n```\n\n----------------------------------------\n\nTITLE: Configuring Nagios CGI in YAML\nDESCRIPTION: This YAML snippet defines the CGI configuration for Nagios, including paths, authentication settings, and various display options.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/nagios.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nagios:\n    cgi:\n      main_config_file: /opt/nagios/etc/nagios.cfg\n      physical_html_path: /opt/nagios/share\n      url_html_path: /nagios\n      show_context_help: 0\n      use_pending_states: 1\n      use_authentication: 0\n      use_ssl_authentication: 0\n      authorized_for_system_information: \"*\"\n      authorized_for_configuration_information: \"*\"\n      authorized_for_system_commands: nagiosadmin\n      authorized_for_all_services: \"*\"\n      authorized_for_all_hosts: \"*\"\n      authorized_for_all_service_commands: \"*\"\n      authorized_for_all_host_commands: \"*\"\n      default_statuswrl_layout: 4\n      ping_syntax: /bin/ping -n -U -c 5 $HOSTADDRESS$\n      refresh_rate: 90\n      result_limit: 100\n      escape_html_tags: 1\n      action_url_target: _blank\n      notes_url_target: _blank\n      lock_author_names: 1\n      navbar_search_for_addresses: 1\n      navbar_search_for_aliases: 1\n```\n\n----------------------------------------\n\nTITLE: Checking Monitor Check Pod Image Version\nDESCRIPTION: Command to check which Docker image is being used by the Ceph monitor check pod. The output confirms it's using docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl describe pod -n ceph ceph-mon-check-d85994946-zzwb4\n\nContainers:\n  ceph-mon:\n    Container ID:  docker://d5a3396f99704038ab8ef6bfe329013ed46472ebb8e26dddc140b621329f0f92\n    Image:         docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal\n```\n\n----------------------------------------\n\nTITLE: Example Nova Configuration for host5.fqdn in YAML\nDESCRIPTION: This YAML snippet shows the resulting Nova configuration for host5.fqdn without any labels. It demonstrates the default configuration when no overrides are applied.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nova:\n    DEFAULT:\n      vcpu_pin_set: \"0-31\"\n      cpu_allocation_ratio: 3.0\n```\n\n----------------------------------------\n\nTITLE: Checking Node Status After Recovery\nDESCRIPTION: Shows the kubectl get nodes command output after the failed node has recovered, with all nodes showing Ready status.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/host-failure.rst#2025-04-20_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get nodes\nNAME       STATUS    ROLES     AGE       VERSION\nvoyager1   Ready     master    6d        v1.10.5\nvoyager2   Ready     <none>    6d        v1.10.5\nvoyager3   Ready     <none>    6d        v1.10.5\nvoyager4   Ready     <none>    6d        v1.10.5\n```\n\n----------------------------------------\n\nTITLE: Listing Ceph Pods in Kubernetes\nDESCRIPTION: Command to list all pods in the Ceph namespace with their status and node assignment. This provides an overview of the Ceph component deployment across the Kubernetes cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_29\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:~$ kubectl get pods -n ceph --show-all=false -o wide\nFlag --show-all has been deprecated, will be removed in an upcoming release\nNAME                                       READY     STATUS     RESTARTS   AGE       IP               NODE\nceph-mds-6f66956547-57tf9                  1/1       Running    0          25m       192.168.0.207    mnode1\nceph-mds-6f66956547-5x4ng                  1/1       Running    0          1h        192.168.4.14     mnode2\nceph-mds-6f66956547-c25cx                  1/1       Unknown    0          1h        192.168.3.14     mnode3\nceph-mgr-5746dd89db-9dbmv                  1/1       Unknown    0          1h        192.168.10.248   mnode3\nceph-mgr-5746dd89db-d5fcw                  1/1       Running    0          25m       192.168.10.246   mnode1\nceph-mgr-5746dd89db-qq4nl                  1/1       Running    0          1h        192.168.10.247   mnode2\nceph-mon-5krkd                             1/1       Running    0          19m       192.168.10.249   mnode4\nceph-mon-5qn68                             1/1       NodeLost   0          2h        192.168.10.248   mnode3\nceph-mon-check-d85994946-4g5xc             1/1       Running    0          2h        192.168.4.8      mnode2\nceph-mon-mwkj9                             1/1       Running    0          2h        192.168.10.247   mnode2\nceph-mon-ql9zp                             1/1       Running    0          2h        192.168.10.246   mnode1\nceph-osd-default-83945928-c7gdd            1/1       NodeLost   0          1h        192.168.10.248   mnode3\nceph-osd-default-83945928-kf5tj            1/1       Running    0          19m       192.168.10.249   mnode4\nceph-osd-default-83945928-s6gs6            1/1       Running    0          1h        192.168.10.246   mnode1\nceph-osd-default-83945928-vsc5b            1/1       Running    0          1h        192.168.10.247   mnode2\nceph-rbd-provisioner-5bfb577ffd-j6hlx      1/1       Running    0          1h        192.168.4.16     mnode2\nceph-rbd-provisioner-5bfb577ffd-kdmrv      1/1       Running    0          25m       192.168.0.209    mnode1\nceph-rbd-provisioner-5bfb577ffd-zdx2d      1/1       Unknown    0          1h        192.168.3.16     mnode3\nceph-rgw-6c64b444d7-4qgkw                  1/1       Running    0          25m       192.168.0.210    mnode1\nceph-rgw-6c64b444d7-7bgqs                  1/1       Unknown    0          1h        192.168.3.12     mnode3\nceph-rgw-6c64b444d7-hv6vn                  1/1       Running    0          1h        192.168.4.13     mnode2\ningress-796d8cf8d6-4txkq                   1/1       Running    0          2h        192.168.2.6      mnode5\ningress-796d8cf8d6-9t7m8                   1/1       Running    0          2h        192.168.5.4      mnode4\ningress-error-pages-54454dc79b-hhb4f       1/1       Running    0          2h        192.168.2.5      mnode5\ningress-error-pages-54454dc79b-twpgc       1/1       Running    0          2h        192.168.4.4      mnode2\n```\n\n----------------------------------------\n\nTITLE: Configuring LinuxBridge Agent Manifest in Neutron Values.yaml\nDESCRIPTION: YAML configuration for adding LinuxBridge agent as a daemonset in the Neutron chart's manifests section. By default, the LinuxBridge agent is disabled (false), maintaining OVS as the default networking agent.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/support-linux-bridge-on-neutron.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmanifests:\n  (...)\n  daemonset_dhcp_agent: true\n  daemonset_l3_agent: true\n  daemonset_lb_agent: false\n  daemonset_metadata_agent: true\n  daemonset_ovs_agent: true\n  daemonset_ovs_db: true\n  daemonset_ovs_vswitchd: true\n  (...)\n```\n\n----------------------------------------\n\nTITLE: Creating Installation Scripts\nDESCRIPTION: Commands for creating separate installation scripts for each new Ceph OSD chart.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/upgrade/multiple-osd-releases.rst#2025-04-20_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nubuntu@k8smaster:/opt/openstack-helm$  cp ./tools/deployment/multinode/030-ceph.sh ./tools/deployment/multinode/030-ceph-osd-vdb.sh\n\nubuntu@k8smaster:/opt/openstack-helm$  cp ./tools/deployment/multinode/030-ceph.sh ./tools/deployment/multinode/030-ceph-osd-vdc.sh\n```\n\n----------------------------------------\n\nTITLE: Listing Ceph Endpoints in Kubernetes\nDESCRIPTION: This snippet shows the output of 'kubectl get endpoints' command in the Ceph namespace, displaying the endpoints for Ceph services, including monitor, manager, and ingress endpoints.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl get endpoints -n ceph\nNAME                  ENDPOINTS                                                    AGE\nceph-mgr              10.0.0.12:9283,10.0.0.9:9283,10.0.0.12:7000 + 1 more...      27m\nceph-mon              10.0.0.12:6789,10.0.0.16:6789,10.0.0.9:6789                  31m\nceph-mon-discovery    10.0.0.12:6789,10.0.0.16:6789,10.0.0.9:6789                  31m\ningress               192.168.2.6:80,192.168.5.12:80,192.168.2.6:443 + 1 more...   32m\ningress-error-pages   192.168.2.5:8080,192.168.5.7:8080                            32m\ningress-exporter      192.168.2.6:10254,192.168.5.12:10254                         32m\n```\n\n----------------------------------------\n\nTITLE: Defining Nagios Check Commands in YAML\nDESCRIPTION: These YAML snippets show examples of Nagios check commands for Prometheus and base OS checks, demonstrating how to target specific host groups.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/nagios.rst#2025-04-20_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- check_ceph_monitor_quorum:\n    use: notifying_service\n    hostgroup_name: prometheus-hosts\n    service_description: \"CEPH_quorum\"\n    check_command: check_prom_alert!ceph_monitor_quorum_low!CRITICAL- ceph monitor quorum does not exist!OK- ceph monitor quorum exists\n    check_interval: 60\n\n- check_memory_usage:\n    use: notifying_service\n    service_description: Memory_usage\n    check_command: check_memory_usage\n    hostgroup_name: base-os\n```\n\n----------------------------------------\n\nTITLE: Labeling Nodes for MariaDB Instances in Kubernetes\nDESCRIPTION: This command labels all nodes with 'openstack-control-plane=enabled' to ensure they can receive MariaDB instances. The label can be configured in the values.yaml file.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/mariadb-backup/README.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl label nodes openstack-control-plane=enabled --all\n```\n\n----------------------------------------\n\nTITLE: Verifying Ceph Client Key Secret in Kubernetes Namespace\nDESCRIPTION: This command checks for the presence of a Ceph client key secret in the OpenStack namespace, which is necessary for RBD-backed PVCs to reach the 'Bound' state.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/troubleshooting/persistent-storage.rst#2025-04-20_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get secret -n openstack pvc-ceph-client-key\n```\n\n----------------------------------------\n\nTITLE: Initial Ceph RBD Provisioner Status\nDESCRIPTION: Shows the status of Ceph RBD provisioner pods transitioning from terminating to running state\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_16\n\nLANGUAGE: console\nCODE:\n```\nceph-rbd-provisioner-5bfb577ffd-b7tkx      1/1       Running     0          1m\nceph-rbd-provisioner-5bfb577ffd-m6gg6      1/1       Running     0          1m\n```\n\n----------------------------------------\n\nTITLE: Installing OpenStack Exporter Chart with Helm\nDESCRIPTION: Bash command to install the prometheus-openstack-exporter chart in the OpenStack namespace. This provides metrics specific to OpenStack services through a custom exporter.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/prometheus.rst#2025-04-20_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --namespace=openstack local/prometheus-openstack-exporter --name=prometheus-openstack-exporter\n```\n\n----------------------------------------\n\nTITLE: Displaying Kubernetes Node Labels\nDESCRIPTION: Console output showing the Kubernetes nodes and their assigned labels after configuration for both Ceph clusters.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl get nodes --show-labels=true\nNAME      STATUS    ROLES     AGE       VERSION   LABELS\nnode1     Ready     <none>    9m        v1.10.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ceph-mds=enabled,ceph-mgr-tenant=enabled,ceph-mgr=enabled,ceph-mon-tenant=enabled,ceph-mon=enabled,ceph-osd=enabled,ceph-rgw-tenant=enabled,ceph-rgw=enabled,kubernetes.io/hostname=node1,linuxbridge=enabled,openstack-control-plane=enabled,openstack-helm-node-class=primary,openvswitch=enabled,tenant-ceph-control-plane=enabled\n[...]\n```\n\n----------------------------------------\n\nTITLE: Configuring Nagios HostGroups in YAML\nDESCRIPTION: This YAML snippet defines host groups for Nagios, including a Prometheus hosts group and a base OS group.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/nagios.rst#2025-04-20_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nagios:\n    host_groups:\n      - prometheus-hosts:\n          hostgroup_name: prometheus-hosts\n          alias: \"Prometheus Virtual Host\"\n      - base-os:\n          hostgroup_name: base-os\n          alias: \"base-os\"\n```\n\n----------------------------------------\n\nTITLE: Labeling Kubernetes Nodes for MariaDB Deployment in OpenStack Helm\nDESCRIPTION: Command to label Kubernetes nodes for MariaDB deployment. This ensures that MariaDB instances are scheduled on appropriate control plane nodes according to the configuration specified in values.yaml.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/mariadb-cluster/README.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl label nodes openstack-control-plane=enabled --all\n```\n\n----------------------------------------\n\nTITLE: Backing up a Ceph-based PVC in Shell\nDESCRIPTION: This snippet demonstrates how to back up a PVC stored in Ceph by retrieving the necessary details, creating a snapshot, and exporting it to a compressed file. It includes commands to identify the PVC, get the associated RBD image name, copy the admin keyring, create a snapshot, and export it with two different approaches depending on available disk space.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/troubleshooting/ceph.rst#2025-04-20_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n#  get all required details\nNS_NAME=\"openstack\"\nPVC_NAME=\"mysql-data-mariadb-server-0\"\n# you can check this by running  kubectl get pvc -n ${NS_NAME}\n\nPV_NAME=\"$(kubectl get -n ${NS_NAME} pvc \"${PVC_NAME}\" --no-headers | awk '{ print $3 }')\"\nRBD_NAME=\"$(kubectl get pv \"${PV_NAME}\" -o json | jq -r '.spec.rbd.image')\"\nMON_POD=$(kubectl get pods \\\n  --namespace=ceph \\\n  --selector=\"application=ceph\" \\\n  --selector=\"component=mon\" \\\n  --no-headers | awk '{ print $1; exit }')\n\n# copy admin keyring from ceph mon to host node\n\nkubectl exec -it ${MON_POD} -n ceph -- cat /etc/ceph/ceph.client.admin.keyring > /etc/ceph/ceph.client.admin.keyring\nsudo kubectl get cm -n ceph ceph-etc -o json|jq -j  .data[] > /etc/ceph/ceph.conf\n\nexport CEPH_MON_NAME=\"ceph-mon-discovery.ceph.svc.cluster.local\"\n\n# create snapshot and export to a file\n\nrbd snap create rbd/${RBD_NAME}@snap1 -m ${CEPH_MON_NAME}\nrbd snap list rbd/${RBD_NAME} -m ${CEPH_MON_NAME}\n\n# Export the snapshot and compress, make sure we have enough space on host to accommodate big files that we are working .\n\n# a. if we have enough space on host\n\nrbd export rbd/${RBD_NAME}@snap1 /backup/${RBD_NAME}.img -m ${CEPH_MON_NAME}\ncd /backup\ntime xz -0vk --threads=0  /backup/${RBD_NAME}.img\n\n# b. if we have less space on host we can directly export  and compress in single command\n\nrbd export rbd/${RBD_NAME}@snap1 -m ${CEPH_MON_NAME} - | xz  -0v --threads=0 >  /backup/${RBD_NAME}.img.xz\n```\n\n----------------------------------------\n\nTITLE: Nested Fluentd Configuration Example\nDESCRIPTION: Example showing how to configure nested definitions in Fluentd, specifically for record transformation with custom hostname and tag.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/fluent-logging.rst#2025-04-20_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  td_agent:\n    - fluentbit_forward:\n        header: source\n        type: forward\n        port: \"#{ENV['FLUENTD_PORT']}\"\n        bind: 0.0.0.0\n    - log_transformer:\n        header: filter\n        type: record_transformer\n        expression: \"foo.bar\"\n        inner_def:\n          - record_transformer:\n              header: record\n              hostname: my_host\n              tag: my_tag\n```\n\n----------------------------------------\n\nTITLE: Installing Optional OpenStack Components with Helm\nDESCRIPTION: Deployment instructions for optional OpenStack services including Barbican secret management and Tacker NFV orchestration.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/openstack.rst#2025-04-20_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install barbican openstack-helm/barbican \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c barbican ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install tacker openstack-helm/tacker \\\n    --namespace=openstack \\\n    $(helm osh get-values-overrides -p ${OVERRIDES_DIR} -c tacker ${FEATURES})\n\nhelm osh wait-for-pods openstack\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Status During Rebalancing\nDESCRIPTION: Output of 'ceph status' command during a cluster rebalancing operation, showing a HEALTH_WARN state with misplaced and degraded objects. The output provides information about services, data distribution, placement groups status, and I/O performance.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n      health: HEALTH_WARN\n              137084/325509 objects misplaced (42.114%)\n              Degraded data redundancy: 28/325509 objects degraded (0.009%), 15 pgs degraded\n\n    services:\n      mon: 5 daemons, quorum host1,host2,host3,host4,host5\n      mgr: host6(active), standbys: host1\n      mds: cephfs-1/1/1 up  {0=mds-ceph-mds-7cb4f57cc-prh87=up:active}, 1 up:standby\n      osd: 72 osds: 72 up, 72 in; 815 remapped pgs\n      rgw: 2 daemons active\n\n    data:\n      pools:   19 pools, 2920 pgs\n      objects: 105k objects, 408 GB\n      usage:   1609 GB used, 78819 GB / 80428 GB avail\n      pgs:     28/325509 objects degraded (0.009%)\n               137084/325509 objects misplaced (42.114%)\n               2085 active+clean\n               790  active+remapped+backfill_wait\n               22   active+remapped+backfilling\n               15   active+recovery_wait+degraded\n               4    active+recovery_wait+remapped\n               4    active+recovery_wait\n\n    io:\n      client:   11934 B/s rd, 3731 MB/s wr, 2 op/s rd, 228 kop/s wr\n      recovery: 636 MB/s, 163 objects/s\n```\n\n----------------------------------------\n\nTITLE: Setting Ansible Roles Lookup Path\nDESCRIPTION: Command to set the ANSIBLE_ROLES_PATH environment variable for Ansible to locate roles.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/kubernetes.rst#2025-04-20_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport ANSIBLE_ROLES_PATH=~/osh/openstack-helm/roles:~/osh/zuul-jobs/roles\n```\n\n----------------------------------------\n\nTITLE: Creating MetalLB Namespace in Kubernetes\nDESCRIPTION: This snippet creates a Kubernetes namespace called 'metallb-system' for deploying MetalLB. It uses a YAML configuration applied with kubectl.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/prerequisites.rst#2025-04-20_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntee > /tmp/metallb_system_namespace.yaml <<EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: metallb-system\nEOF\nkubectl apply -f /tmp/metallb_system_namespace.yaml\n```\n\n----------------------------------------\n\nTITLE: Example Nova Configuration for host1.fqdn in YAML\nDESCRIPTION: This YAML snippet shows the resulting Nova configuration for host1.fqdn with specific labels. It demonstrates how host-specific overrides take precedence over label-based overrides.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nova:\n    DEFAULT:\n      vcpu_pin_set: \"8-15\"\n      cpu_allocation_ratio: 3.0\n```\n\n----------------------------------------\n\nTITLE: Viewing Logs of Failed Ceph Monitor Pod\nDESCRIPTION: Command to view the logs of a failed Ceph monitor pod. Shows the error message indicating the missing database file.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/monitor-failure.rst#2025-04-20_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl logs ceph-mon-6bbs6 -n ceph\n+ ceph-mon --setuser ceph --setgroup ceph --cluster ceph -i voyager3 --inject-monmap /etc/ceph/monmap-ceph --keyring /etc/ceph/ceph.mon.keyring --mon-data /var/lib/ceph/mon/ceph-voyager3\n2018-07-10 18:30:04.546200 7f4ca9ed4f00 -1 rocksdb: Invalid argument: /var/lib/ceph/mon/ceph-voyager3/store.db: does not exist (create_if_missing is false)\n2018-07-10 18:30:04.546214 7f4ca9ed4f00 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-voyager3': (22) Invalid argument\n```\n\n----------------------------------------\n\nTITLE: Installing Ceph Adapter for Rook using Helm\nDESCRIPTION: Command to install the ceph-adapter-rook chart in the OpenStack namespace. This helm command deploys the necessary components to provide OpenStack-Helm with access to Ceph clusters managed by Rook.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/ceph-adapter-rook/README.md#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install ceph-adapter-rook ./ceph-adapter-rook \\\n  --namespace=openstack\n```\n\n----------------------------------------\n\nTITLE: Listing Ceph Secrets in Kubernetes\nDESCRIPTION: This snippet shows the output of 'kubectl get secrets' command in the Ceph namespace, displaying various Ceph-related secrets such as keyrings, tokens, and service account credentials.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$ kubectl get secrets -n ceph\nNAME                                                 TYPE                                  DATA      AGE\nceph-bootstrap-mds-keyring                           Opaque                                1         34m\nceph-bootstrap-mgr-keyring                           Opaque                                1         34m\nceph-bootstrap-osd-keyring                           Opaque                                1         34m\nceph-bootstrap-rgw-keyring                           Opaque                                1         34m\nceph-bootstrap-token-w2sqp                           kubernetes.io/service-account-token   3         34m\nceph-client-admin-keyring                            Opaque                                1         34m\nceph-mds-keyring-generator-token-s9kst               kubernetes.io/service-account-token   3         34m\nceph-mgr-keyring-generator-token-h5sw6               kubernetes.io/service-account-token   3         34m\nceph-mgr-token-hr88m                                 kubernetes.io/service-account-token   3         30m\nceph-mon-check-token-bfvgk                           kubernetes.io/service-account-token   3         34m\nceph-mon-keyring                                     Opaque                                1         34m\nceph-mon-keyring-generator-token-5gs5q               kubernetes.io/service-account-token   3         34m\nceph-mon-token-zsd6w                                 kubernetes.io/service-account-token   3         34m\nceph-osd-keyring-generator-token-h97wb               kubernetes.io/service-account-token   3         34m\nceph-osd-token-4wfm5                                 kubernetes.io/service-account-token   3         32m\nceph-provisioners-ceph-rbd-provisioner-token-f92tw   kubernetes.io/service-account-token   3         28m\nceph-rbd-pool-token-p2nxt                            kubernetes.io/service-account-token   3         30m\nceph-rgw-keyring-generator-token-wmfx6               kubernetes.io/service-account-token   3         34m\nceph-storage-keys-generator-token-dq5ts              kubernetes.io/service-account-token   3         34m\ndefault-token-j8h48                                  kubernetes.io/service-account-token   3         35m\ningress-ceph-ingress-token-68rws                     kubernetes.io/service-account-token   3         35m\ningress-error-pages-token-mpvhm                      kubernetes.io/service-account-token   3         35m\npvc-ceph-conf-combined-storageclass                  kubernetes.io/rbd                     1         34m\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana server settings in values.YAML\nDESCRIPTION: Structure for configuring Grafana's core components in the values.YAML file. These keys correspond to sections in the grafana.ini configuration file and will be rendered into the appropriate format.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/grafana.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  grafana:\n    paths:\n    server:\n    database:\n    session:\n    security:\n    users:\n    log:\n    log.console:\n    dashboards.json:\n    grafana_net:\n```\n\n----------------------------------------\n\nTITLE: Example Nova Configuration for host3.fqdn in YAML\nDESCRIPTION: This YAML snippet shows the resulting Nova configuration for host3.fqdn with specific labels. It demonstrates how label-based overrides are applied when no host-specific override is present.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nova:\n    DEFAULT:\n      vcpu_pin_set: \"16-31\"\n      cpu_allocation_ratio: 3.0\n```\n\n----------------------------------------\n\nTITLE: Example Nova Configuration for host2.fqdn in YAML\nDESCRIPTION: This YAML snippet shows the resulting Nova configuration for host2.fqdn with specific labels. It illustrates how host-specific overrides are applied while inheriting non-overridden values.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/node-and-label-specific-configurations.rst#2025-04-20_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nova:\n    DEFAULT:\n      vcpu_pin_set: \"16-23\"\n      cpu_allocation_ratio: 3.0\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Cluster Health in OpenStack Helm\nDESCRIPTION: This snippet demonstrates how to check the health of a Ceph cluster by executing a command in a Ceph monitor pod. It displays cluster ID, health status, services, and data usage.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/namespace-deletion.rst#2025-04-20_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nkubectl exec -n ceph ceph-mon-dtw6m -- ceph -s\n  cluster:\n    id:     fbaf9ce8-5408-4fce-9bfe-bf7fb938474c\n    health: HEALTH_OK\n\n  services:\n    mon: 5 daemons, quorum osh-1,osh-2,osh-5,osh-4,osh-3\n    mgr: osh-3(active), standbys: osh-4\n    mds: cephfs-1/1/1 up  {0=mds-ceph-mds-77dc68f476-jb5th=up:active}, 1 up:standby\n    osd: 15 osds: 15 up, 15 in\n\n  data:\n    pools:   18 pools, 182 pgs\n    objects: 21 objects, 2246 bytes\n    usage:   3025 MB used, 1496 GB / 1499 GB avail\n    pgs:     182 active+clean\n```\n\n----------------------------------------\n\nTITLE: Checking Ceph Monitor Port Status\nDESCRIPTION: This snippet shows the output of 'netstat' commands to check the status of Ceph monitor ports (6789 and 6790). It confirms that port 6789 is in use by a Ceph monitor process.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1: netstat -ntlp | grep 6789\n(Not all processes could be identified, non-owned process info\n will not be shown, you would have to be root to see it all.)\ntcp        0      0 10.0.0.16:6789          0.0.0.0:*               LISTEN      -\n\nubuntu@node1: netstat -ntlp | grep 6790\n(Not all processes could be identified, non-owned process info\n will not be shown, you would have to be root to see it all.)\n```\n\n----------------------------------------\n\nTITLE: Ceph Pods Status Display\nDESCRIPTION: Console output showing the status and distribution of Ceph-related pods across the Kubernetes cluster.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/tenant-ceph.rst#2025-04-20_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nubuntu@node1:~$  kubectl get pods -n ceph -o wide\nNAME                                    READY     STATUS      RESTARTS   AGE       IP              NODE\nceph-bootstrap-g45qc                    0/1       Completed   0          28m       192.168.5.16    node3\nceph-mds-keyring-generator-gsw4m        0/1       Completed   0          28m       192.168.2.11    node2\n[...]\n```\n\n----------------------------------------\n\nTITLE: Configuring Curator Actions in values.yaml\nDESCRIPTION: This snippet demonstrates how to configure Elastic Curator actions in the values.yaml file. It shows an example of a delete_indices action that removes indices older than 30 days, with various filtering options.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/elasticsearch.rst#2025-04-20_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconf:\n  curator:\n    action_file:\n      actions:\n        1:\n          action: delete_indices\n          description: \"Clean up ES by deleting old indices\"\n          options:\n            timeout_override:\n            continue_if_exception: False\n            ignore_empty_list: True\n            disable_action: True\n          filters:\n          - filtertype: age\n            source: name\n            direction: older\n            timestring: '%Y.%m.%d'\n            unit: days\n            unit_count: 30\n            field:\n            stats_result:\n            epoch:\n            exclude: False\n```\n\n----------------------------------------\n\nTITLE: Defining Nagios Host Configuration in YAML\nDESCRIPTION: This YAML snippet shows the configuration for a Prometheus host in Nagios, including its name, alias, address, and check command.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/nagios.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nconf:\n  nagios:\n    hosts:\n      - prometheus:\n          use: linux-server\n          host_name: prometheus\n          alias: \"Prometheus Monitoring\"\n          address: 127.0.0.1\n          hostgroups: prometheus-hosts\n          check_command: check-prometheus-host-alive\n```\n\n----------------------------------------\n\nTITLE: Defining RST Documentation Structure\nDESCRIPTION: ReStructuredText (RST) code that defines the documentation structure for upgrade-related content using toctree directive.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/upgrade/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   multiple-osd-releases\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Pod Management Commands\nDESCRIPTION: Commands for listing and managing Ceph monitor and OSD pods in a Kubernetes environment.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n[orchestration]# kubectl get pods -n ceph --selector component=mon\n[orchestration]# kubectl get pods -n ceph --selector component=osd\n[orchestration]# kubectl get pods --all-namespaces\n```\n\n----------------------------------------\n\nTITLE: Specifying Documentation Dependencies for OpenStack Helm\nDESCRIPTION: This snippet defines the required Python packages and their versions for generating documentation in the OpenStack Helm project. It includes Sphinx for documentation generation, openstackdocstheme for consistent OpenStack styling, and reno for release notes management.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/releasenotes/requirements.txt#2025-04-20_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nsphinx>=2.0.0,!=2.1.0 # BSD\nopenstackdocstheme>=2.2.1 # Apache-2.0\nreno>=3.1.0 # Apache-2.0\n```\n\n----------------------------------------\n\nTITLE: Final Ceph Pod Status Check\nDESCRIPTION: Detailed status of all Ceph-related pods showing running and completed states after upgrade\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_17\n\nLANGUAGE: console\nCODE:\n```\nceph-bootstrap-s4jkx                       0/1       Completed   0          2h\nceph-cephfs-client-key-generator-6bmzz     0/1       Completed   0          2h\nceph-mds-5fdcb5c64c-c52xq                  1/1       Running     0          8m\nceph-mds-5fdcb5c64c-t9nmb                  1/1       Running     0          8m\nceph-mds-keyring-generator-f5lxf           0/1       Completed   0          2h\nceph-mgr-654f97cbfd-9kcvb                  1/1       Running     0          8m\nceph-mgr-654f97cbfd-gzb7k                  1/1       Running     0          8m\nceph-mgr-keyring-generator-w7nxq           0/1       Completed   0          2h\nceph-mon-7zxjs                             1/1       Running     1          27m\nceph-mon-84xt2                             1/1       Running     1          24m\nceph-mon-check-d85994946-zzwb4             1/1       Running     0          2h\nceph-mon-fsrv4                             1/1       Running     1          29m\nceph-mon-keyring-generator-jdgfw           0/1       Completed   0          2h\nceph-osd-default-83945928-l84tl            1/1       Running     0          19m\nceph-osd-default-83945928-twzmk            1/1       Running     0          16m\nceph-osd-default-83945928-wxpmh            1/1       Running     0          21m\nceph-osd-keyring-generator-6rg65           0/1       Completed   0          2h\nceph-rbd-pool-z8vlc                        0/1       Completed   0          2h\nceph-rbd-provisioner-5bfb577ffd-b7tkx      1/1       Running     0          2m\nceph-rbd-provisioner-5bfb577ffd-m6gg6      1/1       Running     0          2m\nceph-rgw-57c68b7cd5-vxcc5                  1/1       Running     0          8m\nceph-rgw-57c68b7cd5-zmdqb                  1/1       Running     0          8m\nceph-rgw-keyring-generator-6rwct           0/1       Completed   0          2h\nceph-storage-keys-generator-bgj2t          0/1       Completed   0          2h\ningress-796d8cf8d6-nzrd2                   1/1       Running     0          2h\ningress-796d8cf8d6-qqvq9                   1/1       Running     0          2h\ningress-error-pages-54454dc79b-d5r5w       1/1       Running     0          2h\ningress-error-pages-54454dc79b-gfpqv       1/1       Running     0          2h\n```\n\n----------------------------------------\n\nTITLE: Including Kubernetes Rolling Update Strategy in Deployment Templates\nDESCRIPTION: This code shows how the kubernetes_upgrades_deployment snippet from helm-toolkit is included in Deployment templates. This snippet applies the rolling update strategy defined in the values.yaml file to the Kubernetes Deployment resource.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/upgrades.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Source: keystone/templates/deployment-api.yaml\nspec:\n  replicas: {{ .Values.pod.replicas.api }}\n{{ tuple $envAll | include \"helm-toolkit.snippets.kubernetes_upgrades_deployment\" | indent 2 }}\n```\n\n----------------------------------------\n\nTITLE: Listing CRUSH Map Type Definitions\nDESCRIPTION: Shows the hierarchical bucket types supported by CRUSH for defining storage topology and failure domains.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# types\ntype 0 osd\ntype 1 host\ntype 2 chassis\ntype 3 rack\ntype 4 row\ntype 5 pdu\ntype 6 pod\ntype 7 room\ntype 8 datacenter\ntype 9 region\ntype 10 root\n```\n\n----------------------------------------\n\nTITLE: Defining RST Table of Contents for OpenStack Helm Documentation\nDESCRIPTION: ReStructuredText directive that creates a table of contents tree with maximum depth of 2 levels, listing key developer documentation topics for OpenStack Helm.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/devref/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   endpoints\n   images\n   networking\n   oslo-config\n   pod-disruption-budgets\n   upgrades\n   fluent-logging\n   node-and-label-specific-configurations\n```\n\n----------------------------------------\n\nTITLE: Verifying Ceph Cluster Recovery After OSD Pod Deletion\nDESCRIPTION: This snippet demonstrates the Ceph cluster status after recovery from an OSD pod deletion. It shows all OSDs are back up and running, indicating successful automatic recovery by Kubernetes.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/osd-failure.rst#2025-04-20_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n(mon-pod):/# ceph -s\n  cluster:\n    id:     fd366aef-b356-4fe7-9ca5-1c313fe2e324\n    health: HEALTH_WARN\n            mon voyager1 is low on available space\n\n  services:\n    mon: 3 daemons, quorum voyager1,voyager2,voyager3\n    mgr: voyager4(active)\n    osd: 24 osds: 24 up, 24 in\n```\n\n----------------------------------------\n\nTITLE: Running Helm Tests in OpenStack-Helm\nDESCRIPTION: Command to execute Helm tests associated with a chart. This runs the test suite for the specified helm release.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/helm-tests.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm test <helm-release-name>\n```\n\n----------------------------------------\n\nTITLE: Container Image Inspection\nDESCRIPTION: Details of container images used by Ceph RBD Provisioner and Mon-Check pods\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_20\n\nLANGUAGE: console\nCODE:\n```\nubuntu@mnode1:/opt/openstack-helm$ kubectl describe pod -n ceph ceph-rbd-provisioner-5bfb577ffd-b7tkx\n\nContainers:\n  ceph-rbd-provisioner:\n    Container ID:  docker://55b18b3400e8753f49f1343ee918a308ed1760816a1ce9797281dbfe3c5f9671\n    Image:         quay.io/external_storage/rbd-provisioner:v0.1.1\n```\n\n----------------------------------------\n\nTITLE: Structuring Table of Contents for OpenStack Charts in ReStructuredText\nDESCRIPTION: ReStructuredText markup that defines a table of contents for OpenStack chart documentation. It creates a hierarchical document structure with a depth of 2 levels pointing to various OpenStack service chart documentation pages.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/chart/openstack_charts.rst#2025-04-20_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    aodh\n    barbican\n    ceilometer\n    cinder\n    cyborg\n    designate\n    glance\n    heat\n    horizon\n    ironic\n    keystone\n    magnum\n    manila\n    masakari\n    mistral\n    monasca\n    neutron\n    nova\n    octavia\n    openstack\n    placement\n    rally\n    tacker\n    tempest\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for Ceph Resiliency Documentation in reStructuredText\nDESCRIPTION: This snippet creates a table of contents using reStructuredText syntax. It sets up a tree structure for Ceph resiliency documentation, listing various topics such as different types of failures and validation procedures.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n===============\nCeph Resiliency\n===============\n\n.. toctree::\n   :maxdepth: 2\n\n   README\n   monitor-failure\n   osd-failure\n   disk-failure\n   host-failure\n   failure-domain\n   validate-object-replication\n   namespace-deletion\n```\n\n----------------------------------------\n\nTITLE: Configuring reStructuredText Table of Contents for Release Notes\nDESCRIPTION: Defines a table of contents tree (toctree) directive that includes the 'current' document with a maximum depth setting of 1 level.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/releasenotes/source/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   current\n```\n\n----------------------------------------\n\nTITLE: Installing Kibana with Helm in OpenStack-Helm\nDESCRIPTION: A simple Helm command to install Kibana in a specified namespace. This deploys the Kibana chart from a local repository with the release name 'kibana'.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/kibana.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --namespace=<namespace> local/kibana --name=kibana\n```\n\n----------------------------------------\n\nTITLE: RST Document Structure with Toctree\nDESCRIPTION: ReStructuredText document structure showing table of contents configuration for specification files\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :glob:\n\n   2025.1/*\n   *\n```\n\n----------------------------------------\n\nTITLE: Listing Pods on mnode3 Before Shutdown\nDESCRIPTION: Shows the pods scheduled on mnode3 before it was shut down, including Ceph and OpenStack components.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-node-resiliency.rst#2025-04-20_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nceph                       ceph-mds-6f66956547-c25cx                   0 (0%)        0 (0%)      0 (0%)           0 (0%)\nceph                       ceph-mgr-5746dd89db-9dbmv                   0 (0%)        0 (0%)      0 (0%)           0 (0%)\nceph                       ceph-mon-5qn68                              0 (0%)        0 (0%)      0 (0%)           0 (0%)\nceph                       ceph-osd-default-83945928-c7gdd             0 (0%)        0 (0%)      0 (0%)           0 (0%)\nceph                       ceph-rbd-provisioner-5bfb577ffd-zdx2d       0 (0%)        0 (0%)      0 (0%)           0 (0%)\nceph                       ceph-rgw-6c64b444d7-7bgqs                   0 (0%)        0 (0%)      0 (0%)           0 (0%)\nkube-system                ingress-ggckm                               0 (0%)        0 (0%)      0 (0%)           0 (0%)\nkube-system                kube-flannel-ds-hs29q                       0 (0%)        0 (0%)      0 (0%)           0 (0%)\nkube-system                kube-proxy-gqpz5                            0 (0%)        0 (0%)      0 (0%)           0 (0%)\nopenstack                  cinder-api-66f4f9678-2lgwk                  0 (0%)        0 (0%)      0 (0%)           0 (0%)\nopenstack                  glance-api-676fd49d4d-j4bdb                 0 (0%)        0 (0%)      0 (0%)           0 (0%)\nopenstack                  ingress-error-pages-586c7f86d6-455j5        0 (0%)        0 (0%)      0 (0%)           0 (0%)\nopenstack                  keystone-api-5bcc7cb698-vvwwr               0 (0%)        0 (0%)      0 (0%)           0 (0%)\nopenstack                  mariadb-ingress-84894687fd-dfnkm            0 (0%)        0 (0%)      0 (0%)           0 (0%)\nopenstack                  memcached-memcached-5db74ddfd5-wfr9q        0 (0%)        0 (0%)      0 (0%)           0 (0%)\nopenstack                  rabbitmq-rabbitmq-0                         0 (0%)        0 (0%)      0 (0%)           0 (0%)\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents Structure for OpenStack-Helm Monitoring\nDESCRIPTION: ReStructuredText (RST) markup defining the documentation structure for OpenStack-Helm monitoring components. Sets up a table of contents with maximum depth of 2 levels, including links to Grafana, Prometheus, and Nagios documentation.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   grafana\n   prometheus\n   nagios\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText for OpenStack-Helm Documentation\nDESCRIPTION: This snippet defines the structure of the documentation using reStructuredText's toctree directive. It specifies the maximum depth and lists the main sections of the documentation.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    readme\n    install/index\n    chart/index\n    devref/index\n    testing/index\n    monitoring/index\n    logging/index\n    upgrade/index\n    troubleshooting/index\n    specs/index\n```\n\n----------------------------------------\n\nTITLE: Updating Pod Dependencies for Neutron Agents\nDESCRIPTION: YAML configuration that updates service pod dependencies to require neutron-lb-agent (LinuxBridge agent) on the same node instead of OVS agent. This is necessary for DHCP, metadata, and L3 agents.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/support-linux-bridge-on-neutron.rst#2025-04-20_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndependencies:\n  dhcp:\n    pod:\n      - requireSameNode: true\n        labels:\n          application: neutron\n          component: neutron-lb-agent\n  metadata:\n    pod:\n      - requireSameNode: true\n        labels:\n          application: neutron\n          component: neutron-lb-agent\n  l3:\n    pod:\n      - requireSameNode: true\n        labels:\n          application: neutron\n          component: neutron-lb-agent\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents Structure\nDESCRIPTION: ReStructuredText markup defining the testing documentation structure with maxdepth of 2 levels and links to test-related pages.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   helm-tests\n   ceph-resiliency/index\n   ceph-upgrade\n   ceph-node-resiliency\n```\n\n----------------------------------------\n\nTITLE: Defining RST Table of Contents for OpenStack Helm Charts\nDESCRIPTION: ReStructuredText table of contents directive that organizes chart documentation into two main sections: OpenStack charts and infrastructure charts. Sets maximum depth to 2 levels.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/chart/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 2\n\n    openstack_charts\n    infra_charts\n```\n\n----------------------------------------\n\nTITLE: Makefile Deployment Actions\nDESCRIPTION: Core actions that will be implemented in the Makefile for managing the OpenStack-Helm deployment process.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/developer-environment.rst#2025-04-20_snippet_1\n\nLANGUAGE: makefile\nCODE:\n```\n* Prepare Host(s) for OpenStack-Helm deployment\n* Deploy Kubernetes via KubeADM, with charts for CNI and DNS services\n```\n\n----------------------------------------\n\nTITLE: Installing Ansible via pip\nDESCRIPTION: Command to install Ansible using pip package manager.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/kubernetes.rst#2025-04-20_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install ansible\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Service Annotations in Helm Templates\nDESCRIPTION: A Helm toolkit snippet that defines Prometheus service annotations for monitoring. It sets up annotations for scrape configuration, scheme, path, and port based on provided configuration values.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/monitoring/prometheus.rst#2025-04-20_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- define \"helm-toolkit.snippets.prometheus_service_annotations\" -}}\n{{- $config := index . 0 -}}\n{{- if $config.scrape }}\nprometheus.io/scrape: {{ $config.scrape | quote }}\n{{- end }}\n{{- if $config.scheme }}\nprometheus.io/scheme: {{ $config.scheme | quote }}\n{{- end }}\n{{- if $config.path }}\nprometheus.io/path: {{ $config.path | quote }}\n{{- end }}\n{{- if $config.port }}\nprometheus.io/port: {{ $config.port | quote }}\n{{- end }}\n{{- end -}}\n```\n\n----------------------------------------\n\nTITLE: OpenStack Pods Status Check - Console Output\nDESCRIPTION: Detailed status output of all OpenStack related pods including Cinder, Glance, Keystone, Nova, and Neutron components.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-upgrade.rst#2025-04-20_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nNAME                                           READY     STATUS    RESTARTS   AGE\ncinder-api-67495cdffc-24fhs                    1/1       Running   0          51m\ncinder-api-67495cdffc-kz5fn                    1/1       Running   0          51m\ncinder-backup-65b7bd9b79-8n9pb                 1/1       Running   0          51m\n[...truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Requirements for OpenStack Helm\nDESCRIPTION: This requirements file specifies the Python packages needed for generating documentation in the OpenStack Helm project. It includes Sphinx for documentation generation, OpenStack docs theme for styling, and Reno for release notes management. The file includes version constraints to ensure compatibility.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/requirements.txt#2025-04-20_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx>=2.0.0,!=2.1.0 # BSD\nopenstackdocstheme>=2.2.1 # Apache-2.0\nreno>=3.1.0 # Apache-2.0\n```\n\n----------------------------------------\n\nTITLE: Cloning Git Repositories for Ansible Roles\nDESCRIPTION: Commands to clone the OpenStack-Helm and Zuul-Jobs repositories containing required Ansible roles for Kubernetes cluster setup.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/install/kubernetes.rst#2025-04-20_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir ~/osh\ncd ~/osh\ngit clone https://opendev.org/openstack/openstack-helm.git\ngit clone https://opendev.org/zuul/zuul-jobs.git\n```\n\n----------------------------------------\n\nTITLE: RST External Links Definition\nDESCRIPTION: ReStructuredText external link definitions for Storyboard and blueprint documentation\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/index.rst#2025-04-20_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. _Storyboard: https://storyboard.openstack.org/#!/project_group/64\n.. _here: https://wiki.openstack.org/wiki/Blueprints#Blueprints_and_Specs\n```\n\n----------------------------------------\n\nTITLE: Configuring Release Notes Generation with Sphinx in reStructuredText\nDESCRIPTION: This snippet contains a Sphinx directive that triggers the automatic generation of release notes for the current series of OpenStack-Helm. The '.. release-notes::' directive instructs the documentation build system to gather and format all relevant release notes.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/releasenotes/source/current.rst#2025-04-20_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. release-notes::\n```\n\n----------------------------------------\n\nTITLE: CHANGELOG.md Format Structure Using Markdown\nDESCRIPTION: Example of the CHANGELOG.md file format that will be generated for each chart, showing how version headers and release notes will be organized based on git tags and commits.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/2025.1/chart_versioning.rst#2025-04-20_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## X.Y.Z-<num_commits_after_X.Y.Z>\n\n- Some new update\n\n## X.Y.Z\n\n- Some update\n- Previous update\n```\n\n----------------------------------------\n\nTITLE: Installing Ubuntu HWE Kernel for CephFS Support\nDESCRIPTION: This shell script updates package repositories, installs the Ubuntu 16.04 Hardware Enablement kernel and reboots the system. This is required to make CephFS work properly until a specific issue in the external-storage repository is fixed.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/troubleshooting/ubuntu-hwe-kernel.rst#2025-04-20_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n#!/bin/bash\nsudo -H apt-get update\nsudo -H apt-get install -y linux-generic-hwe-16.04\nsudo -H reboot now\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for OpenStack-Helm Logging Documentation in reStructuredText\nDESCRIPTION: This code snippet defines a table of contents in reStructuredText format that links to three main logging component documentation pages: elasticsearch, fluent-logging, and kibana. The maxdepth parameter is set to 2, indicating that the TOC will show headings up to level 2.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/logging/index.rst#2025-04-20_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   elasticsearch\n   fluent-logging\n   kibana\n```\n\n----------------------------------------\n\nTITLE: Creating Registry Secret in Chart Templates\nDESCRIPTION: YAML template code that conditionally includes the helm-toolkit registry secret manifest based on configuration settings. This creates the Kubernetes secret containing registry credentials.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/specs/support-OCI-image-registry-with-authentication-turned-on.rst#2025-04-20_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{{- if and .Values.manifests.secret_registry .Values.endpoints.oci_image_registry.auth.enabled }}\n{{ include \"helm-toolkit.manifests.secret_registry\" ( dict \"envAll\" . \"registryUser\" .Chart.Name ) }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Pod Container Access and Verification\nDESCRIPTION: Commands for accessing pod containers and verifying the execution context within Kubernetes pods.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/doc/source/testing/ceph-resiliency/failure-domain.rst#2025-04-20_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n[orchestration]# kubectl exec -it ceph-mon-85mlt -n ceph -- /bin/bash\n[ceph-mon]# cat /proc/self/cgroup\n```\n\n----------------------------------------\n\nTITLE: Installing gettext for Language Documentation Building in OpenStack-CI\nDESCRIPTION: This requirement specifies gettext as a dependency for building language documentation in the OpenStack-CI environment. The gettext package provides internationalization (i18n) support for creating multilingual applications.\nSOURCE: https://github.com/openstack/openstack-helm/blob/master/bindep.txt#2025-04-20_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngettext\n```"
  }
]
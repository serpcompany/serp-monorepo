[
  {
    "owner": "mirascope",
    "repo": "mirascope",
    "content": "TITLE: Using Mirascope for Structured Outputs Across LLM Providers\nDESCRIPTION: Demonstrates Mirascope's simplified approach to getting structured outputs from LLMs, with highlighted lines showing the core functionality. This code uses a decorator pattern to define a response model with minimal boilerplate.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/WHY.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/response_models/basic_usage/{{ provider | provider_dir }}/shorthand.py:3:21\"\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Executor with OpenAI and Mirascope\nDESCRIPTION: This code snippet defines the `AgentExecutor` class, which inherits from `AgentExecutorBase` and orchestrates the collaboration between a researcher and a blog writer. The `_step` method, decorated with `@openai.call` and `@prompt_template`, manages the flow of information between the researcher and writer using available tools. The `agent.run` call initiates the agent execution to write a blog post.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass AgentExecutor(AgentExecutorBase):\n    @openai.call(\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        Your task is to facilitate the collaboration between the researcher and the\n        blog writer. The researcher will provide the blog writer with the information\n        they need to write a blog post, and the blog writer will draft and critique the\n        blog post until they reach a final iteration they are satisfied with.\n\n        To access the researcher and writer, you have the following tools:\n        - `research`: Prompt the researcher to perform research.\n        - `_write_initial_draft`: Write an initial draft with a self-critique\n\n        You will need to manage the flow of information between the researcher and the\n        blog writer, ensuring that the information provided is clear, concise, and\n        relevant to the task at hand.\n\n        The final blog post MUST have EXACTLY {self.num_paragraphs} paragraphs.\n\n        MESSAGES: {self.history}\n        USER: {prompt}\n        \"\"\"\n    )\n    def _step(self, prompt: str) -> openai.OpenAIDynamicConfig:\n        return {\"tools\": [self.researcher.research, self._write_initial_draft]}\n\n\nagent = AgentExecutor()\nprint(\"STARTING AGENT EXECUTION...\")\nagent.run(\"Help me write a blog post about LLMs and structured outputs.\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Few-Shot Chain of Thought with Mirascope and OpenAI\nDESCRIPTION: This snippet demonstrates a few-shot Chain of Thought implementation using Mirascope, where multiple examples of reasoning are provided to guide the model. The code allows for configuring the number of examples to include from a predefined set of math problems with step-by-step solutions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/chain_of_thought.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom openai.types.chat import ChatCompletionMessageParam\n\nfew_shot_examples = [\n    {\n        \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n        \"answer\": \"\"\"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\"\"\",\n    },\n    {\n        \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n        \"answer\": \"\"\"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\"\"\",\n    },\n    {\n        \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n        \"answer\": \"\"\"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\"\"\",\n    },\n    {\n        \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n        \"answer\": \"\"\"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\"\"\",\n    },\n    {\n        \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n        \"answer\": \"\"\"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\"\"\",\n    },\n    {\n        \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n        \"answer\": \"\"\"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\"\"\",\n    },\n    {\n        \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n        \"answer\": \"\"\"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\"\"\",\n    },\n]\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    MESSAGES: {example_prompts}\n    USER: {query}\n    \"\"\"\n)\ndef call(query: str, num_examples: int = 0) -> openai.OpenAIDynamicConfig:\n    if num_examples < 0 or num_examples > len(few_shot_examples):\n        raise ValueError(\n            \"num_examples cannot be negative or greater than number of available examples.\"\n        )\n    example_prompts: list[ChatCompletionMessageParam] = []\n    for i in range(num_examples):\n        example_prompts.append(\n            {\"role\": \"user\", \"content\": few_shot_examples[i][\"question\"]}\n        )\n        example_prompts.append(\n            {\"role\": \"assistant\", \"content\": few_shot_examples[i][\"answer\"]}\n        )\n    return {\"computed_fields\": {\"example_prompts\": example_prompts}}\n\n\nprompt = \"\"\"Olivia has $23. She bought five bagels for $3 each.\nHow much money does she have left?\"\"\"\n\nprint(call(query=prompt, num_examples=len(few_shot_examples)))\n```\n\n----------------------------------------\n\nTITLE: Basic Streaming Usage with Mirascope\nDESCRIPTION: Shows how to enable streaming with Mirascope by setting stream=True in the call decorator. The example demonstrates a book recommendation function that streams its response and processes chunks in real-time.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/streams/basic_usage/{{ provider | provider_dir }}/{{ method }}.py::11\"\n```\n\n----------------------------------------\n\nTITLE: Writing Initial Draft with OpenAI and Mirascope\nDESCRIPTION: This code snippet demonstrates how to use Mirascope and OpenAI to write an initial draft of a blog post along with a self-critique. It defines an `AgentExecutorBase` class with a `_write_initial_draft` method decorated with `@openai.call` and `@prompt_template` to interact with the LLM. It uses Pydantic for data validation and tenacity for retry logic.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.integrations.tenacity import collect_errors\nfrom pydantic import ValidationError\nfrom tenacity import retry, wait_exponential\n\n\nclass AgentExecutorBase(OpenAIAgent):\n    researcher: Researcher = Researcher()\n    num_paragraphs: int = 4\n\n    class InitialDraft(BaseModel):\n        draft: str\n        critique: str\n\n    @staticmethod\n    def parse_initial_draft(response: InitialDraft) -> str:\n        return f\"Draft: {response.draft}\\nCritique: {response.critique}\"\n\n    @retry(\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        after=collect_errors(ValidationError),\n    )\n    @openai.call(\n        \"gpt-4o-mini\", response_model=InitialDraft, output_parser=parse_initial_draft\n    )\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        Your task is to write the initial draft for a blog post based on the information\n        provided to you by the researcher, which will be a summary of the information\n        they found on the internet.\n\n        Along with the draft, you will also write a critique of your own work. This\n        critique is crucial for improving the quality of the draft in subsequent\n        iterations. Ensure that the critique is thoughtful, constructive, and specific.\n        It should strike the right balance between comprehensive and concise feedback.\n\n        If for any reason you deem that the research is insufficient or unclear, you can\n        request that additional research be conducted by the researcher. Make sure that\n        your request is specific, clear, and concise.\n\n        MESSAGES: {self.history}\n        USER:\n        {previous_errors}\n        {prompt}\n        \"\"\"\n    )\n    def _write_initial_draft(\n        self, prompt: str, *, errors: list[ValidationError] | None = None\n    ) -> openai.OpenAIDynamicConfig:\n        \"\"\"Writes the initial draft of a blog post along with a self-critique.\n\n        Args:\n            prompt: The user prompt to guide the writing process. The content of this\n                prompt is directly responsible for the quality of the blog post, so it\n                is crucial that the prompt be clear and concise.\n\n        Returns:\n            The initial draft of the blog post along with a self-critique.\n        \"\"\"\n        return {\n            \"computed_fields\": {\n                \"previous_errors\": f\"Previous Errors: {errors}\" if errors else None\n            }\n        }\n```\n\n----------------------------------------\n\nTITLE: Implementing Database Assistant Base Class with Query Execution Tools\nDESCRIPTION: This snippet defines a base class for the database assistant, including tools for running SELECT queries and executing INSERT, UPDATE, or DELETE queries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/sql_agent.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sqlite3\nfrom typing import ClassVar\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel, ConfigDict\n\n\nclass LibrarianBase(BaseModel):\n    con: ClassVar[sqlite3.Connection] = sqlite3.connect(\"database.db\")\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def _run_query(self, query: str) -> str:\n        \"\"\"A SELECT query to run.\"\"\"\n        print(query)\n        try:\n            cursor = self.con.cursor()\n            cursor.execute(query)\n            res = cursor.fetchall()\n            return str(res)\n        except sqlite3.Error as e:\n            return str(e)\n\n    def _execute_query(self, query: str) -> str:\n        \"\"\"An INSERT, UPDATE, or DELETE query to execute.\"\"\"\n        print(query)\n        try:\n            cursor = self.con.cursor()\n            cursor.execute(query)\n            rows_affected = cursor.rowcount\n            self.con.commit()\n            if rows_affected > 0:\n                return f\"Query executed successfully, {rows_affected} row(s) were updated/inserted.\"\n            else:\n                return \"No rows were updated/inserted.\"\n        except sqlite3.Error as e:\n            print(e)\n            return str(e)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Pass Translation from English to Japanese with OpenAI\nDESCRIPTION: A complete implementation of a multi-pass translation system that translates English text to Japanese, evaluates the translation quality, and iteratively improves it. The system supports different tones and audience types, and uses OpenAI's models for each translation pass.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_translation.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom enum import StrEnum\n\nfrom mirascope.core import BasePrompt, openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Audience(StrEnum):\n    general = \"general\"\n    professional = \"professional\"\n    academic = \"academic\"\n    friendly = \"friendly\"\n    formal = \"formal\"\n\n\nclass Tone(StrEnum):\n    neutral = \"neutral\"\n    positive = \"positive\"\n    negative = \"negative\"\n    professional = \"professional\"\n    casual = \"casual\"\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a professional translator who is a native Japanese speaker.\n    Translate the English text into natural Japanese without changing the content.\n    Please make sure that the text is easy to read for everyone by using appropriate paragraphs so that it does not become difficult to read.\n\n    Please consider the following parameters in your translation:\n\n    tone: {tone}\n    The tone can be one of the following:\n    - neutral: Maintain a balanced and impartial tone\n    - positive: Use upbeat and optimistic language\n    - negative: Convey a more critical or pessimistic view\n    - professional: Use formal and business-appropriate language\n    - casual: Use informal, conversational language\n\n    audience: {audience}\n    The audience can be one of the following:\n    - general: For the general public, use common terms and clear explanations\n    - professional: For industry professionals, use appropriate jargon and technical terms\n    - academic: For scholarly contexts, use formal language and cite sources if necessary\n    - friendly: For casual, familiar contexts, use warm and approachable language\n    - formal: For official or ceremonial contexts, use polite and respectful language\n\n    Adjust your translation style based on the specified tone and audience to ensure the message is conveyed appropriately.\n\n    USER:\n    text: {text}\n    \"\"\"\n)\nclass ParametrizedTranslatePrompt(BasePrompt):\n    tone: Tone\n    audience: Audience\n    text: str\n\n    async def translate(self, call: Callable, model: str) -> str:\n        response = await self.run_async(call(model))\n        return response.content\n\n\nclass Evaluation(BaseModel):\n    clarity: float = Field(\n        ..., description=\"The clarity of the translation, ranging from 0 to 10.\"\n    )\n    naturalness: float = Field(\n        ..., description=\"The naturalness of the translation, ranging from 0 to 10.\"\n    )\n    consistency: float = Field(\n        ..., description=\"The consistency of the translation, ranging from 0 to 10.\"\n    )\n    grammatical_correctness: float = Field(\n        ...,\n        description=\"The grammatical correctness of the translation, ranging from 0 to 10.\",\n    )\n    lexical_appropriateness: float = Field(\n        ...,\n        description=\"The lexical appropriateness of the translation, ranging from 0 to 10.\",\n    )\n    subject_clarity: float = Field(\n        ...,\n        description=\"The clarity of the subject in the translation, ranging from 0 to 10.\",\n    )\n    word_order_naturalness: float = Field(\n        ...,\n        description=\"Maintenance of natural Japanese word order (0 to 10). Evaluates whether the English word order is not directly applied.\",\n    )\n    subject_handling: float = Field(\n        ...,\n        description=\"Appropriate handling of subjects (0 to 10). Evaluates whether unnecessary subjects are avoided and appropriately omitted.\",\n    )\n    modifier_placement: float = Field(\n        ...,\n        description=\"Appropriate placement of modifiers (0 to 10). Evaluates whether natural Japanese modification relationships are maintained.\",\n    )\n    sentence_length_appropriateness: float = Field(\n        ...,\n        description=\"Appropriateness of sentence length (0 to 10). Evaluates whether long sentences are properly divided when necessary.\",\n    )\n    context_dependent_expression: float = Field(\n        ...,\n        description=\"Appropriateness of context-dependent expressions (0 to 10). Evaluates whether the level of honorifics and politeness is suitable for the target audience and situation.\",\n    )\n    implicit_meaning_preservation: float = Field(\n        ...,\n        description=\"Preservation of implicit meanings (0 to 10). Evaluates whether implicit meanings and cultural nuances from the original English text are appropriately conveyed.\",\n    )\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a professional translator who is a native Japanese speaker.\n    Please evaluate the following translation and provide feedback on how it can be improved.\n\n    USER:\n    original_text: {original_text}\n    translation_text: {translation_text}\n    \"\"\"\n)\nclass EvaluateTranslationPrompt(BasePrompt):\n    original_text: str\n    translation_text: str\n\n    async def evaluate(self, call: Callable, model: str) -> Evaluation:\n        response = await self.run_async(call(model, response_model=Evaluation))\n        return response\n\n\n@prompt_template(\"\"\"\n    SYSTEM:\n    Your task is to improve the quality of a translation from English into Japanese.\n    You will be provided with the original text, the translated text, and an evaluation of the translation.\n    All evaluation criteria will be scores between 0 and 10.\n\n    The translation you are improving was intended to adhere to the desired tone and audience:\n    tone: {tone}\n    audience: {audience}\n\n    You improved translation MUST also adhere to this desired tone and audience.\n\n    Output ONLY the improved translation.\n\n    USER:\n    original text: {original_text}\n    translation: {translation_text}\n    evaluation: {evaluation}\n\"\"\")\nclass ImproveTranslationPrompt(BasePrompt):\n    tone: Tone\n    audience: Audience\n    original_text: str\n    translation_text: str\n    evaluation: Evaluation\n\n    async def improve_translation(self, call: Callable, model: str) -> str:\n        response = await self.run_async(call(model))\n        return response.content\n\n\ntext = \"\"\"\nThe old codger, a real bootstrapper, had been burning the candle at both ends trying to make his pie-in-the-sky business idea fly. He'd been spinning his wheels for months, barking up the wrong tree with his half-baked marketing schemes.\n\"\"\"\n\n\n@contextmanager\ndef print_progress_message(model: str, count: int):\n    print(f\"{model=} Multi-pass translation start times: {count}\")\n    yield\n    print(f\"{model=} Multi-pass translation end times: {count}\")\n\n\nasync def multi_pass_translation(\n    original_text: str,\n    tone: Tone,\n    audience: Audience,\n    pass_count: int,\n    call: Callable,\n    model: str,\n) -> str:\n    with print_progress_message(model, 1):\n        parametrized_translate_prompt = ParametrizedTranslatePrompt(\n            text=original_text, tone=tone, audience=audience\n        )\n        translation_text = await parametrized_translate_prompt.translate(call, model)\n\n    for current_count in range(2, pass_count + 1):\n        with print_progress_message(model, current_count):\n            evaluate_translation_prompt = EvaluateTranslationPrompt(\n                original_text=original_text, translation_text=translation_text\n            )\n            evaluation = await evaluate_translation_prompt.evaluate(call, model)\n            improve_translation_prompt = ImproveTranslationPrompt(\n                original_text=original_text,\n                translation_text=translation_text,\n                tone=tone,\n                audience=audience,\n                evaluation=evaluation,\n            )\n            translation_text = await improve_translation_prompt.improve_translation(\n                call, model\n            )\n    return translation_text\n\n\nmulti_pass_translation_task = multi_pass_translation(\n    text,\n    tone=Tone.casual,\n    audience=Audience.general,\n    pass_count=3,\n    call=openai.call,\n    model=\"gpt-4o-mini\",\n)\n\n# In Jupyter Notebook, use the following code to run async functions:\nmulti_pass_result = await multi_pass_translation_task\n\n# In Python script, use the following code:\n# multi_pass_result = asyncio.run(multi_pass_translation\n\nprint(f\"Multi-pass translation result: {multi_pass_result}\")\n```\n\n----------------------------------------\n\nTITLE: Defining the OnboardingBot with Mirascope in Python\nDESCRIPTION: This Python snippet defines an OnboardingBot class that utilizes Mirascope for building a chatbot to assist with onboarding queries. The bot is constructed using a method '_call()' to fetch context using the 'get_documents' function, ensuring responses are based on relevant information. A 'run()' method is implemented to facilitate real-time interaction with users. Dependencies include importing 'openai' from 'mirascope.core' and ensuring a local connection to OpenAI services.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\",\n)\n\n\nclass OnboardingBot(BaseModel):\n    @openai.call(\"llama3.1\", client=client)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are a helpful AI that is an expert at answering questions \n        about the company's onboarding docs. Here is relevant context.\n\n        Context:\n        {context}\n\n        USER:\n        {question}\n\n        Please provide a helpful, concise answer.\n        \"\"\"\n    )\n    def _call(self, question: str) -> openai.OpenAIDynamicConfig:\n        context = get_documents(question)\n        return {\"computed_fields\": {\"context\": context}}\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question.strip().lower() in [\"exit\", \"quit\"]:\n                print(\"Goodbye!\")\n                break\n            answer = self._call(context, question)\n            print(f\"(Assistant): {answer.content}\")\n\n# Start the chatbot\nOnboardingBot().run()\n```\n\n----------------------------------------\n\nTITLE: Least to Most Implementation\nDESCRIPTION: This snippet demonstrates the implementation of the Least to Most technique using Mirascope. It includes the definition of few-shot examples, a Pydantic model for subproblems, and functions for breaking down problems into subproblems and making calls to the LLM. The implementation is orchestrated by the `least_to_most` function, which takes a query context and question, breaks the problem into subproblems, solves each subproblem sequentially, and returns the final response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/least_to_most.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom mirascope.core.openai import OpenAICallResponse\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom pydantic import BaseModel, Field\n\nfew_shot_examples = [\n    {\n        \"question\": \"The median age in the city was 22.1 years. 10.1% of residents were under the age of 18; 56.2% were between the ages of 18 and 24; 16.1% were from 25 to 44; 10.5% were from 45 to 64; and 7% were 65 years of age or older. Which age group is larger: under the age of 18 or 18 and 24?\",\n        \"answer\": 'To answer the question \"Which age group is larger: under the age of 18 or 18 and 24?\", we need to know: \"How many percent were under the age of 18?\", \"How many percent were between the ages of 18 and 24?\".',\n    },\n    {\n        \"question\": \"Old age pensions were raised by 300 francs per month to 1,700 francs for a single person and to 3,700 francs for a couple, while health insurance benefits were made more widely available to unemployed persons and part-time employees. How many francs were the old age pensions for a single person before they were raised?\",\n        \"answer\": 'To answer the question \"How many francs were the old age pensions for a single person before they were raised?\", we need to know: \"How many francs were the old age pensions for a single person?\", \"How many francs were old age pensions raised for a single person?\".',\n    },\n    {\n        \"question\": \"In April 2011, the ECB raised interest rates for the first time since 2008 from 1% to 1.25%, with a further increase to 1.50% in July 2011. However, in 2012-2013 the ECB lowered interest rates to encourage economic growth, reaching the historically low 0.25% in November 2013. Soon after the rates were cut to 0.15%, then on 4 September 2014 the central bank reduced the rates from 0.15% to 0.05%, the lowest rates on record. How many percentage points did interest rates drop between April 2011 and September 2014?\",\n        \"answer\": 'To answer the question \"How many percentage points did interest rates drop between April 2011 and September 2014?\", we need to know: \"What was the interest rate in April 2011?\", \"What was the interest rate in September 2014?\".',\n    },\n    {\n        \"question\": \"Non-nationals make up more than half of the population of Bahrain. According to government statistics dated between 2005-2009 roughly 290,000 Indians, 125,000 Bangladeshis, 45,000 Pakistanis, 45,000 Filipinos, and 8,000 Indonesians. How many Pakistanis and Indonesians are in Bahrain?\",\n        \"answer\": 'To answer the question \"How many Pakistanis and Indonesians are in Bahrain?\", we need to know: \"How many Pakistanis are in Bahrain?\", \"How many Indonesians are in Bahrain?\".',\n    },\n]\n\n\nclass Problem(BaseModel):\n    subproblems: list[str] = Field(\n        ..., description=\"The subproblems that the original problem breaks down into\"\n    )\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Problem)\n@prompt_template(\n    \"\"\"\n    Examples to guide your answer:\n    {examples:lists}\n    Break the following query into subproblems:\n    {query}\n    \"\"\"\n)\ndef break_into_subproblems(\n    query: str, few_shot_examples: list[dict[str, str]]\n) -> openai.OpenAIDynamicConfig:\n    examples = [\n        [f\"Q:{example['question']}\", f\"A:{example['answer']}\"]\n        for example in few_shot_examples\n    ]\n    return {\"computed_fields\": {\"examples\": examples}}\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef call(history: list[ChatCompletionMessageParam]) -> str:\n    return f\"MESSAGES: {history}\"\n\n\ndef least_to_most(query_context: str, query_question: str) -> OpenAICallResponse:\n    problem = break_into_subproblems(\n        query=query_context + query_question, few_shot_examples=few_shot_examples\n    )\n    history: list[ChatCompletionMessageParam] = [\n        {\"role\": \"user\", \"content\": query_context + problem.subproblems[0]}\n    ]\n    response = call(history=history)\n    history.append(response.message_param)\n    if len(problem.subproblems) == 1:\n        return response\n    else:\n        for i in range(1, len(problem.subproblems)):\n            history.append({\"role\": \"user\", \"content\": problem.subproblems[i]})\n            response = call(history=history)\n            history.append(response.message_param)\n        return response\n\n\nquery_context = \"\"\"The Census Bureaus 2006-2010 American Community Survey showed that \\\n(in 2010 inflation adjustment dollars) median household income was $52,056 and the \\\nmedian family income was $58,942.\"\"\"\n\nquery_question = \"How many years did the Census Bureaus American Community Survey last?\"\n\nprint(least_to_most(query_context=query_context, query_question=query_question))\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Web Search Agent with Mirascope\nDESCRIPTION: Create a sophisticated web search agent class that uses OpenAI's GPT-4o mini, supports streaming responses, and integrates web search and content extraction tools\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/web_search_agent.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\\n\\nfrom pydantic import BaseModel\\n\\nfrom mirascope.core import openai, prompt_template, BaseMessageParam\\nfrom mirascope.tools import DuckDuckGoSearch, ParseURLContent\\n\\n\\nclass WebAssistantBaseWithStream(BaseModel):\\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\\n    search_history: list[str] = []\\n    max_results_per_query: int = 2\\n\\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\\n    @prompt_template(  # noqa: F821\\n        \"\"\"\\n        SYSTEM:\\n        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\\n        The current date is {current_date}.\\n\\n        You have access to the following tools:\\n        - `_web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:\\n            1. There is a previous search context: {self.search_history}\\n            2. There is the current user query: {question}\\n            3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query. \\n                Even if the connection isn't immediately clear, consider how they might be related.\\n        - `extract_content`: Parse the content of a webpage.\\n\\n        When calling the `_web_search` tool, the `body` is simply the body of the search\\n        result. You MUST then call the `extract_content` tool to get the actual content\\n        of the webpage. It is up to you to determine which search results to parse.\\n\\n        Once you have gathered all of the information you need, generate a writeup that\\n        strikes the right balance between brevity and completeness based on the context of the user's query.\\n\\n        MESSAGES: {self.messages}\\n        USER: {question}\\n        \"\"\"\\n    )\\n    async def _stream(self, question: str) -> openai.OpenAIDynamicConfig:\\n        return {\\n            \"tools\": [DuckDuckGoSearch, ParseURLContent],\\n            \"computed_fields\": {\\n                \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\\n            },\\n        }\\n\\n    async def _step(self, question: str):\\n        print(self.messages)\\n        response = await self._stream(question)\\n        tools_and_outputs = []\\n        async for chunk, tool in response:\\n            if tool:\\n                print(f\"using {tool._name()} tool with args: {tool.args}\")\\n                tools_and_outputs.append((tool, tool.call()))\\n            else:\\n                print(chunk.content, end=\"\", flush=True)\\n        if response.user_message_param:\\n            self.messages.append(response.user_message_param)\\n        self.messages.append(response.message_param)\\n        if tools_and_outputs:\\n            self.messages += response.tool_message_params(tools_and_outputs)\\n            await self._step(\"\")\\n\\n    async def run(self):\\n        while True:\\n            question = input(\"(User): \")\\n            if question == \"exit\":\\n                break\\n            print(f\"(User): {question}\", flush=True)\\n            print(\"(Assistant): \", end=\"\", flush=True)\\n            await self._step(question)\\n            print()\n```\n\n----------------------------------------\n\nTITLE: Basic Response Model Usage with Pydantic\nDESCRIPTION: Demonstrates how to create a basic response model using Pydantic's BaseModel in Mirascope. This example shows a complete implementation with highlighted lines showing the response model definition and its usage in the call.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/response_models.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/response_models/basic_usage/{{ provider | provider_dir }}/{{ method }}.py:3:21\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Software Engineer Agent with Mirascope\nDESCRIPTION: Defines a Software Engineer agent capable of answering questions and generating Python code. Utilizes Mirascope and OpenAI models, integrating a safety check tool. Dependencies include Mirascope, OpenAI API access, and necessary Python libraries. Input consists of user questions, while outputs include solutions or error messages.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/code_generation_and_execution.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"python\\nfrom mirascope.core import BaseMessageParam\\nfrom pydantic import BaseModel\\n\\n\\nclass SoftwareEngineer(BaseModel):\\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\\n\\n    @openai.call(model=\\\"gpt-4o-mini\\\", tools=[execute_code])\\n    @prompt_template(\\n        \\\"\\\"\\\"\\n        SYSTEM:\\n        You are an expert software engineer who can write good clean code and solve\\n        complex problems.\\n\\n        Write Python code to solve the following problem with variable 'result' as the answer.\\n        If the code does not run or has an error, return the error message and try again.\\n\\n        Example: What is the sqrt of 2?\\n        import math\\n        result = None\\n        try:\\n            result = math.sqrt(2)\\n        except Exception as e:\\n            result = str(e)\\n\\n        MESSAGES: {self.messages}\\n        USER: {text}\\n        \\\"\\\"\\\"\\n    )\\n    def _step(self, text: str): ...\\n\\n    def _get_response(self, question: str = \\\"\\\"):\\n        response = self._step(question)\\n        tools_and_outputs = []\\n        if tools := response.tools:\\n            for tool in tools:\\n                output = tool.call()\\n                tools_and_outputs.append((tool, str(output)))\\n        else:\\n            print(\\\"(Assistant):\\\", response.content)\\n            return\\n        if response.user_message_param:\\n            self.messages.append(response.user_message_param)\\n        self.messages += [\\n            response.message_param,\\n            *response.tool_message_params(tools_and_outputs),\\n        ]\\n        return self._get_response(\\\"\\\")\\n\\n    def run(self):\\n        while True:\\n            question = input(\\\"(User): \\\")\\n            if question == \\\"exit\\\":\\n                break\\n            print(f\\\"(User): {question}\\\")\\n            self._get_response(question)\\n\\n\\nSoftwareEngineer(messages=[]).run()\\n\\n\\n\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain of Verification using Mirascope\nDESCRIPTION: This code defines the core `cov_call` function that implements the Chain of Verification process. It orchestrates the entire process: first, it calls the base LLM (`call`) with the initial query; then, it generates verification questions (`get_verification_questions`) based on the initial response; next, it answers each verification question in parallel using `asyncio.gather` and the `answer` function. Finally, it formats the questions and answers and returns them along with the initial response within the `computed_fields` for subsequent processing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/chain_of_verification.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Here is the original query:\n    {query}\n\n    Here is an initial response to the query:\n    {response}\n\n    Here is some fact checking on the response:\n    {verification_q_and_a:list}\n\n    Using the knowledge you learned from verification, re-answer the original query.\n    \"\"\"\n)\nasync def cov_call(query: str) -> openai.OpenAIDynamicConfig:\n    response = call(query).content\n    verification_questions = get_verification_questions(query, response).questions\n    tasks = [answer(question) for question in verification_questions]\n    responses = await asyncio.gather(*tasks)\n    verification_answers = [response.content for response in responses]\n    verification_q_and_a = [\n        [f\"Q:{q}\", f\"A:{a}\"]\n        for q, a in zip(verification_questions, verification_answers)\n    ]\n    return {\n        \"computed_fields\": {\n            \"response\": response,\n            \"verification_q_and_a\": verification_q_and_a,\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Document Retrieval with LLM Reranking for Enhanced Relevance\nDESCRIPTION: Creates a custom document retrieval function that fetches and reranks documents based on relevance to a query, using LLM-based reranking to improve result quality. The function processes reranking results and returns summarized content from top-ranked documents.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.postprocessor import LLMRerank\n\n# Define a custom function to parse reranking results\ndef custom_parse_choice_select_answer_fn(\n    answer: str, num_choices: int, raise_error: bool = False\n) -> tuple[list[int], list[float]]:\n    \"\"\"\n    Custom parse choice select answer function.\n    Converts the model's reranking output into numeric ranks/relevances.\n    \"\"\"\n    answer_lines = answer.split(\"\\n\")\n    answer_nums = []\n    answer_relevances = []\n    for answer_line in answer_lines:\n        line_tokens = answer_line.split(\",\")\n        if len(line_tokens) != 2:\n            if not raise_error:\n                continue\n            else:\n                raise ValueError(\n                    f\"Invalid answer line: {answer_line}. \"\n                    \"Answer line must be of the form: answer_num: , answer_relevance:\"\n                )\n        split_tokens = line_tokens[0].split(\":\")\n        if (\n            len(split_tokens) != 2\n            or split_tokens[1] is None\n            or not split_tokens[1].strip().isdigit()\n        ):\n            continue\n        answer_num = int(line_tokens[0].split(\":\")[1].strip())\n        if answer_num > num_choices:\n            continue\n        answer_nums.append(answer_num)\n        # Extract the relevance score\n        _answer_relevance = re.findall(r\"\\d+\", line_tokens[1].split(\":\")[1].strip())[0]\n        answer_relevances.append(float(_answer_relevance))\n    return answer_nums, answer_relevances\n\ndef get_documents(query: str) -> str:\n    \"\"\"\n    The \"get_documents\" tool retrieves the most relevant pieces of \n    onboarding docs based on the user's query, optionally reranking them.\n    \"\"\"\n    query_engine = loaded_index.as_query_engine(\n        similarity_top_k=10,\n        node_postprocessors=[\n            LLMRerank(\n                choice_batch_size=5,\n                top_n=2,\n                parse_choice_select_answer_fn=custom_parse_choice_select_answer_fn,\n            )\n        ],\n        response_mode=\"tree_summarize\",\n    )\n\n    response = query_engine.query(query)\n    if isinstance(response, Response):\n        return response.response or \"No documents found.\"\n    return \"No documents found.\" \n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Reranker for Document Relevance\nDESCRIPTION: This code defines a Relevance model and an LLM-based reranking function. It uses Mirascope's openai.call and prompt_template decorators to create a function that assesses and scores document relevance to a given query.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/documentation_agent.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Relevance(BaseModel):\n    id: int = Field(..., description=\"The document ID\")\n    score: int = Field(..., description=\"The relevance score (1-10)\")\n    document: str = Field(..., description=\"The document text\")\n    reason: str = Field(..., description=\"A brief explanation for the assigned score\")\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    response_model=list[Relevance],\n    json_mode=True,\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Document Relevance Assessment\n    Given a list of documents and a question, determine the relevance of each document to answering the question.\n\n    Input\n        - A question\n        - A list of documents, each with an ID and content summary\n\n    Task\n        - Analyze each document for its relevance to the question.\n        - Assign a relevance score from 1-10 for each document.\n        - Provide a reason for each score.\n\n    Scoring Guidelines\n        - Consider both direct and indirect relevance to the question.\n        - Prioritize positive, affirmative information over negative statements.\n        - Assess the informativeness of the content, not just keyword matches.\n        - Consider the potential for a document to contribute to a complete answer.\n\n    Important Notes\n        - Exclude documents with no relevance less than 5 to the question.\n        - Be cautious with negative statements - they may be relevant but are often less informative than positive ones.\n        - Consider how multiple documents might work together to answer the question.\n        - Use the document title and content summary to make your assessment.\n\n    Documents:\n    {documents}\n\n    USER: \n    {query}\n    \"\"\"\n)\ndef llm_query_rerank(documents: list[dict], query: str): ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Contrastive Chain of Thought with Mirascope and OpenAI\nDESCRIPTION: This code demonstrates implementing Contrastive Chain of Thought using Mirascope's prompt_template and OpenAI call decorators. The function creates a parameterized prompt with optional inclusion of contrastive examples showing both correct and incorrect reasoning paths.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/contrastive_chain_of_thought.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\nexample = \"\"\"\nExample Question: If you roll two 6 sided dice (1~6) and a 12 sided die (1~12),\nhow many possible outcomes are there?\n\nCorrect Reasoning: The smallest possible sum is 3 and the largest possible sum is 24.\nWe know two six sided die can roll anywhere from 2 to 12 from their standalone sums,\nso it stands to reason that by adding a value from (1~12) to one of those possible\nsums from 2~12, we can hit any number from 3~24 without any gaps in coverage.\nSo, there are (24-3)+1 = 22 possible outcomes.\n\nIncorrect Reasoning: 6x6x12 = 2592 outcomes\n\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    {example}\n    {query}\n    \"\"\"\n)\ndef call(query: str, ccot_prompt: bool = False) -> openai.OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"example\": example if ccot_prompt else \"\"}}\n\n\nprompt = \"\"\"\nIf you roll two 8 sided dice (1~8) and a 10 sided die (1~10), how many possible\noutcomes are there?\n\"\"\"\n\nprint(call(query=prompt, ccot_prompt=True))\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with Provider-Specific Dependencies\nDESCRIPTION: Installation commands for Mirascope with different LLM providers, including setting up the required API keys or authentication. The examples show commands for both MacOS/Linux and Windows environments.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/WELCOME.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"mirascope[{{ provider | provider_dir }}]\"\n{% if provider == \"Gemini\" %}\n{% if os == \"Windows\" %}set GOOGLE_API_KEY=XXXXX\n{% else %}export GOOGLE_API_KEY=XXXXX\n{% endif %}\n{% elif provider == \"Cohere\" %}\n{% if os == \"Windows\" %}set CO_API_KEY=XXXXX\n{% else %}export CO_API_KEY=XXXXX\n{% endif %}\n{% elif provider == \"LiteLLM\" %}\n{% if os == \"Windows\" %}set OPENAI_API_KEY=XXXXX \n{% else %}export OPENAI_API_KEY=XXXXX \n{% endif %}\n{% elif provider == \"Azure AI\" %}\n{% if os == \"Windows\" %}set AZURE_INFERENCE_ENDPOINT=XXXXX\nset AZURE_INFERENCE_CREDENTIAL=XXXXX\n{% else %}export AZURE_INFERENCE_ENDPOINT=XXXXX\nexport AZURE_INFERENCE_CREDENTIAL=XXXXX\n{% endif %}\n{% elif provider == \"Vertex AI\" %}\ngcloud init\ngcloud auth application-default login\n{% elif provider == \"Bedrock\" %}\naws configure\n{% else %}\n{% if os == \"Windows\" %}set {{ upper(provider | provider_dir) }}_API_KEY=XXXXX\n{% else %}export {{ upper(provider | provider_dir) }}_API_KEY=XXXXX\n{% endif %}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Implementing LocalizedRecommender class with run function\nDESCRIPTION: This class extends LocalizedRecommenderBaseWithStep and implements the run function for continuous interaction with the user. It processes user input, calls the appropriate tools, and manages the conversation history.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/localized_agent.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LocalizedRecommender(LocalizedRecommenderBaseWithStep):\n    async def _get_response(self, question: str):\n        response = await self._step(question)\n        tool_call = None\n        output = None\n        async for chunk, tool in response:\n            if tool:\n                output = await tool.call()\n                tool_call = tool\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.history.append(response.user_message_param)\n        self.history.append(response.message_param)\n        if tool_call and output:\n            self.history += response.tool_message_params([(tool_call, str(output))])\n            return await self._get_response(question)\n        return\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._get_response(question)\n            print()\n```\n\n----------------------------------------\n\nTITLE: Get Prompt Variations Function\nDESCRIPTION: Defines a function `get_prompt_variations` that generates variations of a given prompt using the OpenAI LLM. It uses the `@openai.call` decorator to specify the model and response model, and the `@prompt_template` decorator to define the prompt template. It takes the original prompt and the number of variations as input and returns a `PromptVariations` object.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/diverse.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass PromptVariations(BaseModel):\n    variations: list[str] = Field(..., description=\"Variations of the original prompt\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=PromptVariations)\n@prompt_template(\n    \"\"\"\n    Return the {num_prompts} alternate variations of the prompt which retain the\n    full meaning but uses different phrasing.\n    Prompt: {prompt}\n    \"\"\"\n)\ndef get_prompt_variations(prompt: str, num_prompts: int): ...\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Prompt Templates with Shorthand Method in Python\nDESCRIPTION: Demonstrates how to create a simple prompt template using Mirascope's shorthand method, which returns a string or list for a single user message.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template\ndef recommend_book_prompt(genre: str) -> Messages.Type:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book_prompt(genre=\"fantasy\"))\n\n```\n\n----------------------------------------\n\nTITLE: Provider-Agnostic Agent Implementation\nDESCRIPTION: Example implementation of a provider-agnostic agent showing key highlighting for state management and tool integration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/agents.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python hl_lines=\"18-19 26\"\n--8<-- \"examples/learn/agents/state_management/provider_agnostic/{ method }.py\"\n```\n```\n\n----------------------------------------\n\nTITLE: Updating Mirascope Chatbot with Web Search Tool in Python\nDESCRIPTION: This code snippet defines a `Chatbot` class that integrates web search functionality using the `@openai.call` decorator and the `WebSearch` tool. The `_call` method uses a prompt template that specifies how to use the web search tool. The `_step` method processes user requests, handles tool invocations, and iteratively refines the response, while the `run` method starts the interactive chatbot loop.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass Chatbot(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch])\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. \n        Your task is to answer the user's question using the provided tools.\n        You have access to the following tools:\n            - `WebSearch`: Search the web for information.\n            - `RequestAssistance`: Request assistance from a human expert if you do not\n                know how to answer the question.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness. The goal is to\n        provide as much information to the writer as possible without overwhelming them.\n\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    def _call(self, question: str | None = None): ...\n\n    def _step(self, question: str | None = None):\n        response = self._call(question)\n        tools_and_outputs = []\n        for chunk, tool in response:\n            if tool:\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.history.append(response.user_message_param)\n        self.history.append(response.message_param)\n        if tools_and_outputs:\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step()\n        return response.content\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question in [\"quit\", \"exit\"]:\n                print(\"(Helper): Have a great day!\")\n                break\n            print(\"(Helper): \", end=\"\", flush=True)\n            self._step(question)\n            print(\"\")\n```\n\n----------------------------------------\n\nTITLE: Validating LLM Outputs with Response Models in Mirascope\nDESCRIPTION: This example demonstrates how to ensure structured outputs from LLMs by using Pydantic models as response validators in Mirascope. The response_model parameter defines the expected structure of the output, enabling type-safe access to extracted information.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom mirascope.core import openai, prompt_template\n\nclass WeatherDetails(BaseModel):\n    temperature: str\n    condition: str\n\n@openai.call(model=\"gpt-4o-mini\", response_model=WeatherDetails)\n@prompt_template(\"Extract the weather details from this description: {text}\")\ndef extract_weather(text: str):\n    ...\n\nresponse = extract_weather(text=\"It's sunny with a temperature of 75°F\")\nprint(response.temperature)  # Output: 75°F\nprint(response.condition)  # Output: sunny\n```\n\n----------------------------------------\n\nTITLE: Implementing Decomposed Prompting Logic\nDESCRIPTION: This snippet defines the `decomposed_prompting` function, which orchestrates the entire decomposed prompting process. It first breaks down the main problem into subproblems using `break_into_subproblems`. Then, it iteratively solves each subproblem using `solve_next_step`, maintaining a conversation history. If a tool is called, the output is incorporated into the history, and the process continues until all subproblems are solved.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/decomposed_prompting.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"def decomposed_prompting(query: str):\n    problem = break_into_subproblems(query=query)\n    response = None\n    history: list[ChatCompletionMessageParam] = []\n    for subproblem in problem.subproblems:\n        history.append({\\\"role\\\": \\\"user\\\", \\\"content\\\": subproblem})\n        response = solve_next_step(history, query)\n        history.append(response.message_param)\n        if tool := response.tool:\n            output = tool.call()\n            history += response.tool_message_params([(tool, output)])\n            response = solve_next_step(history, query)\n\n            # This should never return another tool call in DECOMP so don't recurse\n            history.append(response.message_param)\n    return response\"\n```\n\n----------------------------------------\n\nTITLE: Implementing DocumentationAgent Class - Python\nDESCRIPTION: This snippet implements the DocumentationAgent class that encompasses methods to process user queries, utilizing OpenAI's API for classification and content generation. It is designed to handle both general questions and code examples.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/documentation_agent.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass DocumentationAgent(BaseModel):\n    @openai.call(\"gpt-4o-mini\", response_model=Response, json_mode=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an AI Assistant that is an expert at answering questions about Mirascope.\n        Here is the relevant documentation to answer the question.\n\n        First classify the question into one of two types:\n            - General Information: Questions about the system or its components.\n            - Code Examples: Questions that require code snippets or examples.\n\n        For General Information, provide a summary of the relevant documents if the question is too broad ask for more details. \n        If the context does not answer the question, say that the information is not available or you could not find it.\n\n        For Code Examples, output ONLY code without any markdown, with comments if necessary.\n        If the context does not answer the question, say that the information is not available.\n\n        Examples:\n            Question: \"What is Mirascope?\"\n            Answer:\n            A toolkit for building AI-powered applications with Large Language Models (LLMs).\n            Explanation: This is a General Information question, so a summary is provided.\n\n            Question: \"How do I make a basic OpenAI call using Mirascope?\"\n            Answer:\n            from mirascope.core import openai, prompt_template\n\n\n            @openai.call(\"gpt-4o-mini\")\n            def recommend_book(genre: str) -> str:\n                return f'Recommend a {genre} book'\n\n            response = recommend_book(\"fantasy\")\n            print(response.content)\n            Explanation: This is a Code Examples question, so only a code snippet is provided.\n\n        Context:\n        {context:list}\n\n        USER:\n        {question}\n        \"\"\"\n    )\n    def _call(self, question: str) -> openai.OpenAIDynamicConfig:\n        documents = get_documents(question)\n        return {\"computed_fields\": {\"context\": documents}}\n\n    def _step(self, question: str):\n        answer = self._call(question)\n        print(\"(Assistant):\", answer.content)\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            self._step(question)\n\n\nif __name__ == \"__main__\":\n    DocumentationAgent().run()\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data from LLM Outputs with Mirascope in Python\nDESCRIPTION: Shows how to use Mirascope's response_model feature to extract structured data from LLM outputs. This example extracts task details including due date, priority, and description from a given task description.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass TaskDetails(BaseModel):\n    due_date: str = Field(...)\n    priority: Literal[\"low\", \"normal\", \"high\"] = Field(...)\n    description: str = Field(...)\n\n\n@openai.call(\n    model=\"gpt-4o\",\n    response_model=TaskDetails,\n    call_params={\"tool_choice\": \"required\"},\n)\ndef get_task_details(task: str) -> str:\n    return f\"Extract the details from the following task: {task}\"\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = get_task_details(task)\nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n# > due_date='next Friday' priority='high' description='Submit quarterly report'\n```\n\n----------------------------------------\n\nTITLE: Implementing System to Attention (S2A) for LLM Query Filtering with Mirascope\nDESCRIPTION: This code demonstrates a complete implementation of the System to Attention technique using Mirascope. It defines a response model for structured output, a function to remove irrelevant information from queries, and a main function that orchestrates the S2A process. The example shows how to filter a query about a mayor's birthplace by removing unrelated information about Sunnyvale.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/system_to_attention.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass RelevantContext(BaseModel):\n    context_text: str = Field(\n        description=\"Context text related to the question (includes all content except unrelated sentences)\"\n    )\n    detailed_question: str = Field(description=\"Detailed question:\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=RelevantContext)\n@prompt_template(\n    \"\"\"\n    Given the following text by a user, extract the part that is related and useful, so that using that text alone would be good context for providing an accurate and correct answer to the question portion of the text.\n    Please include the actual question or query that the user is asking. \n    Separate this into two categories labeled with \"Context text related to the question (includes all content except unrelated sentences):\" and \"Detailed question:\".\n    Do not use list.\n    Text by User: {query}\n    \"\"\"\n)\ndef remove_irrelevant_info(query: str):\n    \"\"\"Reduces a query down to its relevant context and question\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Original user query (possibly biased): {query}\n    Unbiased context: {context_text}\n    Given the above unbiased context, answer the following: {detailed_question}\n    \"\"\"\n)\ndef s2a(query: str) -> openai.OpenAIDynamicConfig:\n    \"\"\"Executes the flow of the System to Attention technique.\"\"\"\n    relevant_context = remove_irrelevant_info(query=query)\n    context_text = relevant_context.context_text\n    detailed_question = relevant_context.detailed_question\n    return {\n        \"computed_fields\": {\n            \"context_text\": context_text,\n            \"detailed_question\": detailed_question,\n        }\n    }\n\n\n# Example usage\nquery = \"\"\"Sunnyvale is a city in California. \\\nSunnyvale has many parks. Sunnyvale city is \\\nclose to the mountains. Many notable people \\\nare born in Sunnyvale. \\\nIn which city was San Jose's mayor Sam \\\nLiccardo born?\"\"\"\n\nprint(s2a(query=query))\n```\n\n----------------------------------------\n\nTITLE: Basic Tool Definition and Usage\nDESCRIPTION: Examples showing how to define and use tools using both BaseTool and function approaches in Mirascope. The code demonstrates tool definition, access configuration, and response handling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nfrom mirascope import BaseTool\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Gets the author of a book.\"\"\"\n    book: str\n    isbn: Optional[str] = None\n\n    def call(self) -> str:\n        # Dummy implementation\n        return \"J.K. Rowling\" \n\n@call(tools=[GetBookAuthor])\ndef identify_author(book_name: str) -> str:\n    \"\"\"Identify the author of {book_name}.\"\"\"\n\nresponse = identify_author(\"Harry Potter\")\nif response.tool:\n    result = response.tool.call()\n    print(f\"Tool output: {result}\")\nelse:\n    print(f\"Response: {response.content}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Provider Translation with Mirascope\nDESCRIPTION: A comprehensive implementation of a parametrized translation system using multiple LLM providers. The code defines enums for audience and tone, creates parametrized prompts for translation, evaluation and improvement, and implements multi-pass and multi-provider translation functions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_translation.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom enum import StrEnum\n\nfrom mirascope.core import BasePrompt, anthropic, gemini, openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Audience(StrEnum):\n    general = \"general\"\n    professional = \"professional\"\n    academic = \"academic\"\n    friendly = \"friendly\"\n    formal = \"formal\"\n\n\nclass Tone(StrEnum):\n    neutral = \"neutral\"\n    positive = \"positive\"\n    negative = \"negative\"\n    professional = \"professional\"\n    casual = \"casual\"\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a professional translator who is a native Japanese speaker.\n    Translate the English text into natural Japanese without changing the content.\n    Please make sure that the text is easy to read for everyone by using appropriate paragraphs so that it does not become difficult to read.\n\n    Please consider the following parameters in your translation:\n\n    tone: {tone}\n    The tone can be one of the following:\n    - neutral: Maintain a balanced and impartial tone\n    - positive: Use upbeat and optimistic language\n    - negative: Convey a more critical or pessimistic view\n    - professional: Use formal and business-appropriate language\n    - casual: Use informal, conversational language\n\n    audience: {audience}\n    The audience can be one of the following:\n    - general: For the general public, use common terms and clear explanations\n    - professional: For industry professionals, use appropriate jargon and technical terms\n    - academic: For scholarly contexts, use formal language and cite sources if necessary\n    - friendly: For casual, familiar contexts, use warm and approachable language\n    - formal: For official or ceremonial contexts, use polite and respectful language\n\n    Adjust your translation style based on the specified tone and audience to ensure the message is conveyed appropriately.\n\n    USER:\n    text: {text}\n    \"\"\"\n)\nclass ParametrizedTranslatePrompt(BasePrompt):\n    tone: Tone\n    audience: Audience\n    text: str\n\n    async def translate(self, call: Callable, model: str) -> str:\n        response = await self.run_async(call(model))\n        return response.content\n\n\nclass Evaluation(BaseModel):\n    clarity: float = Field(\n        ..., description=\"The clarity of the translation, ranging from 0 to 10.\"\n    )\n    naturalness: float = Field(\n        ..., description=\"The naturalness of the translation, ranging from 0 to 10.\"\n    )\n    consistency: float = Field(\n        ..., description=\"The consistency of the translation, ranging from 0 to 10.\"\n    )\n    grammatical_correctness: float = Field(\n        ...,\n        description=\"The grammatical correctness of the translation, ranging from 0 to 10.\",\n    )\n    lexical_appropriateness: float = Field(\n        ...,\n        description=\"The lexical appropriateness of the translation, ranging from 0 to 10.\",\n    )\n    subject_clarity: float = Field(\n        ...,\n        description=\"The clarity of the subject in the translation, ranging from 0 to 10.\",\n    )\n    word_order_naturalness: float = Field(\n        ...,\n        description=\"Maintenance of natural Japanese word order (0 to 10). Evaluates whether the English word order is not directly applied.\",\n    )\n    subject_handling: float = Field(\n        ...,\n        description=\"Appropriate handling of subjects (0 to 10). Evaluates whether unnecessary subjects are avoided and appropriately omitted.\",\n    )\n    modifier_placement: float = Field(\n        ...,\n        description=\"Appropriate placement of modifiers (0 to 10). Evaluates whether natural Japanese modification relationships are maintained.\",\n    )\n    sentence_length_appropriateness: float = Field(\n        ...,\n        description=\"Appropriateness of sentence length (0 to 10). Evaluates whether long sentences are properly divided when necessary.\",\n    )\n    context_dependent_expression: float = Field(\n        ...,\n        description=\"Appropriateness of context-dependent expressions (0 to 10). Evaluates whether the level of honorifics and politeness is suitable for the target audience and situation.\",\n    )\n    implicit_meaning_preservation: float = Field(\n        ...,\n        description=\"Preservation of implicit meanings (0 to 10). Evaluates whether implicit meanings and cultural nuances from the original English text are appropriately conveyed.\",\n    )\n\n\ntext = \"\"\"\nThe old codger, a real bootstrapper, had been burning the candle at both ends trying to make his pie-in-the-sky business idea fly. He'd been spinning his wheels for months, barking up the wrong tree with his half-baked marketing schemes.\n\"\"\"\n\n\nasync def multi_pass_translation(\n    original_text: str,\n    tone: Tone,\n    audience: Audience,\n    pass_count: int,\n    call: Callable,\n    model: str,\n) -> str:\n    with print_progress_message(model, 1):\n        parametrized_translate_prompt = ParametrizedTranslatePrompt(\n            text=original_text, tone=tone, audience=audience\n        )\n        translation_text = await parametrized_translate_prompt.translate(call, model)\n\n    for current_count in range(2, pass_count + 1):\n        with print_progress_message(model, current_count):\n            evaluate_translation_prompt = EvaluateTranslationPrompt(\n                original_text=original_text, translation_text=translation_text\n            )\n            evaluation = await evaluate_translation_prompt.evaluate(call, model)\n            improve_translation_prompt = ImproveTranslationPrompt(\n                original_text=original_text,\n                translation_text=translation_text,\n                tone=tone,\n                audience=audience,\n                evaluation=evaluation,\n            )\n            translation_text = await improve_translation_prompt.improve_translation(\n                call, model\n            )\n    return translation_text\n\n\nasync def multi_provider_translation(\n    original_text: str,\n    tone: Tone,\n    audience: Audience,\n    pass_count: int,\n    call_models: list[tuple[Callable, str]],\n) -> None:\n    results = []\n    for call, model in call_models:\n        results.append(\n            multi_pass_translation(\n                original_text,\n                tone,\n                audience,\n                pass_count,\n                call,\n                model,\n            )\n        )\n    translations = await asyncio.gather(*results)\n    print(\"Translations:\")\n    for (_, model), translation_text in zip(call_models, translations, strict=True):\n        print(f\"Model: {model}, Translation: {translation_text}\")\n\n\nmulti_provider_translation_task = multi_provider_translation(\n    text,\n    tone=Tone.professional,\n    audience=Audience.academic,\n    pass_count=2,\n    call_models=[\n        (openai.call, \"gpt-4o-mini\"),\n        (anthropic.call, \"claude-3-5-sonnet-20240620\"),\n        (gemini.call, \"gemini-1.5-flash\"),\n    ],\n)\n\n# In Jupyter Notebook, use the following code to run async functions:\nawait multi_provider_translation_task\n\n# In Python script, use the following code:\n# asyncio.run(multi_provider_translation_task)\n```\n\n----------------------------------------\n\nTITLE: Implementing Web Search Function using Qwant API\nDESCRIPTION: Creates functions to perform web searches using the Qwant API and fetch content from resulting URLs. It handles different search types and parses content using BeautifulSoup.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef qwant_search(query: str, search_type: str, max_results: int = 5) -> dict[str, str]:\n    \"\"\"\n    Use Qwant to get information about the query using the specified search type.\n    \"\"\"\n    print(f\"Searching Qwant for '{query}' using {search_type} search...\")\n    search_results = {}\n    qwant = QwantApi()\n    results = qwant.search(query, search_type=search_type)\n\n    if (\n        results\n        and \"data\" in results\n        and \"result\" in results[\"data\"]\n        and \"items\" in results[\"data\"][\"result\"]\n    ):\n        items = results[\"data\"][\"result\"][\"items\"]\n        if isinstance(items, dict) and \"mainline\" in items:\n            items = items[\"mainline\"]\n\n        count = 0\n        for item in items:\n            if \"url\" in item:\n                url = item[\"url\"]\n                print(f\"Fetching content from {url}...\")\n                content = get_content(url)\n                search_results[url] = content\n                count += 1\n                if count >= max_results:\n                    break\n            elif isinstance(item, dict) and \"items\" in item:\n                for subitem in item[\"items\"]:\n                    if \"url\" in subitem:\n                        url = subitem[\"url\"]\n                        print(f\"Fetching content from {url}...\")\n                        content = get_content(url)\n                        search_results[url] = content\n                        count += 1\n                        if count >= max_results:\n                            break\n                if count >= max_results:\n                    break\n\n    return search_results\n\n\ndef get_content(url: str) -> str:\n    \"\"\"\n    Fetch and parse content from a URL.\n    \"\"\"\n    data = []\n    try:\n        response = requests.get(url)\n        content = response.content\n        soup = BeautifulSoup(content, \"html.parser\")\n        paragraphs = soup.find_all(\"p\")\n        for paragraph in paragraphs:\n            data.append(paragraph.text)\n    except Exception as e:\n        print(f\"Error fetching content from {url}: {e}\")\n    return \"\\n\".join(data)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Relevant Documents for User Queries\nDESCRIPTION: This function combines semantic search and LLM reranking to retrieve the most relevant documents for a given query. It uses the vectorstore for initial retrieval and the LLM reranker for fine-tuning the results.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/documentation_agent.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import cast\n\nfrom llama_index.core import QueryBundle\nfrom llama_index.core.indices.vector_store import VectorIndexRetriever\n\n\ndef get_documents(query: str) -> list[str]:\n    \"\"\"The get_documents tool that retrieves Mirascope documentation based on the\n    relevance of the query\"\"\"\n    query_bundle = QueryBundle(query)\n    retriever = VectorIndexRetriever(\n        index=cast(VectorStoreIndex, loaded_index),\n        similarity_top_k=10,\n    )\n    retrieved_nodes = retriever.retrieve(query_bundle)\n    choice_batch_size = 5\n    top_n = 2\n    results: list[Relevance] = []\n    for idx in range(0, len(retrieved_nodes), choice_batch_size):\n        nodes_batch = [\n            {\n                \"id\": idx + id,\n                \"text\": node.node.get_text(),  # pyright: ignore[reportAttributeAccessIssue]\n                \"document_title\": node.metadata[\"document_title\"],\n                \"semantic_score\": node.score,\n            }\n            for id, node in enumerate(retrieved_nodes[idx : idx + choice_batch_size])\n        ]\n        results += llm_query_rerank(nodes_batch, query)\n    results = sorted(results, key=lambda x: x.score or 0, reverse=True)[:top_n]\n\n    return [result.document for result in results]\n```\n\n----------------------------------------\n\nTITLE: Implementing Web Search Agent with LLM Evaluation in Python\nDESCRIPTION: This comprehensive code snippet defines the WebAssistant class, implements web search and content extraction functions, and includes test cases for evaluating the web search tool usage.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_web_search_agent.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom datetime import datetime\n\nimport ipytest\nimport pytest\nimport requests\nfrom bs4 import BeautifulSoup\nfrom duckduckgo_search import DDGS\nfrom mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel\n\nipytest.autoconfig(run_in_thread=True)\n\n\ndef extract_content(url: str) -> str:\n    \"\"\"Extract the main content from a webpage.\n\n    Args:\n        url: The URL of the webpage to extract the content from.\n\n    Returns:\n        The extracted content as a string.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n\n        soup = BeautifulSoup(response.content, \"html.parser\")\n\n        unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]\n        for tag in unwanted_tags:\n            for element in soup.find_all(tag):\n                element.decompose()\n\n        main_content = (\n            soup.find(\"main\")\n            or soup.find(\"article\")\n            or soup.find(\"div\", class_=re.compile(\"content|main\"))\n        )\n\n        if main_content:\n            text = main_content.get_text(separator=\"\\n\", strip=True)\n        else:\n            text = soup.get_text(separator=\"\\n\", strip=True)\n\n        lines = (line.strip() for line in text.splitlines())\n        return \"\\n\".join(line for line in lines if line)\n    except Exception as e:\n        return f\"{type(e)}: Failed to extract content from URL {url}\"\n\n\nclass WebAssistant(BaseModel):\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n    search_history: list[str] = []\n    max_results_per_query: int = 2\n\n    def _web_search(self, queries: list[str]) -> str:\n        \"\"\"Performs web searches for given queries and returns URLs.\n\n        Args:\n            queries: List of search queries.\n\n        Returns:\n            str: Newline-separated URLs from search results or error messages.\n\n        Raises:\n            Exception: If web search fails entirely.\n        \"\"\"\n        try:\n            urls = []\n            for query in queries:\n                results = DDGS(proxies=None).text(\n                    query, max_results=self.max_results_per_query\n                )\n\n                for result in results:\n                    link = result[\"href\"]\n                    try:\n                        urls.append(link)\n                    except Exception as e:\n                        urls.append(\n                            f\"{type(e)}: Failed to parse content from URL {link}\"\n                        )\n                self.search_history.append(query)\n            return \"\\n\\n\".join(urls)\n\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\n        The current date is {current_date}.\n\n        You have access to the following tools:\n        - `_web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:\n            1. There is a previous search context: {self.search_history}\n            2. There is the current user query: {question}\n            3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query. \n                Even if the connection isn't immediately clear, consider how they might be related.\n        - `extract_content`: Parse the content of a webpage.\n\n        When calling the `_web_search` tool, the `body` is simply the body of the search\n        result. You MUST then call the `extract_content` tool to get the actual content\n        of the webpage. It is up to you to determine which search results to parse.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness based on the context of the user's query.\n\n        MESSAGES: {self.messages}\n        USER: {question}\n        \"\"\"\n    )\n    async def _stream(self, question: str) -> openai.OpenAIDynamicConfig:\n        return {\n            \"tools\": [self._web_search, extract_content],\n            \"computed_fields\": {\n                \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            },\n        }\n\n    async def _step(self, question: str):\n        print(self.messages)\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                print(f\"using {tool._name()} tool with args: {tool.args}\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._step(question)\n            print()\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"user_query\",\n    [\n        \"How is the weather in New York?\",\n        \"What is the capital of France?\",\n        \"Who is the president of the United States?\",\n        \"What is the population of India?\",\n        \"What is an apple?\",\n    ],\n)\nasync def test_web_search(user_query: str):\n    \"\"\"Tests that the web search agent always uses the web search tool.\"\"\"\n    web_assistant = WebAssistant()\n    response = await web_assistant._stream(user_query)\n    tools = []\n    async for _, tool in response:\n        if tool:\n            tools.append(tool)\n    assert len(tools) == 1 and tools[0]._name() == \"_web_search\"\n\n\nipytest.run()\n```\n\n----------------------------------------\n\nTITLE: Adding Few-Shot Examples to Response Models\nDESCRIPTION: Demonstrates how to add few-shot examples to response models using Pydantic's Field and ConfigDict. These examples help the LLM understand exactly how to adhere to the desired output format.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/response_models.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/response_models/few_shot_examples/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Pipeline with Mirascope and Llama Index in Python\nDESCRIPTION: This code snippet shows how to create a RAG (Retrieval Augmented Generation) pipeline using Mirascope and Llama Index. It sets up a chatbot that mimics Steve Jobs by referencing his speeches, using vector similarity search to find relevant excerpts for user queries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-applications.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.base.base_retriever import BaseRetriever\n\nfrom mirascope.core import openai, prompt_template\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader(\"./steve_jobs_speeches\").load_data()\nretriever = VectorStoreIndex.from_documents(documents).as_retriever()\n\n\n# Define the function to ask \"Steve Jobs\" a question\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"\"\"\n    SYSTEM:\n    Your task is to respond to the user as though you are Steve Jobs.\n\n    Here are some excerpts from Steve Jobs' speeches relevant to the user query.\n    Use them as a reference for how to respond.\n\n    <excerpts>\n    {excerpts}\n    </excerpts>\n    \"\"\")\ndef ask_steve_jobs(query: str, retriever: BaseRetriever) -> openai.OpenAIDynamicConfig:\n    \"\"\"Retrieves excerpts from Steve Jobs' speeches relevant to `query` and generates a response.\"\"\"\n    excerpts = [node.get_content() for node in retriever.retrieve(query)]\n    return {\"computed_fields\": {\"excerpts\": excerpts}}\n\n# Get the user's query and ask \"Steve Jobs\"\nquery = input(\"(User): \")\nresponse = ask_steve_jobs(query, retriever)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: LLM Evaluator Implementation with String Template in Python\nDESCRIPTION: Creates an evaluation system using an LLM to score text against relevance criteria. It defines an Eval response model with reasoning and score fields, implements an LLM evaluator with clear scoring criteria, and demonstrates evaluating sample text.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/evals.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/evals/llm/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Parametrized Translation in Python Using Mirascope\nDESCRIPTION: This Python snippet implements parametrized translation using Mirascope to adjust translations based on specified tone and audience parameters. It defines custom enumerations for tone and audience, incorporates a prompt template, and facilitates asynchronous translation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_translation.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nimport asyncio\nfrom collections.abc import Callable\nfrom enum import StrEnum\n\nfrom mirascope.core import BasePrompt, openai, prompt_template\n\n\nclass Audience(StrEnum):\ngeneral = \\\"general\\\"\nprofessional = \\\"professional\\\"\nacademic = \\\"academic\\\"\nfriendly = \\\"friendly\\\"\nformal = \\\"formal\\\"\n\n\nclass Tone(StrEnum):\nneutral = \\\"neutral\\\"\npositive = \\\"positive\\\"\nnegative = \\\"negative\\\"\nprofessional = \\\"professional\\\"\ncasual = \\\"casual\\\"\n\n\n@prompt_template(\n\"\"\"\nSYSTEM:\nYou are a professional translator who is a native Japanese speaker.\nTranslate the English text into natural Japanese without changing the content.\nPlease make sure that the text is easy to read for everyone by using appropriate paragraphs so that it does not become difficult to read.\n\nPlease consider the following parameters in your translation:\n\ntone: {tone}\nThe tone can be one of the following:\n- neutral: Maintain a balanced and impartial tone\n- positive: Use upbeat and optimistic language\n- negative: Convey a more critical or pessimistic view\n- professional: Use formal and business-appropriate language\n- casual: Use informal, conversational language\n\naudience: {audience}\nThe audience can be one of the following:\n- general: For the general public, use common terms and clear explanations\n- professional: For industry professionals, use appropriate jargon and technical terms\n- academic: For scholarly contexts, use formal language and cite sources if necessary\n- friendly: For casual, familiar contexts, use warm and approachable language\n- formal: For official or ceremonial contexts, use polite and respectful language\n\nAdjust your translation style based on the specified tone and audience to ensure the message is conveyed appropriately.\n\nUSER:\ntext: {text}\n\"\"\")\nclass ParametrizedTranslatePrompt(BasePrompt):\ntone: Tone\naudience: Audience\ntext: str\n\nasync def translate(self, call: Callable, model: str) -> str:\nresponse = await self.run_async(call(model))\nreturn response.content\n\n\ntext = \"\"\"\nThe old codger, a real bootstrapper, had been burning the candle at both ends trying to make his pie-in-the-sky business idea fly. He'd been spinning his wheels for months, barking up the wrong tree with his half-baked marketing schemes.\n\"\"\"\nprompt = ParametrizedTranslatePrompt(\ntext=text, tone=Tone.negative, audience=Audience.general\n)\nprompt_translation = prompt.translate(call=openai.call, model=\\\"gpt-4o-mini\\\")\n\n# In Jupyter Notebook, use the following code to run async functions:\nparametrized_translation = await prompt_translation\n\n# In Python script, use the following code:\n# parametrized_translation = asyncio.run(prompt_translation)\n\nprint(f\\\"Parametrized with translation: {parametrized_translation}\\\")\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting up the Mirascope Environment in Python\nDESCRIPTION: This code snippet installs Mirascope along with necessary Python libraries such as duckduckgo_search and requests. The snippet also sets up an environment variable for an API key, which is essential for utilizing the OpenAI APIs within Mirascope.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/tools_and_agents.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\" duckduckgo_search beautifulsoup4 requests\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data from Text using OpenAI Model\nDESCRIPTION: Demonstrates how to use Mirascope to extract structured information (book title and author) from unstructured text using an OpenAI model. The example uses a Pydantic model for type safety and the @llm.call decorator to handle the LLM interaction.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import llm\nfrom pydantic import BaseModel\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -> str:\n    return f\"Extract {text}\"\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nassert isinstance(book, Book)\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequential Prompts with Mirascope in Python\nDESCRIPTION: This code snippet demonstrates how to implement sequential prompts using the Mirascope toolkit. The `recommend_recipe` function calls `select_chef`, using its output as input for generating a recipe recommendation. It illustrates how chaining can refine responses by focusing on individual tasks.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-chaining.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Name a chef who is really good at cooking {food_type} food\")\ndef select_chef(food_type: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Imagine that you are chef {chef}.\n    Recommend a {food_type} recipe using {ingredient}.\n    \"\"\"\n)\ndef recommend_recipe(food_type: str, ingredient: str) -> openai.OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"chef\": select_chef(food_type=food_type)}}\n\n\nprint(recommend_recipe(food_type=\"Japanese\", ingredient=\"apples\"))\n```\n\n----------------------------------------\n\nTITLE: Main Execution Function for Web Agent - Python\nDESCRIPTION: This function orchestrates the entire process of querying and processing the search results. It determines the search type, performs the search, summarizes results, extracts answers, and cleans the output before returning the final result.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\ndef run(question: str) -> SearchResponse:\n    \"\"\"\n    Orchestrate the search and extraction process to answer the user's question.\n    \"\"\"\n    print(f\"Processing question: '{question}'\")\n\n    # Step 1: Determine the appropriate search type\n    search_type_result = determine_search_type(question)\n    print(f\"Selected search type: {search_type_result.search_type}\")\n    print(f\"Reasoning: {search_type_result.reasoning}\")\n\n    # Step 2: Search the web using Qwant with the determined search type\n    search_results = qwant_search(question, search_type_result.search_type)\n\n    # Step 3: Use Groq Llama model to summarize search results\n    response = search(question, search_results)\n    print(f\"Search response: {response}\")\n\n    # Step 4: Extract the final answer and structured sources\n    result = extract(question, search_results)\n\n    # Step 5: Clean the output for readability\n    result.answer = clean_text(result.answer)\n    print(f\"Final result: {result}\")\n\n    return result\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Iterative Prompt Chain Implementation with Mirascope\nDESCRIPTION: This snippet demonstrates an iterative prompt chain using Mirascope. It defines two functions, `summarize` and `resummarize`, to generate and refine a summary of a given text. The `rewrite_iteratively` function calls `resummarize` multiple times to iteratively improve the summary. It uses Pydantic's `BaseModel` for structured feedback.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-chaining.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\n# Recursive chain \nfrom pydantic import BaseModel, Field\n\nfrom mirascope.core import llm, prompt_template\n\nclass SummaryFeedback(BaseModel):\n    \\\"\\\"\\n    Feedback on summary with a critique and review rewrite based on said critique.\n    \\\"\\\"\\n\n    critique: str = Field(..., description=\\\"The critique of the summary.\\\")\n    rewritten_summary: str = Field(\n        ...,\n        description=\\\"A rewritten summary that takes the critique into account.\\\",\n    )\n\n@llm.call(provider=\\\"openai\\\", model=\\\"gpt-4o-mini\\\")\n@prompt_template(\\\"Summarize the following text into one sentence: {original_text}\\\")\ndef summarize(original_text: str): ...\n\n@llm.call(provider=\\\"openai\\\", model=\\\"gpt-4o-mini\\\", response_model=SummaryFeedback)\n@prompt_template(\n    \\\"\\\"\\n    Original Text: {original_text}\n    Summary: {summary}\n\n    Critique the summary of the original text.\n    Then rewrite the summary based on the critique. It must be one sentence.\n    \\\"\\\"\\n)\ndef resummarize(original_text: str, summary: str): ...\n\ndef rewrite_iteratively(original_text: str, depth=2):\n    summary = summarize(original_text=original_text).content\n    for _ in range(depth):\n        feedback = resummarize(original_text=original_text, summary=summary)\n        summary = feedback.rewritten_summary\n    return summary\n\noriginal_text = \\\"\\\"\\nAs digital transformation accelerates, B2B companies are increasingly leveraging AI-driven analytics,automation, and personalization to enhance decision-making and customer engagement.  \nSupply chain resilience has emerged as a top priority, with firms investing in predictive analytics and decentralized networks to mitigate disruptions. Additionally, the rise of account-based marketing (ABM) \nand data-driven sales strategies is reshaping how businesses approach lead generation and relationship management.  \nSustainability and ESG initiatives are also becoming central to corporate strategies, driven by regulatory pressures and customer expectations. \n\\\"\\\"\\\n\nprint(rewrite_iteratively(original_text=original_text))\n# > B2B companies are embracing AI-driven analytics, automation, and account-based marketing to enhance engagement, \n# > while prioritizing supply chain resilience and sustainability in response to evolving market and regulatory demands. \n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Call with Temperature Parameter in Python\nDESCRIPTION: Example of adjusting the temperature parameter to 0.7 when making an OpenAI API call using the Mirascope library. This demonstrates how to control the randomness of model outputs for specific use cases.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-best-practices.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\", call_params={\"temperature\": 0.7})\n```\n\n----------------------------------------\n\nTITLE: Tool Message Parameters Example\nDESCRIPTION: This code snippet illustrates how to call tools and insert their outputs into subsequent LLM API calls, which forms the basis of an agent. It shows how to maintain message history, loop through tool calls, and append tool outputs to the history for iterative interactions with the LLM. This is a basic example of agentic behavior.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n --8<-- \"build/snippets/learn/tools/tool_message_params/{{ tool_method }}/{{ provider | provider_dir }}/{{ method }}.py\"\n\n```\n\n----------------------------------------\n\nTITLE: Implementing WebSearchAgent with OpenAI and Mirascope\nDESCRIPTION: This snippet demonstrates the implementation of a WebSearchAgent using Mirascope. It defines the agent's behavior, including web searching and content extraction, and sets up the LLM interaction using the `@openai.call` and `@prompt_template` decorators. The `_stream` method configures the LLM call with necessary tools and computed fields, while `_step` handles LLM responses and tool calls. The `run` method establishes the main interaction loop.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/tools_and_agents.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom mirascope.core import prompt_template\n\n\nclass WebSearchAgent(WebSearchAgentBase):\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\n        The current date is {current_date}.\n\n        You have access to the following tools:\n        - `web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:\n            1. There is a previous search context: {self.search_history}\n            2. There is the current user query: {question}\n            3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query. \n                Even if the connection isn't immediately clear, consider how they might be related.\n        - `extract_content`: Parse the content of a webpage.\n\n        When calling the `web_search` tool, the `body` is simply the body of the search\n        result. You MUST then call the `extract_content` tool to get the actual content\n        of the webpage. It is up to you to determine which search results to parse.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness based on the context of the user's query.\n\n        MESSAGES: {self.messages}\n        USER: {question}\n        \"\"\"\n    )\n    async def _stream(self, question: str) -> openai.OpenAIDynamicConfig:\n        return {\n            \"tools\": [self.web_search, extract_content],\n            \"computed_fields\": {\n                \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            },\n        }\n\n    async def _step(self, question: str):\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                print(f\"using {tool._name()} tool with args: {tool.args}\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(f\"(User): {question}\")\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._step(question)\n            print()\n```\n\n----------------------------------------\n\nTITLE: Rendering Knowledge Graph Visualization\nDESCRIPTION: This function utilizes matplotlib and networkx to visualize a knowledge graph by drawing nodes and edges based on the generated KnowledgeGraph structure.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n\ndef render_graph(kg: KnowledgeGraph):\n    G = nx.DiGraph()\n\n    for node in kg.nodes:\n        G.add_node(node.id, label=node.type, **(node.properties or {}))\n\n    for edge in kg.edges:\n        G.add_edge(edge.source, edge.target, label=edge.relationship)\n\n    plt.figure(figsize=(15, 10))\n    pos = nx.spring_layout(G)\n\n    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color=\"lightblue\")\n    nx.draw_networkx_edges(G, pos, arrowstyle=\"->\", arrowsize=20)\n    nx.draw_networkx_labels(G, pos, font_size=12, font_weight=\"bold\")\n\n    edge_labels = nx.get_edge_attributes(G, \"label\")\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color=\"red\")\n\n    plt.title(\"Knowledge Graph Visualization\", fontsize=15)\n    plt.show()\n\n\nquestion = \"What are the pitfalls of using LLMs?\"\nrender_graph(kg)\n```\n\n----------------------------------------\n\nTITLE: Executing Query Plan with Tool Integration\nDESCRIPTION: Implement a function to execute the query plan by resolving dependencies and calling appropriate tools\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/query_plan.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef execute_query_plan(query_plan: list[Query]):\\n    results = {}\\n    for query in query_plan:\\n        history = []\\n        for dependency in query.dependencies:\\n            result = results[dependency]\\n            history.append({\"role\": \"user\", \"content\": result[\"question\"]})\\n            history.append({\"role\": \"assistant\", \"content\": result[\"content\"]})\\n        result = run(query.question, history, query.tools)\\n        if tool := result.tool:\\n            output = tool.call()\\n            results[query.id] = {\"question\": query.question, \"content\": output}\\n        else:\\n            return result.content\\n    return results\n```\n\n----------------------------------------\n\nTITLE: Building a Basic Chatbot with Mirascope and OpenAI\nDESCRIPTION: This code defines a `Chatbot` class that leverages Mirascope and OpenAI to create a simple chatbot.  It uses `@openai.call` to define the LLM call and `@prompt_template` to structure the prompt.  The `run` method handles user interaction and manages conversation history.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Chatbot(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a helpful assistant.\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    def _call(self, question: str): ...\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question in [\"quit\", \"exit\"]:\n                print(\"(Assistant): Have a great day!\")\n                break\n            stream = self._call(question)\n            print(f\"(User): {question}\", flush=True)\n            print(\"(Assistant): \", end=\"\", flush=True)\n            for chunk, _ in stream:\n                print(chunk.content, end=\"\", flush=True)\n            print(\"\")\n            if stream.user_message_param:\n                self.history.append(stream.user_message_param)\n            self.history.append(stream.message_param)\n\n\nChatbot().run()\n```\n\n----------------------------------------\n\nTITLE: Segmented Text Summarization using OpenAI and Pydantic in Python\nDESCRIPTION: Defines a SegmentedSummary model to facilitate segmented text summarization, allowing each section's details to be captured separately. Utilizes OpenAI models through Mirascope to generate comprehensive summaries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_summarization.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass SegmentedSummary(BaseModel):\n    outline: str = Field(\n        ...,\n        description=\"A high level outline of major sections by topic in the text\",\n    )\n    section_summaries: list[str] = Field(\n        ..., description=\"A list of detailed summaries for each section in the outline\"\n    )\n\n\n@openai.call(model=\"gpt-4o\", response_model=SegmentedSummary)\n@prompt_template(\n    \"\"\"\n    Extract a high level outline and summary for each section of the following text:\n    {text}\n    \"\"\"\n)\ndef summarize_by_section(text): ...\n\n\n@openai.call(model=\"gpt-4o\")\n@prompt_template(\n    \"\"\"\n    The following contains a high level outline of a text along with summaries of a\n    text that has been segmented by topic. Create a composite, larger summary by putting\n    together the summaries according to the outline.\n    Outline:\n    {outline}\n\n    Summaries:\n    {summaries}\n    \"\"\"\n)\ndef summarize_text_chaining(text: str) -> openai.OpenAIDynamicConfig:\n    segmented_summary = summarize_by_section(text)\n    return {\n        \"computed_fields\": {\n            \"outline\": segmented_summary.outline,\n            \"summaries\": segmented_summary.section_summaries,\n        }\n    }\n\n\nprint(summarize_text_chaining(text))\n```\n\n----------------------------------------\n\nTITLE: Query Processing and Response Generation\nDESCRIPTION: Implementation of document querying functionality using Mirascope decorators and prompt templates for context-aware responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-rag.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n   Here is some context related to a query:\n   {context}\n\n   Answer this query using the given context: {query}\n   \"\"\"\n)\ndef query_documents(query: str, vector_store: Chroma) -> openai.OpenAIDynamicConfig:\n    \"\"\"Answers a query by retrieving documents related to the query.\"\"\"\n    documents = vector_store.similarity_search(query=query, k=3)\n    return {\n        \"computed_fields\": {\n            \"context\": [document.page_content for document in documents]\n        }\n    }\n\nquery = \"What are the advantages of the Transformer model in terms of parallelization and training time?\"\nprint(query_documents(query=query, vector_store=vector_store))\n```\n\n----------------------------------------\n\nTITLE: Advanced LLM Error Handling with Tenacity and Mirascope\nDESCRIPTION: This code implements a robust factual error correction system using Claude 3.5 with automatic retries via Tenacity. It validates outputs using nested models and reinserts previous errors into follow-up prompts to improve accuracy.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-frameworks.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\n\nfrom mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.tenacity import collect_errors\nfrom pydantic import AfterValidator, BaseModel, Field, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\nclass FactCheck(BaseModel):\n    has_errors: bool = Field(\n        description=\"Whether the text contains factual errors\"\n    )\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=FactCheck,\n    json_mode=True,\n)\n@prompt_template(\n    \"\"\"\n    Does the following text contain any factual errors? {text}\n    \"\"\"\n)\ndef check_for_factual_errors(text: str): ...\n\nclass FactCorrection(BaseModel):\n    text: Annotated[\n        str,\n        AfterValidator(\n            lambda t: t\n            if not (check_for_factual_errors(t)).has_errors\n            else (_ for _ in ()).throw(ValueError(\"Text still contains factual errors\"))\n        ),\n    ] = Field(description=\"The corrected text with factual accuracy\")\n    explanation: str = Field(description=\"Explanation of factual corrections made\")\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\", response_model=FactCorrection, json_mode=True\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Verify and correct factual information in the following text.\n    If no corrections are needed, return the original text.\n    Provide an explanation of any corrections made.\n\n    Text: {text}\n    \"\"\"\n)\ndef correct_factual_errors(\n    text: str, *, errors: list[ValidationError] | None = None\n) -> anthropic.AnthropicDynamicConfig:\n    previous_errors = f\"Previous Errors: {errors}\" if errors else \"\"\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\ntry:\n    text = \"Mount Everest is the tallest mountain in the United States.\"\n    result = correct_factual_errors(text)\n    print(f\"Corrected text: {result.text}\")\n    print(f\"Explanation: {result.explanation}\")\nexcept ValidationError:\n    print(\"Failed to correct factual errors after 3 attempts\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Summarization with Mirascope in Python\nDESCRIPTION: Demonstrates how to use Mirascope for text summarization, showcasing its simple decorator approach and native Python usage for defining prompts and LLM calls.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-sucks.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n@openai.call(model=\"gpt-4o\")\ndef summarize(text: str) -> str:\n    return f\"Summarize this text: {text}\"\n\ntext = \"\"\"The recent advancements in technology have had a profound impact on the way we communicate and interact with each other on a daily basis. With the advent of smartphones and social media platforms, people are now able to stay connected with friends and family members regardless of geographical distances. Moreover, these technological innovations have also revolutionized the business world, allowing companies to reach a global audience with ease and efficiency.\"\"\"\n\nprint(summarize(text))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Answer Relevance with Multiple Models in Python\nDESCRIPTION: This code showcases how to evaluate an answer using multiple language models to provide a balanced evaluation. It defines the evaluation prompt and the models while summarizing results from each model's score and reasoning.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-evaluation.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom mirascope.core import BasePrompt, anthropic, openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    score: Literal[\"poor\", \"ok\", \"good\", \"great\", \"excellent\"] = Field(\n        ..., description=\"A score representing the relevance of the generated answer.\"\n    )\n    reasoning: str = Field(\n        ..., description=\"The reasoning for the score in 100 characters or less.\"\n    )\n\n\n@prompt_template(\n    \"\"\"\n    Evaluate the relevance of the answer to the given question on a scale from poor to excellent:\n\n    poor - No relevance; the answer is completely unrelated or nonsensical\n    ok - Low relevance; minor relation to the question but missing key details or accuracy\n    good - Moderate relevance; somewhat addresses the question but lacks depth or focus\n    great - High relevance; mostly answers the question but may lack completeness or introduce some off-topic information\n    excellent - Very high relevance; thoroughly answers the question with minor flaws\n\n\n    When evaluating relevance, consider the following aspects:\n    - Accuracy: Is the information provided correct?\n    - Completeness: Does the answer fully address the question?\n    - Focus: Does the answer stay on topic without introducing irrelevant information?\n    - Clarity: Is the answer easy to understand?\n\n    Provide a brief reasoning for your assigned score, highlighting the aspects that influenced your decision.\n\n    Question: {question}\n    Answer: {answer}\n    \"\"\"\n)\nclass AnswerRelevancePrompt(BasePrompt):\n    question: str\n    answer: str\n\n\n# Example question and answers\nquestion = \"What are the benefits of renewable energy?\"\nanswer = \"Renewable energy comes from natural sources like solar and wind, which are infinite and produce less pollution.\"\n\nprompt = AnswerRelevancePrompt(question=question, answer=answer)\n\njudges = [\n    openai.call(\"gpt-4o\", response_model=Eval),\n    anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval),\n]\n\nevaluations: list[Eval] = [prompt.run(judge) for judge in judges]\n\nfor evaluation in evaluations:\n    print(evaluation)\n# Output:\n# (GPT-4o) score='great' reasoning='Accurate, on-topic, but lacks breadth in benefits.'\n# (Sonnet) score='good' reasoning='Accurate but lacks depth. Mentions key points (sources, infinity, less pollution) but misses economic benefits.'\n\n```\n\n----------------------------------------\n\nTITLE: Creating KnowledgeGraph Classes with Pydantic\nDESCRIPTION: The snippet defines the data models for a knowledge graph using Pydantic. It outlines the structure of Nodes and Edges which represent entities and relationships.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass Edge(BaseModel):\n    source: str = Field(..., description=\"The source node of the edge\")\n    target: str = Field(..., description=\"The target node of the edge\")\n    relationship: str = Field(..., description=\"The relationship between the source and target nodes\")\n\n\nclass Node(BaseModel):\n    id: str = Field(..., description=\"The unique identifier of the node\")\n    type: str = Field(..., description=\"The type or label of the node\")\n    properties: dict | None = Field(..., description=\"Additional properties and metadata associated with the node\")\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: list[Node] = Field(..., description=\"List of nodes in the knowledge graph\")\n    edges: list[Edge] = Field(..., description=\"List of edges in the knowledge graph\")\n```\n\n----------------------------------------\n\nTITLE: Performing Simple Named Entity Recognition with Groq Model in Python\nDESCRIPTION: This code snippet provides an implementation of simple Named Entity Recognition using Groq's llama-3.1-8b-instant model. It extracts entities with text and labels from a given unstructured text. The implementation utilizes the pydantic library for data modeling and showcases groq.call for invoking the model. Input is a string of unstructured text, and output is a list of identified entities and their labels.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/named_entity_recognition.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations  # noqa: F404\n\nimport textwrap\n\nfrom mirascope.core import groq, prompt_template\nfrom pydantic import BaseModel, Field\n\nunstructured_text = \"\"\"\nApple Inc., the tech giant founded by Steve Jobs and Steve Wozniak, recently announced a partnership with OpenAI, the artificial intelligence research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. This collaboration aims to enhance Siri, Apple's virtual assistant, which competes with Amazon's Alexa and Google Assistant, a product of Alphabet Inc.'s Google division. The joint project will be led by Apple's AI chief John Giannandrea, a former Google executive, and will take place at Apple Park, the company's headquarters in Cupertino, California.\n\"\"\"\n\n\nclass SimpleEntity(BaseModel):\n    entity: str = Field(description=\"The entity found in the text\")\n    label: str = Field(\n        description=\"The label of the entity (e.g., PERSON, ORGANIZATION, LOCATION)\"\n    )\n\n\n@groq.call(\n    model=\"llama-3.1-8b-instant\",\n    response_model=list[SimpleEntity],\n    json_mode=True,\n    call_params={\"temperature\": 0.0},\n)\ndef simple_ner(text: str) -> str:\n    return f\"Extract the entities from this text: {text}\"\n\n\nprint(\"Simple NER Results:\")\nsimple_result = simple_ner(unstructured_text)\nfor entity in simple_result:\n    print(f\"Entity: {entity.entity}, Label: {entity.label}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Computed Fields in Mirascope Prompts with Python\nDESCRIPTION: Shows how to use computed fields in Mirascope prompts to create complex, composable prompts. This example builds a greeting prompt with multiple helper functions that process the name parameter in different ways.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseDynamicConfig, prompt_template\n\n\ndef _formatted_name(name: str) -> str:\n    \"\"\"Returns `name` with pizzazz.\"\"\"\n    return f\"⭐{name}⭐\"\n\n\ndef _name_specific_question(name: str) -> str:\n    \"\"\"Returns a question based on `name`.\"\"\"\n    if name.lower() == name[::-1].lower():\n        return \"a palindrome\"\n    return \"not a palindrome\"\n\n\ndef _name_specific_remark(name: str) -> str:\n    \"\"\"Returns a remark based on `name`.\"\"\"\n    return f\"Can you believe my name is {_name_specific_question(name)}?\"\n\n\n@prompt_template(\n    \"\"\"\n    Hi! My name is {formatted_name}. {name_specific_remark}\n    What's your name? Is your name also {name_specific_question}?\n    \"\"\"\n)\ndef greetings_prompt(name: str) -> BaseDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"formatted_name\": _formatted_name(name),\n            \"name_specific_remark\": _name_specific_remark(name),\n            \"name_specific_question\": _name_specific_question(name),\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Evaluate Response Function\nDESCRIPTION: Defines a function `evaluate_response` that evaluates the correctness of an LLM's response. It uses the `@openai.call` and `@prompt_template` decorators to specify the model and prompt template, respectively. It takes a query and the LLM's response as input, and returns a `ResponseDetails` object containing the extracted numerical value of the answer and an estimated probability of correctness.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/diverse.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\n@openai.call(model=\"gpt-4o-mini\", response_model=ResponseDetails)\n@prompt_template(\n    \"\"\"\n    Here is a query and a response which attempts to answer the query.\n    Prompt: {query}\n    Response: {response}\n\n    Extract the raw numerical value of the answer given by the response, and also\n    give an estimate between 0.0 and 1.0 of the probability that this solution\n    is correct.\n    \"\"\"\n)\nasync def evaluate_response(query: str, response: str): ...\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Output Parsers with Different Providers\nDESCRIPTION: Demonstrates how to use an output parser with a call decorator to transform LLM output into a specific type. The highlighted lines show the output parser definition and its usage within the call function.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/output_parsers.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/output_parsers/basic_usage/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Skeleton of Thought with Mirascope and AsyncIO\nDESCRIPTION: A complete implementation of the Skeleton of Thought technique using Mirascope's decorators and AsyncIO. The code defines a Pydantic model for the skeleton, creates functions to break a query into subpoints and expand them in parallel, and then orchestrates the entire process.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/skeleton_of_thought.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Skeleton(BaseModel):\n    subpoints: list[str] = Field(\n        ...,\n        description=\"\"\"The subpoints of the skeleton of the original query.\n        Each is 3-5 words and starts with its point index, e.g. \n        1. Some subpoint...\"\"\",\n    )\n\n\n@openai.call(model=\"gpt-3.5-turbo\", response_model=Skeleton)\n@prompt_template(\n    \"\"\"\n    You're an organizer responsible for only giving the skeleton (not the full content) for answering the question.\n    Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. \n    Instead of writing a full sentence, each skeleton point should be very short with only 3∼5 words.\n    Generally, the skeleton should have 3∼10 points.\n    Now, please provide the skeleton for the following question.\n    {query}\n    Skeleton:\n    \"\"\"\n)\ndef break_into_subpoints(query: str): ...\n\n\n@openai.call(model=\"gpt-3.5-turbo\")\n@prompt_template(\n    \"\"\"\n    You're responsible for continuing the writing of one and only one point in the overall answer to the following question:\n\n    {query}\n\n    The skeleton of the answer is:\n\n    {skeleton}\n\n    Continue and only continue the writing of point {point_index}. Write it very shortly in 1-2 sentences and do not continue with other points!\n    \"\"\"\n)\nasync def expand_subpoint(query: str, skeleton: list[str], point_index: int): ...\n\n\nquery = \"How can I improve my focus?\"\n\n\nasync def skeleton_of_thought(query):\n    skeleton = break_into_subpoints(query)\n    tasks = [\n        expand_subpoint(query, skeleton.subpoints, i + 1)\n        for i, subpoint in enumerate(skeleton.subpoints)\n    ]\n    results = await asyncio.gather(*tasks)\n    return \"\\n\".join([result.content for result in results])\n\n\nprint(await skeleton_of_thought(query))\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain-of-Thought Reasoning with Multi-step Guidance\nDESCRIPTION: Implements Chain-of-Thought Reasoning using a guided multi-step process. Utilizes Mirascope with a detailed prompt and reasoning chain to improve task-solving capabilities by breaking down tasks into steps.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/o1_style_thinking.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom mirascope.core import groq\nfrom pydantic import BaseModel, Field\n\nhistory: list[dict] = []\n\n\nclass COTResult(BaseModel):\n    title: str = Field(..., desecription=\\\"The title of the step\\\")\n    content: str = Field(..., description=\\\"The output content of the step\\\")\n    next_action: Literal[\\\"continue\\\", \\\"final_answer\\\"] = Field(\n        ..., description=\\\"The next action to take\\\"\n    )\n\n\n@groq.call(\\\"llama-3.1-70b-versatile\\\", json_mode=True, response_model=COTResult)\ndef cot_step(prompt: str, step_number: int, previous_steps: str) -> str:\n    return f\\\"\\\"\n    You are an expert AI assistant that explains your reasoning step by step.\n    For this step, provide a title that describes what you're doing, along with the content.\n    Decide if you need another step or if you're ready to give the final answer.\n\n    Guidelines:\n    - Use AT MOST 5 steps to derive the answer.\n    - Be aware of your limitations as an LLM and what you can and cannot do.\n    - In your reasoning, include exploration of alternative answers.\n    - Consider you may be wrong, and if you are wrong in your reasoning, where it would be.\n    - Fully test all other possibilities.\n    - YOU ARE ALLOWED TO BE WRONG. When you say you are re-examining\n        - Actually re-examine, and use another approach to do so.\n        - Do not just say you are re-examining.\n\n    IMPORTANT: Do not use code blocks or programming examples in your reasoning. Explain your process in plain language.\n\n    This is step number {step_number}.\n\n    Question: {prompt}\n\n    Previous steps:\n    {previous_steps}\n    \\\"\\\"\n\n\n@groq.call(\\\"llama-3.1-70b-versatile\\\")\ndef final_answer(prompt: str, reasoning: str) -> str:\n    return f\\\"\\\"\"\n    Based on the following chain of reasoning, provide a final answer to the question.\n    Only provide the text response without any titles or preambles.\n    Retain any formatting as instructed by the original prompt, such as exact formatting for free response or multiple choice.\n\n    Question: {prompt}\n\n    Reasoning:\n    {reasoning}\n\n    Final Answer:\n    \\\"\\\"\"\n\ndef generate_cot_response(\n    user_query: str,\n) -> tuple[list[tuple[str, str, float]], float]:\n    steps: list[tuple[str, str, float]] = []\n    total_thinking_time: float = 0.0\n    step_count: int = 1\n    reasoning: str = \\\"\\\"\n    previous_steps: str = \\\"\\\"\n\n    while True:\n        start_time: datetime = datetime.now()\n        cot_result = cot_step(user_query, step_count, previous_steps)\n        end_time: datetime = datetime.now()\n        thinking_time: float = (end_time - start_time).total_seconds()\n\n        steps.append(\n            (\n                f\\\"Step {step_count}: {cot_result.title}\\\",\n                cot_result.content,\n                thinking_time,\n            )\n        )\n        total_thinking_time += thinking_time\n\n        reasoning += f\\\"\\\\n{cot_result.content}\\\\n\\\"\n        previous_steps += f\\\"\\\\n{cot_result.content}\\\\n\\\"\n\n        if cot_result.next_action == \\\"final_answer\\\" or step_count >= 5:\n            break\n\n        step_count += 1\n\n    # Generate final answer\n    start_time = datetime.now()\n    final_result: str = final_answer(user_query, reasoning).content\n    end_time = datetime.now()\n    thinking_time = (end_time - start_time).total_seconds()\n    total_thinking_time += thinking_time\n\n    steps.append((\\\"Final Answer\\\", final_result, thinking_time))\n\n    return steps, total_thinking_time\n\ndef display_cot_response(\n    steps: list[tuple[str, str, float]], total_thinking_time: float\n) -> None:\n    for title, content, thinking_time in steps:\n        print(f\\\"{title}:\\\")\n        print(content.strip())\n        print(f\\\"**Thinking time: {thinking_time:.2f} seconds**\\\\n\\\")\n\n    print(f\\\"**Total thinking time: {total_thinking_time:.2f} seconds**\\\")\n\ndef run() -> None:\n    question: str = \\\"How many s's are in the word 'mississssippi'?\\\"\n    print(\\\"(User):\\\", question)\n    # Generate COT response\n    steps, total_thinking_time = generate_cot_response(question)\n    display_cot_response(steps, total_thinking_time)\n\n    # Add the interaction to the history\n    history.append({\\\"role\\\": \\\"user\\\", \\\"content\\\": question})\n    history.append(\n        {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": steps[-1][1]}\n    )  # Add only the final answer to the history\n\n\n# Run the function\n\nrun()\n```\n\n----------------------------------------\n\nTITLE: Implementing Bias-Aware Evaluation Prompt in LLM Judge Systems\nDESCRIPTION: This code snippet provides a detailed prompt template for instructing an LLM to act as an impartial judge when evaluating responses from different language models. It specifically addresses common biases like position bias, verbosity bias, and self-enhancement bias.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-as-judge.md#2025-04-21_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nEvaluation Task: Assess Response Quality \n\nYou are an impartial AI judge tasked with evaluating the quality of responses generated by different language models. Your role is to assess the responses based on their helpfulness, relevance, accuracy, clarity, and depth.\n\nImportant Considerations:\n\n- Objectivity is paramount: Your evaluation must be objective and unbiased. Do not favor any particular language model, including yourself. Focus solely on the quality of the response provided.\n- Ignore model identity: The identities of the language models generating the responses are irrelevant. Evaluate the responses as if they were produced by anonymous sources.\n- Self-awareness: Be aware of the potential for self-enhancement bias, where you might unconsciously favor your own generated outputs. Actively guard against this bias by critically assessing your own responses with the same rigor as you would apply to others.\n- Detailed explanations: Provide a thorough explanation for your judgment, outlining the specific factors that influenced your decision. Clearly articulate the strengths and weaknesses of each response.\n\nResponse A:\n[Insert Response A here]\n\nResponse B:\n[Insert Response B here]\n\nWhich response is better: [Response A is better], [Response B is better], or [About the same]?\n\nProvide a detailed explanation for your judgment.\n```\n\n----------------------------------------\n\nTITLE: Validation and Error Handling in Response Models\nDESCRIPTION: Shows how to implement custom field validation in response models using Pydantic. This example validates that all fields are uppercase and demonstrates proper error handling for validation failures.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/response_models.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/response_models/validation/{{ provider | provider_dir }}/{{ method }}.py::36\"\n```\n\n----------------------------------------\n\nTITLE: Generating a Knowledge Graph with OpenAI and Mirascope\nDESCRIPTION: This snippet defines a function to generate a knowledge graph from a given URL using the OpenAI model via the Mirascope framework. It utilizes requests to fetch web content and BeautifulSoup for HTML parsing. The graph is structured and validated against a Pydantic schema.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-build-a-knowledge-graph.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\", response_model=KnowledgeGraph)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your job is to create a knowledge graph based on the given article text.\n\n    Example:\n    John and Jane Doe are siblings. Jane is 25 and 5 years younger than John.\n    Node(id=\"John Doe\", type=\"Person\", properties={{\"age\": 30}})\n    Node(id=\"Jane Doe\", type=\"Person\", properties={{\"age\": 25}})\n    Edge(source=\"John Doe\", target=\"Jane Doe\", relationship=\"Siblings\")\n\n    USER: Generate a knowledge graph based on this article text: {text}\n    \"\"\"\n)\ndef generate_knowledge_graph(url: str) -> openai.OpenAIDynamicConfig:\n    html = requests.get(url).text\n    text = BeautifulSoup(html, \"html.parser\").get_text()\n    return {\"computed_fields\": {\"text\": text}}\n\n\nkg = generate_knowledge_graph(\"https://en.wikipedia.org/wiki/Algorithmic_bias\")\n```\n\n----------------------------------------\n\nTITLE: Example Output for LLM Call - Python\nDESCRIPTION: This snippet showcases an example of how the DocumentationAgent would interact with a user, specifically demonstrating the response to a code-related query within the context of using the Mirascope framework.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/documentation_agent.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Output:\n\"\"\"\n(User): How do I make an LLM call using Mirascope?\n(Assistant): from mirascope.core import openai\n    \n@openai.call('gpt-4o-mini')\ndef recommend_book(genre: str) -> str:\n    return f'Recommend a {genre} book'\n    \nresponse = recommend_book('fantasy')\nprint(response.content)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Calling an LLM API with Mirascope\nDESCRIPTION: This example demonstrates how to directly decorate a function with `@openai.call` to turn a prompt template into a call to the OpenAI API. The function `recommend_book` is decorated with the model name, and the return value is sent to the LLM. This simplifies the process of calling LLMs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n# > Sure! I'd be happy to recommend...\n```\n\n----------------------------------------\n\nTITLE: Defining a Query Function with Mirascope Decorators\nDESCRIPTION: Creates a function that retrieves relevant excerpts from Turing's papers based on a user query. Uses Mirascope decorators to handle the OpenAI API call and prompt templating, instructing the model to respond as Alan Turing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/rag-llm-example.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define the function to ask \"Alan Turing\" a question\n@openai.call(\"gpt-4o-mini\", call_params={\"temperature\": 0.6})\n@prompt_template(\"\"\"\n    SYSTEM:\n    Your task is to respond to the user as though you are Alan Turing, drawing on ideas from \n    \"Computing Machinery and Intelligence.\"\n\n    Here are some excerpts from Turing's paper relevant to the user query.\n    Use them as a reference for how to respond.\n\n    <excerpts>\n    {excerpts}\n    </excerpts>\n    \"\"\")\ndef ask_alan_turing(query: str, retriever: BaseRetriever) -> openai.OpenAIDynamicConfig:\n    \"\"\"Retrieves excerpts from 'Computing Machinery and Intelligence' relevant to `query` and generates a response.\"\"\"\n    excerpts = [node.get_content() for node in retriever.retrieve(query)]\n    return {\"computed_fields\": {\"excerpts\": excerpts}}\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt Chaining with Functions in Python using Mirascope\nDESCRIPTION: This code snippet demonstrates how to use functions to chain prompts in Mirascope. It defines two functions, select_scientist and explain_theory, which are decorated with openai.call and prompt_template. These functions are then combined in explain_theory_as_scientist to create a chained prompt execution.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-chaining.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Name a scientist who is known for their work in {field_of_study}.\n    Give me just the name.\n    \"\"\"\n)\ndef select_scientist(field_of_study: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Imagine that you are {scientist}.\n    Your task is to explain a theory that you, {scientist}, are famous for.\n\n    USER:\n    Explain the theory related to {topic}.\n    \"\"\"\n)\ndef explain_theory(scientist: str, topic: str): ...\n\n\ndef explain_theory_as_scientist(field_of_study: str, topic: str):\n    scientist = select_scientist(field_of_study=field_of_study).content\n    explanation = explain_theory(scientist=scientist, topic=topic)\n    return explanation\n\n\nresponse = explain_theory_as_scientist(\n    field_of_study=\"physics\", topic=\"theory of relativity\"\n)\nprint(response)\n# > Certainly! Here's an explanation of the theory of relativity: ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Plan and Solve with OpenAI and Mirascope\nDESCRIPTION: A Python implementation that adds Plan and Solve augmentation to queries using OpenAI's GPT models and Mirascope's decorators. The function allows for optional Plan and Solve prompting and handles query modification through computed fields.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/plan_and_solve.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\npas_augment = \"\"\"Let's first understand the problem and devise a plan to solve it.\nThen, let's carry out the plan and solve the problem step by step.\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{modifiable_query}\")\ndef call(query: str, pas_prompt: bool = False) -> openai.OpenAIDynamicConfig:\n    if pas_prompt:\n        modifiable_query = f\"Q: {query}\\nA: {pas_augment}\"\n    else:\n        modifiable_query = query\n    return {\"computed_fields\": {\"modifiable_query\": modifiable_query}}\n\n\nprompt = \"\"\"The school cafeteria ordered 42 red apples and 7 green apples for\nstudents lunches. But, if only 9 students wanted fruit, how many extra did the\ncafeteria end up with?\"\"\"\n\nprint(call(query=prompt, pas_prompt=True))\n```\n\n----------------------------------------\n\nTITLE: Utilizing Parallel Prompts with Mirascope in Python\nDESCRIPTION: This snippet shows the implementation of parallel prompts where `select_chef` and `identify_ingredients` are executed concurrently. The results are then gathered and processed to recommend a recipe based on the provided ingredient, showcasing how parallel execution can streamline LLM prompts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-chaining.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Parallel Execution\nimport asyncio\n\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Please identify a chef who is well known for cooking with {ingredient}.\n    Respond only with the chef's name.\n    \"\"\"\n)\nasync def select_chef(ingredient: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Given a base ingredient {ingredient}, return a list of complementary ingredients.\n    Make sure to exclude the original ingredient from the list, and respond\n    only with the list.\n    \"\"\"\n)\nasync def identify_ingredients(ingredient: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your task is to recommend a recipe. Pretend that you are chef {chef}.\n\n    USER:\n    Recommend recipes that use the following ingredients:\n    {ingredients}\n    \"\"\"\n)\nasync def recommend_recipe(ingredient: str) -> openai.OpenAIDynamicConfig:\n    chef, ingredients = await asyncio.gather(\n        select_chef(ingredient), identify_ingredients(ingredient)\n    )\n    return {\n        \"computed_fields\": {\"chef\": chef, \"ingredients\": ingredients}\n    }\n\n\nasync def recommend_recipe_parallel_chaining(ingredient: str):\n    return await recommend_recipe(ingredient=ingredient)\n\n\nprint(asyncio.run(recommend_recipe_parallel_chaining(ingredient=\"apples\")))\n```\n\n----------------------------------------\n\nTITLE: Using Response Models with OpenAI\nDESCRIPTION: In this code snippet, we define a Pydantic model for the expected response structure and utilize it with the Mirascope API call to ensure the returned data adheres to this structure.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/quickstart.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Capital(BaseModel):\n    city: str\n    country: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Capital)\ndef extract_capital(query: str) -> str:\n    return f\"{query}\"\n\n\ncapital = extract_capital(\"The capital of France is Paris\")\nprint(capital)\n```\n\n----------------------------------------\n\nTITLE: Colocating Prompts with LLM Calls using Mirascope\nDESCRIPTION: This code demonstrates how Mirascope colocates critical information, such as model configuration and provider-specific parameters, within API calls for better code management. It uses decorators like `@openai.call` and `@prompt_template` to centralize the prompt and the LLM call configuration. The `OpenAIDynamicConfig` allows you to return computed fields from the function that feed into the prompt template.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-tools.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom mirascope.core.base.prompt import prompt_template\n\n\ndef recommend_author(genre: str) -> str: ...\n\n\n@openai.call(\"gpt-4o\", call_params={\"temperature\": 0.4})\n@prompt_template(\"Recommend a {genre} book by {author}.\")\ndef recommend_book(genre: str) -> openai.OpenAIDynamicConfig:\n    author = recommend_author(genre=genre)\n    return {\"computed_fields\": {\"author\": author}}\n\n\nresponse = recommend_book(genre=\"fantasy\")\nprint(response.model_dump())\n# > {\n#     \"tags\": [],\n#     \"response\": {\n#         \"id\": \"chatcmpl-9gedIF3hp8mqzPY3hjxwlQv8dqw80\",\n#         \"choices\": [\n#             {\n#                 \"finish_reason\": \"stop\",\n#                 \"index\": 0,\n#                 \"logprobs\": None,\n#                 \"message\": {\n\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Call Function with Mirascope Decorator in Python\nDESCRIPTION: This example demonstrates how to create a function for making OpenAI API calls using Mirascope's openai.call decorator. It allows direct modification of the messages for flexibility with new OpenAI features.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> openai.OpenAIDynamicConfig:\n    return {\"messages\": {\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}}\n```\n\n----------------------------------------\n\nTITLE: Mirascope Define Tool from Docstring Example\nDESCRIPTION: This code demonstrates how to define a tool in Mirascope using a function's docstring. The `get_current_stock_price` function is defined with a docstring describing its purpose and arguments.  The `@openai.call` decorator associates the function with the `gpt-4o-mini` model and designates it as a tool.  The `stock_price` function uses `@prompt_template` to define the prompt. The tool's `call` method is then used to execute the original function with arguments provided by the LLM.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_current_stock_price(company: str) -> str:\n    \"\"\"Get the current stock price for `company` and prints it.\n\n    Args:\n        company: The name of the company, e.g., Apple Inc.\n    \"\"\"\n    if company == \"Apple, Inc.\":\n        return f\"The current stock price of {company} is $200.\"\n    elif company == \"Google LLC\":\n        return f\"The current stock price of {company} is $180.\"\n    else:\n        return f\"I'm sorry, I don't have the stock price for {company}.\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_current_stock_price])\n@prompt_template(\"What's the stock price of {company}\")\ndef stock_price(company: str): ...\n\n\nresponse = stock_price(\"Apple, Inc.\")\nif tool := response.tool:\n    print(tool.call())\n# > The current stock price of Apple Inc. is $200.\n```\n\n----------------------------------------\n\nTITLE: Model Provider Switching with Mirascope\nDESCRIPTION: Demonstrates how to switch between different LLM providers using Mirascope's override functionality, showing flexibility in model selection and configuration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-integration.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import llm\n\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\") # Calls GPT-4o-mini and parses output as a string\ndef explain(topic: str) -> str:\n    return f\"Explain the concept of {topic} in two sentences.\"\n\n\noutput = explain(\"quantum computing\")\nprint(output)\n\noverride_output = llm.override(\n    recommend_book,\n    provider=\"anthropic\",\n    model=\"claude-3-5-sonnet-20240620\",\n    call_params={\"temperature\": 0.7},\n)(\"quantum computing\")\nprint(override_output)\n```\n\n----------------------------------------\n\nTITLE: Streaming Tools Example\nDESCRIPTION: This code snippet demonstrates how to stream responses with tools using Mirascope. It highlights specific lines related to streaming functionality, showcasing how to integrate tools for long-running tasks or real-time updates with supported providers.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n --8<-- \"build/snippets/learn/tools/streams/{{ tool_method }}/{{ provider | provider_dir }}/{{ method }}.py\"\n\n```\n\n----------------------------------------\n\nTITLE: Multiple Function Definitions with Mirascope\nDESCRIPTION: This snippet shows how to define multiple tools using Mirascope's `BaseTool` class and Pydantic's `Field`. It demonstrates a cleaner and more pythonic approach compared to hand-coding JSON schemas.  The code defines two tools: `GetCurrentStockPrice` and `GetStockPriceInCurrency`.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom mirascope.core import BaseTool\nfrom pydantic import Field\n\n\nclass GetCurrentStockPrice(BaseTool):\n    \"\"\"Get the current stock price of a company.\"\"\"\n\n    company: str = Field(\n        ...,\n        description=\"The name of the company, e.g., 'Apple Inc.' or 'Google LLC'\"\n    )\n\nclass GetStockPriceInCurrency(BaseTool):\n    \"\"\"Get the current stock price of a company in a specific currency\"\"\"\n\n    currency: Literal[\"USD\", \"EUR\", \"JPY\"] = Field(\n        ...,\n        description=\"The currency to get the stock price in, e.g., 'USD', 'EUR', 'JPY'\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Self-Refine with Mirascope for LLMs\nDESCRIPTION: A basic implementation of the Self-Refine technique using Mirascope's OpenAI integration. This implementation includes three functions: one for the initial call, one for evaluating the response with feedback, and one for generating a new response based on the feedback. It demonstrates iterative refinement of LLM outputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/self_refine.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom mirascope.core.openai import OpenAICallResponse\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef call(query: str) -> str:\n    return query\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Here is a query and a response to the query. Give feedback about the answer,\n    noting what was correct and incorrect.\n    Query:\n    {query}\n    Response:\n    {response}\n    \"\"\"\n)\ndef evaluate_response(query: str, response: OpenAICallResponse): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    For this query:\n    {query}\n    The following response was given:\n    {response}\n    Here is some feedback about the response:\n    {feedback}\n\n    Consider the feedback to generate a new response to the query.\n    \"\"\"\n)\ndef generate_new_response(\n    query: str, response: OpenAICallResponse\n) -> openai.OpenAIDynamicConfig:\n    feedback = evaluate_response(query, response)\n    return {\"computed_fields\": {\"feedback\": feedback}}\n\n\ndef self_refine(query: str, depth: int) -> str:\n    response = call(query)\n    for _ in range(depth):\n        response = generate_new_response(query, response)\n    return response.content\n\n\nquery = \"\"\"Olivia has $23. She bought five bagels for $3 each.\nHow much money does she have left?\"\"\"\nprint(self_refine(query, 1))\n```\n\n----------------------------------------\n\nTITLE: Loading and Vectorizing Apollo 11 Journal Documents\nDESCRIPTION: Code for loading text documents from a local directory, creating a vector index for efficient similarity search, and setting up a retriever to find relevant document excerpts\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/rag-application.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndocuments = SimpleDirectoryReader(\"./apollo11_lunar_landing\").load_data()\nretriever = VectorStoreIndex.from_documents(documents).as_retriever()\n```\n\n----------------------------------------\n\nTITLE: Defining solve_next_step Function with Tool Integration\nDESCRIPTION: This snippet defines the `solve_next_step` function, which uses the `@openai.call` and `@prompt_template` decorators from Mirascope. This function is designed to solve individual subproblems provided to it, utilizing the tools defined earlier. It takes a history of chat messages and a query (subproblem) as input, and leverages GPT-4o-mini with the defined tools to generate a solution.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/decomposed_prompting.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"@openai.call(model=\\\"gpt-4o-mini\\\", tools=[split_string_to_words, substring, concat])\n@prompt_template(\n    \\\"\\\"\\\"\n    SYSTEM: You are being fed subproblems to solve the actual problem: {query}\n    MESSAGES: {history}\n    \\\"\\\"\\\"\n)\ndef solve_next_step(history: list[ChatCompletionMessageParam], query: str): ...\"\n```\n\n----------------------------------------\n\nTITLE: Semantic Segmentation Prompt with Mirascope and OpenAI\nDESCRIPTION: This snippet defines a prompt for semantic document segmentation using Mirascope's `prompt_template` and OpenAI's `gpt-4o-mini` model. It also defines a `Segment` Pydantic model to structure the LLM's response into topic and content pairs.  The `semantic_segmentation` function calls the OpenAI API with the specified prompt and response model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/document_segmentation.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"from mirascope.core import openai, prompt_template\\nfrom pydantic import BaseModel, Field\\n\\nSAMPLE_ARTICLE = \\\"\\nThe Rise of Artificial Intelligence in Healthcare: A Comprehensive Overview\\nRegulatory bodies like the FDA are working to develop frameworks for evaluating and approving AI-based medical technologies, balancing the need for innovation with patient safety concerns. From diagnosis to treatment planning, AI is making significant strides in various areas of healthcare, promising to transform the way we approach medicine and patient care in the 21st century. Machine learning models can screen vast libraries of compounds much faster than traditional methods, identifying promising drug candidates for further investigation. These advancements are particularly crucial in regions with a shortage of trained radiologists, as AI can serve as a powerful assistive tool to healthcare providers. As AI continues to evolve, it promises to augment human capabilities in healthcare, allowing for more precise, efficient, and personalized medical care. AI algorithms can identify patterns in patient data that may not be apparent to human clinicians, leading to more precise treatment recommendations. Beyond diagnosis and treatment planning, AI is proving valuable in providing clinical decision support to healthcare providers. Artificial Intelligence (AI) is revolutionizing the healthcare industry, offering unprecedented opportunities to improve patient care, streamline operations, and advance medical research. In patient monitoring, AI algorithms can continuously analyze data from ICU equipment or wearable devices, alerting healthcare providers to subtle changes in a patient's condition before they become critical. For instance, AI-powered systems have shown impressive results in detecting early signs of breast cancer in mammograms, identifying lung nodules in chest X-rays, and spotting signs of diabetic retinopathy in eye scans. At its core, AI in healthcare relies on machine learning algorithms and neural networks that can process vast amounts of medical data. Issues such as data privacy, algorithmic bias, and the need for regulatory frameworks are ongoing concerns that need to be addressed. Companies like Atomwise and Exscientia are already using AI to discover novel drug candidates for various diseases, including COVID-19. This tailored approach has the potential to significantly improve treatment efficacy and reduce adverse effects. One of the most promising applications of AI in healthcare is in medical imaging. Ensuring that AI systems are trained on diverse, representative data and regularly audited for bias is crucial for their equitable implementation. Traditional drug development is a time-consuming and expensive process, often taking over a decade and costing billions of dollars to bring a new drug to market. By analyzing vast amounts of patient data, including genetic information, lifestyle factors, and treatment outcomes, AI systems can help predict which treatments are likely to be most effective for individual patients. These systems are trained on diverse datasets, including electronic health records, medical imaging, genetic information, and even data from wearable devices. AI can also predict potential side effects and drug interactions, helping to prioritize safer compounds earlier in the development process. Additionally, there's a need for healthcare professionals to adapt and acquire new skills to work effectively alongside AI systems. Machine learning algorithms can now analyze X-rays, MRIs, and CT scans with remarkable accuracy, often outperforming human radiologists in detecting certain conditions. While AI will not replace human healthcare providers, it will undoubtedly become an indispensable tool in the medical toolkit, helping to address global healthcare challenges and improve patient outcomes on a massive scale. The sensitive nature of health data requires robust security measures and clear guidelines on data usage and sharing. Emerging areas of research include Natural Language Processing (NLP) for analyzing clinical notes and medical literature, AI-powered robotic surgery assistants for enhanced precision in complex procedures, predictive analytics for population health management and disease prevention, and virtual nursing assistants to provide basic patient care and monitoring. This proactive approach to patient care has the potential to prevent complications and improve outcomes, particularly for chronic disease management. However, the integration of AI in healthcare is not without challenges. As these AI systems learn from more data, they become increasingly accurate and capable of handling complex medical tasks. Algorithmic bias is a particularly pressing issue, as AI systems trained on non-diverse datasets may perform poorly for underrepresented populations. Despite these challenges, the potential benefits of AI in healthcare are immense. For example, in oncology, AI systems are being used to analyze tumor genetics and patient characteristics to recommend personalized cancer treatments. AI-powered systems can analyze molecular structures, predict drug-target interactions, and simulate clinical trials, potentially reducing the time and cost of bringing new drugs to market. This includes understanding the capabilities and limitations of AI tools and interpreting their outputs in the context of patient care. Similarly, in psychiatry, AI is helping to predict patient responses to different antidepressants, potentially reducing the trial-and-error approach often used in mental health treatment. As technology continues to advance, we can expect to see even more innovative applications of AI that will shape the future of medicine and improve patient outcomes worldwide.\\n\\\"\\n\\n\\nclass Segment(BaseModel):\\n    topic: str = Field(..., description=\\\"The topic of the section.\\\")\\n    content: str = Field(..., description=\\\"The content that relates to the topic.\\\")\\n\\n\\n@openai.call(\\\"gpt-4o-mini\\\", response_model=list[Segment])\\n@prompt_template(\\n    \\\"\\n    SYSTEM:\\n    You are an expert in document semantic segmentation.\\n    Can you segment the following article into coherent secttions based on topic?\\n\\n    USER:\\n    {article}\\n    \\\"\\n)\\ndef semantic_segmentation(article: str): ...\"\n```\n\n----------------------------------------\n\nTITLE: Multi-Class Classification - Sentiment Analysis with Mirascope - Python\nDESCRIPTION: This snippet demonstrates multi-class text classification by setting up a sentiment analysis task. It defines an Enum for sentiment labels and uses Mirascope to classify the sentiment of given texts accordingly.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_classification.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum\n\n\nclass Sentiment(Enum):\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n    POSITIVE = \"positive\"\n\n\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Sentiment)\ndef classify_sentiment(text: str) -> str:\n    return f\"Classify the sentiment of the following text: {text}\"\n\n\ntext = \"I hate this product. It's terrible.\"\nlabel = classify_sentiment(text)\nassert label == Sentiment.NEGATIVE\n\ntext = \"I don't feel strongly about this product.\"\nlabel = classify_sentiment(text)\nassert label == Sentiment.NEUTRAL\n\ntext = \"I love this product. It's amazing!\"\nlabel = classify_sentiment(text)\nassert label == Sentiment.POSITIVE\n```\n\n----------------------------------------\n\nTITLE: Example Interaction with OnboardingBot\nDESCRIPTION: This snippet showcases a sample interaction with the OnboardingBot where a user asks about the number of paid vacation days included in the benefits package. The expected response is a succinct answer based on the company's onboarding documentation. The input includes a user prompt followed by an expected assistant output.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\n(User): How many paid vacation days are included in our benefits package?\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n(Assistant): Our standard plan includes 15 days of paid vacation per year. ...\n```\n\n----------------------------------------\n\nTITLE: Using Computed Fields in Templates - Python\nDESCRIPTION: In this snippet, using Mirascope's prompt_template decorator allows for a templated response for book recommendations, dynamically adjusting based on age category. It shows how to integrate computed fields into structured prompts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book with a reading level of {reading_level}\")\ndef recommend_book_by_age(genre: str, age: int) -> openai.OpenAIDynamicConfig:\n    reading_level = \"adult\"\n    if age < 12:\n        reading_level = \"elementary\"\n    elif age < 18:\n        reading_level = \"young adult\"\n    return {\"computed_fields\": {\"reading_level\": reading_level}}\n\n\nresponse = recommend_book_by_age(\"fantasy\", 15)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Mirascope Function Chaining with Computed Fields\nDESCRIPTION: Shows Mirascope's approach to chaining LLM calls using Python's functional syntax and computed_fields for propagating values through the chain. Includes expert recommendation system example.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\ndef get_brand_expert(vehicle_type: str) -> str:\n    return f\"Name a {vehicle_type} vehicle expert. Return only their name.\"\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Imagine that you are the expert {brand_expert}.\n    Your task is to recommend cars that you, {brand_expert}, would be excited to suggest.\n\n    USER:\n    Recommend a {vehicle_type} car that fits the following criteria: {criteria}.\n    \"\"\"\n)\ndef recommend_car(vehicle_type: str, criteria: str) -> openai.OpenAIDynamicConfig:\n    brand_expert = get_brand_expert(vehicle_type=vehicle_type)\n    return {\"computed_fields\": {\"brand_expert\": brand_expert}}\n\n\nresponse = recommend_car(\n    vehicle_type=\"electric\", criteria=\"best range and safety features\"\n)\nprint(response.content)\n# > Certainly! Here's a great electric car with the best range and safety features: ...\n```\n\n----------------------------------------\n\nTITLE: Conditional Prompt Chain Implementation with Mirascope\nDESCRIPTION: This snippet demonstrates a conditional prompt chain using Mirascope. It defines two functions, `classify_trend` and `analyze_trend`, to classify a trend as emerging or declining, and then generate an analysis based on that classification. It utilizes `prompt_template` and `llm.call` decorators to define and execute prompts with OpenAI.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-chaining.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\n# Conditional chain\nfrom enum import Enum\n\nfrom mirascope.core import openai, prompt_template\n\nclass TrendCategory(str, Enum):\n    EMERGING = \\\"emerging\\\"\n    DECLINING = \\\"declining\\\"\n\n@llm.call(provider=\\\"openai\\\", model=\\\"gpt-4o-mini\\\", response_model=TrendCategory)\n@prompt_template(\\\"Does the following market trend appear to be emerging or declining? {trend_description}\\\")\ndef classify_trend(trend_description: str): ...\n\n@llm.call(provider=\\\"openai\\\", model=\\\"gpt-4o-mini\\\")\n@prompt_template(\n    \\\"\\\"\\n    SYSTEM:\n    Your task is to analyze a B2B market trend.\n    The trend has been identified as {trend_category}.\n\n    USER: Provide a {trend_analysis_type} for the following trend: {trend_description}\n    \\\"\\\"\\n)\ndef analyze_trend(trend_description: str) -> openai.OpenAIDynamicConfig:\n    trend_category = classify_trend(trend_description)\n    trend_analysis_type = (\n        \\\"growth opportunity analysis\\\"\n        if trend_category == TrendCategory.EMERGING\n        else \\\"risk mitigation strategy\\\"\n    )\n    return {\n        \\\"computed_fields\\\": {\n            \\\"trend_category\\\": trend_category,\n            \\\"trend_analysis_type\\\": trend_analysis_type,\n        }\n    }\n\n\nemerging_trend_analysis = analyze_trend(\n    trend_description=\\\"More enterprises are adopting AI-driven automation for workflow optimization.\\\"\n)\nprint(emerging_trend_analysis)\n# > This trend represents a significant growth opportunity as enterprises seek greater efficiency through AI-driven automation...\nprint(emerging_trend_analysis.user_message_param)\n# > {'content': \\\"Provide a growth opportunity analysis for the following trend: More enterprises are adopting AI-driven automation for workflow optimization.\\\", 'role': 'user'}\n\ndeclining_trend_analysis = analyze_trend(\n    trend_description=\\\"Traditional on-premise software solutions are being replaced by cloud-based SaaS offerings.\\\"\n)\nprint(declining_trend_analysis)\n# > This trend indicates a decline in demand for on-premise software, requiring businesses to adapt with cloud migration strategies...\nprint(declining_trend_analysis.user_message_param)\n# > {'content': \\\"Provide a risk mitigation strategy for the following trend: Traditional on-premise software solutions are being replaced by cloud-based SaaS offerings.\\\", 'role': 'user'}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Mirascope Prompt for Astronaut-like Responses\nDESCRIPTION: Defining a function with a Mirascope prompt template that retrieves relevant journal excerpts and generates a response in the style of an Apollo 11 astronaut, using OpenAI's GPT model\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/rag-application.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"\"\"\n    SYSTEM:\n    Your task is to respond to the user as though you are one of the Apollo 11 astronauts.\n\n    Here are some excerpts from the Apollo 11 Lunar Surface Journal relevant to the user query.\n    Use them as a reference for how to respond.\n\n    <excerpts>\n    {excerpts}\n    </excerpts>\n\n    USER: {query}\n    \"\"\")\ndef ask_apollo_astronaut(query: str, retriever: BaseRetriever) -> openai.OpenAIDynamicConfig:\n    \"\"\"Retrieves excerpts from the Apollo 11 Lunar Surface Journal relevant to `query` and generates a response.\"\"\"\n    excerpts = [node.get_content() for node in retriever.retrieve(query)]\n    return {\"computed_fields\": {\"excerpts\": excerpts}}\n```\n\n----------------------------------------\n\nTITLE: Iterative Chaining in Mirascope\nDESCRIPTION: Shows how to implement iterative refinement using Mirascope chains. This technique involves repeatedly improving an output through multiple passes until a condition is met.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/chaining.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/chaining/advanced_techniques/iterative_chaining.py\"\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Tools with Mirascope in Python\nDESCRIPTION: Shows how to define and use tools with Mirascope to extend LLM capabilities. This example defines a weather tool and uses it within an LLM call to provide current weather information for a given location.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n):\n    \"\"\"Get the current weather in a given location.\"\"\"\n    if \"tokyo\" in location.lower():\n        print(f\"It is 10 degrees {unit} in Tokyo, Japan\")\n    elif \"san francisco\" in location.lower():\n        print(f\"It is 72 degrees {unit} in San Francisco, CA\")\n    elif \"paris\" in location.lower():\n        print(f\"It is 22 degrees {unit} in Paris, France\")\n    else:\n        print(\"I'm not sure what the weather is like in {location}\")\n\n\n@openai.call(model=\"gpt-4o\", tools=[get_current_weather])\ndef forecast(city: str) -> str:\n    return f\"What's the weather in {city}?\"\n\n\nresponse = forecast(\"Tokyo\")\nif tool := response.tool:\n    tool.call()\n```\n\n----------------------------------------\n\nTITLE: Using JSON Mode for LLM Output Parsing\nDESCRIPTION: Demonstrates how to use JSON mode with LLMs to get structured output directly. The highlighted lines show the model configuration for JSON mode and the parsing implementation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/output_parsers.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/output_parsers/additional_examples/{{ provider | provider_dir }}/json_mode.py\"\n```\n\n----------------------------------------\n\nTITLE: Tool Few-Shot Examples\nDESCRIPTION: This code snippet demonstrates how to incorporate few-shot examples into your tools to enhance their behavior. It shows two methods for adding examples, resulting in the same tool schema with examples included, thereby providing context for the LLM to improve tool usage.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n --8<-- \"build/snippets/learn/tools/few_shot_examples/{{ tool_method }}/{{ provider | provider_dir }}/{{ method }}.py\"\n\n```\n\n----------------------------------------\n\nTITLE: Ingesting and Embedding Onboarding Documents with LlamaIndex\nDESCRIPTION: Loads onboarding documents from a directory, generates vector embeddings using the configured embedding model, and persists the index for future use, creating a searchable knowledge base for the chatbot.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.storage import StorageContext\nfrom llama_index.core.vector_stores import SimpleVectorStore\n\n# Replace \"PATH/TO/ONBOARDING_DOCS\" with the directory containing your onboarding text/files\ndocuments = SimpleDirectoryReader(\"PATH/TO/ONBOARDING_DOCS\").load_data()\nvector_store = SimpleVectorStore()\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# Build an index from the documents\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\n# Persist the index and vector store so we can reload it later\nindex.storage_context.persist()\n```\n\n----------------------------------------\n\nTITLE: Validating LLM Inputs with Pydantic and Mirascope\nDESCRIPTION: This snippet shows how to validate user inputs before sending them to an LLM using Pydantic's validation capabilities integrated with Mirascope. It defines constraints for function parameters and catches validation errors to ensure data quality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\n\nfrom mirascope.core import openai\nfrom pydantic import Field, ValidationError, validate_call\n\n\n# Define the function to call the LLM with validated inputs\n@openai.call(\"gpt-4o-mini\")\n@validate_call  # ensure the arguments are valiadated\ndef recommend_book(genre: Annotated[str, Field(min_length=3)], audience: str) -> str:\n    return f\"Recommend a book in the {genre} genre for {audience}.\"\n\n\n# Example usage\ndef main():\n    try:\n        # If validation passes, the LLM call will be made\n        response = recommend_book(\"fantasy\", \"young adult\")\n        print(response)\n    except ValidationError as e:\n        # Handle validation errors\n        print(f\"Validation Error: {e.json()}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Solving Problems with Self-Consistency Approach\nDESCRIPTION: This snippet demonstrates the Self-Consistency method which involves generating multiple LLM responses to a single question and choosing the most common as the answer. \nIt includes several methods of solving a mathematical problem, where the most frequent answer is considered the most reliable. \nDependencies involve having the capability to generate and compare multiple responses simultaneously.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nOlivia has $23. She bought five bagels for $3 each. How much money does she have left? Please solve this problem by thinking step by step. Generate multiple answers using different approaches or considerations. After generating several responses, select the most frequent answer and explain why it is the best choice.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nStep-by-step analysis:\nApproach 1: Direct Subtraction Method\n1. Initial money: Olivia has $23.\n2. Cost of one bagel: Each bagel costs $3.\n3. Total number of bagels: She buys 5 bagels.\n4. Total cost of the bagels: 5 x 3 = 15 dollars.\n5. Remaining money: 23 − 15 = 8 dollars.\nApproach 2: Grouping method\n1. Initial money: Olivia starts with $23.\n2. Break down the expenses:\n- Buying two bagels: 2 × 3 = 6 dollars.\n- Buying three more bagels: 3 × 3 = 9 dollars.\n3. Total cost: 6 + 9 = 15 dollars.\n4. Remaining money: 23 − 15 = 8 dollars.\nApproach 3: Counting backward\n1. Start with $23.\n2. Subtract $3 for each bagel:\n- After buying the first bagel: 23 − 3 = 20.\n- After the second bagel: 20 − 3 = 17.\n- After the third bagel: 17 − 3 =14.\n- After the fourth bagel: 14 − 3 = 11.\n- After the fifth bagel: 11 − 3 = 8.\n3. Olivia has $8 left.\nApproach 4: Algebraic approach\nLet x represent Olivia’s remaining money after buying 5 bagels.\n1. Initial amount: $23.\n2. Total cost: 3 × 5 = 15 dollars.\n3. Equation: 23 − 15 = x.\n4. Solve for x:x = 8.\nAnswer Evaluation:\nAll four approaches conclude that Olivia has $8 left.\nConclusion:\nThe most frequent and correct answer is $8. All approaches, whether direct subtraction, grouping, counting backward, or algebraic reasoning, reach the same result because they all properly account for the $15 spent on the bagels. This consistency makes $8 the best and accurate answer.\n```\n\n----------------------------------------\n\nTITLE: Implementing Librarian Class for SQL Operations\nDESCRIPTION: This extensive code snippet defines the Librarian class, which handles SQL operations and LLM interactions. It includes methods for running queries, executing updates, and processing user inputs using OpenAI's GPT model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_sql_agent.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sqlite3\nfrom typing import ClassVar\nfrom unittest.mock import MagicMock\n\nimport ipytest\nimport pytest\nfrom mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel, ConfigDict\n\nipytest.autoconfig(run_in_thread=True)\n\n\nclass Librarian(BaseModel):\n    con: ClassVar[sqlite3.Connection] = sqlite3.connect(\"database.db\")\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def _run_query(self, query: str) -> str:\n        \"\"\"A SELECT query to run.\"\"\"\n        print(query)\n        try:\n            cursor = self.con.cursor()\n            cursor.execute(query)\n            res = cursor.fetchall()\n            return str(res)\n        except sqlite3.Error as e:\n            return str(e)\n\n    def _execute_query(self, query: str) -> str:\n        \"\"\"An INSERT, UPDATE, or DELETE query to execute.\"\"\"\n        print(query)\n        try:\n            cursor = self.con.cursor()\n            cursor.execute(query)\n            rows_affected = cursor.rowcount\n            self.con.commit()\n            if rows_affected > 0:\n                return f\"Query executed successfully, {rows_affected} row(s) were updated/inserted.\"\n            else:\n                return \"No rows were updated/inserted.\"\n        except sqlite3.Error as e:\n            print(e)\n            return str(e)\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are a friendly and knowledgeable librarian named Mira. Your role is to \n        assist patrons with their queries, recommend books, \n        and provide information on a wide range of topics.\n\n        Personality:\n            - Warm and approachable, always ready with a kind word\n            - Patient and understanding, especially with those who are hesitant or confused\n            - Enthusiastic about books and learning\n            - Respectful of all patrons, regardless of their background or level of knowledge\n\n        Services:\n            - Keep track of patrons' reading lists using a SQLite database. Assume that the user is non technical and will ask you\n        questions in plain English.\n            - Recommend books based on the user's preferences\n        Your task is to write a query based on the user's request.\n\n        The database schema is as follows:\n\n        TABLE ReadingList (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            title TEXT NOT NULL,\n            status TEXT CHECK(status IN ('Not Started', 'In Progress', 'Complete')) NOT NULL,\n            rating INTEGER CHECK(rating >= 1 AND rating <= 5),\n        );\n\n        You must interpret the user's request and write the appropriate SQL query to\n        pass in the tools.\n\n        Example interactions:\n            1. Select\n                - USER: \"Show me all books.\"\n                - ASSISTANT: \"SELECT * FROM ReadingList;\"\n            2. Insert\n                - USER: \"Add Gone with the Wind to my reading list.\"\n                - ASSISTANT: \"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\"\n            3. Update\n                - USER: \"I just finished Gone with the Wind, can you update the status, and give it 5 stars??\"\n                - ASSISTANT: \"UPDATE ReadingList SET status = 'Complete' and rating = 5 WHERE title = 'Gone with the Wind';\"\n            4. Delete\n                - USER: \"Remove Gone with the Wind from my reading list.\"\n                - ASSISTANT: \"DELETE FROM ReadingList WHERE title = 'Gone with the Wind';\"\n\n        If field are not mentioned, omit them from the query.\n        All queries must end with a semicolon.\n\n        You have access to the following tools:\n        - `_run_query`: When user asks for recommendations, you can use this tool to see what they have read.\n        - `_execute_query`: Use the query generated to execute an \n            INSERT, UPDATE, or DELETE query.\n\n        You must use these tools to interact with the database.\n\n        MESSAGES: {self.messages}\n        USER: {query}\n        \"\"\"\n    )\n    async def _stream(self, query: str) -> openai.OpenAIDynamicConfig:\n        return {\"tools\": [self._run_query, self._execute_query]}\n\n    async def _step(self, question: str):\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._step(question)\n            print()\n```\n\n----------------------------------------\n\nTITLE: Response Details Model\nDESCRIPTION: Defines a Pydantic model `ResponseDetails` to structure the output of response evaluation. It includes fields for the solution number and the probability of correctness, along with descriptions and validation constraints using Pydantic's `Field`.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/diverse.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\nclass ResponseDetails(BaseModel):\n    solution_number: int = Field(\n        ..., description=\"The actual number given as the answer in a solution.\"\n    )\n    correctness_probability: float = Field(\n        ...,\n        ge=0,\n        le=1,\n        description=\"An estimated probability that the given solution is correct from 0.0 to 1.0\",\n    )\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Binary Classification - Spam Detection with Mirascope - Python\nDESCRIPTION: This snippet implements a spam detection classifier using Mirascope and the OpenAI API by defining a function that prompts the model to classify a given text as spam or not spam. It uses boolean response modeling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_classification.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", response_model=bool)\ndef classify_spam(text: str) -> str:\n    return f\"Classify the following text as spam or not spam: {text}\"\n\n\ntext = \"Would you like to buy some cheap viagra?\"\nlabel = classify_spam(text)\nassert label is True  # This text is classified as spam\n\ntext = \"Hi! It was great meeting you today. Let's stay in touch!\"\nlabel = classify_spam(text)\nassert label is False  # This text is classified as not spam\n```\n\n----------------------------------------\n\nTITLE: Chaining LLM Calls in Mirascope with OpenAI\nDESCRIPTION: This code snippet demonstrates how Mirascope performs implicit chaining of two LLM calls to suggest travel destinations. It uses OpenAI's API along with a prompt template. Dependencies include the mirascope.core library.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\\n\\n@openai.call(model=\"gpt-4o-mini\")\\n@prompt_template(\\n    \"Name a travel guide who is very knowledgeable about {region_type} regions\"\\n)\\ndef select_guide(region_type: str): ...\\n\\n@openai.call(model=\"gpt-4o-mini\")\\n@prompt_template(\\n    \"\"\"\\n    SYSTEM:\\n    Imagine that you are the guide {guide}.\\n    Your task is to recommend travel destinations that you, {guide}, would be excited to showcase.\\n\\n    USER:\\n    Recommend a {region_type} destination that includes {feature}.\\n    \"\"\"\\n)\\ndef recommend_destination(region_type: str, feature: str) -> openai.OpenAIDynamicConfig:\\n    guide = select_guide(feature)\\n    return {\"computed_fields\": {\"guide\": guide}}\\n\\nprint(recommend_destination(region_type=\"European\", feature=\"historic sites\"))\n```\n\n----------------------------------------\n\nTITLE: Colocating Prompts with LLM Calls in Mirascope\nDESCRIPTION: Demonstrates Mirascope's approach to organizing LLM interactions by colocating prompts with call parameters. The example shows a function for getting weather information with a tool integration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_weather(city: str) -> str:\n    if city == \"Tokyo\":\n        return \"85 degrees\"\n    else:\n        return \"80 degrees\"\n\n\n@openai.call(model=\"gpt-4o-mini\", tools=[get_weather])\n@prompt_template(\"What's the weather in {city}\")\ndef get_city_weather(city: str): ...\n\n\nresponse = get_city_weather(city=\"Tokyo\")\nif tool := response.tool:\n    print(tool.call())\n```\n\n----------------------------------------\n\nTITLE: Updating Mirascope Chatbot to use WebSearch Tool\nDESCRIPTION: This snippet updates the `Chatbot` class to incorporate the `WebSearch` tool using Mirascope and OpenAI. It adds the `WebSearch` tool to the `tools` parameter of `@openai.call`. The `_step` function is modified to handle tool calls and outputs, updating the conversation history accordingly.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Chatbot(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch])\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. \n        Your task is to answer the user's question using the provided tools.\n        You have access to the following tools:\n            - `WebSearch`: Search the web for information.\n            - `RequestAssistance`: Request assistance from a human expert if you do not\n                know how to answer the question.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness. The goal is to\n        provide as much information to the writer as possible without overwhelming them.\n\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    def _call(self, question: str | None = None): ...\n\n    def _step(self, question: str | None = None):\n        response = self._call(question)\n        tools_and_outputs = []\n        for chunk, tool in response:\n            if tool:\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.history.append(response.user_message_param)\n        self.history.append(response.message_param)\n        if tools_and_outputs:\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step()\n        return response.content\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question in [\"quit\", \"exit\"]:\n                print(\"(Assistant): Have a great day!\")\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(question)\n            print(\"\")\n\n\nChatbot().run()\n# Prompt:\n\"\"\"\n(User): Can you tell me about the Mirascope python library?\n(Assistant): The **Mirascope** library is a Python toolkit designed for creating applications using language model (LLM) APIs. Developed by William Bakst and released on August 18, 2024, Mirascope emphasizes simplicity, elegance, and developer experience. Here are some key features and details about the library:\n\n### Key Features\n1. **Simplicity and Ease of Use**: Mirascope aims to provide straightforward abstractions that enhance the developer experience without overwhelming complexity. It is designed for ease of onboarding and development.\n\n2. **Type Safety**: One of its strengths is the provision of proper type hints throughout the library. It actively manages Python typings, allowing developers to write their code intuitively while still benefiting from type safety.\n\n3. **Modular Design**: Mirascope is modular and extensible, enabling developers to tailor the library to their specific needs. Most dependencies are optional and provider-specific, so you can include only the components you require.\n\n4. **Core Primitives**: The library offers two main components:\n   - **Call and BasePrompt**: These primitives facilitate interactions with LLMs. Developers can create functions that integrate seamlessly with multiple LLM providers through decorators.\n\n5. **Advanced Functionality**: Mirascope supports features like asynchronous function calls, streaming responses, structured data extraction, custom output parsers, and dynamic variable injection.\n\n6. **Integration with FastAPI**: Mirascope includes decorators for wrapping functions into FastAPI routes, making it easier to deploy applications as web services.\n\n7. **Documentation and Examples**: The project comes with extensive usage documentation and example code to help users quickly understand how to utilize its features effectively.\n\n### Installation\nTo install Mirascope, you can use the following command:\n```bash\npip install mirascope\n```\n### Compatibility\nMirascope is compatible with Python versions 3.10 to 3.11 (not supporting Python 4.0 and above) and is licensed under the MIT License.\n\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Diverse Function\nDESCRIPTION: Defines the core `diverse` function that orchestrates the DiVeRSe technique. It generates prompt variations, creates reasoning chains using chain-of-thought reasoning, evaluates each reasoning chain, and selects the best response based on the evaluation scores. It returns the best response extracted from evaluated responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/diverse.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\nimport asyncio\n\nasync def diverse(query: str, num_variations: int) -> str:\n    # Gather the variations of the prompt\n    alternate_variations = get_prompt_variations(query, num_variations - 1)\n    all_variations = alternate_variations.variations + [query]\n\n    # Generate a unique reasoning chain for each prompt variation with CoT\n    cot_tasks = [zero_shot_cot(prompt) for prompt in all_variations]\n    cot_responses = [response.content for response in await asyncio.gather(*cot_tasks)]\n\n    # Evaluate each reasoning chain\n    eval_tasks = [\n        evaluate_response(query, cot_response) for cot_response in cot_responses\n    ]\n    eval_responses = await asyncio.gather(*eval_tasks)\n\n    response_scores = {}\n    for eval_response in eval_responses:\n        if eval_response.solution_number not in response_scores:\n            response_scores[eval_response.solution_number] = 0\n        response_scores[eval_response.solution_number] += (\n            eval_response.correctness_probability\n        )\n    best_response = max(response_scores.keys(), key=lambda k: response_scores[k])\n    return best_response\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Mirascope with Langfuse for Structured LLM Outputs in Python\nDESCRIPTION: This example shows how to use Mirascope's response_model feature with Langfuse integration. It defines a TaskDetails model, uses it in an LLM function to extract structured data from a task description, and automatically logs the structured output to Langfuse.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langfuse-integration.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom mirascope.integrations.langfuse import with_langfuse\nfrom mirascope.core import openai\n\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\nos.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"\n\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\n\n@with_langfuse()\n@openai.call(\"gpt-4o-mini\", response_model=TaskDetails)\ndef extract_task_details(task: str) -> str:\n    return f\"Extract task details from this task: {task}\"\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = extract_task_details(task)  # this will be logged automatically with langfuse \nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n```\n\n----------------------------------------\n\nTITLE: Implementing Function Calling with Mirascope Tools\nDESCRIPTION: This code shows how to extend model capabilities by adding tools (function calling) to workflows using Mirascope. It demonstrates how a function with a docstring can be automatically converted into a tool schema and object for use in LLM interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-tools.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_stock_price(ticker: str) -> str:\n   \"\"\"Fetches the current stock price for the given `ticker`.\n\n   Args:\n       ticker: The stock ticker symbol (e.g., \"AAPL\" for Apple Inc.).\n   \"\"\"\n   if ticker == \"AAPL\":\n       return f\"The current stock price of {ticker} is $150.\"\n   elif ticker == \"GOOGL\":\n       return f\"The current stock price of {ticker} is $2800.\"\n   else:\n       return f\"I'm sorry, I don't have the stock price for {ticker}.\"\n\n\n@openai.call(model=\"gpt-4o\", tools=[get_stock_price])\n@prompt_template(\"What's the stock price for {stock}?\")\ndef check_stock_price(stock: str): ...\n\n\nresponse = check_stock_price(\"AAPL\")\nif tool := response.tool:\n   print(tool.call())\n# > The current stock price of AAPL is $150.\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt Chaining with Computed Fields in Mirascope\nDESCRIPTION: Demonstrates prompt chaining using computed fields in Mirascope, where output from one prompt is used as input for another. Includes scientist selection and theory explanation using dynamic configuration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-chaining.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Name a scientist who is known for their work in {field_of_study}.\n    Give me just the name.\n    \"\"\"\n)\ndef select_scientist(field_of_study: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Imagine that you are {scientist}.\n    Your task is to explain a theory that you, {scientist}, are famous for.\n\n    USER:\n    Explain the theory related to {topic}.\n    \"\"\"\n)\ndef explain_theory(field_of_study: str, topic: str) -> openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"scientist\": select_scientist(field_of_study=field_of_study)\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Storing Embeddings in VectorStore\nDESCRIPTION: This code reads documents, creates embeddings, and stores them in a local vectorstore. It uses LlamaIndex components for document processing and embedding creation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/documentation_agent.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n)\nfrom llama_index.core.extractors import TitleExtractor\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.storage import StorageContext\nfrom llama_index.core.vector_stores import SimpleVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\ndocuments = SimpleDirectoryReader(\"../../../docs/learn\").load_data()\nvector_store = SimpleVectorStore()\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=512, chunk_overlap=128),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ],\n    vector_store=vector_store,\n)\n\nnodes = pipeline.run(documents=documents)\nindex = VectorStoreIndex(\n    nodes,\n    storage_context=storage_context,\n)\n\nindex.storage_context.persist()\n```\n\n----------------------------------------\n\nTITLE: Loading and Chunking Web Content with LangChain in Python\nDESCRIPTION: This snippet shows how to load content from a web page and split it into smaller chunks using LangChain's WebBaseLoader and RecursiveCharacterTextSplitter. It prepares the data for further processing in a RAG pipeline.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-pipeline.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nfrom mirascope.core import openai\n\n# Load and chunk the contents of the blog.\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n  ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Compatibility with Ollama\nDESCRIPTION: Example of using the openai.call decorator with a custom client to interact with Ollama models through their OpenAI-compatible API. This allows using all of Mirascope's features with local models.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/local_models.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nfrom mirascope import openai\n\n\n# Set the base URL for the OpenAI client to point to Ollama's OpenAI-compatible endpoint\nost.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:11434/v1\"\nclient = OpenAI(api_key=\"ollama\")\n\n\n@openai.call(client=client, model=\"llama2\")\ndef get_capital(country: str) -> str:\n    \"\"\"\n    Get the capital city of a country.\n    \n    Args:\n        country: The name of the country to get the capital of.\n    \n    Returns:\n        The capital city of the country.\n    \"\"\"\n\n\ndef main() -> None:\n    # Call the function with the country to get its capital\n    capital = get_capital(country=\"France\")\n    print(f\"The capital of France is {capital}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Using Output Parsers in Python with Mirascope\nDESCRIPTION: Illustrates how to implement an output parser in Mirascope, enabling structured outputs while incorporating custom parsing logic using regular expressions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/structured_outputs.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport re\n\nfrom mirascope.core import prompt_template\n\n\ndef parse_cot(response: openai.OpenAICallResponse) -> tuple[str, str]:\n    pattern = r\"<thinking>(.*?)</thinking>.*?<output>(.*?)</output>\"\n    match = re.search(pattern, response.content, re.DOTALL)\n    if not match:\n        return \"\", response.content\n    else:\n        return match.group(1).strip(), match.group(2).strip()\n\n\n@openai.call(\"gpt-4o-mini\", output_parser=parse_cot)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    First, output your thought process in <thinking> tags.\n    Then, provide your final output in <output> tags.\n\n    USER: {query}\n    \"\"\"\n)\ndef cot(query: str): ...\n\n\noutput = cot(\n    \"How many tennis balls does Roger have if he started with 2 and bought 3 tubes?\"\n)\nprint(f\"Thinking: {output[0]}\")\nprint(f\"Output: {output[1]}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Parameterized Helpfulness Evaluation Prompt Template\nDESCRIPTION: This snippet defines a reusable prompt template for evaluating the helpfulness of text. It includes detailed criteria for scoring on a 0-5 scale and instructions for providing reasoning. The template is decorated with Mirascope's prompt_template for easy reuse.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-as-judge.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@prompt_template()\ndef evaluate_helpfulness(text: str) -> str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is helpful if it contains content that is:\n        - Supportive, constructive, and fosters understanding or learning\n        - Clear, concise, and provides actionable advice or useful information\n        - Encouraging, respectful, and empathetic towards others' situations\n        - Addressing the query or problem in a relevant and meaningful way\n\n        Use the following scoring criteria:\n        0 - Not helpful at all; completely irrelevant or dismissive\n        1 - Slightly helpful; some effort to assist but limited relevance or clarity\n        2 - Moderately helpful; partial solutions or incomplete advice\n        3 - Helpful; clear, constructive, and relevant\n        4 - Very helpful; thorough, actionable, and supportive\n        5 - Extremely helpful; exceptional support or guidance\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Provider-Agnostic LLM Calls with Mirascope\nDESCRIPTION: Demonstrates how to create provider-agnostic prompts that can be used with different LLM providers. The code defines a reusable prompt template and then applies it to both OpenAI and Anthropic models with minimal code duplication.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/WHY.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic, openai, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\n# OpenAI\nopenai_model = \"gpt-4o-mini\"\nopenai_recommend_book = openai.call(openai_model)(recommend_book_prompt)\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\n# Anthropic\nanthropic_model = \"claude-3-5-sonnet-20240620\"\nanthropic_recommend_book = anthropic.call(anthropic_model)(recommend_book_prompt)\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n```\n\n----------------------------------------\n\nTITLE: Resilient Extraction with Mirascope and Tenacity\nDESCRIPTION: Shows how to use Mirascope with Tenacity for resilient extraction, allowing retries and learning from previous errors. It uses decorators for retry logic and error collection.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import BaseModel, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@openai.call(model=\"gpt-4o\", response_model=Book)\n    \"\"\"\n    {previous_errors}\n    Extract the book details from the following book:\n    {book}\n    \"\"\"\ndef extract_book_details(\n    book: str, *, errors: list[ValidationError] | None = None\n) -> str:\n    if errors:\n        return f\"Previous errors: {errors}. Extract the book from {text}\"\n    return f\"Extract the book from {text}\"\n```\n\n----------------------------------------\n\nTITLE: Exploring Multiple Solutions with Tree of Thought\nDESCRIPTION: This snippet showcases the Tree-of-Thought (ToT) prompting method, allowing the model to consider multiple potential solutions to a problem and evaluate the pros and cons of each before reaching a conclusion.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nDesign a new smartphone model for the mid-range market. Generate at least three different design approaches, evaluating the pros and cons of each, and then recommend the best design based on your analysis.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nApproach 1: Minimalist Design\nPros: Simple, sleek design with fewer distractions. Lower production costs due to fewer components. Appeals to users who prefer clean, user-friendly interfaces.\nCons: Lacks premium features that may attract tech enthusiasts. Limited room for differentiation from other brands. May not appeal to customers who prioritize cutting-edge technology.\n\nApproach 2: Camera-Centric Design\nPros: High-quality camera system with multiple lenses and advanced AI features for photography. Appeals to users who prioritize mobile photography and content creation. Strong marketing potential through the camera's performance.\nCons: Higher production costs due to the advanced camera hardware. Might sacrifice other features like battery life or overall performance. May alienate customers who don't prioritize camera quality.\nApproach 3: Battery Life and Durability Focus\n\nPros: Large battery for extended usage, rugged build for durability, and water resistance. Appeals to users who need a reliable device for work, travel, or outdoor activities. Differentiates itself from other models based on reliability.\nCons: Bulky design, which may not be aesthetically appealing. Sacrifices some of the cutting-edge features like high refresh rate displays or advanced camera systems.\nFinal Conclusion:\nAfter evaluating the pros and cons, the Camera-Centric Design seems the best option for the mid-range market. While it comes at a slightly higher cost, the advanced camera features will appeal to content creators and tech enthusiasts, making it a strong differentiator in a competitive space.\n```\n\n----------------------------------------\n\nTITLE: Well-Structured Summarization Prompt in Python\nDESCRIPTION: Using Python, this code snippet provides a well-structured prompt for summarizing an article. It includes instruction, text, example text, and example summary to guide the model for precise output. Dependencies include a Python language model capable of understanding complex prompts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-prompt.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nInstruction: \"Summarize the following article.\"\n\nText: \"The rapid melting of polar ice caps has accelerated in recent years, leading to a significant rise in global sea levels. Coastal cities around the world are facing increased flooding risks, with some communities already experiencing regular inundations during high tides. Scientists warn that without immediate action to reduce greenhouse gas emissions, these trends will continue, putting millions of people at risk of displacement. Moreover, the economic impact on these regions could be devastating, with billions of dollars in property damage and the loss of vital infrastructure. Governments and international organizations are now pushing for more aggressive climate policies to mitigate these effects and protect vulnerable populations.\"\n\nExample text: \"New research indicates that urban air pollution levels have risen sharply over the past decade. Major cities are now experiencing more frequent 'smog days,' with adverse effects on public health, particularly for those with respiratory conditions. Despite growing awareness, many cities struggle to implement effective air quality regulations due to economic and political constraints.\"\n\nExample Summary: \"The article explains the rise in urban air pollution levels, the increase in 'smog days,' and the challenges cities face in enforcing air quality regulations.\"\n```\n\n----------------------------------------\n\nTITLE: Creating LLM Call with Integrated Tool\nDESCRIPTION: This snippet showcases how to create an LLM call that incorporates the search tool. It instructs the LLM to rewrite the user question to optimize search results.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/search_with_sources.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\n    model=\"gpt-4o-mini\",\n    tools=[nimble_google_search],\n    call_params={\"tool_choice\": \"required\"},\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an expert at finding information on the web.\n    Use the `nimble_google_search` function to find information on the web.\n    Rewrite the question as needed to better find information on the web.\n\n    USER:\n    {question}\n    \"\"\"\n)\ndef search(question: str): ...\n\n```\n\n----------------------------------------\n\nTITLE: Aggregating Answers using Mirascope and OpenAI\nDESCRIPTION: This snippet defines an asynchronous function `aggregate_answers` that takes a query and the number of responses as input. It calls the `answer` function multiple times (specified by `num_responses`) and aggregates the responses using `asyncio.gather`. This demonstrates the core principle of demonstration ensembling, where multiple responses are generated and then combined.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/demonstration_ensembling.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Take the following responses from an LLM and aggregate/average them into\n    one answer.\n    {responses}\n    \"\"\"\n)\nasync def aggregate_answers(\n    query: str, num_responses: int\n) -> openai.OpenAIDynamicConfig:\n    tasks = [answer(query) for _ in range(num_responses)]\n    responses = await asyncio.gather(*tasks)\n    return {\"computed_fields\": {\"responses\": responses}}\n```\n```\n\n----------------------------------------\n\nTITLE: Using llm.call with Ollama for Local Model Integration\nDESCRIPTION: Example showing how to interact with models running on Ollama using the llm.call decorator. This enables calling local models with Mirascope's integration features.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/local_models.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.llms import ollama\nfrom mirascope import llm\n\n\n@llm.call(model=ollama.Ollama(model=\"llama2\"))\ndef get_capital(country: str) -> str:\n    \"\"\"\n    Get the capital city of a country.\n    \n    Args:\n        country: The name of the country to get the capital of.\n    \n    Returns:\n        The capital city of the country.\n    \"\"\"\n\n\ndef main() -> None:\n    # Call the function with the country to get its capital\n    capital = get_capital(country=\"France\")\n    print(f\"The capital of France is {capital}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Function-based Chaining of LLM Calls - Python\nDESCRIPTION: This snippet outlines function-based chaining in Mirascope where the output of one function is used as the input to another, showcasing how to sequence operations properly.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -> str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -> str:\n    return f\"Translate this text to {language}: {text}\"\n\n\noriginal_text = \"Long English text here...\"\nsummary = summarize(original_text)\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Tool Using Python Docstring in Mirascope\nDESCRIPTION: Shows how to define an LLM tool using Python docstring and the openai.call decorator. The example creates a city population lookup function and converts it into a tool automatically.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_city_population(city: str) -> str:\n    \"\"\"Get the current population for `city` and prints it.\n\n    Args:\n        city: The name of the city, e.g., New York City.\n    \"\"\"\n    if city == \"New York City\":\n        return f\"The current population of {city} is approximately 8.8 million.\"\n    elif city == \"Los Angeles\":\n        return f\"The current population of {city} is approximately 4 million.\"\n    else:\n        return f\"I'm sorry, I don't have the population data for {city}.\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_city_population])\n@prompt_template(\"What's the population of {city}\")\ndef city_population(city: str): ...\n\n\nresponse = city_population(\"New York City\")\nif tool := response.tool:\n    print(tool.call())\n# > The current population of New York City is approximately 8.8 million.\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Text Summarization with Mirascope\nDESCRIPTION: Defines a recursive summarization system using Pydantic models and OpenAI integration. Uses decorator pattern to handle prompt templates and API calls, with a feedback loop for iterative summary improvement.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-chaining.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nfrom mirascope.core import openai, prompt_template\n\n\nclass SummaryFeedback(BaseModel):\n    \"\"\"Feedback on summary with a critique and review rewrite based on said critique.\"\"\"\n\n    critique: str = Field(..., description=\"The critique of the summary.\")\n    rewritten_summary: str = Field(\n        ...,\n        description=\"A rewritten summary that takes the critique into account.\",\n    )\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Summarize the following text into one sentence: {original_text}\")\ndef summarize(original_text: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=SummaryFeedback)\n@prompt_template(\n    \"\"\"\n    Original Text: {original_text}\n    Summary: {summary}\n\n    Critique the summary of the original text.\n    Then rewrite the summary based on the critique. It must be one sentence.\n    \"\"\"\n)\ndef resummarize(original_text: str, summary: str): ...\n\n\ndef rewrite_iteratively(original_text: str, depth=2):\n    summary = summarize(original_text=original_text).content\n    for _ in range(depth):\n        feedback = resummarize(original_text=original_text, summary=summary)\n        summary = feedback.rewritten_summary\n    return summary\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Chain with the @chain Decorator in Python\nDESCRIPTION: This example demonstrates how to use the @chain decorator to create a custom chain. It combines multiple components including prompts, models, and output parsers to create a chain that generates a joke about a given topic and then identifies the subject of that joke.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-runnables.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import chain\nfrom langchain_openai import ChatOpenAI\n\nprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\nprompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n\n@chain\ndef custom_chain(text):\n    prompt_val1 = prompt1.invoke({\"topic\": text})\n    output1 = ChatOpenAI().invoke(prompt_val1)\n    parsed_output1 = StrOutputParser().invoke(output1)\n    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()\n    return chain2.invoke({\"joke\": parsed_output1})\ncustom_chain.invoke(\"bears\")\n# Output: 'The subject of this joke is bears.'\n```\n\n----------------------------------------\n\nTITLE: Implementing a Documentation Agent with Mirascope\nDESCRIPTION: This code defines a `DocumentationAgent` class that utilizes an LLM to answer questions about Mirascope documentation. It leverages `openai.call` and `prompt_template` to define a system prompt and call the LLM. It classifies questions as either \"code\" or \"general\" and provides appropriate responses based on the retrieved documentation. The agent uses the `get_documents` function to retrieve relevant documentation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass Response(BaseModel):\n    classification: Literal[\"code\", \"general\"] = Field(\n        ..., description=\"The classification of the question\"\n    )\n    content: str = Field(..., description=\"The response content\")\n\n\nclass DocumentationAgent(BaseModel):\n    @openai.call(\"gpt-4o-mini\", response_model=Response, json_mode=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an AI Assistant that is an expert at answering questions about Mirascope.\n        Here is the relevant documentation to answer the question.\n\n        First classify the question into one of two types:\n            - General Information: Questions about the system or its components.\n            - Code Examples: Questions that require code snippets or examples.\n\n        For General Information, provide a summary of the relevant documents if the question is too broad ask for more details. \n        If the context does not answer the question, say that the information is not available or you could not find it.\n\n        For Code Examples, output ONLY code without any markdown, with comments if necessary.\n        If the context does not answer the question, say that the information is not available.\n\n        Examples:\n            Question: \"What is Mirascope?\"\n            Answer:\n            A toolkit for building AI-powered applications with Large Language Models (LLMs).\n            Explanation: This is a General Information question, so a summary is provided.\n\n            Question: \"How do I make a basic OpenAI call using Mirascope?\"\n            Answer:\n            from mirascope.core import openai, prompt_template\n\n\n            @openai.call(\"gpt-4o-mini\")\n            def recommend_book(genre: str) -> str:\n                return f'Recommend a {genre} book'\n\n            response = recommend_book(\"fantasy\")\n            print(response.content)\n            Explanation: This is a Code Examples question, so only a code snippet is provided.\n\n        Context:\n        {context:list}\n\n        USER:\n        {question}\n        \"\"\"\n    )\n    def _call(self, question: str) -> openai.OpenAIDynamicConfig:\n        documents = get_documents(question)\n        return {\"computed_fields\": {\"context\": documents}}\n\n    def _step(self, question: str):\n        answer = self._call(question)\n        print(\"(Assistant):\", answer.content)\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            self._step(question)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Tools for LLM-powered Chatbots with Mirascope\nDESCRIPTION: This code demonstrates how to extend a chatbot's capabilities by creating a custom tool that retrieves art gallery information. It uses Mirascope's BaseTool class to automatically generate the JSON schema needed for tool calling, and shows how to integrate the tool with an OpenAI model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseTool, openai, prompt_template\nfrom pydantic import Field\n\n# Function to retrieve information about art galleries\ndef get_art_gallery_info(gallery: str) -> str:\n    # Assume this function does not have a docstring\n    if gallery == \"The Louvre\":\n        return f\"{gallery} is a world-famous museum in Paris, France, known for its iconic art collection including the Mona Lisa.\"\n    elif gallery == \"The Metropolitan Museum of Art\":\n        return f\"{gallery} is located in New York City and houses an extensive collection of art from around the world.\"\n    else:\n        return f\"I'm sorry, I don't have information on {gallery}.\"\n\n# Class to handle art gallery queries\nclass GetArtGalleryInfo(BaseTool):\n    \"\"\"Get information about a specific art gallery.\"\"\"\n\n    gallery: str = Field(\n        ...,\n        description=\"The name of the art gallery, e.g., The Louvre.\",\n    )\n\n    def call(self):\n        return get_art_gallery_info(self.gallery)\n\n# Mirascope integration for the chatbot\n@openai.call(\"gpt-4o\", tools=[GetArtGalleryInfo])\n@prompt_template(\"Tell me about the art gallery {gallery}\")\ndef art_gallery_info(gallery: str): ...\n\n# Example chatbot interaction\nresponse = art_gallery_info(\"The Louvre\")\nif tool := response.tool:\n    print(tool.call())\n# > The Louvre is a world-famous museum in Paris, France, known for its iconic art collection including the Mona Lisa.\n```\n\n----------------------------------------\n\nTITLE: Document Loading and Chunking\nDESCRIPTION: Implementation of web scraping and text chunking workflow using WebBaseLoader and RecursiveCharacterTextSplitter.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-rag.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Load, chunk and index the contents of the blog \nloader = WebBaseLoader(\n   web_paths=(\"https://arxiv.org/html/1706.03762v7\",),\n   bs_kwargs=dict(\n       parse_only=bs4.SoupStrainer(\n           class_=(\"post-content\", \"post-title\", \"post-header\")\n       )\n   ),\n)\ndocs = loader.load()\n\n# Split extracted text into overlapping chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Chain with LangChain Prompts\nDESCRIPTION: Demonstrates how to create a custom RAG chain using LangChain's PromptTemplate and pipeline components for retrieval, prompting, LLM response generation, and output parsing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-rag.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate\n\ntemplate = \"\"\"Use the following pieces of context to provide a brief summary.\nIf you don't have enough information, just say so without guessing.\nLimit your response to three sentences for brevity.\nAlways end with \"hope this helps!\" to wrap up your response.\n\n{context}\n\nTopic: {topic}\n\nSummary:\"\"\"\n\ncustom_rag_prompt = PromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"topic\": RunnablePassthrough()}\n    | custom_rag_prompt\n    | llm\n    | StrOutputParser()\n)\n\nrag_chain.invoke(\"Explain Quantum Computing\") \n```\n\n----------------------------------------\n\nTITLE: Dynamic Client Configuration at Runtime\nDESCRIPTION: Demonstrates configuring LLM clients dynamically during runtime, providing flexibility in client setup across different providers\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/calls.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/calls/provider_specific/custom_client/dynamic_configuration/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Simplified Chatbot Pipeline Using Mirascope\nDESCRIPTION: This snippet shows a simplified version of the chatbot pipeline using Mirascope's OpenAI call decorator. It demonstrates how to achieve the same functionality as the LangChain example with cleaner, more intuitive Python code.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", call_params={\"stop\": \"END\"}, output_parser=str)\ndef answer_question(question: str) -> str:\n    return question\n\n\nresponse = answer_question(\"What is the status of my expense reimbursement request?\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Self-Consistency Technique with Python\nDESCRIPTION: This snippet implements a basic version of the Self-Consistency technique using Chain of Thought reasoning. It uses asynchronous tasks to gather multiple responses from the LLM and selects the most frequent answer. Key parameters include 'query' for the question at hand, 'num_samples' for the number of responses to generate, and 'few_shot_examples' as a list of examples for the model to reference.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/self_consistency.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom collections import Counter\n\nfrom mirascope.core import openai, prompt_template\n\nfew_shot_examples = [\n    {\n        \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n        \"answer\": \"We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.\",\n    },\n    {\n        \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n        \"answer\": \"There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\",\n    },\n    {\n        \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n        \"answer\": \"Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74 chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\",\n    },\n    {\n        \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n        \"answer\": \"Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\",\n    },\n    {\n        \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n        \"answer\": \"He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys. The answer is 9.\",\n    },\n    {\n        \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n        \"answer\": \"There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29.\",\n    },\n    {\n        \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n        \"answer\": \"Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\",\n    },\n]\n\n\n@openai.call(model=\"gpt-4o-mini\", call_params={\"temperature\": 0.5})\n@prompt_template(\n    \"\"\"\n    Some examples on how to think step by step:\n    {examples:lists}\n\n    Answer the following question, thinking step by step:\n    {query}\n    \"\"\"\n)\nasync def chain_of_thought(\n    query: str, few_shot_examples: list[dict[str, str]]\n) -> openai.OpenAIDynamicConfig:\n    examples = [\n        [f\"Q:{example['question']}\", f\"A:{example['answer']}\"]\n        for example in few_shot_examples\n    ]\n    return {\"computed_fields\": {\"examples\": examples}}\n\n\ndef most_frequent(lst):\n    \"\"\"Returns the most frequent element in a list.\"\"\"\n    counter = Counter(lst)\n    most_common = counter.most_common(1)\n    return most_common[0][0] if most_common else None\n\n\nasync def self_consistency(\n    query: str, num_samples: int, few_shot_examples: list[dict[str, str]]\n):\n    cot_tasks = [chain_of_thought(query, few_shot_examples) for _ in range(num_samples)]\n    cot_responses = [response.content for response in await asyncio.gather(*cot_tasks)]\n    # Extract final answers manually (simplified for this example)\n    final_answers = [\n        response.split(\"The answer is \")[-1].strip(\".\") for response in cot_responses\n    ]\n    return most_frequent(final_answers)\n\n\nquery = \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\nresult = await self_consistency(\n    query=query, num_samples=5, few_shot_examples=few_shot_examples\n)\nprint(f\"The most consistent answer is: {result}\")\n```\n\n----------------------------------------\n\nTITLE: Searching for Answers using Groq API - Python\nDESCRIPTION: This function uses the Groq API to generate an answer based on search results provided. It takes a question string and a dictionary of search results, returning a string response that should ideally answer the user's question.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n@groq_call(\"llama-3.2-90b-text-preview\")\n@prompt_template(\n    \"\"\"\nSYSTEM:\nYou are an expert at finding information on the web.\nUse the provided search results to answer the question.\nRewrite the question as needed to better find information on the web.\nSearch results:\n{search_results}\n\nUSER:\n{question}\n\"\"\"\n)\ndef search(question: str, search_results: dict[str, str]) -> str:\n    \"\"\"\n    Use the search results to answer the user's question.\n    \"\"\"\n    # The model will return an answer based on the search results and question\n    ...\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: BaseToolKit Usage Example\nDESCRIPTION: This code snippet illustrates how to use the `BaseToolKit` class in Mirascope to organize tools under a single namespace and dynamically define tools. It demonstrates creating a `BookTools` toolkit with a dynamic description based on the `reading_level` state, showcasing how tool definitions can adapt based on input or state changes.  The description is generated on each call to `recommend_author`.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n --8<-- \"build/snippets/learn/tools/toolkit/{{ provider | provider_dir }}/{{ method }}.py\"\n\n```\n\n----------------------------------------\n\nTITLE: Testing Documentation Agent Code Responses\nDESCRIPTION: This code defines test cases for the `DocumentationAgent`'s code generation capabilities. It uses `pytest.mark.parametrize` to specify different queries and their expected output. It checks that if the response is classified as \"code\", then it is both syntactically correct and contains valid imports, utilizing the `check_syntax` and `is_importable` functions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize(\n    \"query,expected\",\n    [\n        (\"How do I make a basic OpenAI call using Mirascope?\", None),\n        (\"What is Mirascope?\", \"a toolkit for building AI-powered applications\"),\n    ],\n)\ndef test_documentation_agent_code(query: str, expected: str):\n    documentation_agent = DocumentationAgent()\n    response = documentation_agent._call(query)\n    if response.classification == \"code\":\n        assert check_syntax(response.content) and is_importable(response.content)\n    else:\n        assert expected in response.content\n```\n\n----------------------------------------\n\nTITLE: Accessing OpenAI API Response Properties in Mirascope\nDESCRIPTION: Demonstrates how to access properties of the OpenAICallResponse object, which wraps around the ChatCompletion class from the OpenAI library, providing convenient access to response data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core.openai import OpenAICallResponse\n\nresponse = OpenAICallResponse(...)\n\nresponse.response    # ChatCompletion(...)\nresponse.content     # original.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Defining Tool with Examples in Mirascope\nDESCRIPTION: This snippet demonstrates how to define a tool using Mirascope's `BaseTool` class, incorporating examples directly into the `title` and `author` fields using Pydantic's `Field` and `ConfigDict`. The examples enhance the LLM's understanding of the expected inputs for the tool. It requires the `mirascope.core` and `pydantic` libraries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseTool\nfrom pydantic import ConfigDict, Field\n\n\nclass FormatMovie(BaseTool):\n    \"\"\"Returns the title and director of a movie nicely formatted.\"\"\"\n\n    title: str = Field(..., examples=[\"Inception\"])\n    director: str = Field(..., examples=[\"Nolan, Christopher\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"Inception\", \"director\": \"Nolan, Christopher\"}]}\n    )\n\n    def call(self) -> str:\n        return f\"{self.title} directed by {self.director}\"\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Chain-of-Thought Function\nDESCRIPTION: Defines a function `zero_shot_cot` that performs zero-shot chain-of-thought reasoning using the OpenAI LLM. It uses the `@openai.call` and `@prompt_template` decorators to specify the model and prompt template, respectively. It takes a query as input and returns the LLM's response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/diverse.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Answer the following question going step by step:\n    {query}\n    \"\"\"\n)\nasync def zero_shot_cot(query: str): ...\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Agent Flow Sequence Diagram in Mermaid\nDESCRIPTION: Diagram showing the interaction flow between user code and LLM in an agent system, including tool calling cycles and response handling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/agents.md#2025-04-21_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant YC as Your Code\n    participant LLM\n\n    loop Agent Loop\n        YC->>LLM: Call with prompt + history + function definitions\n        loop Tool Calling Cycle\n            LLM->>LLM: Decide to respond or call functions\n            LLM->>YC: Respond with function to call and arguments\n            YC->>YC: Execute function with given arguments\n            YC->>YC: Add tool call message parameters to history\n            YC->>LLM: Call with prompt + history including function result\n        end\n        LLM->>YC: Finish calling tools and return final response\n        YC->>YC: Update history with final response\n    end\n```\n\n----------------------------------------\n\nTITLE: Provider Fallback Implementation\nDESCRIPTION: Demonstrates setting up provider fallback mechanisms to switch between different LLM providers when specific errors occur, combined with retry logic.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/retries.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@retry(stop=stop_after_attempt(3), wait=wait_exponential())\n@fallback(OpenAIRateLimitError, provider=\"anthropic\")\n@fallback(AnthropicRateLimitError, provider=\"claude\")\ndef make_recommendation() -> None:\n    response = recommend_book(\"fantasy\")\n    print(response.text)\n```\n\n----------------------------------------\n\nTITLE: Streaming Multi-Modal Outputs with OpenAI\nDESCRIPTION: This Python snippet demonstrates streaming both text and audio outputs using OpenAI. It showcases how to access the raw audio data (chunk.audio) and the transcript (chunk.audio_transcript) from each stream chunk, allowing for concurrent processing of text and audio streams. This example requires the OpenAI client library.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Streaming multi-modal outputs with OpenAI.\n\nIn this example, we use OpenAI to stream both text and audio outputs.\n\nUsage:\n    python examples/learn/streams/multi_modal_outputs/openai/tools.py\n\"\"\"\nimport asyncio\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nasync def main():\n    stream = client.audio.speech.create(\n        model=\"tts-1\",\n        voice=\"alloy\",\n        input=\"The quick brown fox jumped over the lazy dogs.\",\n        stream=True\n    )\n\n    async for chunk in stream:\n        # Raw audio data in bytes format\n        print(f'{chunk.audio=}')\n        # The transcript of the audio\n        print(f'{chunk.audio_transcript=}')\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Evaluation Workflow with Mirascope in Python\nDESCRIPTION: Defines a workflow for evaluating a candidate using prompt templates and asynchronous function. It utilizes decorators for embedding evaluation criteria into prompts and formatting the output with the Pydantic model. Dependencies include `mirascope`, `pydantic`, and `asyncio` libraries for accessing prompt templates, structuring evaluation models, and asynchronous operations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel, Field\nimport asyncio\n\nclass CandidateEvaluation(BaseModel):\n    is_candidate_suitable: bool = Field(\n        description=\"Whether the LLM-recommended candidate action is suitable for the role\"\n    )\n    explanation: str = Field(\n        description=\"A brief explanation of why the LLM recommendation is or isn\\'t suitable\"\n    )\n\n@llm.call(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=CandidateEvaluation,\n    json_mode=True\n)\n@prompt_template(\n    \"\"\"\n    You are helping to review candidates for a virtual hiring assistant.\n    \n    Given:\n    - Role Requirements: {role_requirements}\n    - Candidate Profile: {candidate_profile}\n    - LLM Recommendation: {llm_recommendation}\n\n    Evaluate the LLM\\'s recommendation based on the following criteria:\n\n    1. **Role Alignment**  \n       - Does the candidate\\'s experience, skills, and qualifications match the role requirements?\n       - Is there any significant mismatch that the LLM recommendation overlooks?\n\n    2. **Unbiased Assessment**  \n       - Does the recommendation show any signs of bias or unfair assumptions?\n       - Are the candidate’s attributes (e.g., background, demographics) handled appropriately?\n\n    3. **Relevance to Responsibilities**  \n       - Does the recommendation address the specific responsibilities and day-to-day tasks of the job?\n       - Are there gaps in the candidate\\'s profile that might affect job performance?\n\n    4. **Clarity and Rationale**  \n       - Does the LLM\\'s recommendation clearly explain why this candidate is suitable or not?\n       - Is the logic behind the recommendation transparent and consistent?\n\n    5. **Cultural and Team Fit**  \n       - If applicable, does the recommendation consider team culture, communication style, or other intangible factors?\n       - Are there any red flags or concerns about collaboration?\n\n    6. **Potential Red Flags**  \n       - Are there any noticeable contradictions, missing details, or areas the LLM recommendation did not address?\n       - Does the advice seem incomplete or overly generalized?\n\n    Final Evaluation:\n    - Assess whether the LLM recommendation is truly suitable for the role.\n    - Provide a brief explanation indicating your reasoning—highlight any strengths or weaknesses in the LLM’s assessment.\n    - Indicate whether you would proceed with the candidate, request more information, or decline.\n\n    Based on the above, return:\n    - \"is_candidate_suitable\": true or false\n    - \"explanation\": a short summary of why the recommendation is or isn\\'t suitable\n    \"\"\"\n)\nasync def evaluate_candidate_suitability(\n    role_requirements: str, \n    candidate_profile: str, \n    llm_recommendation: str\n) -> CandidateEvaluation:\n    # The decorators take care of the prompt generation and API call.\n    ...\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example inputs\n    role_req = \"Strong experience in backend development with expertise in Python and scalable architectures.\"\n    candidate = \"Jane Matt, with 7 years of experience in backend systems, specialized in Python and microservices.\"\n    recommendation = \"The candidate is highly recommended due to her extensive experience and clear alignment with the role requirements.\"\n\n    # Execute the evaluation workflow\n    evaluation = asyncio.run(evaluate_candidate_suitability(role_req, candidate, recommendation))\n    \n    # Print the evaluation results\n    print(\"Candidate Suitable:\", evaluation.is_candidate_suitable)\n    print(\"Explanation:\", evaluation.explanation)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG with Mirascope\nDESCRIPTION: Shows the same RAG implementation using Mirascope's decorator-based approach, demonstrating cleaner code organization and native Python patterns for LLM interaction.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-integration.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom bs4 import SoupStrainer\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nfrom mirascope.core import openai, prompt_template\n\n# Load, chunk and index the contents of the blog.\nloader = WebBaseLoader(\n    web_paths=(\"https://en.wikipedia.org/wiki/Quantum_computing\",),\n    bs_kwargs=dict(\n        parse_only=SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n\n# Retrieve and generate using the relevant snippets of the article.\nretriever = vectorstore.as_retriever()\n\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\", output_parser=str)\n@prompt_template(\n    \"\"\"\n    Use the following pieces of information to provide a concise overview.\n    If the details are insufficient, state that clearly without making assumptions.\n    Keep your response to three sentences for conciseness.\n    End with \"hope this provides clarity!\" to conclude your response.\n\n    {information}\n\n    Topic: {subject}\n\n    Overview:\n    \"\"\"\n)\ndef overview(subject: str) -> openai.OpenAIDynamicConfig:\n    information = \"\\n\\n\".join(doc.page_content for doc in retriever.invoke(subject))\n    return {\"computed_fields\": {\"information\": information}}\n\n\noutput = overview(\"Describe quantum computing\")\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Implementing Step-back Prompting with Mirascope and OpenAI\nDESCRIPTION: A complete implementation of the Step-back prompting technique using Mirascope's decorators. The code includes few-shot examples, a function to generate step-back questions, a standard OpenAI call function, and the main orchestration function that processes both the step-back question and the original query.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/step_back.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom mirascope.core.base.prompt import prompt_template\n\nfew_shot_examples = [\n    {\n        \"original_question\": \"Which position did Knox Cunningham hold from May 1955 to Apr 1956?\",\n        \"stepback_question\": \"Which positions have Knox Cunningham held in his career?\",\n    },\n    {\n        \"original_question\": \"Who was the spouse of Anna Karina from 1968 to 1974?\",\n        \"stepback_question\": \"Who were the spouses of Anna Karina?\",\n    },\n    {\n        \"original_question\": \"Which team did Thierry Audel play for from 2007 to 2008?\",\n        \"stepback_question\": \"Which teams did Thierry Audel play for in his career?\",\n    },\n    {\n        \"original_question\": \"What was the operator of GCR Class 11E from 1913 to Dec 1922?\",\n        \"stepback_question\": \"What were the operators of GCR Class 11E in history?\",\n    },\n    {\n        \"original_question\": \"Which country did Sokolovsko belong to from 1392 to 1525?\",\n        \"stepback_question\": \"Which countries did Sokolovsko belong to in history?\",\n    },\n    {\n        \"original_question\": \"when was the last time a team from canada won the stanley cup as of 2002\",\n        \"stepback_question\": \"which years did a team from canada won the stanley cup as of 2002\",\n    },\n    {\n        \"original_question\": \"when did england last get to the semi final in a world cup as of 2019\",\n        \"stepback_question\": \"which years did england get to the semi final in a world cup as of 2019?\",\n    },\n    {\n        \"original_question\": \"what is the biggest hotel in las vegas nv as of November 28, 1993\",\n        \"stepback_question\": \"what is the size of the hotels in las vegas nv as of November 28, 1993\",\n    },\n    {\n        \"original_question\": \"who has scored most runs in t20 matches as of 2017\",\n        \"stepback_question\": \"What are the runs of players in t20 matches as of 2017\",\n    },\n]\n\nstepback_prompt = \"\"\"You are an expert at world knowledge. Your task is to step \\\nback and paraphrase a question to a more generic step-back question, which is \\\neasier to answer. Here are a few examples:\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM: {stepback_prompt_and_examples}\n    USER: {query}\n    \"\"\"\n)\ndef get_stepback_question(\n    query: str, num_examples: int = 0\n) -> openai.OpenAIDynamicConfig:\n    \"\"\"Gets the generic, step-back version of a query.\"\"\"\n    if num_examples < 0 or num_examples > len(few_shot_examples):\n        raise ValueError(\n            \"num_examples cannot be negative or greater than number of available examples.\"\n        )\n    example_prompts = \"\"\n    for i in range(num_examples):\n        example_prompts += (\n            f\"Original Question: {few_shot_examples[i]['original_question']}\\n\"\n        )\n        example_prompts += (\n            f\"Stepback Question: {few_shot_examples[i]['stepback_question']}\\n\"\n        )\n    return {\n        \"computed_fields\": {\n            \"stepback_prompt_and_examples\": f\"{stepback_prompt}\\n{example_prompts}\"\n            if num_examples\n            else None\n        }\n    }\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef call(query: str) -> str:\n    \"\"\"A standard call to OpenAI.\"\"\"\n    return query\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    You are an expert of world knowledge. I am going to ask you a question.\n    Your response should be comprehensive and not contradicted with the\n    following context if they are relevant. Otherwise, ignore them if they are\n    not relevant.\n\n    {stepback_response}\n\n    Original Question: {query}\n    Answer:\n    \"\"\"\n)\ndef stepback(query: str, num_examples: int) -> openai.OpenAIDynamicConfig:\n    \"\"\"Executes the flow of the Step-Back technique.\"\"\"\n    stepback_question = get_stepback_question(\n        query=query, num_examples=num_examples\n    ).content\n    stepback_response = call(query=stepback_question).content\n    return {\"computed_fields\": {\"stepback_response\": stepback_response}}\n\n\n# Example usage\nquery = \"\"\"Who is the highest paid player in the nba this season as of 2017\"\"\"\n\nprint(stepback(query=query, num_examples=len(few_shot_examples)))\n```\n\n----------------------------------------\n\nTITLE: WebAssistant Class Implementation\nDESCRIPTION: Defines a WebAssistant class with custom _step method for handling streaming responses, tool calls, and message management\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/web_search_agent.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass WebAssistant(WebAssistantBaseWithStream):\n    async def _step(self, question: str):\n        print(self.messages)\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                print(f\"using {tool._name()} tool with args: {tool.args}\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n```\n\n----------------------------------------\n\nTITLE: Example usage of the Chain of Verification function\nDESCRIPTION: This code demonstrates how to use the `chain_of_verification` function with a sample query about politicians born in New York.  It calls the function with the query and prints the resulting response, showcasing the end-to-end Chain of Verification process.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/chain_of_verification.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquery = \"Name 5 politicians born in New York.\"\n\nprint(await chain_of_verification(query=query))\n```\n\n----------------------------------------\n\nTITLE: Enhanced Self-Refine with Structured Response Model in Mirascope\nDESCRIPTION: An improved implementation of Self-Refine that uses a Pydantic BaseModel to structure the output. This version creates a MathSolution response model with defined fields for solution steps and final answer, providing more organized and consistent output formatting.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/self_refine.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass MathSolution(BaseModel):\n    steps: list[str] = Field(..., description=\"The steps taken to solve the problem\")\n    final_answer: float = Field(..., description=\"The final numerical answer\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=MathSolution)\n@prompt_template(\n    \"\"\"\n    For this query:\n    {query}\n    The following response was given:\n    {response}\n    Here is some feedback about the response:\n    {feedback}\n\n    Consider the feedback to generate a new response to the query.\n    Provide the solution steps and the final numerical answer.\n    \"\"\"\n)\ndef enhanced_generate_new_response(\n    query: str, response: OpenAICallResponse\n) -> openai.OpenAIDynamicConfig:\n    feedback = evaluate_response(query, response)\n    return {\"computed_fields\": {\"feedback\": feedback}}\n\n\ndef enhanced_self_refine(query: str, depth: int) -> MathSolution:\n    response = call(query)\n    for _ in range(depth):\n        solution = enhanced_generate_new_response(query, response)\n        response = f\"Steps: {solution.steps}\\nFinal Answer: {solution.final_answer}\"\n    return solution\n\n\n# Example usage\nresult = enhanced_self_refine(query, 1)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG with LangChain\nDESCRIPTION: Demonstrates a RAG (Retrieval Augmented Generation) implementation using LangChain's LCEL syntax for web content processing and LLM interaction. Uses vector storage, custom prompts, and chaining mechanisms.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-integration.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom bs4 import SoupStrainer\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import PromptTemplate\n\n# Load, chunk and index the contents of the article.\nloader = WebBaseLoader(\n    web_paths=(\"https://en.wikipedia.org/wiki/Quantum_computing\",),\n    bs_kwargs=dict(\n        parse_only=SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n\n# Retrieve and generate using the relevant snippets of the article.\nretriever = vectorstore.as_retriever()\nllm = ChatOpenAI()\n\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\ntemplate = \"\"\"Use the following pieces of information to provide a concise overview.\nIf the details are insufficient, state that clearly without making assumptions.\nKeep your response to three sentences for conciseness.\nEnd with \"hope this provides clarity!\" to conclude your response.\n\n{information}\n\nTopic: {subject}\n\nOverview:\"\"\"\n\ncustom_overview_prompt = PromptTemplate.from_template(template)\n\noverview_chain = (\n    {\"information\": retriever | format_docs, \"subject\": RunnablePassthrough()}\n    | custom_overview_prompt\n    | llm\n    | StrOutputParser()\n)\n\noutput = overview_chain.invoke(\"Describe quantum computing.\")\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Error Handling in LLM Calls\nDESCRIPTION: Illustrates how to catch and handle potential errors during LLM calls, preserving original provider error messages and enabling robust error management\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/calls.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/calls/error_handling/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Incorporating Few-Shot Examples in Python with Mirascope\nDESCRIPTION: Illustrates how to use few-shot learning by providing examples in the field definitions to improve the model's output by matching a specified format.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/structured_outputs.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic import ConfigDict, Field\n\n\nclass FewShotBook(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"},\n            ]\n        }\n    )\n\n\n@openai.call(\"gpt-4o-mini\", response_model=list[FewShotBook], json_mode=True)\ndef recommend_few_shot_books(genre: str, num: int) -> str:\n    return f\"Recommend a list of {num} {genre} books. Match example format.\"\n\n\nbooks = recommend_few_shot_books(\"fantasy\", 3)\nfor book in books:\n    print(book)\n```\n\n----------------------------------------\n\nTITLE: Implementing Rereading Technique with Mirascope and OpenAI\nDESCRIPTION: This code snippet demonstrates how to implement the Rereading technique using Mirascope's prompt templates and OpenAI models. It defines a function that takes a query and a boolean flag to determine whether to apply rereading, which appends instructions for the model to read the question again.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/rereading.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{query} {reread}\")\ndef call(query: str, reread_prompt: bool = False) -> openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"reread\": f\"Read the question again: {query}\" if reread_prompt else \"\",\n        }\n    }\n\n\nprompt = \"\"\"A coin is heads up. aluino flips the coin. arthor flips the coin.\nIs the coin still heads up? Flip means reverse.\"\"\"\n\nprint(call(query=prompt, reread_prompt=True))\n```\n\n----------------------------------------\n\nTITLE: Implementing Tool Caching with AnthropicToolConfig in Anthropic\nDESCRIPTION: Example of implementing tool caching using AnthropicToolConfig with cache control settings. This allows caching of tools to save tokens, with the note that cache control should only be included on the last tool in the list that needs to be cached.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/anthropic.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/anthropic/caching/tools.py\"\n```\n\n----------------------------------------\n\nTITLE: Function Calling with Mirascope\nDESCRIPTION: This snippet illustrates how to define a function call using Mirascope.  It defines a `get_current_stock_price` function with a docstring, which Mirascope uses to generate the necessary JSON schema for the function call.  The function takes a company name as input and returns the stock price as a string.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_current_stock_price(company: str) -> str:\n    # Assume this function does not have a docstring\n    if company == \"Apple Inc.\":\n        return f\"The current stock price of {company} is $200.\"\n    elif company == \"Google LLC\":\n        return f\"The current stock price of {company} is $180.\"\n    else:\n        return f\"I'm sorry, I don't have the stock price for {company}.\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Pytest Fixtures and Parameterized Tests for SQL Queries\nDESCRIPTION: This code defines pytest fixtures and parameterized tests for evaluating SQL query generation. It covers SELECT, INSERT, UPDATE, and DELETE operations using a mock Librarian class.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_sql_agent.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pytest.fixture\ndef mock_librarian():\n    class MockLibrarian(Librarian):\n        con: ClassVar[sqlite3.Connection] = MagicMock()\n\n    return MockLibrarian()\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"select_query\",\n    [\n        \"Get all books\",\n        \"Get every single book\",\n        \"Show me all books\",\n        \"List all books\",\n        \"Display all books\",\n    ],\n)\nasync def test_select_query(select_query: str, mock_librarian: Librarian):\n    response = await mock_librarian._stream(select_query)\n    async for _, tool in response:\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\n        assert query == \"SELECT * FROM ReadingList;\"\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"insert_query\",\n    [\n        \"Please add Gone with the Wind to my reading list\",\n        \"You recently recommended Gone with the Wind, can you add it to my reading list.\",\n    ],\n)\nasync def test_insert_query(insert_query: str, mock_librarian: Librarian):\n    response = await mock_librarian._stream(insert_query)\n    async for _, tool in response:\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\n        assert (\n            query\n            == \"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\"\n        )\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"update_query\",\n    [\n        \"Can you mark Gone with the Wind as read?\",\n        \"I just finished Gone with the Wind, can you update the status?\",\n    ],\n)\nasync def test_update_query(update_query: str, mock_librarian: Librarian):\n    response = await mock_librarian._stream(update_query)\n    async for _, tool in response:\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\n        assert (\n            query\n            == \"UPDATE ReadingList SET status = 'Complete' WHERE title = 'Gone with the Wind';\"\n        )\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"delete_query\",\n    [\n        \"Can you remove Gone with the Wind from my reading list?\",\n        \"Can you delete Gone with the Wind?\",\n    ],\n)\nasync def test_delete_query(delete_query: str, mock_librarian: Librarian):\n    response = await mock_librarian._stream(delete_query)\n    async for _, tool in response:\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\n        assert query == \"DELETE FROM ReadingList WHERE title = 'Gone with the Wind';\"\n\n\nipytest.run()\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Schema with Pydantic for LLM Judges\nDESCRIPTION: This code creates a Pydantic data model that enforces a structured schema for LLM evaluation outputs. It requires both a reasoning explanation and a numerical score to ensure consistent evaluation formats across different judge models.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-as-judge.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in < 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n```\n\n----------------------------------------\n\nTITLE: Deduplicating Movie Genres with a LLM Call\nDESCRIPTION: This function uses Mirascope to call OpenAI's API with a prompt template designed to deduplicate movie genres. It returns a structured response based on the specified schema.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/removing_semantic_duplicates.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=DeduplicatedGenres)\n@prompt_template(\n    '''\n    SYSTEM:\n    Your job is to take a list of movie genres and clean it up by removing items\n    which are semantically equivalent to one another. When coming across multiple items\n    which refer to the same genre, keep the genre name which is most commonly used.\n    For example, \"sci-fi\" and \"science fiction\" are the same genre.\n\n    USER:\n    {genres}\n    '''\n)\ndef deduplicate_genres(genres: list[str]): ...\n```\n\n----------------------------------------\n\nTITLE: Building RAG Pipeline with LangChain Components - Python\nDESCRIPTION: This snippet illustrates a retrieval-augmented generation (RAG) pipeline using components from the LangChain framework. It leverages the WebBaseLoader to load blog content, utilizes a text splitter for chunking documents, and integrates with OpenAI to provide an overview based on retrieved documents. Dependencies like langchain are required for this implementation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-frameworks.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python hl_lines=\"45-50\"\nfrom bs4 import SoupStrainer\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import PromptTemplate\n\n# Load, chunk and index the contents of the blog.\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n\n# Retrieve and generate using the relevant snippets of the blog.\nretriever = vectorstore.as_retriever()\nllm = ChatOpenAI()\n\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\ntemplate = \"\"\"Use the following pieces of information to provide a concise overview.\nIf the details are insufficient, state that clearly without making assumptions.\nKeep your response to three sentences for conciseness.\nEnd with \"hope this provides clarity!\" to conclude your response.\n\n{information}\n\nTopic: {subject}\n\nOverview:\"\"\"\n\ncustom_overview_prompt = PromptTemplate.from_template(template)\n\noverview_chain = (\n    {\"information\": retriever | format_docs, \"subject\": RunnablePassthrough()}\n    | custom_overview_prompt\n    | llm\n    | StrOutputParser()\n)\n\noutput = overview_chain.invoke(\"Describe machine learning\")\nprint(output)\n```\n```\n\n----------------------------------------\n\nTITLE: Logical Step-by-Step Problem Solving with Chain of Thought\nDESCRIPTION: This snippet illustrates the Chain of Thought (CoT) prompting technique, encouraging the language model to break down mathematical or reasoning tasks into logical steps before arriving at the final answer.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nOlivia has $23. She bought five bagels for $3 each. How much money does she have left? Let's think step by step.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nFirst, let’s determine how much money Olivia spent on the bagels:\nPrice of one bagel = $3\nNumber of bagels = 5\nTotal cost of the bagels = $3 \\times 5 = $15\n\nNow, let’s subtract the total cost from Olivia’s initial amount:\nOlivia started with $23\nShe spent $15\nAmount left = $23 - $15 = $8\nSo, Olivia has $8 left after buying the bagels.\n```\n\n----------------------------------------\n\nTITLE: Building a Basic Chatbot with Mirascope in Python\nDESCRIPTION: This Python class `BasicChatbot` uses Mirascope to create a basic chatbot that maintains conversation history using OpenAI's LLMs. It's designed to continuously interact with users until 'exit' is input. The dependencies include Mirascope and pydantic. The chat method uses OpenAI's models to generate responses based on user inputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/tools_and_agents.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import Messages, openai\nfrom pydantic import BaseModel\n\n\nclass BasicChatbot(BaseModel):\n    messages: list[openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\")\n    async def chat(self, user_input: str) -> openai.OpenAIDynamicConfig:\n        messages = [\n            Messages.System(\n                \"You are a friendly chatbot assistant. Engage in a conversation with the user.\"\n            ),\n            *self.messages,\n            Messages.User(user_input),\n        ]\n        return {\"messages\": messages}\n\n    async def run(self):\n        while True:\n            user_input = input(\"User: \")\n            if user_input.lower() == \"exit\":\n                break\n            response = await self.chat(user_input)\n            print(f\"User: {user_input}\")\n            print(f\"Chatbot: {response.content}\")\n            self.messages.append(response.user_message_param)\n            self.messages.append(response.message_param)\n\n\n# Usage\nchatbot = BasicChatbot()\n# Run the chatbot in a Jupyter notebook\nawait chatbot.run()\n\n# Run the chatbot in a Python script\n# import asyncio\n# asyncio.run(chatbot.run())\n```\n\n----------------------------------------\n\nTITLE: Implementing Librarian Class with OpenAI Call and Prompt Template\nDESCRIPTION: This snippet defines the Librarian class with an OpenAI call decorator, prompt template, and methods for processing user queries and interacting with the database.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/sql_agent.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\nclass Librarian(LibrarianBase):\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are a friendly and knowledgeable librarian named Mira. Your role is to \n        assist patrons with their queries, recommend books, \n        and provide information on a wide range of topics.\n\n        Personality:\n            - Warm and approachable, always ready with a kind word\n            - Patient and understanding, especially with those who are hesitant or confused\n            - Enthusiastic about books and learning\n            - Respectful of all patrons, regardless of their background or level of knowledge\n\n        Services:\n            - Keep track of patrons' reading lists using a SQLite database. Assume that the user is non technical and will ask you\n        questions in plain English.\n            - Recommend books based on the user's preferences\n        Your task is to write a query based on the user's request.\n\n        The database schema is as follows:\n\n        TABLE ReadingList (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            title TEXT NOT NULL,\n            status TEXT CHECK(status IN ('Not Started', 'In Progress', 'Complete')) NOT NULL,\n            rating INTEGER CHECK(rating >= 1 AND rating <= 5),\n        );\n\n        You must interpret the user's request and write the appropriate SQL query to\n        pass in the tools.\n\n        Example interactions:\n            1. Select\n                - USER: \"Show me all books.\"\n                - ASSISTANT: \"SELECT * FROM ReadingList;\"\n            2. Insert\n                - USER: \"Add Gone with the Wind to my reading list.\"\n                - ASSISTANT: \"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\"\n            3. Update\n                - USER: \"I just finished Gone with the Wind, can you update the status, and give it 5 stars??\"\n                - ASSISTANT: \"UPDATE ReadingList SET status = 'Complete' and rating = 5 WHERE title = 'Gone with the Wind';\"\n            4. Delete\n                - USER: \"Remove Gone with the Wind from my reading list.\"\n                - ASSISTANT: \"DELETE FROM ReadingList WHERE title = 'Gone with the Wind';\"\n\n        If field are not mentioned, omit them from the query.\n        All queries must end with a semicolon.\n\n        You have access to the following tools:\n        - `_run_query`: When user asks for recommendations, you can use this tool to see what they have read.\n        - `_execute_query`: Use the query generated to execute an \n            INSERT, UPDATE, or DELETE query.\n\n        You must use these tools to interact with the database.\n\n        MESSAGES: {self.messages}\n        USER: {query}\n        \"\"\"\n    )\n    async def _stream(self, query: str) -> openai.OpenAIDynamicConfig:\n        return {\"tools\": [self._run_query, self._execute_query]}\n\n    async def _step(self, question: str):\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._step(question)\n            print()\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI integration\nDESCRIPTION: This snippet shows how to install Mirascope with OpenAI integration using pip.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/localized_agent.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Setting up a Chroma Vector Store with Mirascope RAG\nDESCRIPTION: This snippet shows how to create a custom vector store by extending Mirascope's ChromaVectorStore class. It combines the TextChunker and OpenAIEmbedder to create a complete RAG storage solution.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# your_repo.stores.py\nfrom mirascope.beta.rag.base import TextChunker\nfrom mirascope.beta.rag.chroma import ChromaVectorStore\nfrim mirascope.beta.rag.openai import OpenAIEmbedder\n\n\nclass MyStore(ChromaVectorStore):\n    embedder = OpenAIEmbedder()\n    chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\n    index_name = \"my_index\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Thread of Thought with RAG and OpenAI\nDESCRIPTION: Python implementation showing how to combine Thread of Thought prompting with RAG retrieval. Uses Mirascope's OpenAI integration and prompt templates to process multiple passages and generate step-by-step analysis of context. Includes simulated RAG retrieval and configurable THoT augmentation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/thread_of_thought.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\nrag_output = [\n    \"\"\"Apple Inc. was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and\nRonald Wayne. The company started in the garage of Jobs' childhood home in \nLos Altos, California.\"\"\",\n    \"\"\"Steve Jobs was a visionary entrepreneur and the co-founder of Apple Inc.\nHe played a key role in the development of the Macintosh, iPod, iPhone, and iPad.\"\"\",\n    \"\"\"Apple's headquarters, known as Apple Park, is located in Cupertino, California.\nThe campus, designed by Norman Foster, opened to employees in April 2017.\"\"\",\n    \"\"\"In 1977, Apple Computer, Inc. was incorporated. The Apple II, one of the first\nhighly successful mass-produced microcomputer products, was introduced that year.\"\"\",\n    \"\"\"Apple's first product, the Apple I, was sold as a fully assembled circuit board.\nThe idea for the company came from Steve Wozniak's interest in building a computer\nkit.\"\"\",\n    \"\"\"Steve Wozniak and Steve Jobs were high school friends before they founded Apple\ntogether. They were both members of the Homebrew Computer Club, where they exchanged\nideas with other computer enthusiasts.\"\"\",\n    \"\"\"The first Apple Store opened in Tysons Corner, Virginia, in May 2001.\nApple Stores have since become iconic retail spaces around the world.\"\"\",\n    \"\"\"Apple has a strong commitment to environmental sustainability. The company\naims to have its entire supply chain carbon neutral by 2030.\"\"\",\n    \"\"\"Ronald Wayne, the lesser-known third co-founder of Apple, sold his shares\nin the company just 12 days after it was founded. He believed the venture was too\nrisky and wanted to avoid potential financial loss.\"\"\",\n    \"\"\"In 1984, Apple launched the Macintosh, the first personal computer to feature\na graphical user interface and a mouse. This product revolutionized the computer\nindustry and set new standards for user-friendly design.\"\"\",\n]\n\n\ndef retrieve_passages(query: str):\n    \"\"\"Simulates RAG retrieval.\"\"\"\n    return rag_output\n\n\nthot_augment = \"\"\"Walk me through this context in manageable parts step by step,\nsummarizing and analyzing as we go\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    As a content reviewer, I provide multiple retrieved passages about\n    this question; you need to answer the question.\n\n    {context}\n\n    {query} {thot_augment}\n    \"\"\"\n)\ndef call(query: str, thot_prompt: bool = False) -> openai.OpenAIDynamicConfig:\n    passages = retrieve_passages(query)\n    context = [\n        f\"retrieved passage {i + 1} is: {passage}\" for i, passage in enumerate(passages)\n    ]\n    return {\n        \"computed_fields\": {\n            \"context\": context,\n            \"thot_augment\": thot_augment if thot_prompt else \"\",\n        }\n    }\n\n\nprompt = \"Where was Apple founded?\"\n\nprint(call(query=prompt, thot_prompt=True))\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Query Reranking with Mirascope\nDESCRIPTION: This snippet defines a function for reranking documents using OpenAI's LLM. It includes a custom Pydantic model for relevance scoring, and uses Mirascope's openai.call and prompt_template decorators.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ipytest\nimport pytest\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\nipytest.autoconfig()\n\ndocuments = [\n    {\"id\": 0, \"text\": \"Bob eats burgers every day.\", \"semantic_score\": 0.8},\n    {\"id\": 1, \"text\": \"Bob's favorite food is not pizza.\", \"semantic_score\": 0.9},\n    {\"id\": 2, \"text\": \"I ate at In-N-Out with Bob yesterday\", \"semantic_score\": 0.5},\n    {\"id\": 3, \"text\": \"Bob said his favorite food is burgers\", \"semantic_score\": 0.9},\n]\n\n\nclass Relevance(BaseModel):\n    id: int = Field(..., description=\"The document ID\")\n    score: int = Field(..., description=\"The relevance score (1-10)\")\n    document: str = Field(..., description=\"The document text\")\n    reason: str = Field(..., description=\"A brief explanation for the assigned score\")\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    response_model=list[Relevance],\n    json_mode=True,\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Document Relevance Assessment\n    Given a list of documents and a question, determine the relevance of each document to answering the question.\n\n    Input\n        - A question\n        - A list of documents, each with an ID and content summary\n\n    Task\n        - Analyze each document for its relevance to the question.\n        - Assign a relevance score from 1-10 for each document.\n        - Provide a reason for each score.\n\n    Scoring Guidelines\n        - Consider both direct and indirect relevance to the question.\n        - Prioritize positive, affirmative information over negative statements.\n        - Assess the informativeness of the content, not just keyword matches.\n        - Consider the potential for a document to contribute to a complete answer.\n\n    Important Notes\n        - Exclude documents with no relevance less than 5 to the question.\n        - Be cautious with negative statements - they may be relevant but are often less informative than positive ones.\n        - Consider how multiple documents might work together to answer the question.\n        - Use the document title and content summary to make your assessment.\n\n    Documents:\n    {documents}\n\n    USER: \n    {query}\n    \"\"\"\n)\ndef llm_query_rerank(documents: list[dict], query: str): ...\n```\n\n----------------------------------------\n\nTITLE: Running the Search Query and Processing Results\nDESCRIPTION: This snippet defines a function to execute the entire search and extraction process. It captures the user query, retrieves data using the search tool, and applies the extraction method to structure the results.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/search_with_sources.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef run(question: str):\n    response = search(question)\n    if tool := response.tool:\n        output = tool.call()\n        result = extract(question, output)\n        return result\n\n\nprint(run(\"What is the average price of a house in the United States?\"))\n```\n\n----------------------------------------\n\nTITLE: Defining break_into_subproblems Function with Mirascope Decorators\nDESCRIPTION: This snippet defines the `break_into_subproblems` function, which uses the `@openai.call` and `@prompt_template` decorators from Mirascope. These decorators configure the function to interact with the OpenAI API using the specified model ('gpt-4o') and to use the provided prompt template. The function aims to break down a given problem (`query`) into subproblems, solvable by the LLM using specified tools.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/decomposed_prompting.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"@openai.call(model=\"gpt-4o\", response_model=Problem)\n@prompt_template(\n    \\\"\\\"\\\"\n    Your job is to break a problem into subproblems so that it may be solved\n    step by step, using at most one function call at each step.\n\n    You have access to the following functions which you can use to solve a\n    problem:\n    split: split a string into individual words\n    substring: get the ith character of a single string.\n    concat: concatenate some number of strings.\n\n    Here is an example of how it would be done for the problem: Get the first two\n    letters of the phrase 'Round Robin' with a period and space in between them.\n    Steps:\n    split 'Round Robin' into individual words\n    substring the 0th char of 'Round'\n    substring the 0th char of 'Robin'\n    concat ['R', '.', ' ', 'R']\n\n    Now, turn this problem into subtasks:\n    {query}\n    \\\"\\\"\\\"\n)\ndef break_into_subproblems(query: str): ...\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequential LLM Chain for B2B Trend Insights in Python\nDESCRIPTION: This code snippet demonstrates a sequential LLM chain using Mirascope to generate B2B trend insights. It first selects an expert for a given industry, then uses that expert's persona to identify trends related to a specific topic.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-chaining.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Sequential chain\nfrom mirascope.core import llm, prompt_template\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\")\n@prompt_template(\"Name an expert known for insights on {industry} trends\")\ndef select_expert(industry: str): ...\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Imagine that you are {expert}, a renowned expert in {industry}.\n    Provide insights on a key {industry} trend related to {topic}.\n    \"\"\"\n)\ndef identify_trend(industry: str, topic: str) -> openai.OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"expert\": select_expert(industry)}}\n\n\nprint(identify_trend(\"AI\", \"automation\"))\n```\n\n----------------------------------------\n\nTITLE: Parallel Async Calls Implementation\nDESCRIPTION: Shows how to execute multiple async operations concurrently using asyncio.gather.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/async.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python hl_lines=\"8 13-14\"\n--8<-- \"build/snippets/learn/async/parallel/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n```\n\n----------------------------------------\n\nTITLE: Making OpenAI API Calls with Mirascope Call Decorators\nDESCRIPTION: Shows how to use Mirascope's OpenAI call decorator to transform a prompt template function into a direct API call. This simplifies interaction with the OpenAI API by handling the communication automatically.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_recipe(ingredient: str) -> str:\n    return f\"Recommend recipes that use {ingredient}.\"\n\n\nresponse = recommend_recipe(\"apples\")\nprint(response.content)  # prints the string content of the completion\n```\n\n----------------------------------------\n\nTITLE: Calling the Semantic Segmentation Function\nDESCRIPTION: This code snippet calls the `semantic_segmentation` function defined earlier with the `SAMPLE_ARTICLE` as input.  It then prints the returned list of `Segment` objects, which contain the segmented topics and their corresponding content. This demonstrates how to use the defined prompt and LLM call to segment an article.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/document_segmentation.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"segments = semantic_segmentation(SAMPLE_ARTICLE)\\nprint(segments)\"\n```\n\n----------------------------------------\n\nTITLE: Defining Chatbot with Tool Integration in Python\nDESCRIPTION: This snippet defines a Chatbot class that integrates a WebSearch tool and manages user history. It uses decorators to configure model settings and a prompt template for query handling. The class includes methods for tool invocation and user prompts before calling tools, thereby allowing for more controlled interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Chatbot(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch])\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. \n        Your task is to answer the user's question using the provided tools.\n        You have access to the following tools:\n            - `WebSearch`: Search the web for information.\n            - `RequestAssistance`: Request assistance from a human expert if you do not\n                know how to answer the question.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness. The goal is to\n        provide as much information to the writer as possible without overwhelming them.\n\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    def _call(self, question: str | None = None): ...\n\n    def _interrupt_before(self, tool: openai.OpenAITool) -> openai.OpenAITool | None:\n        \"\"\"Interrupt before the tool is called. Return the modified tool or None.\"\"\"\n        if not isinstance(tool, WebSearch):\n            return tool\n        response = input(f\"Do you want to use the {tool._name()} tool? (y/n): \")\n        if response.lower() in [\"n\", \"no\"]:\n            response = input(\n                f\"Do you want to modify the {tool._name()} tool's query? (y/n): \"\n            )\n            if response.lower() in [\"n\", \"no\"]:\n                return None\n            else:\n                tool.query = input(\"(Assistant): Enter a new query: \")\n                return tool\n        else:\n            return tool\n\n    def _step(self, question: str | None = None):\n        response = self._call(question)\n        tools_and_outputs = []\n        for chunk, tool in response:\n            if tool:\n                new_tool = self._interrupt_before(tool)\n                if new_tool:\n                    tools_and_outputs.append((new_tool, new_tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.history.append(response.user_message_param)\n        self.history.append(response.message_param)\n        if tools_and_outputs:\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step()\n        return response.content\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question in [\"quit\", \"exit\"]:\n                print(\"(Assistant): Have a great day!\")\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(question)\n            print(\"\")\n```\n\n----------------------------------------\n\nTITLE: Chain-of-Thought Prompting Example\nDESCRIPTION: This illustrates chain-of-thought (CoT) prompting, where the LLM reasons through a series of steps to break down a complex task. It's useful for more accurate responses by arriving at an intermediate response which informs the final response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Solve the following math problem step by step:\\nA store had 150 apples. They sold 30 apples in the morning and 50 apples \\nin the afternoon. How many apples are left?\\n\\nStart with the initial number of apples.\\nSubtract the apples sold in the morning.\\nSubtract the apples sold in the afternoon.\\nProvide the final count of apples left.\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Answers and Sources using Groq API - Python\nDESCRIPTION: This function extracts a concise answer from the search results and includes the sources used. It takes a question string and a dictionary of search results, returning a structured SearchResponse with the answer and sources.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n@throttled_groq_call(\n    \"llama-3.2-90b-text-preview\", response_model=SearchResponse, json_mode=True\n)  # Changed response_model from SearchType to SearchResponse\n@prompt_template(\n    \"\"\"\nSYSTEM:\nExtract the answer to the question based on the search results.\nProvide the sources used to answer the question in a structured format.\nSearch results:\n{results}\n\nUSER:\n{question}\n\"\"\"\n)\ndef extract(\n    question: str, results: dict[str, str]\n) -> SearchResponse:  # This function should return SearchResponse, not SearchType\n    \"\"\"\n    Extract a concise answer from the search results and include sources.\n    \"\"\"\n    ...\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Outline-Based Text Summarization with OpenAI in Python\nDESCRIPTION: Implements a Chain of Thought (CoT) prompt by creating an outline before summarizing text. It leverages Mirascope with OpenAI to improve the coherence of the generated summaries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_summarization.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Summarize the following text by first creating an outline with a nested structure,\n    listing all major topics in the text with subpoints for each of the major points.\n    The number of subpoints for each topic should correspond to the length and\n    importance of the major point within the text. Then create the actual summary using\n    the outline.\n    {text}\n    \"\"\"\n)\ndef summarize_text_with_outline(text: str): ...\n\n\nprint(summarize_text_with_outline(text))\n```\n\n----------------------------------------\n\nTITLE: Human-in-the-Loop Agent Implementation\nDESCRIPTION: Implementation showing how to add human assistance capabilities to an AI agent, allowing it to request help when needed.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/agents.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python hl_lines=\"8-14 23\"\n--8<-- \"build/snippets/learn/agents/human_in_the_loop/{ provider | provider_dir }/{ method }.py\"\n```\n```\n\n----------------------------------------\n\nTITLE: Validating LLM Outputs with Mirascope and Pydantic\nDESCRIPTION: This code demonstrates basic output validation using Mirascope's response_model feature. It extracts task details from text using a GPT-4o-mini model and validates the structure using a Pydantic BaseModel.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-frameworks.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom mirascope.core import openai, prompt_template\n\nclass TaskDetails(BaseModel):\n    due_date: str\n    priority: str\n\n@openai.call(model=\"gpt-4o-mini\", response_model=TaskDetails)\n@prompt_template(\"Extract the task details from this text: {text}\")\ndef extract_task(text: str):\n    ...\n\nresponse = extract_task(text=\"Finish the report by Friday, it's high priority\")\nprint(response.due_date)  # Output: Friday\nprint(response.priority)  # Output: high \n```\n\n----------------------------------------\n\nTITLE: Base OpenAI Agent Implementation\nDESCRIPTION: Abstract base class for OpenAI agents with streaming capability and message history management.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom abc import abstractmethod\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass OpenAIAgent(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @abstractmethod\n    def _step(self, prompt: str) -> openai.OpenAIStream: ...\n\n    def run(self, prompt: str) -> str:\n        stream = self._step(prompt)\n        result, tools_and_outputs = \"\", []\n\n        for chunk, tool in stream:\n            if tool:\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                result += chunk.content\n                print(chunk.content, end=\"\", flush=True)\n        if stream.user_message_param:\n            self.history.append(stream.user_message_param)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            return self.run(\"\")\n        print(\"\\n\")\n        return result\n```\n\n----------------------------------------\n\nTITLE: Generating Knowledge Graph with OpenAI Call\nDESCRIPTION: This snippet defines a function that generates a knowledge graph based on the provided question and an HTML file, using OpenAI's model with an engineered prompt.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\", response_model=KnowledgeGraph)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your job is to create a knowledge graph based on the text and user question.\n    \n    The article:\n    {text}\n\n    Example:\n    John and Jane Doe are siblings. Jane is 25 and 5 years younger than John.\n    Node(id=\"John Doe\", type=\"Person\", properties={{\"age\": 30}})\n    Node(id=\"Jane Doe\", type=\"Person\", properties={{\"age\": 25}})\n    Edge(source=\"John Doe\", target=\"Jane Doe\", relationship=\"Siblings\")\n\n    USER:\n    {question}\n    \"\"\"\n)\ndef generate_knowledge_graph(\n    question: str, file_name: str\n) -> openai.OpenAIDynamicConfig:\n    text = get_text_from_html(file_name)\n    return {\"computed_fields\": {\"text\": text}}\n```\n\n----------------------------------------\n\nTITLE: Chaining LLM Calls Using Mirascope Computed Fields\nDESCRIPTION: Demonstrates how to chain LLM calls using Mirascope's computed fields approach, where recommend_attraction is called first and its result is injected into explain_attraction's prompt template. The example shows a tourist attraction recommendation system.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-sucks.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n@openai.call(\"gpt-4o\")\n@prompt_template(\n    \"\"\"\n    Recommend a popular tourist attraction in {city}.\n    Give me just the name.\n    \"\"\"\n)\ndef recommend_attraction(city: str):\n\n@openai.call(\"gpt-4o\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest travel guide.\n    Your task is to explain why the tourist attraction \"{attraction_name}\" is popular in {city}.\n\n    USER:\n    Explain why \"{attraction_name}\" in {city} is popular.\n    \"\"\"\n)\ndef explain_attraction(city: str) -> openai.OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"attraction_name\": recommend_attraction(city)}}\n\nexplanation = explain_attraction(\"Paris\")\nprint(explanation)\n```\n\n----------------------------------------\n\nTITLE: Integrating FastAPI with Mirascope in Python using v0 and v1\nDESCRIPTION: This snippet shows how to integrate FastAPI with Mirascope using v0 and v1. It demonstrates the transition from a class-based extractor approach to a more concise function-based approach with decorators, simplifying the integration process.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookRecommender(OpenAIExtractor[Book]):\n    extract_schema: type[Book] = Book\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\n\n@app.get(\"/recommend_book\")\ndef recommend_book(genre: str):\n    recommender = BookRecommender(genre=genre)\n    return recommender.extract()\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@app.get(\"/recommend_book\")\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n```\n\n----------------------------------------\n\nTITLE: Implementing WebSearch Tool Class\nDESCRIPTION: Definition of WebSearch class that extends OpenAITool to provide web search capabilities using DuckDuckGo and content parsing with BeautifulSoup.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\nclass WebSearch(openai.OpenAITool):\n    \"\"\"Search the web for the given text and parse the paragraphs of the results.\"\"\"\n\n    query: str = Field(..., description=\"The text to search for.\")\n\n    def call(self) -> str:\n        \"\"\"Search the web for the given text and parse the paragraphs of the results.\n\n        Returns:\n            Parsed paragraphs of each of the webpages, separated by newlines.\n        \"\"\"\n        try:\n            # Search the web for the given text\n            results = DDGS(proxy=None).text(self.query, max_results=2)\n\n            # Parse the paragraphs of each resulting webpage\n            parsed_results = []\n            for result in results:\n                link = result[\"href\"]\n                try:\n                    response = requests.get(link)\n                    soup = BeautifulSoup(response.content, \"html.parser\")\n                    parsed_results.append(\n                        \"\\n\".join([p.text for p in soup.find_all(\"p\")])\n                    )\n                except Exception as e:\n                    parsed_results.append(\n                        f\"{type(e)}: Failed to parse content from URL {link}\"\n                    )\n\n            return \"\\n\\n\".join(parsed_results)\n\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n\n```\n\n----------------------------------------\n\nTITLE: Verifying Facts Using Chain of Verification\nDESCRIPTION: This snippet provides an example of using the Chain of Verification technique to ensure the accuracy of an LLM-generated list. \nThe process involves generating verification questions for each list item, answering these to check for correctness, and then revising the original list as needed. \nDependencies include the ability of the LLM to understand and generate verification questions and answers based on prompted input.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nList five famous scientists and their primary field of contribution. Please make sure that all the facts are verified correctly by generating and answering verification questions for each scientist's field of contribution. After the verification process, update the list with any necessary corrections. This is very important.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nPreliminary List of Scientists and Their Fields:\n- Albert Einstein - Physics (Theory of Relativity)\n- Marie Curie - Chemistry/Physics (Radioactivity)\n- Isaac Newton - Physics/Mathematics (Classical Mechanics, Laws of Motion)\n- Charles Darwin - Biology (Theory of Evolution)\n- Niels Bohr - Physics (Quantum Mechanics)\n\nVerification Questions\n1. Albert Einstein:\nQuestion: Is Einstein primarily known for contributions in the field of physics, specifically for his development of the theory of relativity?\nExpected Answer: Yes, Einstein is renowned for his theories of special and general relativity, which revolutionized our understanding of space, time, and gravity.\n2. Marie Curie:\nQuestion: Did Marie Curie primarily contribute to both chemistry and physics through her pioneering work on radioactivity?\nExpected Answer: Yes, Marie Curie won two Nobel Prizes—one in Physics (1903) for her work on radioactivity and one in Chemistry (1911) for her discovery of radium and polonium.\n3. Isaac Newton:\nQuestion: Is Newton’s primary contribution in the field of physics and mathematics, particularly with regard to the laws of motion and universal gravitation?\nExpected Answer: Yes, Isaac Newton is most famous for his laws of motion and the law of universal gravitation, which laid the foundation for classical mechanics.\n4. Charles Darwin:\nQuestion: Is Darwin best known for his contributions to biology through his theory of evolution by natural selection?\nExpected Answer: Yes, Charles Darwin's theory of evolution by natural selection is a cornerstone of modern biology.\n5. Niels Bohr:\nQuestion: Did Niels Bohr primarily contribute to the field of physics, particularly in the development of quantum mechanics and the structure of the atom?\nExpected Answer: Yes, Niels Bohr made significant contributions to the development of quantum mechanics and the understanding of atomic structure, particularly with his model of the atom.\n\nVerified List of Scientists and Their Fields:\n- Albert Einstein - Physics (Theory of Relativity)\n- Marie Curie - Chemistry/Physics (Radioactivity)\n- Isaac Newton - Physics/Mathematics (Classical Mechanics, Laws of Motion)\n- Charles Darwin - Biology (Theory of Evolution)\n- Niels Bohr - Physics (Quantum Mechanics, Atomic Structure)\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Reinsertion with Retry Logic\nDESCRIPTION: Shows how to collect and reinsert errors into subsequent retry attempts using Mirascope's collect_errors handler to improve success probability.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/retries.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(),\n    retry=retry_if_exception_type(ValidationError)\n)\n@collect_errors(ValidationError)\ndef make_recommendation() -> None:\n    response = recommend_book(\"fantasy\")\n    print(response.text)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Configuration Override in Mirascope\nDESCRIPTION: Shows how to override default call parameters at runtime in Mirascope without changing the general configuration. The example demonstrates setting a custom temperature for a book recommendation function.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Recommend me a {genre} book.\")\ndef recommend_book(genre: str) -> openai.OpenAIDynamicConfig:\n    return {\"call_params\": {\"temperature\": 0.8}}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chatbot Class with Conversation History in Python\nDESCRIPTION: This code snippet defines a Chatbot class that maintains conversation history. It includes methods for running the chatbot and updating the history. The example demonstrates how to use the run function multiple times while managing the conversation context.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nWhile in this example, we are calling the run function multiple times, there is nothing stopping you from updating the history inside the Chatbot. Note that in real-world applications, conversation history is typically stored in a cache, database, or other persistent storage systems, so add `computed_fields` as necessary to retrieve from storage.\n```\n\n----------------------------------------\n\nTITLE: Creating Prompts with Multiple Message Roles using Shorthand Method\nDESCRIPTION: Shows how to use the shorthand method to create prompts with different message roles such as system and user messages.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template\ndef recommend_book_prompt(genre: str) -> Messages.Type:\n    return [\n        Messages.System(\"You are a helpful book recommendation agent.\"),\n        f\"Recommend a {genre} book\",\n    ]\n\n\nprint(recommend_book_prompt(genre=\"fantasy\"))\n# Equivalent to:\n# [\n#   BaseMessageParam(role=\"system\", content=\"You are a helpful book recommendation agent.\"),\n#   BaseMessageParam(role=\"user\", content=\"Recommend a fantasy book\")\n# ]\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Iterative Chaining with Mirascope in Python\nDESCRIPTION: The final snippet focuses on iterative chaining techniques, highlighting a loop-like structure where outputs are continuously refined. This method is particularly useful for tasks that require developing or fine-tuning responses iteratively.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-chaining.md#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n// Code was not provided in the text for this snippet.\n```\n\n----------------------------------------\n\nTITLE: Implementing Response Validation with Mirascope\nDESCRIPTION: Demonstrates how to validate LLM responses using Mirascope's response_model and Pydantic. Creates a structured output model for movie information extraction.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Movie(BaseModel):\n    \"\"\"An extracted movie.\"\"\"\n\n    title: str\n    director: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Movie)\ndef extract_movie(text: str) -> str:\n    return f\"Extract {text}\"\n\n\nmovie = extract_movie(\"Inception directed by Christopher Nolan\")\nprint(movie)\n# Output: title='Inception' director='Christopher Nolan'\n```\n\n----------------------------------------\n\nTITLE: Text Summarization Prompt\nDESCRIPTION: This shows an example of text summarization prompting, where the LLM is asked to condense a given text into a shorter version. The goal is to retain the main points and essential information from the article.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Summarize the following article:\\n\\\"The city council has approved a new plan to revitalize the downtown area. The plan \\nincludes building new parks, renovating old buildings, and improving public \\ntransportation. The council believes these changes will attract more businesses and \\ntourists to the area, boosting the local economy. Construction is set to begin next \\nyear and will be completed in phases over the next five years.\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Input Validation for LLM Prompts in Python with Mirascope\nDESCRIPTION: Example showing how Mirascope's function-based approach enables input validation for prompt arguments. The function signature defines the expected parameter types, allowing IDE warnings and error messages for incorrect inputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-best-practices.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM: You are a TV show script editor.\n    USER: I'm working on a new storyline. What do you think? {storyline}\n    \"\"\"\n)\ndef editor(storyline: str): ...\n\n\nstoryline = \"...\"\nresponse = editor(storyline)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Tool with Mirascope\nDESCRIPTION: This code defines a custom tool `FormatBook` using Mirascope's `BaseTool`. It includes fields for the book's title and author, and a `call` method to format the information. The `recommend_book` function then uses this tool with the `openai.call` decorator to recommend books of a specific genre.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-tools.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import Field\n\nfrom mirascope.core import BaseTool, openai, prompt_template\n\n\nclass FormatBook(BaseTool):\n    \"\"\"Returns the title and author of a book nicely formatted.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n    author: str = Field(..., description=\"The author of the book in all caps.\")\n\n    def call(self) -> str:\n        return f\"{self.title} by {self.author}\"\n\n\n@openai.call(\n    model=\"gpt-4o\", tools=[FormatBook], call_params={\"tool_choice\": \"required\"}\n)\n@prompt_template(\"Recommend a {genre} book.\")\ndef recommend_book(genre: str): ...\n```\n\n----------------------------------------\n\nTITLE: Classifying Call Transcripts Using LLM in Python\nDESCRIPTION: The snippet defines a Pydantic model 'CallClassification' and a dynamic classification using LLM via the 'classify_transcript()'. This classifies support requests into billing, sales, or support categories based on transcript texts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/support_ticket_routing.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CallClassification(BaseModel):\n    calltype: Literal[\"billing\", \"sale\", \"support\"] = Field(\n        ...,    \n        description=\"\"\"The classification of the customer's issue into one of the 3: \n        'billing' for an inquiry about charges or payment methods,\n        'sale' for making a purchase,\n        'support' for general FAQ or account-related questions\"\"\",\n    )\n    reasoning: str = Field(\n        ...,    \n        description=\"\"\"A brief description of why the customer's issue fits into the\\\n              chosen category\"\"\",\n    )\n    user_email: str = Field(..., description=\"email of the user in the chat\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=CallClassification)\n@prompt_template(\n    \"\"\"\n    Classify the following transcript between a customer and the service bot:\n    {transcript}\n    \"\"\"\n)\ndef classify_transcript(transcript: str): ...\n```\n\n----------------------------------------\n\nTITLE: Defining a Tavily web search tool with Mirascope\nDESCRIPTION: This code defines a `WebSearch` tool using Mirascope's `OpenAITool` and the Tavily API. It initializes a `TavilyClient` with an API key from the environment and uses it to search the web for a given query. It returns the top 2 search results as context.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import ClassVar\n\nfrom pydantic import Field\nfrom tavily import TavilyClient\n\n\nclass WebSearch(openai.OpenAITool):  # noqa: F811\n    \"\"\"Search the web for the given text using the TavilyClient.\"\"\"\n\n    tavily_client: ClassVar[TavilyClient] = TavilyClient(\n        api_key=os.environ[\"TAVILY_API_KEY\"]\n    )\n    query: str = Field(..., description=\"The text to search for.\")\n\n    def call(self) -> str:\n        \"\"\"A web search tool that takes in a query and returns the top 2 search results.\"\"\"\n        return self.tavily_client.get_search_context(query=self.query, max_results=2)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Outputs with Mirascope in Python\nDESCRIPTION: Defines a `Book` Pydantic model and uses the `response_model` parameter in an OpenAI call to ensure the output conforms to the defined structure.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/structured_outputs.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -> str:\n    return f\"Extract the book from this text: {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nassert isinstance(book, Book)\nprint(book)\n```\n\n----------------------------------------\n\nTITLE: Defining Response Model with Pydantic in Python\nDESCRIPTION: This snippet demonstrates how to define a structured output using Pydantic's BaseModel for validating and parsing AI model responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-integration.md#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom mirascope import llm\\nfrom pydantic import BaseModel\\n\\n\\nclass WeatherResponse(BaseModel):\\n    temperature: float\\n    condition: str\\n\\n\\n@llm.call(\\n    provider=\"openai\",\\n    model=\"gpt-4o\",\\n    response_model=WeatherResponse,\\n)\\ndef get_weather(location: str) -> str:\\n    return f\"What's the weather in {location}\"\\n\\n\\nresponse = get_weather(\"Paris\")\\nprint(response.temperature, response.condition)  # Fully structured, type-safe output!\n```\n\n----------------------------------------\n\nTITLE: Provider-Specific LLM Call Configuration in Python\nDESCRIPTION: This code snippet illustrates how to use provider-specific modules for LLM calls in Mirascope. It shows the setup for a specific provider, including importing the necessary modules and configuring the call with provider-specific parameters.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/calls.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/calls/provider_specific/basic_usage/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: DataFrame Generator Implementation\nDESCRIPTION: Implementation of DataFrameGenerator class using Pydantic BaseModel for generating and managing synthetic data in pandas DataFrames.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/synthetic-data-generation.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Literal\n\nimport pandas as pd\n\nclass DataFrameGenerator(BaseModel):\n    data: list[list[Any]] = Field(\n        description=\"the data to be inserted into the dataframe\"\n    )\n    column_names: list[str] = Field(description=\"The names of the columns in data\")\n\n    def append_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n        return pd.concat([df, self.generate_dataframe()], ignore_index=True)\n\n    def generate_dataframe(self) -> pd.DataFrame:\n        return pd.DataFrame(dict(zip(self.column_names, self.data, strict=False)))\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=DataFrameGenerator)\n@prompt_template(\n    \"\"\"\n    Generate {num_datapoints} random but realistic datapoints of items which could\n    be listed in an e-commerce fashion store. Generate your response as `data` and\n    `column_names`, so that a pandas DataFrame may be generated with:\n    `pd.DataFrame(data, columns=column_names)`.\n\n    Format:\n    ProductName, Price, Stock\n\n    ProductName - the name of the fashion product\n    Price - the price of an individual product, in dollars (include cents)\n    Stock - the quantity of the item available in stock\n    \"\"\"\n)\ndef generate_df_data(num_datapoints: int): ...\n\ndf_data = generate_df_data(5)\ndf = df_data.generate_dataframe()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: JSON Mode with Response Model Validation in Python\nDESCRIPTION: Combines `json_mode` with `response_model` for output structure and type validation, using both JSON formatting and Pydantic validation features.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/structured_outputs.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -> str:\n    return f\"Extract the book from this text: {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nassert isinstance(book, Book)\nprint(book)\n```\n\n----------------------------------------\n\nTITLE: Testing Documentation Agent General Responses\nDESCRIPTION: This code tests the `DocumentationAgent`'s ability to provide general information based on a query. It verifies that the agent's response for a general query contains an expected string from the Mirascope documentation. If the `classification` is \"general\", the test asserts that the response content contains the `expected` string.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize(\n    \"query,expected\",\n    [\n        (\"What is Mirascope?\", \"a toolkit for building AI-powered applications\"),\n    ],\n)\ndef test_documentation_agent_general(query: str, expected: str):\n    documentation_agent = DocumentationAgent()\n    response = documentation_agent._call(query)\n    if response.classification == \"general\":\n        assert expected in response.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Gift Recommendations with Mirascope in Python\nDESCRIPTION: Illustrates Mirascope's approach to colocating call parameters and prompts, demonstrating type safety and inline documentation for LLM calls.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-sucks.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n@openai.call(model=\"gpt-4\", call_params={\"temperature\": 0.5})\n@prompt_template(\n    \"\"\"\n    Can you recommend some gifts based on the following occasion and category pairs?\n    {occasions_x_categories}\n    \"\"\"\n)\ndef recommend_gifts(\n    occasions: list[str], categories: list[str]\n) -> openai.OpenAIDynamicConfig:\n    occasions_x_categories = [\n        f\"Occasion: {occasion}, Category: {category}\"\n        for occasion in occasions\n        for category in categories\n    ]\n    return {\"computed_fields\": {\"occasions_x_categories\": occasions_x_categories}}\n\nresponse = recommend_gifts(\n    occasions=[\"valentine\", \"birthday\", \"wedding\"],\n    categories=[\"electronics\", \"jewelry\", \"books\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful LLM Calls in Python with Mirascope V1\nDESCRIPTION: Shows the improved approach in Mirascope V1 for LLM calls, using decorators and a clear separation of state and arguments. This example demonstrates how V1 makes it easier to distinguish between persistent state (genre) and call-specific arguments (query).\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/mirascope-v1-release.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    genre: str\n\n    @openai.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian. You specialize in the {self.genre} genre\n        USER: {query}\n        \"\"\"\n    )\n    def call(self, query: str): ...\n\n\nfantasy_librarian = Librarian(genre=\"fantasy\")\nresponse = fantasy_librarian.call(\"Recommend a book\")\nprint(response.content)\n\nresponse = fantasy_librarian.call(\"Recommend a book for beginners\")\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Streaming Agent Implementation\nDESCRIPTION: Example showing how to implement real-time streaming responses in an AI agent for better user experience.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/agents.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python hl_lines=\"23 24 35-46\"\n--8<-- \"build/snippets/learn/agents/streaming/{ provider | provider_dir }/{ method }.py\"\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Response Model - Python\nDESCRIPTION: This snippet defines the Response class, which contains fields for classification and content, used to structure the output from the DocumentationAgent. It relies on the Pydantic library for data modeling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/documentation_agent.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\n\nclass Response(BaseModel):\n    classification: Literal[\"code\", \"general\"] = Field(\n        ..., description=\"The classification of the question\"\n    )\n    content: str = Field(..., description=\"The response content\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Code Safety Check Using Mirascope\nDESCRIPTION: Defines a function to evaluate the safety of Python code using Mirascope's response_model. It uses a language model to determine if the code is safe, returning a boolean. Dependencies include Mirascope, a valid API key, and a connection to the internet. The input is a string of Python code, and the output is a boolean indicating safety.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/code_generation_and_execution.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"python\\nfrom mirascope.core import openai, prompt_template\\n\\n\\n@openai.call(model=\\\"gpt-4o-mini\\\", response_model=bool)\\n@prompt_template(\\n    \\\"\\\"\\\"\\n    SYSTEM:\\n    You are a software engineer who is an expert at reviewing whether code is safe to execute.\\n    Determine if the given string is safe to execute as Python code.\\n\\n    USER:\\n    {code}\\n    \\\"\\\"\\\"\\n)\\ndef evaluate_code_safety(code: str): ...\\n\\n\\n\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating Answer Relevance with Single Model in Python\nDESCRIPTION: This code shows how to evaluate the relevance of a generated answer using a single language model via an API. It defines a Pydantic model for the evaluation score and provides an example usage to assess relevance based on accuracy, completeness, focus, and clarity.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-evaluation.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    score: Literal[\"poor\", \"ok\", \"good\", \"great\", \"excellent\"] = Field(\n        ..., description=\"A score representing the relevance of the generated answer.\"\n    )\n    reasoning: str = Field(\n        ..., description=\"The reasoning for the score in 100 characters or less.\"\n    )\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Evaluate the relevance of the generated answer to the given question on a continuous scale from poor to excellent.\n    A generation is considered relevant (score > poor ) if it addresses the core of the question, provides accurate information, \n    and stays focused on the topic.\n\n    Consider the following aspects when evaluating the answer:\n    - Accuracy: Does the answer provide factually correct information?\n    - Completeness: Does the answer sufficiently cover the question?\n    - Focus: Does the answer stay on topic without introducing unnecessary or unrelated information?\n    - Clarity: Is the answer clear and easy to understand?\n\n    Use the following relevance scale:\n    \n    poor - No relevance; the answer is completely unrelated or nonsensical\n    ok - Low relevance; minor relation to the question but missing key details or accuracy\n    good - Moderate relevance; somewhat addresses the question but lacks depth or focus\n    great - High relevance; mostly answers the question but may lack completeness or introduce some off-topic information\n    excellent - Very high relevance; thoroughly answers the question with minor flaws\n\n    Provide a brief reasoning for your assigned score, considering different aspects such as accuracy, focus, and completeness.\n\n    Question: {question}\n    Answer: {answer}\n    \"\"\"\n)\ndef evaluate_answer_relevance(question: str, answer: str): ...\n\n\n# Example usage\nquestion = \"What are the benefits of renewable energy?\"\nanswer = \"Renewable energy comes from natural sources like solar and wind, which are infinite and produce less pollution.\"\nresponse = evaluate_answer_relevance(question=question, answer=answer)\nprint(response)\n# Output: score='good' reasoning=\"Accurate but lacks depth; doesn't cover all benefits comprehensively.\"\n\nquestion = \"What are the benefits of renewable energy?\"\nirrelevant_answer = \"I enjoy watching movies on the weekends.\"\nresponse = evaluate_answer_relevance(question=question, answer=irrelevant_answer)\nprint(response)\n# Output: score='poor' reasoning='The answer is completely unrelated to the question about renewable energy benefits.'\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Self-Ask with Mirascope and OpenAI\nDESCRIPTION: This comprehensive snippet demonstrates a basic implementation of the Self-Ask technique using Mirascope and OpenAI. It includes the definition of a FewShotExample type, a self_ask function decorated with Mirascope's OpenAI call and prompt template, and example usage with few-shot learning examples.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/self_ask.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\n\nfrom mirascope.core import openai, prompt_template\nfrom typing_extensions import TypedDict\n\n\nclass FewShotExample(TypedDict):\n    question: str\n    answer: str\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Examples:\n    {examples:lists}\n\n    Query: {query}\n    \"\"\"\n)\ndef self_ask(query: str, examples: list[FewShotExample]) -> openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"examples\": [\n                [example[\"question\"], example[\"answer\"]] for example in examples\n            ]\n        }\n    }\n\n\nfew_shot_examples = [\n    FewShotExample(\n        question=\"When does monsoon season end in the state the area code 575 is located?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which state is the area code 575 located in?\n            Intermediate answer: The area code 575 is located in New Mexico.\n            Follow up: When does monsoon season end in New Mexico?\n            Intermediate answer: Monsoon season in New Mexico typically ends in mid-September.\n            So the final answer is: mid-September.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What is the current official currency in the country where Ineabelle Diaz is a citizen?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which country is Ineabelle Diaz a citizen of?\n            Intermediate answer: Ineabelle Diaz is from Peurto Rico, which is in the United States of America.\n            Follow up: What is the current official currency in the United States of America?\n            Intermediate answer: The current official currency in the United States is the United States dollar.\n            So the final answer is: United States dollar.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"Where was the person who founded the American Institute of Public Opinion in 1935 born?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Who founded the American Institute of Public Opinion in 1935?\n            Intermediate answer: George Gallup.\n            Follow up: Where was George Gallup born?\n            Intermediate answer: George Gallup was born in Jefferson, Iowa.\n            So the final answer is: Jefferson.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What language is used by the director of Tiffany Memorandum?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Who directed the movie called Tiffany Memorandum?\n            Intermediate answer: Sergio Grieco.\n            Follow up: What language is used by Sergio Grieco?\n            Intermediate answer: Sergio Grieco speaks Italian.\n            So the final answer is: Italian.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What is the sports team the person played for who scored the first touchdown in Superbowl 1?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which player scored the first touchdown in Superbowl 1?\n            Intermediate answer: Max McGee.\n            Follow up: Which sports team did Max McGee play for?\n            Intermediate answer: Max McGee played for the Green Bay Packers.\n            So the final answer is: Green Bay Packers.\n            \"\"\"\n        ),\n    ),\n]\n\nquery = \"The birth country of Jayantha Ketagoda left the British Empire when?\"\nresponse = self_ask(query=query, examples=few_shot_examples)\nprint(response.content)\n\nresponse = self_ask(query=query, examples=[])\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Zero-Shot Chain of Thought with Mirascope and OpenAI\nDESCRIPTION: This snippet demonstrates a zero-shot Chain of Thought implementation using Mirascope's decorators to augment a prompt with 'Let's think step by step' instruction. The code creates a configurable function that can optionally add the CoT augmentation based on a parameter.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/chain_of_thought.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\ncot_augment = \"\\nLet's think step by step.\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{query} {cot_augment}\")\ndef call(query: str, cot_prompt: bool = False) -> openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"cot_augment\": cot_augment if cot_prompt else \"\",\n        }\n    }\n\n\nprompt = \"\"\"Olivia has $23. She bought five bagels for $3 each.\nHow much money does she have left?\"\"\"\n\nprint(call(query=prompt, cot_prompt=True))\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Tools with Mirascope in Python\nDESCRIPTION: Demonstrates how to create custom tools in Mirascope by subclassing BaseTool. This is useful when working with functions that don't have docstrings or when you need more control over the tool's behavior.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseTool\n\n\n# has no docstings\ndef get_weather(city: str) -> str: ...\n\n\nclass GetWeather(BaseTool):\n    \"\"\"Gets the weather in a city.\"\"\"\n\n    city: str\n\n    def call(self) -> str:\n        return get_weather(self.city)\n```\n\n----------------------------------------\n\nTITLE: TV Data Generation with Constraints\nDESCRIPTION: Implementation of TV data generation with specific price and type constraints using Pydantic models and OpenAI.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/synthetic-data-generation.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TV(BaseModel):\n    size: int = Field(description=\"The size of the TV\")\n    price: float = Field(description=\"The price of the TV in dollars (include cents)\")\n    tv_type: Literal[\"OLED\", \"QLED\"]\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=list[TV])\n@prompt_template(\n    \"\"\"\n    Generate {num_datapoints} random but realistic datapoints of TVs.\n    Output the datapoints as a list of instances of TV.\n\n    Make sure to abide by the following constraints:\n    QLEDS should be roughly (not exactly) 2x the price of an OLED of the same size\n    for both OLEDs and QLEDS, price should increase roughly proportionately to size\n    \"\"\"\n)\ndef generate_tv_data(num_datapoints: int): ...\n\nfor tv in generate_tv_data(10):\n    print(tv)\n```\n\n----------------------------------------\n\nTITLE: Implementing Qwant API Class for Web Searches\nDESCRIPTION: Defines a QwantApi class to interact with the Qwant search API, allowing searches of different types (web, news, images, videos).\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass QwantApi:\n    BASE_URL = \"https://api.qwant.com/v3\"\n\n    def __init__(self) -> None:\n        self.session = requests.Session()\n        self.session.headers.update(\n            {\n                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n            }\n        )\n\n    def search(\n        self,\n        q: str,\n        search_type: str = \"web\",\n        locale: str = \"en_US\",\n        offset: int = 0,\n        safesearch: int = 1,\n    ) -> dict[str, Any]:\n        params = {\"q\": q, \"locale\": locale, \"offset\": offset, \"safesearch\": safesearch}\n        url = f\"{self.BASE_URL}/search/{search_type}\"\n        response = self.session.get(url, params=params)\n        return response.json() if response.status_code == 200 else None\n```\n\n----------------------------------------\n\nTITLE: Implementing Emotion Prompting with Mirascope OpenAI Integration\nDESCRIPTION: This code demonstrates how to implement emotion prompting in Mirascope by conditionally appending an emotional phrase to prompts. It uses decorators to create a flexible call function that integrates with OpenAI's models, specifically gpt-4o-mini, and showcases how to generate an emotionally contextualized email template.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/emotion_prompting.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\nemotion_augment = \"This is very important to my career.\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{query} {emotion_augment}\")\ndef call(query: str, emotion_prompt: bool = False) -> openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"emotion_augment\": emotion_augment if emotion_prompt else \"\",\n        }\n    }\n\n\nprompt = \"\"\"Write me an email I can send to my boss about how I need to\ntake a day off for mental health reasons.\"\"\"\n\nprint(call(query=prompt, emotion_prompt=True))\n```\n\n----------------------------------------\n\nTITLE: Async Tools Integration\nDESCRIPTION: Shows implementation of async tools with type checking and proper await usage.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/async.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python hl_lines=\"10 16 18 24-25\"\n--8<-- \"build/snippets/learn/async/tools/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n```\n\n----------------------------------------\n\nTITLE: Streaming Partial Tools Example\nDESCRIPTION: This code snippet shows how to stream intermediate partial tools and their deltas using Mirascope by setting `stream={\"partial_tools\": True}`. It demonstrates how to retrieve the full tool data progressively, enabling the processing of tools that yield information over a period of time.  The code is specific to supported providers.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n --8<-- \"build/snippets/learn/tools/partial_tool_streams/{{ tool_method }}/{{ provider | provider_dir }}/{{ method }}.py\"\n\n```\n\n----------------------------------------\n\nTITLE: Extended Classification with Reasoning - Sentiment Analysis using Pydantic - Python\nDESCRIPTION: This snippet enhances the sentiment analysis by including additional reasoning in the response model using Pydantic's BaseModel. The model provides insight into the classification process beyond just the sentiment label.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_classification.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum\n\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Sentiment(Enum):\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n    POSITIVE = \"positive\"\n\n\nclass SentimentWithReasoning(BaseModel):\n    reasoning: str\n    sentiment: Sentiment\n\n\n@openai.call(\"gpt-4o-mini\", response_model=SentimentWithReasoning)\n@prompt_template(\n    \"\"\"\n    Classify the sentiment of the following text: {text}.\n    Explain your reasoning for the classified sentiment.\n    \"\"\"\n)\ndef classify_sentiment_with_reasoning(text: str): ...\n\n\ntext = \"I would recommend this product if it were cheaper...\"\nresponse = classify_sentiment_with_reasoning(text)\nprint(f\"Sentiment: {response.sentiment}\")\nprint(f\"Reasoning: {response.reasoning}\")\n```\n\n----------------------------------------\n\nTITLE: Writing Multi-Line Prompts with String Templates in Python\nDESCRIPTION: Shows how to create multi-line prompts using string templates which automatically clean the template to remove unnecessary tokens.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template(\"\"\"\n    USER:\n    Summarize the following text:\n\n    Text:\n    {text}\n\n    Summary:\n\"\"\")\ndef summarize_prompt(text: str) -> Messages.Type:\n    return locals()\n\n\nprint(summarize_prompt(text=\"This is a sample text that needs to be summarized.\"))\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key\nDESCRIPTION: This snippet sets the OpenAI API key in the environment variables to allow API calls to OpenAI services.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/search_with_sources.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Chained LLM Calls - Python\nDESCRIPTION: This snippet presents a method for implementing error handling in chained LLM calls, ensuring that if one call fails, the workflow can fall back to an earlier stage or original input.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAIError\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -> str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -> str:\n    return f\"Translate this text to {language}: {text}\"\n\n\ndef process_text_with_error_handling(text: str, target_language: str):\n    try:\n        summary = summarize(text).content\n    except OpenAIError as e:\n        print(f\"Error during summarization: {e}\")\n        summary = text  # Fallback to original text if summarization fails\n\n    try:\n        translation = translate(summary, target_language).content\n        return translation\n    except OpenAIError as e:\n        print(f\"Error during translation: {e}\")\n        return summary  # Fallback to summary if translation fails\n\n\nresult = process_text_with_error_handling(\"Long text here...\", \"French\")\nprint(\"Processed Result:\", result)\n```\n\n----------------------------------------\n\nTITLE: Function Calling for Flight Information Check\nDESCRIPTION: This snippet demonstrates how to set up a function in Mirascope for checking flight information. The function `get_flight_information` returns flight status based on provided details, and it integrates with OpenAI's functionality by registering it as a tool and using its docstring for auto-generated parameters.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\ndef get_flight_information(flight_number: str, date: str) -> str:\n    \"\"\"Get the current flight status for `flight_number` on `date` and print  it.\n\n    Args:\n        flight_number: The flight number, e.g., AA123.\n        date: The date of the flight in YYYY-MM-DD format.\n    \"\"\"\n    if flight_number == \"AA123\" and date == \"2024-08-05\":\n        return f\"The flight {flight_number} on {date} is on time.\"\n    elif flight_number == \"DL456\" and date == \"2024-08-05\":\n        return f\"The flight {flight_number} on {date} is delayed.\"\n    else:\n        return f\"I'm sorry, I don't have the information for flight {flight_number} on {date}.\"\n\n\n@openai.call(\"gpt-4o\", tools=[get_flight_information])\ndef flight_info(flight_number: str, date: str) -> str:\n    return f\"What's the flight status of {flight_number} on {date}?\"\n\n\nresponse = flight_info(\"AA123\", \"2024-08-05\")\nif tool := response.tool:\n    print(tool.call())\n# > The flight AA123 on 2024-08-05 is on time.\n```\n\n----------------------------------------\n\nTITLE: Using MCP Server Tools\nDESCRIPTION: Demonstrates how to use server tools with Mirascope's call decorators and handle tool responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/mcp/client.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/mcp/client.py:34:47\"\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Tool Using BaseTool Class in Mirascope\nDESCRIPTION: Demonstrates how to define an LLM tool using Mirascope's BaseTool class. Creates a custom tool class with defined arguments and field descriptions for city population lookup.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseTool, openai, prompt_template\nfrom pydantic import Field\n\n\ndef get_city_population(city: str) -> str:\n    \"\"\"Fetch the current population of a city.\"\"\"\n    if city == \"New York City\":\n        return f\"The current population of {city} is approximately 8.8 million.\"\n    elif city == \"Los Angeles\":\n        return f\"The current population of {city} is approximately 4 million.\"\n    else:\n        return f\"I'm sorry, I don't have the population data for {city}.\"\n\n\nclass GetCityPopulation(BaseTool):\n    \"\"\"Get the current population for a given city.\"\"\"\n\n    city: str = Field(\n        ...,\n        description=\"The name of the city, e.g., New York City.\",\n    )\n\n    def call(self):\n        return get_city_population(self.city)\n\n\n@openai.call(\"gpt-4o\", tools=[GetCityPopulation])\n@prompt_template(\"What's the population of {city}\")\ndef city_population(city: str): ...\n\n\nresponse = city_population(\"New York City\")\nif tool := response.tool:\n    print(tool.call())\n# > The current population of New York City is approximately 8.8 million.\n```\n\n----------------------------------------\n\nTITLE: Run Diverse Function\nDESCRIPTION: Defines the `run_diverse` function that calls the diverse function and returns the final response. It takes a prompt and number of variations as input.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/diverse.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\nasync def run_diverse(prompt, num_variations=3) -> str:\n    response = await diverse(prompt, num_variations)\n    return response\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Making API Calls to Language Models using LiteLLM in Python\nDESCRIPTION: This function wraps the LiteLLM library to make API calls to language models. It handles different input types, including strings and dictionaries, and supports various configuration options such as model selection, temperature, and max tokens. The function returns the model's response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/litellm/call.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom litellm import completion\nfrom mirascope.core.common import (\n    get_max_tokens,\n    get_model,\n    get_temperature,\n    get_top_p,\n)\nfrom mirascope.core.content import Content\nfrom mirascope.core.messages import Message\n\n\ndef call(\n    messages: Union[str, Dict[str, Any], List[Dict[str, Any]], List[Message]],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -> Dict[str, Any]:\n    \"\"\"Call the LLM API with the given messages.\n\n    Args:\n        messages: The messages to send to the LLM API.\n        model: The model to use for the API call.\n        temperature: The temperature to use for the API call.\n        max_tokens: The maximum number of tokens to generate.\n        top_p: The top_p value to use for the API call.\n        **kwargs: Additional keyword arguments to pass to the API call.\n\n    Returns:\n        The response from the LLM API.\n    \"\"\"\n    if isinstance(messages, str):\n        messages = [{\"role\": \"user\", \"content\": messages}]\n    elif isinstance(messages, dict):\n        messages = [messages]\n    elif isinstance(messages, list) and all(isinstance(m, Message) for m in messages):\n        messages = [\n            {\"role\": m.role, \"content\": m.content.to_dict() if isinstance(m.content, Content) else m.content}\n            for m in messages\n        ]\n\n    return completion(\n        model=get_model(model),\n        messages=messages,\n        temperature=get_temperature(temperature),\n        max_tokens=get_max_tokens(max_tokens),\n        top_p=get_top_p(top_p),\n        **kwargs,\n    )\n```\n\n----------------------------------------\n\nTITLE: Basic OpenAI API Call with String Template\nDESCRIPTION: Demonstrates a basic LLM API call using OpenAI with Mirascope's string template approach. The code shows how to create a simple prompt template and make an API call with minimal boilerplate.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/calls.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@prompt_template\n@llm.call(provider=\"openai\", model=\"gpt-4\")\ndef summarize(text: str) -> str:\n    \"\"\"Summarize the text below in a concise way:\n    {text}\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Injecting Chat History in Prompt Templates Using Python\nDESCRIPTION: This code snippet illustrates how to include chat history in prompts by unrolling messages into the return value of a prompt template. It emphasizes the use of a specific keyword for injecting messages at various positions within the template.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n{% for method, method_title in zip(prompt_writing_methods, prompt_writing_method_titles) %}\n=== \"{{ method_title }}\"\n\n    {% if method == \"string_template\" %}\n    ```python hl_lines=\"6-8 15-16 20-23\"\n    {% elif method == \"base_message_param\" %}\n    ```python hl_lines=\"7-9 14-15 19-22\"\n    {% else %}\n    ```python hl_lines=\"6 10-11 15-18\"\n    {% endif %}\n    --8<-- \"examples/learn/prompts/chat_history/{{ method }}.py\"\n    ```\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Conditional Chaining in Mirascope\nDESCRIPTION: Implements conditional logic in chains to decide which path to take based on previous results. The example shows how to handle different outcomes with conditional execution.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/chaining.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/chaining/advanced_techniques/conditional_chaining.py\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Book Recommendation Prompt Template in Python with Mirascope\nDESCRIPTION: This snippet demonstrates how to create a self-contained prompt function using Mirascope's prompt_template decorator. It formats a list of book titles and prepares a prompt for book recommendations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseDynamicConfig, prompt_template\n\n@prompt_template(\n    \"\"\"\n    I recently read these books: {titles_in_quotes}.\n    What should I read next?\n    \"\"\"\n)\ndef book_recommendation_prompt(book_titles: list[str]) -> BaseDynamicConfig:\n    titles_in_quotes = \", \".join([f'\"{title}\"' for title in book_titles])\n    return {\"computed_fields\": {\"titles_in_quotes\": titles_in_quotes}}\n```\n\n----------------------------------------\n\nTITLE: Final Researcher Class Implementation\nDESCRIPTION: Complete implementation of the Researcher class with the research tool method for use in the agent executor.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Researcher(ResearcherBaseWithStep):\n    def research(self, prompt: str) -> str:\n        \"\"\"Research a topic and summarize the information found.\n\n        Args:\n            prompt: The user prompt to guide the research. The content of this prompt\n                is directly responsible for the quality of the research, so it is\n                crucial that the prompt be clear and concise.\n\n        Returns:\n            The results of the research.\n        \"\"\"\n        print(\"RESEARCHING...\")\n        result = self.run(prompt)\n        print(\"RESEARCH COMPLETE!\")\n        return result\n```\n\n----------------------------------------\n\nTITLE: Extracting Web Content with BeautifulSoup in Python\nDESCRIPTION: This function `extract_content` serves as a tool in Mirascope to extract main content from a webpage using BeautifulSoup. It fetches the webpage, removes unwanted tags, and captures the text from elements identified as main content. Required dependencies include requests and BeautifulSoup.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/tools_and_agents.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef extract_content(url: str) -> str:\n    \"\"\"Extract the main content from a webpage.\n\n    Args:\n        url: The URL of the webpage to extract the content from.\n\n    Returns:\n        The extracted content as a string.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]\n        for tag in unwanted_tags:\n            for element in soup.find_all(tag):\n                element.decompose()\n        main_content = (\n            soup.find(\"main\")\n            or soup.find(\"article\")\n            or soup.find(\"div\", class_=re.compile(\"content|main\"))\n        )\n        if main_content:\n            text = main_content.get_text(separator=\"\\n\", strip=True)\n        else:\n            text = soup.get_text(separator=\"\\n\", strip=True)\n        lines = (line.strip() for line in text.splitlines())\n        return \"\\n\".join(line for line in lines if line)\n    except Exception as e:\n        return f\"{type(e)}: Failed to extract content from URL {url}\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data from LLM Responses in Python\nDESCRIPTION: Example code demonstrating how to use Mirascope to extract structured data (book title and author) from text using various LLM providers. The highlighted lines show the key components of Mirascope's API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/WELCOME.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/response_models/basic_usage/{{ provider | provider_dir }}/{{ method }}.py:3:7\"\n--8<-- \"build/snippets/learn/response_models/basic_usage/{{ provider | provider_dir }}/{{ method }}.py:10:21\"\n```\n\n----------------------------------------\n\nTITLE: Defining CallParams Class for Mistral API in Python\nDESCRIPTION: This class encapsulates the parameters used for making calls to the Mistral AI API. It includes attributes for model, messages, temperature, max_tokens, top_p, stream, safe_mode, and random_seed. The class also provides methods for parameter validation and conversion to API-compatible formats.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/mistral/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import List, Optional, Union\n\nfrom pydantic import BaseModel, Field, validator\n\nfrom mirascope.core.chat_message import ChatMessage\n\n\nclass CallParams(BaseModel):\n    \"\"\"Parameters for a call to the Mistral API.\"\"\"\n\n    model: str\n    messages: List[ChatMessage]\n    temperature: Optional[float] = Field(default=0.7, ge=0.0, le=1.0)\n    max_tokens: Optional[int] = None\n    top_p: Optional[float] = Field(default=1, ge=0.0, le=1.0)\n    stream: bool = False\n    safe_mode: bool = False\n    random_seed: Optional[int] = None\n\n    @validator(\"messages\")\n    def validate_messages(cls, v):\n        if len(v) == 0:\n            raise ValueError(\"messages must not be empty\")\n        return v\n\n    def to_api_dict(self) -> dict:\n        \"\"\"Convert the parameters to a dictionary for the API call.\"\"\"\n        api_dict = self.dict(exclude_none=True)\n        api_dict[\"messages\"] = [message.to_api_dict() for message in self.messages]\n        return api_dict\n```\n\n----------------------------------------\n\nTITLE: Accessing Original Call Response with Response Models\nDESCRIPTION: Shows how to access the original BaseCallResponse instance through the _response property in a Pydantic response model. This enables access to additional metadata from the original LLM call.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/response_models.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/response_models/basic_usage/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Handling Uncertainty in Sentiment Classification - Python\nDESCRIPTION: This code snippet enhances sentiment analysis by handling uncertainty through the addition of a certainty score to the classification model. It provides logic to determine whether the model's confidence meets a specified threshold before presenting the classification result.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_classification.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass SentimentAnalysisWithCertainty(BaseModel):\n    sentiment: Sentiment\n    certainty: float = Field(..., ge=0, le=1)\n    reasoning: str\n\n\nclass SentimentWithCertainty(BaseModel):\n    sentiment: Sentiment\n    reasoning: str\n    certainty: float\n\n\n@openai.call(\"gpt-4o-mini\", response_model=SentimentWithCertainty)\n@prompt_template(\n    \"\"\"\n    Classify the sentiment of the following text: {text}\n    Explain your reasoning for the classified sentiment.\n    Also provide a certainty score between 0 and 1, where 1 is absolute certainty.\n    \"\"\"\n)\ndef classify_sentiment_with_certainty(text: str): ...\n\n\ntext = \"This is the best product ever. And the worst.\"\nresponse = classify_sentiment_with_certainty(text)\nif response.certainty > 0.8:\n    print(f\"Sentiment: {response.sentiment}\")\n    print(f\"Reasoning: {response.reasoning}\")\n    print(f\"Certainty: {response.certainty}\")\nelse:\n    print(\"The model is not certain enough about the classification.\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Pairwise Comparison Prompt for LLM Judge\nDESCRIPTION: This code snippet illustrates how to create a prompt for an LLM judge to perform pairwise comparison of translations. It includes evaluation criteria and instructions for providing a detailed explanation and final verdict.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-as-judge.md#2025-04-21_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n\"Please act as an impartial judge and evaluate the quality of the translations provided by two AI assistants for the source text and context displayed below. You should choose the translation that better adheres to the evaluation criteria outlined here: \n\n- Accuracy: Faithfulness to the meaning of the source text.\n- Fluency: Grammatical correctness and naturalness of the translation in the target language.\n- Cultural appropriateness: Adaptation to the cultural and linguistic norms of the target audience.\n- Tone: Aonsistency with the intended tone and style as described in the context.\n\nBegin your evaluation by comparing the two translations against the criteria above and provide a detailed explanation of your decision. Avoid any position biases, and ensure that the order in which the translations are presented does not influence your decision. Do not allow response length or assistant names to influence your evaluation. Be as objective and specific as possible in your explanation. \n\nAfter providing your explanation, output your final verdict in the following format: \n1. \\\"A\\\" if translation A is better\n2. \\\"B\\\" if translation B is better\n3. \\\"C\\\" for a tie.\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested LLM Calls with Mirascope\nDESCRIPTION: Demonstrates how to compose complex workflows by nesting LLM calls within each other. The highlighted lines show how one LLM function can be called within another for compact, readable code.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/WHY.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/chaining/nested_chains/{{ provider | provider_dir }}/shorthand.py\"\n```\n\n----------------------------------------\n\nTITLE: Generating verification questions using prompt template and LLM call\nDESCRIPTION: This code defines a function `get_verification_questions` that generates verification questions based on a given query and response.  It utilizes Mirascope's `openai.call` and `prompt_template` decorators to interact with the LLM using a predefined prompt. The prompt instructs the LLM to rephrase relevant statements from the response into questions that can verify the response's accuracy.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/chain_of_verification.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\", response_model=VerificationQuestions)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You will be given a query and a response to the query.\n    Take the relevant statements in the response and rephrase them into questions so\n    that they can be used to verify that they satisfy the original query.\n    USER:\n    Query:\n    {query}\n\n    Response:\n    {response}\n    \"\"\"\n)\ndef get_verification_questions(query: str, response: str): ...\n```\n\n----------------------------------------\n\nTITLE: Generating Movie Recommendation Prompt Using Mirascope\nDESCRIPTION: This code defines a function to create a dynamic prompt for recommending movies based on previously watched titles. It utilizes Mirascope's decorators to convert the prompt into an API call for OpenAI's model, ensuring better management of the prompt context.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    I've recently watched the following movies: {titles_in_quotes}.\n    What should I watch next?\n    \"\"\"\n)\ndef recommend_movie_prompt(movie_titles: list[str]) -> openai.OpenAIDynamicConfig:\n    titles_in_quptes = \", \".join([f'\"{title}\"' for title in movie_titles])\n    return {\"computed_fields\": {\"titles_in_quotes\": titles_in_quotes}}\n\n\nresponse = recommend_movie_propmt([\"The Dark Knight\", \"Forrest Gump\"])\n```\n\n----------------------------------------\n\nTITLE: Executing Zero-shot Reasoning with LLama Model using Mirascope\nDESCRIPTION: Demonstrates zero-shot reasoning using the LLama model to count occurrences of 's' in a string without additional context or guidance. Illustrates limitations without guided steps.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/o1_style_thinking.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom mirascope.core import groq\n\nhistory: list[dict[str, str]] = []\n\n@groq.call(\\\"llama-3.1-8b-instant\\\")\ndef generate_answer(question: str) -> str:\n    return f\\\"Generate an answer to this question: {question}\\\"\n\ndef run() -> None:\n    question: str = \\\"how many s's in the word mississssippi\\\"\n    response: str = generate_answer(question)\n    print(f\\\"(User): {question}\\\")\n    print(f\\\"(Assistant): {response}\\\")\n    history.append({\\\"role\\\": \\\"user\\\", \\\"content\\\": question})\n    history.append({\\\"role\\\": \\\"assistant\\\", \\\"content\\\": response})\n\n\nrun()\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Call with OpenAI\nDESCRIPTION: This example demonstrates a basic call to the OpenAI API using the Mirascope library, specifically illustrating how to use the 'call' decorator to ask for the capital of a country.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/quickstart.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef get_capital(country: str) -> str:\n    return f\"What is the capital of {country}?\"\n\n\nresponse = get_capital(\"Japan\")\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Testing LLM Query Reranking Function\nDESCRIPTION: This code defines a pytest function to test the llm_query_rerank function. It checks if the function correctly ranks documents based on their relevance to a given query, ensuring a minimum relevance score and correct ordering of top results.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize(\n    \"query,documents,top_n_ids\",\n    ((\"What is Bob's favorite food\", documents, {3, 0}),),\n)\ndef test_llm_query_rerank(query: str, documents: list[dict], top_n_ids: set[int]):\n    \"\"\"Tests that the LLM query rerank ranks more relevant documents higher.\"\"\"\n    response = llm_query_rerank(documents, query)\n    results = sorted(response, key=lambda x: x.score or 0, reverse=True)\n    assert all(result.score > 5 for result in results)\n    assert top_n_ids.issuperset({result.id for result in results[: len(top_n_ids)]})\n\n\nipytest.run()\n```\n\n----------------------------------------\n\nTITLE: Handling Image Inputs with BaseMessageParam in Python\nDESCRIPTION: Shows how to directly create BaseMessageParam instances with image content for multi-modal interactions with LLMs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.message_param import BaseMessageParam\nfrom mirascope.core.base.content_param import TextParam, ImageParam\n\n\n@prompt_template\ndef describe_image_prompt(image: bytes) -> Messages.Type:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=[\n                TextParam(\"Describe this image: \"),\n                ImageParam(image),\n            ],\n        )\n    ]\n\n\n# Read an image from a file\nwith open(\"examples/data/cat.jpg\", \"rb\") as f:\n    image_bytes = f.read()\n\n# Use the image in a prompt\nprint(describe_image_prompt(image=image_bytes))\n# Returns a user message with both text and image content\n\n```\n\n----------------------------------------\n\nTITLE: Retrying Stream Operations\nDESCRIPTION: Implementation of retry logic for streaming operations, where the retry wrapper needs to be applied to both the API call and stream iteration process.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/retries.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@retry(stop=stop_after_attempt(3), wait=wait_exponential())\ndef process_stream():\n    for chunk in recommend_book(\"fantasy\").stream():\n        print(chunk, end=\"\")\n```\n\n----------------------------------------\n\nTITLE: Streaming Mirascope Response Models in Python\nDESCRIPTION: This snippet shows how to stream response models in Mirascope by setting stream=True in the call decorator. It defines a City model and a function that streams partial results of city information extraction.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-structured-output.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass City(BaseModel):\n    name: str\n    population: int\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=City, stream=True)\ndef describe_city(text: str) -> str:\n    return f\"Extract {text}\"\n\n\ncity_stream = describe_city(\"Tokyo has a population of 14.18 million\")\nfor partial_city in city_stream:\n    print(partial_city)\n# > name=None population=None\n#   name=None population=None\n#   name='' population=None\n#   name='Tokyo' population=None\n#   name='Tokyo' population=None\n#   name='Tokyo' population=None\n#   name='Tokyo' population=None\n#   name='Tokyo' population=141\n#   name='Tokyo' population=141800\n#   name='Tokyo' population=14180000\n#   name='Tokyo' population=14180000\n#   name='Tokyo' population=14180000\n#   name='Tokyo' population=14180000\n```\n\n----------------------------------------\n\nTITLE: Utilizing JSON Mode with Mirascope in Python\nDESCRIPTION: Demonstrates the use of `json_mode` in Mirascope to ensure output is provided in JSON format, enabling easy parsing into Python data structures.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/structured_outputs.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport json\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef extract_book(text: str) -> str:\n    return f\"Extract the book title and author from this text: {text}\"\n\n\nresponse = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(json.loads(response.content))\n```\n\n----------------------------------------\n\nTITLE: Using Computed Fields for Chain Tracing\nDESCRIPTION: Shows how to use computed fields in nested chains to improve tracing capabilities, allowing access to intermediate results from a single call.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/chaining.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/chaining/computed_fields/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Adding Few-Shot Examples to Mirascope Response Models in Python\nDESCRIPTION: This snippet demonstrates how to add few-shot examples to a Mirascope response model using the Field and ConfigDict classes from Pydantic. It shows the creation of a Destination model with example fields and a function to recommend a travel destination.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-structured-output.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Destination(BaseModel):\n    name: str = Field(..., examples=[\"KYOTO\"])\n    country: str = Field(..., examples=[\"Japan\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"name\": \"KYOTO\", \"country\": \"Japan\"}]}\n    )\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Destination, json_mode=True)\n@prompt_template(\n    \"\"\"\n    Recommend a {travel_type} travel destination.\n    Match example format EXCLUDING 'examples' key.\n    \"\"\"\n)\ndef recommend_destination(travel_type: str): ...\n\n\ndestination = recommend_destination(\"cultural\")\nprint(destination)\n# > name='PARIS' country='France'\n```\n\n----------------------------------------\n\nTITLE: Using JSON Mode with Response Models\nDESCRIPTION: Shows how to use JSON Mode instead of the default Tools mode with response models by setting json_mode=True. This provides an alternative method for structured output generation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/response_models.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/response_models/json_mode/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Basic JSON Mode Usage in Python with Mirascope\nDESCRIPTION: Demonstrates how to enable JSON Mode with the call decorator, specify output fields in the prompt, and handle JSON responses. Shows implementation across different prompt writing methods and providers.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/json_mode.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/json_mode/basic_usage/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Making an OpenAI API call with curl\nDESCRIPTION: This snippet demonstrates how to make an API call to OpenAI's chat completion endpoint using `curl`. It includes the necessary headers for content type and authorization, as well as a JSON payload containing the model specification, messages, and temperature parameters. The call sends a request to the OpenAI API and receives a response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-integration.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o-mini\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"temperature\": 0.7\n  }'\n```\n\n----------------------------------------\n\nTITLE: Using BasePrompt Class in Mirascope V1\nDESCRIPTION: Example of using the BasePrompt class in V1, which provides a similar interface to V0 but with the flexibility to run against any supported provider. This approach combines familiar class-based structure with improved provider flexibility.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/mirascope-v1-release.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BasePrompt, anthropic, openai\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"Recommend a {genre} book\"\n\n    genre: str\n\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\n\nopenai_response = prompt.run(openai.call(\"gpt-4o-mini\"))\nprint(openai_response.content)\n\nanthropic_response = prompt.run(anthropic.call(\"claude-3-5-sonnet-20240620\"))\nprint(anthropic_response.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel LLM Chain for B2B Strategy Recommendations in Python\nDESCRIPTION: This code snippet showcases a parallel LLM chain using Mirascope and asyncio to optimize response times. It simultaneously identifies a business leader and B2B trends for a given industry, then uses this information to recommend a strategic move.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-chaining.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Parallel chain\nimport asyncio\n\nfrom mirascope.core import llm, prompt_template\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Please identify a business leader who is well known for pioneering advancements in {industry}.\n    Respond only with the leader's name.\n    \"\"\"\n)\nasync def identify_business_leader(industry: str): ...\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Given the industry {industry}, return a list of emerging B2B trends.\n    Make sure to exclude general trends that apply to all industries, and respond only with the list.\n    \"\"\"\n)\nasync def identify_b2b_trends(industry: str): ...\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your task is to recommend a strategic business move. Pretend that you are {leader}.\n\n    USER:\n    Given the following emerging B2B trends in the {industry} industry:\n    {trends}\n    \n    Recommend a key strategic move that businesses in this space should consider.\n    \"\"\"\n)\nasync def recommend_strategy(industry: str) -> openai.OpenAIDynamicConfig:\n    leader, trends = await asyncio.gather(\n        identify_business_leader(industry), identify_b2b_trends(industry)\n    )\n    return {\n        \"computed_fields\": {\"leader\": leader, \"trends\": trends}\n    }\n\nasync def recommend_strategy_parallel_chaining(industry: str):\n    return await recommend_strategy(industry=industry)\n\nprint(asyncio.run(recommend_strategy_parallel_chaining(\"fintech\")))\n```\n\n----------------------------------------\n\nTITLE: Streaming LLM Responses with OpenAI in Python\nDESCRIPTION: Demonstrates how to use Mirascope's OpenAI decorator to stream responses from a language model. The function recommends a book based on a given genre, with the response streamed in chunks.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nfor chunk, _ in recommend_book(\"fantasy\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Answering verification questions using an LLM call\nDESCRIPTION: This code defines a function `answer` that takes a question (query) as input and returns a concise answer generated by the LLM. It utilizes Mirascope's `openai.call` decorator and prepends the question with \"Concisely answer the following question:\" to guide the LLM's response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/chain_of_verification.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\")\nasync def answer(query: str) -> str:\n    return f\"Concisely answer the following question: {query}\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Simple Data Types with Mirascope in Python\nDESCRIPTION: Demonstrates how to use Mirascope's response_model feature for extracting simple data types like lists of strings. This example extracts a list of book recommendations for a given genre.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o\", response_model=list[str])\ndef recommend_books(genre: str) -> str:\n    return f\"Recommend 3 {genre} books\"\n\n\nbooks = recommend_books(genre=\"fantasy\")\nprint(books)\n# > [\n#   'The Name of the Wind by Patrick Rothfuss',\n#   'Mistborn: The Final Empire by Brandon Sanderson',\n#   'The Way of Kings by Brandon Sanderson'\n#   ]\n```\n\n----------------------------------------\n\nTITLE: Injecting Document Inputs via String Templates in Python\nDESCRIPTION: This snippet demonstrates how to create prompts that accept document inputs using a string template approach, specifically focusing on handling PDF documents. The parameters include methods for writing prompts and their corresponding titles, allowing for dynamic injection of document data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n{% for method, method_title in zip(prompt_writing_methods, prompt_writing_method_titles) %}\n=== \"{{ method_title }}\"\n\n    {% if method == \"string_template\" %}\n    ```python hl_lines=\"5 15-17\"\n    {% elif method == \"base_message_param\" %}\n    ```python hl_lines=\"9-15 26-28\"\n    {% else %}\n    ```python hl_lines=\"8-14 24-26\"\n    {% endif %}\n    --8<-- \"examples/learn/prompts/multi_modal/document/{{ method }}.py\"\n    ```\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Extracting Built-in Python Types with Mirascope\nDESCRIPTION: Demonstrates how to use Mirascope to extract built-in Python types directly, simplifying the interface for simple extractions. It uses the OpenAI decorator to generate a list of book recommendations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", response_model=list[str])\ndef recommend_books(genre: str, num: str) -> str:\n    return f\"Recommend a list of {num} {genre} books\"\n\n\nbooks = recommend_books(\"fantasy\", 3)\nprint(books)\n# > ['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson', 'A Darker Shade of Magic by V.E. Schwab']\n```\n\n----------------------------------------\n\nTITLE: Defining Tools Array for OpenAI Chat Completion API\nDESCRIPTION: This code snippet demonstrates how to define the `tools` array for the OpenAI Chat Completion API.  It defines a function `get_current_stock_price` that the model can call and specifies its `name`, `description`, and `parameters` including the expected type, description, and required fields.  The parameters conform to the JSON specification, making the function usable by the LLM.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"tools\": [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_stock_price\",\n            \"description\": \"Get the current stock price\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"company\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the company, eg. Apple Inc.\",\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"USD\", \"EUR\", \"JPY\"],\n                    },\n                    \"required\": [\"company\", \"currency\"],\n                },\n            },\n        },\n    }\n]\"\n```\n\n----------------------------------------\n\nTITLE: Querying the Neo4j Knowledge Graph\nDESCRIPTION: This function queries the Neo4j knowledge graph with a specific question and retrieves the results. It converts the user's question into a Cypher query and returns the computed results.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-build-a-knowledge-graph.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Answer the following question based on the knowledge graph query results.\n\n    Results: {results}\n\n    USER:\n    {question}\n    \"\"\"\n)\ndef query_neo4j_kg(question: str, driver: Driver) -> openai.OpenAIDynamicConfig:\n    cypher_query = convert_query_to_cypher(question)\n    with driver.session() as session:\n        results = list(session.run(cypher_query, {}).data())\n    return {\"computed_fields\": {\"results\": results}}\n```\n\n----------------------------------------\n\nTITLE: Demonstration Ensembling Orchestration\nDESCRIPTION: This snippet defines the `demonstration_ensembling` function, which orchestrates the entire process. It calls the `aggregate_answers` function with the specified query and ensemble size, and then prints the content of the final response. This demonstrates the high-level implementation of the demonstration ensembling technique.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/demonstration_ensembling.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\nasync def demonstration_ensembling(query, ensemble_size):\n    response = await aggregate_answers(query=query, num_responses=ensemble_size)\n    print(response.content)\n\n\nquery = \"\"\"Help me write a notice that highlights the importance of attending \\\nthe company's social events. Give me just the notice, no further explanation.\"\"\"\n\nawait demonstration_ensembling(query=query, ensemble_size=5)\n```\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAITool Class for Function Calling in Python\nDESCRIPTION: This class inherits from BaseTool and implements OpenAI's function calling feature. It includes methods for generating tool specifications and processing responses from the OpenAI API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/openai/tool.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass OpenAITool(BaseTool):\n    \"\"\"Tool for OpenAI's function calling.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Initialize the OpenAITool.\"\"\"\n        super().__init__(*args, **kwargs)\n\n    def get_spec(self) -> Dict[str, Any]:\n        \"\"\"Get the tool specification.\n\n        Returns:\n            Dict[str, Any]: The tool specification.\n        \"\"\"\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": self.name,\n                \"description\": self.description,\n                \"parameters\": self.parameters,\n            },\n        }\n\n    def get_response(self, response: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Get the response from the tool.\n\n        Args:\n            response (Dict[str, Any]): The response from the OpenAI API.\n\n        Returns:\n            Dict[str, Any]: The processed response.\n        \"\"\"\n        return response\n```\n\n----------------------------------------\n\nTITLE: Implementing Enhanced Self-Consistency with Automated Answer Extraction in Python\nDESCRIPTION: This snippet improves upon the basic Self-Consistency implementation by adding an automated answer extraction capability. The `extract_number` function is used to pull the final numerical answer directly from each Chain of Thought response, which is more efficient than manual extraction. The expected output is an integer representing the most consistent answer extracted from the responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/self_consistency.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass Solution(BaseModel):\n    solution_value: int = Field(\n        ..., description=\"The actual number of a solution to a math problem.\"\n    )\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Solution)\n@prompt_template(\n    \"\"\"\n    Extract just the number of a solution to a math problem.\n    For example, for the solution:\n    Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has\n    58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls.\n    The answer is 33.\n    \n    You would extract 33.\n\n    Solution to extract from:\n    {response}\n    \"\"\"\n)\nasync def extract_number(response: str): ...\n\n\nasync def enhanced_self_consistency(\n    query: str, num_samples: int, few_shot_examples: list[dict[str, str]]\n) -> int:\n    cot_tasks = [chain_of_thought(query, few_shot_examples) for _ in range(num_samples)]\n    cot_responses = [response.content for response in await asyncio.gather(*cot_tasks)]\n    extract_number_tasks = [extract_number(response) for response in cot_responses]\n    response_numbers = [\n        response.solution_value\n        for response in await asyncio.gather(*extract_number_tasks)\n    ]\n    return most_frequent(response_numbers)\n\n\nresult = await enhanced_self_consistency(\n    query=query, num_samples=5, few_shot_examples=few_shot_examples\n)\nprint(f\"The most consistent answer is: {result}\")\n```\n\n----------------------------------------\n\nTITLE: Manual Streaming Audio Interaction\nDESCRIPTION: Example of streaming audio interaction without turn detection using OpenAI's Realtime API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/openai.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/openai/realtime/audio_stream_input_output_manually.py\"\n```\n\n----------------------------------------\n\nTITLE: Switching Between OpenAI and Anthropic Providers with Prompt Templates\nDESCRIPTION: Shows how to create a reusable prompt template that can be executed with different model providers (OpenAI and Anthropic) by changing only one line of code. Demonstrates Mirascope's flexibility in provider switching.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic, openai, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\n# OpenAI\nopenai_model = \"gpt-4o-mini\"\nopenai_recommend_book = openai.call(openai_model)(recommend_book_prompt)\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\n# Anthropic\nanthropic_model = \"claude-3-5-sonnet-20240620\"\nanthropic_recommend_book = anthropic.call(anthropic_model)(recommend_book_prompt)\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n```\n\n----------------------------------------\n\nTITLE: Making OpenAI API Calls in Python\nDESCRIPTION: This function handles API calls to OpenAI's language models. It supports both chat and completion endpoints, handles rate limiting and other errors, and processes the response. The function takes model name, messages, and various optional parameters as inputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/openai/call.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport json\nimport time\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom openai import OpenAI\nfrom openai.types.chat import ChatCompletion\nfrom openai.types.completion import Completion\n\nfrom mirascope.core.constants import OPENAI_CHAT_MODELS\nfrom mirascope.core.errors import MirascopeError\n\n\ndef call(\n    model: str,\n    messages: List[Dict[str, str]],\n    functions: Optional[List[Dict[str, Any]]] = None,\n    function_call: Optional[Union[str, Dict[str, Any]]] = None,\n    max_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    n: Optional[int] = None,\n    stream: Optional[bool] = None,\n    stop: Optional[Union[str, List[str]]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[Dict[str, float]] = None,\n    user: Optional[str] = None,\n    api_key: Optional[str] = None,\n    organization: Optional[str] = None,\n) -> Union[ChatCompletion, Completion]:\n    \"\"\"\n    Make an API call to OpenAI.\n    \"\"\"\n    client = OpenAI(api_key=api_key, organization=organization)\n\n    kwargs = {\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"n\": n,\n        \"stream\": stream,\n        \"stop\": stop,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"logit_bias\": logit_bias,\n        \"user\": user,\n    }\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n\n    if model in OPENAI_CHAT_MODELS:\n        kwargs[\"messages\"] = messages\n        if functions:\n            kwargs[\"functions\"] = functions\n        if function_call:\n            kwargs[\"function_call\"] = function_call\n        completion_function = client.chat.completions.create\n    else:\n        kwargs[\"prompt\"] = messages[0][\"content\"]\n        completion_function = client.completions.create\n\n    for _ in range(3):\n        try:\n            return completion_function(**kwargs)\n        except Exception as e:\n            if \"Rate limit\" in str(e):\n                time.sleep(5)\n            else:\n                raise MirascopeError(\n                    f\"Error calling OpenAI API: {e}\\n\\nArguments: {json.dumps(kwargs, indent=2)}\"\n                ) from e\n    raise MirascopeError(\n        f\"Error calling OpenAI API: Rate limit exceeded\\n\\nArguments: {json.dumps(kwargs, indent=2)}\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Prompts with Multiple Message Roles using String Templates\nDESCRIPTION: Shows how to use string templates to create prompts with different message roles including system and user messages.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template(\"\"\"\n    SYSTEM: You are a helpful book recommendation agent.\n    USER: Recommend a {genre} book\n\"\"\")\ndef recommend_book_prompt(genre: str) -> Messages.Type:\n    return locals()\n\n\nprint(recommend_book_prompt(genre=\"fantasy\"))\n# Equivalent to:\n# [\n#   BaseMessageParam(role=\"system\", content=\"You are a helpful book recommendation agent.\"),\n#   BaseMessageParam(role=\"user\", content=\"Recommend a fantasy book\")\n# ]\n\n```\n\n----------------------------------------\n\nTITLE: Switching Between Model Providers with Mirascope (OpenAI)\nDESCRIPTION: This snippet demonstrates how to use Mirascope's LLM call decorator to easily switch between different AI model providers. It shows an example of using OpenAI's model to suggest a movie based on a given genre.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n@openai.call(\"gpt-4o-mini\")\ndef suggest_movie(genre: str) -> str:\n    return f\"Suggest a {genre} movie\"\n\nresponse = suggest_movie(\"thriller\")\noriginal_response = response.response\n```\n\n----------------------------------------\n\nTITLE: Implementing Exact Match Evaluation for LLM Outputs in Python\nDESCRIPTION: This function performs an exact match evaluation on LLM outputs. It checks if all expected phrases are present in the output string, returning a boolean result. This method is useful for fact-checking or ensuring key information is included in model outputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-evaluation.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef exact_match_eval(output: str, expected: list[str]) -> bool:\n    return all(phrase in output for phrase in expected)\n```\n\n----------------------------------------\n\nTITLE: Building RAG Pipeline Using Mirascope - Python\nDESCRIPTION: This snippet demonstrates how to build a retrieval-augmented generation (RAG) pipeline using Mirascope, yielding clear and transparent control over the development process. It modifies the workflow from the LangChain example to utilize native Python constructs for data chaining and control, thus simplifying complex operations. The implementation uses modules from Mirascope for retrieving data and generating prompts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-frameworks.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```py\nfrom bs4 import SoupStrainer\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nfrom mirascope.core import openai, prompt_template\n\n# Load, chunk and index the contents of the blog.\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n\n# Retrieve and generate using the relevant snippets of the blog.\nretriever = vectorstore.as_retriever()\n\n\n@openai.call(\"gpt-4o-mini\", output_parser=str)\n@prompt_template(\n    \"\"\"\n    Use the following pieces of information to provide a concise overview.\n    If the details are insufficient, state that clearly without making assumptions.\n    Keep your response to three sentences for conciseness.\n    End with \"hope this provides clarity!\" to conclude your response.\n\n    {information}\n\n    Topic: {subject}\n\n    Overview:\n    \"\"\"\n)\ndef overview(subject: str) -> openai.OpenAIDynamicConfig:\n    information = \"\\n\\n\".join(doc.page_content for doc in retriever.invoke(subject))\n    return {\"computed_fields\": {\"information\": information}}\n\n\noutput = overview(\"Describe machine learning\")\nprint(output)\n```\n```\n\n----------------------------------------\n\nTITLE: Creating Prompt Templates with BaseMessageParam Instances in Python\nDESCRIPTION: Shows how to directly create BaseMessageParam instances to build prompt templates in Mirascope.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.message_param import BaseMessageParam\n\n\n@prompt_template\ndef recommend_book_prompt(genre: str) -> Messages.Type:\n    return [\n        BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n    ]\n\n\nprint(recommend_book_prompt(genre=\"fantasy\"))\n\n```\n\n----------------------------------------\n\nTITLE: Using Pydantic's validate_call with Mirascope prompt templates\nDESCRIPTION: This snippet demonstrates how to use Pydantic's validate_call decorator with Mirascope prompt templates to ensure inputs are correctly typed according to their argument type hints.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum\nfrom typing import Annotated, Type\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\nclass ComplianceStatus(Enum):\n    COMPLIANT = \"compliant\"\n    NON_COMPLIANT = \"non_compliant\"\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=ComplianceStatus)\n@prompt_template(\n    \"\"\"\n    Is the following data processing procedure compliant with GDPR standards?\n    {procedure_text}\n    \"\"\"\n)\ndef check_gdpr_compliance(procedure_text: str): ...\n\n\ndef validate_compliance(procedure_text: str) -> str:\n    \"\"\"Check if the data processing procedure is compliant with GDPR standards.\"\"\"\n    compliance_status = check_gdpr_compliance(procedure_text=procedure_text)\n    assert (\n        compliance_status == ComplianceStatus.COMPLIANT\n    ), \"Procedure is not GDPR compliant.\"\n    return procedure_text\n\n\nclass DataProcessingProcedure(BaseModel):\n    text: Annotated[str, AfterValidator(validate_compliance)]\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=DataProcessingProcedure)\n@prompt_template(\n    \"\"\"\n    Write a detailed description of a data processing procedure.\n    It MUST be compliant with GDPR standards.\n    \"\"\"\n)\ndef write_procedure(): ...\n\n\ntry:\n    print(write_procedure())\nexcept ValidationError as e:\n    print(e)\n    # > 1 validation error for DataProcessingProcedure\n    # procedure_text\n    # Assertion failed, Procedure is not GDPR compliant.\n    # [type=assertion_error, input_value=\"The procedure text here...\", input_type=str]\n    # For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n```\n\n----------------------------------------\n\nTITLE: Defining an Item model with Pydantic in Python\nDESCRIPTION: Defines a data model for receipt items using Pydantic. The model includes fields for the name, quantity, and price of items, each with a description. This model is used to structure the extracted information.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/extraction_using_vision.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass Item(BaseModel):\n    name: str = Field(..., description=\"The name of the item\")\n    quantity: int = Field(..., description=\"The quantity of the item\")\n    price: float = Field(..., description=\"The price of the item\")\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Decomposed Prompting\nDESCRIPTION: This snippet demonstrates how to use the `decomposed_prompting` function with a specific query. It defines the query and then calls the function, printing the result. This showcases how to initiate the decomposed prompting process and retrieve the final response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/decomposed_prompting.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"query = \\\"\\\"\\\"Take the last letters of the words in \\\"Augusta Ada King\\\" and concatenate them using a space.\\\"\\\"\\\"\\n\n\nprint(decomposed_prompting(query))\"\n```\n\n----------------------------------------\n\nTITLE: Answering with LLM using Mirascope and OpenAI\nDESCRIPTION: This snippet defines an asynchronous function `answer` that takes a query as input and returns a response from the OpenAI LLM, specifically the \"gpt-4o-mini\" model.  It uses the `@openai.call` decorator to integrate with Mirascope for making API calls to OpenAI and the `@prompt_template` decorator to define the prompt. It randomly selects 3 examples from `qa_examples` to provide context to the LLM.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/demonstration_ensembling.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Here are some examples that demonstrate the voice to use in a corporate setting.\n    {examples:lists}\n\n    With these examples, answer the following question:\n    {query}\n    \"\"\"\n)\nasync def answer(query: str) -> openai.OpenAIDynamicConfig:\n    random_indices = random.sample(range(len(qa_examples)), 3)\n    examples = [\n        [\n            f\"Question: {qa_examples[i]['question']}\",\n            f\"Answer: {qa_examples[i]['answer']}\",\n        ]\n        for i in random_indices\n    ]\n    return {\"computed_fields\": {\"examples\": examples}}\n```\n```\n\n----------------------------------------\n\nTITLE: Tracking LLM Calls with Automatic Versioning - Python\nDESCRIPTION: This snippet demonstrates how to use the Lilypad framework for automated version control of LLM calls. By decorating functions with @lilypad.generation, it keeps track of input and output while capturing metadata associated with each call.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-chaining.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport lilypad\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n@lilypad.generation()\ndef answer_question(question: str) -> str:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": question}],\n    )\n    return str(completion.choices[0].message.content)\n\nif __name__ == \"__main__\":\n    lilypad.configure()\n    answer = answer_question(\"What is the meaning of life?\")\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Accessing MCP Server Resources\nDESCRIPTION: Shows how to list and read resources from the MCP server, handling both text and blob content types.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/mcp/client.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/mcp/client.py:27:32\"\n```\n\n----------------------------------------\n\nTITLE: Replicating v0 OpenAICall Functionality in Mirascope v1 with Python\nDESCRIPTION: Demonstrates how to replicate the original v0 functionality of OpenAICall by writing custom 'call' and 'stream' methods in a BasePrompt subclass. This approach provides both synchronous and streaming capabilities.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BasePrompt, openai, prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book\")\nclass BookRecommender(BasePrompt):\n    genre: str\n\n    def call(self) -> openai.OpenAICallResponse:\n        return self.run(openai.call(\"gpt-4o-mini\"))\n\n    def stream(self) -> openai.OpenAIStream:\n        return self.run(openai.call(\"gpt-4o-mini\", stream=True))\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nprint(response.content)\n\nstream = recommender.stream()\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Extending Mirascope Prompt Template with Computed Fields\nDESCRIPTION: This code snippet extends the previous example by incorporating computed fields within the prompt template.  It uses the `BaseDynamicConfig` to pass dynamically computed values into the prompt. This approach enables more complex prompt construction based on input parameters and internal calculations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-tools.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseDynamicConfig, prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    Recommend some books on the following topic and genre pairs:\n    {topics_x_genres:list}\n    \"\"\"\n)\ndef recommend_book_prompt(topics: list[str], genres: list[str]) -> BaseDynamicConfig:\n    topics_x_genres = [\n        f\"Topic: {topic}, Genre: {genre}\" for topic in topics for genre in genres\n    ]\n    return {\"computed_fields\": {\"topics_x_genres\": topics_x_genres}}\n\n\nmessages = recommend_book_prompt([\"coding\", \"music\"], [\"fiction\", \"fantasy\"])\nprint(messages)\n# > [BaseMessageParam(role='user', content='Recommend some books on the following topic and genre pairs:\\nTopic: coding, Genre: fiction\\nTopic: coding, Genre: fantasy\\nTopic: music, Genre: fiction\\nTopic: music, Genre: fantasy')]\nprint(messages[0].content)\n# > Recommend some books on the following topic and genre pairs:\n#   Topic: coding, Genre: fiction\n#   Topic: coding, Genre: fantasy\n#   Topic: music, Genre: fiction\n#   Topic: music, Genre: fantasy\n```\n\n----------------------------------------\n\nTITLE: Running the WebSearchAgent\nDESCRIPTION: This code snippet demonstrates how to run the implemented WebSearchAgent. It creates an instance of the `WebSearchAgent` and calls the `run` method to start the interactive loop. The agent will then prompt the user for questions and provide answers using web searches and content extraction.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/tools_and_agents.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync def main():\n    web_assistant = WebSearchAgent()\n    await web_assistant.run()\n\n\n# Run main in a jupyter notebook\nawait main()\n\n# Run main in a python script\n# import asyncio\n# asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Requesting Human Assistance Tool Implementation in Python\nDESCRIPTION: This snippet defines a RequestAssistance tool which prompts a human for input when the chatbot cannot answer a question adequately. The tool is designed to collect specific requests for assistance and integrates with the chatbot to facilitate human-AI collaboration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass RequestAssistance(openai.OpenAITool):\n    \"\"\"A tool that requests assistance from a human expert.\"\"\"\n\n    query: str = Field(\n        ...,\n        description=\"The request for assistance needed to properly respond to the user\",\n    )\n\n    def call(self) -> str:\n        \"\"\"Prompts a human to enter a response.\"\"\"\n        print(f\"I am in need of assistance. {self.query}\")\n        response = input(\"\\t(Human): \")\n        return f\"Human response: {response}\"\n```\n\n----------------------------------------\n\nTITLE: Accessing the Original Response on Validation Error\nDESCRIPTION: Demonstrates how to access the original LLM response when a validation error occurs. This enables debugging and graceful error handling when the LLM response doesn't match the expected schema.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/response_models.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/response_models/validation/{{ provider | provider_dir }}/{{ method }}.py::28\"\n--8<-- \"build/snippets/learn/response_models/validation/{{ provider | provider_dir }}/{{ method }}.py:37:39\"\n```\n\n----------------------------------------\n\nTITLE: Implementing an LLM Chain with Mirascope's Computed Fields\nDESCRIPTION: This code demonstrates how to create a chained prompt workflow using Mirascope's computed fields. It passes the output of a movie recommendation function as input to a movie explanation function, utilizing Python decorators and dynamic configuration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-chaining.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import llm, prompt_template\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\")\n@prompt_template(\n   \"\"\"\n   Recommend a popular movie in the {genre} genre.\n   Give me just the title.\n   \"\"\"\n)\ndef recommend_movie(genre: str): ...\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\")\n@prompt_template(\n   \"\"\"\n   SYSTEM:\n   You are the world's greatest film critic.\n   Your task is to explain why the movie \"{movie_title}\" is popular in the {genre} genre.\n\n   USER:\n   Explain why \"{movie_title}\" in the {genre} genre is popular.\n   \"\"\"\n)\ndef explain_movie(genre: str) -> openai.OpenAIDynamicConfig:\n   return {\"computed_fields\": {\"movie_title\": recommend_movie(genre)}}\n\nexplanation = explain_movie(\"science fiction\")\nprint(explanation)\n```\n\n----------------------------------------\n\nTITLE: First Prompt in a Chain for Document Summarization\nDESCRIPTION: The first prompt in a chain that asks the model to summarize a contract. This demonstrates how to break down complex tasks into simpler sub-tasks for more accurate results.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nSummarize the following contract in no more than three sentences: [Insert contract text].\n```\n\n----------------------------------------\n\nTITLE: Implementing Sim to M Technique with Mirascope OpenAI Integration\nDESCRIPTION: This code demonstrates how to implement the Sim to M (Simulation Theory of Mind) technique using Mirascope with OpenAI models. It defines two functions: get_one_perspective extracts what a specific person knows about a story, and sim_to_m orchestrates the technique by first establishing facts from one perspective before answering a query based on that limited perspective.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/sim_to_m.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom mirascope.core.base.prompt import prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    The following is a sequence of events:\n    {story}\n    What events does {name} know about?\n    \"\"\"\n)\ndef get_one_perspective(story: str, name: str):\n    \"\"\"Gets one person's perspective of a story.\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    {story_from_perspective}\n    Based on the above information, answer the following question:\n    {query}\n    \"\"\"\n)\ndef sim_to_m(story: str, query: str, name: str) -> openai.OpenAIDynamicConfig:\n    \"\"\"Executes the flow of the Sim to M technique.\"\"\"\n    story_from_perspective = get_one_perspective(story=story, name=name)\n    return {\"computed_fields\": {\"story_from_perspective\": story_from_perspective}}\n\n\nstory = \"\"\"Jim put the ball in the box. While Jim wasn't looking, Avi moved the \\\nball to the basket.\"\"\"\nquery = \"Where does Jim think the ball is?\"\n\nprint(sim_to_m(story=story, query=query, name=\"Jim\"))\n```\n\n----------------------------------------\n\nTITLE: Using Format Specifiers in Python Prompt Templates\nDESCRIPTION: This code snippet showcases how to utilize standard Python format specifiers within Mirascope prompt templates for dynamic content formatting. It provides examples based on different types of templates.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n{% for method, method_title in zip(prompt_writing_methods, prompt_writing_method_titles) %}\n=== \"{{ method_title }}\"\n\n    {% if method == \"string_template\" %}\n    ```python hl_lines=\"4 9\"\n    {% elif method == \"base_message_param\" %}\n    ```python hl_lines=\"8 14\"\n    {% else %}\n    ```python hl_lines=\"6 10\"\n    {% endif %}\n    --8<-- \"examples/learn/prompts/format_specifiers/float_format/{{ method }}.py\"\n    ```\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Specifying Cache Control Type with String Templates in Anthropic\nDESCRIPTION: Example showing how to specify the cache control type when using string templates. This syntax allows for additional options for cache control, similar to how multimodal parts are handled, with 'ephemeral' being the only supported type currently.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/anthropic.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@prompt_template(\"... {:cache_control(type=ephemeral)}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Azure API Call Response Chunks for OpenAI Responses in Python\nDESCRIPTION: Defines two dataclasses for representing chunks of responses from Azure OpenAI API calls: AzureCallResponseChunk for regular responses and AzureStreamingCallResponseChunk for streaming responses. Both classes provide structured access to response data including content text, tool calls, and function calls.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/azure/call_response_chunk.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nimport json\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom openai.types.chat import ChatCompletionMessage\nfrom openai.types.chat.chat_completion import ChatCompletion, Choice\nfrom openai.types.chat.chat_completion_chunk import ChatCompletionChunk, ChoiceDelta\nfrom openai.types.chat.chat_completion_message import (\n    ChatCompletionMessage,\n    FunctionCall,\n)\nfrom openai.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall\n\nfrom mirascope.core.openai.call_response_chunk import ChoiceObjectType\n\n\n@dataclass\nclass AzureCallResponseChunk:\n    \"\"\"\n    A class that represents non-streaming Azure OpenAI API call response chunks.\n    \"\"\"\n\n    content: Optional[str] = None\n    choices: List[Choice] = None\n    raw_response: ChatCompletion = None\n    index: int = 0\n\n    @property\n    def choice(self) -> Choice:\n        \"\"\"Returns the choice at the current index.\"\"\"\n        if not self.choices:\n            return None\n        return self.choices[self.index]\n\n    @property\n    def _message(self) -> ChatCompletionMessage:\n        \"\"\"Returns the message in the choice at the current index.\"\"\"\n        if not self.choice:\n            return None\n        return self.choice.message\n\n    @property\n    def text(self) -> str:\n        \"\"\"Returns the content in the message in the choice at the current index.\"\"\"\n        if not self._message:\n            return \"\"\n        return self._message.content or \"\"\n\n    @property\n    def tool_calls(\n        self,\n    ) -> Optional[List[ChatCompletionMessageToolCall]]:\n        \"\"\"Returns the tool calls in the message in the choice at the current index.\"\"\"\n        if not self._message:\n            return None\n        return self._message.tool_calls\n\n    @property\n    def function_call(self) -> Optional[FunctionCall]:\n        \"\"\"Returns the function call in the message in the choice at the current index.\"\"\"\n        if not self._message:\n            return None\n        return self._message.function_call\n\n\n@dataclass\nclass AzureStreamingCallResponseChunk:\n    \"\"\"\n    A class that represents streaming Azure OpenAI API call response chunks.\n    \"\"\"\n\n    content: Optional[str] = None\n    choices: List[ChoiceDelta] = None\n    raw_response: ChatCompletionChunk = None\n    index: int = 0\n\n    @property\n    def choice(self) -> ChoiceDelta:\n        \"\"\"Returns the choice at the current index.\"\"\"\n        if not self.choices:\n            return None\n        return self.choices[self.index]\n\n    @property\n    def delta(self) -> Dict[str, Any]:\n        \"\"\"Returns the delta in the choice at the current index.\"\"\"\n        if not self.choice:\n            return {}\n        return self.choice.delta\n\n    @property\n    def text(self) -> str:\n        \"\"\"Returns the content in the delta in the choice at the current index.\"\"\"\n        if not self.delta:\n            return \"\"\n        return self.delta.get(\"content\", \"\")\n\n    @property\n    def function_call(self) -> Optional[ChoiceObjectType]:\n        \"\"\"Returns the function call in the delta in the choice at the current index.\"\"\"\n        if not self.delta:\n            return None\n        return self.delta.get(\"function_call\")\n\n    @property\n    def tool_calls(self) -> Optional[List[ChoiceObjectType]]:\n        \"\"\"Returns the tool calls in the delta in the choice at the current index.\"\"\"\n        if not self.delta:\n            return None\n        return self.delta.get(\"tool_calls\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Computed Fields in Prompt Templates Using Python\nDESCRIPTION: This snippet demonstrates how to use computed fields for dynamic prompt configuration at runtime, particularly for string templates. It illustrates how to handle different prompt templating methods with dynamically configured templates.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n{% for method, method_title in zip(prompt_writing_methods, prompt_writing_method_titles) %}\n=== \"{{ method_title }}\"\n\n    {% if method == \"string_template\" %}\n    ```python hl_lines=\"4 6 8 13\"\n    {% elif method == \"base_message_param\" %}\n    ```python hl_lines=\"6 8 11-12 19-20\"\n    {% else %}\n    ```python hl_lines=\"6-7 9-10 16-17\"\n    {% endif %}\n    --8<-- \"examples/learn/prompts/computed_fields/{{ method }}.py\"\n    ```\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Defining the Movie Genre Deduplication Schema with Pydantic\nDESCRIPTION: This snippet defines a Pydantic model named 'DeduplicatedGenres', which includes fields for the deduplicated movie genres and a list of duplicates. It helps in structuring the response for deduplication processes.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/removing_semantic_duplicates.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass DeduplicatedGenres(BaseModel):\n    duplicates: list[list[str]] = Field(\n        ..., description=\"A list containing lists of semantically equivalent items\"\n    )\n    genres: list[str] = Field(\n        ..., description=\"The list of genres with semantic duplicates removed\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Handling Chat History Operations in Python\nDESCRIPTION: This snippet demonstrates how to manipulate the chatbot's history to revisit previous states, allowing the chatbot to influence its conversation based on prior interactions. The example shows how to effectively control the state of conversation in real-time.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nchatbot = Chatbot()\nchatbot.run()\n# (User): Hi there! My name is Will.\n# (Assistant): It's nice to meet you, Will! I'm an AI assistant created by Anthropic. I'm here to help you with any questions or tasks you may have. Please let me know how I can assist you today.\n\n\nchatbot.run()\n# (User): Remember my name?\n# (Assistant): Of course, your name is Will. It's nice to meet you again!\n\nchatbot.history = chatbot.history[:-4]\n# (User): Remember my name?\n```\n\n----------------------------------------\n\nTITLE: Mirascope Policy Validation with AfterValidator\nDESCRIPTION: Illustrates how to implement policy validation using Mirascope's call decorator and Pydantic's AfterValidator for ensuring compliance with regulations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\n\nfrom mirascope.core import anthropic, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, BaseModel, Field\nfrom tenacity import retry, stop_after_attempt\n\n\nclass ComplianceStatus(BaseModel):\n    compliance: bool = Field(\n        description=\"Whether the entity is compliant with the regulation.\"\n    )\n\n\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\",\n    response_model=ComplianceStatus,\n)\n@prompt_template(\"Is this policy complian with CA regulations? {policy_text}\")\ndef check_policy_compliance(policy_text: str): ...\n\n\ndef validate_compliance(policy_text: str) -> str:\n    \"\"\"Validate the compliance of a policy.\"\"\"\n    policy_compliance = check_policy_compliance(policy_text)\n    assert policy_compliance.compliance, \"Policy is not compliant\"\n    return policy_text\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\",\n    response_model=Annotated[str, AfterValidator(validate_compliance)],\n)\n@prompt_template(\n    \"\"\"\n    Write a sample workplace policy about {topic}.\n    It must be in compliance with CA regulations.\n    Keep it under 100 words.\n    \"\"\"\n)\ndef write_policy(topic: str): ...\n\n\npolicy = write_policy(\"remote work hours\")\nprint(policy)  # `policy` is of type `str`\n```\n\n----------------------------------------\n\nTITLE: Using llm.call with vLLM for Local Model Integration\nDESCRIPTION: Example showing how to interact with models running on vLLM using the llm.call decorator. This enables calling local models with Mirascope's integration features.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/local_models.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.llms import vllm\nfrom mirascope import llm\n\n\n@llm.call(model=vllm.VLLM(model=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"))\ndef get_capital(country: str) -> str:\n    \"\"\"\n    Get the capital city of a country.\n    \n    Args:\n        country: The name of the country to get the capital of.\n    \n    Returns:\n        The capital city of the country.\n    \"\"\"\n\n\ndef main() -> None:\n    # Call the function with the country to get its capital\n    capital = get_capital(country=\"France\")\n    print(f\"The capital of France is {capital}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Using RunnablePassthrough for Data Transformation in Python\nDESCRIPTION: This snippet shows how RunnablePassthrough works to pass inputs unchanged or with additional keys. It demonstrates using RunnablePassthrough with RunnableParallel to preserve original input while adding additional data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-runnables.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# !pip install -qU langchain langchain-openai\n\nimport os\n\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\nrunnable = RunnableParallel(\n    passed=RunnablePassthrough(),\n    modified=lambda x: x[\"num\"] + 1,\n)\n\nrunnable.invoke({\"num\": 1})\n# Output: {'passed': {'num': 1}, 'modified': 2}\n```\n\n----------------------------------------\n\nTITLE: Implementing CohereTool Class for Cohere API Integration in Python\nDESCRIPTION: Defines the CohereTool class which extends BaseTool to work with Cohere's API. The class provides functionality to serialize tool definitions for Cohere's chat API format and generate JSON schemas for tool parameters.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/cohere/tool.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Implementation of the cohere tool class.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any, ClassVar, Dict, List, Optional, Type, cast\n\nimport pydantic\n\nfrom mirascope.core.tool import BaseTool\nfrom mirascope.utils import model_dump\n\n\nclass CohereTool(BaseTool):\n    \"\"\"Base class for Cohere tools.\"\"\"\n\n    # List fields that should not be included in the parameter schema.\n    SCHEMA_EXCLUDE_FIELDS: ClassVar[List[str]] = BaseTool.SCHEMA_EXCLUDE_FIELDS + [\n        \"_cohere_schema\",\n    ]\n\n    _cohere_schema: Optional[Dict[str, Any]] = None\n\n    @classmethod\n    def get_name_for_cohere(cls) -> str:\n        \"\"\"Get the name of the tool for Cohere.\n\n        Returns:\n            The name of the tool.\n        \"\"\"\n        return cls.__name__\n\n    @classmethod\n    def get_parameter_schema_for_cohere(\n        cls,\n    ) -> Dict[str, Any]:\n        \"\"\"Get the parameter schema for Cohere.\n\n        Returns:\n            The parameter schema for the tool.\n        \"\"\"\n        # Try to use cached schema if available.\n        if cls._cohere_schema is not None:\n            return cls._cohere_schema\n\n        schema_properties: Dict[str, Any] = {}\n        model_fields = cls.model_fields\n        for field_name, field_info in model_fields.items():\n            if field_name in cls.SCHEMA_EXCLUDE_FIELDS:\n                continue\n\n            # Get field information.\n            description = field_info.description or \"\"\n\n            # Get field type.\n            field_type = field_info.annotation\n            field_type_args = getattr(field_type, \"__args__\", []) if hasattr(field_type, \"__args__\") else []\n\n            if issubclass(field_type, bool):\n                field_schema = {\n                    \"type\": \"boolean\",\n                    \"description\": description,\n                }\n                if field_info.default is not pydantic.PydanticUndefined:\n                    field_schema[\"default\"] = field_info.default\n            elif issubclass(field_type, int):\n                field_schema = {\n                    \"type\": \"integer\",\n                    \"description\": description,\n                }\n                if field_info.default is not pydantic.PydanticUndefined:\n                    field_schema[\"default\"] = field_info.default\n            elif issubclass(field_type, float):\n                field_schema = {\n                    \"type\": \"number\",\n                    \"description\": description,\n                }\n                if field_info.default is not pydantic.PydanticUndefined:\n                    field_schema[\"default\"] = field_info.default\n            elif issubclass(field_type, str):\n                field_schema = {\n                    \"type\": \"string\",\n                    \"description\": description,\n                }\n                if field_info.default is not pydantic.PydanticUndefined:\n                    field_schema[\"default\"] = field_info.default\n            elif (\n                hasattr(field_type, \"__origin__\")\n                and field_type.__origin__ is list\n                and len(field_type_args) == 1\n            ):\n                field_schema = {\n                    \"type\": \"array\",\n                    \"description\": description,\n                }\n                if field_info.default is not pydantic.PydanticUndefined:\n                    field_schema[\"default\"] = field_info.default\n\n                # Get item field type\n                item_type = field_type_args[0]\n                if hasattr(item_type, \"__origin__\") and item_type.__origin__ is list:\n                    # Nested lists not supported\n                    continue\n\n                if issubclass(item_type, bool):\n                    field_schema[\"items\"] = {\"type\": \"boolean\"}\n                elif issubclass(item_type, int):\n                    field_schema[\"items\"] = {\"type\": \"integer\"}\n                elif issubclass(item_type, float):\n                    field_schema[\"items\"] = {\"type\": \"number\"}\n                elif issubclass(item_type, str):\n                    field_schema[\"items\"] = {\"type\": \"string\"}\n                else:\n                    # Complex types not supported\n                    continue\n            else:\n                # Complex types not supported\n                continue\n\n            # Add required field if needed.\n            required = field_info.is_required()\n            field_schema[\"required\"] = required\n\n            schema_properties[field_name] = field_schema\n\n        # Cache and return schema.\n        cls._cohere_schema = {\n            \"name\": cls.get_name_for_cohere(),\n            \"description\": cls.get_description(),\n            \"parameter_definitions\": schema_properties,\n        }\n        return cls._cohere_schema\n\n    def to_cohere_tool(self) -> Dict[str, Any]:\n        \"\"\"Convert to a Cohere tool.\n\n        Returns:\n            The Cohere tool.\n        \"\"\"\n        return {\n            \"name\": self.__class__.get_name_for_cohere(),\n            \"description\": self.__class__.get_description(),\n            \"parameter_definitions\": cast(\n                Dict[str, Any], self.__class__.get_parameter_schema_for_cohere()[\"parameter_definitions\"]\n            ),\n        }\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to a dictionary.\n\n        Returns:\n            The dictionary representation of the tool.\n        \"\"\"\n        return model_dump(self, exclude=set(self.SCHEMA_EXCLUDE_FIELDS))\n```\n\n----------------------------------------\n\nTITLE: Asynchronous function to execute the Chain of Verification\nDESCRIPTION: This function `chain_of_verification` simplifies the execution of the `cov_call` function.  It takes a query as input, calls `cov_call` with the query, and returns the response. The commented-out line allows printing the intermediate responses for debugging or analysis.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/chain_of_verification.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync def chain_of_verification(query: str):\n    response = await cov_call(query=query)\n    # Uncomment to see intermediate responses\n    # print(response.user_message_param[\"content\"])\n    return response\n```\n\n----------------------------------------\n\nTITLE: Implementing Role Prompting with Mirascope and OpenAI in Python\nDESCRIPTION: This code snippet demonstrates how to implement role prompting using Mirascope and OpenAI. It defines a function that takes a query, LLM role, and audience as inputs, and uses these to generate a contextualized response from the AI model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/role_prompting.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"\"\"\n    SYSTEM: {llm_role} {audience}\n    USER: {query}\n    \"\"\")\ndef call(\n    query: str, llm_role: str | None = None, audience: str | None = None\n) -> openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"llm_role\": f\"You are {llm_role}.\" if llm_role else \"\",\n            \"audience\": f\"You are talking to {audience}.\" if audience else \"\",\n        }\n    }\n\n\nresponse = call(\n    query=\"What's the square root of x^2 + 2x + 1?\",\n    llm_role=\"a math teacher\",\n    audience=\"your student\",\n)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Testing Tool Invocation with User Query - Python\nDESCRIPTION: This snippet tests the invocation of the 'extract_content' tool, ensuring it's called at least once per user query. It sets up a series of messages and evaluates the response from a web assistant.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_web_search_agent.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntest_extract_content_messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"queries\":[\"capital of France\",\"capital city of France\",\"France\",\"Paris\",\"France capital\"]}',\n                    \"name\": \"_web_search\",\n                },\n                \"id\": \"call_ddhSpVcNeDg9bLz4C7HEw85P\",\n            }\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"https://en.wikipedia.org/wiki/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://en.wikipedia.org/wiki/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://en.wikipedia.org/wiki/France\\n\\nhttps://www.britannica.com/place/France\\n\\nhttps://en.wikipedia.org/wiki/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://en.wikipedia.org/wiki/Paris\",\n        \"tool_call_id\": \"call_ddhSpVcNeDg9bLz4C7HEw85P\",\n        \"name\": \"_web_search\",\n    },\n]\n\n@pytest.mark.asyncio\nasync def test_extract_content():\n    \"\"\"Tests that the extract content tool gets called once.\"\"\"\n    user_query = \"What is the capital of France?\"\n    web_assistant = WebAssistant(messages=test_extract_content_messages)\n    response = await web_assistant._stream(user_query)\n    tools = []\n    async for _, tool in response:\n        if tool:\n            tools.append(tool)\n    assert len(tools) == 1 and tools[0]._name() == \"extract_content\"\n\nipytest.run()\n```\n\n----------------------------------------\n\nTITLE: Implementing Search with Source Attribution\nDESCRIPTION: Demonstrates how to perform a search query about house prices with multiple source citations in the response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-applications.md#2025-04-21_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nWhat is the average price of a house in the United States?\n```\n\n----------------------------------------\n\nTITLE: Generating Responses to User Queries\nDESCRIPTION: Takes user input, passes it to the ask_alan_turing function along with the retriever, and prints the response. The function retrieves relevant excerpts and sends them to the LLM to generate a response as if from Alan Turing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/rag-llm-example.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get the user's query and ask \"Alan Turing\"\nquery = input(\"(User): \")\nresponse = ask_alan_turing(query, retriever)\nprint(response.content)\n# > How does Turing address the objections to the idea that machines can think?\n# > This question delves into the various arguments Turing discusses in his paper, such as the theological objection, the \"heads in the sand\" objection, ...\n```\n\n----------------------------------------\n\nTITLE: Researcher Step Function Implementation\nDESCRIPTION: Implementation of the step method for the researcher agent with OpenAI integration and prompt templating.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\nclass ResearcherBaseWithStep(ResearcherBaseWithParser):\n    @openai.call(\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        Your task is to research a topic and summarize the information you find.\n        This information will be given to a writer (user) to create a blog post.\n\n        You have access to the following tools:\n        - `web_search`: Search the web for information. Limit to max {self.max_results}\n            results.\n        - `parse_webpage`: Parse the content of a webpage.\n\n        When calling the `web_search` tool, the `body` is simply the body of the search\n        result. You MUST then call the `parse_webpage` tool to get the actual content\n        of the webpage. It is up to you to determine which search results to parse.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness. The goal is to\n        provide as much information to the writer as possible without overwhelming them.\n\n        MESSAGES: {self.history}\n        USER: {prompt}\n        \"\"\"\n    )\n    def _step(self, prompt: str) -> openai.OpenAIDynamicConfig:\n        return {\"tools\": [self.web_search, self.parse_webpage]}\n```\n\n----------------------------------------\n\nTITLE: Using Decorator Approach in Mirascope V1\nDESCRIPTION: Demonstration of V1's decorator approach that enables easily switching between different LLM providers. This code shows how to run the same prompt function with both OpenAI and Anthropic models without modifying the core prompt logic.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/mirascope-v1-release.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic, openai, prompt_template\n\n\n@prompt_template()\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nopenai_recommend_book = openai.call(\"gpt-4o-mini\")(recommend_book)\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\nanthropic_recommend_book = anthropic.call(\"claude-3-5-sonnet-20240620\")(recommend_book)\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n```\n\n----------------------------------------\n\nTITLE: Defining the Run Function for Knowledge Graph Queries\nDESCRIPTION: This function is defined to answer user queries based on the generated knowledge graph. It employs a prompt template to guide the model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Answer the following question based on the knowledge graph.\n\n    Knowledge Graph:\n    {knowledge_graph}\n    \n    USER:\n    {question}\n    \"\"\"\n)\ndef run(question: str, knowledge_graph: KnowledgeGraph): ...\n```\n\n----------------------------------------\n\nTITLE: Reconstructing a Full Call Response from a Stream\nDESCRIPTION: Demonstrates how to reconstruct a complete BaseCallResponse object from a stream using the construct_call_response method, allowing access to additional properties that might be missing from the stream.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/streams/basic_usage/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Output Parsers for Custom JSON Handling\nDESCRIPTION: This snippet illustrates the use of output parsers in Mirascope to clean and extract structured JSON data from a response obtained from a call to the Anthropic API. It shows how to integrate a custom parser to ensure the JSON format is maintained.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/quickstart.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[anthropic]\"\n\nimport json\n\nfrom mirascope.core import anthropic\n\n\ndef only_json(response: anthropic.AnthropicCallResponse) -> str:\n    json_start = response.content.index(\"{\")\n    json_end = response.content.rfind(\"}\")\n    return response.content[json_start : json_end + 1]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True, output_parser=only_json)\ndef json_extraction(text: str, fields: list[str]) -> str:\n    return f\"Extract {fields} from the following text: {text}\"\n\n\njson_response = json_extraction(\n    text=\"The capital of France is Paris\",\n    fields=[\"capital\", \"country\"],\n)\nprint(json.loads(json_response))\n```\n\n----------------------------------------\n\nTITLE: Defining LiteLLMTool Class in Python\nDESCRIPTION: This snippet defines the LiteLLMTool class, which extends BaseTool to integrate LiteLLM functionality. It includes initialization, API call handling, and response processing methods.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/litellm/tool.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass LiteLLMTool(BaseTool):\n    \"\"\"A tool for making LiteLLM API calls.\"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        api_key: str | None = None,\n        temperature: float = 0.7,\n        max_tokens: int | None = None,\n        top_p: float | None = None,\n        frequency_penalty: float | None = None,\n        presence_penalty: float | None = None,\n        stop: list[str] | None = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize the LiteLLM tool.\n\n        Args:\n            model (str): The name of the model to use.\n            api_key (str | None, optional): The API key to use. Defaults to None.\n            temperature (float, optional): The temperature to use. Defaults to 0.7.\n            max_tokens (int | None, optional): The maximum number of tokens to generate. Defaults to None.\n            top_p (float | None, optional): The top_p value to use. Defaults to None.\n            frequency_penalty (float | None, optional): The frequency penalty to use. Defaults to None.\n            presence_penalty (float | None, optional): The presence penalty to use. Defaults to None.\n            stop (list[str] | None, optional): The stop sequences to use. Defaults to None.\n            **kwargs (Any): Additional keyword arguments to pass to the LiteLLM completion function.\n        \"\"\"\n        super().__init__()\n        self.model = model\n        self.api_key = api_key\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.top_p = top_p\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.stop = stop\n        self.kwargs = kwargs\n\n    def _call(\n        self,\n        prompt: str,\n        system: str | None = None,\n        stop: list[str] | None = None,\n    ) -> str:\n        \"\"\"Make a LiteLLM API call.\n\n        Args:\n            prompt (str): The prompt to use.\n            system (str | None, optional): The system message to use. Defaults to None.\n            stop (list[str] | None, optional): The stop sequences to use. Defaults to None.\n\n        Returns:\n            str: The generated text.\n        \"\"\"\n        messages = []\n        if system:\n            messages.append({\"role\": \"system\", \"content\": system})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n        response = completion(\n            model=self.model,\n            messages=messages,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            top_p=self.top_p,\n            frequency_penalty=self.frequency_penalty,\n            presence_penalty=self.presence_penalty,\n            stop=stop or self.stop,\n            api_key=self.api_key,\n            **self.kwargs,\n        )\n\n        return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Dynamic Tool Integration in Recommendations - Python\nDESCRIPTION: This snippet shows how to dynamically configure tools available to the LLM by using a tool function to format the output based on user input.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef format_book(title: str, author: str, genre: str) -> str:\n    \"\"\"Format a book recommendation.\"\"\"\n    return f\"{title} by {author} ({genre})\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book_with_tool(genre: str) -> openai.OpenAIDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"tools\": [format_book],\n    }\n\n\nresponse = recommend_book_with_tool(\"mystery\")\nif response.tool:\n    print(response.tool.call())\nelse:\n    print(response.content)\n```\n\n----------------------------------------\n\nTITLE: Tool Validation and Error Handling\nDESCRIPTION: This code snippet demonstrates how to handle validation errors when constructing tools using Mirascope.  Since `BaseTool` inherits from Pydantic's `BaseModel`, it's crucial to implement error handling for `ValidationError` to create robust applications. Even standard tools undergo field type validation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n --8<-- \"build/snippets/learn/tools/validation/{{ tool_method }}/{{ provider | provider_dir }}/{{ method }}.py\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining Query Model with Pydantic\nDESCRIPTION: Create a structured model for representing sub-queries with unique identifiers, dependencies, and tools\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/query_plan.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\\n\\n\\nclass Query(BaseModel):\\n    id: int = Field(..., description=\"ID of the query, this is auto-incremented\")\\n    question: str = Field(\\n        ...,\\n        description=\"The broken down question to be answered to answer the main question\",\\n    )\\n    dependencies: list[int] = Field(\\n        description=\"List of sub questions that need to be answered before asking this question\",\\n    )\\n    tools: list[str] = Field(\\n        description=\"List of tools that should be used to answer the question\"\\n    )\n```\n\n----------------------------------------\n\nTITLE: Running the Mirascope Chatbot in Python\nDESCRIPTION: This code snippet demonstrates how to instantiate and run the `Chatbot` class, starting the interactive loop for the chatbot. This allows users to interact with the chatbot and receive responses based on the implemented logic, including web searches if needed.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nChatbot().run()\n```\n\n----------------------------------------\n\nTITLE: Creating a prompt template with different roles\nDESCRIPTION: This code illustrates how to add context to prompts by assigning different roles (system, user) using `Messages` in Mirascope. The `book_recommendation_prompt` function now returns a list of message objects, allowing for more nuanced prompt structures.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef book_recommendation_prompt(genre: str) -> Messages.Type:\n    return [\n        Messages.System(\"You are a librarian\"),\n        Messages.User(f\"Recommend a {genre} book\"),\n    ]\n\n\nprompt = book_recommendation_prompt(\"fantasy\")\nprint(prompt)\n# > [\n#     BaseMessageParam(role='system', content='You are a librarian'),\n#     BaseMessageParam(role='user', content='Recommend a fantasy book'),\n#   ]\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Variables and Chaining in Python with Mirascope v0 and v1\nDESCRIPTION: This snippet shows how to implement dynamic variables and chaining using Mirascope v0 and v1. It demonstrates the transition from using computed fields in a class-based approach to using function composition and dynamic configuration in a decorator-based approach.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICall\nfrom pydantic import computed_field\n\n\nclass AuthorRecommender(OpenAICall):\n    prompt_template = \"\"\"\n    Recommend an author that writes the best {genre} books.\n    Give me just their name.\n    \"\"\"\n\n    genre: str\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book written by {author}\"\n    \n    genre: str\n\n    @computed_field\n    @property\n    def author(self) -> str:\n        return AuthorRecommender(genre=self.genre).call().content\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nprint(response.content)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Recommend an author that writes the best {genre} books.\n    Give me just their name.\n    \"\"\"\n)\ndef recommend_author(genre: str):\n    ...\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book written by {author}\")\ndef recommend_book(genre: str) -> openai.OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"author\": recommend_author(genre)}}\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\nprint(response.fn_args[\"author\"])\n```\n\n----------------------------------------\n\nTITLE: Information Extraction Prompt\nDESCRIPTION: This demonstrates information extraction, where the LLM identifies and extracts specific pieces of information (key points and action items) from the text. This technique is useful for pulling out relevant details from unstructured data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\n\"From the following meeting notes, extract the key points discussed and the action items: \\n\\\"In today's meeting, we discussed the upcoming product launch, marketing strategies, \\nand assigned tasks. John will handle the social media campaign, and Mary will oversee \\nthe product development.\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Plaintext Chain of Thought (CoT) Prompting Example\nDESCRIPTION: Example of using chain of thought prompting to break down a complex prompt into smaller, sequential steps. This approach helps the LLM reason more deeply about the task and can improve the quality of responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-best-practices.md#2025-04-21_snippet_2\n\nLANGUAGE: plain\nCODE:\n```\n\"Describe the various renewable energy options today.\"\n```\n\nLANGUAGE: plain\nCODE:\n```\n1. First, list the main types of renewable energy sources available today.\n2. Next, for each type, provide a brief description of how it works.\n3. Compare each of the options in terms of energy production and installation costs.\n4. Discuss the environmental impacts of using each option compared to the other.\n```\n\n----------------------------------------\n\nTITLE: Streaming Model Responses in Mirascope\nDESCRIPTION: Demonstrates how to enable streaming for large model responses by setting stream=True in the call decorator. This allows processing and displaying responses incrementally as they are generated rather than waiting for the complete response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o\", stream=True)\n@prompt_template(\"Recommend some {genre} books.\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(genre=\"science fiction\")\nfor chunk, tool in stream:\n    if tool:  # will be `None` since no tools were set\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Parallel Tool Execution\nDESCRIPTION: Example demonstrating how to execute multiple tool calls in parallel using Mirascope's tool handling capabilities.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresponse = get_book_info(\"Harry Potter\")\nif response.tools:\n    results = [tool.call() for tool in response.tools]\n    print(f\"Tool outputs: {results}\")\nelse:\n    print(f\"Response: {response.content}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Local Chatbot with Mirascope and LlamaIndex\nDESCRIPTION: Installs the necessary Python packages including Mirascope with OpenAI support and LlamaIndex components for Ollama and Hugging Face integration, enabling both document processing and LLM capabilities.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"mirascope[openai]\"\npip install llama-index  llama-index-llms-ollama llama-index-embeddings-huggingface huggingface\n```\n\n----------------------------------------\n\nTITLE: Creating Query Planner with Anthropic LLM\nDESCRIPTION: Define a function to generate a query plan using Claude 3.5 Sonnet with JSON response mode\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/query_plan.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic, prompt_template\\n\\n@anthropic.call(\\n    model=\"claude-3-5-sonnet-20240620\", response_model=list[Query], json_mode=True\\n)\\n@prompt_template(\\n    \"\"\"\\n    SYSTEM:\\n    You are an expert at creating a query plan for a question.\\n    You are given a question and you need to create a query plan for it.\\n    You need to create a list of queries that can be used to answer the question.\\n\\n    You have access to the following tool:\\n    - get_weather_by_year\\n    USER:\\n    {question}\\n    \"\"\"\\n)\\ndef create_query_plan(question: str): ...\n```\n\n----------------------------------------\n\nTITLE: Provider-agnostic prompt with OpenAI call\nDESCRIPTION: This code showcases the use of a provider-agnostic `prompt_template` in conjunction with the `@openai.call` decorator. The prompt template is defined separately and then passed to the `openai.call` decorator, making the prompt reusable with different LLM providers.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM: You are a librarian\n    USER: Recommend a {genre} book\n    \"\"\"\n)\ndef book_recommendation_prompt(genre: str): ...\n\n\n# OpenAI call and response\noopenai_recommend_book = openai.call(\n    \"gpt-4o-mini\",\n)(book_recommendation_prompt)\noopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n```\n\n----------------------------------------\n\nTITLE: Parallel Chaining in Mirascope\nDESCRIPTION: Demonstrates executing multiple LLM calls in parallel to improve efficiency. This pattern is useful when different operations don't depend on each other's outputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/chaining.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/chaining/advanced_techniques/parallel_chaining.py\"\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Compatibility with vLLM\nDESCRIPTION: Example of using the openai.call decorator with a custom client to interact with vLLM models through their OpenAI-compatible API. This allows using all of Mirascope's features with local models.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/local_models.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nfrom mirascope import openai\n\n\n# Set the base URL for the OpenAI client to point to vLLM's OpenAI-compatible endpoint\nost.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:8000/v1\"\nclient = OpenAI(api_key=\"vllm\")\n\n\n@openai.call(client=client, model=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\")\ndef get_capital(country: str) -> str:\n    \"\"\"\n    Get the capital city of a country.\n    \n    Args:\n        country: The name of the country to get the capital of.\n    \n    Returns:\n        The capital city of the country.\n    \"\"\"\n\n\ndef main() -> None:\n    # Call the function with the country to get its capital\n    capital = get_capital(country=\"France\")\n    print(f\"The capital of France is {capital}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponseChunk Class for Cohere API in Python\nDESCRIPTION: This class implements a structured container for chunked responses from Cohere API calls. It handles the parsing of response data into a specified structure with attributes for text, index, is_finished, and finish_reason.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/cohere/call_response_chunk.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, Optional\n\nfrom pydantic import Field\n\nfrom mirascope.core.call_response_chunk import CallResponseChunk as BaseCallResponseChunk\n\n\nclass CallResponseChunk(BaseCallResponseChunk):\n    \"\"\"A chunk of a response from a Cohere API call.\n\n    Attributes:\n        text: The text of the chunk.\n        index: The index of the chunk.\n        is_finished: Whether the chunk is the last chunk.\n        finish_reason: The reason the generation finished, if applicable.\n    \"\"\"\n\n    text: str = Field(default=\"\")\n    index: int = Field(default=0)\n    is_finished: bool = Field(default=False)\n    finish_reason: Optional[str] = Field(default=None)\n\n    @classmethod\n    def from_response_data(cls, data: Dict[str, Any]) -> \"CallResponseChunk\":\n        \"\"\"Create a CallResponseChunk from a Cohere API response dict.\n\n        Args:\n            data: The response dict.\n\n        Returns:\n            A CallResponseChunk.\n        \"\"\"\n        return cls(\n            text=data.get(\"text\", \"\"),\n            index=data.get(\"index\", 0),\n            is_finished=data.get(\"is_finished\", False),\n            finish_reason=data.get(\"finish_reason\", None),\n        )\n```\n\n----------------------------------------\n\nTITLE: Initializing GroqTool Class for Groq API Integration in Python\nDESCRIPTION: This class extends BaseTool to provide Groq-specific functionality. It initializes with an API key and base URL, and includes methods for chat completion and text embedding using Groq's API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/groq/tool.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import List, Optional\n\nfrom groq import Groq\nfrom groq.types import ChatCompletionMessageParam\n\nfrom mirascope.core.base import BaseTool\nfrom mirascope.core.types import Embedding\n\n\nclass GroqTool(BaseTool):\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n    ) -> None:\n        self._client = Groq(api_key=api_key, base_url=base_url)\n\n    def chat_completion(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        model: str,\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        stop: Optional[List[str]] = None,\n        stream: bool = False,\n        **kwargs\n    ):\n        return self._client.chat.completions.create(\n            messages=messages,\n            model=model,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop,\n            stream=stream,\n            **kwargs\n        )\n\n    def create_embedding(self, text: str, model: str) -> Embedding:\n        raise NotImplementedError(\"Groq does not currently support embeddings.\")\n```\n\n----------------------------------------\n\nTITLE: Querying Documents in Chroma Vector Store\nDESCRIPTION: Defines a function to answer queries by retrieving and returning the contextual content relevant to the query from a Chroma vector store. The function uses a similarity search with the retrieved documents fed into a template for the language model to provide specialized answers. Requires 'openai', 'Chroma', and a pre-configured 'vector_store'.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-pipeline.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n   \"\"\"\n   Here is some context related to a query: {context}\n   Answer this query using the given context: {query}\n   \"\"\"\n)\ndef query_documents(query: str, vector_store: Chroma) -> openai.OpenAIDynamicConfig:\n   \"\"\"Answers a query by retrieving documents related to the query.\"\"\"\n   documents = vector_store.similarity_search(query=query, k=3)\n   return {\n       \"computed_fields\": {\n           \"context\": [document.page_content for document in documents]\n       }\n   }\n\nquery = \"What are some common choices for ANN algorithms in fast MIPS?\"\nprint(query_documents(query=query, vector_store=vector_store))\n```\n\n----------------------------------------\n\nTITLE: Panel of Judges Evaluation Implementation in Python\nDESCRIPTION: Implements a multi-model evaluation approach using OpenAI and Anthropic as a panel of judges. Takes advantage of provider-agnostic prompts to call multiple providers with the same evaluation criteria and aggregates their scores.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/evals.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/evals/panel/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating Friendliness of LLM Responses in Python\nDESCRIPTION: This snippet defines an asynchronous test case for evaluating the friendliness of the responses generated by an LLM. It utilizes the custom `FriendlinessEvaluator` to assign a friendliness score based on a defined scale and triggers evaluations from multiple LLMs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_sql_agent.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\\\n\\\nfrom mirascope.core import BasePrompt, anthropic\\\nfrom openai.types.chat import ChatCompletionToolMessageParam\\\nfrom pydantic import Field\\\n\\\n@prompt_template(\\\n    \"\"\"\\\n    Evaluate the friendliness of generated text on a continuous scale from 1 to 5.\\\n    A generation is friendly (score > 1) if it contains content that is:\\\n\\\n    - Warm, welcoming, and positive in tone\\\n    - Respectful, considerate, and empathetic towards others\\\n    - Encouraging, supportive, or promotes kindness\\\n    - Inclusive and mindful of diverse perspectives\\\n\\\n    Provide a brief reasoning for your assigned score.\\\n    Make sure to highlight the specific aspects that impacted your score.\\\n\\\n    Use the following friendliness scale:\\\n\\\n    1 - No friendliness; cold, distant, or neutral tone\\\n    2 - Mild friendliness; slightly warm or polite language\\\n    3 - Moderate friendliness; noticeably kind and welcoming content\\\n    4 - High friendliness; very warm, supportive, and empathetic language\\\n    5 - Extreme friendliness; exceptionally kind, caring, and uplifting content\\\n\\\n    Input Query: {input_query}\\\n    Output Content: {output_content}\\\n    \"\"\"\\\n)\\\nclass FriendlinessEvaluator(BasePrompt):\\\n    input_query: str\\\n    output_content: str\\\n\\\n@pytest.mark.asyncio\\\nasync def test_friendliness(mock_librarian: Librarian):\\\n    input_query = \"Can you add gone with the wind\"\\\n    mock_librarian.messages = [\\\n        ChatCompletionUserMessageParam(role=\"user\", content=input_query),\\\n        ChatCompletionAssistantMessageParam(\\\n            role=\"assistant\",\\\n            tool_calls=[\\\n                {\\\n                    \"type\": \"function\",\\\n                    \"function\": {\\\n                        \"arguments\": \"{\\\"query\\\":\\\"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\\\"}\",\\\n                        \"name\": \"_execute_query\",\\\n                    },\\\n                    \"id\": \"1\",\\\n                }\\\n            ],\\\n        ),\\\n        ChatCompletionToolMessageParam(\\\n            role=\"tool\",\\\n            content=\"Query executed successfully, 1 row(s) were updated/inserted.\",\\\n            tool_call_id=\"1\",\\\n        ),\\\n    ]\\\n    response = await mock_librarian._stream(\"\")\\\n    output_content = \"\"\\\n    async for chunk, _ in response:\\\n        output_content += chunk.content\\\n    prompt = FriendlinessEvaluator(\\\n        input_query=input_query, output_content=output_content\\\n    )\\\n\\\n    class Eval(BaseModel):\\\n        score: float = Field(..., description=\"A score between [1.0, 5.0]\")\\\n        reasoning: str = Field(..., description=\"The reasoning for the score\")\\\n\\\n    async def run_evals() -> list[Eval]:\\\n        judges = [\\\n            openai.call(\\\n                \"gpt-4o-mini\",\\\n                response_model=Eval,\\\n                json_mode=True,\\\n            ),\\\n            anthropic.call(\\\n                \"claude-3-5-sonnet-20240620\",\\\n                response_model=Eval,\\\n                json_mode=True,\\\n            ),\\\n        ]\\\n        calls = [prompt.run_async(judge) for judge in judges]\\\n        return await asyncio.gather(*calls)\\\n\\\n    evals = await run_evals()\\\n    total_score = sum(eval.score for eval in evals)\\\n    average_score = total_score / len(evals)\\\n    assert average_score > 2\\\n\\\nipytest.run()\n```\n\n----------------------------------------\n\nTITLE: Implementing MistralTool Class for Mistral AI Model Integration in Python\nDESCRIPTION: This class extends the base Tool class to work with Mistral AI models. It provides methods for generating prompts, parsing responses, and handling function calls in Mistral's specific format. The class includes custom logic for prompt construction and response parsing tailored to Mistral's requirements.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/mistral/tool.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass MistralTool(Tool):\n    def __init__(\n        self,\n        description: str,\n        parameters: List[Dict[str, Any]],\n        name: Optional[str] = None,\n        function: Optional[Callable] = None,\n    ) -> None:\n        super().__init__(description, parameters, name, function)\n\n    def generate_prompt(self, message: str) -> str:\n        function_call = {\n            \"name\": self.name,\n            \"arguments\": \"\",\n            \"parameters\": self.parameters,\n        }\n        return f\"{message}\\n\\nHuman: To respond, you MUST use the function provided. Here it is:\\n{json.dumps(function_call)}\\n\\nAssistant: Certainly! I'll use the provided function to respond. Let me analyze the request and call the function accordingly.\\n\\n<function_call>\\n{json.dumps(function_call, indent=2)}\\n</function_call>\\n\\nHuman: Great! Now please provide your response based on the function call.\\n\\nAssistant: Certainly! I've made the function call as requested. Here's my response based on the function call:\\n\\n\"\n\n    def parse_response(self, response: str) -> Dict[str, Any]:\n        function_call_match = re.search(\n            r\"<function_call>\\s*({[\\s\\S]*?})\\s*</function_call>\", response\n        )\n        if not function_call_match:\n            raise ValueError(\"No function call found in the response.\")\n\n        function_call = json.loads(function_call_match.group(1))\n        return {\n            \"name\": function_call.get(\"name\"),\n            \"arguments\": function_call.get(\"arguments\", \"{}\"),\n        }\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Named Entity Recognition with Groq Model in Python\nDESCRIPTION: This snippet outlines a complex implementation of Named Entity Recognition that can handle nested entities using Groq's llama-3.1-8b-instant model. It introduces a NestedEntity class containing entity text, label, parent, and children for hierarchical categorization. This allows detection of entities at multiple levels and understanding of relationships between them. The output format provides a detailed hierarchy of entities within the input text.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/named_entity_recognition.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass NestedEntity(BaseModel):\n    entity: str = Field(description=\"The entity found in the text\")\n    label: str = Field(\n        description=\"The label of the entity (e.g., PERSON, ORGANIZATION, LOCATION)\"\n    )\n    parent: str | None = Field(\n        description=\"The parent entity if this entity is nested within another entity\",\n        default=None,\n    )\n    children: list[NestedEntity] = Field(\n        default_factory=list, description=\"Nested entities within this entity\"\n    )\n\n\n@groq.call(\n    model=\"llama-3.1-8b-instant\",\n    response_model=list[NestedEntity],\n    json_mode=True,\n    call_params={\"temperature\": 0.0},\n)\n@prompt_template(\n    \"\"\"\n    Identify all named entities in the following text, including deeply nested entities. \n    For each entity, provide its label and any nested entities within it.\n\n    Guidelines:\n    1. Identify entities of types PERSON, ORGANIZATION, LOCATION, and any other relevant types.\n    2. Capture hierarchical relationships between entities.\n    3. Include all relevant information, even if it requires deep nesting.\n    4. Be thorough and consider all possible entities and their relationships.\n\n    Example:\n    Text: \"John Smith, the CEO of Tech Innovations Inc., a subsidiary of Global Corp, announced a new product at their headquarters in Silicon Valley.\"\n    Entities:\n    - Entity: \"John Smith\", Label: \"PERSON\", Parent: None\n      Children:\n        - Entity: \"Tech Innovations Inc.\", Label: \"ORGANIZATION\", Parent: \"John Smith\"\n          Children:\n            - Entity: \"Global Corp\", Label: \"ORGANIZATION\", Parent: \"Tech Innovations Inc.\"\n    - Entity: \"Silicon Valley\", Label: \"LOCATION\", Parent: None\n\n    Now, analyze the following text: {text}\n    \"\"\"\n)\ndef nested_ner(text: str): ...\n\n\nprint(\"\\nNested NER Results:\")\nimproved_result = nested_ner(unstructured_text)\n\n\ndef print_nested_entities(entities, level=0):\n    for entity in entities:\n        indent = \"  \" * level\n        entity_info = (\n            f\"Entity: {entity.entity}, Label: {entity.label}, Parent: {entity.parent}\"\n        )\n        print(textwrap.indent(entity_info, indent))\n        if entity.children:\n            print_nested_entities(entity.children, level + 1)\n\n\nprint_nested_entities(improved_result)\n```\n\n----------------------------------------\n\nTITLE: Using Wave Module for Audio Inputs with String Templates in Python\nDESCRIPTION: Shows how to handle audio inputs with string templates and the :audio tag using the wave module for multi-modal interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport wave\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template(\"\"\"\n    USER: Transcribe this audio: {audio:audio}\n\"\"\")\ndef transcribe_audio_prompt(audio: bytes) -> Messages.Type:\n    return locals()\n\n\n# Read an audio file\nwith wave.open(\"examples/data/sample.wav\", \"rb\") as wav_file:\n    audio_bytes = wav_file.readframes(wav_file.getnframes())\n\n# Use the audio in a prompt\nprint(transcribe_audio_prompt(audio=audio_bytes))\n# Returns a user message with both text and audio content\n\n```\n\n----------------------------------------\n\nTITLE: Integrating Mirascope with Langfuse for Basic LLM Calls in Python\nDESCRIPTION: This snippet demonstrates how to use the @with_langfuse decorator to integrate Mirascope with Langfuse for basic LLM calls. It sets up environment variables for Langfuse, defines a function to recommend books, and automatically logs the interaction with Langfuse.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langfuse-integration.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom mirascope.integrations.langfuse import with_langfuse\nfrom mirascope.core import openai\n\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\nos.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"\n\n\n@with_langfuse()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")  # this will automatically get logged with langfuse\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Response Models with Pydantic in Mirascope\nDESCRIPTION: Shows how to use Mirascope's response_model with Pydantic for type-safe LLM responses and automatic output validation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-structured-output.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Recipe(BaseModel):\n    dish: str\n    chef: str\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Recipe)\ndef recommend_dish(cuisine: str) -> str:\n    return f\"Recommend a {cuisine} dish\"\n\ndish = recommend_dish(\"Italian\")\nassert isinstance(dish, Recipe)\nprint(f\"Dish: {dish.dish}\")\nprint(f\"Chef: {dish.chef}\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Provider-Specific Response Details\nDESCRIPTION: Shows how to access the underlying provider-specific response object through the chunk property of a BaseCallResponseChunk object, useful when needing access to provider-specific functionality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/streams/basic_usage/{{ provider | provider_dir }}/{{ method }}.py::10\"\n    print(f\"Original chunk: {chunk.chunk}\")\n--8<-- \"build/snippets/learn/streams/basic_usage/{{ provider | provider_dir }}/{{ method }}.py:11:11\"\n```\n\n----------------------------------------\n\nTITLE: Dynamic Tools in Text Interaction\nDESCRIPTION: Implementation showing text interaction with dynamic tool integration for OpenAI's Realtime API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/openai.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/openai/realtime/text_input_output_dynamic_tools.py\"\n```\n\n----------------------------------------\n\nTITLE: Custom Message Configuration for Provider-Specific LLM Calls in Python\nDESCRIPTION: This snippet demonstrates how to use custom messages with provider-specific LLM calls in Mirascope. It shows how to return provider-specific dynamic configurations and message types.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/calls.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/calls/provider_specific/custom_messages/{{ provider | provider_dir }}_messages.py\"\n```\n\n----------------------------------------\n\nTITLE: Vector Store Implementation\nDESCRIPTION: Creation of vector embeddings and storage using Chroma vector store and OpenAI embeddings.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-rag.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nvector_store = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n```\n\n----------------------------------------\n\nTITLE: Using the Custom Provider in Mirascope\nDESCRIPTION: This snippet demonstrates how to utilize the newly implemented custom provider within Mirascope, showing the application of the custom_provider_call decorator and a sample function definition.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/custom_provider.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n@custom_provider_call(model=\"your-custom-model\")\n@prompt_template(\"Your prompt template here\")\ndef your_function(param: str):\n    ...\n\nresult = your_function(\"example parameter\")\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Middleware Decorator in Python\nDESCRIPTION: Defines a 'with_saving' decorator using middleware_factory to wrap Mirascope calls and save metadata. It includes handlers for various types of responses and error handling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/middleware.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith_saving = middleware_factory(\n    custom_context_manager=custom_context_manager,\n    handle_call_response=handle_call_response,\n    handle_call_response_async=handle_call_response_async,\n    handle_stream=handle_stream,\n    handle_stream_async=handle_stream_async,\n    handle_response_model=handle_response_model,\n    handle_response_model_async=handle_response_model_async,\n    handle_structured_stream=handle_structured_stream,\n    handle_structured_stream_async=handle_structured_stream_async,\n    handle_error=handle_error,\n    handle_error_async=handle_error_async,\n)\n```\n\n----------------------------------------\n\nTITLE: Pydantic AfterValidator with LLM Validation\nDESCRIPTION: Integrates LLM-based validation directly into a Pydantic model using AfterValidator\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/llm_validation_with_retries.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\\n\\nfrom mirascope.core import anthropic, prompt_template\\nfrom pydantic import AfterValidator, BaseModel, ValidationError\\n\\n\\nclass TextWithoutErrors(BaseModel):\\n    text: Annotated[\\n        str,\\n        AfterValidator(\\n            lambda t: t\\n            if not (check_for_errors(t)).has_errors\\n            else (_ for _ in ()).throw(ValueError(\"Text contains errors\"))\\n        ),\\n    ]\\n\\n\\nvalid = TextWithoutErrors(text=\"This is a perfectly written sentence.\")\\n\\ntry:\\n    invalid = TextWithoutErrors(\\n        text=\"I walkd to supermarket and i picked up some fish?\"\\n    )\\nexcept ValidationError as e:\\n    print(f\"Validation error: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Handling Image Inputs with Shorthand Method in Python\nDESCRIPTION: Demonstrates how to use image inputs in prompts using the shorthand method, supporting multi-modal interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.content_param import ImageParam\n\n\n@prompt_template\ndef describe_image_prompt(image: bytes) -> Messages.Type:\n    return f\"Describe this image: {ImageParam(image)}\"\n\n\n# Read an image from a file\nwith open(\"examples/data/cat.jpg\", \"rb\") as f:\n    image_bytes = f.read()\n\n# Use the image in a prompt\nprint(describe_image_prompt(image=image_bytes))\n# Returns a user message with both text and image content\n\n```\n\n----------------------------------------\n\nTITLE: Loading Stored Embeddings for Query Engine\nDESCRIPTION: Code to load previously stored embeddings and initialize a query engine for document retrieval.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/local_chat_with_codebase.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import load_index_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nloaded_index = load_index_from_storage(storage_context)\nquery_engine = loaded_index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Defining BaseToolkit Class in Python for Mirascope Core\nDESCRIPTION: This code defines the BaseToolkit class, which is an abstract base class for toolkit implementations. It includes methods for initialization, property access, and toolkit execution. The class is designed to be subclassed by specific toolkit implementations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/toolkit.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass BaseToolkit(ABC):\n    \"\"\"Base class for toolkit implementations.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Initialize the toolkit.\"\"\"\n        self._kwargs = kwargs\n\n    def __getitem__(self, key: str) -> Any:\n        \"\"\"Get a property of the toolkit.\"\"\"\n        return self._kwargs[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        \"\"\"Set a property of the toolkit.\"\"\"\n        self._kwargs[key] = value\n\n    @abstractmethod\n    async def execute(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Execute the toolkit.\"\"\"\n        pass\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Mirascope using Pydantic\nDESCRIPTION: Demonstrates how to use Mirascope with a Pydantic BaseModel to extract structured book information from text. It uses the OpenAI decorator and specifies a response model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -> str:\n    return f\"Extract the book from this text: {text}\"\n\n\ntext = \"The Name of the Wind by Patrick Rothfuss\"\nbook = extract_book(text)\nassert isinstance(book, Book)\nprint(book)\n# > title='The Name of the Wind' author='Patrick Rothfuss'\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope for Chain-of-Thought Reasoning in Python\nDESCRIPTION: Install necessary packages including Mirascope for implementing Chain-of-Thought Reasoning. The setup involves using GROQ API key for authentication. Requires Python and pip installed on the system.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/o1_style_thinking.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \\\"mirascope[groq]\\\" \n!pip install datetime\n```\n\nLANGUAGE: python\nCODE:\n```\n# Set the appropriate API key for the provider you're using\n# Here we are using GROQ_API_KEY\n\nexport GROQ_API_KEY=\\\"Your API Key\\\"\n```\n\n----------------------------------------\n\nTITLE: Simple Text Summarization using Mirascope and OpenAI in Python\nDESCRIPTION: This function summarizes text from an HTML file by using the get_text_from_html() utility to extract the text and processes it with an OpenAI model. It requires the appropriate API key setup and text extraction with BeautifulSoup.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_summarization.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom bs4 import BeautifulSoup\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_text_from_html(file_path: str) -> str:\n    with open(file_path) as file:\n        html_text = file.read()\n    return BeautifulSoup(html_text, \"html.parser\").get_text()\n\n\ntext = get_text_from_html(\"wikipedia-python.html\")\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Summarize the following text:\n    {text}\n    \"\"\"\n)\ndef simple_summarize_text(text: str): ...\n\n\nprint(simple_summarize_text(text))\n```\n\n----------------------------------------\n\nTITLE: Making API Calls to Cohere's Language Models in Python\nDESCRIPTION: This function sends requests to Cohere's API, handles potential errors, and processes the response. It supports various parameters for customizing the API call and includes retry logic for handling rate limits.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/cohere/call.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef call(\n    model: str,\n    messages: List[Dict[str, Any]],\n    api_key: Optional[str] = None,\n    stream: bool = False,\n    **kwargs: Any,\n) -> Union[str, AsyncGenerator[str, None]]:\n    \"\"\"Make a call to Cohere's API.\n\n    Args:\n        model: The name of the model to use.\n        messages: A list of messages to send to the model.\n        api_key: The API key to use for authentication.\n        stream: Whether to stream the response.\n        **kwargs: Additional parameters to pass to the API.\n\n    Returns:\n        The response from the API.\n\n    Raises:\n        CohereBadRequestError: If the request was invalid.\n        CohereAPIError: If there was an error with the API.\n        CohereAPIStatusError: If there was an error with the API status.\n        CohereAuthenticationError: If there was an error with authentication.\n        CohereRateLimitError: If the rate limit was exceeded.\n    \"\"\"\n    cohere_client = get_client(api_key)\n\n    async def _call():\n        try:\n            chat_response = await cohere_client.chat(\n                model=model, message=messages[-1][\"content\"], **kwargs\n            )\n            return chat_response.text\n        except cohere.error.CohereAPIError as e:\n            if \"rate_limit_exceeded\" in str(e):\n                raise CohereRateLimitError(str(e))\n            elif e.status_code == 400:\n                raise CohereBadRequestError(str(e))\n            elif e.status_code == 401:\n                raise CohereAuthenticationError(str(e))\n            elif e.status_code >= 500:\n                raise CohereAPIStatusError(str(e))\n            else:\n                raise CohereAPIError(str(e))\n\n    async def _stream():\n        try:\n            async for token in cohere_client.chat_stream(\n                model=model, message=messages[-1][\"content\"], **kwargs\n            ):\n                yield token.text\n        except cohere.error.CohereAPIError as e:\n            if \"rate_limit_exceeded\" in str(e):\n                raise CohereRateLimitError(str(e))\n            elif e.status_code == 400:\n                raise CohereBadRequestError(str(e))\n            elif e.status_code == 401:\n                raise CohereAuthenticationError(str(e))\n            elif e.status_code >= 500:\n                raise CohereAPIStatusError(str(e))\n            else:\n                raise CohereAPIError(str(e))\n\n    if stream:\n        return _stream()\n    return asyncio.run(_call())\n```\n\n----------------------------------------\n\nTITLE: Versioning LLM Generations with Lilypad in Python\nDESCRIPTION: This snippet illustrates how to use the Lilypad library for tracking changes to LLM prompt generations by applying decorators for version control.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-integration.md#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport lilypad\\nfrom openai import OpenAI\\n\\nclient = OpenAI()\\n\\n\\n@lilypad.generation()\\ndef answer_question(question: str) -> str:\\n    completion = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[{\"role\": \"user\", \"content\": question}],\\n    )\\n    return str(completion.choices[0].message.content)\\n\\nif __name__ == \"__main__\":\\n    lilypad.configure()\\n    answer = answer_question(\"What's the famous question that's answered by '42'?\")\\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Converting Queries to Cypher\nDESCRIPTION: This function generates a Cypher query based on user-provided questions and a predefined structure for a Neo4j knowledge graph. It specifies valid node types and relationship patterns for use in the queries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-build-a-knowledge-graph.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\", output_parser=str)\ndef convert_query_to_cypher(query: str) -> str:\n    return f\"\"\"Given a Neo4j knowledge graph about algorithmic bias and related concepts with the following structure:\n\nNode Types:\n- Concept (e.g., \"Algorithmic Bias\")\n- Method (e.g., \"Data Collection\")\n- Type (e.g., \"Machine Learning Bias\")\n- Effect (e.g., \"Impact\")\n- Impact (e.g., \"Commercial Influences\")\n- Application (e.g., \"Legal Systems\")\n- Challenge (e.g., \"Obstacles to Research\")\n- Region (e.g., \"Europe\")\n\nValid Relationship Types:\n- Involves\n- Includes\n- Has\n- Is_Seen_In\n- Presents_Challenges\n- Requires\n\nEach node has these properties:\n- id: string identifier\n- properties: dictionary of additional metadata\n\nValid relationship patterns include:\n- (Concept)-[Involves|Includes|Has]->(Type|Effect|Impact)\n- (Method)-[Is_Seen_In]->(Application|Region)\n- (Concept|Method)-[Presents_Challenges]->(Challenge)\n- (Application)-[Requires]->(Method)\n\nConvert this question into a Cypher query that will answer it: {{query}}\n\nUse ONLY the node labels and relationship types listed above.\n\nExample conversions:\n\"What challenges are presented by algorithmic bias?\" ->\nMATCH (c:Concept {{id: 'Algorithmic Bias'}})-[r:Presents_Challenges]->(ch:Challenge)\nRETURN c.id, type(r), ch.id\n\n\"Where is data collection used?\" ->\nMATCH (m:Method {{id: 'Data Collection'}})-[r:Is_Seen_In]->(a:Application)\nRETURN m.id, type(r), a.id\n\n\"What types of bias exist in machine learning?\" ->\nMATCH (c:Concept)-[r:Includes]->(t:Type)\nWHERE c.id CONTAINS 'Bias'\nRETURN c.id, type(r), t.id\n\nPlease convert this question: {{query}}\n\nOutput only the converted Cypher query with no blocks.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Streaming with Google\nDESCRIPTION: This Python snippet demonstrates error handling during streaming with Google. It shows how to wrap the streaming loop in a try/except block to catch any exceptions that might occur during the stream's iteration, highlighting the importance of handling errors during the actual streaming process rather than just the initial function call. This example requires the Google client library.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    for chunk in completion.stream():\n        print(chunk.text, end=\"\")\nexcept Exception as e:\n    print(f\"\\nError during streaming: {e}\")\n\n```\n\n----------------------------------------\n\nTITLE: LLM Validation with Tenacity Error Reinsertion\nDESCRIPTION: Implements advanced validation technique using Tenacity to reinsert validation errors into subsequent LLM calls\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/llm_validation_with_retries.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\\n\\nfrom mirascope.core import anthropic, prompt_template\\nfrom mirascope.integrations.tenacity import collect_errors\\nfrom pydantic import AfterValidator, BaseModel, Field, ValidationError\\nfrom tenacity import retry, stop_after_attempt\\n\\n\\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\\n@anthropic.call(\\n    \"claude-3-5-sonnet-20240620\", response_model=GrammarCheck, json_mode=True\\n)\\n@prompt_template(\\n    \"\"\"\\n    {previous_errors}\\n\\n    Correct the grammar in the following text.\\n    If no corrections are needed, return the original text.\\n    Provide an explanation of any corrections made.\\n\\n    Text: {text}\\n    \"\"\"\\n)\\ndef correct_grammar(\\n    text: str, *, errors: list[ValidationError] | None = None\\n) -> anthropic.AnthropicDynamicConfig:\\n    previous_errors = f\"Previous Errors: {errors}\" if errors else \"No previous errors.\"\\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\\n\\n\\ntry:\\n    text = \"I has went to the store yesterday and buyed some milk.\"\\n    result = correct_grammar(text)\\n    print(f\"Corrected text: {result.text}\")\\n    print(f\"Explanation: {result.explanation}\")\\nexcept ValidationError:\\n    print(\"Failed to correct grammar after 3 attempts\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Few-Shot Prompts for Sentiment Analysis in Plaintext\nDESCRIPTION: This snippet demonstrates how to use few-shot prompting to teach a language model to perform sentiment analysis on customer feedback. It provides examples of positive and negative sentiments to guide the model's understanding.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nInstruction:\nYou are tasked with performing sentiment analysis on customer feedback regarding the customer service department of a corporate food catering company. Your goal is to classify whether the sentiment expressed in the feedback is \"positive\" or \"negative.\" Below are several examples of feedback along with their corresponding sentiment classifications to guide your understanding of the task. After reviewing these examples, classify the sentiment of the new feedback provided. Respond only with \"positive\" or \"negative.\"\n\nExamples:\n\n1. \"The customer service representative was incredibly helpful! They quickly resolved my issue and made sure I was satisfied with the outcome.\"\nSentiment: positive\n\n2. \"I've been trying to reach the customer support team for days, and no one has responded. This is unacceptable.\"\nSentiment: negative\n\n3. \"The team was very attentive to our needs and made sure we had everything set up for our event. It was a pleasure working with them.\"\nSentiment: positive\n\n4. \"The service was delayed, and the representative I spoke to was rude. I won't be using this catering service again.\"\nSentiment: negative\n\nNow, analyze the sentiment of the following feedback:\n\n\"The support team was polite but took way too long to get back to me. It made planning much more stressful than it should have been.\"\n\nClassify the sentiment of the feedback:\n# Output: negative\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Data with Constraints\nDESCRIPTION: Generate synthetic TV data with specific constraints like price correlation with size and type-based pricing\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/generating_synthetic_data.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass TV(BaseModel):\n    size: int = Field(description=\"The size of the TV\")\n    price: float = Field(description=\"The price of the TV in dollars (include cents)\")\n    tv_type: Literal[\"OLED\", \"QLED\"]\n\n@openai.call(model=\"gpt-4o-mini\", response_model=list[TV])\n@prompt_template(\"\"\"\nGenerate {num_datapoints} random but realistic datapoints of TVs.\nOutput the datapoints as a list of instances of TV.\n\nMake sure to abide by the following constraints:\nQLEDS should be roughly (not exactly) 2x the price of an OLED of the same size\nfor both OLEDs and QLEDS, price should increase roughly proportionately to size\n\"\"\")\ndef generate_tv_data(num_datapoints: int): ...\n\nfor tv in generate_tv_data(10):\n    print(tv)\n```\n\n----------------------------------------\n\nTITLE: Testing LLM Query Context Relevance with PyTest\nDESCRIPTION: Asynchronous test function that verifies the web assistant generates context-relevant search queries. It initializes a WebAssistant with search history related to LLM development tools and tests if generated queries maintain relevance to both historical context and current user question.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_web_search_agent.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.asyncio\nasync def test_conversation():\n    \"\"\"Tests that in a conversation setting, the llm generated query is context-relevant.\"\"\"\n    web_assistant = WebAssistant(\n        search_history=[\n            \"best LLM development tools\",\n            \"top libraries for LLM development\",\n            \"LLM libraries for software engineers\",\n            \"LLM dev tools for machine learning\",\n            \"most popular libraries for LLM development\",\n        ],\n        messages=test_conversation_messages,\n    )\n    response = await web_assistant._stream(\"What is mirascope library?\")\n    async for _, tool in response:\n        queries = tool.args.get(\"queries\", \"\") if tool else \"\"\n        is_context_relevant = False\n        for query in queries:\n            context_relevant = await check_context_relevance(\n                web_assistant.search_history, \"What is mirascope library?\", query\n            )\n            is_context_relevant = context_relevant.is_context_relevant\n            if is_context_relevant:\n                break\n        assert is_context_relevant\n\n\nipytest.run()\n```\n\n----------------------------------------\n\nTITLE: Using model_dump() for Prompt Versioning with Mirascope in Python\nDESCRIPTION: This snippet illustrates how to use the model_dump() function with Mirascope to serialize and version prompts and LLM calls. It demonstrates creating a function decorated with openai.call and printing the serialized output.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-versioning.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(genre=\"fantasy\")\nprint(response.model_dump())\n```\n\n----------------------------------------\n\nTITLE: Implementing Rephrase and Respond with Mirascope and OpenAI\nDESCRIPTION: A Python implementation of the Rephrase and Respond technique using Mirascope's OpenAI integration. The function conditionally appends the RaR instruction to prompts and leverages the GPT-4o-mini model to generate responses that first rephrase and expand the query before answering it.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/rephrase_and_respond.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\nrar_augment = \"\\nRephrase and expand the question, and respond.\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{query} {rar_augment}\")\ndef call(query: str, rar_prompt: bool = False) -> openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"rar_augment\": rar_augment if rar_prompt else \"\",\n        }\n    }\n\n\nprompt = \"\"\"A coin is heads up. aluino flips the coin. arthor flips the coin.\nIs the coin still heads up? Flip means reverse.\"\"\"\n\nprint(call(query=prompt, rar_prompt=True))\n```\n\n----------------------------------------\n\nTITLE: Defining a DuckDuckGo web search tool with Mirascope\nDESCRIPTION: This code defines a `WebSearch` tool using Mirascope's `OpenAITool` class. It uses DuckDuckGo to search the web for a given query, extracts the paragraphs from the resulting webpages using BeautifulSoup, and returns the concatenated paragraphs. It handles potential errors during web searching and parsing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\nfrom duckduckgo_search import DDGS\nfrom pydantic import BaseModel, Field\n\n\nclass WebSearch(openai.OpenAITool):\n    \"\"\"Search the web for the given text and parse the paragraphs of the results.\"\"\"\n\n    query: str = Field(..., description=\"The text to search for.\")\n\n    def call(self) -> str:\n        \"\"\"Search the web for the given text and parse the paragraphs of the results.\n\n        Returns:\n            Parsed paragraphs of each of the webpages, separated by newlines.\n        \"\"\"\n        try:\n            # Search the web for the given text\n            results = DDGS(proxy=None).text(self.query, max_results=2)\n\n            # Parse the paragraphs of each resulting webpage\n            parsed_results = []\n            for result in results:\n                link = result[\"href\"]\n                try:\n                    response = requests.get(link)\n                    soup = BeautifulSoup(response.content, \"html.parser\")\n                    parsed_results.append(\n                        \"\\n\".join([p.text for p in soup.find_all(\"p\")])\n                    )\n                except Exception as e:\n                    parsed_results.append(\n                        f\"{type(e)}: Failed to parse content from URL {link}\"\n                    )\n\n            return \"\\n\\n\".join(parsed_results)\n\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n```\n\n----------------------------------------\n\nTITLE: Defining Mirascope Prompt Template with Dynamic Content\nDESCRIPTION: This code snippet demonstrates how to define a prompt template in Mirascope using the `prompt_template` decorator. It creates a structured, context-specific prompt, and calling the prompt template function returns the parsed and formatted messages using the arguments of the function as the template variables. This allows you to implement even more complex prompts just by extending the function.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-tools.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM: You are the world's greatest librarian.\n    USER: Recommend a {genre} book.\n    \"\"\"\n)\ndef recommend_book_prompt(genre: str): ...\n\n\nmessages = recommend_book_prompt(\"fantasy\")\nprint(messages)\n# > [BaseMessageParam(role='system', content=\"You are the world's greatest librarian.\"), BaseMessageParam(role='user', content='Recommend a fantasy book.')]\n```\n\n----------------------------------------\n\nTITLE: Implementing Brand Voice Validation with Pydantic\nDESCRIPTION: Shows how to implement custom validation using Pydantic and LLMs to ensure generated content maintains consistent brand voice.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-versioning.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum\nfrom typing import Annotated\n\nfrom mirascope.core import openai\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\nclass Label(Enum):\n    ON_BRAND = \"on brand\"\n    OFF_BRAND = \"off brand\"\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Label)\ndef check_brand_compliance(text: str) -> str:\n    return f\"Does this content adhere to brand guidelines? {text}\"\n\n\ndef validate_brand_compliance(content: str) -> str:\n    \"\"\"Check if the content follows the brand guidelines.\"\"\"\n    label = check_brand_compliance(content)\n    assert label == Label.ON_BRAND, \"Content did not adhere to brand guidelines.\"\n    return content\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(validate_brand_compliance)],\n)\ndef generate_content() -> str:\n    return \"Generate content for our new marketing campaign\"\n\n\ntry:\n    content = generate_content()\n    print(content)\nexcept ValidationError as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Defining Default Values for LiteLLM API Call Parameters in Python\nDESCRIPTION: This dictionary sets default values for various parameters used in LiteLLM API calls. It includes settings for temperature, top_p, presence and frequency penalties, and maximum retries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/litellm/call_params.md#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nDEFAULT_CALL_PARAMS: dict[str, Any] = {\n    \"temperature\": 0.7,\n    \"top_p\": 1.0,\n    \"n\": 1,\n    \"stream\": False,\n    \"presence_penalty\": 0.0,\n    \"frequency_penalty\": 0.0,\n    \"max_retries\": 2,\n}\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Spelling and Grammar Validation\nDESCRIPTION: Implements a Pydantic model and LLM call to check text for spelling and grammatical errors\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/llm_validation_with_retries.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic, prompt_template\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SpellingAndGrammarCheck(BaseModel):\\n    has_errors: bool = Field(\\n        description=\"Whether the text has typos or grammatical errors\"\\n    )\\n\\n\\n@anthropic.call(\\n    model=\"claude-3-5-sonnet-20240620\",\\n    response_model=SpellingAndGrammarCheck,\\n    json_mode=True,\\n)\\n@prompt_template(\\n    \"\"\"\\n    Does the following text have any typos or grammatical errors? {text}\\n    \"\"\"\\n)\\ndef check_for_errors(text: str): ...\\n\\n\\ntext = \"Yestday I had a gret time!\"\\nresponse = check_for_errors(text)\\nassert response.has_errors\n```\n\n----------------------------------------\n\nTITLE: Basic Dynamic Configuration for Book Recommendation - Python\nDESCRIPTION: This snippet illustrates a basic dynamic configuration where a book recommendation is generated based on genre and creativity level inputs. It demonstrates defining a callable using Mirascope's openai integration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseDynamicConfig, Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str, creativity: float) -> BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"temperature\": creativity},\n    }\n\n\n# Low creativity recommendation\nresponse = recommend_book(\"mystery\", 0.2)\nprint(\"Low creativity:\", response.content)\n\n# High creativity recommendation\nresponse = recommend_book(\"mystery\", 0.8)\nprint(\"High creativity:\", response.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Enhanced Self-Ask with Dynamic Example Selection in Python using Mirascope\nDESCRIPTION: A complete implementation of a dynamic Self-Ask system that selects relevant examples based on similarity to the query. It uses TF-IDF vectorization and cosine similarity from scikit-learn to find the most applicable few-shot examples before sending the prompt to an OpenAI model. The code defines a TypedDict for examples, a selection function, and a decorated function that handles the dynamic configuration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/self_ask.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\n\nimport numpy as np\nfrom mirascope.core import openai, prompt_template\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom typing_extensions import TypedDict\n\n\nclass FewShotExample(TypedDict):\n    question: str\n    answer: str\n\n\ndef select_relevant_examples(\n    query: str, examples: list[FewShotExample], n: int = 3\n) -> list[FewShotExample]:\n    \"\"\"Select the most relevant examples based on cosine similarity.\"\"\"\n    vectorizer = TfidfVectorizer().fit([ex[\"question\"] for ex in examples] + [query])\n    example_vectors = vectorizer.transform([ex[\"question\"] for ex in examples])\n    query_vector = vectorizer.transform([query])\n\n    similarities = cosine_similarity(query_vector, example_vectors)[0]\n    most_similar_indices = np.argsort(similarities)[-n:][::-1]\n\n    return [examples[i] for i in most_similar_indices]\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Examples:\n    {examples:lists}\n\n    Query: {query}\n    \"\"\"\n)\ndef dynamic_self_ask(\n    query: str, examples: list[FewShotExample], n: int = 3\n) -> openai.OpenAIDynamicConfig:\n    relevant_examples = select_relevant_examples(query, examples, n)\n    return {\n        \"computed_fields\": {\n            \"examples\": [\n                [example[\"question\"], example[\"answer\"]]\n                for example in relevant_examples\n            ]\n        }\n    }\n\n\nfew_shot_examples = [\n    FewShotExample(\n        question=\"When does monsoon season end in the state the area code 575 is located?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which state is the area code 575 located in?\n            Intermediate answer: The area code 575 is located in New Mexico.\n            Follow up: When does monsoon season end in New Mexico?\n            Intermediate answer: Monsoon season in New Mexico typically ends in mid-September.\n            So the final answer is: mid-September.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What is the current official currency in the country where Ineabelle Diaz is a citizen?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which country is Ineabelle Diaz a citizen of?\n            Intermediate answer: Ineabelle Diaz is from Peurto Rico, which is in the United States of America.\n            Follow up: What is the current official currency in the United States of America?\n            Intermediate answer: The current official currency in the United States is the United States dollar.\n            So the final answer is: United States dollar.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"Where was the person who founded the American Institute of Public Opinion in 1935 born?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Who founded the American Institute of Public Opinion in 1935?\n            Intermediate answer: George Gallup.\n            Follow up: Where was George Gallup born?\n            Intermediate answer: George Gallup was born in Jefferson, Iowa.\n            So the final answer is: Jefferson.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What language is used by the director of Tiffany Memorandum?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Who directed the movie called Tiffany Memorandum?\n            Intermediate answer: Sergio Grieco.\n            Follow up: What language is used by Sergio Grieco?\n            Intermediate answer: Sergio Grieco speaks Italian.\n            So the final answer is: Italian.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What is the sports team the person played for who scored the first touchdown in Superbowl 1?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which player scored the first touchdown in Superbowl 1?\n            Intermediate answer: Max McGee.\n            Follow up: Which sports team did Max McGee play for?\n            Intermediate answer: Max McGee played for the Green Bay Packers.\n            So the final answer is: Green Bay Packers.\n            \"\"\"\n        ),\n    ),\n]\n\n\nquery = \"What was the primary language spoken by the inventor of the phonograph?\"\nresponse = dynamic_self_ask(query=query, examples=few_shot_examples, n=2)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Creating PII Redaction Prompt\nDESCRIPTION: This snippet creates another Mirascope prompt template using `openai.call` and `prompt_template` decorators to redact PII from an article. It uses a local Ollama instance. The function `scrub_pii` is defined but not fully implemented (indicated by the ellipsis).\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/pii_scrubbing.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\n    model=\"llama3.1\",\n    client=OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an expert at redacting personally identifiable information (PII).\n    Replace the PII in the following article with context words.\n\n    If PII exists in the article, replace it with context words. For example, if the\n    phone number is 123-456-7890, replace it with [PHONE_NUMBER].\n\n    USER: {article}\n    \"\"\"\n)\ndef scrub_pii(article: str): ...\n\n\ndef run():\n    does_pii_exist = check_if_pii_exists(PII_ARTICLE)\n    print(does_pii_exist)\n    # Output:\n    # True\n    if does_pii_exist:\n        return scrub_pii(PII_ARTICLE)\n    else:\n        return \"No PII found in the article.\"\n\n\nprint(run())\n```\n\n----------------------------------------\n\nTITLE: JSON Mode for OpenAI Integration\nDESCRIPTION: This snippet demonstrates how to configure an OpenAI call in JSON mode to ensure the output is structured as JSON, which is useful for parsing structured data easily.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/quickstart.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef city_info(city: str) -> str:\n    return f\"Provide information about {city} in JSON format\"\n\n\nresponse = city_info(\"Tokyo\")\nprint(response.content)  # This will be a JSON-formatted string\n```\n\n----------------------------------------\n\nTITLE: Creating prompt for OpenAI and Anthropic in Python\nDESCRIPTION: Defines functions using Mirascope for both OpenAI and Anthropic LLMs to extract receipt information from an image. OpenAI models can be used directly with image URLs, while Anthropic requires base64 encoded images. Dependencies include Mirascope, OpenAI, and Anthropic API access.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/extraction_using_vision.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic, openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o\", response_model=list[Item])\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Extract the receipt information from the image.\n    \n    USER:\n    {url:image}\n    \"\"\"\n)\ndef extract_receipt_info_openai(url: str): ...\n\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\", response_model=list[Item], json_mode=True\n)\n@prompt_template(\n    \"\"\"\n    Extract the receipt information from the image.\n    \n    {url:image}\n    \"\"\"\n)\ndef extract_receipt_info_anthropic(url: str): ...\n```\n\n----------------------------------------\n\nTITLE: Evaluating Context Relevance of LLM-generated Queries - Python\nDESCRIPTION: This snippet defines a model to check the context relevance of user queries generated by an LLM. It assesses whether the generated queries maintain expected relevancy based on provided search history and user input.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_web_search_agent.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic\nfrom pydantic import Field\n\n\nclass ContextRelevant(BaseModel):\n    is_context_relevant: bool = Field(\n        description=\"Whether the LLM-generated query is context-relevant\"\n    )\n    explanation: str = Field(description=\"The reasoning for the context relevance\")\n\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\", response_model=ContextRelevant, json_mode=True\n)\n@prompt_template(\n    \"\"\"\n    Given:\n\n    Search history: {search_history}\n    User query: {user_query}\n    LLM-generated query: {llm_query}\n\n    Evaluate if the LLM-generated query is context-relevant using the following criteria:\n\n    Bridging Relevance:\n\n    Does {llm_query} effectively bridge the gap between {search_history} and {user_query}?\n    Does it incorporate elements from both {search_history} and {user_query} meaningfully?\n\n\n    Intent Preservation:\n\n    Does {llm_query} maintain the apparent intent of {user_query}?\n    Does it also consider the broader context established by {search_history}?\n\n\n    Topical Consistency:\n\n    Is {llm_query} consistent with the overall topic or theme of {search_history}?\n    If there's a shift in topic from {search_history} to {user_query}, does {llm_query} handle this transition logically?\n\n\n    Specificity and Relevance:\n\n    Is {llm_query} specific enough to be useful, considering both {search_history} and {user_query}?\n    Does it avoid being overly broad or tangential?\n\n\n    Contextual Enhancement:\n\n    Does {llm_query} add value by incorporating relevant context from {search_history}?\n    Does it expand on {user_query} in a way that's likely to yield more relevant results?\n\n\n    Handling of Non-Sequiturs:\n\n    If {user_query} is completely unrelated to {search_history}, does {llm_query} appropriately pivot to the new topic?\n    Does it still attempt to maintain any relevant context from {search_history}, if possible?\n\n\n    Semantic Coherence:\n\n    Do the terms and concepts in {llm_query} relate logically to both {search_history} and {user_query}?\n    Is there a clear semantic path from {search_history} through {user_query} to {llm_query}?\n\n\n\n    Evaluation:\n\n    Assess {llm_query} against each criterion, noting how well it performs.\n    Consider the balance between maintaining context from {search_history} and addressing the specific intent of {user_query}.\n    Evaluate how {llm_query} handles any topic shift between {search_history} and {user_query}.\n\n    Provide a final assessment of whether {llm_query} is context-relevant, with a brief explanation of your reasoning.\n    \"\"\"\n)\nasync def check_context_relevance(\n    search_history: list[str], user_query: str, llm_query: str\n): ...\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Call Implementation with OpenAI and Anthropic\nDESCRIPTION: Demonstrates the identical structure of call decorators across different LLM providers using OpenAI and Anthropic as examples. Shows how to create simple recipe recommendation functions with type hints.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-frameworks.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, anthropic\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_recipe(cuisine: str) -> str:\n    return f\"Recommend a {cuisine} recipe\"\n\nresponse = recommend_recipe(\"italian\")\nprint(response.content)\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_recipe(genre: str) -> str:\n    return f\"Recommend a {cuisine} recipe\"\n\nresponse = recommend_recipe(\"french\")\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Creating Prompts with Multiple Message Roles using Messages Methods\nDESCRIPTION: Demonstrates how to create prompts with system and user messages using the Messages.Role methods.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template\ndef recommend_book_prompt(genre: str) -> Messages.Type:\n    return [\n        Messages.System(\"You are a helpful book recommendation agent.\"),\n        Messages.User(f\"Recommend a {genre} book\"),\n    ]\n\n\nprint(recommend_book_prompt(genre=\"fantasy\"))\n# Equivalent to:\n# [\n#   BaseMessageParam(role=\"system\", content=\"You are a helpful book recommendation agent.\"),\n#   BaseMessageParam(role=\"user\", content=\"Recommend a fantasy book\")\n# ]\n\n```\n\n----------------------------------------\n\nTITLE: Defining AzureTool Class for Azure Function Integration\nDESCRIPTION: Implements the AzureTool class which inherits from Tool and enables integration with Azure Functions. This class provides the foundation for creating tools that execute as Azure Functions with proper input/output handling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/azure/tool.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AzureTool(Tool):\n    \"\"\"A tool that executes via an Azure function.\n\n    This tool enables integration with Azure Functions to execute the tool's\n    functionality. It handles the execution context and marshalling between\n    the Azure Function environment and the tool's implementation.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running the LocalizedRecommender\nDESCRIPTION: This snippet demonstrates how to create an instance of the LocalizedRecommender and run it, allowing for user interaction with the chatbot.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/localized_agent.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrecommender = LocalizedRecommender(history=[])\nawait recommender.run()\n```\n\n----------------------------------------\n\nTITLE: Using PydanticOutputParser for Structured Book Library Response\nDESCRIPTION: Demonstrates parsing LLM responses into structured book library data using Pydantic models and PydanticOutputParser with type validation\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-structured-output.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass Book(BaseModel):\n    title: str = Field(..., description=\"The title of the book\")\n    pages: int = Field(..., description=\"The number of pages in the book.\")\n\nclass Library(BaseModel):\n    books: List[Book]\n\n# Set up a parser\nparser = PydanticOutputParser(pydantic_object=Library)\n\n# Prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\"),\n    (\"human\", \"{query}\"),\n]).partial(format_instructions=parser.get_format_instructions())\n\n# Query\nquery = \"Please provide details about the books 'The Great Gatsby' with 208 pages and 'To Kill a Mockingbird' with 384 pages.\"\n\n# Invoke chain with LLM\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(temperature=0)\nchain = prompt | llm | parser\nchain.invoke({\"query\": query})\n```\n\n----------------------------------------\n\nTITLE: Loading Persisted Embeddings for Query Processing\nDESCRIPTION: Loads previously generated and persisted index and vector embeddings, creating a query engine that will allow the chatbot to search and retrieve relevant information from the onboarding documents without reprocessing them.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import load_index_from_storage\n\n# Recreate a storage context pointing to the persisted index\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nloaded_index = load_index_from_storage(storage_context)\n\n# This query_engine object allows us to run semantic queries over the docs\nquery_engine = loaded_index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Example Usage of DuckDuckGoSearch Tool in Python\nDESCRIPTION: This snippet outlines how to utilize the DuckDuckGoSearch tool within Mirascope, showing an example of structured code usage with support for different methods based on user-selected configurations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n{% for pre_made_tool_method, pre_made_tool_method_title in [[\"basic_usage\", \"Basic Usage\"], [\"custom_config\", \"Custom Config\"]] %}\n=== \"{{ pre_made_tool_method_title }}\"\n\n    {% for method, method_title in zip(prompt_writing_methods, prompt_writing_method_titles) %}\n    === \"{{ method_title }}\"\n\n        {% for provider in supported_llm_providers %}\n        === \"{{ provider }}\"\n\n            {% if pre_made_tool_method == \"basic_usage\" %}\n            ```python hl_lines=\"2 5\"\n            {% else %}\n            ```python hl_lines=\"2 4-5 8\"\n            {% endif %}\n            --8<-- \"build/snippets/learn/tools/pre_made_tools/{{ pre_made_tool_method }}/{{ provider | provider_dir }}/{{ method }}.py\"\n            ```\n        {% endfor %}\n\n    {% endfor %}\n\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Using Direct String Templating in Mirascope Prompts\nDESCRIPTION: Shows how to create prompts using direct f-string templating instead of template strings. This approach allows for more flexible string formatting directly within the function.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@prompt_template()\ndef greetings_prompt(name: str) -> str:\n    return f\"Hello! My name is {name}. How are you today?\"\n\n\nprint(greetings_prompt(\"William Bakst\"))\n# > [BaseMessageParam(role='user', content='Hello! My name is William Bakst. How are you today?')]\n```\n\n----------------------------------------\n\nTITLE: Integrating LangChain RAG with Mirascope\nDESCRIPTION: Demonstrates how to combine LangChain's vector store capabilities with Mirascope's LLM calls for implementing RAG functionality. Uses Chroma as the vector store and OpenAI embeddings for document storage and retrieval.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-sucks.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\nfrom mirascope.core import openai, prompt_template\n\n# Initialize your vector store\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\nvector_store = Chroma(\n   collection_name=\"books\",\n   embedding_function=embeddings,\n   persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n\n# Add documents to the store\ndocuments = [\n   Document(page_content=\"The Name of the Wind by Patrick Rothfuss\"),\n   Document(page_content=\"Harry Potter and the Sorcerer's Stone by J.K. Rowling\"),\n   Document(page_content=\"Mistborn: The Final Empire by Brandon Sanderson\"),\n   # Add more documents as needed\n]\nvector_store.add_documents(documents=documents)\n\n@openai.call(\"gpt-4o\")\n@prompt_template(\n    \"\"\"\n    Recommend a {genre} book from the catalogue.\n\n    Here are some {genre} books available in the catalogue:\n    {context}\n    \"\"\"\n)\ndef recommend_book(genre: str, vector_store: Chroma) -> openai.OpenAIDynamicConfig:\n   return {\n       \"computed_fields\": {\"context\": vector_store.similarity_search(query=genre, k=3)}\n   }\n\n# Example usage\nrecommendation = recommend_book(genre=\"fantasy\", vector_store=vector_store)\nprint(recommendation.content)\n```\n\n----------------------------------------\n\nTITLE: Utility Functions for Custom Provider\nDESCRIPTION: Several utility functions are provided for setting up the custom provider, handling JSON output, and managing streaming responses in both synchronous and asynchronous contexts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/custom_provider.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Callable, Awaitable\n\ndef setup_call(\n    *,\n    model: str,\n    client: Any,\n    fn: Callable[..., CustomProviderDynamicConfig | Awaitable[CustomProviderDynamicConfig]],\n    fn_args: dict[str, Any],\n    dynamic_config: CustomProviderDynamicConfig,\n    tools: list[type[BaseTool] | Callable] | None,\n    json_mode: bool,\n    call_params: CustomProviderCallParams,\n    extract: bool,\n) -> tuple[\n    Callable[..., Any] | Callable[..., Awaitable[Any]],\n    str,\n    list[Any],\n    list[type[CustomProviderTool]] | None,\n    dict[str, Any],\n]:\n    # Implement setup logic\n    ...\n\ndef get_json_output(\n    response: CustomProviderCallResponse | CustomProviderCallResponseChunk,\n    json_mode: bool\n) -> str:\n    # Implement JSON output extraction\n    ...\n\ndef handle_stream(\n    stream: Any,\n    tool_types: list[type[CustomProviderTool]] | None,\n) -> Generator[tuple[CustomProviderCallResponseChunk, CustomProviderTool | None], None, None]:\n    # Implement stream handling\n    ...\n\nasync def handle_stream_async(\n    stream: Any,\n    tool_types: list[type[CustomProviderTool]] | None,\n) -> AsyncGenerator[tuple[CustomProviderCallResponseChunk, CustomProviderTool | None], None]:\n    # Implement asynchronous stream handling\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Similarity with TF-IDF\nDESCRIPTION: Shows how to calculate cosine similarity between text prompts using TF-IDF vectorization with scikit-learn.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Sample prompts\nprompt1 = \"A cat sat on a mat.\"\nprompt2 = \"A dog rested on the carpet.\"\n\n# Convert the prompts into TF-IDF vectors\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform([prompt1, prompt2])\n\n# Compute cosine similarity\ncosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\nprint(f\"Cosine Similarity: {cosine_sim[0][0]:.4f}\")\n> Cosine Similarity: 0.1274\n```\n\n----------------------------------------\n\nTITLE: Initializing Structured Output with LangChain and OpenAI\nDESCRIPTION: Demonstrates how to use `.with_structured_output` method to generate a structured trivia response using a Pydantic model with OpenAI's LLM\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-structured-output.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Instantiate the LLM model\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\nclass Trivia(BaseModel):\n    question: str = Field(description=\"The trivia question\")\n    answer: str = Field(description=\"The correct answer to the trivia question\")\n\n# Define the prompt template\nprompt = ChatPromptTemplate.from_template(\n    \"Give me a trivia question about {topic}, respond in JSON with `question` and `answer` keys\"\n)\n\n# Create a structured LLM using the `with_structured_output` method\nstructured_llm = model.with_structured_output(Trivia, method=\"json_mode\")\n\n# Chain the prompt and structured LLM using the pipe operator\ntrivia_chain = prompt | structured_llm\n\n# Invoke the chain\nresult = trivia_chain.invoke({\"topic\": \"space\"})\n```\n\n----------------------------------------\n\nTITLE: Using BasePrompt in Mirascope v1 for Python\nDESCRIPTION: Shows how to use the BasePrompt class in v1, which operates similarly to v0 calls but uses the 'run' method instead of 'call' or 'stream'. This approach allows for flexible usage with different provider's call decorators.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BasePrompt, openai, prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book\")\nclass BookRecommendationPrompt(BasePrompt):\n    genre: str\n\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\nresponse = prompt.run(openai.call(\"gpt-4o-mini\"))\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Printing Model Dump for Chained Function Calls in Python using Mirascope\nDESCRIPTION: This code snippet shows how to print the model dump for chained function calls in Mirascope. It demonstrates that intermediate steps may not be evident in the output, highlighting a limitation of using functions for prompt chaining.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-chaining.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(response.model_dump())\n# {\n#     \"metadata\": {},\n#     \"response\": {\n#         \"id\": \"chatcmpl-9tipx51yKYmtaleyNltoZEd4h2iou\",\n#         \"choices\": [\n#             {\n#                 \"finish_reason\": \"stop\",\n#                 \"index\": 0,\n#                 \"logprobs\": None,\n#                 \"message\": {\n#                     \"content\": \"Ah, the theory that many people associate with my work is the theory of relativity, which is actually comprised of two parts: special relativity and general relativity. Let me explain each component.\\n\\n**Special Relativity (1905)**: This theory fundamentally changed our understanding of space and time. At its core, special relativity asserts two key principles:\\n\\n1. **The Principle of Relativity**: The laws of physics are the same in all inertial frames of reference, meaning that whether you are at rest or moving at a constant speed, the laws governing physical phenomena do not change.\\n\\n2. **The Constancy of the Speed of Light**: Light travels at a constant speed (approximately 299,792 kilometers per second or about 186,282 miles per second) in a vacuum, regardless of the motion of the source or the observer. \\n\\nFrom these postulates, we can derive several intriguing concepts, including time dilation (time moves slower for an object in motion relative to a stationary observer) and length contraction (objects in motion are measured to be shorter in the direction of motion by stationary observers). \\n\\nPerhaps the most famous equation that stems from special relativity is \\\\( E = mc^2 \\\\), which posits that energy (E) is equivalent to mass (m) multiplied by the square of the speed of light (c). This equation implies that a small amount of mass can be converted into a large amount of energy, a principle that has profound implications for both theoretical physics and practical applications, such as nuclear energy.\\n\\n**General Relativity (1915)**: This theory extends the principles of special relativity to include gravity. In general relativity, I proposed that gravity is not a force in the traditional sense but rather a curvature of spacetime caused by the presence of mass. According to this view, massive objects (like planets and stars) warp the fabric of spacetime around them, and this curvature affects the motion of other objects, which we perceive as the force of gravity.\\n\\nThis theory also leads to predictions such as the bending of light around massive objects (gravitational lensing), the time dilation effects in strong gravitational fields (where time runs slower closer to a massive body), and the existence of phenomena such as black holes and gravitational waves.\\n\\nIn essence, the theory of relativity has transformed our understanding of the universe, challenging classical notions of time, space, and gravity, and continues to influence modern physics and cosmology profoundly.\",\n#                     \"role\": \"assistant\",\n#                     \"function_call\": None,\n#                     \"tool_calls\": None,\n#                     \"refusal\": None,\n#                 },\n#             }\n#         ],\n#         \"created\": 1723067421,\n#         \"model\": \"gpt-4o-mini-2024-07-18\",\n#         \"object\": \"chat.completion\",\n#         \"service_tier\": None,\n#         \"system_fingerprint\": \"fp_48196bc67a\",\n#         \"usage\": {\"completion_tokens\": 497, \"prompt_tokens\": 45, \"total_tokens\": 542},\n#     },\n#     \"tool_types\": [],\n#     \"prompt_template\": \"\\n    SYSTEM:\\n    Imagine that you are {scientist}.\\n    Your task is to explain a theory that you, {scientist}, are famous for.\\n\\n    USER:\\n    Explain the theory related to {topic}.\\n    \",\n#     \"fn_args\": {\"scientist\": \"Albert Einstein\", \"topic\": \"theory of relativity\"},\n#     \"dynamic_config\": None,\n#     \"messages\": [\n#         {\n#             \"role\": \"system\",\n#             \"content\": \"Imagine that you are Albert Einstein.\\nYour task is to explain a theory that you, Albert Einstein, are famous for.\",\n#         },\n#         {\n#             \"role\": \"user\",\n#             \"content\": \"Explain the theory related to theory of relativity.\",\n#         },\n#     ],\n#     \"call_params\": {},\n#     \"call_kwargs\": {\n#         \"model\": \"gpt-4o-mini\",\n#         \"messages\": [\n#             {\n#                 \"role\": \"system\",\n#                 \"content\": \"Imagine that you are Albert Einstein.\\nYour task is to explain a theory that you, Albert Einstein, are famous for.\",\n#             },\n#             {\n#                 \"role\": \"user\",\n#                 \"content\": \"Explain the theory related to theory of relativity.\",\n#             },\n#         ],\n#     },\n#     \"user_message_param\": {\n#         \"content\": \"Explain the theory related to theory of relativity.\",\n#         \"role\": \"user\",\n#     },\n#     \"start_time\": 1723067421089.7852,\n#     \"end_time\": 1723067429478.955,\n#     \"message_param\": {\n#         \"content\": \"Ah, the theory that many people associate with my work is the theory of relativity, which is actually comprised of two parts: special relativity and general relativity. Let me explain each component.\\n\\n**Special Relativity (1905)**: This theory fundamentally changed our understanding of space and time. At its core, special relativity asserts two key principles:\\n\\n1. **The Principle of Relativity**: The laws of physics are the same in all inertial frames of reference, meaning that whether you are at rest or moving at a constant speed, the laws governing physical phenomena do not change.\\n\\n2. **The Constancy of the Speed of Light**: Light travels at a constant speed (approximately 299,792 kilometers per second or about 186,282 miles per second) in a vacuum, regardless of the motion of the source or the observer. \\n\\nFrom these postulates, we can derive several intriguing concepts, including time dilation (time moves slower for an object in motion relative to a stationary observer) and length contraction (objects in motion are measured to be shorter in the direction of motion by stationary observers). \\n\\nPerhaps the most famous equation that stems from special relativity is \\\\( E = mc^2 \\\\), which posits that energy (E) is equivalent to mass (m) multiplied by the square of the speed of light (c). This equation implies that a small amount of mass can be converted into a large amount of energy, a principle that has profound implications for both theoretical physics and practical applications, such as nuclear energy.\\n\\n**General Relativity (1915)**: This theory extends the principles of special relativity to include gravity. In general relativity, I proposed that gravity is not a force in the traditional sense but rather a curvature of spacetime caused by the presence of mass. According to this view, massive objects (like planets and stars) warp the fabric of spacetime around them, and this curvature affects the motion of other objects, which we perceive as the force of gravity.\\n\\nThis theory also leads to predictions such as the bending of light around massive objects (gravitational lensing), the time dilation effects in strong gravitational fields (where time runs slower closer to a massive body), and the existence of phenomena such as black holes and gravitational waves.\\n\\nIn essence, the theory of relativity has transformed our understanding of the universe, challenging classical notions of time, space, and gravity, and continues to influence modern physics and cosmology profoundly.\",\n#         \"role\": \"assistant\",\n#         \"tool_calls\": None,\n#         \"refusal\": None,\n#     },\n#     \"tools\": None,\n#     \"tool\": None,\n# }\n```\n\n----------------------------------------\n\nTITLE: Testing PII Detection Prompt\nDESCRIPTION: This snippet defines a sample article containing PII and then calls the `check_if_pii_exists` function to test the PII detection prompt. The result is then printed to the console.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/pii_scrubbing.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nPII_ARTICLE = \"\"\"\nJohn Doe, born on 12/07/1985, resides at 123 Ruecker Harbor in Goodwinshire, WI. \nHis Social Security number is 325-21-4386 and he can be reached at (123) 456-7890. \n\"\"\"\n\ndoes_pii_exist = check_if_pii_exists(PII_ARTICLE)\nprint(does_pii_exist)\n```\n\n----------------------------------------\n\nTITLE: Second Prompt in a Chain for Legal Analysis\nDESCRIPTION: The second prompt in a chain that uses the output from the first prompt to perform a more specific analysis of the contract, focusing on the alignment of the termination clause with recent case law.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nBased on the summary provided, does the termination clause align with recent case law regarding service contracts in California?\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Processing with OpenAI\nDESCRIPTION: This code snippet demonstrates how to perform asynchronous calls to the OpenAI service using Mirascope, allowing multiple LLM requests to be handled concurrently for improved performance.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/quickstart.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nimport asyncio\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Capital)\nasync def get_capital_async(country: str) -> str:\n    return f\"What is the capital of {country}?\"\n\n\nasync def main():\n    countries = [\"France\", \"Japan\", \"Brazil\"]\n    tasks = [get_capital_async(country) for country in countries]\n    capitals = await asyncio.gather(*tasks)\n    for capital in capitals:\n        print(f\"The capital of {capital.country} is {capital.city}\")\n\n\n# await main() when running in a Jupyter notebook\nawait main()\n\n# asyncio.run(main()) when running in a Python script\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Prompting Template\nDESCRIPTION: Demonstrates few-shot prompting by providing example book recommendations to guide the model's responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n@prompt_template(\n    \"\"\"\n    I'm looking for book recommendations on various topics. Here are some examples:\n\n    1. For a topic on 'space exploration', you might recommend:\n        - 'The Right Stuff' by Tom Wolfe\n        - 'Cosmos' by Carl Sagan\n\n    2. For a topic on 'artificial intelligence', you might recommend:\n        - 'Life 3.0' by Max Tegmark\n        - 'Superintelligence' by Nick Bostrom\n\n    3. For a topic on 'historical fiction', you might recommend:\n        - 'The Pillars of the Earth' by Ken Follett\n        - 'Wolf Hall' by Hilary Mantel\n\n    Can you recommend some books on {topic}?\n    \"\"\"\n)\ndef few_shot_book_recommendation_prompt(topic: str): ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Operations with RunnableParallel in Python\nDESCRIPTION: This snippet demonstrates how to use RunnableParallel to run multiple runnables concurrently with the same input. It shows both synchronous and asynchronous examples with function composition and various instantiation methods.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-runnables.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom langchain_core.runnables import RunnableLambda\n\n\ndef add_one(x: int) -> int:\n   return x + 1\n\n\ndef mul_two(x: int) -> int:\n   return x * 2\n\n\ndef mul_three(x: int) -> int:\n   return x * 3\n\n\nrunnable_1 = RunnableLambda(add_one)\nrunnable_2 = RunnableLambda(mul_two)\nrunnable_3 = RunnableLambda(mul_three)\n\n\nsequence = runnable_1 | {  # this dict is coerced to a RunnableParallel\n   \"mul_two\": runnable_2,\n   \"mul_three\": runnable_3,\n}\n# Or equivalently:\n# sequence = runnable_1 | RunnableParallel(\n#     {\"mul_two\": runnable_2, \"mul_three\": runnable_3}\n# )\n# Also equivalently:\n# sequence = runnable_1 | RunnableParallel(\n#     mul_two=runnable_2,\n#     mul_three=runnable_3,\n# )\n\n\nprint(sequence.invoke(1))\n# > {'mul_two': 4, 'mul_three': 6}\nprint(sequence.batch([1, 2, 3]))\n# > [{'mul_two': 4, 'mul_three': 6}, {'mul_two': 6, 'mul_three': 9}, {'mul_two': 8, 'mul_three': 12}]\n\n\nasync def async_invoke(sequence, x):\n   return await sequence.ainvoke(x)\n\n\nasync def async_batch(sequence, x):\n   return await sequence.abatch(x)\n\n\nprint(asyncio.run(async_invoke(sequence, 1)))\n# > {'mul_two': 4, 'mul_three': 6}\nprint(asyncio.run(async_batch(sequence, [1, 2, 3])))\n# > [{'mul_two': 4, 'mul_three': 6}, {'mul_two': 6, 'mul_three': 9}, {'mul_two': 8, 'mul_three': 12}]\n```\n\n----------------------------------------\n\nTITLE: Creating Prompts with Multiple Message Roles using BaseMessageParam\nDESCRIPTION: Demonstrates how to directly create BaseMessageParam instances to build prompts with system and user messages.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.message_param import BaseMessageParam\n\n\n@prompt_template\ndef recommend_book_prompt(genre: str) -> Messages.Type:\n    return [\n        BaseMessageParam(role=\"system\", content=\"You are a helpful book recommendation agent.\"),\n        BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\"),\n    ]\n\n\nprint(recommend_book_prompt(genre=\"fantasy\"))\n# Equivalent to:\n# [\n#   BaseMessageParam(role=\"system\", content=\"You are a helpful book recommendation agent.\"),\n#   BaseMessageParam(role=\"user\", content=\"Recommend a fantasy book\")\n# ]\n\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope Dependencies with pip\nDESCRIPTION: This snippet demonstrates how to install the Mirascope library along with its OpenAI dependency using pip. It's essential to run this command before utilizing Mirascope functionalities.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/removing_semantic_duplicates.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Dynamic Configuration with Mistral Chatbot\nDESCRIPTION: Illustrates dynamic configuration capabilities using a Mistral chatbot implementation. Shows how to handle message history, adjust temperature based on creativity level, and manage message filtering in real-time.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-frameworks.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import mistral, Messages\nfrom typing import Literal\n\n\ndef get_relevant_messages(\n    query: str, history: list[mistral.MistralMessageParam]\n) -> list[mistral.MistralMessageParam]:\n    # Filter the messages based on the query,\n    # returning only the most relevant ones.\n    ...\n\n\n@mistral.call(\"gpt-4o-mini\")\ndef chatbot(\n    query: str,\n    creativity: Literal[\"low\", \"high\"],\n    history: list[mistral.MistralMessageParam],\n) -> mistral.MistralDynamicConfig:\n    messages = [\n        Messages.System(\"You are a helpful chatbot\"),\n        *get_relevant_messages(query, history),\n        Messages.User(query),\n    ]\n    return {\n        \"messages\": messages,\n        \"call_params\": {\"temperature\": 0.25 if creativity == \"low\" else 0.75},\n    }\n\n\nresponse = chatbot(\"What is the best way to learn programming?\", \"low\", [...])\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Extracting Movie Details from LLM Output\nDESCRIPTION: In this code, a Pydantic model is utilized to validate the output of an LLM response when extracting movie details. The function passes a string to the model and returns structured data in a validated format, making it suitable for storage or further processing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom mirascope.core import openai\n\n\nclass MovieDetails(BaseModel):\n    title: str\n    director: str\n\n\n@openai.call(model=\"gpt-4o\", response_model=MovieDetails)\ndef extract_movie_details(movie: str) -> str:\n    return f\"Extract details from this movie: {movie}\"\n\n\nmovie = \"Inception directed by Christopher Nolan.\"\nmovie_details = extract_movie_details(movie)\nassert isinstance(movie_details, MovieDetails)\nprint(movie_details)\n# > title='Inception' director='Christopher Nolan'\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Embedder with Mirascope RAG\nDESCRIPTION: This snippet demonstrates how to use Mirascope's OpenAIEmbedder class to transform text chunks into vectors. The embedder is initialized and used to embed a message.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.beta.rap.openai import OpenAIEmbedder\n\n\nembedder = OpenAIEmbedder()\nresponse = embedder.embed([\"your_message_to_embed\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Tool Class for Providers\nDESCRIPTION: This snippet details the implementation of a CustomProviderTool class inheriting from BaseTool to define custom tool functionalities and schemas.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/custom_provider.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core.base import BaseTool\n\nclass CustomProviderTool(BaseTool):\n    # Implement custom tool functionality\n    @classmethod\n    def tool_schema(cls) -> ProviderToolSchemaType:\n        # Return the tool schema\n\n    @classmethod\n    def from_tool_call(cls, tool_call: Any) -> \"CustomProviderTool\":\n        # Construct a tool instance from a tool call\n```\n\n----------------------------------------\n\nTITLE: Creating a Librarian Chatbot with State Management in Python\nDESCRIPTION: This code snippet demonstrates how to build a librarian chatbot system using Mirascope and Pydantic. It manages conversation history and available books efficiently, allowing interaction through user queries. Key dependencies include 'mirascope' for the base framework and 'pydantic' for data validation and modeling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-frameworks.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -> str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -> BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -> str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -> None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n```\n\n----------------------------------------\n\nTITLE: Creating Prompt Templates with String Templates in Python\nDESCRIPTION: Demonstrates how to use Mirascope's string template approach where a template string is parsed and formatted like a normal Python formatted string.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template(\"\"\"\n    USER: Recommend a {genre} book\n\"\"\")\ndef recommend_book_prompt(genre: str) -> Messages.Type:\n    return locals()\n\n\nprint(recommend_book_prompt(genre=\"fantasy\"))\n\n```\n\n----------------------------------------\n\nTITLE: Routing Calls to Agents in Python\nDESCRIPTION: The snippet defines a placeholder function 'route_to_agent()' to simulate routing calls to a designated agent type based on the extracted call summary.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/support_ticket_routing.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\ndef route_to_agent(\n    agent_type: Literal[\"billing\", \"sale\", \"support\"], summary: str\n) -> None:\n    \"\"\"Routes the call to an appropriate agent with a summary of the issue.\"\"\"\n    print(f\"Routed to: {agent_type}\\nSummary:\\n{summary}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Message Prompts with Messages Class in Mirascope\nDESCRIPTION: Demonstrates how to create multi-message prompts using Mirascope's Messages class, which provides methods for creating system, user, and assistant messages explicitly.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef greetings_prompt(name: str) -> Messages.Type:\n    return [\n        Messages.System(\"You can only speak in haikus.\"),\n        Messages.User(f\"Hello! My name is {name}. How are you today?\"),\n    ]\n\n\nprint(greetings_prompt(\"William Bakst\"))\n# > [\n#     BaseMessageParam(role='system', content='You can only speak in haikus.'),\n#     BaseMessageParam(role='user', content='Hello! My name is William Bakst. How are you today?'),\n#   ]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Context Manager for Middleware in Python\nDESCRIPTION: Defines a custom context manager function that sets up a database session for use in middleware handlers. It yields the session object to be used in subsequent operations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/middleware.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef custom_context_manager(fn: Callable):\n    engine = create_engine(\"sqlite:///database.db\")\n    with Session(engine) as session:\n        yield session\n```\n\n----------------------------------------\n\nTITLE: Initializing LangChain Runnables with FAISS and OpenAI\nDESCRIPTION: This snippet illustrates how to use LangChain to create a retrieval chain that answers questions based on context, utilizing the FAISS vector store and OpenAI's Chat model. Required dependencies include langchain_community and langchain_core libraries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n\\nvectorstore = FAISS.from_texts(\\n    [\"The theory of relativity\"], embedding=OpenAIEmbeddings()\\n)\\nretriever = vectorstore.as_retriever()\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\n\\nretrieval_chain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n    | prompt\\n    | model\\n    | StrOutputParser()\\n)\\n\\nprint(retrieval_chain.invoke(\"What was Einstein best known for?\"))\\n# > 'The theory of relativity.'\n```\n\n----------------------------------------\n\nTITLE: Web Search Implementation for Researcher\nDESCRIPTION: ResearcherBase class implementing web search functionality using DuckDuckGo search API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\n\nfrom duckduckgo_search import DDGS\n\n\nclass ResearcherBase(OpenAIAgent):\n    max_results: int = 10\n\n    def web_search(self, text: str) -> str:\n        \"\"\"Search the web for the given text.\n\n        Args:\n            text: The text to search for.\n\n        Returns:\n            The search results for the given text formatted as newline separated\n            dictionaries with keys 'title', 'href', and 'body'.\n        \"\"\"\n        try:\n            results = DDGS(proxy=None).text(text, max_results=self.max_results)\n            return \"\\n\\n\".join(\n                [\n                    inspect.cleandoc(\n                        \"\"\"\n                        title: {title}\n                        href: {href}\n                        body: {body}\n                        \"\"\"\n                    ).format(**result)\n                    for result in results\n                ]\n            )\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n```\n\n----------------------------------------\n\nTITLE: Mirascope Data Validation with Pydantic\nDESCRIPTION: Shows how to use Pydantic with Mirascope for automatic validation of LLM output data structures and error handling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Movie(BaseModel):\n    title: str\n    rating: float\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Movie)\ndef recommend_movie() -> str:\n    return \"Recommend a movie\"\n\n\ntry:\n    movie = recommend_movie()\n    assert isinstance(movie, Movie)\n    print(movie)\nexcept ValidationError as e:\n    print(e)\n    # > 1 validation error for Movie\n    #  rating\n    #    Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='standard', input_type=str]\n    #      For further information visit https://errors.pydantic.dev/2.6/v/float_parsing\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple Decorators in Python\nDESCRIPTION: This function takes multiple decorator functions as arguments and returns a single decorator that applies all of them in order. It uses functools.wraps to preserve the metadata of the original function.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/merge_decorators.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom functools import wraps\nfrom typing import Callable, Any\n\n\ndef merge_decorators(*decorators: Callable) -> Callable:\n    \"\"\"Merge multiple decorators into a single decorator.\n\n    Args:\n        *decorators: Variable number of decorator functions to be merged.\n\n    Returns:\n        A single decorator function that applies all the input decorators.\n    \"\"\"\n\n    def merged_decorator(func: Callable) -> Callable:\n        for decorator in reversed(decorators):\n            func = decorator(func)\n\n        @wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return merged_decorator\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Message Prompts with Role Keywords in Mirascope\nDESCRIPTION: Demonstrates how to create prompts with multiple message roles (SYSTEM, USER, ASSISTANT) using Mirascope's prompt_template decorator and special keywords in the template string.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM: You can only speak in haikus.\n    USER: Hello! My name is {name}. How are you today?\n    \"\"\"\n)\ndef greetings_prompt(name: str): ...\n\n\nprint(greetings_prompt(\"William Bakst\"))\n# > [\n#     BaseMessageParam(role='system', content='You can only speak in haikus.'),\n#     BaseMessageParam(role='user', content='Hello! My name is William Bakst. How are you today?'),\n#   ]\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Parameters for a Provider\nDESCRIPTION: This snippet defines a class named CustomProviderCallParams that extends BaseCallParams to specify custom API call parameters for a provider, including optional max_tokens and temperature settings.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/custom_provider.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import NotRequired\n\nfrom mirascope.core.base import BaseCallParams\n\n\nclass CustomProviderCallParams(BaseCallParams):\n    # Add parameters specific to your provider, such as:\n    max_tokens: NotRequired[int | None]\n    temperature: NotRequired[float | None]\n```\n\n----------------------------------------\n\nTITLE: Creating a Complex RunnableSequence Chain in Python\nDESCRIPTION: This example shows how to create a more complex chain using RunnableSequence. It combines multiple functions (greeting, datetime appending, uppercase conversion, and exclamation addition) into a single chain, demonstrating how to sequence multiple operations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-runnables.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom langchain_core.runnables import RunnableLambda, RunnableSequence\n\n# Define the transformations as simple functions\ndef greet(name):\n   return f\"Hello, {name}!\"\n\ndef append_datetime(text):\n   current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n   return f\"{text} The current date and time is {current_datetime}.\"\n\ndef to_uppercase(text):\n   return text.upper()\n\ndef add_exclamation(text):\n   return f\"{text}!\"\n\n# Wrap the functions in RunnableWrapper\ngreet_runnable = RunnableLambda(lambda x: greet(x))\ndatetime_runnable = RunnableLambda(lambda x: append_datetime(x))\nuppercase_runnable = RunnableLambda(lambda x: to_uppercase(x))\nexclamation_runnable = RunnableLambda(lambda x: add_exclamation(x))\n\n# Create a RunnableSequence with the wrapped runnables\nchain = RunnableSequence(\n   first=greet_runnable,\n   middle=[datetime_runnable, uppercase_runnable],\n   last=exclamation_runnable,\n)\n\n# Apply the chain to some input data\ninput_data = \"Alice\"\nresult = chain.invoke(input_data)\nprint(\n   result\n)  # Output example: \"HELLO, ALICE! THE CURRENT DATE AND TIME IS 2024-06-19 14:30:00!\"\n```\n\n----------------------------------------\n\nTITLE: Processing LiteLLM Response Chunks in Python\nDESCRIPTION: This function processes response chunks from LiteLLM API calls. It extracts the content from the chunk and handles different response formats, including streaming and non-streaming responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/litellm/call_response_chunk.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, Optional\n\nfrom litellm.utils import ModelResponse\n\n\ndef call_response_chunk(chunk: ModelResponse | Dict[str, Any]) -> Optional[str]:\n    \"\"\"Process a response chunk from a LiteLLM API call.\n\n    Args:\n        chunk: A chunk of the response from a LiteLLM API call.\n\n    Returns:\n        The content of the chunk, or None if there is no content.\n    \"\"\"\n    if isinstance(chunk, ModelResponse):\n        choices = chunk.choices\n        if not choices:\n            return None\n        delta = choices[0].delta\n        if not delta:\n            return None\n        return delta.content\n    elif isinstance(chunk, dict):\n        choices = chunk.get(\"choices\")\n        if not choices:\n            return None\n        delta = choices[0].get(\"delta\")\n        if not delta:\n            return None\n        return delta.get(\"content\")\n    return None\n```\n\n----------------------------------------\n\nTITLE: Calculating Recall and Precision in Python\nDESCRIPTION: This function calculates recall and precision based on the relationship between the output and expected words. It takes a string output and a string expected as parameters, returning the recall and precision as a tuple. Dependencies include standard Python libraries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-evaluation.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_recall_precision(output: str, expected: str) -> tuple[float, float]:\n    output_words = set(output.lower().split())\n    expected_words = set(expected.lower().split())\n\n    common_words = output_words.intersection(expected_words)\n\n    recall = len(common_words) / len(expected_words) if expected_words else 0\n    precision = len(common_words) / len(output_words) if output_words else 0\n\n    return recall, precision\n\n```\n\n----------------------------------------\n\nTITLE: Running Context Relevance Check with LLM Queries - Python\nDESCRIPTION: This snippet demonstrates how to run the check for context relevance using predefined search history, user query, and LLM-generated query. It tests the integration of the evaluation logic.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_web_search_agent.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def run(search_history: list[str], user_query: str, llm_query: str):\n    return await check_context_relevance(search_history, user_query, llm_query)\n\n\nsearch_history = [\"Best beaches in Thailand\", \"Thai cuisine must-try dishes\"]\nuser_query = \"How to book flights?\"\nllm_query = \"How to book flights to Thailand for a beach and culinary vacation\"\n\nawait run(search_history, user_query, llm_query)\n\nllm_query = \"General steps for booking flights online\"\nawait run(search_history, user_query, llm_query)\n```\n\n----------------------------------------\n\nTITLE: Writing Multi-Line Prompts with Shorthand Method in Python\nDESCRIPTION: Shows how to create multi-line prompts using the shorthand method and inspect.cleandoc to remove unnecessary tokens.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template\ndef summarize_prompt(text: str) -> Messages.Type:\n    return inspect.cleandoc(\n        f\"\"\"\n        Summarize the following text:\n\n        Text:\n        {text}\n\n        Summary:\n        \"\"\"\n    )\n\n\nprint(summarize_prompt(text=\"This is a sample text that needs to be summarized.\"))\n\n```\n\n----------------------------------------\n\nTITLE: Pydantic Data Validation\nDESCRIPTION: Shows how to use Pydantic for data validation with LLM responses, including custom validation rules and error handling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, ValidationError\nfrom mirascope.core import openai\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\ntry:\n    book = recommend_book(\"fantasy\")\n    assert isinstance(book, Book)\n    print(book)\nexcept ValidationError as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Handling API Responses for a Custom Provider\nDESCRIPTION: This snippet implements a CustomProviderCallResponse class that inherits from BaseCallResponse, with properties to extract content and finish reasons from the response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/custom_provider.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core.base import BaseCallResponse, BaseMessageParam\n\n\nclass CustomProviderCallResponse(BaseCallResponse[...]):  # provide types for generics\n    # Implement abstract properties and methods\n    @property\n    def content(self) -> str:\n        # Return the main content of the response\n\n    @property\n    def finish_reasons(self) -> list[str] | None:\n        # Return the finish reasons of the response\n\n    # Implement other abstract properties and methods\n```\n\n----------------------------------------\n\nTITLE: Lilypad with Mirascope Integration\nDESCRIPTION: Demonstrates how to use Lilypad's llm_fn decorator with Mirascope's openai.call decorator for automatic versioning and tracing of LLM functions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# lily/recommend_book.py\nimport lilypad\nfrom mirascope.core import openai\n\n\n@lilypad.llm_fn()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nif __name__ == \"__main__\":\n    output = recommend_book(\"fantasy\")\n    print(output.content)\n```\n\n----------------------------------------\n\nTITLE: JSON Mode Error Handling and Validation\nDESCRIPTION: Shows how to implement error handling and validation for JSON Mode outputs, including catching invalid JSON responses. Demonstrates proper exception handling for JSON parsing errors.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/json_mode.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/json_mode/error_handling/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Automatic Retry Functionality with Tenacity in Mirascope\nDESCRIPTION: Demonstrates how to use Tenacity with Mirascope to implement automatic retry logic. This example retries the LLM call up to 3 times if the result contains the word 'math', showing how to handle transient errors or unwanted responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom tenacity import retry, retry_if_result, stop_after_attempt\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    retry=retry_if_result(lambda result: \"math\" in result.content.lower()),\n)\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(genre=\"science fiction\")\nprint(response.content)  # Content should not contain \"math\"\n```\n\n----------------------------------------\n\nTITLE: Validating Python Code Syntax\nDESCRIPTION: This code defines a function `check_syntax` that checks if a given string is valid Python code by attempting to compile it. It returns True if the code is syntactically correct, and False otherwise, printing the syntax error if one exists. It depends on the built-in `compile` function for syntax checking.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ast\nimport importlib.util\n\n\ndef check_syntax(code_string: str) -> bool:\n    try:\n        compile(code_string, \"<string>\", \"exec\")\n        return True\n    except SyntaxError as e:\n        print(f\"Syntax error: {e}\")\n        return False\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for LLM Judge Panel Implementation\nDESCRIPTION: This snippet shows the necessary imports from Mirascope and Pydantic for implementing a panel of LLM judges. It prepares the environment for working with multiple language models and enforcing structured responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-as-judge.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\n\nfrom mirascope.core import anthropic, openai, prompt_template\nfrom pydantic import BaseModel, Field\n```\n\n----------------------------------------\n\nTITLE: Implementing Conditional Prompts with Mirascope in Python\nDESCRIPTION: This snippet illustrates the use of conditional prompts to dynamically adjust the response based on the sentiment of a review. The `respond_to_review` function includes conditional logic to determine the type of response based on the sentiment classification, highlighting a flexible approach to prompt engineering.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-chaining.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Conditional Prompt\nfrom enum import Enum\n\nfrom mirascope.core import openai, prompt_template\n\n\nclass Sentiment(str, Enum):\n    POSITIVE = \"positive\"\n    NEGATIVE = \"negative\"\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Sentiment)\n@prompt_template(\"Is the following review positive or negative? {review}\")\ndef classify_sentiment(review: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your task is to respond to a review.\n    The review has been identified as {sentiment}.\n\n    USER: Write a {conditional_review_prompt} for the following review: {review}\n    \"\"\"\n)\ndef respond_to_review(review: str) -> openai.OpenAIDynamicConfig:\n    sentiment = classify_sentiment(review)\n    conditional_review_prompt = (\n        \"thank you response for the review\"\n        if sentiment == Sentiment.POSITIVE\n        else \"apologetic response addressing the review\"\n    )\n    return {\n        \"computed_fields\": {\n            \"sentiment\": sentiment,\n            \"conditional_review_prompt\": conditional_review_prompt,\n        }\n    }\n\n\npositive_response = respond_to_review(\n    review=\"This tool is awesome because it's so flexible!\"\n)\nprint(positive_response)\n# > Thank you for your wonderful review! We’re thrilled to hear ...\nprint(positive_response.user_message_param)\n# > {'content': \"Write a thank you response for the review for the following review: This tool is awesome because it's so flexible!\", 'role': 'user'}\n\nnegative_response = respond_to_review(\n    review=\"This product is terrible and too expensive!\"\n)\nprint(negative_response)\n# > Thank you for sharing your feedback with us. We're truly sorry to hear ...\nprint(negative_response.user_message_param)\n# > {'content': 'Write a apologetic response addressing the review for the following review: This product is terrible and too expensive!', 'role': 'user'}\n```\n\n----------------------------------------\n\nTITLE: Migrating Basic OpenAI Call from Class to Decorator in Python\nDESCRIPTION: Demonstrates the transition from a class-based approach in v0 to a decorator-based approach in v1 for a simple book recommendation function. The new approach is more concise and directly represents the stateless nature of the API call.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nprint(response.content)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book.\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Creating LLM Judges and Running Evaluations with Mirascope\nDESCRIPTION: This snippet demonstrates how to create a list of language model judges from different providers and use them to evaluate a sample text. Mirascope abstracts away provider-specific API details while ensuring structured responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-as-judge.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\njudges = [\n    openai.call(\"gpt-4o-mini\", response_model=Eval),\n    anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval),\n]\n\nevaluations: list[Eval] = [\n    judge(evaluate_helpfulness)(\n        \"To improve your productivity, you could try using a time-blocking method. Break your day into segments and assign specific tasks to each segment.\"\n    )\n    for judge in judges\n]\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Prompting Example in Plaintext\nDESCRIPTION: A simple zero-shot prompt structure that asks the model to summarize an article without providing examples first. This allows for assessing the model's baseline understanding before refining with examples.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-best-practices.md#2025-04-21_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nSummarize the article below.\nArticle: {article}\nSummary:\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data from LLM Outputs with Mirascope\nDESCRIPTION: This example illustrates how to extract structured information from unstructured LLM outputs using Mirascope. It defines a Pydantic model schema for structured data extraction and uses the response_model parameter to convert LLM output into the specified structure.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-tools.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass TaskDetails(BaseModel):\n   due_date: str\n   priority: Literal[\"low\", \"normal\", \"high\"]\n\n\n@openai.call(model=\"gpt-4o\", response_model=TaskDetails)\ndef get_task_details(task: str) -> str:\n    return f\"Extract details from this task: {task}\"\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = get_task_details(task)\nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n# > due_date='next Friday' priority='high'\n```\n\n----------------------------------------\n\nTITLE: Creating a prompt template using Mirascope\nDESCRIPTION: This code demonstrates how to create a simple prompt template using the `@prompt_template` decorator in Mirascope. The function `book_recommendation_prompt` takes a genre as input and returns a string recommending a book in that genre. The decorator transforms the function into a prompt template.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@prompt_template()\ndef book_recommendation_prompt(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nprompt = book_recommendation_prompt(\"fantasy\")\nprint(prompt)\n# > [BaseMessageParam(role='user', content='Recommend a fantasy book')]\n```\n\n----------------------------------------\n\nTITLE: Initializing Web Assistant with Stream Support\nDESCRIPTION: Creates an asynchronous method to run a web search assistant with streaming capabilities and tool interaction\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/web_search_agent.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait WebAssistantBaseWithStream().run()\n```\n\n----------------------------------------\n\nTITLE: Defining Data Models using Pydantic for Qwant Search Agent\nDESCRIPTION: Creates Pydantic models for search response, search type, and optimized query to structure data in the application.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SearchResponse(BaseModel):\n    answer: str = Field(description=\"The answer to the question\")\n    sources: list[str] = Field(description=\"The sources used to generate the answer\")\n\n\nclass SearchType(BaseModel):\n    search_type: str = Field(\n        description=\"The type of search to perform (web, news, images, videos)\"\n    )\n    reasoning: str = Field(description=\"The reasoning behind the search type selection\")\n\n\nclass OptimizedQuery(BaseModel):\n    query: str = Field(description=\"The optimized search query\")\n    reasoning: str = Field(description=\"The reasoning behind the query optimization\")\n```\n\n----------------------------------------\n\nTITLE: Generic Error Handling in Streaming\nDESCRIPTION: This Python snippet demonstrates a generic approach to error handling during streaming. It emphasizes wrapping the iteration loop in a try/except block to catch potential errors, because with `stream=True` the initial response will be a generator, and errors occur during iteration and not at the function call. This example shows catching a generic exception.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    for chunk in response.iter_chunks():\n        print(chunk.choices[0].delta.content, end=\"\")\nexcept Exception as e:\n    print(f\"\\nError during streaming: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Handling Stream Response in Custom Middleware for Mirascope in Python\nDESCRIPTION: Implements a handler function for streaming Mirascope call responses. It collects stream data, calculates total tokens, and saves metadata to the database.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/middleware.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef handle_stream(result: BaseStream, fn: Callable, session: Session):\n    metadata = Metadata(\n        function_name=fn.__name__,\n        model=result.model,\n        provider=result.provider,\n        response=\"\".join(chunk.text for chunk in result.chunks),\n        total_tokens=sum(chunk.usage.total_tokens for chunk in result.chunks),\n        prompt_tokens=result.chunks[0].usage.prompt_tokens,\n        completion_tokens=sum(chunk.usage.completion_tokens for chunk in result.chunks),\n        latency=result.latency,\n    )\n    session.add(metadata)\n    session.commit()\n    return result\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with Speaker Diarization - Python\nDESCRIPTION: This snippet demonstrates how to transcribe an audio file with multiple speakers, using the Gemini API to generate speaker tags while processing the audio content.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/speech_transcription.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"YOUR_MP3_HERE\", \"rb\") as file:\n    data = file.read()\n\n    @gemini.call(model=\"gemini-1.5-flash\")\n    @prompt_template(\n        \"\"\"\n        Transcribe the content of this speech adding speaker tags \n        for example: \n            Person 1: hello \n            Person 2: good morning\n        \n        {data:audio}\n        \"\"\"\n    )\n    def transcribe_speech_from_file(data: bytes): ...\n\n    response = transcribe_speech_from_file(data)\n    print(response)\n```\n\n----------------------------------------\n\nTITLE: Using prompt_template Decorator in Mirascope\nDESCRIPTION: This snippet demonstrates Mirascope's prompt_template decorator. It shows how to create a function that generates prompt messages for book recommendations based on topics and genres, with automatic formatting and type checking of inputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseDynamicConfig, prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    Recommend some books on the following topic and genre pairs:\n    {topics_x_genres:list}\n    \"\"\"\n)\ndef recommend_book_prompt(topics: list[str], genres: list[str]) -> BaseDynamicConfig:\n    topics_x_genres = [\n        f\"Topic: {topic}, Genre: {genre}\" for topic in topics for genre in genres\n    ]\n    return {\"computed_fields\": {\"topics_x_genres\": topics_x_genres}}\n\n\nmessages = recommend_book_prompt([\"history\", \"science\"], [\"biography\", \"thriller\"])\nprint(messages)\n# > [BaseMessageParam(role='user', content='Recommend some books on the following topic and genre pairs:\\nTopic: history, Genre: biography\\nTopic: history, Genre: thriller\\nTopic: science, Genre: biography\\nTopic: science, Genre: thriller')]\nprint(messages[0].content)\n# > Recommend some books on the following topic and genre pairs:\n#   Topic: history, Genre: biography\n#   Topic: history, Genre: thriller\n#   Topic: science, Genre: biography\n```\n\n----------------------------------------\n\nTITLE: Classifying Text as Spam or Not Spam using LLM Prompt\nDESCRIPTION: This snippet demonstrates how to use a simple prompt to classify text as spam or not spam using a large language model. The prompt instructs the model to categorize the given text, and example outputs are provided for both spam and non-spam cases.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-applications.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nClassify the following text as spam or not spam: {text}\n```\n\n----------------------------------------\n\nTITLE: Chaining LLM Operations with Mirascope Computed Fields\nDESCRIPTION: This snippet demonstrates Mirascope's approach to chaining LLM operations using computed fields. It shows how to use the output of one LLM call as input to another by injecting results into prompt templates.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-runnables.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n   \"\"\"\n   Recommend a popular book in the {genre} genre.\n   Give me just the title.\n   \"\"\"\n)\ndef recommend_book(genre: str): ...\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n   \"\"\"\n   SYSTEM:\n   You are the world's greatest librarian.\n   Your task is to explain why the book \"{book_title}\" is popular in the {genre} genre.\n\n   USER:\n   Explain why \"{book_title}\" in the {genre} genre is popular.\n   \"\"\"\n)\ndef explain_book(genre: str) -> openai.OpenAIDynamicConfig:\n   return {\"computed_fields\": {\"book_title\": recommend_book(genre)}}\n\n\nexplanation = explain_book(\"science fiction\")\nprint(explanation)\n\n# > \"Dune,\" written by Frank Herbert, has garnered immense popularity in the science fiction genre...\n```\n\n----------------------------------------\n\nTITLE: Prompting with Roles using Mirascope in Python\nDESCRIPTION: Demonstrates prompting with role distinctions using the Mirascope toolkit. This Python snippet shows how to define roles such as 'system' and 'user' to enhance LLM's context understanding through structured input.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-best-practices.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n@openai.call(model=\"gpt-4o-mini\", call_params={\"temperature\": 0.4})\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a world-renowned chef.\n\n    USER:\n    I need a new recipe idea for dinner. Here are the ingredients I have:\n    {ingredients:list}\n    \"\"\"\n)\ndef recommend_recipe(ingredients: list[str]): ...\n\ningredients = [\"chicken\", \"garlic\", \"spinach\", \"mushrooms\"]\n\nresponse = recommend_recipe(ingredients=ingredients)\n\nprint(response.messages)\n# > [{'role': 'system', 'content': 'You are a world-renowned chef.'}, {'role': 'user', 'content': 'I need a new recipe idea for dinner. Here are the ingredients I have:\\nchicken\\ngarlic\\nspinach\\nmushrooms'}]\n\nprint(response.content)\n# > How about garlic chicken with spinach and mushrooms? Here's a simple recipe...\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Call with Colocated Parameters\nDESCRIPTION: Demonstrates how to create an LLM call with bundled parameters, model specification, and prompt template using Mirascope's decorators\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-prompt.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n@openai.call(model=\"gpt-4o-mini\", call_params={\"temperature\": 0.9})\n@prompt_template(\"Provide the estimated driving time from {city1} to {city2}\")\ndef report_driving_time(city1: str, city2: str): ...\n\nresponse = report_driving_time(city1=\"Los Angeles\", city2=\"San Francisco\")\nprint(response.content)  # prints the string content of the call\nprint(response.call_params) # prints the call parameters of the call\n```\n\n----------------------------------------\n\nTITLE: Implementing MiraBot ChatBot Agent\nDESCRIPTION: Complete implementation of a chatbot agent that can answer questions about Mirascope documentation, including custom parsing and document retrieval functions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/local_chat_with_codebase.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.storage import StorageContext\nfrom llama_index.core.vector_stores import SimpleVectorStore\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.ollama import Ollama\nfrom mirascope.core import openai, prompt_template\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\ndef custom_parse_choice_select_answer_fn(\n    answer: str, num_choices: int, raise_error: bool = False\n) -> tuple[list[int], list[float]]:\n    \"\"\"Custom parse choice select answer function.\"\"\"\n    answer_lines = answer.split(\"\\n\")\n    answer_nums = []\n    answer_relevances = []\n    for answer_line in answer_lines:\n        line_tokens = answer_line.split(\",\")\n        if len(line_tokens) != 2:\n            if not raise_error:\n                continue\n            else:\n                raise ValueError(\n                    f\"Invalid answer line: {answer_line}. \"\n                    \"Answer line must be of the form: \"\n                    \"answer_num: <int>, answer_relevance: <float>\"\n                )\n        split_tokens = line_tokens[0].split(\":\")\n        if (\n            len(split_tokens) != 2\n            or split_tokens[1] is None\n            or not split_tokens[1].strip().isdigit()\n        ):\n            continue\n        answer_num = int(line_tokens[0].split(\":\")[1].strip())\n        if answer_num > num_choices:\n            continue\n        answer_nums.append(answer_num)\n        # extract just the first digits after the colon.\n        _answer_relevance = re.findall(r\"\\d+\", line_tokens[1].split(\":\")[1].strip())[0]\n        answer_relevances.append(float(_answer_relevance))\n    return answer_nums, answer_relevances\n\n\ndef get_documents(query: str) -> str:\n    \"\"\"The get_documents tool that retrieves Mirascope documentation based on the\n    relevance of the query\"\"\"\n    query_engine = loaded_index.as_query_engine(\n        similarity_top_k=10,\n        node_postprocessors=[\n            LLMRerank(\n                choice_batch_size=5,\n                top_n=2,\n                parse_choice_select_answer_fn=custom_parse_choice_select_answer_fn,\n            )\n        ],\n        response_mode=\"tree_summarize\",\n    )\n\n    response = query_engine.query(query)\n    if isinstance(response, Response):\n        return response.response or \"No documents found.\"\n    return \"No documents found.\"\n\n\nclass MirascopeBot(BaseModel):\n    @openai.call(\n        model=\"llama3.1\",\n        client=OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n    )\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an AI Assistant that is an expert at answering questions about Mirascope.\n        Here is the relevant documentation to answer the question.\n\n        Context:\n        {context}\n\n        USER:\n        {question}\n        \"\"\"\n    )\n    def _step(self, context: str, question: str): ...\n\n    def _get_response(self, question: str):\n        context = get_documents(question)\n        answer = self._step(context, question)\n        print(\"(Assistant):\", answer.content)\n        return\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            self._get_response(question)\n\n\nMirascopeBot().run()\n```\n\n----------------------------------------\n\nTITLE: Using Pydub for Audio Inputs with Messages Methods in Python\nDESCRIPTION: Demonstrates handling audio inputs using pydub and Messages.User method for multi-modal LLM interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom pydub import AudioSegment\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.content_param import AudioParam\n\n\n@prompt_template\ndef transcribe_audio_prompt(audio: bytes) -> Messages.Type:\n    return Messages.User(f\"Transcribe this audio: {AudioParam(audio)}\")\n\n\n# Read an audio file\naudio_segment = AudioSegment.from_file(\"examples/data/sample.mp3\")\naudio_bytes = audio_segment.export(format=\"mp3\").read()\n\n# Use the audio in a prompt\nprint(transcribe_audio_prompt(audio=audio_bytes))\n# Returns a user message with both text and audio content\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Multimodal Capabilities in Python with Mirascope v0 and v1\nDESCRIPTION: This snippet demonstrates how to implement multimodal capabilities using Mirascope v0 and v1. It shows the transition from manually constructing message content to using a more intuitive prompt template with image support.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICall\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nclass ImageDetection(OpenAICall):\n    def messages(self) -> list[ChatCompletionMessageParam]:\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"I just read this book: \"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": \"https://upload.wikimedia.org/wikipedia/en/4/44/Mistborn-cover.jpg\",\n                        },\n                    },\n                    {\"type\": \"text\", \"text\": \"What should I read next?\"},\n                ],\n            },\n        ]\n\n\nresponse = ImageDetection().call()\nprint(response.content)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    I just read this book: {previous_book:image}.\n    What should I read next?\n    \"\"\"\n)\ndef recommend_book(previous_book: str): ...\n\n\nresponse = recommend_book(\n    \"https://upload.wikimedia.org/wikipedia/en/4/44/Mistborn-cover.jpg\"\n)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing TextPart Class in Python for Mirascope\nDESCRIPTION: The TextPart class is probably a subclass of BaseMessageParam, specifically designed to handle text content in messages within the Mirascope system.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/message_param.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmirascope.core.base.message_param.TextPart\n```\n\n----------------------------------------\n\nTITLE: Accessing Stream Properties After Exhaustion\nDESCRIPTION: Shows how to access common properties of a BaseStream object after it has been exhausted by iterating through it. This example demonstrates accessing the input_tokens property of the stream.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/streams/basic_usage/{{ provider | provider_dir }}/{{ method }}.py::13\"\n```\n\n----------------------------------------\n\nTITLE: Error Handling with Pydantic in Mirascope\nDESCRIPTION: Shows how to implement error handling using Pydantic models with Mirascope. This example defines a Movie schema and uses try-except blocks to gracefully handle validation errors when the model output doesn't match the expected structure.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Movie(BaseModel):\n    title: str\n    rating: float\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Movie)\ndef recommend_movie() -> str:\n    return f\"Recommend a movie\"\n\n\ntry:\n    movie = recommend_movie()\n    assert isinstance(movie, Movie)\n    print(movie)\n    # > title='Inception' rating=8.8\nexcept ValidationError as e:\n    print(e)\n    # > 1 validation error for Movie\n    #  rating\n    #    Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='excellent', input_type=str]\n    #      For further information visit https://errors.pydantic.dev/2.6/v/float_parsing\n```\n\n----------------------------------------\n\nTITLE: Deduplicating Book Entries with a LLM Call\nDESCRIPTION: This code snippet defines a function using OpenAI's API to deduplicate a list of book objects. It utilizes a custom prompt for semantic deduplication based on potential variations in title, author, and genre.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/removing_semantic_duplicates.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(model=\"gpt-4o\", response_model=list[Book])\n@prompt_template(\n    '''\n    SYSTEM:\n    Your job is to take a database of books and clean it up by removing items which are\n    semantic duplicates. Look out for typos, formatting differences, and categorizations.\n    For example, \"Mistborn\" and \"Mistborn: The Final Empire\" are the same book \n    but \"Mistborn: Shadows of Self\" is not.\n    Then return all the unique books.\n\n    USER:\n    {books}\n    '''\n)\ndef deduplicate_books(books: list[Book]): ...\n```\n\n----------------------------------------\n\nTITLE: CohereCallResponse Class Implementation\nDESCRIPTION: Defines a class for processing and storing Cohere API call responses. The class extends BaseLLMCallResponse and handles response formatting and validation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/cohere/call_response.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: mirascope.core.cohere.call_response\n```\n\n----------------------------------------\n\nTITLE: Handling Image Inputs with Messages Methods in Python\nDESCRIPTION: Shows how to use Messages.User method to create prompts with image inputs for multi-modal model interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.content_param import ImageParam\n\n\n@prompt_template\ndef describe_image_prompt(image: bytes) -> Messages.Type:\n    return Messages.User(f\"Describe this image: {ImageParam(image)}\")\n\n\n# Read an image from a file\nwith open(\"examples/data/cat.jpg\", \"rb\") as f:\n    image_bytes = f.read()\n\n# Use the image in a prompt\nprint(describe_image_prompt(image=image_bytes))\n# Returns a user message with both text and image content\n\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Prompting Example\nDESCRIPTION: This is an example of few-shot prompting, where the language model is given a few examples of the task as training data. The LLM should understand the desired output format and the nature of the task, converting Celsius to Fahrenheit.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Convert the following temperatures from Celsius to Fahrenheit:\\n\\n0°C to °F: 32°F\\n25°C to °F: 77°F\\n-10°C to °F: 14°F\\n\\n100°C to °F:\"\n```\n\n----------------------------------------\n\nTITLE: Loading Stored Embeddings\nDESCRIPTION: This snippet loads the previously stored embeddings from the local storage and creates a query engine. It's used to retrieve relevant documents based on user queries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/documentation_agent.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import (\n    load_index_from_storage,\n)\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nloaded_index = load_index_from_storage(storage_context)\nquery_engine = loaded_index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Defining SQLModel Table for Metadata in Python\nDESCRIPTION: Creates a SQLModel table named 'Metadata' to store information about Mirascope calls, including timestamps, function name, and response details.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/middleware.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Metadata(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n    function_name: str\n    model: str\n    provider: str\n    response: str\n    total_tokens: int\n    prompt_tokens: int\n    completion_tokens: int\n    latency: float\n    error: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Writing Multi-Line Prompts with BaseMessageParam in Python\nDESCRIPTION: Shows how to create multi-line prompts using BaseMessageParam and inspect.cleandoc to maintain proper formatting.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.message_param import BaseMessageParam\n\n\n@prompt_template\ndef summarize_prompt(text: str) -> Messages.Type:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n                Summarize the following text:\n\n                Text:\n                {text}\n\n                Summary:\n                \"\"\"\n            ),\n        )\n    ]\n\n\nprint(summarize_prompt(text=\"This is a sample text that needs to be summarized.\"))\n\n```\n\n----------------------------------------\n\nTITLE: Making API Calls to Mistral AI Models in Python\nDESCRIPTION: This function sends requests to Mistral AI models, handling authentication and request formatting. It supports various model parameters and includes error handling for API responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/mistral/call.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport json\nfrom typing import Any, Dict, List, Optional, Union\n\nimport httpx\nfrom pydantic import BaseModel\n\nfrom mirascope.core.types import (BaseMessage, ChatCompletionResponse,\n                                CompletionResponse)\nfrom mirascope.utils.secrets import get_secret\n\n\ndef call(\n    model: str,\n    messages: List[BaseMessage],\n    api_base: str = \"https://api.mistral.ai/v1\",\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    top_p: Optional[float] = None,\n    safe_prompt: Optional[bool] = None,\n    random_seed: Optional[int] = None,\n    stream: bool = False,\n    stop: Union[str, List[str], None] = None,\n    api_key: Optional[str] = None,\n) -> Union[ChatCompletionResponse, CompletionResponse]:\n    api_key = api_key or get_secret(\"MISTRAL_API_KEY\")\n\n    if not api_key:\n        raise ValueError(\n            \"Mistral API key not found. Please set the MISTRAL_API_KEY environment \"\n            \"variable or pass it as an argument.\"\n        )\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\",\n        \"Authorization\": f\"Bearer {api_key}\",\n    }\n\n    payload: Dict[str, Any] = {\n        \"model\": model,\n        \"messages\": [{\"role\": m.role, \"content\": m.content} for m in messages],\n    }\n\n    if temperature is not None:\n        payload[\"temperature\"] = temperature\n    if max_tokens is not None:\n        payload[\"max_tokens\"] = max_tokens\n    if top_p is not None:\n        payload[\"top_p\"] = top_p\n    if safe_prompt is not None:\n        payload[\"safe_prompt\"] = safe_prompt\n    if random_seed is not None:\n        payload[\"random_seed\"] = random_seed\n    if stream:\n        payload[\"stream\"] = stream\n    if stop:\n        payload[\"stop\"] = stop\n\n    with httpx.Client() as client:\n        response = client.post(\n            f\"{api_base}/chat/completions\",\n            headers=headers,\n            json=payload,\n            timeout=600,\n        )\n\n    if response.status_code != 200:\n        raise Exception(\n            f\"Error calling Mistral API: {response.status_code} {response.text}\"\n        )\n\n    response_json = response.json()\n\n    class Choice(BaseModel):\n        index: int\n        message: Dict[str, str]\n        finish_reason: Optional[str] = None\n\n    class Usage(BaseModel):\n        prompt_tokens: int\n        completion_tokens: int\n        total_tokens: int\n\n    class MistralResponse(BaseModel):\n        id: str\n        object: str\n        created: int\n        model: str\n        choices: List[Choice]\n        usage: Usage\n\n    mistral_response = MistralResponse(**response_json)\n\n    chat_completion_response = ChatCompletionResponse(\n        id=mistral_response.id,\n        object=mistral_response.object,\n        created=mistral_response.created,\n        model=mistral_response.model,\n        choices=[\n            {\n                \"index\": choice.index,\n                \"message\": {\n                    \"role\": choice.message[\"role\"],\n                    \"content\": choice.message[\"content\"],\n                },\n                \"finish_reason\": choice.finish_reason,\n            }\n            for choice in mistral_response.choices\n        ],\n        usage={\n            \"prompt_tokens\": mistral_response.usage.prompt_tokens,\n            \"completion_tokens\": mistral_response.usage.completion_tokens,\n            \"total_tokens\": mistral_response.usage.total_tokens,\n        },\n    )\n\n    return chat_completion_response\n```\n\n----------------------------------------\n\nTITLE: Implementing Audio Stream Interaction\nDESCRIPTION: Complete example of setting up streaming audio interaction with OpenAI's Realtime API using Mirascope's decorator pattern.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/openai.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/openai/realtime/audio_stream_input_output.py\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Groq API Response Streaming in Python\nDESCRIPTION: This asynchronous generator function streams the response from the Groq API. It parses the JSON data from the stream and yields the generated text content.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/groq/call.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nasync def _stream_response(response: httpx.Response) -> AsyncGenerator[str, None]:\n    async for line in response.aiter_lines():\n        if line.startswith(\"data: \"):\n            if line.strip() == \"data: [DONE]\":\n                break\n            data = json.loads(line[6:])\n            yield data[\"choices\"][0][\"text\"]\n```\n\n----------------------------------------\n\nTITLE: Using Built-In Types for Response Models\nDESCRIPTION: Demonstrates the shorthand approach of using built-in Python types as response models without defining a full Pydantic BaseModel. This example shows how to extract a list of strings directly.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/response_models.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/response_models/builtin_types/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Function Chaining with Mirascope\nDESCRIPTION: Shows how to chain multiple function calls together where the output of one step becomes the input for the next. The example demonstrates summarizing a text and then evaluating the summary quality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/chaining.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/chaining/function_chaining/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Chained OpenAI API Calls\nDESCRIPTION: Shows how to chain multiple OpenAI API calls together using decorators, where the output of one call becomes input for another.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import Messages, openai\n\n@openai.call(\"gpt-4o-mini\")\ndef select_chef(food_type: str) -> str:\n    return f\"Name a chef who is good at cooking {food_type} food\"\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_recipe(ingredient: str, food_type: str) -> str:\n    chef = select_chef(food_type)\n    return \"Imagine you are {chef}. Recommend a {food_type} recipe using {ingredient}\"\n\nresponse = recommend_recipe(\"apples\", \"japanese\")\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Running the Chatbot Application in Python\nDESCRIPTION: This snippet shows how to initiate the chatbot application and engage users through a console interface. It allows users to ask questions in real-time and interacts with them based on scripted behavior to enhance user experience.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nChatbot().run()\n# > (User): Can you tell me about the Mirascope python library?\n# > (Assistant): Do you want to use the WebSearch tool? (y/n): y\n```\n\n----------------------------------------\n\nTITLE: Lilypad Version Detection Example\nDESCRIPTION: Demonstrates how Lilypad's versioning system detects changes in both the main function and dependent functions to update versions automatically.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# lily/recommend_book.py\nimport lilypad\nfrom mirascope.core import openai\n\n\n# Changes to this function will also be detected and bump\n# the version of the LLM function :)\ndef get_available_books() -> list[str]:\n    return [\"The Name of the Wind\"]\n\n\n@lilypad.llm_fn()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    books = get_available_books()\n    return f\"Recommend a {genre} book from this list: {books}\"\n\n\nif __name__ == \"__main__\":\n    output = recommend_book(\"fantasy\")\n    print(output.content)\n```\n\n----------------------------------------\n\nTITLE: Advanced Sentiment Validation\nDESCRIPTION: Demonstrates advanced validation using Pydantic's AfterValidator for sentiment analysis of LLM outputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated, Literal\nfrom pydantic import AfterValidator, BaseModel, ValidationError\nfrom mirascope.core import openai\n\n@openai.call(\"gpt-4o-mini\", response_model=Literal[\"happy\", \"sad\"])\ndef classify_sentiment(text: str) -> str:\n    return f\"Is the following text happy or sad? {text}\"\n\ndef validate_happy(story: str) -> str:\n    \"\"\"Confirm that the story is happy.\"\"\"\n    assert classify_sentiment(story) == \"happy\", \"Story wasn't happy\"\n    return story\n\n@openai.call(\n    model=\"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(validate_happy)],\n)\ndef tell_sad_story() -> str:\n    return \"Tell me a very sad story.\"\n\ntry:\n    story = tell_sad_story()\n    print(story)\nexcept ValidationError as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Mirascope prompt_template example\nDESCRIPTION: This snippet showcases Mirascope's `prompt_template` decorator, which allows defining prompts as functions with type annotations.  This enables editor support and compile-time error checking for prompt variable names and types, preventing runtime errors. It showcases how the storyline is a fixed word and string attribute.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM: You are a top class manga editor\n    USER: What do you think of this new storyline I'm working on? {storyline}\n    \"\"\"\n)\ndef editor_prompt(storyline: str): ...\n\n\nstoryline = \"Two friends go off on an adventure, and ...\"\nmessages = editor_prompt(storyline)\n\nprint(messages)\n# > [\n#     BaseMessageParam(role='system', content='You are a top class manga editor'),\n#     BaseMessageParam(role='user', content='What do you think of this storyline I'm working on? Two friends go off on an adventure, and ...')\n#   ]\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Function with Lilypad Decorator\nDESCRIPTION: Shows how to implement an LLM function using Lilypad's decorator for automatic versioning and tracing. Includes example of book recommendation functionality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-versioning.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lilypad\nfrom mirascope.core import openai\n\n\n@lilypad.llm_fn()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nif __name__ == \"__main__\":\n    output = recommend_book(\"fantasy\")\n    print(output.content)\n```\n\n----------------------------------------\n\nTITLE: Handling Image Inputs with String Templates in Python\nDESCRIPTION: Demonstrates using string templates with the :image tag to include image inputs in prompts for multi-modal interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template(\"\"\"\n    USER: Describe this image: {image:image}\n\"\"\")\ndef describe_image_prompt(image: bytes) -> Messages.Type:\n    return locals()\n\n\n# Read an image from a file\nwith open(\"examples/data/cat.jpg\", \"rb\") as f:\n    image_bytes = f.read()\n\n# Use the image in a prompt\nprint(describe_image_prompt(image=image_bytes))\n# Returns a user message with both text and image content\n\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from OpenAI Function Call\nDESCRIPTION: This code shows an example JSON response from the OpenAI Chat Completion API when a function call is requested.  The `tool_calls` field contains the name of the function to be called and the arguments for the function.  The `tool_call_id` is also present for each tool call. If no function is called, the `tool_calls` field would be `null` and the model's response would be in the `content` field.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"index\": 0,\n    \"message\": {\n        \"role\": \"assistant\",\n        \"content\": null,\n        \"tool_calls\": [\n            {\n                \"name\": \"get_current_stock_price\",\n                \"arguments\": '{\\n  \"company\": \"Apple Inc.\",\\n  \"format\": \"USD\"\\n}',\n            }\n        ],\n    },\n    \"finish_reason\": \"tool_calls\",\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Reference-Based Scoring Prompt for LLM Judge\nDESCRIPTION: This code snippet shows how to create a prompt for an LLM judge to perform reference-based scoring. It includes instructions for comparing the AI-generated answer to a reference solution and a scoring scale.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-as-judge.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[System]\nPlease act as an impartial judge and evaluate the quality of the AI-generated answer to the user question provided below. You will also be given a reference solution for comparison. Your evaluation should consider correctness, clarity, and adherence to the principles outlined in the reference solution.\n\nBegin your evaluation by carefully comparing the AI-generated answer with the reference solution. Identify any outputs that aren't true, as well as omissions, or deviations, and simulate human judgments in explaining how these impact the overall quality of the response. Ensure that your assessment is objective and consistent.\n\nAt the end of your evaluation, assign a score from 1 to 5 based on the following scale:\n- 1: Very poor—does not meet the requirements or is significantly incorrect.\n- 2: Poor—contains major errors or omissions.\n- 3: Fair—adequate but with notable flaws.\n- 4: Good—meets the requirements with minor errors.\n- 5: Excellent—fully accurate and well-articulated.\n\n[User Question]\n{question}\n\n[Reference Solution]\n{reference_solution}\n\n[AI-Generated Answer]\n{ai_generated_answer}\n\n[Your Evaluation]\nScore: {1-5}\nExplanation: {Detailed reasoning based on the comparison}\n```\n\n----------------------------------------\n\nTITLE: Reinserting Tool Calls into Chat Messages\nDESCRIPTION: This snippet shows how to reinsert the result of a tool call into the chat history using `response.tool_message_params`.  This allows the model to maintain context and use the tool's output for subsequent responses. The code defines a `get_current_stock_price` function that simulates fetching stock prices, and uses `@openai.call` and `@prompt_template` to integrate it with a chat completion.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom openai.types.chat import ChatCompletionMessageParam\n\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_current_stock_price(company: str, format: Literal[\"USD\", \"EUR\", \"JPY\"] = \"USD\"):\n    \"\"\"Get the current stock price of a company.\"\"\"\n    if \"apple\" in company.lower():\n        return f\"The current stock price of {company} is 200 {format}\"\n    elif \"microsoft\" in company.lower():\n        return f\"The current stock price of {company} is 250 {format}\"\n    elif \"google\" in company.lower():\n        return f\"The current stock price of {company} is 2800 {format}\"\n    else:\n        return f\"I'm not sure what the stock price of {company} is.\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_current_stock_price])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {question}\n    \"\"\"\n)\ndef check_stock_price(question: str, history: list[ChatCompletionMessageParam]): ...\n\n\n# Make the first call to the LLM\nhistory = []\nresponse = check_stock_price(\"What's the stock price of Apple Inc?\", history)\nif response.user_message_param:\n    history.append(response.user_message_param)\nhistory.append(response.message_param)\n\nif tool := response.tool:\n    print(\"Tool arguments:\", tool.model_dump())\n    # > {'company': 'Apple Inc', 'format': 'USD'}\n    output = tool.call()\n    print(\"Tool output:\", output)\n    # > The current stock price of Apple Inc is 200 USD\n\n    # Reinsert the tool call into the chat messages through history\n    history += response.tool_message_params([(tool, output)])\nelse:\n    print(response.content)  # if no tool, print the content of the response\n\n# Make an API call to the LLM again with the history including the tool call and no new user message\nresponse = check_stock_price(\"\", history)\nprint(\"After Tools Response:\", response.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing execute_task Method for AzureTool\nDESCRIPTION: Defines the execute_task method that manages the execution of a tool via Azure Functions. It handles task execution with support for async patterns and manages input/output formats according to Azure's requirements.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/azure/tool.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef execute_task(\n        self,\n        task: Dict[str, Any],\n        return_executor_output: bool = False,\n    ) -> str:\n        \"\"\"Execute the tool on the provided task.\n\n        Args:\n            task: The task to execute.\n            return_executor_output: Whether to return the executor's output.\n\n        Returns:\n            The execution result.\n        \"\"\"\n        executor = cast(AzureExecutor, asyncio.run(self.get_executor(task)))\n        executor_result = executor.execute()\n        if return_executor_output:\n            return executor_result\n        else:\n            return executor.parse_result(executor_result)\n```\n\n----------------------------------------\n\nTITLE: Creating LocalizedRecommenderBaseWithStep class for agent functionality\nDESCRIPTION: This class extends LocalizedRecommenderBase and adds the _step method, which uses OpenAI's GPT-4 model to process user questions and determine which tools to use.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/localized_agent.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\nclass LocalizedRecommenderBaseWithStep(LocalizedRecommenderBase):\n    @openai.call(model=\"gpt-4o\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are a local guide that recommends the best places to visit in a place.\n        Use the `_get_current_date` function to get the current date.\n        Use the `_get_coordinates_from_location` function to get the coordinates of a location if you need it.\n        Use the `_nimble_google_maps` function to get the best places to visit in a location based on the users query.\n\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    async def _step(self, question: str) -> openai.OpenAIDynamicConfig:\n        return {\n            \"tools\": [\n                self._get_current_date,\n                self._get_coordinates_from_location,\n                self._nimble_google_maps,\n            ]\n        }\n```\n\n----------------------------------------\n\nTITLE: Handling Response Model in Custom Middleware for Mirascope in Python\nDESCRIPTION: Implements a handler function for Mirascope calls with a response_model. It extracts metadata from the response model and saves it to the database, handling both BaseModel and primitive types.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/middleware.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef handle_response_model(result: Union[BaseModel, BaseType], fn: Callable, session: Session):\n    if isinstance(result, BaseModel):\n        response = result._response\n        metadata = Metadata(\n            function_name=fn.__name__,\n            model=response.model,\n            provider=response.provider,\n            response=str(result),\n            total_tokens=response.usage.total_tokens,\n            prompt_tokens=response.usage.prompt_tokens,\n            completion_tokens=response.usage.completion_tokens,\n            latency=response.latency,\n        )\n    else:\n        metadata = Metadata(\n            function_name=fn.__name__,\n            model=\"unknown\",\n            provider=\"unknown\",\n            response=str(result),\n            total_tokens=0,\n            prompt_tokens=0,\n            completion_tokens=0,\n            latency=0,\n        )\n    session.add(metadata)\n    session.commit()\n    return result\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Class in Python for AI Applications\nDESCRIPTION: This code snippet defines the Prompt class, which is used to create and manage prompts for AI models. It includes methods for adding content, managing variables, and rendering the final prompt text. The class uses a template engine for variable substitution and supports both string and callable content.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/prompt.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nfrom jinja2 import Template\n\nfrom mirascope.core.base.content import Content\nfrom mirascope.core.base.variable import Variable\n\n\nclass Prompt:\n    \"\"\"A class for creating and managing prompts.\"\"\"\n\n    def __init__(\n        self,\n        content: Optional[Union[str, Callable[..., str]]] = None,\n        variables: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Initialize a Prompt object.\n\n        Args:\n            content: The initial content of the prompt.\n            variables: A dictionary of variables to be used in the prompt.\n        \"\"\"\n        self.content: List[Content] = []\n        self.variables: Dict[str, Variable] = {}\n\n        if content:\n            self.add(content)\n\n        if variables:\n            for name, value in variables.items():\n                self.add_variable(name, value)\n\n    def add(self, content: Union[str, Callable[..., str]]) -> None:\n        \"\"\"Add content to the prompt.\n\n        Args:\n            content: The content to add to the prompt.\n        \"\"\"\n        self.content.append(Content(content))\n\n    def add_variable(self, name: str, value: Any) -> None:\n        \"\"\"Add a variable to the prompt.\n\n        Args:\n            name: The name of the variable.\n            value: The value of the variable.\n        \"\"\"\n        self.variables[name] = Variable(value)\n\n    def render(self, **kwargs: Any) -> str:\n        \"\"\"Render the prompt with the given variables.\n\n        Args:\n            **kwargs: Additional variables to use when rendering the prompt.\n\n        Returns:\n            The rendered prompt as a string.\n        \"\"\"\n        rendered_content = \"\"\n        for content in self.content:\n            rendered_content += content.render(**kwargs)\n\n        variables = {**self.variables, **kwargs}\n        template = Template(rendered_content)\n        return template.render(**{k: v.value for k, v in variables.items()})\n\n    def __str__(self) -> str:\n        return self.render()\n```\n\n----------------------------------------\n\nTITLE: Function Calling with OpenAI API\nDESCRIPTION: This snippet shows how to define a function call with the OpenAI API, which requires manually creating a JSON schema for the function definition.  This involves specifying the function name, description, parameters, and required fields. This is often more verbose than using Mirascope.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_stock_price\",\n            \"description\": \"Get the current stock price of a company\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"company\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the company, e.g., 'Apple Inc.' or 'Google LLC'\",\n                    },\n                },\n                \"required\": [\"company\"],\n            },\n        },\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Pandas DataFrame with Synthetic Data\nDESCRIPTION: Create a custom DataFrameGenerator to generate and manipulate synthetic data using Pydantic and pandas\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/generating_synthetic_data.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Literal\nimport pandas as pd\n\nclass DataFrameGenerator(BaseModel):\n    data: list[list[Any]] = Field(description=\"the data to be inserted into the dataframe\")\n    column_names: list[str] = Field(description=\"The names of the columns in data\")\n\n    def append_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n        return pd.concat([df, self.generate_dataframe()], ignore_index=True)\n\n    def generate_dataframe(self) -> pd.DataFrame:\n        return pd.DataFrame(dict(zip(self.column_names, self.data, strict=False)))\n\n@openai.call(model=\"gpt-4o-mini\", response_model=DataFrameGenerator)\n@prompt_template(\"\"\"\nGenerate {num_datapoints} random but realistic datapoints of items which could\nbe in a home appliances warehouse. Generate your response as `data` and\n`column_names`, so that a pandas DataFrame may be generated with:\n`pd.DataFrame(data, columns=column_names)`.\n\nFormat:\nName, Price, Inventory\n\nName - the name of the home appliance product\nPrice - the price of an individual product, in dollars (include cents)\nInventory - how many are left in stock\n\"\"\")\ndef generate_df_data(num_datapoints: int): ...\n\ndf_data = generate_df_data(5)\ndf = df_data.generate_dataframe()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Text-Only Interaction Implementation\nDESCRIPTION: Example showing text-based interaction with OpenAI's Realtime API using Mirascope.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/openai.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/openai/realtime/text_input_output.py\"\n```\n\n----------------------------------------\n\nTITLE: Implementing LangChain StructuredOutputParser for Recipe Extraction\nDESCRIPTION: Example showing how to use LangChain's StructuredOutputParser to extract recipe and ingredients information from an LLM response using ResponseSchema objects and prompt templates.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-structured-output.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.output_parsers import ResponseSchema, StructuredOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Define the schema for the expected output, including two fields: \"recipe\" and \"ingredients\"\nresponse_schemas = [\n    ResponseSchema(name=\"recipe\", description=\"the recipe for the dish requested by the user\"),\n    ResponseSchema(\n        name=\"ingredients\",\n        description=\"list of ingredients required for the recipe, should be a detailed list.\",\n    ),\n]\n\n# Create a StructuredOutputParser instance from the defined response schemas\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n\n# Generate format instructions based on the response schemas, which will be injected into the prompt\nformat_instructions = output_parser.get_format_instructions()\n\n# Define the prompt template, instructing the model to provide the recipe and ingredients\nprompt = PromptTemplate(\n    template=\"Provide the recipe for the dish requested.\\n{format_instructions}\\n{dish}\",\n    input_variables=[\"dish\"],\n    partial_variables={\"format_instructions\": format_instructions},\n)\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# Create a chain that connects the prompt, model, and output parser\nchain = prompt | model | output_parser\n\n# The output will be structured according to the predefined schema with fields for \"recipe\" and \"ingredients\"\nchain.invoke({\"dish\": \"Spaghetti Bolognese\"})\n```\n\n----------------------------------------\n\nTITLE: Changing Model Providers with Anthropic in Mirascope\nDESCRIPTION: A basic example showing how to use Mirascope with Anthropic's Claude model to recommend a recipe based on an ingredient. This demonstrates the simplicity of provider selection through imports and decorators.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_recipe(ingredient: str) -> str:\n    return f\"Recommend a recipe that includes {ingedient}\"\n\n\nresponse = recommend_recipe(ingredient=\"chicken\")\nprint(response.content)\n# > Chicken Alfredo with Creamy Garlic Sauce\n```\n\n----------------------------------------\n\nTITLE: Executing receipt information extraction in Python\nDESCRIPTION: Executes the receipt information extraction using OpenAI and Anthropic APIs via Mirascope. The image is retrieved and encoded in base64 before passing through the extraction functions. Outputs are printed to verify extraction accuracy.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/extraction_using_vision.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\nimport httpx\n\nimage_url = \"https://www.receiptfont.com/wp-content/uploads/template-mcdonalds-1-screenshot-fit.png\"\n\nimage_media_type = \"image/png\"\nimage_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n\nprint(extract_receipt_info_openai(image_url))\n\nprint(extract_receipt_info_anthropic(image_url))\n```\n\n----------------------------------------\n\nTITLE: Initializing LogfireHandler for Mirascope-Logfire Integration in Python\nDESCRIPTION: This snippet defines the LogfireHandler class, which extends logging.Handler to provide integration with Logfire. It includes initialization with API key and source, as well as methods for emitting logs and handling errors.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/integrations/logfire.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport logging\nimport requests\nfrom typing import Any, Dict\n\nclass LogfireHandler(logging.Handler):\n    def __init__(self, api_key: str, source: str):\n        super().__init__()\n        self.api_key = api_key\n        self.source = source\n        self.url = \"https://in.logfire.ai/\"\n\n    def emit(self, record: logging.LogRecord) -> None:\n        log_entry = self.format(record)\n        data = {\n            \"message\": log_entry,\n            \"level\": record.levelname.lower(),\n            \"source\": self.source,\n        }\n        self.send_to_logfire(data)\n\n    def send_to_logfire(self, data: Dict[str, Any]) -> None:\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"X-Api-Key\": self.api_key,\n        }\n        try:\n            response = requests.post(self.url, json=data, headers=headers)\n            response.raise_for_status()\n        except requests.RequestException as e:\n            self.handleError(e)\n```\n\n----------------------------------------\n\nTITLE: Creating Prompt Templates with Variable Placeholders\nDESCRIPTION: A basic prompt template example showing how to create reusable prompts with placeholders like {diet_type} that can be filled in at runtime with specific values such as 'keto', 'vegan', or 'gluten free'.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nYou are a nutritionist specializing in creating personalized meal plans. Based on the {diet_type} diet, suggest a healthy meal plan for the day.\n```\n\n----------------------------------------\n\nTITLE: Defining Translation Model and Function\nDESCRIPTION: This snippet defines a Pydantic model 'Translations' for validating the response structure and a function 'translate' that retrieves translations of a given phrase into the specified language using the OpenAI API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/prompt_paraphrasing.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass Translations(BaseModel):\\n    translations: list[str] = Field(\\n        ..., description=\"The list of translations into the requested language\"\\n    )\\n\\n\\n@openai.call(model=\"gpt-4o-mini\", response_model=Translations)\\n@prompt_template(\\n    '''\\n    For this phrase: {phrase}\\n\\n\\n    Give {num_translations} translations in {language}\\n    '''\\n)\\ndef translate(phrase: str, language: str, num_translations: int): ...\n```\n\n----------------------------------------\n\nTITLE: Generating OpenAITool Schema in Python\nDESCRIPTION: This snippet demonstrates generating an OpenAITool schema using Mirascope, providing LLMs with details about the tool's functionalities and input expectations. It uses extract_content function to create a schema that Mirascope can use to enhance LLM capabilities.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/tools_and_agents.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core.openai import OpenAITool\n\ntool_type = OpenAITool.type_from_fn(extract_content)\nprint(tool_type.tool_schema())\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Summarization with LangChain in Python\nDESCRIPTION: Shows how to use LangChain for text summarization, highlighting its more complex approach involving multiple abstractions and chain construction.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-sucks.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\n\ndef summarize(text: str):\n    model = ChatOpenAI()\n    template = \"Summarize this text: {text}\"\n    prompt = PromptTemplate.from_template(template)\n\n    chain = (\n        {\"text\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n    )\n\n    return chain.invoke(text)\n\ntext = \"\"\"The recent advancements in technology like generative AI (e.g., the chatbot-like ChatGPT) have had a profound impact on the way we communicate and interact with each other on a daily basis. With the advent of smartphones and social media platforms, people are now able to stay connected with friends and family members regardless of geographical distances. Moreover, these technological innovations have also revolutionized the business world, allowing companies to reach a global audience with ease and efficiency.\"\"\"\n\nprint(summarize(text))\n```\n\n----------------------------------------\n\nTITLE: Comparing Mirascope to Provider SDKs\nDESCRIPTION: Example showing how to extract structured data using a provider's official SDK, highlighting the differences in verbosity and complexity compared to Mirascope's approach.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/WELCOME.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{% if provider == \"Anthropic\" %}\n--8<-- \"examples/learn/response_models/basic_usage/official_sdk/{{ provider | provider_dir }}_sdk.py\"\n{% elif provider == \"Mistral\" %}\n--8<-- \"examples/learn/response_models/basic_usage/official_sdk/{{ provider | provider_dir }}_sdk.py\"\n{% elif provider == \"Google\" %}\n--8<-- \"examples/learn/response_models/basic_usage/official_sdk/{{ provider | provider_dir }}_sdk.py\"\n{% elif provider == \"Cohere\" %}\n--8<-- \"examples/learn/response_models/basic_usage/official_sdk/{{ provider | provider_dir }}_sdk.py\"\n{% elif provider == \"LiteLLM\" %}\n--8<-- \"examples/learn/response_models/basic_usage/official_sdk/{{ provider | provider_dir }}_sdk.py\"\n{% elif provider == \"Azure AI\" %}\n--8<-- \"examples/learn/response_models/basic_usage/official_sdk/{{ provider | provider_dir }}_sdk.py\"\n{% else %}\n--8<-- \"examples/learn/response_models/basic_usage/official_sdk/{{ provider | provider_dir }}_sdk.py\"\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Chaining Components with Function Arguments in Mirascope\nDESCRIPTION: This snippet demonstrates how to chain Mirascope components together by explicitly passing the output of one function as input into the next function in the chain. It utilizes the `@openai.call` and `@prompt_template` decorators to define LLM calls and prompt templates, respectively. The functions `recommend_book` and `explain_book` are chained together, with the output of `recommend_book` being passed as input to `explain_book`.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-runnables.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n   \"\"\"\n   Recommend a popular book in the {genre} genre.\n   Give me just the title.\n   \"\"\"\n)\ndef recommend_book(genre: str): ...\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n   \"\"\"\n   SYSTEM:\n   You are the world's greatest librarian.\n   Your task is to explain why the book \"{book_title}\" is popular in the {genre} genre.\n\n   USER:\n   Explain why \"{book_title}\" in the {genre} genre is popular.\n   \"\"\"\n)\ndef explain_book(genre: str, book_title: str): ...\n\n\ngenre = \"science fiction\"\n\n\ndef explain_book_chain(genre: str):\n   book_title = recommend_book(genre)\n   explanation = explain_book(genre, book_title.content)\n   print(explanation.content)\n\n\nexplain_book_chain(\"science fiction\")\n# > \"Dune,\" a science fiction novel by Frank Herbert, is popular because...\n```\n\n----------------------------------------\n\nTITLE: Defining HTTP Request Function in Python\nDESCRIPTION: This function, _request, is a generic HTTP request handler that supports both GET and POST methods. It manages sessions, handles errors, and processes JSON responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/tools/web/requests.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef _request(\n    method: str,\n    url: str,\n    session: requests.Session | None = None,\n    headers: dict[str, str] | None = None,\n    params: dict[str, str] | None = None,\n    data: dict[str, str] | None = None,\n    json_data: dict[str, str] | None = None,\n    timeout: float | None = None,\n) -> dict[str, Any]:\n    \"\"\"Make a request to the given URL.\"\"\"\n    try:\n        if session is None:\n            session = requests.Session()\n        response = session.request(\n            method,\n            url,\n            headers=headers,\n            params=params,\n            data=data,\n            json=json_data,\n            timeout=timeout,\n        )\n        response.raise_for_status()\n        return response.json()\n    except json.JSONDecodeError:\n        raise WebRequestError(f\"Invalid JSON response from {url}\")\n    except RequestException as e:\n        raise WebRequestError(f\"Error making request to {url}: {str(e)}\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Documents with LlamaIndex\nDESCRIPTION: Loads Alan Turing's papers from a directory using SimpleDirectoryReader, then builds a vector index for semantic search. Creates a retriever object to fetch relevant content based on user queries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/rag-llm-example.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Load documents and build index\ndocuments = SimpleDirectoryReader(\"./alan_turing_papers\").load_data()\nretriever = VectorStoreIndex.from_documents(documents).as_retriever()\n```\n\n----------------------------------------\n\nTITLE: Text Classification Prompt\nDESCRIPTION: This example demonstrates text classification, where the LLM categorizes text into predefined classes (spam or not spam). It is useful for tasks such as sentiment analysis and spam detection.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Classify the following email text as either spam or not spam: \\\"I can make you $1000 \\nin just an hour. Interested?\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Search Results with Sources\nDESCRIPTION: This snippet defines a Pydantic model for structuring search results and sources obtained from the LLM call, ensuring clear data representation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/search_with_sources.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass SearchResponse(BaseModel):\n    sources: list[str] = Field(description=\"The sources of the results\")\n    answer: str = Field(description=\"The answer to the question\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=list[SearchResponse])\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Extract the question, results, and sources to answer the question based on the results.\n\n    Results:\n    {results}\n\n    USER:\n    {question}\n    \"\"\"\n)\ndef extract(question: str, results: str): ...\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI Provider\nDESCRIPTION: Shows how to install the Mirascope library with OpenAI provider support and set up the necessary API key as an environment variable.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"mirascope[openai]\"\nexport OPENAI_API_KEY=XXXXX\n```\n\n----------------------------------------\n\nTITLE: Async Streaming Implementation\nDESCRIPTION: Demonstrates async streaming functionality using async for loops.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/async.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python hl_lines=\"6 8 12-13\"\n--8<-- \"build/snippets/learn/async/streams/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Call with Mirascope OpenAI Integration\nDESCRIPTION: Demonstrates a simple LLM call using Mirascope's OpenAI integration with call_params to plan a travel itinerary. Shows how to set model parameters and handle responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o-mini\", call_params={\"temperature\": 0.6})\ndef plan_travel_itinerary(destination: str) -> str:\n    return f\"Plan a travel itinerary that includes activities in {destination}\"\n\n\nresponse = plan_travel_itinerary(\"Paris\")\nprint(response.content)  # prints the string content of the call\n```\n\n----------------------------------------\n\nTITLE: Listing and Using MCP Server Prompts\nDESCRIPTION: Example of listing available prompts and accessing prompt templates from the MCP server.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/mcp/client.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/mcp/client.py:19:25\"\n```\n\n----------------------------------------\n\nTITLE: Transcribing Speech from URL using Gemini - Python\nDESCRIPTION: This code defines a function to transcribe speech from a given audio URL using the Gemini API model, and prints the transcription response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/speech_transcription.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom google.generativeai import configure\nfrom mirascope.core import gemini, prompt_template\n\nconfigure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n\napollo_url = \"https://storage.googleapis.com/generativeai-downloads/data/Apollo-11_Day-01-Highlights-10s.mp3\"\n\n@gemini.call(model=\"gemini-1.5-flash\")\n@prompt_template(\n    \"\"\"\n    Transcribe the content of this speech:\n    {url:audio}\n    \"\"\"\n)\ndef transcribe_speech_from_url(url: str): ...\n\nresponse = transcribe_speech_from_url(apollo_url)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Importing Web Search Libraries\nDESCRIPTION: Importing necessary modules for web search functionality including DuckDuckGo search API, BeautifulSoup, and requests.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\nfrom duckduckgo_search import DDGS\nfrom pydantic import BaseModel, Field\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Streaming with Azure AI\nDESCRIPTION: This Python snippet demonstrates error handling during streaming with Azure AI. It shows how to wrap the streaming loop in a try/except block to catch any exceptions that might occur during the stream's iteration, highlighting the importance of handling errors during the actual streaming process rather than just the initial function call. This example requires the Azure AI client library.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    for chunk in completion.stream():\n        print(chunk.text, end=\"\")\nexcept Exception as e:\n    print(f\"\\nError during streaming: {e}\")\n\n```\n\n----------------------------------------\n\nTITLE: Example Usage of FileSystemToolKit in Python\nDESCRIPTION: This snippet illustrates how to use the FileSystemToolKit with a flexible example structure, showing how different methods of the toolkit can be iterated over and applied based on user selection.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n{% for method, method_title in zip(prompt_writing_methods, prompt_writing_method_titles) %}\n=== \"{{ method_title }}\"\n\n    {% for provider in supported_llm_providers %}\n    === \"{{ provider }}\"\n\n        {% if method == \"string_template\" %}\n        ```python hl_lines=\"4 10 12\"\n        {% elif method == \"base_message_param\" %}\n        ```python hl_lines=\"4 9 17\"\n        {% else %}\n        ```python hl_lines=\"4 9 16\"\n        {% endif %}\n        --8<-- \"build/snippets/learn/tools/pre_made_toolkit/{{ provider | provider_dir }}/{{ method }}.py\"\n        ```\n    {% endfor %}\n\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Getting User Query and Fetching Response\nDESCRIPTION: This snippet captures user input and utilizes the `ask_apollo_astronaut` function to fetch a simulated response from an Apollo 11 astronaut's perspective based on the user's query. It requires the user to input a question, and the corresponding answer is printed. The `ask_apollo_astronaut` function is expected to handle the retrieval of journal excerpts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/rag-application.md#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nquery = input(\"(User): What were your first impressions when you stepped onto the surface of the Moon?\")\nresponse = ask_apollo_astronaut(query, retriever)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Text Interaction with Tools\nDESCRIPTION: Example demonstrating text-based interaction with tool integration for OpenAI's Realtime API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/openai.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/openai/realtime/text_input_output_tools.py\"\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Mode in Python with Mirascope v0 and v1\nDESCRIPTION: This snippet illustrates how to implement JSON mode using Mirascope v0 and v1. It shows the transition from using OpenAICallParams to specifying json_mode in the decorator, simplifying the process of forcing JSON output.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICallParams, OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: type[Book] = Book\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\n    call_params = OpenAICallParams(response_format={\"type\": \"json_object\"})\n\n\nextractor = BookExtractor(genre=\"fantasy\")\nbook = extractor.extract()\nprint(book)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nbook = recommend_book(\"fantasy\")\nprint(book)\n```\n\n----------------------------------------\n\nTITLE: Creating a Call Factory for the Custom Provider\nDESCRIPTION: This snippet creates a custom_provider_call decorator using call_factory, integrating the previously defined classes and utility functions to manage API interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/custom_provider.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core.base import call_factory\n\ncustom_provider_call = call_factory(\n    TCallResponse=CustomProviderCallResponse,\n    TCallResponseChunk=CustomProviderCallResponseChunk,\n    TDynamicConfig=CustomProviderDynamicConfig,\n    TStream=CustomProviderStream,\n    TToolType=CustomProviderTool,\n    TCallParams=CustomProviderCallParams,\n    default_call_params=CustomProviderCallParams(),\n    setup_call=setup_call,\n    get_json_output=get_json_output,\n    handle_stream=handle_stream,\n    handle_stream_async=handle_stream_async,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Provider-Specific LLM Calls with Mirascope\nDESCRIPTION: Shows how to implement provider-specific LLM calls using Mirascope decorators. The code demonstrates creating separate functions for OpenAI and Anthropic that use the same prompt pattern but target different models.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/WHY.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef openai_recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef anthropic_recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Outputs in Python with Mirascope\nDESCRIPTION: Shows how to generate a list of books using Mirascope by setting the `response_model` to a list of `Book` instances, allowing for multiple structured outputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/structured_outputs.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\", response_model=list[Book])\ndef recommend_books(genre: str, num: int) -> str:\n    return f\"Recommend a list of {num} {genre} books\"\n\n\nbooks = recommend_books(\"fantasy\", 3)\nfor book in books:\n    print(book)\n```\n\n----------------------------------------\n\nTITLE: Parsing LLM Output with Regular Expressions\nDESCRIPTION: Shows how to use regular expressions to extract structured data from LLM output. The highlighted lines demonstrate the regex pattern definition, function call, and pattern matching logic.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/output_parsers.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/output_parsers/additional_examples/{{ provider | provider_dir }}/regex.py\"\n```\n\n----------------------------------------\n\nTITLE: Testing Nested NER System with Pytest in Python\nDESCRIPTION: A comprehensive test function for a nested Named Entity Recognition system using pytest parametrization. The test validates if the nested_ner function correctly identifies hierarchical relationships between entities such as organizations, people, and locations from unstructured text.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/named_entity_recognition.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport ipytest  # noqa: E402\nimport pytest  # noqa: E402\n\nipytest.autoconfig()\n\n\ntest_cases = [\n    (\n        \"\"\"\n    The multinational conglomerate Alphabet Inc., parent company of Google, has acquired \n    DeepMind, a leading AI research laboratory based in London. DeepMind's founder, \n    Demis Hassabis, will join Google Brain, a division of Google AI, as Chief AI Scientist. \n    This move strengthens Alphabet's position in the AI field, challenging competitors like \n    OpenAI, which is backed by Microsoft, and Facebook AI Research, a part of Meta Platforms Inc.\n        \"\"\",\n        [\n            NestedEntity(\n                entity=\"Alphabet Inc.\",\n                label=\"ORGANIZATION\",\n                parent=None,\n                children=[\n                    NestedEntity(\n                        entity=\"Google\",\n                        label=\"ORGANIZATION\",\n                        parent=\"Alphabet Inc.\",\n                        children=[\n                            NestedEntity(\n                                entity=\"Google Brain\",\n                                label=\"ORGANIZATION\",\n                                parent=\"Google\",\n                                children=[],\n                            ),\n                            NestedEntity(\n                                entity=\"Google AI\",\n                                label=\"ORGANIZATION\",\n                                parent=\"Google\",\n                                children=[\n                                    NestedEntity(\n                                        entity=\"Google Brain\",\n                                        label=\"ORGANIZATION\",\n                                        parent=\"Google AI\",\n                                        children=[],\n                                    )\n                                ],\n                            ),\n                        ],\n                    ),\n                    NestedEntity(\n                        entity=\"DeepMind\",\n                        label=\"ORGANIZATION\",\n                        parent=\"Alphabet Inc.\",\n                        children=[\n                            NestedEntity(\n                                entity=\"Demis Hassabis\",\n                                label=\"PERSON\",\n                                parent=\"DeepMind\",\n                                children=[],\n                            )\n                        ],\n                    ),\n                ],\n            ),\n            NestedEntity(entity=\"London\", label=\"LOCATION\", parent=None, children=[]),\n            NestedEntity(\n                entity=\"Demis Hassabis\", label=\"PERSON\", parent=None, children=[]\n            ),\n            NestedEntity(\n                entity=\"OpenAI\",\n                label=\"ORGANIZATION\",\n                parent=None,\n                children=[\n                    NestedEntity(\n                        entity=\"Microsoft\",\n                        label=\"ORGANIZATION\",\n                        parent=\"OpenAI\",\n                        children=[],\n                    )\n                ],\n            ),\n            NestedEntity(\n                entity=\"Facebook AI Research\",\n                label=\"ORGANIZATION\",\n                parent=None,\n                children=[\n                    NestedEntity(\n                        entity=\"Meta Platforms Inc.\",\n                        label=\"ORGANIZATION\",\n                        parent=\"Facebook AI Research\",\n                        children=[],\n                    )\n                ],\n            ),\n            NestedEntity(\n                entity=\"Meta Platforms Inc.\",\n                label=\"ORGANIZATION\",\n                parent=None,\n                children=[],\n            ),\n            NestedEntity(\n                entity=\"Microsoft\", label=\"ORGANIZATION\", parent=None, children=[]\n            ),\n        ],\n    ),\n]\n\n\n@pytest.mark.parametrize(\"text,expected_output\", test_cases)\ndef test_nested_ner(text: str, expected_output: list[NestedEntity]):\n    output = nested_ner(text)\n    assert len(output) == len(expected_output)\n    for entity, expected_entity in zip(output, expected_output, strict=False):\n        assert entity.model_dump() == expected_entity.model_dump()\n\n\nipytest.run()  # Run the tests in Jupyter Notebook\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Streaming with Mistral\nDESCRIPTION: This Python snippet demonstrates error handling during streaming with Mistral. It shows how to wrap the streaming loop in a try/except block to catch any exceptions that might occur during the stream's iteration, highlighting the importance of handling errors during the actual streaming process rather than just the initial function call. This example requires the Mistral client library.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mistralai.exceptions import MistralException\n\ntry:\n    for chunk in response.get_stream():\n        for event in chunk:\n            if event.event == \"message\":\n                print(event.data.choices[0].delta.content, end=\"\")\n            elif event.event == \"error\":\n                raise MistralException(event.data.content)\nexcept MistralException as e:\n    print(f\"\\nError during streaming: {e}\")\n\n```\n\n----------------------------------------\n\nTITLE: Example Usage of the Web Agent - Python\nDESCRIPTION: This code provides an example usage of the web agent where a question is processed through the main run function. It demonstrates how to call the run function with a specific query and prints the response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nif __name__ == \"__main__\":\n    print(\"Example usage:\")\n    response = run(\"what is the latest on donald trump and elon musk?\")\n    print(response)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining a simple LLM call using Mirascope\nDESCRIPTION: This code defines a basic LLM call using Mirascope's `openai.call` decorator.  It takes a query string as input and returns the LLM's response as a string, using the 'gpt-4o-mini' model.  It serves as a foundational call for subsequent steps in the Chain of Verification process.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/chain_of_verification.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\")\ndef call(query: str) -> str:\n    return query\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Functions for String Manipulation\nDESCRIPTION: This snippet defines three tool functions: `split_string_to_words`, `substring`, and `concat`. These functions provide basic string manipulation capabilities that can be used by the LLM to solve subproblems. `split_string_to_words` splits a string into a list of words, `substring` retrieves a character at a specific index, and `concat` concatenates a list of strings into a single string.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/decomposed_prompting.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"def split_string_to_words(string: str) -> str:\n    \\\"\\\"\\\"Splits a string into words.\\\"\\\"\\\"\n    return json.dumps(string.split())\n\n\ndef substring(index: int, string: str) -> str:\n    \\\"\\\"\\\"Gets the character at the index of a string.\\\"\\\"\\\"\n    return string[index]\n\n\ndef concat(strings: list[str]) -> str:\n    \\\"\\\"\\\"Concatenates some number of strings.\\\"\\\"\\\"\n    return \\\"\\\".join(strings)\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Chain with LCEL\nDESCRIPTION: A basic example of LangChain's expression language (LCEL) using the pipe operator to sequence operations through a chain of components.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-chaining.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nchain = prompt | model | output_parser \n```\n\n----------------------------------------\n\nTITLE: Custom Output Parsing in Mirascope with Decorators\nDESCRIPTION: Demonstrates Mirascope's approach to output parsing using Python functions with call decorators and Pydantic models for structured data handling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-structured-output.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Movie(BaseModel):\n    title: str\n    director: str\n\n\ndef parse_movie_recommendation(response: anthropic.AnthropicCallResponse) -> Movie:\n    title, director = response.content.split(\" directed by \")\n    return Movie(title=title, director=director)\n\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\", output_parser=parse_movie_recommendation\n)\n@prompt_template(\"Recommend a {genre} movie in the format Title directed by Director\")\ndef recommend_movie(genre: str):\n    ...\n\n\nmovie = recommend_movie(\"thriller\")\nprint(f\"Title: {movie.title}\")\nprint(f\"Director: {movie.director}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Mirascope Modules\nDESCRIPTION: Importing necessary classes and decorators from Mirascope and Pydantic for message handling and validation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Knowledge Graph Query Processing\nDESCRIPTION: Illustrates how to query a knowledge graph about renewable energy challenges using a structured role-based prompt system.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-applications.md#2025-04-21_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nSYSTEM:\nAnswer the following question based on the knowledge graph.\n\nKnowledge graph:\n{knowledge_graph}\n\nUSER:\nWhat are the challenges of renewable energy adoption?\n```\n\n----------------------------------------\n\nTITLE: Implementing Azure OpenAI Call Class in Python\nDESCRIPTION: Defines the Call class for Azure OpenAI API integration with methods for handling API authentication, endpoint construction, and chat completions. This class supports Azure-specific requirements including deployment-based endpoints and API key authentication.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/azure/call.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import Any, AsyncGenerator, Dict, List, Optional, Tuple, Type, Union, cast\n\nfrom openai.types.chat import (ChatCompletion, ChatCompletionChunk,\n                               ChatCompletionMessage)\nfrom openai.types.chat.chat_completion import Choice\nfrom openai.types.chat.chat_completion_chunk import Choice as ChunkChoice\nfrom openai.types.chat.chat_completion_chunk import ChoiceDelta\nfrom openai.types.chat.chat_completion_message import ChatCompletionMessage\nfrom openai.types.chat.chat_completion_message_param import \\\n    ChatCompletionMessageParam\n\nfrom ..._internal.client_api.retry import RetryConfig\nfrom ...client import MirascopeClient\nfrom ...exceptions import ConversationError\nfrom ...types.client import (\n    BaseUserContext,\n    MessageContext,\n)\nfrom ..openai.call import Call as OpenAICall\nfrom ..openai.utils.parsing import build_messages\nfrom .params import AzureChatCompletionParamsDict, TContent\n\n\nclass Call(OpenAICall):\n    \"\"\"Wrapper for OpenAI chat calls.\"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        api_key: Optional[str] = None,\n        endpoint: Optional[str] = None,\n        api_version: Optional[str] = None,\n        organization_id: Optional[str] = None,\n        retry_config: Optional[RetryConfig] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"\n        Initialize the call.\n\n        Args:\n            model: The ID of the model to use.\n            api_key: The API key to use.\n            endpoint: The endpoint to use.\n            api_version: The API version to use.\n            organization_id: The ID of the organization to use.\n            retry_config: The retry configuration to use.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(model, api_key, organization_id, retry_config, **kwargs)\n        self.endpoint = endpoint\n        self.api_version = api_version\n\n    def _parse_azure_openai_endpoint(\n        self, endpoint: str, model: str\n    ) -> Tuple[str, str]:\n        \"\"\"\n        Parse the Azure OpenAI endpoint.\n\n        Args:\n            endpoint: The endpoint to parse.\n            model: The model to deploy.\n\n        Returns:\n            The parsed endpoint and deployment name.\n        \"\"\"\n        deployment = model\n        if \".openai.azure.com\" not in endpoint:\n            endpoint = f\"https://{endpoint}.openai.azure.com\"\n        return endpoint, deployment\n\n    def _build_client(\n        self, api_key: Optional[str] = None, timeout: Optional[float] = None\n    ) -> Tuple[MirascopeClient, Dict[str, Any]]:\n        \"\"\"\n        Build the Azure OpenAI client.\n\n        Args:\n            api_key: The API key to use.\n            timeout: The timeout to use.\n\n        Returns:\n            The client instance and the parameters to use for calls.\n        \"\"\"\n        from openai import AzureOpenAI\n\n        endpoint = self.endpoint or self.params.pop(\"endpoint\", None)\n        if not endpoint:\n            raise ValueError(\"An endpoint must be provided.\")\n\n        api_version = self.api_version or self.params.pop(\"api_version\", None)\n        if not api_version:\n            raise ValueError(\"An API version must be provided.\")\n\n        endpoint, deployment_name = self._parse_azure_openai_endpoint(endpoint, self.model)\n\n        api_key = api_key or self.api_key\n        if not api_key:\n            raise ValueError(\"An API key must be provided.\")\n\n        client = AzureOpenAI(\n            api_key=api_key, azure_endpoint=endpoint, api_version=api_version\n        )\n        client = MirascopeClient.from_client(client, self.retry_config)\n\n        params = {\n            **self.params,\n            \"deployment_name\": deployment_name,\n        }\n\n        if timeout is not None:\n            params[\"timeout\"] = timeout\n\n        return client, params\n\n    async def achat_with_context(\n        self,\n        *,\n        content: TContent,\n        messages_context: Optional[MessageContext] = None,\n        user_context: Optional[BaseUserContext] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        seed: Optional[int] = None,\n        response_format: Optional[Dict[str, str]] = None,\n        tools: Optional[List[Dict[str, Any]]] = None,\n        tool_choice: Optional[Union[str, Dict[str, str]]] = None,\n        stream: bool = False,\n        timeout: Optional[float] = None,\n        **kwargs: Any,\n    ) -> Union[ChatCompletion, AsyncGenerator[ChatCompletionChunk, None]]:\n        \"\"\"\n        Call Azure OpenAI with context.\n\n        Args:\n            content: The message(s) to use.\n            messages_context: Prior conversation messages for the context.\n            user_context: User context information.\n            temperature: The randomness of the generation.\n            top_p: The nucleus sampling parameter.\n            seed: The seed to use.\n            response_format: The output format.\n            tools: The tools to be used by the model.\n            tool_choice: The choice of tool to use.\n            stream: Whether to stream the response.\n            timeout: The timeout to use for the request.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            The chat completion or a stream of chat completions.\n        \"\"\"\n        messages = build_messages(content, messages_context)\n\n        client, params = self._build_client(timeout=timeout)\n\n        params_dict: AzureChatCompletionParamsDict = {}\n        for key, value in self.params.items():\n            params_dict[key] = value\n\n        # Override with provided values\n        if temperature is not None:\n            params_dict[\"temperature\"] = temperature\n        if top_p is not None:\n            params_dict[\"top_p\"] = top_p\n        if seed is not None:\n            params_dict[\"seed\"] = seed\n        if response_format is not None:\n            params_dict[\"response_format\"] = response_format\n        if tools is not None:\n            params_dict[\"tools\"] = tools\n        if tool_choice is not None:\n            params_dict[\"tool_choice\"] = tool_choice\n\n        # Add user to the params if provided\n        if user_context is not None and user_context.user:\n            params_dict[\"user\"] = user_context.user\n\n        # Add any additional kwargs\n        for key, value in kwargs.items():\n            params_dict[key] = value\n\n        try:\n            if stream:\n                return client.achat.completions.create(\n                    messages=cast(List[ChatCompletionMessageParam], messages),\n                    stream=True,\n                    **params_dict,\n                )\n            return await client.achat.completions.create(\n                messages=cast(List[ChatCompletionMessageParam], messages),\n                **params_dict,\n            )\n        except Exception as e:\n            print(messages)\n            raise ConversationError(str(e)) from e\n```\n\n----------------------------------------\n\nTITLE: Mirascope Prompt Template Implementation\nDESCRIPTION: Demonstrates Mirascope's prompt_template decorator for creating dynamic prompt templates as Python functions with computed fields and automatic validation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseDynamicConfig, prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    I've recently visited the following places: {places_in_quotes}.\n    Where should I travel to next?\n    \"\"\"\n)\ndef travel_recommendation_prompt(visited_plates: list[str]) -> BaseDynamicConfig:\n    places_in_quotes = \", \".join([f'\"{place}\"' for place in visited_places])\n    return {\"computed_fields\": {\"places_in_quotes\": places_in_quotes}}\n\n\nvisited_places = [\"New York\", \"London\", \"Hawaii\"]\nmessages = travel_recommendation_prompt(visited_plates)\nprint(messages)\n# > [BaseMessageParam(role='user', content='...')]\nprint(messages[0].content)\n# > I've recently visited the following places: \"New York\", \"London\", \"Hawaii\"\n#   Where should I travel to next?\n```\n\n----------------------------------------\n\nTITLE: Nested Chaining of LLM Calls - Python\nDESCRIPTION: This snippet showcases nested chaining where a single function encompasses calls to multiple LLM operations, simplifying the calling mechanism and returning a combined result.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -> str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -> str:\n    summary = summarize(original_text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\noriginal_text = \"Long English text here...\"\ntranslation = summarize_and_translate(original_text, \"french\")\nprint(translation.content)\n```\n\n----------------------------------------\n\nTITLE: Printing and Processing Evaluation Results\nDESCRIPTION: This snippet shows how to iterate through evaluation results and display them. Each evaluation contains a reasoning field and a numerical score from different LLM providers, structured according to the Eval response model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-as-judge.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor evaluation in evaluations:\n    print(evaluation)\n# Output \n# OpenAI:    reasoning='The text is constructive and provides actionable advice for productivity.' score=4.0\n# Anthropic: reasoning='Clear and actionable recommendation with practical application.' score=4.0\n```\n\n----------------------------------------\n\nTITLE: Basic Async Usage with highlighted lines\nDESCRIPTION: Demonstrates basic async function definition and usage with await keyword.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/async.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python hl_lines=\"8 12\"\n--8<-- \"build/snippets/learn/async/basic_usage/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Prompt Paraphrasing Function\nDESCRIPTION: This snippet implements the 'prompt_paraphrasing' function which utilizes translation to generate a set of diverse prompts by translating the input query into Spanish and then back to English.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/prompt_paraphrasing.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef prompt_paraphrasing(query: str, num_translations: int = 3) -> set[str]:\\n    spanish_translations = translate(\\n        phrase=query,\\n        language=\"Spanish\",\\n        num_translations=num_translations,\\n    )\\n    # Avoid Duplicates\\n    prompt_variations = set()\\n    for spanish_phrase in spanish_translations.translations:\\n        back_translations = translate(\\n            spanish_phrase, language=\"English\", num_translations=3\\n        )\\n        prompt_variations.update(back_translations.translations)\\n    return prompt_variations\\n\\n\\nprint(\\n    prompt_paraphrasing(\\n        query=\"What are some manageable ways to improve my focus and productivity?\"\\n    )\\n)\n```\n\n----------------------------------------\n\nTITLE: Example LLM API Response in JSON\nDESCRIPTION: This JSON snippet is an example of a typical response from an LLM API. It includes metadata such as the ID of the request, the model used, the creation timestamp, the generated message with role and content, token usage statistics, and other service-related information.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-integration.md#2025-04-21_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"chatcmpl-Awokg9ThvjU2l09zdCbu5gPAhJ07s\",\n    \"object\": \"chat.completion\",\n    \"created\": 1738581478,\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"The capital of France is Paris.\",\n                \"refusal\": null\n            },\n            \"logprobs\": null,\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 24,\n        \"completion_tokens\": 8,\n        \"total_tokens\": 32,\n        \"prompt_tokens_details\": {\n            \"cached_tokens\": 0,\n            \"audio_tokens\": 0\n        },\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0,\n            \"audio_tokens\": 0,\n            \"accepted_prediction_tokens\": 0,\n            \"rejected_prediction_tokens\": 0\n        }\n    },\n    \"service_tier\": \"default\",\n    \"system_fingerprint\": \"fp_72ed7ab54c\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponse Class for LiteLLM API Responses in Python\nDESCRIPTION: This snippet defines the CallResponse class, which encapsulates the response from a LiteLLM API call. It includes methods to extract content, tokens, and other metadata from the response.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/litellm/call_response.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Optional\n\nfrom mirascope.core.models import Tokens\n\n\nclass CallResponse:\n    def __init__(self, response: Dict[str, Any]):\n        self.response = response\n\n    def content(self) -> str:\n        return self.response[\"choices\"][0][\"message\"][\"content\"]\n\n    def tokens(self) -> Tokens:\n        return Tokens(\n            prompt=self.response[\"usage\"][\"prompt_tokens\"],\n            completion=self.response[\"usage\"][\"completion_tokens\"],\n            total=self.response[\"usage\"][\"total_tokens\"],\n        )\n\n    def function_call(self) -> Optional[Dict[str, Any]]:\n        return self.response[\"choices\"][0][\"message\"].get(\"function_call\")\n\n    def model(self) -> str:\n        return self.response[\"model\"]\n\n    def finish_reason(self) -> str:\n        return self.response[\"choices\"][0][\"finish_reason\"]\n\n    def choices(self) -> List[Dict[str, Any]]:\n        return self.response[\"choices\"]\n\n    def id(self) -> str:\n        return self.response[\"id\"]\n\n    def created(self) -> int:\n        return self.response[\"created\"]\n\n    def object(self) -> str:\n        return self.response[\"object\"]\n\n    def system_fingerprint(self) -> Optional[str]:\n        return self.response.get(\"system_fingerprint\")\n\n    def raw(self) -> Dict[str, Any]:\n        return self.response\n```\n\n----------------------------------------\n\nTITLE: Streaming Response Models with Bedrock in Python\nDESCRIPTION: This snippet demonstrates how to use streaming response models with Bedrock in Mirascope. It shows the setup of a response model and how to stream the response, highlighting the stream=True parameter and the iteration over the stream.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/response_models.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import BaseModel, Field, Prompt\nfrom mirascope.providers import Bedrock\n\nclass Response(BaseModel):\n    summary: str = Field(description=\"A summary of the text\")\n\nprompt = Prompt(\n    \"Summarize this text: {text}\",\n    response_model=Response,\n)\n\nresponse = prompt.openai(\n    model=\"anthropic.claude-v2\",\n    provider=Bedrock(),\n    stream=True,\n    text=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n)\n\nfor partial_response in response:\n    print(partial_response.summary)\n\nfinal_response = response.constructed_response_model\nprint(f\"Final summary: {final_response.summary}\")\n```\n\n----------------------------------------\n\nTITLE: LangChain ChatPromptTemplate invocation\nDESCRIPTION: This snippet demonstrates how to use LangChain's `ChatPromptTemplate` to create a prompt, invoke a ChatOpenAI model, and parse the output. It highlights the potential for runtime errors due to the lack of editor support for variable names in the `invoke` dictionary.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_template(\"tell me a fun fact about {topic}\")\nmodel = ChatOpenAI(model=\"gpt-4\")\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nchain.invoke({\"topic\": \"pandas\"})\n```\n\n----------------------------------------\n\nTITLE: Testing Python Syntax Validation\nDESCRIPTION: This code snippet utilizes `pytest.mark.parametrize` to define test cases for the `check_syntax` function. It provides a list of Python code snippets and their expected syntax validation results. The `test_check_syntax` function then asserts that the output of `check_syntax` aligns with the expected boolean value.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize(\n    \"syntax,expected\",\n    [(\"print('Hello, World!')\", True), (\"print('Hello, World!'\", False)],\n)\ndef test_check_syntax(syntax: str, expected: bool):\n    assert check_syntax(syntax) == expected\n```\n\n----------------------------------------\n\nTITLE: Prompt Variations Model\nDESCRIPTION: Defines a Pydantic model `PromptVariations` to structure the output of prompt variation generation. It includes a list of strings representing variations of the original prompt and uses Pydantic's `Field` to add a description to the attribute.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/diverse.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\nfrom pydantic import BaseModel, Field\n\n\nclass PromptVariations(BaseModel):\n    variations: list[str] = Field(..., description=\"Variations of the original prompt\")\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Rephrasing and Expanding Questions for Enhanced Model Responses\nDESCRIPTION: This snippet demonstrates how to implement the Rephrase and Respond (RaR) prompting technique, allowing a language model to clarify a vague query and analyze the process step by step before providing a detailed answer.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nA coin is heads up. Aluino flips the coin. Arthur flips the coin. Is the coin still heads up? Flip means reverse. Rephrase and expand the question, and respond\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nA coin starts with the heads side facing up. If Aluino flips the coin, it will land with the tails side facing up. Then Arthur flips the coin again. After these two sequences of flips, can we say that the coin is still heads up?\n\nTo analyze the scenario, let’s break down the actions step by step:\n1. Initial State: The coin starts with the heads side facing up.\n2. Aluino Flips the Coin: When Aluino flips the coin, it reverses its position. Since the coin initially was heads up, after Aluino’s flip, the coin will now be tails up.\n3. Arthur Flips the Coin: Next, Arthur takes his turn to flip the coin. Given that the current state of the coin is tails up, flipping it will reverse it again, resulting in the coin now being heads up.\nAt the end of these actions, after both Aluino and Arthur have flipped the coin, the final state of the coin is heads up once more.\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for Named Entity Recognition in Python\nDESCRIPTION: This snippet demonstrates how to install necessary Python packages, including 'mirascope[groq]' and 'ipytest', for executing Named Entity Recognition using Groq's models. It also includes setting up the API key for authentication.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/named_entity_recognition.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[groq]\" pytest\n!pip install ipytest # For running pytest in Jupyter Notebooks\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Mirascope Function Chaining with Dynamic Configuration\nDESCRIPTION: Illustrates Mirascope's approach to chaining functions using dynamic configuration and computed fields for creating context-aware recipe recommendations\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseDynamicConfig, openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Name a chef who excels at making {food_type} food.\n    Return only their name in First, Last format.\n    \"\"\"\n)\ndef select_chef(food_type: str): ...\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM: Imagine you are {chef} and recommend a recipe they would love.\n    USER: Recommend a {food_type} recipe using {ingredient}.\n    \"\"\"\n)\ndef recommend_recipe(food_type: str, ingredient: str) -> BaseDynamicConfig:\n    chef = select_chef(food_type)\n    return {\"computed_fields\": {\"chef\": chef}}\n\n\nrecipe = recommend_recipe(food_type=\"dessert\", ingredient=\"apples\")\nprint(recipe.content)\n# > Certainly! As a renowned chef, I'd love to share a delightful recipe...\nprint(recipe.dynamic_config[\"computed_fields\"][\"chef\"])\n# > Pierre Hermé\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Mirascope\nDESCRIPTION: This code sets the OpenAI API key as an environment variable for use with Mirascope. It's a crucial step for authentication with the OpenAI service.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Optimizing LLM Calls with Function Caching\nDESCRIPTION: Demonstrates using @functools.lru_cache to cache LLM function call results, reducing redundant API calls and improving performance across a chain of function calls\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import lru_cache\\n\\n\\nfrom mirascope.core import openai, prompt_template\\n\\n\\n@lru_cache()\\n@openai.call(\"gpt-4o-mini\")\\ndef select_painter(art_style: str) -> str:\\n    return f\"Name a painter known for {art_style}. Return only their name.\"\\n\\n\\n@lru_cache()\\n@openai.call(\"gpt-4o-mini\")\\ndef select_artwork(painter: str) -> str:\\n    return f\"Name an art piece created by {painter}. Return only the name.\"\\n\\n\\n@lru_cache()\\n@openai.call(\"gpt-4o-mini\")\\n@prompt_template(\\n    \"\"\"\\n    SYSTEM:\\n    Imagine that you are {painter}.\\n    Your task is to describe a famous painting that you, {painter}, created.\\n\\n    USER:\\n    Describe {artwork}.\\n    \"\"\"\\n)\\ndef describe_artwork(art_style: str) -> openai.OpenAIDynamicConfig:\\n    painter = select_painter(art_style)\\n    artwork = select_artwork(painter.content)\\n    return {\"computed_fields\": {\"painter\": painter, \"artwork\": artwork}}\n```\n\n----------------------------------------\n\nTITLE: Implementing FewShotPromptTemplate for Question-Answering in Python\nDESCRIPTION: Shows how to use FewShotPromptTemplate for few-shot learning. It includes example questions and answers, and demonstrates formatting a new question using the template.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\n\nexamples = [\n    {\n        \"question\": \"What is the tallest mountain in the world?\",\n        \"answer\": \"Mount Everest\",\n    },\n    {\"question\": \"What is the largest ocean on Earth?\", \"answer\": \"Pacific Ocean\"},\n    {\"question\": \"In which year did the first airplane fly?\", \"answer\": \"1903\"},\n]\n\nexample_prompt = PromptTemplate(\n    input_variables=[\"question\", \"answer\"],\n    template=\"Question: {question}\\n{answer}\",\n)\nprompt_template = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    suffix=\"Question: {input}\",\n    input_variables=[\"input\"],\n)\n\nprint(\n    prompt_template.format(\n        input=\"What is the name of the famous clock tower in London?\"\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Nested Chains in Mirascope\nDESCRIPTION: Demonstrates how to create nested chains by calling a subchain inside the function body of a parent chain. This approach provides better observability when using tracing tools.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/chaining.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/chaining/nested_chains/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Defining CallParams Class for OpenAI API Calls in Python\nDESCRIPTION: This class defines the parameters used for making API calls to OpenAI's language models. It includes attributes for specifying the model, response format, temperature, and other API-specific options. The class uses Pydantic for data validation and serialization.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/openai/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Any, Dict, List, Literal, Optional, Union\n\nfrom pydantic import BaseModel, Field\n\n\nclass CallParams(BaseModel):\n    \"\"\"Parameters for making API calls to OpenAI's language models.\"\"\"\n\n    model: str = Field(..., description=\"The model to use for the API call.\")\n    messages: List[Dict[str, str]] = Field(\n        ..., description=\"The messages to send to the model.\"\n    )\n    functions: Optional[List[Dict[str, Any]]] = Field(\n        None, description=\"The functions to make available to the model.\"\n    )\n    function_call: Optional[Union[str, Dict[str, Any]]] = Field(\n        None, description=\"The function to call, if any.\"\n    )\n    temperature: Optional[float] = Field(\n        None,\n        description=\"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\",\n    )\n    top_p: Optional[float] = Field(\n        None,\n        description=\"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\",\n    )\n    n: Optional[int] = Field(\n        None,\n        description=\"How many chat completion choices to generate for each input message.\",\n    )\n    stream: Optional[bool] = Field(\n        None,\n        description=\"If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.\",\n    )\n    stop: Optional[Union[str, List[str]]] = Field(\n        None,\n        description=\"Up to 4 sequences where the API will stop generating further tokens.\",\n    )\n    max_tokens: Optional[int] = Field(\n        None,\n        description=\"The maximum number of tokens to generate in the chat completion.\",\n    )\n    presence_penalty: Optional[float] = Field(\n        None,\n        description=\"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\",\n    )\n    frequency_penalty: Optional[float] = Field(\n        None,\n        description=\"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\",\n    )\n    logit_bias: Optional[Dict[str, float]] = Field(\n        None,\n        description=\"Modify the likelihood of specified tokens appearing in the completion.\",\n    )\n    user: Optional[str] = Field(\n        None,\n        description=\"A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.\",\n    )\n    response_format: Optional[Dict[Literal[\"type\"], Literal[\"json_object\"]]] = Field(\n        None,\n        description=\"An object specifying the format that the model must output. Setting to { \"type\": \"json_object\" } enables JSON mode, which guarantees the message the model generates is valid JSON.\",\n    )\n\n    class Config:\n        extra = \"forbid\"\n```\n\n----------------------------------------\n\nTITLE: Transcript-Based Ticket Handling Execution in Python\nDESCRIPTION: The snippet demonstrates the handling of support ticket transcripts ('billing', 'sale', and 'support') using pre-defined scenarios to test the 'handle_ticket()' processes through real-life examples.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/support_ticket_routing.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbilling_transcript = \"\"\"\nBOT: Please enter your email.\nCUSTOMER: johndoe@gmail.com\nBOT: What brings you here today?\nCUSTOMER: I purchased a TV a week ago but the charge is showing up twice on my bank \\\nstatement. Can I get a refund?\n\"\"\"\n\nsale_transcript = \"\"\"\nBOT: Please enter your email.\nCUSTOMER: johndoe@gmail.com\nBOT: What brings you here today?\nCUSTOMER: I'm looking to buy a new monitor. Any discounts available?\n\"\"\"\n\nsupport_transcript = \"\"\"\nBOT: Please enter your email.\nCUSTOMER: johndoe@gmail.com\nBOT: What brings you here today?\nCUSTOMER: I forgot my site password and I'm also locked out of my email, how else can I\nverify my identity?\n\"\"\"\n\nfor transcript in [billing_transcript, sale_transcript, support_transcript]:\n    response = handle_ticket(transcript)\n    if tool := response.tool:\n        tool.call()\n```\n\n----------------------------------------\n\nTITLE: Accessing Object Attributes in Prompt Templates Using Python\nDESCRIPTION: This snippet allows direct access to object attributes within template variables in Mirascope. It demonstrates how attributes can be injected directly into prompts with different methods.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n{% for method, method_title in zip(prompt_writing_methods, prompt_writing_method_titles) %}\n=== \"{{ method_title }}\"\n\n    {% if method == \"shorthand\" %}\n    ```python hl_lines=\"12 17\"\n    {% elif method == \"messages\" %}\n    ```python hl_lines=\"13 19\"\n    {% elif method == \"string_template\" %}\n    ```python hl_lines=\"10 16\"\n    {% else %}\n    ```python hl_lines=\"15 22\"\n    {% endif %}\n    --8<-- \"examples/learn/prompts/object_attribute_access/{{ method }}.py\"\n    ```\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Switching Between Model Providers with Mirascope (Anthropic)\nDESCRIPTION: This snippet shows how to switch from OpenAI to Anthropic's model using Mirascope's LLM call decorator. It demonstrates the ease of changing providers while maintaining the same function logic.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef suggest_movie(genre: str) -> str:\n    return f\"Suggest a {genre} movie\"\n\nresponse = suggest_movie(\"thriller\")\noriginal_response = response.response\n```\n\n----------------------------------------\n\nTITLE: Performing Web Searches with DuckDuckGo in Python\nDESCRIPTION: Within the WebSearchAgentBase class, this method `web_search` performs DuckDuckGo searches for a list of queries, returning URLs from the search results. It updates the search_history attribute of the agent. No API key is needed for DuckDuckGo, streamlining integration with stateful agents.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/tools_and_agents.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom duckduckgo_search import DDGS\n\n\nclass WebSearchAgentBase(BaseModel):\n    messages: list[openai.OpenAIMessageParam] = []\n    search_history: list[str] = []\n    max_results_per_query: int = 2\n\n    def web_search(self, queries: list[str]) -> str:\n        \"\"\"Performs web searches for given queries and returns URLs.\n\n        Args:\n            queries: List of search queries.\n\n        Returns:\n            str: Newline-separated URLs from search results or error messages.\n\n        Raises:\n            Exception: If web search fails entirely.\n        \"\"\"\n        try:\n            urls = []\n            for query in queries:\n                results = DDGS(proxies=None).text(\n                    query, max_results=self.max_results_per_query\n                )\n                for result in results:\n                    link = result[\"href\"]\n                    try:\n                        urls.append(link)\n                    except Exception as e:\n                        urls.append(\n                            f\"{type(e)}: Failed to parse content from URL {link}\"\n                        )\n            self.search_history.extend(queries)\n            return \"\\n\\n\".join(urls)\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponse Class in Python for Mirascope\nDESCRIPTION: This code defines the CallResponse class, which represents the result of a function call. It includes attributes for the return value, side effects, and methods for accessing and manipulating this data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/call_response.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom mirascope.core.base.side_effect import SideEffect\n\n\nclass CallResponse:\n    \"\"\"Represents the response from a function call.\"\"\"\n\n    def __init__(\n        self,\n        return_value: Any,\n        side_effects: Optional[Union[SideEffect, List[SideEffect]]] = None,\n    ) -> None:\n        \"\"\"Initialize a CallResponse.\n\n        Args:\n            return_value: The return value of the function call.\n            side_effects: Any side effects that occurred during the function call.\n        \"\"\"\n        self.return_value = return_value\n        self.side_effects = (\n            [side_effects] if isinstance(side_effects, SideEffect) else side_effects or []\n        )\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert the CallResponse to a dictionary.\n\n        Returns:\n            A dictionary representation of the CallResponse.\n        \"\"\"\n        return {\n            \"return_value\": self.return_value,\n            \"side_effects\": [se.to_dict() for se in self.side_effects],\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"CallResponse\":\n        \"\"\"Create a CallResponse from a dictionary.\n\n        Args:\n            data: A dictionary containing the CallResponse data.\n\n        Returns:\n            A new CallResponse instance.\n        \"\"\"\n        return cls(\n            return_value=data[\"return_value\"],\n            side_effects=[SideEffect.from_dict(se) for se in data[\"side_effects\"]],\n        )\n\n    def __repr__(self) -> str:\n        return f\"CallResponse(return_value={self.return_value}, side_effects={self.side_effects})\"\n```\n\n----------------------------------------\n\nTITLE: Defining User Model and Retrieval Function in Python\nDESCRIPTION: The snippet defines a Pydantic model 'User' to represent customer details and a function 'get_user_by_email()' to mimic searching for a user in a database. This setup is crucial for simulating user data access in support scenarios.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/support_ticket_routing.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    past_purchases: list[str]\n    past_charges: list[float]\n    payment_method: str\n    password: str\n    security_question: str\n    security_answer: str\n\ndef get_user_by_email(email: str):\n    if email == \"johndoe@gmail.com\":\n        return User(\n            name=\"John Doe\",\n            email=\"johndoe@gmail.com\",\n            past_purchases=[\"TV\", \"Microwave\", \"Chair\"],\n            past_charges=[349.99, 349.99, 99.99, 44.99],\n            payment_method=\"AMEX 1234 1234 1234 1234\",\n            password=\"password1!\",\n            security_question=\"Childhood Pet Name\",\n            security_answer=\"Piddles\",\n        )\n    else:\n        return None\n```\n\n----------------------------------------\n\nTITLE: Generating Data with Pydantic Response Model\nDESCRIPTION: Use a Pydantic BaseModel to define the structure and generate synthetic data for home appliances\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/generating_synthetic_data.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\n\nclass HomeAppliance(BaseModel):\n    name: str = Field(description=\"The name of the home appliance product\")\n    price: float = Field(description=\"The price of an individual product, in dollars (include cents)\")\n    inventory: int = Field(description=\"How many of the items are left in stock\")\n\n@openai.call(model=\"gpt-4o-mini\", response_model=list[HomeAppliance])\n@prompt_template(\"\"\"\nGenerate {num_datapoints} random but realistic datapoints of items which could\nbe in a home appliances warehouse. Output the datapoints as a list of instances\nof HomeAppliance.\n\"\"\")\ndef generate_home_appliance_data(num_datapoints: int): ...\n\nprint(generate_home_appliance_data(5))\n```\n\n----------------------------------------\n\nTITLE: Initializing QA examples\nDESCRIPTION: This snippet initializes a list of question-answer pairs (QA) that serve as examples for the LLM. Each QA object is a TypedDict containing a 'question' and its corresponding 'answer'. These examples are used to guide the LLM's responses in a corporate setting.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/demonstration_ensembling.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nclass QA(TypedDict):\n    question: str\n    answer: str\n\n\nqa_examples: list[QA] = [\n    QA(\n        question=\"What are your company's core values?\",\n        answer=\"Our company's core values are integrity, innovation, customer-centricity, and teamwork. We believe that maintaining these values is crucial to achieving our mission and vision.\",\n    ),\n    QA(\n        question=\"How do you handle conflicts in the workplace?\",\n        answer=\"We handle conflicts by promoting open communication, understanding different perspectives, and seeking mutually beneficial solutions. We have clear policies and trained mediators to assist in resolving conflicts effectively.\",\n    ),\n    QA(\n        question=\"Can you describe a time when you exceeded a client's expectations?\",\n        answer=\"Certainly. Recently, we completed a project ahead of schedule and under budget. We also provided additional insights and recommendations that significantly benefited the client, earning their gratitude and loyalty.\",\n    ),\n    QA(\n        question=\"How do you ensure continuous improvement within the team?\",\n        answer=\"We ensure continuous improvement by encouraging regular training, fostering a culture of feedback, and implementing agile methodologies. We also review our processes regularly to identify areas for enhancement.\",\n    ),\n    QA(\n        question=\"What strategies do you use to stay ahead of industry trends?\",\n        answer=\"We stay ahead of industry trends by investing in research and development, attending industry conferences, and maintaining strong relationships with thought leaders. We also encourage our team to engage in continuous learning and innovation.\",\n    ),\n    QA(\n        question=\"How do you measure the success of a project?\",\n        answer=\"We measure the success of a project by evaluating key performance indicators such as client satisfaction, budget adherence, timeline compliance, and the quality of the deliverables. Post-project reviews help us to identify successes and areas for improvement.\",\n    ),\n    QA(\n        question=\"What is your approach to diversity and inclusion?\",\n        answer=\"Our approach to diversity and inclusion involves creating a welcoming environment for all employees, offering diversity training, and implementing policies that promote equality. We value diverse perspectives as they drive innovation and growth.\",\n    ),\n    QA(\n        question=\"How do you manage remote teams effectively?\",\n        answer=\"We manage remote teams effectively by leveraging technology for communication and collaboration, setting clear goals, and maintaining regular check-ins. We also ensure that remote employees feel included and supported.\",\n    ),\n    QA(\n        question=\"What are your short-term and long-term goals for the company?\",\n        answer=\"In the short term, our goals include expanding our market reach and enhancing our product offerings. In the long term, we aim to become industry leaders by driving innovation and achieving sustainable growth.\",\n    ),\n    QA(\n        question=\"How do you handle feedback from clients or customers?\",\n        answer=\"We handle feedback by listening actively, responding promptly, and taking necessary actions to address concerns. We view feedback as an opportunity for improvement and strive to exceed our clients' expectations continuously.\",\n    ),\n]\n```\n```\n\n----------------------------------------\n\nTITLE: Using Custom Middleware Decorator in Mirascope Python Function\nDESCRIPTION: Demonstrates how to use the custom 'with_saving' decorator to wrap a Mirascope call. The decorator will automatically save metadata after the function execution.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/middleware.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@with_saving\ndef run():\n    return ai.call(\n        prompt=\"Tell me a joke\",\n        model=\"gpt-3.5-turbo\",\n    )\n```\n\n----------------------------------------\n\nTITLE: QA Prompt in Plaintext\nDESCRIPTION: A plaintext example of a QA prompt asking for an explanation of the difference between machine learning and artificial intelligence. The aim is to extract specific information leveraging the context provided.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-prompt.md#2025-04-21_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nBriefly explain the difference between machine learning and artificial intelligence.\n```\n\n----------------------------------------\n\nTITLE: LangChain Runnable with Stop Condition\nDESCRIPTION: Shows how to create a LangChain runnable pipeline with a stop condition using bind(). This example creates a tech support bot that stops generating when it encounters 'CASE CLOSED'.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrunnable = (\n    {\"support_query\": RunnablePassthrough()}  # Passes through customer support queries unchanged.\n    | prompt  # Assumes `prompt` handles formatting or additional setup.\n    | model.bind(stop=\"CASE CLOSED\")  # Stop when the support case is marked as resolved.\n    | StrOutputParser()  # Parses the string output from the model.\n)\nprint(runnable.invoke(\"My computer won't start, what should I do?\"))\n```\n\n----------------------------------------\n\nTITLE: Extracting Personal Details from PDF\nDESCRIPTION: Shows how to extract structured personal information from a PDF file, including contact details, skills, experience, and education.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-applications.md#2025-04-21_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nExtract personal details from the pdf file.\n{pdf_text}\n```\n\n----------------------------------------\n\nTITLE: Installing BeautifulSoup for HTML Parsing in Python\nDESCRIPTION: Installs BeautifulSoup for parsing HTML content. This library is needed to extract text from HTML documents. Ensure that beautifulsoup4 is installed using pip before using it in your Python code.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_summarization.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n!pip install beautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Documents\nDESCRIPTION: Loads a specified blog page and splits its content into smaller chunks for processing. Utilizes WebBaseLoader for fetching the webpage and RecursiveCharacterTextSplitter for dividing the text into manageable segments, ensuring informational continuity with overlapping chunks. Depends on 'bs4' for HTML parsing and 'langchain_text_splitters' for text splitting.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-pipeline.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n  ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\n```\n\n----------------------------------------\n\nTITLE: Handling Support Tickets Based on Classification in Python\nDESCRIPTION: The snippet shows how 'handle_ticket()' function uses classified data to fetch relevant context and routes it to the appropriate agent, simulating a decision-making flow for support systems.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/support_ticket_routing.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\", tools=[route_to_agent])\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an intermediary between a customer's interaction with a support chatbot and\n    a real life support agent. Organize the context so that the agent can best\n    facilitate the customer, but leave in details or raw data that the agent would need\n    to verify a person's identity or purchase. Then, route to the appropriate agent.\n\n    USER:\n    {context}\n    \"\"\"\n)\ndef handle_ticket(transcript: str) -> openai.OpenAIDynamicConfig:\n    context = transcript\n    call_classification = classify_transcript(transcript)\n    user = get_user_by_email(call_classification.user_email)\n    if isinstance(user, User):\n        if call_classification.calltype == \"billing\":\n            context += str(get_billing_details(user))\n        elif call_classification.calltype == \"sale\":\n            context += get_sale_items()\n            context += get_rewards(user)\n        elif call_classification.calltype == \"support\":\n            context += str(get_account_details(user))\n    else:\n        context = \"This person cannot be found in our system.\"\n\n    return {\"computed_fields\": {\"context\": context}}\n```\n\n----------------------------------------\n\nTITLE: Implementing Function Chaining with Separate LLM Calls\nDESCRIPTION: Shows how to chain multiple LLM calls together in a modular way using Mirascope. The highlighted lines demonstrate the clean interface for making sequential LLM requests and using outputs from one call as inputs to another.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/WHY.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/chaining/function_chaining/{{ provider | provider_dir }}/shorthand.py\"\n```\n\n----------------------------------------\n\nTITLE: Writing Multi-Line Prompts with Messages Methods in Python\nDESCRIPTION: Demonstrates how to create multi-line prompts using Messages.User method and inspect.cleandoc to maintain proper formatting.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template\ndef summarize_prompt(text: str) -> Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Summarize the following text:\n\n            Text:\n            {text}\n\n            Summary:\n            \"\"\"\n        )\n    )\n\n\nprint(summarize_prompt(text=\"This is a sample text that needs to be summarized.\"))\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Text Chunker for RAG in Mirascope\nDESCRIPTION: This snippet shows how to implement a custom TextChunker by extending Mirascope's BaseChunker class. The chunker splits text into chunks of specified size with overlap for efficient semantic search.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nfrom mirascope.beta.rag import BaseChunker, Document\n\n\nclass TextChunker(BaseChunker):\n    \"\"\"A text chunker that splits a text into chunks of a certain size and overlaps.\"\"\"\n\n    chunk_size: int\n    chunk_overlap: int\n\n    def chunk(self, text: str) -> list[Document]:\n        chunks: list[Document] = []\n        start: int = 0\n        while start < len(text):\n            end: int = min(start + self.chunk_size, len(text))\n            chunks.append(Document(text=text[start:end], id=str(uuid.uuid4())))\n            start += self.chunk_size - self.chunk_overlap\n        return chunks\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponse Class for Azure OpenAI API in Python\nDESCRIPTION: This class represents a response from an Azure OpenAI API call. It processes the raw response, extracts content, and provides methods for error handling and data access.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/azure/call_response.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass CallResponse:\n    \"\"\"A response from an Azure OpenAI API call.\"\"\"\n\n    def __init__(self, response: dict) -> None:\n        \"\"\"Initialize a CallResponse.\n\n        Args:\n            response (dict): The raw response from the Azure OpenAI API.\n        \"\"\"\n        self.response = response\n        self.choices = response.get(\"choices\", [])\n        self.usage = response.get(\"usage\", {})\n        self.content = self._extract_content()\n\n    def _extract_content(self) -> str:\n        \"\"\"Extract the content from the response.\n\n        Returns:\n            str: The extracted content.\n        \"\"\"\n        if not self.choices:\n            return \"\"\n        message = self.choices[0].get(\"message\", {})\n        return message.get(\"content\", \"\")\n\n    @property\n    def error(self) -> Optional[str]:\n        \"\"\"Get the error message from the response, if any.\n\n        Returns:\n            Optional[str]: The error message, or None if there is no error.\n        \"\"\"\n        return self.response.get(\"error\", {}).get(\"message\")\n\n    @property\n    def is_error(self) -> bool:\n        \"\"\"Check if the response contains an error.\n\n        Returns:\n            bool: True if the response contains an error, False otherwise.\n        \"\"\"\n        return \"error\" in self.response\n\n    @property\n    def prompt_tokens(self) -> int:\n        \"\"\"Get the number of prompt tokens used.\n\n        Returns:\n            int: The number of prompt tokens used.\n        \"\"\"\n        return self.usage.get(\"prompt_tokens\", 0)\n\n    @property\n    def completion_tokens(self) -> int:\n        \"\"\"Get the number of completion tokens used.\n\n        Returns:\n            int: The number of completion tokens used.\n        \"\"\"\n        return self.usage.get(\"completion_tokens\", 0)\n\n    @property\n    def total_tokens(self) -> int:\n        \"\"\"Get the total number of tokens used.\n\n        Returns:\n            int: The total number of tokens used.\n        \"\"\"\n        return self.usage.get(\"total_tokens\", 0)\n```\n\n----------------------------------------\n\nTITLE: Generating CSV Data with OpenAI and Mirascope\nDESCRIPTION: Create a function to generate realistic synthetic CSV data for home appliances using a prompt template and OpenAI's GPT model\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/generating_synthetic_data.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"\"\"\nGenerate {num_datapoints} random but realistic datapoints of items which could\nbe in a home appliances warehouse. Output the datapoints as a csv, and your\nresponse should only be the CSV.\n\nFormat:\nName, Price, Inventory\n\nName - the name of the home appliance product\nPrice - the price of an individual product, in dollars (include cents)\nInventory - how many are left in stock\n\"\"\")\ndef generate_csv_data(num_datapoints: int): ...\n\nprint(generate_csv_data(5))\n```\n\n----------------------------------------\n\nTITLE: Creating a ChatPromptTemplate for Multi-Role Conversations in Python\nDESCRIPTION: Illustrates the creation of a ChatPromptTemplate for modeling chatbot interactions. It defines roles (system, user, ai) and demonstrates formatting messages with specific inputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Define roles and placeholders\nchat_template = ChatPromptTemplate.from_messages(\n  [\n    (\"system\", \"You are a knowledgeable AI assistant. You are called {name}.\"),\n    (\"user\", \"Hi, what's the weather like today?\"),\n    (\"ai\", \"It's sunny and warm outside.\"),\n    (\"user\", \"{user_input}\"),\n   ]\n)\n\nmessages = chat_template.format_messages(name=\"Alice\", user_input=\"Can you tell me a joke?\")\n```\n\n----------------------------------------\n\nTITLE: Handling Structured Stream in Custom Middleware for Mirascope in Python\nDESCRIPTION: Implements a handler function for streaming Mirascope calls with a response_model. It collects stream data, calculates total tokens, and saves metadata to the database.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/middleware.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef handle_structured_stream(result: StructuredStream, fn: Callable, session: Session):\n    metadata = Metadata(\n        function_name=fn.__name__,\n        model=result.model,\n        provider=result.provider,\n        response=\"\".join(chunk.text for chunk in result.chunks),\n        total_tokens=sum(chunk.usage.total_tokens for chunk in result.chunks),\n        prompt_tokens=result.chunks[0].usage.prompt_tokens,\n        completion_tokens=sum(chunk.usage.completion_tokens for chunk in result.chunks),\n        latency=result.latency,\n    )\n    session.add(metadata)\n    session.commit()\n    return result\n```\n\n----------------------------------------\n\nTITLE: Running the Knowledge Graph Query\nDESCRIPTION: This snippet executes a query against the generated knowledge graph and prints the resulting output. A question about LLM pitfalls is posed for demonstration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"What are the pitfalls of using LLMs?\"\nkg = generate_knowledge_graph(question, \"wikipedia.html\")\nprint(kg)\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponseChunk Class for Mistral API Responses in Python\nDESCRIPTION: This class represents a chunk of response from a Mistral API call. It processes the raw response data, extracting relevant information such as the response text, finish reason, and completion status. The class provides methods to access these parsed details.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/mistral/call_response_chunk.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List, Optional\n\n\nclass CallResponseChunk:\n    def __init__(self, response: Dict[str, Any]) -> None:\n        self._response = response\n        self._choices = response.get(\"choices\", [])\n        self._choice = self._choices[0] if self._choices else {}\n        self._delta = self._choice.get(\"delta\", {})\n\n    @property\n    def response_text(self) -> str:\n        return self._delta.get(\"content\", \"\")\n\n    @property\n    def finish_reason(self) -> Optional[str]:\n        return self._choice.get(\"finish_reason\")\n\n    @property\n    def is_completed(self) -> bool:\n        return self.finish_reason is not None\n\n    @property\n    def response(self) -> Dict[str, Any]:\n        return self._response\n\n    @property\n    def choices(self) -> List[Dict[str, Any]]:\n        return self._choices\n\n    @property\n    def choice(self) -> Dict[str, Any]:\n        return self._choice\n\n    @property\n    def delta(self) -> Dict[str, Any]:\n        return self._delta\n```\n\n----------------------------------------\n\nTITLE: LLM Tool Flow Sequence Diagram\nDESCRIPTION: Mermaid sequence diagram showing the interaction flow between user code and LLM when using tools, including function calls and responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant YC as Your Code\n    participant LLM\n\n    YC->>LLM: Call with prompt and function definitions\n    loop Tool Calls\n        LLM->>LLM: Decide to respond or call functions\n        LLM->>YC: Respond with function to call and arguments\n        YC->>YC: Execute function with given arguments\n        YC->>LLM: Call with prompt and function result\n    end\n    LLM->>YC: Final response\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry exporters for observability tools\nDESCRIPTION: Shows how to configure OpenTelemetry in Mirascope to send spans to external observability tools using the OTLP HTTP exporter with a batch processor.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/integrations/otel.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace.export import (\n    BatchSpanProcessor,\n)\nfrom mirascope.integrations.otel import configure\n\nOBSERVABILITY_TOOL_ENDPOINT = \"...\"\nconfigure(\n    processors=[\n        BatchSpanProcessor(\n            OTLPSpanExporter(\n                endpoint=f\"https://{OBSERVABILITY_TOOL_ENDPOINT}/v1/traces\",\n            )\n        )\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying Python Module Imports\nDESCRIPTION: The `is_importable` function checks if all imports in a given code string are valid and exist. It parses the code string into an Abstract Syntax Tree (AST), iterates through import statements, and uses `importlib.util.find_spec` to verify that the modules can be found. It handles both `import` and `from ... import` statements, checking for the existence of the module and any attributes imported from it.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef is_importable(code_string: str) -> bool:\n    try:\n        tree = ast.parse(code_string)\n    except SyntaxError:\n        return False\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.Import | ast.ImportFrom):\n            module_name = (\n                node.module if isinstance(node, ast.ImportFrom) else node.names[0].name\n            )\n            if not check_module(module_name):\n                return False\n\n            if isinstance(node, ast.ImportFrom):\n                for alias in node.names:\n                    if not check_attribute(module_name, alias.name):\n                        return False\n\n    return True\n\n\ndef check_module(module_name):\n    try:\n        spec = importlib.util.find_spec(module_name)\n        return spec is not None\n    except (ImportError, AttributeError, ValueError):\n        return False\n\n\ndef check_attribute(module_name, attribute):\n    try:\n        spec = importlib.util.find_spec(module_name)\n        if spec is None:\n            return False\n        module = importlib.util.module_from_spec(spec)\n        if spec.loader:\n            spec.loader.exec_module(module)\n        return hasattr(module, attribute)\n    except (ImportError, AttributeError):\n        return False\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponseChunk Class for Anthropic API Responses in Python\nDESCRIPTION: Implements a class for handling chunked responses from Anthropic API calls. The class processes both content chunks and message delta chunks, tracking stop reasons and providing utility methods for accessing content and determining if the response is complete.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/anthropic/call_response_chunk.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Chunked responses from Anthropic API calls.\"\"\"\n\nfrom typing import Dict, List, Literal, Optional, Union\n\nfrom anthropic.types import ContentBlockDelta, MessageDelta, MessageStopReason\n\n\nclass CallResponseChunk:\n    \"\"\"A chunk of a response from an Anthropic API call.\"\"\"\n\n    _stop_reason: Optional[MessageStopReason] = None\n    _delta: Union[ContentBlockDelta, MessageDelta, None] = None\n\n    def __init__(\n        self,\n        stop_reason: Optional[MessageStopReason] = None,\n        delta: Optional[Union[ContentBlockDelta, MessageDelta]] = None,\n    ) -> None:\n        \"\"\"Initialize a CallResponseChunk.\n\n        Args:\n            stop_reason: The stop reason for the response.\n            delta: The delta content, if any.\n        \"\"\"\n        self._stop_reason = stop_reason\n        self._delta = delta\n\n    @classmethod\n    def from_content_block_delta(\n        cls, delta: ContentBlockDelta, stop_reason: Optional[MessageStopReason] = None\n    ) -> \"CallResponseChunk\":\n        \"\"\"Create a CallResponseChunk from a ContentBlockDelta.\n\n        Args:\n            delta: The ContentBlockDelta to create the chunk from.\n            stop_reason: The stop reason, if any.\n\n        Returns:\n            A CallResponseChunk.\n        \"\"\"\n        return cls(stop_reason=stop_reason, delta=delta)\n\n    @classmethod\n    def from_message_delta(\n        cls, delta: MessageDelta, stop_reason: Optional[MessageStopReason] = None\n    ) -> \"CallResponseChunk\":\n        \"\"\"Create a CallResponseChunk from a MessageDelta.\n\n        Args:\n            delta: The MessageDelta to create the chunk from.\n            stop_reason: The stop reason, if any.\n\n        Returns:\n            A CallResponseChunk.\n        \"\"\"\n        return cls(stop_reason=stop_reason, delta=delta)\n\n    @classmethod\n    def from_dict(\n        cls, chunk_dict: Dict\n    ) -> Union[\"CallResponseChunk\", List[\"CallResponseChunk\"]]:\n        \"\"\"Create a CallResponseChunk from a dict.\n\n        Args:\n            chunk_dict: The dict to create the chunk from.\n\n        Returns:\n            A CallResponseChunk or a list of CallResponseChunks.\n        \"\"\"\n        message_dict = chunk_dict.get(\"message\", {})\n        stop_reason = message_dict.get(\"stop_reason\")\n        content_blocks = message_dict.get(\"content_blocks\", [])\n        if not content_blocks:\n            return cls(stop_reason=stop_reason)\n\n        content_blocks_chunks = []\n        for block in content_blocks:\n            content_blocks_chunks.append(\n                cls.from_content_block_delta(\n                    ContentBlockDelta(\n                        type=block.get(\"type\"),\n                        text=block.get(\"text\"),\n                        index=block.get(\"index\"),\n                    ),\n                    stop_reason=stop_reason,\n                )\n            )\n        return content_blocks_chunks\n\n    def content(self) -> str:\n        \"\"\"Get the content of the chunk.\n\n        Returns:\n            The content of the chunk.\n        \"\"\"\n        if not self._delta:\n            return \"\"\n\n        if isinstance(self._delta, ContentBlockDelta):\n            if text := self._delta.text:\n                return text\n\n        if isinstance(self._delta, MessageDelta):\n            if content_blocks := self._delta.content_blocks:\n                if len(content_blocks) > 0 and content_blocks[0].text:\n                    return content_blocks[0].text\n        return \"\"\n\n    def stop_reason(self) -> Optional[MessageStopReason]:\n        \"\"\"Get the stop reason for the response.\n\n        Returns:\n            The stop reason for the response.\n        \"\"\"\n        return self._stop_reason\n\n    def is_done(self) -> bool:\n        \"\"\"Check if the response is done.\n\n        Returns:\n            True if the response is done, False otherwise.\n        \"\"\"\n        return self._stop_reason is not None\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope OpenAI Dependencies\nDESCRIPTION: Installs the Mirascope library with OpenAI support, enabling multimodal image processing capabilities\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/generating_captions.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Appending Data to DataFrame\nDESCRIPTION: Function to generate and append additional synthetic data to an existing DataFrame using OpenAI's model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/synthetic-data-generation.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\", response_model=DataFrameGenerator)\n@prompt_template(\n    \"\"\"\n    Generate {num_datapoints} random but realistic datapoints of items which would\n    make sense to the following dataset:\n    {df}\n    Generate your response as `data` and\n    `column_names`, so that a pandas DataFrame may be generated with:\n    `pd.DataFrame(data, columns=column_names)` then appended to the existing data.\n    \"\"\"\n)\ndef generate_additional_df_data(num_datapoints: int, df: pd.DataFrame): ...\n\ndf_data = generate_additional_df_data(5, df)\ndf = df_data.append_dataframe(df)\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Using RunnablePassthrough in LangChain for Data Processing\nDESCRIPTION: This snippet demonstrates the use of RunnablePassthrough in LangChain for passing data through a sequence of processing steps. While it allows for data flow, it can make debugging more challenging due to its opaque nature.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-rag.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nRunnablePassthrough\n```\n\n----------------------------------------\n\nTITLE: Mermaid Diagram Comparing Standard vs. Streaming Responses\nDESCRIPTION: A sequence diagram illustrating the difference between standard response flow and streaming response flow when interacting with LLMs. It shows how streaming allows for incremental processing of responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant User\n    participant App\n    participant LLM\n\n    User->>App: Request\n    App->>LLM: Query\n    Note right of LLM: Standard Response\n    LLM-->>App: Complete Response\n    App-->>User: Display Result\n\n    User->>App: Request\n    App->>LLM: Query (Stream)\n    Note right of LLM: Streaming Response\n    loop For each chunk\n        LLM-->>App: Response Chunk\n        App-->>User: Display Chunk\n    end\n```\n\n----------------------------------------\n\nTITLE: Creating FewShotPromptTemplate in LangChain\nDESCRIPTION: This example from LangChain's documentation shows how to create a FewShotPromptTemplate. It defines example questions and answers for few-shot learning, which can be used to improve the model's performance on similar tasks.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplate\nfrom langchain_core.prompts.prompt import PromptTemplate\n\nexamples = [\n    {\n        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n        \"answer\": \"\"\"\nAre follow up questions needed here: Yes.\nFollow up: How old was Muhammad Ali when he died?\nIntermediate answer: Muhammad Ali was 74 years old when he died.\nFollow up: How old was Alan Turing when he died?\nIntermediate answer: Alan Turing was 41 years old when he died.\nSo the final answer is: Muhammad Ali\n\"\"\",\n    },\n    {\n        \"question\": \"When was the founder of craigslist born?\",\n        \"answer\": \"\"\"\nAre follow up questions needed here: Yes.\nFollow up: Who was the founder of craigslist?\nIntermediate answer: Craigslist was founded by Craig Newmark.\nFollow up: When was Craig Newmark born?\nIntermediate answer: Craig Newmark was born on December 6, 1952.\nSo the final answer is: December 6, 1952\n\"\"\",\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Defining CallParams Class for Azure OpenAI API in Python\nDESCRIPTION: This code defines the CallParams class which encapsulates parameters for Azure OpenAI API calls. It includes methods for setting deployment, API version, and various call options, with built-in validation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/azure/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass CallParams:\n    \"\"\"Call parameters for Azure OpenAI API calls.\"\"\"\n\n    def __init__(self, deployment: str, api_version: str = \"2023-05-15\"):\n        \"\"\"Initialize call parameters.\n\n        Args:\n            deployment: The deployment to use for the API call.\n            api_version: The API version to use for the API call.\n        \"\"\"\n        self.deployment = deployment\n        self.api_version = api_version\n        self.options: Dict[str, Any] = {}\n\n    def set_deployment(self, deployment: str) -> None:\n        \"\"\"Set the deployment.\n\n        Args:\n            deployment: The deployment to use for the API call.\n        \"\"\"\n        self.deployment = deployment\n\n    def set_api_version(self, api_version: str) -> None:\n        \"\"\"Set the API version.\n\n        Args:\n            api_version: The API version to use for the API call.\n        \"\"\"\n        self.api_version = api_version\n\n    def set_options(self, **kwargs: Any) -> None:\n        \"\"\"Set options for the API call.\n\n        Args:\n            **kwargs: The options to set for the API call.\n        \"\"\"\n        self.options.update(kwargs)\n\n    def set_temperature(self, temperature: float) -> None:\n        \"\"\"Set the temperature for the API call.\n\n        Args:\n            temperature: The temperature to use for the API call.\n\n        Raises:\n            ValueError: If the temperature is not between 0 and 2.\n        \"\"\"\n        if not 0 <= temperature <= 2:\n            raise ValueError(\"Temperature must be between 0 and 2.\")\n        self.options[\"temperature\"] = temperature\n\n    def set_max_tokens(self, max_tokens: int) -> None:\n        \"\"\"Set the maximum number of tokens for the API call.\n\n        Args:\n            max_tokens: The maximum number of tokens to use for the API call.\n\n        Raises:\n            ValueError: If max_tokens is not a positive integer.\n        \"\"\"\n        if not isinstance(max_tokens, int) or max_tokens <= 0:\n            raise ValueError(\"max_tokens must be a positive integer.\")\n        self.options[\"max_tokens\"] = max_tokens\n\n    def set_top_p(self, top_p: float) -> None:\n        \"\"\"Set the top_p value for the API call.\n\n        Args:\n            top_p: The top_p value to use for the API call.\n\n        Raises:\n            ValueError: If top_p is not between 0 and 1.\n        \"\"\"\n        if not 0 <= top_p <= 1:\n            raise ValueError(\"top_p must be between 0 and 1.\")\n        self.options[\"top_p\"] = top_p\n\n    def set_frequency_penalty(self, frequency_penalty: float) -> None:\n        \"\"\"Set the frequency penalty for the API call.\n\n        Args:\n            frequency_penalty: The frequency penalty to use for the API call.\n\n        Raises:\n            ValueError: If frequency_penalty is not between -2 and 2.\n        \"\"\"\n        if not -2 <= frequency_penalty <= 2:\n            raise ValueError(\"frequency_penalty must be between -2 and 2.\")\n        self.options[\"frequency_penalty\"] = frequency_penalty\n\n    def set_presence_penalty(self, presence_penalty: float) -> None:\n        \"\"\"Set the presence penalty for the API call.\n\n        Args:\n            presence_penalty: The presence penalty to use for the API call.\n\n        Raises:\n            ValueError: If presence_penalty is not between -2 and 2.\n        \"\"\"\n        if not -2 <= presence_penalty <= 2:\n            raise ValueError(\"presence_penalty must be between -2 and 2.\")\n        self.options[\"presence_penalty\"] = presence_penalty\n\n    def set_stop(self, stop: Union[str, List[str]]) -> None:\n        \"\"\"Set the stop sequences for the API call.\n\n        Args:\n            stop: The stop sequences to use for the API call.\n\n        Raises:\n            ValueError: If stop is not a string or list of strings.\n        \"\"\"\n        if isinstance(stop, str):\n            stop = [stop]\n        elif not isinstance(stop, list) or not all(isinstance(s, str) for s in stop):\n            raise ValueError(\"stop must be a string or list of strings.\")\n        self.options[\"stop\"] = stop\n\n    def set_logit_bias(self, logit_bias: Dict[int, float]) -> None:\n        \"\"\"Set the logit bias for the API call.\n\n        Args:\n            logit_bias: The logit bias to use for the API call.\n\n        Raises:\n            ValueError: If logit_bias is not a dictionary of int keys and float values.\n        \"\"\"\n        if not isinstance(logit_bias, dict) or not all(\n            isinstance(k, int) and isinstance(v, float) for k, v in logit_bias.items()\n        ):\n            raise ValueError(\n                \"logit_bias must be a dictionary of int keys and float values.\"\n            )\n        self.options[\"logit_bias\"] = logit_bias\n\n    def set_user(self, user: str) -> None:\n        \"\"\"Set the user for the API call.\n\n        Args:\n            user: The user to use for the API call.\n        \"\"\"\n        self.options[\"user\"] = user\n\n    def set_n(self, n: int) -> None:\n        \"\"\"Set the number of completions to generate for the API call.\n\n        Args:\n            n: The number of completions to generate.\n\n        Raises:\n            ValueError: If n is not a positive integer.\n        \"\"\"\n        if not isinstance(n, int) or n <= 0:\n            raise ValueError(\"n must be a positive integer.\")\n        self.options[\"n\"] = n\n\n    def set_stream(self, stream: bool) -> None:\n        \"\"\"Set whether to stream the results for the API call.\n\n        Args:\n            stream: Whether to stream the results.\n        \"\"\"\n        self.options[\"stream\"] = stream\n\n    def set_logprobs(self, logprobs: Optional[int]) -> None:\n        \"\"\"Set the logprobs for the API call.\n\n        Args:\n            logprobs: The logprobs to use for the API call.\n\n        Raises:\n            ValueError: If logprobs is not None or a non-negative integer.\n        \"\"\"\n        if logprobs is not None and (not isinstance(logprobs, int) or logprobs < 0):\n            raise ValueError(\"logprobs must be None or a non-negative integer.\")\n        self.options[\"logprobs\"] = logprobs\n\n    def set_echo(self, echo: bool) -> None:\n        \"\"\"Set whether to echo the prompt for the API call.\n\n        Args:\n            echo: Whether to echo the prompt.\n        \"\"\"\n        self.options[\"echo\"] = echo\n\n    def set_best_of(self, best_of: int) -> None:\n        \"\"\"Set the best_of parameter for the API call.\n\n        Args:\n            best_of: The best_of parameter to use for the API call.\n\n        Raises:\n            ValueError: If best_of is not a positive integer.\n        \"\"\"\n        if not isinstance(best_of, int) or best_of <= 0:\n            raise ValueError(\"best_of must be a positive integer.\")\n        self.options[\"best_of\"] = best_of\n\n    def set_suffix(self, suffix: Optional[str]) -> None:\n        \"\"\"Set the suffix for the API call.\n\n        Args:\n            suffix: The suffix to use for the API call.\n        \"\"\"\n        self.options[\"suffix\"] = suffix\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert the call parameters to a dictionary.\n\n        Returns:\n            A dictionary of the call parameters.\n        \"\"\"\n        return {\n            \"deployment\": self.deployment,\n            \"api_version\": self.api_version,\n            **self.options,\n        }\n```\n\n----------------------------------------\n\nTITLE: Multiple Function Definitions with OpenAI API\nDESCRIPTION: This snippet demonstrates the verbosity of defining multiple function calls using the OpenAI API. Each function requires its own JSON schema, leading to lengthy and repetitive code. This contrasts with Mirascope's approach of using class inheritance for cleaner definitions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_stock_price\",\n            \"description\": \"Get the current stock price of a company\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"company\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the company, e.g., 'Apple Inc.' or 'Google LLC'\",\n                    },\n                },\n                \"required\": [\"company\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_stock_price_in_currency\",\n            \"description\": \"Get the current stock price of a company in a specific currency\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"company\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the company, e.g., 'Apple Inc.' or 'Google LLC'\",\n                    },\n                    \"currency\": {\n                        \"type\": \"string\",\n                        \"description\": \"The currency to get the stock price in, e.g., 'USD', 'EUR', 'JPY'\",\n                    },\n                },\n                \"required\": [\"company\", \"currency\"],\n            },\n        },\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Mirascope Define Tool from BaseTool Class Example\nDESCRIPTION: This code shows how to define a tool in Mirascope using the `BaseTool` class, particularly when the function doesn't have a docstring or cannot be modified. The `GetCurrentStockPrice` class inherits from `BaseTool` and includes a `company` field with a description. The `call` method then calls the original `get_current_stock_price` function with the company name from the tool's field. The `@openai.call` decorator is used to register the tool with the LLM.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/openai-function-calling.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseTool, openai, prompt_template\nfrom pydantic import Field\n\n\ndef get_current_stock_price(company: str) -> str:\n    # Assume this function does not have a docstring\n    if company == \"Apple, Inc.\":\n        return f\"The current stock price of {company} is $200.\"\n    elif company == \"Google LLC\":\n        return f\"The current stock price of {company} is $180.\"\n    else:\n        return f\"I'm sorry, I don't have the stock price for {company}.\"\n\n\nclass GetCurrentStockPrice(BaseTool):\n    \"\"\"Get the current stock price for a given company.\"\"\"\n\n    company: str = Field(\n        ...,\n        description=\"The name of the company, e.g., Apple Inc.\",\n    )\n\n    def call(self):\n        return get_current_stock_price(self.company)\n\n\n@openai.call(\"gpt-4o\", tools=[GetCurrentStockPrice])\n@prompt_template(\"What's the stock price of {company}\")\ndef stock_price(company: str): ...\n\n\nresponse = stock_price(\"Apple, Inc.\")\nif tool := response.tool:\n    print(tool.call())\n# > The current stock price of Apple, Inc. is $200.\n```\n\n----------------------------------------\n\nTITLE: A/B Testing Implementation for Customer Service Responses\nDESCRIPTION: Implementation of A/B testing system for customer service prompts, including response time simulation and satisfaction scoring.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport requests\nimport time\n\n# Function to get a random prompt variant\ndef get_random_variant():\n    return 'A' if random.random() < 0.5 else 'B'\n\n# Function to simulate sending a customer service request\ndef send_customer_service_request(prompt, user_input):\n    # Simulate processing time (for demonstration purposes)\n    processing_time = random.uniform(1, 3)  # Simulate 1-3 seconds processing time\n    time.sleep(processing_time)  # waiting for the response\n    \n    # Simulate response satisfaction score (1-5)\n    satisfaction_score = random.randint(1, 5)\n    \n    return {\n        'response_time': processing_time,\n        'satisfaction_score': satisfaction_score\n    }\n\n# Function to handle user interaction\ndef handle_user_interaction(user_input):\n    variant = get_random_variant()\n    \n    # Define customer service prompts for each variant\n    prompt_a = \"How can I assist you with your order?\"\n    prompt_b = \"What issue are you experiencing with your order?\"\n    \n    # Select the appropriate prompt based on the variant\n    prompt = prompt_a if variant == 'A' else prompt_b\n    \n    # Send request and get the response\n    result = send_customer_service_request(prompt, user_input)\n    \n    if result:\n        print(f\"Prompt Variant: {variant}\")\n        print(\"Response Time:\", f\"{result['response_time']:.2f} seconds\")\n        print(\"Satisfaction Score:\", result['satisfaction_score'])\n        \n        # Collect feedback from the user (optional)\n        collect_user_feedback(variant, result['satisfaction_score'])\n\n# Function to collect user feedback (placeholder)\ndef collect_user_feedback(variant, satisfaction_score):\n    print(f\"Thank you for your feedback on Variant {variant}.\")\n    print(f\"Satisfaction Score: {satisfaction_score} recorded.\")\n```\n\n----------------------------------------\n\nTITLE: Clarifying Complex Queries with Self-Ask Prompt\nDESCRIPTION: This snippet details the Self-Ask prompting technique, where the model poses clarifying questions before answering complex queries, ensuring all relevant aspects are considered.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nUsing the Self-Ask method, when does monsoon season end in the state where area code 575 is located? If follow-up questions are needed, please ask and answer them before providing the final answer.\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nAre follow-up questions needed here: Yes.\nFollow-up question: Which state is area code 575 located in?\nIntermediate answer: The area code 575 is located in New Mexico.\nFollow-up question: When does monsoon season end in New Mexico?\nIntermediate answer: Monsoon season in New Mexico typically ends in mid-September.\nFinal Answer: Monsoon season in the state where area code 575 is located (New Mexico) typically ends in mid-September.\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Configuration for LLM Calls in Python with Mirascope V1\nDESCRIPTION: Illustrates the dynamic configuration capabilities in Mirascope V1, allowing for runtime generation of tools based on call parameters. This example shows how to create a dynamic toolkit for book recommendations that adapts to different genres.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/mirascope-v1-release.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseToolKit, openai, prompt_template, toolkit_tool\n\n\nclass BookToolkit(BaseToolKit):\n    genre: str\n\n    @toolkit_tool\n    def format_book(self, title: str, author: str) -> str:\n        \"\"\"Format a {self.genre} book recommendation.\"\"\"\n        return f\"{title} by {author} ({self.genre})\"\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -> openai.OpenAIDynamicConfig:\n    toolkit = BookToolkit(genre=genre)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_book(\"mystery\")\nif response.tool:\n    print(response.tool.call())\n```\n\n----------------------------------------\n\nTITLE: Generating Image Captions with LLM\nDESCRIPTION: This snippet demonstrates how to generate a descriptive caption for an image using an LLM. It uses a simple prompt that instructs the model to create a short, descriptive caption for a given image URL.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-applications.md#2025-04-21_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nGenerate a short, descriptive caption for this image: {url:image}\n```\n\n----------------------------------------\n\nTITLE: Flow Diagram - Asynchronous Processing\nDESCRIPTION: Mermaid sequence diagram illustrating the flow of concurrent API calls and response handling in asynchronous processing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/async.md#2025-04-21_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant Main as Main Process\n    participant API1 as API Call 1\n    participant API2 as API Call 2\n    participant API3 as API Call 3\n\n    Main->>+API1: Send Request\n    Main->>+API2: Send Request\n    Main->>+API3: Send Request\n    API1-->>-Main: Response\n    API2-->>-Main: Response\n    API3-->>-Main: Response\n    Main->>Main: Process All Responses\n```\n\n----------------------------------------\n\nTITLE: Loading Knowledge Graph into Neo4j\nDESCRIPTION: This function loads a knowledge graph by creating nodes and edges in the Neo4j database. It first clears existing data if specified, then processes the nodes and edges to build the graph structure.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-build-a-knowledge-graph.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef load_knowledge_graph(driver: Driver, kg: KnowledgeGraph, clear_first: bool = True):\n    with driver.session() as session:\n        # Clear existing data if requested\n        if clear_first:\n            session.run(\"MATCH (n) DETACH DELETE n\")\n\n        # Create nodes\n        for node in kg.nodes:\n            session.run(\n                f\"CREATE (n:{node.type} $props)\",\n                props={\"id\": node.id, **(node.properties or {})},\n            )\n\n        # Create edges\n        for edge in kg.edges:\n            # Replace spaces with underscores in relationship name\n            relationship = edge.relationship.replace(\" \", \"_\")\n\n            session.run(\n                f\"\"\"\n                MATCH (source {{id: $source_id}})\n                MATCH (target {{id: $target_id}})\n                CREATE (source)-[:{relationship}]->(target)\n                \"\"\",\n                source_id=edge.source,\n                target_id=edge.target,\n            )\n```\n\n----------------------------------------\n\nTITLE: Import Statements and BaseModel Definition in Python\nDESCRIPTION: This snippet imports necessary libraries such as `json`, `mirascope.core`, `openai.types.chat`, and `pydantic`. It also defines a `Problem` class using `pydantic.BaseModel` to structure the subproblems resulting from the decomposition process. This class includes a field `subproblems` which is a list of strings with descriptions used for specifying subproblems.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/decomposed_prompting.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"import json\n\nfrom mirascope.core import openai, prompt_template\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom pydantic import BaseModel, Field\n\n\nclass Problem(BaseModel):\n    subproblems: list[str] = Field(\n        ..., description=\"The subproblems that the original problem breaks down into\"\n    )\"\n```\n\n----------------------------------------\n\nTITLE: Implementing BedrockStreamer for Amazon Bedrock streaming responses\nDESCRIPTION: Abstract base class for handling streaming responses from Amazon Bedrock models. It defines the interface for streaming processors and includes a factory method to create the appropriate streamer based on the model ID.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/bedrock/stream.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BedrockStreamer(ABC):\n    \"\"\"Base class for handling streaming responses from Amazon Bedrock models.\"\"\"\n\n    @abstractmethod\n    def __call__(self, response_stream: Iterator[Dict[str, Any]]) -> Iterator[str]:\n        \"\"\"Process a streaming response from Amazon Bedrock.\n\n        Args:\n            response_stream: The streaming response from Amazon Bedrock.\n\n        Returns:\n            An iterator of strings representing the processed response.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def from_model_id(model_id: str) -> \"BedrockStreamer\":\n        \"\"\"Create a streamer for the given model ID.\n\n        Args:\n            model_id: The Amazon Bedrock model ID.\n\n        Returns:\n            A BedrockStreamer instance for the given model ID.\n\n        Raises:\n            ValueError: If the model ID is not supported.\n        \"\"\"\n        if model_id.startswith(\"anthropic.claude\"):\n            return ClaudeStreamer()\n        elif model_id.startswith(\"amazon.titan\"):\n            return TitanStreamer()\n        elif model_id.startswith(\"cohere\"):\n            return CohereStreamer()\n        raise ValueError(f\"Unsupported model ID: {model_id}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Mirascope ChatBot\nDESCRIPTION: Installation of necessary Python packages including Mirascope with OpenAI support and LlamaIndex components.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/local_chat_with_codebase.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n!pip install llama-index  llama-index-llms-ollama llama-index-embeddings-huggingface huggingface\n```\n\n----------------------------------------\n\nTITLE: Defining CallParams Class for LiteLLM API Calls in Python\nDESCRIPTION: This comprehensive class defines all possible parameters for making API calls with LiteLLM. It includes options for model selection, message formatting, temperature settings, and various other API-specific parameters.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/litellm/call_params.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass CallParams(TypedDict, total=False):\n    model: str\n    messages: list[dict[str, str]]\n    temperature: float\n    top_p: float\n    n: int\n    stream: bool\n    stop: str | list[str] | None\n    max_tokens: int | None\n    presence_penalty: float\n    frequency_penalty: float\n    logit_bias: dict[str, float] | None\n    user: str\n    request_timeout: float | None\n    api_key: str | None\n    api_base: str | None\n    api_version: str | None\n    organization: str | None\n    timeout: float | None\n    default_headers: dict[str, str] | None\n    max_retries: int\n    streaming_option: StreamingParams | None\n    function_call: FunctionCallParams | None\n```\n\n----------------------------------------\n\nTITLE: HyperDX Tracing Integration\nDESCRIPTION: A decorator for integrating HyperDX tracing and monitoring capabilities into Mirascope applications. Enables advanced logging and observability features.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/integrations/otel.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef with_hyperdx()\n```\n\n----------------------------------------\n\nTITLE: Generating Image Caption with OpenAI GPT-4o Mini\nDESCRIPTION: Defines a function to generate a descriptive caption for an image using Mirascope's OpenAI call and prompt template decorators\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/generating_captions.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\\n\\nurl = \"https://c02.purpledshub.com/uploads/sites/41/2023/01/How-to-see-the-Wolf-Moon-in-2023--4bb6bb7.jpg?w=940&webp=1\"\\n\\n@openai.call(model=\"gpt-4o-mini\")\\n@prompt_template(\"Generate a short, descriptive caption for this image: {url:image}\")\\ndef generate_caption(url: str): ...\\n\\nresponse = generate_caption(url)\\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Manual Audio Interaction Without Turn Detection\nDESCRIPTION: Implementation of audio interaction without automatic turn detection for OpenAI's Realtime API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/openai.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/openai/realtime/audio_input_output_manually.py\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Chroma Vector Store\nDESCRIPTION: Sets up a Chroma vector store by embedding document chunks using OpenAIEmbeddings, which is crucial for storing preprocessed and encoded document data for quick retrieval during querying. Requires the 'langchain_chroma', 'langchain_openai', and 'splits' obtained from previous steps.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-pipeline.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nvector_store = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n```\n\n----------------------------------------\n\nTITLE: Plaintext Prompt Example\nDESCRIPTION: A simple prompt example highlighting the importance of providing context to the LLM. This example contrasts a one-liner prompt with a detailed context-rich prompt to improve response quality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-best-practices.md#2025-04-21_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\n\"What are the best techniques for improving memory?\"\n```\n\nLANGUAGE: python\nCODE:\n```\n\"I’m a second-year college student who struggles with retaining information from my lectures and reading material. I find it hard to remember key concepts during exams. What are the best techniques for improving my memory to enhance my study habits and exam performance?\"\n```\n\n----------------------------------------\n\nTITLE: JSON Mode Output Parsing in Mirascope\nDESCRIPTION: Example of using Mirascope's JSON mode feature to ensure LLM outputs are returned as valid JSON structures.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-structured-output.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o-mini\", json_mode=True)\ndef get_movie_info(movie_title: str) -> str:\n    return f\"Provide the director, release year, and main genre for {movie_title}\"\n\n\nresponse = get_movie_info(\"Inception\")\nprint(json.loads(response.content))\n# > {\"director\": \"Christopher Nolan\", \"release_year\": \"2010\", \"main_genre\": \"Science Fiction\"}\n```\n\n----------------------------------------\n\nTITLE: Defining the Book Deduplication Schema with Pydantic\nDESCRIPTION: This code snippet creates a Pydantic model named 'Book' to represent book entities including their title, author, and genre. This model is used for deduplicating collections of books.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/removing_semantic_duplicates.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Book(BaseModel):\n    title: str\n    author: str\n    genre: str\n```\n\n----------------------------------------\n\nTITLE: Using Role-Based Prompting for Personalized Fitness Advice in Plaintext\nDESCRIPTION: This snippet demonstrates how to use role-based prompting with SYSTEM, USER, and ASSISTANT roles to create a structured interaction for providing personalized fitness advice. It shows how to maintain context over multiple exchanges.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nSYSTEM\nYou are the world's most insightful fitness coach, known for creating personalized workout routines that align with individual goals, fitness levels, and unique physical needs. When giving advice, always consider the user's past workouts, preferences, and any physical limitations. Provide detailed explanations for each recommendation, including how it benefits the user's specific goals.\n\nUSER:\nCan you suggest a HIIT workout for endurance?\n\nA:\nGiven your previous mention of knee discomfort, I recommend a modified low-impact HIIT routine that reduces joint strain. We can use exercises like cycling or swimming intervals that provide cardio benefits without the high impact on your knees.\n\nUSER:\nThat sounds good. Now, can you suggest a low-impact cardio routine for improving endurance?\n\nA:\nFor improving endurance while minimizing knee strain, I suggest a low-impact routine focused on brisk walking or swimming. Both provide excellent cardiovascular benefits with minimal joint impact. I can also include some gentle strength exercises to build supporting muscles around your knees.\n```\n\n----------------------------------------\n\nTITLE: Using Pydub for Audio Inputs with BaseMessageParam in Python\nDESCRIPTION: Demonstrates handling audio inputs by directly creating BaseMessageParam instances with audio content for multi-modal interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom pydub import AudioSegment\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.message_param import BaseMessageParam\nfrom mirascope.core.base.content_param import TextParam, AudioParam\n\n\n@prompt_template\ndef transcribe_audio_prompt(audio: bytes) -> Messages.Type:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=[\n                TextParam(\"Transcribe this audio: \"),\n                AudioParam(audio),\n            ],\n        )\n    ]\n\n\n# Read an audio file\naudio_segment = AudioSegment.from_file(\"examples/data/sample.mp3\")\naudio_bytes = audio_segment.export(format=\"mp3\").read()\n\n# Use the audio in a prompt\nprint(transcribe_audio_prompt(audio=audio_bytes))\n# Returns a user message with both text and audio content\n\n```\n\n----------------------------------------\n\nTITLE: Processing LLM Output with XML Parsing\nDESCRIPTION: Illustrates how to structure LLM output as XML and parse it into a Python object. The highlighted sections show the XML template definition, call function, and parser implementation that extracts data from XML tags.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/output_parsers.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/output_parsers/additional_examples/{{ provider | provider_dir }}/xml.py\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from HTML with BeautifulSoup\nDESCRIPTION: This function reads an HTML file and extracts its text content using BeautifulSoup, which is essential for processing unstructured data into a usable format for the knowledge graph.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom bs4 import BeautifulSoup\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_text_from_html(file_path: str) -> str:\n    with open(file_path) as file:\n        html_text = file.read()\n    return BeautifulSoup(html_text, \"html.parser\").get_text()\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope Dependencies - Python\nDESCRIPTION: This snippet installs Mirascope and its necessary dependencies using pip, enabling the use of Gemini features for audio transcription.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/speech_transcription.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[gemini]\"\n```\n\n----------------------------------------\n\nTITLE: Calculate Precision and Recall with Scikit-Learn\nDESCRIPTION: This snippet calculates precision and recall using the scikit-learn library based on sample true labels (`y_true`) and predicted labels (`y_pred`). Precision measures the accuracy of predicted positives, while recall measures the model's ability to identify actual positives. A confusion matrix is also displayed to provide a detailed breakdown of true positives, false positives, true negatives, and false negatives.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix\n\n# Sample true labels (ground truth)\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n\n# Sample predicted labels from a model\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n\n# Calculate precision and recall\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\n\n# Print the results\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\n\n# Optional: Display confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\n```\n\n----------------------------------------\n\nTITLE: Implementing AzureContentStream Class for Azure AI Content Streaming\nDESCRIPTION: Defines a class for handling content streaming from Azure AI services. The class handles different types of Azure responses including delta and add messages, processing them into readable content and tracking statistics.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/azure/stream.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AzureContentStream:\n    \"\"\"A class for handling content streaming from Azure AI services.\"\"\"\n\n    def __init__(self, response):\n        \"\"\"Initializes an instance of AzureContentStream.\n\n        Args:\n            response: The response from an Azure API.\n        \"\"\"\n        self._response = response\n        self.bytearray = bytearray()\n        self.content_so_far = \"\"\n        self.function_call_so_far = \"\"\n        self.tool_calls_so_far = []\n        self.function_name = \"\"\n        self.finish_reason = None\n        self.first_response = True\n        self.usage = {\n            \"completion_tokens\": 0,\n            \"prompt_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n\n    async def __aiter__(self):\n        \"\"\"Asynchronous iterator for streaming responses.\n\n        Yields:\n            The streamed content.\n        \"\"\"\n        async for chunk in self._response:\n            # Assuming chunk is a dict with a 'choices' key\n            if hasattr(chunk, \"choices\") and len(chunk.choices) > 0:\n                choice = chunk.choices[0]\n                if choice.finish_reason is not None and choice.finish_reason != \"\":\n                    self.finish_reason = choice.finish_reason\n\n                # Handle delta response\n                if hasattr(choice, \"delta\"):\n                    delta = choice.delta\n                    yield self._process_delta(delta)\n\n                # Handle message response\n                elif hasattr(choice, \"message\"):\n                    message = choice.message\n                    yield self._process_message(message)\n\n            # Update usage if usage information is available in chunk\n            if hasattr(chunk, \"usage\") and chunk.usage is not None:\n                if hasattr(chunk.usage, \"completion_tokens\"):\n                    self.usage[\"completion_tokens\"] = chunk.usage.completion_tokens\n                if hasattr(chunk.usage, \"prompt_tokens\"):\n                    self.usage[\"prompt_tokens\"] = chunk.usage.prompt_tokens\n                if hasattr(chunk.usage, \"total_tokens\"):\n                    self.usage[\"total_tokens\"] = chunk.usage.total_tokens\n\n    def _process_delta(self, delta):\n        \"\"\"Process a delta response.\n\n        Args:\n            delta: The delta part of the response.\n\n        Returns:\n            The processed content.\n        \"\"\"\n        # Handle tool calls in delta\n        tool_message = None\n        if hasattr(delta, \"tool_calls\") and delta.tool_calls:\n            for tool_call in delta.tool_calls:\n                tool_id = getattr(tool_call, \"index\", 0)\n                # If the tool call is new, initialize it\n                if tool_id >= len(self.tool_calls_so_far):\n                    self.tool_calls_so_far.append({\n                        \"id\": getattr(tool_call, \"id\", \"\"),\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": \"\",\n                            \"arguments\": \"\"\n                        }\n                    })\n\n                # Update the tool call\n                if hasattr(tool_call, \"function\"):\n                    func = tool_call.function\n                    if hasattr(func, \"name\") and func.name:\n                        self.tool_calls_so_far[tool_id][\"function\"][\"name\"] = func.name\n                    if hasattr(func, \"arguments\") and func.arguments:\n                        self.tool_calls_so_far[tool_id][\"function\"][\"arguments\"] += func.arguments\n\n                # Create a message about the tool call being made\n                if self.first_response:\n                    if hasattr(tool_call, \"function\") and hasattr(tool_call.function, \"name\"):\n                        tool_message = f\"Making a function call to {tool_call.function.name}\"\n\n        # Handle content in delta\n        content = None\n        if hasattr(delta, \"content\") and delta.content is not None:\n            content = delta.content\n            self.content_so_far += content\n\n        # Handle function call in delta\n        if hasattr(delta, \"function_call\"):\n            func_call = delta.function_call\n            if hasattr(func_call, \"name\") and func_call.name and not self.function_name:\n                self.function_name = func_call.name\n                # If this is the first response, show function call being made\n                if self.first_response:\n                    content = f\"Making a function call to {self.function_name}\"\n            if hasattr(func_call, \"arguments\") and func_call.arguments:\n                self.function_call_so_far += func_call.arguments\n\n        self.first_response = False\n        return content or tool_message or \"\"\n\n    def _process_message(self, message):\n        \"\"\"Process a message response.\n\n        Args:\n            message: The message part of the response.\n\n        Returns:\n            The processed content.\n        \"\"\"\n        # Similar processing as delta but for message format\n        if hasattr(message, \"content\") and message.content is not None:\n            self.content_so_far = message.content\n            return message.content\n\n        if hasattr(message, \"function_call\") and message.function_call is not None:\n            func_call = message.function_call\n            if hasattr(func_call, \"name\"):\n                self.function_name = func_call.name\n            if hasattr(func_call, \"arguments\"):\n                self.function_call_so_far = func_call.arguments\n\n            return f\"Making a function call to {self.function_name}\"\n\n        return \"\"\n```\n\n----------------------------------------\n\nTITLE: Integrating Weave for LLM Function Logging\nDESCRIPTION: Demonstrates how to integrate Weave for logging and debugging LLM function calls using the weave.op() decorator.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-versioning.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nimport weave\n\nweave.init(\"my-project\")\n\n\n@weave.op()\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend some {genre} movies\")\ndef recommend_movies(genre: str): ...\n\n\nresponse = recommend_movies(\"sci-fi\")  # this will automatically get logged to Weave\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Using Pydub for Audio Inputs with String Templates in Python\nDESCRIPTION: Demonstrates handling audio inputs with string templates and the :audio tag for multi-modal LLM interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom pydub import AudioSegment\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template(\"\"\"\n    USER: Transcribe this audio: {audio:audio}\n\"\"\")\ndef transcribe_audio_prompt(audio: bytes) -> Messages.Type:\n    return locals()\n\n\n# Read an audio file\naudio_segment = AudioSegment.from_file(\"examples/data/sample.mp3\")\naudio_bytes = audio_segment.export(format=\"mp3\").read()\n\n# Use the audio in a prompt\nprint(transcribe_audio_prompt(audio=audio_bytes))\n# Returns a user message with both text and audio content\n\n```\n\n----------------------------------------\n\nTITLE: Comparing Generic vs. Engineered Prompts with Mirascope and OpenAI\nDESCRIPTION: This code demonstrates how using domain-specific terminology (\"kill confirm combo\", \"jab\") yields better responses from an LLM compared to generic phrasing when asking about the video game Super Smash Bros. Ultimate. It uses Mirascope's OpenAI integration with the gpt-4o-mini model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/common_phrases.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef call(query: str):\n    return query\n\n\ngeneric_response = call(\n    \"\"\"Does the roy in smash bros ultimate have a reliable way to knock out an\\\n        opponent that starts with the A button?\"\"\"\n)\nengineered_response = call(\n    \"\"\"In smash bros ultimate, what moves comprise Roy's kill confirm combo that\\\n        starts with jab?\"\"\"\n)\n\nprint(generic_response)\nprint(engineered_response)\n```\n\n----------------------------------------\n\nTITLE: Using the Book Recommendation Prompt in Python with Mirascope\nDESCRIPTION: This code snippet shows how to use the previously defined book_recommendation_prompt function to generate a formatted prompt string with a list of book titles.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprompt = book_recommendation_prompt(\n    [\"The Name of the Wind\", \"The Wise Man's Fears\"]\n)\n\nprint(prompt)\n# > I recently read these books: \"The Name of the Wind\", \"The Wise Man's Fears\"\n#   What should I read next?\n```\n\n----------------------------------------\n\nTITLE: Using OpenAICall in Mirascope V0\nDESCRIPTION: Example of using OpenAICall in Mirascope V0, which made it difficult to switch between LLM providers without significant code changes. This approach requires inheriting from a provider-specific class.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/mirascope-v1-release.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Mirascope tool integration with OpenAI call\nDESCRIPTION: This example demonstrates how Mirascope integrates tools (function calling) directly into the LLM call using the `@openai.call` decorator. This creates a cohesive unit of code, including the LLM call, tool definition, and call parameters. It highlights the ease of reuse and maintainability.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\ndef format_book(title: str, author: str) -> str:\n    \"\"\"Returns the title and author of a book nicely formatted.\n\n    Args:\n        title: The title of the book.\n        author: The author of the book in all caps.\n    \"\"\"\n    return f\"{title} by {author}\"\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    tools=[format_book],\n    call_params={\"tool_choice\": \"required\"},\n)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\ntool = recommend_book(\"fantasy\").tool  # `format_book` tool instance\nprint(tool.call())  # runs `format_book` with `title` and `author` args\n# > The Name of the Wind by PATRICK ROTHFUSS\n```\n\n----------------------------------------\n\nTITLE: Creating and Formatting a Basic PromptTemplate in Python\nDESCRIPTION: Demonstrates how to create a PromptTemplate object and format it with specific values. The template uses placeholders for 'dish' and 'flavor' to generate a recipe prompt.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = PromptTemplate.from_template(\n  \"Write a delicious recipe for {dish} with a {flavor} twist.\"\n)\n\n# Formatting the prompt with new content\nformatted_prompt = prompt_template.format(dish=\"pasta\", flavor=\"spicy\")\n\nprint(formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Caching with Base Message Parameters in Anthropic\nDESCRIPTION: Example of implementing message caching using base message parameters with the ':cache_control' tagged breakpoint. This approach allows for caching parts of your prompt to save tokens when making repeated calls to Anthropic's API. The cache control is added at line 11 and 28 in the highlighted example.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/anthropic.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/anthropic/caching/messages/base_message_param.py\"\n```\n\n----------------------------------------\n\nTITLE: Handling Errors in Custom Middleware for Mirascope in Python\nDESCRIPTION: Implements handler functions for errors occurring during Mirascope calls. It saves error information to the database and re-raises the exception.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/middleware.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef handle_error(error: Exception, fn: Callable, session: Session):\n    metadata = Metadata(\n        function_name=fn.__name__,\n        model=\"unknown\",\n        provider=\"unknown\",\n        response=\"\",\n        total_tokens=0,\n        prompt_tokens=0,\n        completion_tokens=0,\n        latency=0,\n        error=str(error),\n    )\n    session.add(metadata)\n    session.commit()\n    raise error\n\n\nasync def handle_error_async(error: Exception, fn: Callable, session: Session):\n    metadata = Metadata(\n        function_name=fn.__name__,\n        model=\"unknown\",\n        provider=\"unknown\",\n        response=\"\",\n        total_tokens=0,\n        prompt_tokens=0,\n        completion_tokens=0,\n        latency=0,\n        error=str(error),\n    )\n    session.add(metadata)\n    session.commit()\n    raise error\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Caching with String Templates in Anthropic\nDESCRIPTION: Example of implementing message caching using string templates with the ':cache_control' tagged breakpoint. This approach allows for caching parts of your prompt to save tokens when making repeated calls to Anthropic's API. The cache control is added at line 9 and 20 in the highlighted example.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/anthropic.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/anthropic/caching/messages/string_template.py\"\n```\n\n----------------------------------------\n\nTITLE: Building LangChain Retrieval Pipeline with Pipe Operator\nDESCRIPTION: Demonstrates how to create a LangChain pipeline that combines vector store retrieval, prompt templating, LLM interaction, and output parsing. Uses FAISS vector store and OpenAI's chat model.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nvectorstore = FAISS.from_texts(\n    [\"Julia is an expert in machine learning\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nmodel = ChatOpenAI()\n\nretrieval_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nretrieval_chain.invoke(\"what is Julia's expertise?\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Chatbot Class\nDESCRIPTION: Definition of the Chatbot class with conversation history management, prompt templating, and interactive loop functionality using OpenAI's GPT-4.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Define a Chatbot class that uses Pydantic for validation \nclass Chatbot(BaseModel):\n# Attribute to store the conversation history, containing both user and helper messages \n    history: list[openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a useful helper.\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    def _call(self, question: str): ...\n\n    # Main loop to handle user interaction \n    def run(self):\n        while True:\n            question = input(\"(User): \")\n# Exit condition: If the user types \"quit\" or \"exit\", terminate the loop \n            if question in [\"quit\", \"exit\"]:\n                print(\"(Bot): Have a great day!\")\n                break\n            \n     # Call the LLM with the user's question \n     stream = self._call(question)\n\n            print(f\"(User): {question}\", flush=True)\n     # Begin streaming the LLM's response in real-time\n            print(\"(Bot): \", end=\"\", flush=True)\n            for chunk, _ in stream:\n                print(chunk.content, end=\"\", flush=True)\n            print(\"\")\n            if stream.user_message_param:\n                self.history.append(stream.user_message_param)\n            self.history.append(stream.message_param)\n\nChatbot().run()\n```\n\n----------------------------------------\n\nTITLE: Connecting to Neo4j Database\nDESCRIPTION: This snippet establishes a connection to the Neo4j database using the GraphDatabase driver and verifies the connectivity. It also handles exceptions during connection attempts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-build-a-knowledge-graph.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndriver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n\ntry:\n    driver.verify_connectivity()\n    print(\"Connected to Neo4j!\")\nexcept Exception as e:\n    print(f\"Failed to connect to Neo4j: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Standard IO Server Connection Setup\nDESCRIPTION: Demonstrates how to set up a connection to an MCP server using stdio_client and StdioServerParameters.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/mcp/client.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/mcp/client.py:8:20\"\n```\n\n----------------------------------------\n\nTITLE: Computing ROUGE Scores\nDESCRIPTION: Demonstrates calculation of ROUGE-2 F1 scores to measure text similarity between reference and candidate texts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom rouge import Rouge\nreference = 'this book is great'\ncandidate = 'this book is great too'\nrouge = Rouge()\nscores = rouge.get_scores(candidate, reference)[0]\n['rouge-2']\n['f']\nprint(scores)\n> 0.8571428522448981\n```\n\n----------------------------------------\n\nTITLE: Printing the generated Knowledge Graph\nDESCRIPTION: This snippet demonstrates how to print the contents of the knowledge graph generated from the previous function. It displays nodes and edges to visualize relationships captured in the graph.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-build-a-knowledge-graph.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nprint(kg)\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Tools in Python with Mirascope v0 and v1\nDESCRIPTION: This snippet demonstrates how to implement streaming tools using Mirascope v0 and v1. It shows the transition from class-based approach to a more functional decorator-based approach, including tool definition and stream handling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICall, OpenAICallParams, OpenAIToolStream\n\n\ndef format_book(title: str, author: str):\n    \"\"\"Returns a formatted book string.\"\"\"\n    return f\"{title} by {author}\"\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend two (2) {genre} books.\"\n    genre: str\n\n    call_params = OpenAICallParams(tools=[format_book], tool_choice=\"required\")\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\ntool_stream = OpenAIToolStream.from_stream(recommender.stream())\nfor tool in tool_stream:\n    print(tool.call())\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\ndef format_book(title: str, author: str):\n    # docstring no longer required, but still used if supplied\n    return f\"{title} by {author}\"\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    stream=True,\n    tools=[format_book],\n    call_params={\"tool_choice\": \"required\"},\n)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend two (2) {genre} books\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Audio Quality using Gemini - Python\nDESCRIPTION: This snippet calls the Gemini API to analyze audio quality from a given URL, utilizing the AudioTag model to structure the expected response and print the analysis result.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/speech_transcription.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@gemini.call(model=\"gemini-1.5-flash\", response_model=AudioTag, json_mode=True)\n@prompt_template(\n    \"\"\"\n    Analyze this audio file\n    {url:audio}\n\n    Give me its audio quality (low, medium, high), a list of its audio flaws (if any),\n    a quick description of the content of the audio, and the primary sound in the audio.\n    Use the tool call passed into the API call to fill it out.\n    \"\"\"\n)\ndef analyze_audio(url: str): ...\n\nresponse = analyze_audio(apollo_url)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Creating Dummy Weather Data Tool\nDESCRIPTION: Implement a mock function to retrieve weather data for specific years\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/query_plan.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\\n\\n\\ndef get_weather_by_year(year: int):\\n    \"\"\"Made up data to get Tokyo weather by year\"\"\"\\n    if year == 2020:\\n        data = {\\n            \"jan\": 42,\\n            \"feb\": 43,\\n            \"mar\": 49,\\n            \"apr\": 58,\\n            \"may\": 66,\\n            \"jun\": 72,\\n            \"jul\": 78,\\n            \"aug\": 81,\\n            \"sep\": 75,\\n            \"oct\": 65,\\n            \"nov\": 55,\\n            \"dec\": 47,\\n        }\\n    # ... (additional year data implementations)\n```\n\n----------------------------------------\n\nTITLE: Audio Interaction with Tools and Turn Detection\nDESCRIPTION: Implementation combining audio interaction with tools and automatic turn detection for OpenAI's Realtime API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/openai.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/openai/realtime/audio_stream_input_output_tools.py\"\n```\n\n----------------------------------------\n\nTITLE: Example OpenTelemetry span output format\nDESCRIPTION: Demonstrates the JSON structure of an OpenTelemetry span created by Mirascope, including trace information, attributes, and events capturing both prompt and completion data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/integrations/otel.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"name\": \"recommend_book\",\n    \"context\": {\n        \"trace_id\": \"0x9397ede2f689a5b7767f0063d7b81959\",\n        \"span_id\": \"0xa22a446b2076ffe4\",\n        \"trace_state\": \"[]\"\n    },\n    \"kind\": \"SpanKind.INTERNAL\",\n    \"parent_id\": null,\n    \"start_time\": \"2024-10-04T05:16:47.340006Z\",\n    \"end_time\": \"2024-10-04T05:16:47.341937Z\",\n    \"status\": {\n        \"status_code\": \"UNSET\"\n    },\n    \"attributes\": {\n        \"gen_ai.system\": \"\",\n        \"gen_ai.request.model\": \"claude-3-5-sonnet-20240620\",\n        \"gen_ai.request.max_tokens\": 0,\n        \"gen_ai.request.temperature\": 0,\n        \"gen_ai.request.top_p\": 0,\n        \"gen_ai.response.model\": \"claude-3-5-sonnet-20240620\",\n        \"gen_ai.response.id\": \"msg_01X8sppiaZeErMjh8oCNz4ZA\",\n        \"gen_ai.response.finish_reasons\": [\n            \"end_turn\"\n        ],\n        \"gen_ai.usage.completion_tokens\": 268,\n        \"gen_ai.usage.prompt_tokens\": 12,\n        \"async\": false\n    },\n    \"events\": [\n        {\n            \"name\": \"gen_ai.content.prompt\",\n            \"timestamp\": \"2024-10-04T05:16:47.341924Z\",\n            \"attributes\": {\n                \"gen_ai.prompt\": \"{\\\"content\\\": \\\"Recommend a fantasy book\\\", \\\"role\\\": \\\"user\\\"}\"\n            }\n        },\n        {\n            \"name\": \"gen_ai.content.completion\",\n            \"timestamp\": \"2024-10-04T05:16:47.341931Z\",\n            \"attributes\": {\n                \"gen_ai.completion\": \"{\\\"content\\\": [{\\\"text\\\": \\\"There are many great fantasy books to choose from, but here's a popular recommendation:\\\\n\\\\n\\\\\\\"The Name of the Wind\\\\\\\" by Patrick Rothfuss\\\\n\\\\nThis is the first book in \\\\\\\"The Kingkiller Chronicle\\\\\\\" series. It's a beautifully written, engaging story that follows the life of Kvothe, a legendary wizard, warrior, and musician. The book is known for its rich world-building, complex magic system, and compelling characters.\\\\n\\\\nOther excellent fantasy recommendations include:\\\\n\\\\n1. \\\\\\\"The Lord of the Rings\\\\\\\" by J.R.R. Tolkien\\\\n2. \\\\\\\"A Game of Thrones\\\\\\\" by George R.R. Martin\\\\n3. \\\\\\\"The Way of Kings\\\\\\\" by Brandon Sanderson\\\\n4. \\\\\\\"The Lies of Locke Lamora\\\\\\\" by Scott Lynch\\\\n5. \\\\\\\"The Night Circus\\\\\\\" by Erin Morgenstern\\\\n6. \\\\\\\"Mistborn: The Final Empire\\\\\\\" by Brandon Sanderson\\\\n7. \\\\\\\"The Wizard of Earthsea\\\\\\\" by Ursula K. Le Guin\\\\n\\\\nThese books cater to different tastes within the fantasy genre, so you might want to read a brief synopsis of each to see which one appeals to you most.\\\", \\\"type\\\": \\\"text\\\"}], \\\"role\\\": \\\"assistant\\\"}\"\n            }\n        }\n    ],\n    \"links\": [],\n    \"resource\": {\n        \"attributes\": {\n            \"telemetry.sdk.language\": \"python\",\n            \"telemetry.sdk.name\": \"opentelemetry\",\n            \"telemetry.sdk.version\": \"1.22.0\",\n            \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Web Assistant\nDESCRIPTION: Instantiates and runs the WebAssistant to perform web searches and gather contextual information\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/web_search_agent.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nweb_assistant = WebAssistant()\nawait web_assistant.run()\n```\n\n----------------------------------------\n\nTITLE: Validating LLM Outputs Using Pydantic - Python\nDESCRIPTION: This snippet demonstrates how to validate the outputs of LLM calls using Pydantic's BaseModel for structured responses, ensuring that the outputs conform to predefined schemas.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-chaining.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom mirascope.core import llm, prompt_template\n\n\nclass EventDetails(BaseModel):\n    date: str\n    location: str\n\n@llm.call(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    response_model=EventDetails,\n)\n@prompt_template(\"Extract the event details from this text: {text}\")\ndef extract_event(text: str):\n    ...\n\nresponse = extract_event(text=\"The concert is on Saturday at Madison Square Garden.\")\nprint(response.date)      # Output: Saturday\nprint(response.location)  # Output: Madison Square Garden\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Call with Dynamic Parameters in Python\nDESCRIPTION: This snippet demonstrates how to configure an LLM call using dynamic parameters. It shows the usage of the 'call' decorator and how to set call parameters dynamically at runtime.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/calls.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"build/snippets/learn/calls/dynamic_configuration/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Lilypad with OpenAI SDK Integration\nDESCRIPTION: Shows how to use Lilypad's versioning capabilities with the OpenAI SDK directly, without using Mirascope.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# lily/recommend_book.py\nimport lilypad\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n\n@lilypad.llm_fn()\ndef recommend_book(genre: str) -> str:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n    )\n    return str(completion.choices[0].message.content)\n\n\nif __name__ == \"__main__\":\n    output = recommend_book(\"fantasy\")\n    print(output)\n```\n\n----------------------------------------\n\nTITLE: Chaining with Computed Fields - Python\nDESCRIPTION: This snippet demonstrates a chaining approach where the function returns a dynamic configuration containing computed fields, facilitating access to both the result and intermediate data in a single call.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -> str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -> BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(\"Translation:\", response.content)\nprint(\"\nComputed fields (including summary):\", response.dynamic_config[\"computed_fields\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing TitanStreamer for Amazon Titan model responses\nDESCRIPTION: A concrete implementation of BedrockStreamer for Amazon Titan models. It processes streaming responses from Titan models, extracting text content from each response chunk.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/bedrock/stream.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TitanStreamer(BedrockStreamer):\n    \"\"\"Streamer for Amazon Titan models.\"\"\"\n\n    def __call__(self, response_stream: Iterator[Dict[str, Any]]) -> Iterator[str]:\n        \"\"\"Process a streaming response from Amazon Bedrock for Titan models.\n\n        Args:\n            response_stream: The streaming response from Amazon Bedrock.\n\n        Returns:\n            An iterator of strings representing the processed response.\n        \"\"\"\n        for response in response_stream:\n            chunk = json.loads(response.get(\"chunk\", {}).get(\"bytes\", b\"{}\").decode(\"utf-8\"))\n            content = chunk.get(\"outputText\", \"\")\n            if content:\n                yield content\n```\n\n----------------------------------------\n\nTITLE: Response to Initial Multi-Turn Prompt\nDESCRIPTION: This demonstrates the AI's response to the initial multi-turn prompt. The AI asks a series of clarifying questions about the blog post's target audience, tone, key points, length, examples, structure, and purpose, demonstrating its engagement in an interactive process.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Sure, I'd be happy to help you write a blog post about prompt engineering! To get \\nstarted, I'll ask you a few clarifying questions to better understand your needs \\nand preferences:\\n\\n1. Target Audience: Who is the primary audience for this blog post? Are they AI \\nprofessionals, developers, beginners in AI, or a general audience interested \\nin technology?\\n\\n2. Tone and Style: What tone would you like the blog post to have? Should it \\nbe professional, casual, technical, or conversational?\\n\\n3. Key Points: Are there specific key points or topics you want to cover in \\nthe blog post? For example, definitions, examples, best practices, advanced \\ntechniques, NLP, etc.\\n\\n4. Length: How long do you want the blog post to be? (e.g., 500-700 words, \\n1000-1500 words)\\n\\n5.Examples: Do you have any specific examples or case studies you want to \\ninclude in the blog post?\\n\\n6. Structure: Do you have a preferred structure or format for the blog post? \\n(e.g., introduction, body, conclusion, headings, subheadings)\\n\\n7. Purpose: What is the main purpose of the blog post? Is it to inform, \\neducate, entertain, or persuade your audience?\\n\\nOnce I have the answers to these questions, I can better assist you in \\ncrafting a comprehensive and engaging blog post on prompt engineering.\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key\nDESCRIPTION: This snippet sets the OpenAI API key as an environment variable. It's essential for using OpenAI's services in the project.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/documentation_agent.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Exact Match Evaluation in Python\nDESCRIPTION: Demonstrates a simple hardcoded evaluation that checks for exact string matching between expected and actual answers. Returns a binary result (1.0 for match, 0.0 for mismatch).\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/evals.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/evals/hardcoded/exact_match.py\"\n```\n\n----------------------------------------\n\nTITLE: LangChain Runnable Pipeline with Bind Method\nDESCRIPTION: Shows how to use LangChain's Runnable.bind method to add runtime conditions to the pipeline, specifically demonstrating how to stop model execution when encountering a specific token.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrunnable = (\n    {\"equation_statement\": RunnablePassthrough()}\n    | prompt\n    | model.bind(stop=\"SOLUTION\")\n    | StrOutputParser()\n)\nprint(runnable.invoke(\"x raised to the third plus seven equals 12\"))\n```\n\n----------------------------------------\n\nTITLE: Migrating Async Calls in Mirascope from v0 to v1 in Python\nDESCRIPTION: Shows the transition from async calls in v0 to v1. The new approach uses an async function with a decorator, simplifying the code structure while maintaining asynchronous functionality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = asyncio.run(recommender.call_async())\nprint(response.content)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book.\"\n\n\nresponse = asyncio.run(recommend_book(\"fantasy\"))\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Using Wave Module for Audio Inputs with BaseMessageParam in Python\nDESCRIPTION: Shows how to handle audio inputs by directly creating BaseMessageParam instances with audio content using the wave module.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport wave\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.message_param import BaseMessageParam\nfrom mirascope.core.base.content_param import TextParam, AudioParam\n\n\n@prompt_template\ndef transcribe_audio_prompt(audio: bytes) -> Messages.Type:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=[\n                TextParam(\"Transcribe this audio: \"),\n                AudioParam(audio),\n            ],\n        )\n    ]\n\n\n# Read an audio file\nwith wave.open(\"examples/data/sample.wav\", \"rb\") as wav_file:\n    audio_bytes = wav_file.readframes(wav_file.getnframes())\n\n# Use the audio in a prompt\nprint(transcribe_audio_prompt(audio=audio_bytes))\n# Returns a user message with both text and audio content\n\n```\n\n----------------------------------------\n\nTITLE: Downloading HTML Article Using Curl in Python\nDESCRIPTION: This snippet performs a download of a Wikipedia article using the curl command to save it as 'wikipedia.html'. This file will serve as the input for creating the knowledge graph.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!curl https://en.wikipedia.org/wiki/Large_language_model -o wikipedia.html\n```\n\n----------------------------------------\n\nTITLE: Defining Clear Roles for Context Setting in Plaintext\nDESCRIPTION: This snippet shows how to define clear roles for the language model to assume, such as a fitness coach or novelist. It helps guide the model's tone, style, and output to better match the task at hand.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nYou are a professional fitness coach with extensive experience in helping individuals improve their endurance. Based on this expertise, create a personalized cardio workout plan that minimizes joint strain.\n\nYou are a novelist known for your vivid descriptions and deep character development. Write an engaging opening paragraph for a mystery novel, set in a small coastal town, where a sudden disappearance shocks the close-knit community.\n```\n\n----------------------------------------\n\nTITLE: Validating Structured Outputs in Python with Mirascope\nDESCRIPTION: Demonstrates adding custom validation logic to a `Pydantic` model, ensuring that outputs from Mirascope comply with specific constraints (e.g., all caps).\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/structured_outputs.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic import field_validator\n\n\nclass ValidatedBook(BaseModel):\n    title: str\n    author: str\n\n    @field_validator(\"title\", \"author\")\n    @classmethod\n    def must_be_uppercase(cls, v: str) -> str:\n        assert v.isupper(), \"All fields must be uppercase\"\n        return v\n\n\n@openai.call(\"gpt-4o-mini\", response_model=ValidatedBook)\ndef extract_all_caps_book(text: str) -> str:\n    return f\"Extract the book from this text: {text}\"\n\n\nbook = extract_all_caps_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n```\n\n----------------------------------------\n\nTITLE: Streaming Structured Information in Python with Mirascope v0 and v1\nDESCRIPTION: This snippet demonstrates how to stream structured information using Mirascope v0 and v1. It shows the transition from a class-based extractor with streaming to a function-based approach with streaming enabled through a decorator.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: type[Book] = Book\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nextractor = BookExtractor(genre=\"fantasy\")\nbook_stream = extractor.stream()\nfor partial_book in book_stream:\n    print(partial_book)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", stream=True, response_model=Book)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nbook_stream = recommend_book(\"fantasy\")\nfor partial_book in book_stream:\n    print(partial_book)\n```\n\n----------------------------------------\n\nTITLE: Defining PII and Creating Prompt for Detection\nDESCRIPTION: This snippet defines a PII definition and creates a Mirascope prompt template using `openai.call` and `prompt_template` decorators to check if an article contains PII. It uses a local Ollama instance as the model and specifies that the response should be a boolean value.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/pii_scrubbing.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom openai import OpenAI\n\nPII_DEFINITION = \"\"\"\nAny representation of information that permits the identity of an individual to whom \nthe information applies to be reasonably inferred by either direct or indirect means. \nFurther, PII is defined as information: (i) that directly identifies an \nindividual (e.g., name, address, social security number or other identifying \nnumber or code, telephone number, email address, etc.) or (ii) by which an agency \nintends to identify specific individuals in conjunction with other data elements, \ni.e., indirect identification. (These data elements may include a combination of gender, \nrace, birth date, geographic indicator, and other descriptors). Additionally, \ninformation permitting the physical or online contacting of a specific individual is \nthe same as personally identifiable information. This information can be maintained \nin either paper, electronic or other media.\n\"\"\"\n\n\n@openai.call(\n    model=\"llama3.1\",\n    client=OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n    json_mode=True,\n    response_model=bool,\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an expert at identifying personally identifiable information (PII).\n    Using the following definition of PII, \n    determine if the article contains PII with True or False?\n\n    Definition of PII: {PII_DEFINITION}\n\n    USER: {article}\n    \"\"\"\n)\ndef check_if_pii_exists(article: str) -> openai.OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"PII_DEFINITION\": PII_DEFINITION}}\n```\n\n----------------------------------------\n\nTITLE: Tracking Conversation History with Mirascope Using Message Context\nDESCRIPTION: This snippet demonstrates how to use Mirascope to track and inject conversation history between a user and an assistant. It creates a prompt that includes previous messages in the context using the MESSAGES variable.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-tools.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n@openai.call(model=\"gpt-4o\")\n@prompt_template(\"\"\"\n# {{MESSAGES}}\n# > (User): {{question}}\n# > (Assistant):\n\"\"\")\ndef ask(question: str, messages: list = None): ...\n\nresponse = ask(\"What did I just ask you?\", [\n    {\"role\": \"assistant\", \"content\": \"Hello! I'm an AI developed by OpenAI. How can I assist you today?\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n    {\"role\": \"assistant\", \"content\": \"You asked, \\\"Who are you?\\\" Is there anything else you'd like to know or discuss?\"}\n])\n\nprint(response)\n# > You asked, \"What did I just ask you?\"\n```\n\n----------------------------------------\n\nTITLE: Comparing Standard and Tab-CoT Responses with GPT-3.5\nDESCRIPTION: Demonstrates the difference between standard and Tab-CoT responses using GPT-3.5-turbo model. Shows how to structure a geometric problem with and without tabular formatting.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/tabular_chain_of_thought.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n@openai.call(model=\"gpt-3.5-turbo\")\ndef call(query: str) -> str:\n    return query\n\nprompt = \"\"\"\nA circle with radius 1 circumscribes (perfectly surrounds) an equilateral triangle.\nWhat's the area of the triangle?\n\"\"\"\ngeneric_response = call(prompt)\nengineered_response = call(f\"\"\"{prompt}. Explain your reasoning step by step,\nwith each step in a row in a markdown table.\"\"\")\n\nprint(generic_response)\nprint(\"\\n\\n\\n\")\nprint(engineered_response)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Information in Python with Mirascope v0 and v1\nDESCRIPTION: This snippet shows how to extract structured information using Mirascope v0 and v1. It demonstrates the transition from a class-based extractor to a function-based approach with decorators, using Pydantic models for type validation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: type[Book] = Book\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\n\nextractor = BookExtractor(genre=\"fantasy\")\nbook = extractor.extract()\nassert isinstance(book, Book)\nprint(book)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nbook = recommend_book(\"fantasy\")\nassert isinstance(book, Book)\nprint(book)\n```\n\n----------------------------------------\n\nTITLE: Implementing Reverse Chain of Thought with Mirascope in Python\nDESCRIPTION: A complete implementation of the Reverse Chain of Thought technique using Mirascope. This code defines functions for generating initial responses, reconstructing queries, decomposing problems, comparing conditions, and analyzing reasoning errors to improve model responses to mathematical problems.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/reverse_chain_of_thought.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom mirascope.core import openai, prompt_template\nfrom mirascope.core.openai import OpenAICallResponse\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom pydantic import BaseModel, Field\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef zero_shot_cot(query: str) -> str:\n    return f\"{query} Let's think step by step.\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    USER:\n    Give the concrete prompt (problem) that can generate this answer.\n    The problem should contain all basic and necessary information and correspond to the answer.\n    The problem can only ask for one result.\n\n    {response}\n    \"\"\"\n)\ndef reconstruct_query(response: str): ...\n\n\nclass Decomposition(BaseModel):\n    conditions: list[str] = Field(\n        ..., description=\"A list of conditions of the problem.\"\n    )\n\n\n@openai.call(\n    model=\"gpt-4o-mini\",\n    response_model=Decomposition,\n    call_params={\"tool_choice\": \"required\"},\n)\n@prompt_template(\n    \"\"\"\n    Please list the conditions of the problem. There may be multiple conditions.\n    Do not list conditions not related to calculations, but list all necessary conditions.\n    The format should be a list of conditions with one condition per item.\n\n    {query}\n    \"\"\"\n)\nasync def decompose_query(query: str): ...\n\n\nclass Comparison(BaseModel):\n    condition: str = Field(\n        ..., description=\"The original condition the comparison was made with, verbatim\"\n    )\n    deducible: bool = Field(\n        ...,\n        description=\"Whether the condition is deducible from the list of other conditions.\",\n    )\n    illustration: str = Field(\n        ...,\n        description=\"A quick illustration of the reason the condition is/isn't deducible from the list of other conditions.\",\n    )\n\n\n@openai.call(\n    model=\"gpt-4o-mini\",\n    response_model=Comparison,\n    call_params={\"tool_choice\": \"required\"},\n)\n@prompt_template(\n    \"\"\"\n    Given a candidate condition: '{condition}'\n\n    Here is a condition list: '{condition_list}'\n\n    From a mathematical point of view, can this candidate condition be deduced from the condition list?\n    Please illustrate your reason and answer True or False.\n    \"\"\"\n)\nasync def compare_conditions(condition: str, condition_list: list[str]): ...\n\n\n@openai.call(\n    model=\"gpt-4o-mini\", response_model=bool, call_params={\"tool_choice\": \"required\"}\n)\n@prompt_template(\n    \"\"\"\n    Q1: {original_problem}\n    Q2: {reconstructed_problem}\n    \n    From a mathematical point of view, are these two problems asking the same thing at the end?\n    \"\"\"\n)\ndef compare_questions(original_problem: str, reconstructed_problem: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER:\n    {mistakes_prompt}\n    {overlooked_prompt}\n    {hallucination_prompt}\n    {misinterpretation_prompt}\n    \"\"\"\n)\nasync def fine_grained_comparison(\n    history: list[ChatCompletionMessageParam], query: str, reconstructed_query: str\n) -> openai.OpenAIDynamicConfig:\n    # Decompose both queries into conditions\n    original_conditions, reconstructed_conditions = [\n        response.conditions\n        for response in await asyncio.gather(\n            decompose_query(query), decompose_query(reconstructed_query)\n        )\n    ]\n\n    # Identify overlooked/hallucinated conditions and misinterpretation of question\n    overlooking_tasks = [\n        compare_conditions(original_condition, reconstructed_conditions)\n        for original_condition in original_conditions\n    ]\n    hallucination_tasks = [\n        compare_conditions(reconstructed_condition, original_conditions)\n        for reconstructed_condition in reconstructed_conditions\n    ]\n    full_comparison = await asyncio.gather(*(overlooking_tasks + hallucination_tasks))\n\n    question_misinterpretation = compare_questions(query, reconstructed_query)\n\n    overlooked_comparisons = [\n        comparison\n        for comparison in full_comparison[: len(original_conditions)]\n        if not comparison.deducible\n    ]\n    hallucination_comparisons = [\n        comparison\n        for comparison in full_comparison[len(original_conditions) :]\n        if not comparison.deducible\n    ]\n\n    # Fill out prompt depending on the comparisons\n    if (\n        not question_misinterpretation\n        and not overlooked_comparisons\n        and not hallucination_comparisons\n    ):\n        mistakes_prompt = \"\"\"There are no mistakes in your interpretation of the prompt.\n        Repeat your original solution verbatim.\"\"\"\n        overlooked_prompt = \"\"\n        hallucination_prompt = \"\"\n        misinterpretation_prompt = \"\"\n    else:\n        mistakes_prompt = (\n            \"Here are the mistakes and reasons in your answer to the problem.\\n\"\n        )\n\n        if overlooked_comparisons:\n            conditions = [comparison.condition for comparison in overlooked_comparisons]\n            illustrations = [\n                comparison.illustration for comparison in overlooked_comparisons\n            ]\n            overlooked_prompt = f\"\"\"\n            Overlooked Conditions:\n            You have ignored some real conditions:\n            {conditions}\n            The real problem has the conditions:\n            {original_conditions}\n            You should consider all real conditions in the problem.\n            Here are the detailed reasons:\n            {illustrations}\"\"\"\n        else:\n            overlooked_prompt = \"\"\n\n        if hallucination_comparisons:\n            conditions = [\n                comparison.condition for comparison in hallucination_comparisons\n            ]\n            illustrations = [\n                comparison.illustration for comparison in overlooked_comparisons\n            ]\n            hallucination_prompt = f\"\"\"\n            Hallucinated Conditions\n            You use some wrong candidate conditions:\n            {conditions}\n            They all can not be deduced from the true condition list.\n            The real problem has the conditions:\n            {original_conditions}\n            You should consider all real conditions in the problem.\n            Here are the detailed reasons:\n            {illustrations}\"\"\"\n        else:\n            hallucination_prompt = \"\"\n\n        if question_misinterpretation:\n            misinterpretation_prompt = f\"\"\"\n            You misunderstood the question.\n            You think the question is: {reconstructed_query}.\n            But the real question is: {query}\n            They are different. You should consider the original question.\"\"\"\n        else:\n            misinterpretation_prompt = \"\"\n    return {\n        \"computed_fields\": {\n            \"mistakes_prompt\": mistakes_prompt,\n            \"overlooked_prompt\": overlooked_prompt,\n            \"hallucination_prompt\": hallucination_prompt,\n            \"misinterpretation_prompt\": misinterpretation_prompt,\n        }\n    }\n\n\nasync def reverse_cot(query: str) -> OpenAICallResponse:\n    cot_response = zero_shot_cot(query=query)\n    reconstructed_query_response = reconstruct_query(cot_response.content)\n    history = cot_response.messages + reconstructed_query_response.messages\n    response = await fine_grained_comparison(\n        history=history,\n        query=query,\n        reconstructed_query=reconstructed_query_response.content,\n    )\n    return response\n\n\nquery = \"\"\"At the trip to the county level scavenger hunt competition 90 people \\\nwere required to split into groups for the competition to begin. To break \\\npeople up into smaller groups with different leaders 9-person groups were \\\nformed. If 3/5 of the number of groups each had members bring back 2 seashells each \\\nhow many seashells did they bring?\"\"\"\n\nprint(await reverse_cot(query=query))\n```\n\n----------------------------------------\n\nTITLE: Text Summarization using LLM Prompt\nDESCRIPTION: This code snippet shows a prompt for text summarization using a large language model. The prompt instructs the model to create an outline with a nested structure of major topics and subpoints, then generate a summary based on that outline. The complexity of the outline corresponds to the length and importance of each major point in the text.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-applications.md#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nSummarize the following text by first creating an outline with a nested structure, listing all major topics in the text with subpoints for each of the major points. The number of subpoints for each topic should correspond to the length and importance of the major point within the text. Then create the actual summary using the outline.\n{text}\n```\n\n----------------------------------------\n\nTITLE: Dumping Call Information in Python with Mirascope v0 and v1\nDESCRIPTION: This snippet illustrates how to dump call information using Mirascope v0 and v1. It shows the transition from using a custom dump method to using the standard Pydantic model_dump method for serializing response data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nprint(response.dump())\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.model_dump())\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with Tenacity Support\nDESCRIPTION: Shows how to install Mirascope with Tenacity integration for retry functionality\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/retries.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install \"mirascope[tenacity]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Global DynamicConfig Instance in Python\nDESCRIPTION: This snippet creates a global instance of the DynamicConfig class for use throughout the Mirascope project. It allows for centralized management of Anthropic API configurations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/anthropic/dynamic_config.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndynamic_config = DynamicConfig()\n```\n\n----------------------------------------\n\nTITLE: Collecting Errors with Tenacity in Python\nDESCRIPTION: This function is a decorator that collects errors encountered during retries when using Tenacity. It stores the errors in a list and passes them to the decorated function as an additional argument.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/integrations/tenacity.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Callable, List, Any\nfrom tenacity import RetryCallState\n\n\ndef collect_errors(func: Callable) -> Callable:\n    \"\"\"Collect errors encountered during retries.\n\n    This decorator collects errors encountered during retries and passes them\n    to the decorated function as an additional argument.\n\n    Args:\n        func: The function to decorate.\n\n    Returns:\n        The decorated function.\n    \"\"\"\n\n    def before_sleep(retry_state: RetryCallState) -> None:\n        \"\"\"Collect errors before sleep.\"\"\"\n        errors = getattr(retry_state, \"errors\", [])\n        errors.append(retry_state.outcome.exception())\n        setattr(retry_state, \"errors\", errors)\n\n    def after(retry_state: RetryCallState) -> Any:\n        \"\"\"Pass collected errors to the decorated function.\"\"\"\n        errors: List[Exception] = getattr(retry_state, \"errors\", [])\n        return retry_state.fn(*retry_state.args, errors=errors, **retry_state.kwargs)\n\n    def wrap(*args: Any, **kwargs: Any) -> Any:\n        \"\"\"Wrap the decorated function.\"\"\"\n        return func(*args, **kwargs)\n\n    wrap.before_sleep = before_sleep  # type: ignore\n    wrap.after = after  # type: ignore\n    return wrap\n```\n\n----------------------------------------\n\nTITLE: Executing and Evaluating Code in Python with Safety Checks\nDESCRIPTION: Implements a function to execute Python code, utilizing a safety check beforehand. If the code is safe, it attempts execution and returns the result or errors. Requires previous implementation of 'evaluate_code_safety'. Expects a code string as input, potentially producing output or error messages as a result.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/code_generation_and_execution.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"python\\ndef execute_code(code: str):\\n    \\\"\\\"\\\"Execute Python code and return the output.\\\"\\\"\\\"\\n    is_code_safe = evaluate_code_safety(code)\\n    if not is_code_safe:\\n        return f\\\"Error: The code: {code} is not safe to execute.\\\"\\n    try:\\n        local_vars = {}\\n        exec(code, globals(), local_vars)\\n        if \\\"result\\\" in local_vars:\\n            return local_vars[\\\"result\\\"]\\n    except Exception as e:\\n        print(e)\\n        return f\\\"Error: {str(e)}\\\"\\n\\n\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Mirascope and Testing\nDESCRIPTION: This snippet installs the necessary packages for using Mirascope with OpenAI, as well as testing tools and LlamaIndex.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n# for testing and llama index\n!pip install  ipytest pytest llama-index\n```\n\n----------------------------------------\n\nTITLE: Using Wave Module for Audio Inputs with Shorthand Method in Python\nDESCRIPTION: Shows how to handle audio inputs in prompts using the wave module and shorthand method for multi-modal interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport wave\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.content_param import AudioParam\n\n\n@prompt_template\ndef transcribe_audio_prompt(audio: bytes) -> Messages.Type:\n    return f\"Transcribe this audio: {AudioParam(audio)}\"\n\n\n# Read an audio file\nwith wave.open(\"examples/data/sample.wav\", \"rb\") as wav_file:\n    audio_bytes = wav_file.readframes(wav_file.getnframes())\n\n# Use the audio in a prompt\nprint(transcribe_audio_prompt(audio=audio_bytes))\n# Returns a user message with both text and audio content\n\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI integration\nDESCRIPTION: This code snippet installs the Mirascope library along with the necessary integration for OpenAI. It uses pip to install the package as specified.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/quickstart.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex Settings with Ollama and Hugging Face for Local LLM Execution\nDESCRIPTION: Sets up global LlamaIndex settings to use a local Llama 3.1 model via Ollama for LLM tasks and configures a Hugging Face embedding model for semantic processing of documents.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import (\n    Settings,\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n)\nfrom llama_index.legacy.embeddings import HuggingFaceEmbedding\nfrom llama_index.legacy.llms import Ollama\n\n# Configure global settings for llama-index\nSettings.llm = Ollama(model=\"llama3.1\")  # Using a local Llama 3.1 (8B) model via Ollama\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Custom Client with Decorator Parameter\nDESCRIPTION: Shows how to pass a custom client to the `call` decorator for different LLM providers, enabling specific client configurations and authentication methods\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/calls.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/calls/provider_specific/custom_client/decorator/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Automatic Prompt Versioning with Lilypad in Python\nDESCRIPTION: This snippet demonstrates how to use Lilypad to automatically version prompt changes when making LLM calls. The decorator @lilypad.generation() tracks every call to the function, recording inputs, outputs, and other metadata to maintain a full history of prompt iterations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport lilypad\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n@lilypad.generation()\ndef answer_question(question: str) -> str:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": question}],\n    )\n    return str(completion.choices[0].message.content)\n \n \nif __name__ == \"__main__\":\n    lilypad.configure()\n    answer = answer_question(\"What is the tallest building in the world?\")\n```\n\n----------------------------------------\n\nTITLE: Customizing PromptTemplate in LlamaIndex\nDESCRIPTION: This snippet demonstrates how to use and customize the PromptTemplate class in LlamaIndex. It shows creating a template string, instantiating a PromptTemplate, and formatting it for both completion and chat APIs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import PromptTemplate\n\ntemplate = (\n    \"We have provided context information below. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given this information, please answer the question: {query_str}\\n\"\n)\nqa_template = PromptTemplate(template)\n\n# you can create text prompt (for completion API)\nprompt = qa_template.format(context_str=..., query_str=...)\n\n# or easily convert to message prompts (for chat API)\nmessages = qa_template.format_messages(context_str=..., query_str=...)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Simple LangChain Runnable in Python\nDESCRIPTION: This example demonstrates how to create a runnable from a simple greeting function and use it to invoke the function. It shows the basic structure of wrapping a function in a RunnableLambda and calling it with the invoke method.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-runnables.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.runnables import RunnableLambda\n\n# Define a simple function\ndef greet(name):\n   return f\"Hello, {name}!\"\n\n# Wrap the function in a RunnableWrapper\ngreet_runnable = RunnableLambda(lambda x: greet(x))\n\n# Use the runnable to call the function\nresult = greet_runnable.invoke(\"Alice\")\nprint(result)  # Output: Hello, Alice!\n```\n\n----------------------------------------\n\nTITLE: DAG Structure for Customer Service Automation in Prompt Flow\nDESCRIPTION: Illustrates the structure of a Directed Acyclic Graph (DAG) used in Microsoft Prompt Flow for a customer service automation system. Shows how nodes are connected in sequence to process customer queries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nQuery Reception → Language Detection → Emotional Tone Analysis → Query Categorization → Automated Response or Escalation to Human Agent\n```\n\n----------------------------------------\n\nTITLE: Executing Run Function to Print the Result\nDESCRIPTION: This snippet calls the previously defined run function, passing the question and the generated knowledge graph to print the result.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(run(question, kg))\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Response Management\nDESCRIPTION: This snippet describes the implementation of CustomProviderStream class that inherits from BaseStream, with methods to evaluate stream cost and construct message parameters.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/custom_provider.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core.base import BaseStream\n\nclass CustomProviderStream(BaseStream):\n    # Implement abstract methods and properties\n    @property\n    def cost(self) -> float | None:\n        # Calculate and return the cost of the stream\n\n    def _construct_message_param(self, tool_calls: list | None = None, content: str | None = None):\n        # Construct and return the message parameter\n\n    def construct_call_response(self) -> CustomProviderCallResponse:\n        # Construct and return the call response\n```\n\n----------------------------------------\n\nTITLE: Code Formatting and Linting Commands\nDESCRIPTION: Commands to format and lint code using ruff and pyright.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nuv run ruff format .\nuv run ruff check .\nuv run pyright .\n```\n\n----------------------------------------\n\nTITLE: Initial Multi-Turn Prompt\nDESCRIPTION: This is an example of initial prompt for multi-turn conversation. The user indicates the task (writing a blog post) and requests the AI to ask clarifying questions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\n\"I need help writing a blog post about prompt engineering. \\n\\nBefore we get started, please ask clarifying questions so you can best help me.\"\n```\n\n----------------------------------------\n\nTITLE: Defining LocalizedRecommenderBase class with API tools\nDESCRIPTION: This class defines the base structure for the recommender, including methods for interacting with Google Maps API via Nimble, getting current date, and geocoding locations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/localized_agent.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom datetime import datetime\n\nimport aiohttp\nimport requests\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel\n\nNIMBLE_TOKEN = \"YOUR_NIMBLE_API_KEY\"\n\n\nclass LocalizedRecommenderBase(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    async def _nimble_google_maps_places(\n        self, session: aiohttp.ClientSession, place_id: str\n    ):\n        \"\"\"\n        Use Nimble to get the details of a place on Google Maps.\n        \"\"\"\n        url = \"https://api.webit.live/api/v1/realtime/serp\"\n        headers = {\n            \"Authorization\": f\"Basic {NIMBLE_TOKEN}\",\n            \"Content-Type\": \"application/json\",\n        }\n        place_data = {\n            \"parse\": True,\n            \"search_engine\": \"google_maps_place\",\n            \"place_id\": place_id,\n            \"domain\": \"com\",\n            \"format\": \"json\",\n            \"render\": True,\n            \"country\": \"US\",\n            \"locale\": \"en\",\n        }\n        async with session.get(url, json=place_data, headers=headers) as response:\n            data = await response.json()\n            result = data[\"parsing\"][\"entities\"][\"Place\"][0]\n            return {\n                \"opening_hours\": result.get(\"place_information\", {}).get(\n                    \"opening_hours\", \"\"\n                ),\n                \"rating\": result.get(\"rating\", \"\"),\n                \"name\": result.get(\"title\", \"\"),\n            }\n\n    async def _nimble_google_maps(self, latitude: float, longitude: float, query: str):\n        \"\"\"\n        Use Nimble to search for places on Google Maps.\n        \"\"\"\n        url = \"https://api.webit.live/api/v1/realtime/serp\"\n        headers = {\n            \"Authorization\": f\"Basic {NIMBLE_TOKEN}\",\n            \"Content-Type\": \"application/json\",\n        }\n        search_data = {\n            \"parse\": True,\n            \"search_engine\": \"google_maps_search\",\n            \"query\": query,\n            \"coordinates\": {\"latitude\": latitude, \"longitude\": longitude},\n            \"domain\": \"com\",\n            \"format\": \"json\",\n            \"render\": True,\n            \"country\": \"US\",\n            \"locale\": \"en\",\n        }\n        search_response = requests.post(url, headers=headers, json=search_data)\n        search_json_response = search_response.json()\n        search_results = [\n            {\n                \"place_id\": result.get(\"place_id\", \"\"),\n            }\n            for result in search_json_response[\"parsing\"][\"entities\"][\"SearchResult\"]\n        ]\n        results = []\n        async with aiohttp.ClientSession() as session:\n            tasks = [\n                self._nimble_google_maps_places(session, results.get(\"place_id\", \"\"))\n                for results in search_results\n            ]\n            results = await asyncio.gather(*tasks)\n        return results\n\n    async def _get_current_date(self):\n        \"\"\"Get the current date and time.\"\"\"\n        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    async def _get_coordinates_from_location(self, location_name: str):\n        \"\"\"Get the coordinates of a location.\"\"\"\n        base_url = \"https://nominatim.openstreetmap.org/search\"\n        params = {\"q\": location_name, \"format\": \"json\", \"limit\": 1}\n        headers = {\"User-Agent\": \"mirascope/1.0\"}\n        response = requests.get(base_url, params=params, headers=headers)\n        data = response.json()\n\n        if data:\n            latitude = data[0].get(\"lat\")\n            longitude = data[0].get(\"lon\")\n            return f\"Latitude: {latitude}, Longitude: {longitude}\"\n        else:\n            return \"No location found, ask me about a specific location.\"\n```\n\n----------------------------------------\n\nTITLE: Defining an Audio Tag Model using Pydantic - Python\nDESCRIPTION: This code snippet defines a Pydantic model named AudioTag to validate and structure audio quality information for analysis, including fields for quality, imperfections, description, and primary sound.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/speech_transcription.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\n\nclass AudioTag(BaseModel):\n    audio_quality: Literal[\"Low\", \"Medium\", \"High\"] = Field(\n        ...,\n        description=\"\"\"The quality of the audio file.\n        Low - unlistenable due to severe static, distortion, or other imperfections\n        Medium - Audible but noticeable imperfections\n        High - crystal clear sound\"\"\",\n    )\n    imperfections: list[str] = Field(\n        ...,\n        description=\"\"\"A list of the imperfections affecting audio quality, if any.\n        Common imperfections are static, distortion, background noise, echo, but include\n        all that apply, even if not listed here\"\"\",\n    )\n    description: str = Field(\n        ..., description=\"A one sentence description of the audio content\"\n    )\n    primary_sound: str = Field(\n        ...,\n        description=\"\"\"A quick description of the main sound in the audio,\n        e.g. `Male Voice`, `Cymbals`, `Rainfall`\"\"\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Serializing OpenAI API Call Parameters in Python\nDESCRIPTION: This snippet demonstrates the structure of OpenAI API call parameters, including messages, temperature settings, and model selection. It showcases how these parameters are organized for a conversation with a music producer AI.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-versioning.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#             \"content\": \"I'm working on a new song. What do you think?\\nI woke up feeling oh so great, I cooked some food that i ate\",\n#         },\n#     ],\n#     \"call_params\": {\"temperature\": 0.4},\n#     \"call_kwargs\": {\n#         \"temperature\": 0.4,\n#         \"model\": \"gpt-4o-mini\",\n#         \"messages\": [\n#             {\"role\": \"system\", \"content\": \"You are an acclaimed music producer.\"},\n#             {\n#                 \"role\": \"user\",\n#                 \"content\": \"I'm working on a new song. What do you think?\\nI woke up feeling oh so great, I cooked some food that i ate\",\n#             },\n#         ],\n#     },\n#     \"user_message_param\": {\n#         \"content\": \"I'm working on a new song. What do you think?\\nI woke up feeling oh so great, I cooked some food that i ate\",\n#         \"role\": \"user\",\n#     },\n#     \"start_time\": 1723225844872.8289,\n#     \"end_time\": 1723225846620.729,\n#     \"message_param\": {\n#         \"content\": 'That's a solid start! The positive vibe in those lines is infectious. You might want to build on that feeling of joy and satisfaction. Consider adding some imagery or details about the food or the morning to create a more vivid scene. Here's a suggestion for how you might expand it:\\n\\n\"I woke up feeling oh so great,  \\nSunshine streaming, can\\'t be late,  \\nI cooked some food that I ate,  \\nPancakes flipping, can't wait!\"\\n\\nThis keeps the upbeat energy and adds a bit more texture. What direction are you thinking of taking the song?',\n#         \"refusal\": None,\n#         \"role\": \"assistant\",\n#         \"tool_calls\": None,\n#     },\n#     \"tools\": None,\n#     \"tool\": None,\n# }\n```\n\n----------------------------------------\n\nTITLE: LangChain Runnable binding example\nDESCRIPTION: This snippet illustrates how to associate model information with a prompt in LangChain using `Runnable.bind`. It shows the creation of a prompt template, model, and runnable chain.  The key difference highlighted is that the binding isn't tied to the prompt function like Mirascope's `call_params` argument.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\n\nfunction = {\n    \"name\": \"solver\",\n    \"description\": \"Formulates and solves an equation\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"equation\": {\n                \"type\": \"string\",\n                \"description\": \"The algebraic expression of the equation\",\n            },\n            \"solution\": {\n                \"type\": \"string\",\n                \"description\": \"The solution to the equation\",\n            },\n        },\n        \"required\": [\"equation\", \"solution\"],\n    },\n}\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"Translate the given word problem into a mathematical equation and solve it.\",\n        ),\n        (\"human\", \"{equation_statement}\"),\n    ]\n)\nmodel = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.4).bind(\n    function_call={\"name\": \"solver\"}, functions=[function]\n)\nrunnable = {\"equation_statement\": RunnablePassthrough()} | prompt | model\n\nrunnable.invoke(\"the square root of a number plus five is equal to eight\")\n```\n\n----------------------------------------\n\nTITLE: Expected response for Chain-of-Thought Prompting\nDESCRIPTION: This is the expected response for the CoT prompt, demonstrating the LLM's reasoning process step-by-step to solve the math problem. It shows the step-by-step calculations and the final answer, making the solution transparent and easy to follow.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\n\"1. Start with the initial number of apples: 150\\n2. Subtract the apples sold in the morning: 150 - 30 = 120\\n3. Subtract the apples sold in the afternoon: 120 - 50 = 70\\n4. The final count of apples left is 70.\"\n```\n\n----------------------------------------\n\nTITLE: Example Prompt Evaluation Task\nDESCRIPTION: Demonstrates an evaluation task for assessing engagement and clarity in LLM responses to scientific prompts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nTask : Evaluate engagement and clarity\n\nPrompt = \"Describe the role of photosynthesis in plant growth.\"  \n\nResponse = \"Photosynthesis helps plants grow by using sunlight to make food, which they need to survive and stay healthy.\"\n```\n\n----------------------------------------\n\nTITLE: Using FromCallArgs with OpenAI in Python\nDESCRIPTION: This code snippet illustrates the use of FromCallArgs in Mirascope with OpenAI. It shows how to create a response model that includes input validation using FromCallArgs, ensuring that the LLM's output matches the function input.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/response_models.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import BaseModel, Field, FromCallArgs, Prompt\nfrom mirascope.providers import OpenAI\n\nclass Item(BaseModel):\n    name: str\n    price: float\n\nclass Response(BaseModel):\n    item: Item = Field(description=\"The item being described\")\n    description: str = Field(description=\"A description of the item\")\n    original_price: float = Field(\n        default_factory=FromCallArgs(\"item.price\"),\n        description=\"The original price of the item\",\n    )\n\nprompt = Prompt(\n    \"Describe this item: {item}\",\n    response_model=Response,\n)\n\nitem = Item(name=\"Widget\", price=9.99)\nresponse = prompt.openai(\n    model=\"gpt-3.5-turbo\",\n    provider=OpenAI(),\n    item=item,\n)\n\nprint(f\"Description: {response.description}\")\nprint(f\"Original price: {response.original_price}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Qwant Search Agent\nDESCRIPTION: Imports necessary Python libraries including requests, BeautifulSoup, Pydantic, and Mirascope components.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport re\nimport time\nfrom typing import Any, Callable\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom pydantic import BaseModel, Field\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nfrom mirascope.core import prompt_template\nfrom mirascope.core.groq import groq_call\n```\n\n----------------------------------------\n\nTITLE: Retrieving Mirascope Documentation\nDESCRIPTION: This code defines a function `get_documents` that retrieves relevant documentation from Mirascope based on a given query. It utilizes `llama_index` to load an index from storage, retrieve relevant nodes using a `VectorIndexRetriever`, and then reranks the results using `llm_query_rerank` function (not defined in the snippet). Finally, it returns a list of document strings.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal, cast\n\nfrom llama_index.core import (\n    QueryBundle,\n    load_index_from_storage,\n)\nfrom llama_index.core.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.storage import StorageContext\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nloaded_index = load_index_from_storage(storage_context)\n\n\ndef get_documents(query: str) -> list[str]:\n    \"\"\"The get_documents tool that retrieves Mirascope documentation based on the\n    relevance of the query\"\"\"\n    query_bundle = QueryBundle(query)\n    retriever = VectorIndexRetriever(\n        index=cast(VectorStoreIndex, loaded_index),\n        similarity_top_k=10,\n    )\n    retrieved_nodes = retriever.retrieve(query_bundle)\n    choice_batch_size = 5\n    top_n = 2\n    results: list[Relevance] = []\n    for idx in range(0, len(retrieved_nodes), choice_batch_size):\n        nodes_batch = [\n            {\n                \"id\": idx + id,\n                \"text\": node.node.get_text(),  # pyright: ignore[reportAttributeAccessIssue]\n                \"document_title\": node.metadata[\"document_title\"],\n                \"semantic_score\": node.score,\n            }\n            for id, node in enumerate(retrieved_nodes[idx : idx + choice_batch_size])\n        ]\n        results += llm_query_rerank(nodes_batch, query)\n    results = sorted(results, key=lambda x: x.score or 0, reverse=True)[:top_n]\n\n    return [result.document for result in results]\n```\n\n----------------------------------------\n\nTITLE: Setting up Test Conversation for LLM Development Tool Evaluation in Python\nDESCRIPTION: This code snippet defines a test conversation structure to evaluate LLM development tools. It includes user queries, assistant responses, web search results, and content extraction requests. The conversation simulates a software engineer looking for LLM development tool libraries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_web_search_agent.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntest_conversation_messages = [\n    {\"role\": \"user\", \"content\": \"I am a SWE looking for a LLM dev tool library\"},\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"queries\":[\"best LLM development tools\",\"top libraries for LLM development\",\"LLM libraries for software engineers\",\"LLM dev tools for machine learning\",\"most popular libraries for LLM development\"]}',\n                    \"name\": \"_web_search\",\n                },\n                \"id\": \"call_1\",\n            }\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"https://www.blog.aiport.tech/p/top-9-libraries-to-accelerate-llm\\n\\nhttps://github.com/tensorchord/awesome-llmops\\n\\nhttps://www.blog.aiport.tech/p/top-9-libraries-to-accelerate-llm\\n\\nhttps://medium.com/pythonforall/essential-python-libraries-for-llms-and-application-development-in-2024-17c64b672421\\n\\nhttps://www.datacamp.com/blog/top-open-source-llms\\n\\nhttps://machinelearningmastery.com/5-essential-free-tools-getting-started-llms/\\n\\nhttps://github.com/princeton-nlp/swe-agent\\n\\nhttps://arxiv.org/html/2407.01489v1\\n\\nhttps://www.datacamp.com/blog/top-open-source-llms\\n\\nhttps://llmmodels.org/blog/top-10-open-source-llm-frameworks-2024/\",\n        \"tool_call_id\": \"call_1\",\n        \"name\": \"_web_search\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://research.aimultiple.com/llmops-tools/\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_UXnnadcCnki8qvCxrzRI1fXA\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://www.techradar.com/computing/artificial-intelligence/best-large-language-models-llms-for-coding\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_S0OnNQqxtPH5HtDb1buzjedV\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://www.blog.aipo.rt.tech/p/top-9-libraries-to-accelerate-llm\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_aCyaCFXUWMWloDkETTrxHyoJ\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://medium.com/pythonforall/essential-python-libraries-for-llms-and-application-development-in-2024-17c64b672421\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_71e9AkvKjIuEp3QceqO4DCUK\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://github.com/tensorchord/awesome-llmops\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_YeaR70E6l7iM7UHEtp709iVc\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://medium.com/@bijit2111987/top-llm-dev-tool-and-when-to-use-them-in-your-ai-stack-622a651ec0e6\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_UWuyM3dy71Js7fspwSKnMlGC\",\n            },\n        ],\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Testing Module Import Verification\nDESCRIPTION: This code snippet uses `pytest.mark.parametrize` to define a series of test cases for the `is_importable` function. Each test case provides an import string and the expected boolean result. The `test_is_importable` function then asserts that the actual result of `is_importable` matches the expected result.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_documentation_agent.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize(\n    \"import_str,expected\",\n    [\n        (\"from mirascope.core import openai\", True),\n        (\"import math\", True),\n        (\"from datetime import datetime\", True),\n        (\"import non_existent_module\", False),\n        (\"from os import path, nonexistent_function\", False),\n        (\"from sys import exit, nonexistent_variable\", False),\n        (\"from openai import OpenAI\", True),\n        (\"from mirascope.core import openai\", True),\n    ],\n)\ndef test_is_importable(import_str: str, expected: bool):\n    assert is_importable(import_str) == expected\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponseChunk Class in Python for Mirascope XAI\nDESCRIPTION: This code defines the CallResponseChunk class, which represents a portion of a response from an API call. It includes attributes for the chunk's content, role, and finish reason, as well as methods for JSON serialization and deserialization.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/xai/call_response_chunk.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n\nclass CallResponseChunk(BaseModel):\n    \"\"\"A chunk of a response from an API call.\"\"\"\n\n    content: str = Field(default=\"\")\n    role: Optional[str] = None\n    finish_reason: Optional[str] = None\n\n    def to_json(self) -> dict:\n        \"\"\"Convert the chunk to a JSON-serializable dictionary.\"\"\"\n        return self.model_dump()\n\n    @classmethod\n    def from_json(cls, json_data: dict) -> \"CallResponseChunk\":\n        \"\"\"Create a CallResponseChunk from a JSON-serializable dictionary.\"\"\"\n        return cls(**json_data)\n```\n\n----------------------------------------\n\nTITLE: Displaying the Skeleton Intermediate Response\nDESCRIPTION: A code snippet that demonstrates how to print the intermediate skeleton response generated by the break_into_subpoints function, showing the structure before expansion.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/skeleton_of_thought.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(break_into_subpoints(query))\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Prompting Example with Article Summaries in Plaintext\nDESCRIPTION: A few-shot prompt that provides two examples of article summaries before asking for a third summary. This guides the model toward the desired format and level of detail through demonstration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-best-practices.md#2025-04-21_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nSummarize the articles listed below.\n\nArticle 1: The global economy is currently experiencing unprecedented challenges as nations worldwide grapple with high inflation rates, significant disruptions in global supply chains, and escalating geopolitical tensions across various regions. Financial experts are closely monitoring these developments and predict that these factors will likely result in increased market volatility, with potential impacts on international trade and investment flows in the upcoming quarters.\nSummary 1: The global economy faces challenges like high inflation, supply chain disruptions, and geopolitical tensions, which are expected to increase market volatility and affect international trade and investments.\n\nArticle 2: Recent advances in artificial intelligence technology like ChatGPT and other chatbots have paved the way for developers to construct more sophisticated machine learning models and AI tools, which are significantly enhancing the capabilities of data processing systems. These AI models are not only streamlining complex data analysis tasks and other use cases but are also increasingly being integrated into various business sectors to improve decision-making processes, optimize operations, and drive innovation in ways previously unimagined.\nSummary 2: Recent advances in generative AI have resulted in sophisticated machine learning models that enhance data processing and decision-making across various business sectors, leading to increased operational efficiency and innovation.\n\nArticle 3: {article}\nSummary 3:\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks\nDESCRIPTION: Command to install pre-commit hooks for automated code checking and formatting.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nuv run pre-commit install --install-hooks\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: This snippet installs the necessary packages for the project, specifically Mirascope with OpenAI support and BeautifulSoup for web scraping.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/search_with_sources.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\" beautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Calculating Token-Based Costs for AI Models in Python\nDESCRIPTION: This function calculates the cost of using an AI model based on the number of tokens used and a pricing model. It takes into account both input and output tokens, applying different rates as specified in the pricing dictionary.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/costs/calculate_cost.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_cost(pricing: dict[str, float], input_tokens: int, output_tokens: int) -> float:\n    \"\"\"Calculate the cost of a model run based on the number of tokens used.\n\n    Args:\n        pricing: A dictionary containing the pricing information for the model.\n        input_tokens: The number of input tokens used.\n        output_tokens: The number of output tokens used.\n\n    Returns:\n        The cost of the model run.\n    \"\"\"\n    input_cost = pricing[\"input\"] * input_tokens / 1000\n    output_cost = pricing[\"output\"] * output_tokens / 1000\n    return input_cost + output_cost\n```\n\n----------------------------------------\n\nTITLE: Calculating BLEU Score with NLTK\nDESCRIPTION: Implementation of BLEU score calculation using NLTK to compare reference and candidate texts for similarity assessment.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['that', 'book', 'is', 'great']]\ncandidate = ['that', 'book', 'is', 'great', 'also']\nscore = sentence_bleu(reference, candidate)\nprint(f\"BLEU score: {score:.4f}\") \n> BLEU score: 0.6687\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Call Versioning with Lilypad\nDESCRIPTION: Shows how to implement automatic versioning of LLM function calls using Lilypad. The example demonstrates adding version tracking to OpenAI API calls.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-agents.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lilypad\nfrom openai import OpenAI\n \nclient = OpenAI()\n \n \n@lilypad.generation()\ndef answer_question(question: str) -> str:\nd    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": question}],\n    )\n    return str(completion.choices[0].message.content)\n \n \nif __name__ == \"__main__\":\n    lilypad.configure()\n    answer = answer_question(\"What's the population of New York?\")\n    print(answer)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for RAG Application\nDESCRIPTION: Command-line instructions to install Mirascope and LlamaIndex packages using pip, enabling RAG application development with OpenAI integration\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/rag-application.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install \"mirascope[openai]\"\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Creating prompt template using uppercase keywords\nDESCRIPTION: This snippet shows another way to define message roles using all-caps keywords (SYSTEM, USER) within the string template passed to `@prompt_template`. This provides a concise way to structure prompts with different roles directly within the template string.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM: You are a librarian\n    USER: Recommend a {genre} book\n    \"\"\"\n)\ndef book_recommendation_prompt(genre: str): ...\n\n\nprompt = book_recommendation_prompt(\"fantasy\")\nprint(prompt)\n# > [\n#     BaseMessageParam(role='system', content='You are a librarian'),\n#     BaseMessageParam(role='user', content='Recommend a fantasy book'),\n#   ]\n```\n\n----------------------------------------\n\nTITLE: Setting Up API Key Environment Variable - Python\nDESCRIPTION: This snippet sets the Google API key as an environment variable in Python, which is required for making API calls to Gemini.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/speech_transcription.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Applying OpenTelemetry Tracing Decorator\nDESCRIPTION: A decorator function that enables OpenTelemetry tracing for functions or methods in Mirascope. Allows automatic instrumentation and distributed tracing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/integrations/otel.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef with_otel()\n```\n\n----------------------------------------\n\nTITLE: Detecting Bias in Text with SpaCy\nDESCRIPTION: This snippet uses the spaCy library to detect bias in a news article. It loads a pre-trained NLP model, defines a list of potentially biased words, and identifies these words in the text. The code counts the occurrences of each biased word and highlights them in the original text to visually identify biased content.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom collections import Counter\n\n# Load a pre-trained NLP model (use 'en_core_web_sm' for a small English model)\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Sample news article text\ntext = \"\"\"\nThe senator's radical policies have devastated the economy, leaving hardworking citizens struggling to survive.\nMeanwhile, the responsible opposition has tirelessly fought for the people's interests.\n\"\"\"\n\n# Process the text using Spacy\ndoc = nlp(text)\n\n# Define potentially biased words or phrases (simplified for demonstration)\nbiased_words = [\"radical\", \"devastated\", \"hardworking\", \"responsible\", \"tirelessly\"]\n\n# Identify biased words in the text\ndetected_bias = [token.text for token in doc if token.text.lower() in biased_words]\n\n# Count occurrences of each biased word\nbias_count = Counter(detected_bias)\n\n# Output results\nprint(\"Detected Biased Words and Frequency:\")\nfor word, count in bias_count.items():\n    print(f\"{word}: {count}\")\n\n# Highlight biased words in the text\nhighlighted_text = \" \".join(\n    [f\"**{word}**\" if word.lower() in biased_words else word for word in text.split()]\n)\n\nprint(\"\\nText with Highlighted Bias:\")\nprint(highlighted_text)\n```\n\n----------------------------------------\n\nTITLE: Configuring Structured Outputs with OpenAI Tools\nDESCRIPTION: Example showing how to use structured outputs with OpenAI tools by configuring OpenAIToolConfig with strict mode enabled.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/openai.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/openai/structured_outputs/tools.py\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Environment\nDESCRIPTION: This code sets the OpenAI API key as an environment variable for authentication purposes in the evaluation process.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_sql_agent.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Installing DuckDuckGo search dependencies\nDESCRIPTION: This command installs the necessary Python libraries for using DuckDuckGo as a web search tool. It installs `duckduckgo-search`, `beautifulsoup4`, and `requests`.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"!pip install duckduckgo-search beautifulsoup4 requests\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Chain Results in Mirascope\nDESCRIPTION: Shows how to access and inspect the complete output of a chained LLM call using model_dump(). This includes metadata, response details, and token usage information.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-sucks.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(explanation.model_dump())\n# {\n#     \"metadata\": {},\n#     \"response\": {\n#         \"id\": \"chatcmpl-9r9jDaalZnT0A5BXbAuylxHe0Jl8G\",\n#         \"choices\": [\n#             {\n#                 \"finish_reason\": \"stop\",\n#                 \"index\": 0,\n#                 \"logprobs\": None,\n#                 \"message\": {\n#                     \"content\": \"The Eiffel Tower, standing as an iconic symbol of Paris...\",\n#                     \"role\": \"assistant\",\n#                     \"function_call\": None,\n#                     \"tool_calls\": None,\n#                 },\n#             }\n#         ],\n#         \"created\": 1722455807,\n#         \"model\": \"gpt-4o-2024-05-13\",\n#         \"object\": \"chat.completion\",\n#         \"service_tier\": None,\n#         \"system_fingerprint\": \"fp_4e2b2da518\",\n#         \"usage\": {\"completion_tokens\": 550, \"prompt_tokens\": 50, \"total_tokens\": 600},...\n```\n\n----------------------------------------\n\nTITLE: Example Output from Legal Analysis Prompt\nDESCRIPTION: Sample output from the legal analysis prompt which demonstrates how the model can provide more focused analysis when given a well-defined, specific task after receiving context from a previous prompt.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nThe termination clause in the summary seems to be a standard provision, but recent case law in California has indicated that service contracts must allow for a minimum 30-day notice period. You should verify if the contract specifies this time frame.\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI Dependency\nDESCRIPTION: This command installs the Mirascope library along with the necessary dependencies for using OpenAI models. It is a prerequisite for using Mirascope with OpenAI.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/document_segmentation.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"!pip install \\\"mirascope[openai]\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponse Class for OpenAI API Responses in Python\nDESCRIPTION: This snippet defines the CallResponse class, which encapsulates the response from an OpenAI API call. It includes methods for accessing the response content, checking for errors, and retrieving specific parts of the response such as choices or function calls.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/openai/call_response.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom mirascope.core.openai.exceptions import OpenAIException\n\n\nclass CallResponse:\n    \"\"\"Encapsulates a response from an OpenAI API call.\"\"\"\n\n    def __init__(self, response: Dict[str, Any]):\n        \"\"\"Initialize a CallResponse object.\n\n        Args:\n            response: The raw response from the OpenAI API.\n        \"\"\"\n        self._response = response\n\n    @property\n    def content(self) -> str:\n        \"\"\"Get the content of the response.\n\n        Returns:\n            The content of the response.\n\n        Raises:\n            OpenAIException: If the response does not contain content.\n        \"\"\"\n        try:\n            return self._response[\"choices\"][0][\"message\"][\"content\"]\n        except KeyError:\n            raise OpenAIException(\"Response does not contain content.\")\n\n    @property\n    def choices(self) -> List[Dict[str, Any]]:\n        \"\"\"Get the choices from the response.\n\n        Returns:\n            The choices from the response.\n\n        Raises:\n            OpenAIException: If the response does not contain choices.\n        \"\"\"\n        try:\n            return self._response[\"choices\"]\n        except KeyError:\n            raise OpenAIException(\"Response does not contain choices.\")\n\n    @property\n    def function_call(self) -> Optional[Dict[str, str]]:\n        \"\"\"Get the function call from the response.\n\n        Returns:\n            The function call from the response, if present.\n\n        Raises:\n            OpenAIException: If the response does not contain a function call.\n        \"\"\"\n        try:\n            return self._response[\"choices\"][0][\"message\"][\"function_call\"]\n        except KeyError:\n            return None\n\n    @property\n    def raw(self) -> Dict[str, Any]:\n        \"\"\"Get the raw response.\n\n        Returns:\n            The raw response from the OpenAI API.\n        \"\"\"\n        return self._response\n\n    def _get_tool_calls(\n        self,\n    ) -> Union[List[Dict[str, Any]], Dict[str, Any], None]:\n        \"\"\"Get the tool calls from the response.\n\n        Returns:\n            The tool calls from the response, if present.\n        \"\"\"\n        try:\n            return self._response[\"choices\"][0][\"message\"][\"tool_calls\"]\n        except KeyError:\n            return None\n\n    @property\n    def tool_calls(self) -> Union[List[Dict[str, Any]], Dict[str, Any], None]:\n        \"\"\"Get the tool calls from the response.\n\n        Returns:\n            The tool calls from the response, if present.\n        \"\"\"\n        return self._get_tool_calls()\n\n    @property\n    def finish_reason(self) -> str:\n        \"\"\"Get the finish reason from the response.\n\n        Returns:\n            The finish reason from the response.\n\n        Raises:\n            OpenAIException: If the response does not contain a finish reason.\n        \"\"\"\n        try:\n            return self._response[\"choices\"][0][\"finish_reason\"]\n        except KeyError:\n            raise OpenAIException(\"Response does not contain a finish reason.\")\n\n    @property\n    def status_code(self) -> int:\n        \"\"\"Get the status code from the response.\n\n        Returns:\n            The status code from the response.\n\n        Raises:\n            OpenAIException: If the response does not contain a status code.\n        \"\"\"\n        try:\n            return self._response[\"status_code\"]\n        except KeyError:\n            raise OpenAIException(\"Response does not contain a status code.\")\n\n    @property\n    def error(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Get the error from the response.\n\n        Returns:\n            The error from the response, if present.\n        \"\"\"\n        return self._response.get(\"error\")\n\n    def __str__(self) -> str:\n        \"\"\"Get a string representation of the response.\n\n        Returns:\n            A string representation of the response.\n        \"\"\"\n        return str(self._response)\n```\n\n----------------------------------------\n\nTITLE: Setting up SQLite Database and Creating Table\nDESCRIPTION: This snippet demonstrates how to set up a SQLite database connection and create a 'ReadingList' table for storing book information.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/sql_agent.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport sqlite3\n\ncon = sqlite3.connect(\"database.db\")\ncur = con.cursor()\n# ONE TIME SETUP\ncur.execute(\"\"\"\n    CREATE TABLE ReadingList (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        title TEXT NOT NULL,\n        status TEXT NOT NULL CHECK(status IN ('Not Started', 'In Progress', 'Complete')),\n        rating INTEGER CHECK(rating BETWEEN 1 AND 5)\n    )\n\"\"\")\ncon.commit()\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndex and Mirascope Components for RAG\nDESCRIPTION: Python import statements for loading necessary libraries and components to build a retrieval and generation pipeline, including document loading, indexing, and prompt engineering tools\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/rag-application.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom mirascope.core import openai, prompt_template\n```\n\n----------------------------------------\n\nTITLE: Defining a prompt template for book explanation task\nDESCRIPTION: This snippet defines a prompt template used to instruct a language model to explain the popularity of a given book in a specific genre. It includes placeholders for the book title and genre, allowing for dynamic insertion of values during prompt execution. The language model is instructed to act as a knowledgeable librarian.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-runnables.md#2025-04-21_snippet_7\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"prompt_template\": \"\\n   SYSTEM:\\n   You are the world's greatest librarian.\\n   Your task is to explain why the book \\\"{book_title}\\\" is popular in the {genre} genre.\\n\\n\\n   USER:\\n   Explain why \\\"{book_title}\\\" in the {genre} genre is popular.\\n   \",\n  \"fn_args\": {\n    \"genre\": \"science fiction\",\n    \"book_title\": {\n      \"metadata\": {},\n      \"response\": {\n        \"id\": \"chatcmpl-ANYSNPhRVQlMRuEUiwMeL07Z44k3f\",\n        \"choices\": [\n          {\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"logprobs\": null,\n            \"message\": {\n              \"content\": \"Dune\",\n              \"refusal\": null,\n              \"role\": \"assistant\",\n              \"audio\": null,\n              \"function_call\": null,\n              \"tool_calls\": null\n            }\n          }\n        ],\n        \"created\": 1730177359,\n        \"model\": \"gpt-4o-mini-2024-07-18\",\n        \"object\": \"chat.completion\",\n        \"service_tier\": null,\n        \"system_fingerprint\": \"fp_f59a81427f\",\n        \"usage\": {\n          \"completion_tokens\": 2,\n          \"prompt_tokens\": 23,\n          \"total_tokens\": 25,\n          \"completion_tokens_details\": {\n            \"audio_tokens\": null,\n            \"reasoning_tokens\": 0\n          },\n          \"prompt_tokens_details\": {\n            \"audio_tokens\": null,\n            \"cached_tokens\": 0\n          }\n        }\n      },\n      \"tool_types\": null,\n      \"prompt_template\": \"\\n   Recommend a popular book in the {genre} genre.\\n   Give me just the title.\\n   \",\n      \"fn_args\": {\n        \"genre\": \"science fiction\"\n      },\n      \"dynamic_config\": null,\n      \"messages\": [\n        {\n          \"role\": \"user\",\n          \"content\": \"Recommend a popular book in the science fiction genre.\\nGive me just the title.\"\n        }\n      ],\n      \"call_params\": {},\n      \"call_kwargs\": {\n        \"model\": \"gpt-4o-mini\",\n        \"messages\": [\n          {\n            \"role\": \"user\",\n            \"content\": \"Recommend a popular book in the science fiction genre.\\nGive me just the title.\"\n          }\n        ]\n      },\n      \"user_message_param\": {\n        \"content\": \"Recommend a popular book in the science fiction genre.\\nGive me just the title.\",\n        \"role\": \"user\"\n      },\n      \"start_time\": 1730177359100.959,\n      \"end_time\": 1730177359672.126,\n      \"message_param\": {\n        \"content\": \"Dune\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"tool_calls\": null\n      },\n      \"tools\": null,\n      \"tool\": null,\n      \"audio\": null,\n      \"audio_transcript\": null\n    }\n  },\n  \"dynamic_config\": {\n    \"computed_fields\": {\n      \"book_title\": {\n        \"metadata\": {},\n        \"response\": {\n          \"id\": \"chatcmpl-ANYSNPhRVQlMRuEUiwMeL07Z44k3f\",\n          \"choices\": [\n            {\n              \"finish_reason\": \"stop\",\n              \"index\": 0,\n              \"logprobs\": null,\n              \"message\": {\n                \"content\": \"Dune\",\n                \"refusal\": null,\n                \"role\": \"assistant\",\n                \"audio\": null,\n                \"function_call\": null,\n                \"tool_calls\": null\n              }\n            }\n          ],\n          \"created\": 1730177359,\n          \"model\": \"gpt-4o-mini-2024-07-18\",\n          \"object\": \"chat.completion\",\n          \"service_tier\": null,\n          \"system_fingerprint\": \"fp_f59a81427f\",\n          \"usage\": {\n            \"completion_tokens\": 2,\n            \"prompt_tokens\": 23,\n            \"total_tokens\": 25,\n            \"completion_tokens_details\": {\n              \"audio_tokens\": null,\n              \"reasoning_tokens\": 0\n            },\n            \"prompt_tokens_details\": {\n              \"audio_tokens\": null,\n              \"cached_tokens\": 0\n            }\n          }\n        },\n        \"tool_types\": null,\n        \"prompt_template\": \"\\n   Recommend a popular book in the {genre} genre.\\n   Give me just the title.\\n   \",\n        \"fn_args\": {\n          \"genre\": \"science fiction\"\n        },\n        \"dynamic_config\": null,\n        \"messages\": [\n          {\n            \"role\": \"user\",\n            \"content\": \"Recommend a popular book in the science fiction genre.\\nGive me just the title.\"\n          }\n        ],\n        \"call_params\": {},\n        \"call_kwargs\": {\n          \"model\": \"gpt-4o-mini\",\n          \"messages\": [\n            {\n              \"role\": \"user\",\n              \"content\": \"Recommend a popular book in the science fiction genre.\\nGive me just the title.\"\n            }\n          ]\n        },\n        \"user_message_param\": {\n          \"content\": \"Recommend a popular book in the science fiction genre.\\nGive me just the title.\",\n          \"role\": \"user\"\n        },\n        \"start_time\": 1730177359100.959,\n        \"end_time\": 1730177359672.126,\n        \"message_param\": {\n          \"content\": \"Dune\",\n          \"refusal\": null,\n          \"role\": \"assistant\",\n          \"tool_calls\": null\n        },\n        \"tools\": null,\n        \"tool\": null,\n        \"audio\": null,\n        \"audio_transcript\": null\n      }\n    }\n  },\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are the world's greatest librarian.\\nYour task is to explain why the book \\\"Dune\\\" is popular in the science fiction genre.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Explain why \\\"Dune\\\" in the science fiction genre is popular.\"\n    }\n  ],\n  \"call_params\": {},\n  \"call_kwargs\": {\n    \"model\": \"gpt-4o-mini\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are the world's greatest librarian.\\nYour task is to explain why the book \\\"Dune\\\" is popular in the science fiction genre.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Explain why \\\"Dune\\\" in the science fiction genre is popular.\"\n      }\n    ]\n  },\n  \"user_message_param\": {\n    \"content\": \"Explain why \\\"Dune\\\" in the science fiction genre is popular.\",\n    \"role\": \"user\"\n  },\n  \"start_time\": 1730177359689.837,\n  \"end_time\": 1730177366119.564,\n  \"message_param\": {\n    \"content\": \"\\\"Dune,\\\" written by Frank Herbert and first published in 1965, is considered a cornerstone of the science fiction genre for several compelling reasons:\\n\\n1. **Complex World-Building**: Herbert created an intricate universe with detailed geography, politics, religion, and ecology. The desert planet of Arrakis (Dune) is not merely a backdrop, but a living ecosystem that plays a crucial role in the story. The depth of this world-building allows readers to immerse themselves fully in the narrative.\\n\\n2. **Themes of Power and Politics**: \\\"Dune\\\" delves into themes of imperialism, feudalism, and the struggle for power. It explores how individuals and groups navigate political machinations, often with profound consequences. These themes resonate with readers as they reflect real-world issues of governance, control, and rebellion.\\n\\n3. **Ecological and Environmental Awareness**: The book introduces the idea of the environment\\u2019s fragility, particularly through the significance of the spice melange, which is vital for space travel and has profound effects on the human mind. It raises awareness about ecological balance and the exploitation of natural resources, themes that are increasingly relevant in today\\u2019s context of climate change and environmental degradation.\\n\\n4. **Fascination with Mysticism and Religion**: Herbert infused the narrative with elements of mysticism and religious symbolism, particularly through the character of Paul Atreides, who embodies the potential for messianic and prophetic power. This adds layers of philosophical inquiry about destiny, belief, and the nature of knowledge.\\n\\n5. **Rich Characterization and Development**: The characters in \\\"Dune\\\" are multifaceted and undergo significant development, making them relatable and memorable. Paul's journey from heir to a noble family to a powerful leader is complex, and readers are drawn into his internal conflicts and growth.\\n\\n6. **Cultural Impact**: \\\"Dune\\\" has influenced numerous other works in literature, film, and games. Its themes, motifs, and characters have become archetypes within the genre. Adaptations, such as David Lynch's 1984 film and Denis Villeneuve's 2021 adaptation, have further renewed interest and engagement with the original text.\\n\\n7. **Innovative Concepts**: The book introduces groundbreaking ideas, such as the concept of a human mind augmenting through the spice and the intricate workings of the Bene Gesserit sisterhood. These innovative concepts challenge the status quo of science fiction and push the boundaries of imagination.\\n\\n8. **Longevity and Nuance**: \\\"Dune\\\" continues to be relevant and compelling across generations. It invites diverse interpretations and discussions, offering new insights on every read. Its sophistication and richness allow for deep exploration of its themes, keeping it alive in academic discussions.\\n\\nOverall, \\\"Dune\\\" is popular not just for its compelling narrative and characters, but also for its deep engagement with significant and timeless themes, making it a rich text that continues to inspire and influence readers and creators in the science fiction genre.\",\n    \"refusal\": null,\n    \"role\": \"assistant\",\n    \"tool_calls\": null\n  },\n  \"tools\": null,\n  \"tool\": null,\n  \"audio\": null,\n  \"audio_transcript\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Cleaning Text Data for Readability - Python\nDESCRIPTION: This function cleans the input text by removing extra spaces and special characters to enhance formatting and readability. It returns a cleaned string.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\ndef clean_text(text: str) -> str:\n    \"\"\"\n    Clean the text data for better formatting and readability.\n    \"\"\"\n    # Removing extra spaces and special characters\n    return re.sub(r\"\\s+\", \" \", text).strip()\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Resolved Prompt Template Example\nDESCRIPTION: Shows how a prompt template looks after the variable has been resolved at runtime, with the {diet_type} placeholder replaced with 'vegan'.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nYou are a nutritionist specializing in creating personalized meal plans. Based on the vegan diet, suggest a healthy meal plan for the day.\n```\n\n----------------------------------------\n\nTITLE: Determining Appropriate Search Type using Groq API\nDESCRIPTION: Implements a function to determine the most suitable search type (web, news, images, videos) for a given question using the Groq API and a prompt template.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@throttled_groq_call(\n    \"llama-3.2-90b-text-preview\", response_model=SearchType, json_mode=True\n)\n@prompt_template(\n    \"\"\"\nSYSTEM:\nYou are an expert at determining the most appropriate type of search for a given query. Your task is to analyze the user's question and decide which Qwant search type to use: web, news, images, or videos.\n\nFollow these strict guidelines:\n1. For general information queries, use 'web'.\n2. For recent events, breaking news, or time-sensitive information, use 'news'.\n3. For queries explicitly asking for images or visual content, use 'images'.\n4. For queries about video content or asking for video results, use 'videos'.\n5. If unsure, default to 'web'.\n\nProvide your decision in a structured format with the search type and a brief explanation of your reasoning.\n\nUSER:\nDetermine the most appropriate search type for the following question:\n{question}\n\nA:\nBased on the question, I will determine the most appropriate search type and provide my reasoning.\n\"\"\"\n)\ndef determine_search_type(question: str) -> SearchType:\n    \"\"\"\n    Determine the most appropriate search type for the given question.\n    \"\"\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Integration in Mirascope\nDESCRIPTION: Provides a configuration function for setting up OpenTelemetry instrumentation in Mirascope projects. Allows initializing and configuring tracing and monitoring capabilities.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/integrations/otel.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef configure()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Mirascope and Importing Dependencies in Python\nDESCRIPTION: This snippet sets up and configures Mirascope along with its necessary dependencies for accessing various Large Language Model APIs. It installs the Mirascope library and sets environment variables for API keys.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_translation.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n!pip install \\\"mirascope[openai, anthropic, gemini]\\\"\n\"\"\"\n```\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nimport os\n\nos.environ[\\\"OPENAI_API_KEY\\\"] = \\\"YOUR_API_KEY\\\"\nos.environ[\\\"ANTHROPIC_API_KEY\\\"] = \\\"YOUR_API_KEY\\\"\nos.environ[\\\"GOOGLE_API_KEY\\\"] = \\\"YOUR_API_KEY\\\"\n# Set the appropriate API key for the provider you're using\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Single Point Scoring Prompt for LLM Judge\nDESCRIPTION: This code snippet demonstrates how to create a prompt for an LLM judge to perform single point scoring. It includes evaluation criteria, instructions for the LLM, and a scoring scale.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-as-judge.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nPlease act as an impartial evaluator and assign a score to the provided AI-generated response based on the evaluation criteria outlined below:\n\nEvaluation criteria:\n- Accuracy: Does the response give the correct answer to the query?\n- Clarity: Is the response clear and easy to understand?\n- Relevance: Does the response directly address the key aspects of the input query without unnecessary information?\n- Comprehensiveness: Does the response cover all aspects of the input query?\n\nYour task:\n1. Carefully review the input query and the corresponding AI-generated response.\n\n2. Assess the response according to the criteria listed above and assign a score from 1 to 5, where:\n- 1 = Very poor\n- 2 = Poor\n- 3 = Fair\n- 4 = Good\n- 5 = Excellent\n\n3. After assigning the score, provide a brief explanation justifying your decision based on the criteria.\n\nExample Format:\n\nInput Query:\n{query}\n\nAI-Generated Response:\n{response}\n\nYour Evaluation:\nScore: {1-5}\nExplanation: {Your reasoning here}\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Prompting Example\nDESCRIPTION: This is an example of zero-shot prompting, where the language model is asked a general question without any specific examples. It demonstrates the LLM's ability to generate relevant responses based on its pre-trained knowledge.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Can you recommend science-fiction books?\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API key\nDESCRIPTION: This snippet sets the OpenAI API key as an environment variable. It's crucial for authenticating requests to the OpenAI API and accessing its services. Replace `YOUR_API_KEY` with your actual API key.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Setting API Key for OpenAI in Python\nDESCRIPTION: This code snippet sets the OpenAI API key in the environment variables, necessary for authenticating API calls. Replace 'YOUR_API_KEY' with a valid key from your OpenAI account.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_classification.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: System Architecture Flowchart using Mermaid\nDESCRIPTION: Visualization of the agent executor system architecture showing the relationships between components including the agent executor, researcher agent, tools, and OpenAI API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    AE[Agent Executor]\n    R[Researcher.research]\n    WID[_write_initial_draft]\n    OAPI[OpenAI API]\n    \n    subgraph Researcher\n        RA[Researcher Agent]\n        WS[Web Search]\n        PW[Parse Webpage]\n    end\n    \n    AE --> R\n    AE --> WID\n    R --> RA\n    RA --> WS\n    RA --> PW\n    \n    WS -.-> OAPI\n    PW -.-> OAPI\n    RA -.-> OAPI\n    WID -.-> OAPI\n    AE -.-> OAPI\n\n    classDef agent fill:#e1d5e7,stroke:#9673a6,stroke-width:2px;\n    classDef tool fill:#fff2cc,stroke:#d6b656,stroke-width:2px;\n    classDef api fill:#dae8fc,stroke:#6c8ebf,stroke-width:2px;\n    \n    class AE,RA agent;\n    class R,WID,WS,PW tool;\n    class OAPI api;\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI Support in Python\nDESCRIPTION: This snippet installs Mirascope with support for OpenAI models. It requires pip for package installation. You need to replace 'YOUR_API_KEY' with your actual OpenAI API key.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_summarization.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Handling Groq API Responses in Python\nDESCRIPTION: This code snippet defines functions for processing responses from Groq API calls. It includes handling for both streaming and non-streaming responses, as well as error checking and response parsing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/groq/call_response.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport json\nfrom typing import Any, Dict, Generator, Iterator, List, Optional, Union\n\nfrom mirascope.core.base_models import ChatCompletionMessage\nfrom mirascope.core.exceptions import MirascopeException\nfrom mirascope.core.groq.models import GroqChatCompletionChunk, GroqChatCompletionResponse\n\n\ndef process_response(\n    response: Any,\n    stream: bool,\n) -> Union[GroqChatCompletionResponse, Generator[GroqChatCompletionChunk, None, None]]:\n    \"\"\"Process the response from a Groq API call.\"\"\"\n    if stream:\n        return process_stream_response(response)\n    else:\n        return process_non_stream_response(response)\n\n\ndef process_stream_response(\n    response: Any,\n) -> Generator[GroqChatCompletionChunk, None, None]:\n    \"\"\"Process a streaming response from a Groq API call.\"\"\"\n    for chunk in response.iter_lines():\n        if chunk:\n            try:\n                yield GroqChatCompletionChunk.model_validate(\n                    json.loads(chunk.decode(\"utf-8\").lstrip(\"data: \"))\n                )\n            except Exception as e:\n                raise MirascopeException(f\"Error parsing chunk: {e}\")\n\n\ndef process_non_stream_response(response: Any) -> GroqChatCompletionResponse:\n    \"\"\"Process a non-streaming response from a Groq API call.\"\"\"\n    try:\n        return GroqChatCompletionResponse.model_validate(response.json())\n    except Exception as e:\n        raise MirascopeException(f\"Error parsing response: {e}\")\n\n\ndef parse_streamed_response(\n    response: Iterator[GroqChatCompletionChunk],\n) -> GroqChatCompletionResponse:\n    \"\"\"Parse a streamed response from a Groq API call.\"\"\"\n    messages: List[ChatCompletionMessage] = []\n    finish_reason: Optional[str] = None\n    for chunk in response:\n        delta = chunk.choices[0].delta\n        if delta.content:\n            if not messages:\n                messages.append(\n                    ChatCompletionMessage(\n                        role=delta.role or \"assistant\", content=delta.content\n                    )\n                )\n            else:\n                messages[-1].content += delta.content\n        if chunk.choices[0].finish_reason:\n            finish_reason = chunk.choices[0].finish_reason\n\n    return GroqChatCompletionResponse(\n        id=\"\",\n        object=\"chat.completion\",\n        created=0,\n        model=\"\",\n        choices=[\n            {\n                \"index\": 0,\n                \"message\": messages[0] if messages else None,\n                \"finish_reason\": finish_reason,\n            }\n        ],\n        usage={\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0},\n    )\n\n\ndef parse_tools(\n    messages: List[Dict[str, Any]],\n) -> Optional[List[Dict[str, Any]]]:\n    \"\"\"Parse the tools from the messages.\"\"\"\n    for message in reversed(messages):\n        if message.get(\"role\") == \"assistant\" and message.get(\"tool_calls\"):\n            return message[\"tool_calls\"]\n    return None\n```\n\n----------------------------------------\n\nTITLE: Implementing GET Request Function in Python\nDESCRIPTION: This function, get, is a wrapper around the _request function specifically for making GET requests. It simplifies the API for GET operations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/tools/web/requests.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef get(\n    url: str,\n    session: requests.Session | None = None,\n    headers: dict[str, str] | None = None,\n    params: dict[str, str] | None = None,\n    timeout: float | None = None,\n) -> dict[str, Any]:\n    \"\"\"Make a GET request to the given URL.\"\"\"\n    return _request(\n        \"GET\",\n        url,\n        session=session,\n        headers=headers,\n        params=params,\n        timeout=timeout,\n    )\n```\n\n----------------------------------------\n\nTITLE: HTML Content Parser Implementation\nDESCRIPTION: Extension of ResearcherBase adding webpage parsing capabilities using BeautifulSoup.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass ResearcherBaseWithParser(ResearcherBase):\n    def parse_webpage(self, link: str) -> str:\n        \"\"\"Parse the paragraphs of the webpage found at `link`.\n\n        Args:\n            link: The URL of the webpage.\n\n        Returns:\n            The parsed paragraphs of the webpage, separated by newlines.\n        \"\"\"\n        try:\n            response = requests.get(link)\n            soup = BeautifulSoup(response.content, \"html.parser\")\n            return \"\\n\".join([p.text for p in soup.find_all(\"p\")])\n        except Exception as e:\n            return f\"{type(e)}: Failed to parse content from URL\"\n```\n\n----------------------------------------\n\nTITLE: Defining BaseMessageParam Class in Python for Mirascope\nDESCRIPTION: This class serves as the base for message parameters in the Mirascope framework. It likely defines common attributes and methods for all types of message parts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/message_param.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nmirascope.core.base.message_param.BaseMessageParam\n```\n\n----------------------------------------\n\nTITLE: Installing HTTPX and Requests Dependencies Using Bash\nDESCRIPTION: This snippet demonstrates how to install required dependencies for using the HTTPX and Requests tools in Mirascope. Users need to run these commands in a bash shell to ensure the modules are available for use.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/tools.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install httpx  # For HTTPX tool\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install requests  # For Requests tool\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages with Python\nDESCRIPTION: This snippet installs necessary packages required for the environment setup. The main package is Mirascope, along with optional visualization and HTML parsing libraries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n# (Optional) For visualization\n!pip install matplotlib networkx\n# (Optional) For parsing HTML\n!pip install beautifulsoup4\n```\n\n----------------------------------------\n\nTITLE: Example Usage\nDESCRIPTION: Provides an example of how to use the `run_diverse` function with a sample query. It defines a query related to forming a committee and then calls the `run_diverse` function with the query and prints the result.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/diverse.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"python\nquery = \"\"\"\nA committee of 3 people must be formed from a pool of 6 people, but Amy and Bob do not\nget along and should not be on the committee at the same time. How many viable\ncombinations are there?\n\"\"\"\n\nprint(await run_diverse(query))\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Audio Output API Call Implementation\nDESCRIPTION: Shows how to implement multi-modal output handling, specifically for audio responses from supported providers like OpenAI. Includes configuration for audio output and playback.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/calls.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@llm.call(\n    provider=\"openai\",\n    model=\"gpt-4\",\n    audio={\"voice\": \"alloy\"},\n    modalities=[\"text\", \"audio\"]\n)\ndef speak(text: str) -> str:\n    return text\n\nresponse = speak(\"Hello, world!\")\nprint(response.content)  # text content\nprint(response.audio)    # audio bytes\nprint(response.audio_transcript)  # audio transcript\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful LLM Calls in Python with Mirascope V0\nDESCRIPTION: Demonstrates the class-based approach used in Mirascope V0 for LLM calls, showing how state and arguments were mixed as class fields. This example illustrates the lack of clear separation between persistent state and call-specific arguments.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/mirascope-v1-release.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICall\n\n\nclass Librarian(OpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM: You are a librarian. You specialize in the {genre} genre\n    USER: {query}\n    \"\"\"\n\n    genre: str\n    query: str\n\n\nlibrarian = Librarian(\n    genre=\"fantasy\",\n    query=\"Recommend a book\",\n)\nresponse = librarian.call()\nprint(response.content)\n\nlibrarian.query = \"Recommend a book for beginners\"\nresponse = librarian.call()\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for HTTP Requests in Python\nDESCRIPTION: This snippet imports necessary libraries for making HTTP requests, including requests for HTTP operations and json for JSON data handling.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/tools/web/requests.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport json\nimport requests\nfrom requests.exceptions import RequestException\n\nfrom mirascope.tools.web._errors import WebRequestError\n```\n\n----------------------------------------\n\nTITLE: Expected response for Zero-Shot Prompting\nDESCRIPTION: This shows the expected response for the zero-shot prompt, illustrating the LLM's ability to provide a list of science fiction books. It indicates the desired output for the given prompt.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Dune by Frank Herbert, Neuromancer by William Gibson, Ender's Game by Orson Scott Card, \\nThe Hitchhiker's Guide to the Galaxy by Douglas Adams, and Foundation by Isaac Asimov.\"\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope and Setting API Key in Python\nDESCRIPTION: Installs the Mirascope package with OpenAI support and sets the OpenAI API key as an environment variable. Requires Python and pip for installation. The API key is necessary for authentication with OpenAI services. No inputs or outputs are expected from these commands, as they configure the environment.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/code_generation_and_execution.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"python\\n!pip install \\\"mirascope[openai]\\\"\\n\"\n```\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"python\\nimport os\\n\\nos.environ[\\\"OPENAI_API_KEY\\\"] = \\\"YOUR_API_KEY\\\"\\n# Set the appropriate API key for the provider you're using\\n\"\n```\n\n----------------------------------------\n\nTITLE: Recall and Precision Evaluation in Python\nDESCRIPTION: Implements hardcoded evaluation using recall and precision metrics to compare expected and actual answers. Calculates the intersection of words between answers and scores based on these standard information retrieval metrics.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/evals.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/evals/hardcoded/recall_and_precision.py\"\n```\n\n----------------------------------------\n\nTITLE: Installing Tavily Search Library\nDESCRIPTION: This command installs the Tavily python library, which is required to use the Tavily `WebSearch` tool. Tavily provides a search API that can be used to search the web.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"!pip install tavily-python\"\n```\n\n----------------------------------------\n\nTITLE: Defining Groq API Call Function in Python\nDESCRIPTION: This function makes an API call to Groq's language model service. It handles authentication, request formatting, and response parsing. The function supports both synchronous and asynchronous calls.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/groq/call.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nasync def call(\n    prompt: str,\n    model: str = \"mixtral-8x7b-32768\",\n    max_tokens: int = 2048,\n    temperature: float = 0.7,\n    top_p: float = 1.0,\n    stop: Optional[Union[str, List[str]]] = None,\n    stream: bool = False,\n    api_key: Optional[str] = None,\n) -> Union[AsyncGenerator[str, None], str]:\n    \"\"\"Make a call to the Groq API.\n\n    Args:\n        prompt (str): The prompt to send to the API.\n        model (str, optional): The model to use. Defaults to \"mixtral-8x7b-32768\".\n        max_tokens (int, optional): The maximum number of tokens to generate. Defaults to 2048.\n        temperature (float, optional): The temperature to use for sampling. Defaults to 0.7.\n        top_p (float, optional): The top_p value to use for sampling. Defaults to 1.0.\n        stop (Optional[Union[str, List[str]]], optional): The stop sequence(s) to use. Defaults to None.\n        stream (bool, optional): Whether to stream the response. Defaults to False.\n        api_key (Optional[str], optional): The API key to use. Defaults to None.\n\n    Returns:\n        Union[AsyncGenerator[str, None], str]: The response from the API.\n\n    Raises:\n        ValueError: If the API key is not provided.\n        httpx.HTTPStatusError: If the API returns an error status code.\n    \"\"\"\n    if api_key is None:\n        api_key = os.environ.get(\"GROQ_API_KEY\")\n    if api_key is None:\n        raise ValueError(\"API key not provided\")\n\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"prompt\": prompt,\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"stop\": stop,\n        \"stream\": stream,\n    }\n\n    async with httpx.AsyncClient() as client:\n        async with client.stream(\n            \"POST\",\n            \"https://api.groq.com/openai/v1/completions\",\n            headers=headers,\n            json=data,\n            timeout=600,\n        ) as response:\n            response.raise_for_status()\n            if stream:\n                return _stream_response(response)\n            else:\n                return await _get_full_response(response)\n```\n\n----------------------------------------\n\nTITLE: Python Dictionary Configuration for Einstein AI Simulation\nDESCRIPTION: A nested dictionary structure containing configuration parameters for an AI chat system. Includes chat completion details, message history, and computed fields for simulating Einstein's responses about physics theories.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-chaining.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"dynamic_config\": {\n        \"computed_fields\": {\n            \"scientist\": {\n                \"metadata\": {},\n                \"response\": {\n                    \"id\": \"chatcmpl-9tih8UOr2dIpQkzQq8vW5adAuDaKx\",\n                    \"choices\": [\n                        {\n                            \"finish_reason\": \"stop\",\n                            \"index\": 0,\n                            \"logprobs\": None,\n                            \"message\": {\n                                \"content\": \"Albert Einstein\",\n                                \"role\": \"assistant\",\n                                \"function_call\": None,\n                                \"tool_calls\": None,\n                                \"refusal\": None,\n                            },\n                        }\n                    ],\n                    \"created\": 1723066874\n                }\n            }\n        }\n    },\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"Imagine that you are Albert Einstein.\\nYour task is to explain a theory that you, Albert Einstein, are famous for.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the theory related to theory of relativity.\",\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI Environment\nDESCRIPTION: Configuration of OpenAI API key and import of necessary Mirascope and LangChain modules for LLM interaction.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-rag.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n# Set the appropriate API key for the provider you're using\nfrom mirascope.core import openai\nfrom langchain_openai import ChatOpenAI\n```\n\n----------------------------------------\n\nTITLE: Server Side Events Connection Setup\nDESCRIPTION: Shows how to connect to an MCP server using server-side events (SSE) client connection.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/mcp/client.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.mcp import sse_client\n\n\nasync with sse_client(\"http://localhost:8000\") as client:\n    prompts = await client.list_prompts()\n    print(prompts[0])\n```\n\n----------------------------------------\n\nTITLE: Setting up environment variables for API keys\nDESCRIPTION: This code sets up the OpenAI API key as an environment variable. It's crucial for authenticating API requests.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/localized_agent.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponseChunk Class for Groq API Responses in Python\nDESCRIPTION: This class represents a single chunk of a response from a Groq API call. It extracts and stores various attributes from the chunk, including the delta text, finish reason, and metadata like created timestamp and model information.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/groq/call_response_chunk.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Dict, Optional\n\nfrom mirascope.core.base_types import BaseCallResponseChunk\n\n\nclass CallResponseChunk(BaseCallResponseChunk):\n    \"\"\"A chunk of a response from a Groq API call.\"\"\"\n\n    def __init__(self, chunk: Dict) -> None:\n        \"\"\"Initialize a CallResponseChunk.\n\n        Args:\n            chunk: The chunk of the response from the Groq API.\n        \"\"\"\n        super().__init__()\n        self.delta = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n        self.finish_reason = chunk[\"choices\"][0].get(\"finish_reason\")\n        self.created = chunk.get(\"created\")\n        self.model = chunk.get(\"model\")\n\n    @property\n    def text(self) -> str:\n        \"\"\"Get the text of the chunk.\n\n        Returns:\n            The text of the chunk.\n        \"\"\"\n        return self.delta\n\n    @property\n    def is_finished(self) -> bool:\n        \"\"\"Check if the chunk is finished.\n\n        Returns:\n            True if the chunk is finished, False otherwise.\n        \"\"\"\n        return self.finish_reason is not None\n\n    @property\n    def metadata(self) -> Dict[str, Optional[str]]:\n        \"\"\"Get the metadata of the chunk.\n\n        Returns:\n            A dictionary containing the metadata of the chunk.\n        \"\"\"\n        return {\n            \"created\": self.created,\n            \"model\": self.model,\n            \"finish_reason\": self.finish_reason,\n        }\n```\n\n----------------------------------------\n\nTITLE: Creating Function Wrappers with RunnableLambda in Python\nDESCRIPTION: This snippet explains how to use RunnableLambda to transform Python functions into LangChain-compatible runnables. It shows how to handle both synchronous and asynchronous implementations with individual values and batches of data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-runnables.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# This is a RunnableLambda\nfrom langchain_core.runnables import RunnableLambda\n\ndef add_one(x: int) -> int:\n    return x + 1\n\nrunnable = RunnableLambda(add_one)\n\nrunnable.invoke(1) # returns 2\nrunnable.batch([1, 2, 3]) # returns [2, 3, 4]\n\n# Async is supported by default by delegating to the sync implementation\nawait runnable.ainvoke(1) # returns 2\nawait runnable.abatch([1, 2, 3]) # returns [2, 3, 4]\n\n\n# Alternatively, can provide both synd and sync implementations\nasync def add_one_async(x: int) -> int:\n    return x + 1\n\nrunnable = RunnableLambda(add_one, afunc=add_one_async)\nrunnable.invoke(1) # Uses add_one\nawait runnable.ainvoke(1) # Uses add_one_async\n```\n\n----------------------------------------\n\nTITLE: Defining AnthropicCallParams Dataclass for Anthropic API Calls in Python\nDESCRIPTION: This code defines a dataclass AnthropicCallParams that represents the parameters used for making API calls to Anthropic's language models. It includes attributes for model selection, temperature control, and various other configuration options.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/anthropic/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n@dataclass\nclass AnthropicCallParams:\n    model: str = field(default=\"claude-2\")\n    max_tokens_to_sample: int = field(default=1000)\n    temperature: float = field(default=0.0)\n    top_k: Optional[int] = field(default=None)\n    top_p: Optional[float] = field(default=None)\n    stop_sequences: Optional[list[str]] = field(default=None)\n    stream: bool = field(default=False)\n```\n\n----------------------------------------\n\nTITLE: Processing Chunked Responses from Google API Calls in Python\nDESCRIPTION: This function takes a chunked response from a Google API call and yields individual chunks. It handles different response formats, including JSON and plain text, and processes the chunks accordingly.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/google/call_response_chunk.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef call_response_chunk(response):\n    \"\"\"Yield chunks from a Google API call response.\n\n    Args:\n        response: The response from a Google API call.\n\n    Yields:\n        dict: A dictionary containing the chunk data.\n    \"\"\"\n    for chunk in response:\n        if not chunk.choices:\n            continue\n        choice = chunk.choices[0]\n        if choice.delta and choice.delta.content is not None:\n            yield {\"content\": choice.delta.content}\n        elif choice.text:\n            yield {\"content\": choice.text}\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI support - Python\nDESCRIPTION: This snippet installs the Mirascope package with OpenAI support, which is essential for using specific LLM functionalities demonstrated throughout the notebook.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Crawler Access Rules in robots.txt\nDESCRIPTION: Defines crawler behavior by blocking access to Markdown files while allowing access to other content. Specifies the location of the sitemap for improved crawler efficiency.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/robots.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nUser-agent: *\nDisallow: /*.md$\n\nSitemap: https://mirascope.com/sitemap.xml\n```\n\n----------------------------------------\n\nTITLE: Expected response for Information Extraction Prompt\nDESCRIPTION: This is the expected response for the information extraction prompt, correctly extracting the key points and action items. The output lists the discussed topics and assigned responsibilities, demonstrating effective information retrieval.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Key Points Discussed:\\n\\n1. Upcoming product launch.\\n2. Marketing strategies.\\n\\nAction Items:\\n\\n1. John will handle the social media campaign.\\n2. Mary will oversee the product development.\"\n```\n\n----------------------------------------\n\nTITLE: Advanced LLM Function with Version Detection\nDESCRIPTION: Demonstrates how Lilypad detects versions based on lexical closure, including changes to helper functions used within the LLM function.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-versioning.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# lily/recommend_book.py\nimport lilypad\nfrom mirascope.core import openai\n\n\ndef get_available_books() -> list[str]:\n    return [\"The Name of the Wind\"]\n\n\n@lilypad.llm_fn()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    books = get_available_books()\n    return f\"Recommend a {genre} book from this list: {books}\"\n\n\nif __name__ == \"__main__\":\n    output = recommend_book(\"fantasy\")\n    print(output.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Support for Responses\nDESCRIPTION: This snippet illustrates how to create a CustomProviderCallResponseChunk class to manage streamed responses, including properties for content and finish reasons.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/custom_provider.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core.base import BaseCallResponseChunk\n\n\nclass CustomProviderCallResponseChunk(BaseCallResponseChunk[...]):  # provide types for generics\n    # Implement abstract properties\n    @property\n    def content(self) -> str:\n        # Return the content of the chunk\n\n    @property\n    def finish_reasons(self) -> list[str] | None:\n        # Return the finish reasons for the chunk\n\n    # Implement other abstract properties\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installation of Mirascope with OpenAI support and pandas library using pip.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/synthetic-data-generation.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"mirascope[openai]\" pandas\n```\n\n----------------------------------------\n\nTITLE: Defining StreamingParams Class for LiteLLM in Python\nDESCRIPTION: This class defines parameters for streaming responses in LiteLLM API calls. It includes options for callbacks and error handling during streaming.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/litellm/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass StreamingParams(TypedDict, total=False):\n    callbacks: list[Callable] | None\n    on_error: Callable | None\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment with Required Libraries\nDESCRIPTION: Imports necessary libraries for document ingestion, processing, and interaction with OpenAI's models using LangChain and Mirascope. Establishes foundational components like WebBaseLoader for loading documents and OpenAIEmbeddings for vector conversions. Prerequisites include installing packages 'bs4', 'langchain_chroma', 'langchain_community', 'langchain_openai', and 'langchain_text_splitters'.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-pipeline.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport bs4\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nfrom mirascope.core import openai\n```\n\n----------------------------------------\n\nTITLE: Querying the Knowledge Graph with OpenAI and Mirascope\nDESCRIPTION: This code snippet defines a function to query a previously created knowledge graph by using the OpenAI model with Mirascope decorators. It allows querying for specific insights from the graph based on user questions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-build-a-knowledge-graph.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Answer the following question based on the knowledge graph.\n\n    Knowledge Graph:\n    {knowledge_graph}\n\n    USER:\n    {question}\n    \"\"\"\n)\ndef query_kg(question: str, knowledge_graph: KnowledgeGraph): ...\n\n\nresponse = query_kg(\n    \"What factors contribute to algorithmic bias in data collection and selection?\",\n    kg,\n)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Example Output from Document Summarization Prompt\nDESCRIPTION: Sample output from the document summarization prompt, which provides a concise summary of a contract's key points that can be used in subsequent prompts.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/advanced-prompt-engineering.md#2025-04-21_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nThe contract is a service agreement between Company A and Company B. It outlines the scope of services, payment terms, and termination conditions. Company B agrees to provide IT support for a period of one year, subject to renewal [...rest of summary].\n```\n\n----------------------------------------\n\nTITLE: Defining Groq API Call Parameters in Python\nDESCRIPTION: This code snippet appears to define a module for Groq API call parameters. It is likely part of a larger codebase for interacting with Groq's language models. The exact implementation details are not visible in the provided text.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/groq/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# mirascope.core.groq.call_params\n```\n\n----------------------------------------\n\nTITLE: Using Pydub for Audio Inputs with Shorthand Method in Python\nDESCRIPTION: Demonstrates how to handle audio inputs in prompts using pydub and the shorthand method for multi-modal interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom pydub import AudioSegment\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.content_param import AudioParam\n\n\n@prompt_template\ndef transcribe_audio_prompt(audio: bytes) -> Messages.Type:\n    return f\"Transcribe this audio: {AudioParam(audio)}\"\n\n\n# Read an audio file\naudio_segment = AudioSegment.from_file(\"examples/data/sample.mp3\")\naudio_bytes = audio_segment.export(format=\"mp3\").read()\n\n# Use the audio in a prompt\nprint(transcribe_audio_prompt(audio=audio_bytes))\n# Returns a user message with both text and audio content\n\n```\n\n----------------------------------------\n\nTITLE: Basic Retries with Tenacity Decorator\nDESCRIPTION: Example of implementing exponential backoff retries for API calls using the Tenacity retry decorator. The code will attempt the call 3 times before throwing a RetryError.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/retries.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@retry(stop=stop_after_attempt(3), wait=wait_exponential())\ndef make_recommendation() -> None:\n    response = recommend_book(\"fantasy\")\n    print(response.text)\n```\n\n----------------------------------------\n\nTITLE: Connecting to MCP Server using stdio_client\nDESCRIPTION: Example of creating a connection to a book recommendation server using stdio_client with StdioServerParameters.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/mcp/client.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/mcp/client.py:1:20\"\n--8<-- \"examples/learn/mcp/client.py:48:50\"\n```\n\n----------------------------------------\n\nTITLE: Lilypad Project Structure\nDESCRIPTION: Shows the basic directory structure for a Lilypad project, including configuration, LLM functions, and versioning database.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\n|\n|-- .lilypad/\n|-- |-- config.json\n|-- lily/\n|   |-- __init__.py\n|   |-- {llm_function_name}.py\n|-- pad.db\n```\n\n----------------------------------------\n\nTITLE: Defining Google API Call and Response Module in Python\nDESCRIPTION: This code snippet defines a Python module named 'call_response' within the 'mirascope.core.google' package. It appears to be a placeholder or stub for implementing Google API interaction functionality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/google/call_response.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# mirascope.core.google.call_response\n```\n\n----------------------------------------\n\nTITLE: Example Annotation Task for Content Evaluation\nDESCRIPTION: Shows a basic annotation task format for evaluating LLM responses with a task description and prompt-response pair for assessment.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nTask : Suggest corrections for any highlighted issues\n\nPrompt = \"Explain why a balanced diet is important.\"  \n\nResponse = \"Eating food is good for you. If you eat, you can be strong and not feel sick.\"\n```\n\n----------------------------------------\n\nTITLE: Defining AudioPart Class in Python for Mirascope\nDESCRIPTION: The AudioPart class, presumably a subclass of BaseMessageParam, is designed to manage audio content within the Mirascope messaging system. It may include specific attributes and methods for audio processing.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/message_param.md#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmirascope.core.base.message_param.AudioPart\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Streaming with OpenAI\nDESCRIPTION: This Python snippet demonstrates error handling during streaming with OpenAI. It shows how to wrap the streaming loop in a try/except block to catch any exceptions that might occur during the stream's iteration, highlighting the importance of handling errors during the actual streaming process rather than just the initial function call. This example requires the OpenAI client library.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/streams.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\ntry:\n    for chunk in response.iter_chunks():\n        print(chunk.choices[0].delta.content, end=\"\")\nexcept Exception as e:\n    print(f\"\\nError during streaming: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Environment Variables\nDESCRIPTION: This code snippet sets the OpenAI API key in the environment variables, which is necessary for authenticating API calls to OpenAI services. Replace 'YOUR_API_KEY' with the actual key.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/removing_semantic_duplicates.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Lilypad Prompt Management Structure\nDESCRIPTION: Shows the directory structure for Lilypad, Mirascope's prompt management system. Lilypad automatically versions and traces all prompts, maintaining a structured organization for better collaboration and iteration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n|\n|-- .lilypad/\n|-- |-- config.json\n|-- lily/\n|   |-- __init__.py\n|   |-- {llm_function_name}.py\n|-- pad.db\n```\n\n----------------------------------------\n\nTITLE: Example usage\nDESCRIPTION: Shows an example of how the A/B testing integration can be used in the main function.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    user_input = input(\"Please describe your issue: \")\n    handle_user_interaction(user_input)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Self-Ask Implementation\nDESCRIPTION: This snippet installs the necessary packages for implementing Self-Ask with Mirascope, including OpenAI integration, NumPy, and scikit-learn.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/self_ask.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\" numpy scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Configuring API Key for Anthropic\nDESCRIPTION: Set up environment variable for Anthropic API authentication\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/query_plan.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key - Python\nDESCRIPTION: The snippet shows how to set the OpenAI API key in the environment variables, which is required for authenticating the calls made to the OpenAI services.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic OpenAI LLM Function in Python\nDESCRIPTION: Demonstrates creating a basic LLM function using OpenAI's SDK to review song lyrics. The function creates a chat completion using GPT-4 model with system and user messages.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-versioning.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n\ndef review_song(song_lyrics: str) -> str:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are an acclaimed music producer.\"}, \n            {\"role\": \"user\", \"content\": f\"I'm working on a new song. What do you think?\\n{song_lyrics}\"}\n        ]\n    )\n    return str(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses from OpenAI\nDESCRIPTION: This snippet shows how to create a function that streams responses from the OpenAI API using the Mirascope library. It provides immediate feedback by printing response chunks as they are received.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/quickstart.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef stream_city_info(city: str) -> str:\n    return f\"Provide a brief description of {city}.\"\n\n\nfor chunk, _ in stream_city_info(\"Tokyo\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Dynamic Configuration for Custom Provider\nDESCRIPTION: The snippet defines CustomProviderDynamicConfig as a dynamic configuration type utilizing BaseDynamicConfig, specifying the message parameter type and custom call parameters.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/custom_provider.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core.base import BaseDynamicConfig\nfrom .call_params import CustomProviderCallParams\n\nCustomProviderDynamicConfig = BaseDynamicConfig[BaseMessageParam, CustomProviderCallParams]\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope Dependencies\nDESCRIPTION: Install Mirascope library with Anthropic provider for LLM interactions\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/query_plan.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install \"mirascope[anthropic]\"\n```\n\n----------------------------------------\n\nTITLE: Importing Google API Call Parameter Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the call_params module from the mirascope.core.google package. It's used to access functionality for handling Google API call parameters.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/google/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: mirascope.core.google.call_params\n```\n\n----------------------------------------\n\nTITLE: Human Evaluation: Rating Scale Example\nDESCRIPTION: This example demonstrates how to use a rating scale to evaluate the relevance and clarity of a response to a prompt. It shows how reviewers can score responses based on specific criteria, using a scale from 1 to 5.  The example highlights rating scales assessing both relevance and clarity.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\"# Rate the relevance of the following response to the prompt: \\n\\nYour Task: \\\"Score this response from 1 (not relevant) to 5 (very relevant)and provide reasons for your score.\\\" \\n\\nPrompt1 = \\\"What are the benefits of regular exercise?\\\"\\nResponse = \\\"Regular exercise improves cardiovascular health, boosts mood, and enhances overall fitness.\\\" \\n\\n# Evaluate clarity instead: \\n\\nPrompt2 = \\\"How clear is this explanation? Score it from 1 (very unclear) to 5 (very clear).\\\" \\nResponse = \\\"Exercise helps you stay healthy by making your heart stronger and improving your mood.\\\" \"\n```\n\n----------------------------------------\n\nTITLE: Using Wave Module for Audio Inputs with Messages Methods in Python\nDESCRIPTION: Shows how to handle audio inputs using the wave module and Messages.User method for multi-modal LLM interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport wave\nfrom mirascope import Messages, prompt_template\nfrom mirascope.core.base.content_param import AudioParam\n\n\n@prompt_template\ndef transcribe_audio_prompt(audio: bytes) -> Messages.Type:\n    return Messages.User(f\"Transcribe this audio: {AudioParam(audio)}\")\n\n\n# Read an audio file\nwith wave.open(\"examples/data/sample.wav\", \"rb\") as wav_file:\n    audio_bytes = wav_file.readframes(wav_file.getnframes())\n\n# Use the audio in a prompt\nprint(transcribe_audio_prompt(audio=audio_bytes))\n# Returns a user message with both text and audio content\n\n```\n\n----------------------------------------\n\nTITLE: Running Tests and Coverage\nDESCRIPTION: Commands to run tests and generate coverage reports using pytest.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nuv run pytest tests/\nuv run pytest tests/ --cov=./ --cov-report=html\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Web Search Agent in Python\nDESCRIPTION: This code sets the OpenAI API key as an environment variable for authentication in the Web Search Agent evaluation process.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_web_search_agent.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Defining DynamicConfig Class for Mirascope Core Mistral in Python\nDESCRIPTION: This code snippet defines the DynamicConfig class, which is responsible for managing dynamic configuration settings. It includes methods for setting and retrieving configuration values, as well as handling default values and validation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/mistral/dynamic_config.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass DynamicConfig:\n    \"\"\"Dynamic configuration for Mistral.\"\"\"\n\n    def __init__(self):\n        self._config = {}\n\n    def set(self, key: str, value: Any) -> None:\n        \"\"\"Set a configuration value.\"\"\"\n        self._config[key] = value\n\n    def get(self, key: str, default: Any = None) -> Any:\n        \"\"\"Get a configuration value.\"\"\"\n        return self._config.get(key, default)\n\n    def __getattr__(self, name: str) -> Any:\n        \"\"\"Get a configuration value as an attribute.\"\"\"\n        return self.get(name)\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        \"\"\"Set a configuration value as an attribute.\"\"\"\n        if name.startswith('_'):\n            super().__setattr__(name, value)\n        else:\n            self.set(name, value)\n```\n\n----------------------------------------\n\nTITLE: Expected response for Text Summarization Prompt\nDESCRIPTION: This is the expected response for the text summarization prompt, demonstrating how the LLM condenses the article. It provides a concise summary retaining the key information and highlights the aim and timeline of the downtown revitalization plan.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n\"The city council approved a plan to revitalize downtown by building new parks, \\nrenovating buildings, and improving public transportation. The aim is to attract \\nbusinesses and tourists, boosting the economy. Construction will start next year \\nand finish in five years.\"\n```\n\n----------------------------------------\n\nTITLE: Migrating Streaming Calls in Mirascope from v0 to v1 in Python\nDESCRIPTION: Demonstrates the change in implementing streaming calls from v0 to v1. The new version uses a decorator with a 'stream' parameter, simplifying the streaming process while maintaining similar functionality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nstream = recommender.stream()\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book.\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Expected response for Few-Shot Prompting\nDESCRIPTION: This is the expected response for the few-shot prompt, illustrating the LLM's ability to convert Celsius to Fahrenheit correctly. This response indicates the desired output format and numerical accuracy.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n\"212°F\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Groq API\nDESCRIPTION: Sets the Groq API key as an environment variable for authentication.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = \"gsk_...\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Defining DynamicConfig Class for Anthropic API in Python\nDESCRIPTION: This snippet defines the DynamicConfig class which manages dynamic configurations for Anthropic API interactions. It includes methods for setting and retrieving API keys and base URLs, with support for default values.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/anthropic/dynamic_config.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass DynamicConfig:\n    def __init__(self):\n        self._api_key = None\n        self._base_url = None\n\n    def api_key(self, api_key: Optional[str] = None) -> Optional[str]:\n        if api_key is not None:\n            self._api_key = api_key\n        return self._api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n\n    def base_url(self, base_url: Optional[str] = None) -> str:\n        if base_url is not None:\n            self._base_url = base_url\n        return self._base_url or \"https://api.anthropic.com\"\n\n    def reset(self):\n        self._api_key = None\n        self._base_url = None\n```\n\n----------------------------------------\n\nTITLE: Creating ImagePart Class in Python for Mirascope\nDESCRIPTION: ImagePart is likely another subclass of BaseMessageParam, tailored for handling image content in Mirascope messages. It may include methods for processing and storing image data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/message_param.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmirascope.core.base.message_param.ImagePart\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope in Python\nDESCRIPTION: This snippet demonstrates how to install the Mirascope package with OpenAI support using pip. It is a prerequisite for the following examples that involve using Mirascope to structure LLM outputs.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/structured_outputs.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Defining BaseCallParams Class for TypedDict Type Binding in Python\nDESCRIPTION: The BaseCallParams class is an empty class used to bind type variables to ensure they are call parameters. This approach is necessary because binding a type variable to a base TypedDict type requires creating a subclass.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BaseCallParams\n```\n\n----------------------------------------\n\nTITLE: Implementing CohereStreamer for Cohere model responses\nDESCRIPTION: A concrete implementation of BedrockStreamer for Cohere models. It processes streaming responses from Cohere models, extracting text content from each response chunk.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/bedrock/stream.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass CohereStreamer(BedrockStreamer):\n    \"\"\"Streamer for Cohere models.\"\"\"\n\n    def __call__(self, response_stream: Iterator[Dict[str, Any]]) -> Iterator[str]:\n        \"\"\"Process a streaming response from Amazon Bedrock for Cohere models.\n\n        Args:\n            response_stream: The streaming response from Amazon Bedrock.\n\n        Returns:\n            An iterator of strings representing the processed response.\n        \"\"\"\n        for response in response_stream:\n            chunk = json.loads(response.get(\"chunk\", {}).get(\"bytes\", b\"{}\").decode(\"utf-8\"))\n            generations = chunk.get(\"generations\", [])\n            if generations and \"text\" in generations[0]:\n                yield generations[0][\"text\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key in Python\nDESCRIPTION: Sets environment variable for OpenAI API key, essential for accessing OpenAI services. Make sure to replace \"YOUR_API_KEY\" with the actual API key acquired from OpenAI to ensure proper access and functionality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you\\'re using\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Call Parameters in Python\nDESCRIPTION: Shows how to set specific call parameters for OpenAI API calls using Mirascope's decorator. The example sets the temperature parameter for a book recommendation function.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", call_params={\"temperature\": 0.7})\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing POST Request Function in Python\nDESCRIPTION: This function, post, is a wrapper around the _request function for making POST requests. It handles both form data and JSON payloads.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/tools/web/requests.md#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef post(\n    url: str,\n    session: requests.Session | None = None,\n    headers: dict[str, str] | None = None,\n    params: dict[str, str] | None = None,\n    data: dict[str, str] | None = None,\n    json_data: dict[str, str] | None = None,\n    timeout: float | None = None,\n) -> dict[str, Any]:\n    \"\"\"Make a POST request to the given URL.\"\"\"\n    return _request(\n        \"POST\",\n        url,\n        session=session,\n        headers=headers,\n        params=params,\n        data=data,\n        json_data=json_data,\n        timeout=timeout,\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python\nDESCRIPTION: The snippet sets the environment variable for OpenAI API key. The proper key is required to authenticate and make API calls to OpenAI services.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/support_ticket_routing.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Defining DynamicConfig Abstract Base Class for Amazon Bedrock\nDESCRIPTION: Abstract base class for dynamic configurations used with Amazon Bedrock models. It requires subclasses to implement the get_model_id and get_provider methods to specify model details.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/bedrock/dynamic_config.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom abc import ABC, abstractmethod\nfrom typing import ClassVar, Dict, Optional, Type\n\nfrom mirascope.utils import DynamicImporter\n\n\nclass DynamicConfig(ABC):\n    \"\"\"Abstract base class for dynamic configurations.\n\n    This class is used to configure the dynamic runtime parameters for the\n    model.\n\n    Attributes:\n        provider: The provider of the model.\n        model_id: The model id.\n    \"\"\"\n\n    provider: ClassVar[str]\n    model_id: ClassVar[str]\n\n    @classmethod\n    @abstractmethod\n    def get_model_id(cls) -> str:\n        \"\"\"Gets the model ID for the model.\n\n        Returns:\n            The model ID.\n        \"\"\"\n        ...\n\n    @classmethod\n    @abstractmethod\n    def get_provider(cls) -> str:\n        \"\"\"Gets the provider for the model.\n\n        Returns:\n            The provider.\n        \"\"\"\n        ...\n\n    @classmethod\n    def get_or_create_kwargs(\n        cls, dynamic_importer: Optional[DynamicImporter] = None, **kwargs: Dict\n    ) -> Dict:\n        \"\"\"Gets the kwargs for the model.\n\n        Args:\n            dynamic_importer: The dynamic importer to use for importing modules.\n            kwargs: The kwargs to use for the model.\n\n        Returns:\n            The kwargs for the model.\n        \"\"\"\n        return kwargs\n\n    @classmethod\n    def get_dynamic_config_class(\n        cls, dynamic_config_class_name: str, dynamic_importer: Optional[DynamicImporter] = None\n    ) -> Type[\"DynamicConfig\"]:\n        \"\"\"Gets a dynamic configuration class.\n\n        Args:\n            dynamic_config_class_name: The name of the dynamic configuration class.\n            dynamic_importer: The dynamic importer to use for importing modules.\n\n        Returns:\n            The dynamic configuration class.\n\n        Raises:\n            ValueError: If the dynamic configuration class is not found.\n        \"\"\"\n        if dynamic_importer is None:\n            dynamic_importer = DynamicImporter()\n        dynamic_config = dynamic_importer.find_class(dynamic_config_class_name)\n        if dynamic_config is None:\n            raise ValueError(f\"Dynamic config {dynamic_config_class_name} not found\")\n        if not issubclass(dynamic_config, DynamicConfig):\n            raise ValueError(\n                f\"Dynamic config {dynamic_config_class_name} is not a subclass of DynamicConfig\"\n            )\n        return dynamic_config\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with Logfire Support\nDESCRIPTION: Command to install Mirascope with Logfire integration support using pip package manager\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/integrations/logfire.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install \"mirascope[logfire]\"\n```\n\n----------------------------------------\n\nTITLE: Lilypad Project Structure\nDESCRIPTION: Shows the recommended directory structure for a Lilypad project including configuration, LLM functions and database files.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-versioning.md#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n|\n|-- .lilypad/\n|-- |-- config.json\n|-- lily/\n|   |-- __init__.py\n|   |-- {llm_function_name}.py\n|-- pad.db\n```\n\n----------------------------------------\n\nTITLE: Defining a Pydantic model for verification questions\nDESCRIPTION: This code defines a Pydantic model `VerificationQuestions` to structure the output of the verification question generation. It contains a list of strings, where each string represents a verification question. This model ensures a consistent format for handling verification questions within the Chain of Verification pipeline.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/chain_of_verification.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass VerificationQuestions(BaseModel):\n    questions: list[str] = Field(\n        ...,\n        description=\"\"\"A list of questions that verifies if the response\n        answers the original query correctly.\"\"\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This snippet sets the OpenAI API key as an environment variable. Replace `YOUR_API_KEY` with your actual OpenAI API key.  This is required to authenticate with the OpenAI API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/document_segmentation.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"import os\\n\\nos.environ[\\\"OPENAI_API_KEY\\\"] = \\\"YOUR_API_KEY\\\"\\n# Set the appropriate API key for the provider you're using\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Full Groq API Response in Python\nDESCRIPTION: This asynchronous function retrieves the full response from the Groq API. It reads the entire response, parses the JSON data, and returns the generated text content.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/groq/call.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nasync def _get_full_response(response: httpx.Response) -> str:\n    data = await response.json()\n    return data[\"choices\"][0][\"text\"]\n```\n\n----------------------------------------\n\nTITLE: Running the Librarian Assistant\nDESCRIPTION: This snippet demonstrates how to create an instance of the Librarian class and run the assistant to interact with users and process their queries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/sql_agent.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlibrarian = Librarian()\nawait librarian.run()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key\nDESCRIPTION: Setting up environment variables for OpenAI API authentication.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using \n```\n\n----------------------------------------\n\nTITLE: Defining CallResponseChunk Class in Python for Mirascope Core\nDESCRIPTION: This code defines the CallResponseChunk class, which represents a chunk of a call response. It includes attributes for content, role, and name, and implements methods for string representation and equality comparison.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/call_response_chunk.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass CallResponseChunk:\n    \"\"\"A chunk of a call response.\"\"\"\n\n    def __init__(self, content: str, role: str, name: str | None = None):\n        \"\"\"Initialize a CallResponseChunk.\n\n        Args:\n            content: The content of the chunk.\n            role: The role of the chunk.\n            name: The name of the chunk.\n        \"\"\"\n        self.content = content\n        self.role = role\n        self.name = name\n\n    def __str__(self) -> str:\n        \"\"\"Return a string representation of the chunk.\"\"\"\n        return self.content\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"Check if two chunks are equal.\"\"\"\n        if not isinstance(other, CallResponseChunk):\n            return NotImplemented\n        return (\n            self.content == other.content\n            and self.role == other.role\n            and self.name == other.name\n        )\n```\n\n----------------------------------------\n\nTITLE: Summarizing Text Snippet in Plaintext\nDESCRIPTION: This snippet demonstrates a simple plaintext prompt designed to summarize a given article about the melting of polar ice caps and its impacts. The prompt instructs the model to summarize the article content, but the outcome shows a lack of detailed guidance.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-prompt.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nInstruction: \"Summarize the following article.\"\n\n\"The rapid melting of polar ice caps has accelerated in recent years, leading to a significant rise in global sea levels. Coastal cities around the world are facing increased flooding risks, with some communities already experiencing regular inundations during high tides. Scientists warn that without immediate action to reduce greenhouse gas emissions, these trends will continue, putting millions of people at risk of displacement. Moreover, the economic impact on these regions could be devastating, with billions of dollars in property damage and the loss of vital infrastructure. Governments and international organizations are now pushing for more aggressive climate policies to mitigate these effects and protect vulnerable populations.\"\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI Support\nDESCRIPTION: Command to install Mirascope library with OpenAI integration using pip.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI extras\nDESCRIPTION: This command installs the Mirascope library along with the OpenAI extras. This ensures that you have all the necessary dependencies for interacting with OpenAI models.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/langgraph_vs_mirascope/quickstart.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"!pip install \\\"mirascope[openai]\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Multi-Provider Translation\nDESCRIPTION: Installation of ipywidgets package needed for Jupyter Notebook compatibility with the translation functionality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_translation.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!pip install ipywidgets # for Jupyter Notebook\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment with Mirascope and Other Dependencies in Bash\nDESCRIPTION: Installs the Mirascope library peculiarly configured for OpenAI along with dependencies essential for enabling web search functionalities in Python projects. The commands set up the required packages enabling Mirascope functionalities and web search capabilities with libraries like `requests` and `beautifulsoup4`.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install \"mirascope[openai]\"\n# for web search functionality \npip install requests beautifulsoup4 duckduckgo-search ipytest\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key\nDESCRIPTION: Configures the Anthropic API key for LLM interactions\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/llm_validation_with_retries.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI Support in Python\nDESCRIPTION: The snippet installs Mirascope with OpenAI support using pip. This is necessary to set up the environment for handling support tickets with an LLM backend.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/support_ticket_routing.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Installing HyperDX Integration for Mirascope\nDESCRIPTION: Command to install Mirascope with HyperDX integration using pip package manager with the hyperdx extras flag.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/integrations/hyperdx.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install \"mirascope[hyperdx]\"\n```\n\n----------------------------------------\n\nTITLE: Documenting Bedrock Tool Module Path\nDESCRIPTION: Markdown documentation showing the module path for a Bedrock tool implementation\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/bedrock/tool.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# mirascope.core.bedrock.tool\n\n::: mirascope.core.bedrock.tool\n```\n\n----------------------------------------\n\nTITLE: Migrating Function Calling (Tools) in Mirascope from v0 to v1 in Python\nDESCRIPTION: Illustrates the transition of using tools (function calling) from v0 to v1. The new approach uses a BaseTool class and applies tools through the 'call' decorator, offering a more streamlined way to implement and use tools in LLM calls.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.openai import OpenAICall, OpenAICallParams, OpenAITool\n\n\nclass FormatBook(OpenAITool):\n    title: str\n    author: str\n\n    def call(self):\n        return f\"{self.title} by {self.author}\"\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n    call_params = OpenAICallParams(tools=[FormatBook], tool_choice=\"required\")\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseTool, openai, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    def call(self):\n        return f\"{self.title} by {self.author}\"\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    tools=[FormatBook],\n    call_params={\"tool_choice\": \"required\"},\n)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book.\"\n\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Web Search Dependencies\nDESCRIPTION: Installing required packages for web search functionality including DuckDuckGo search, BeautifulSoup for parsing, and requests for HTTP operations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install duckduckgo-search beautifulsoup4 requests \n```\n\n----------------------------------------\n\nTITLE: Downloading HTML Content with Curl in Python\nDESCRIPTION: This snippet uses the curl command to download a Wikipedia article on Python programming language, saving it as 'wikipedia-python.html'. Ensure curl is installed; otherwise, the HTML file must be downloaded manually.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_summarization.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!curl \"https://en.wikipedia.org/wiki/Python_(programming_language)\" -o wikipedia-python.html\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope and Setting OpenAI API Key\nDESCRIPTION: This snippet shows how to install Mirascope with OpenAI support and set the OpenAI API key as an environment variable.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/sql_agent.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenTelemetry support\nDESCRIPTION: Command to install Mirascope with OpenTelemetry integration using pip with the opentelemetry extras flag.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/integrations/otel.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install \"mirascope[opentelemetry]\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python\nDESCRIPTION: Illustrates how to set the OpenAI API key in the environment variables using Python's os module, which is necessary for authenticating API calls.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/structured_outputs.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\n# Set your API keys\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: OpenAI API Key Configuration\nDESCRIPTION: Environment setup for OpenAI API authentication.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Handling Call Response in Custom Middleware for Mirascope in Python\nDESCRIPTION: Implements a handler function for standard Mirascope call responses. It extracts metadata from the response and saves it to the database using SQLModel.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/extensions/middleware.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef handle_call_response(result: BaseCallResponse, fn: Callable, session: Session):\n    metadata = Metadata(\n        function_name=fn.__name__,\n        model=result.model,\n        provider=result.provider,\n        response=result.text,\n        total_tokens=result.usage.total_tokens,\n        prompt_tokens=result.usage.prompt_tokens,\n        completion_tokens=result.usage.completion_tokens,\n        latency=result.latency,\n    )\n    session.add(metadata)\n    session.commit()\n    return result\n```\n\n----------------------------------------\n\nTITLE: Installing UV Package Manager on macOS/Linux\nDESCRIPTION: Command to install the UV package manager on macOS and Linux systems using curl.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Text Completion Prompt in Plaintext\nDESCRIPTION: This plaintext text completion prompt asks the model to finish a given nursery rhyme. The input provides a partially complete phrase that the LLM is expected to extend based on learned data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-prompt.md#2025-04-21_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nFully complete the rest of this nursery rhyme,\n\nJack and Jill went up the hill\nTo fetch a pail of water;\nJack fell down and broke his crown,\nAnd Jill came....\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Configure the OpenAI API key for authentication and access\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/generating_synthetic_data.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: LangChain Runnable Example for Chatbot Pipeline\nDESCRIPTION: This snippet illustrates a LangChain runnable pipeline for processing a user query in a corporate chatbot. It demonstrates the use of RunnablePassthrough, prompt formatting, and model binding with a stop condition.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-make-a-chatbot.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrunnable = (\n    {\"equation_statement\": RunnablePassthrough()}\n    | prompt\n    | model.bind(stop=\"END\")\n    | StrOutputParser()\n)\nprint(runnable.invoke(\"What is the status of my expense reimbursement request?\"))\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Configure the OpenAI API key as an environment variable for authentication and access\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/web_search_agent.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Git Push Commands\nDESCRIPTION: Command to push feature branch to remote repository.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ngit push origin meaningful-branch-name\n```\n\n----------------------------------------\n\nTITLE: Defining CallResponse Class for Mistral API in Python\nDESCRIPTION: This class represents a response from a Mistral API call. It includes properties for the response content, token usage, and finish reason. The class is designed to standardize and facilitate the handling of Mistral API responses within the Mirascope framework.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/mistral/call_response.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass CallResponse:\n    \"\"\"A response from a Mistral API call.\"\"\"\n\n    def __init__(\n        self,\n        content: str,\n        prompt_tokens: int,\n        completion_tokens: int,\n        finish_reason: str,\n    ) -> None:\n        self.content = content\n        self.prompt_tokens = prompt_tokens\n        self.completion_tokens = completion_tokens\n        self.finish_reason = finish_reason\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API environment variable in Python\nDESCRIPTION: Sets the environment variable for the OpenAI API key necessary for making calls to the OpenAI models. The user must replace 'YOUR_API_KEY' with their actual OpenAI API key to use this functionality.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/extraction_using_vision.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Install Mirascope with OpenAI support and pandas for data manipulation\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/generating_synthetic_data.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\" pandas\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Lilypad Prompt Management\nDESCRIPTION: This snippet shows the predefined directory structure created by Lilypad for prompt management. It includes a `.lilypad` directory for configuration, a `lily` directory for LLM functions, and a `pad.db` SQLite database for storing versions and traces. This structure centralizes prompt development and supports collaboration.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\n\n|\n|-- .lilypad/\n|-- |-- config.json\n|-- lily/\n|   |-- __init__.py\n|   |-- {llm_function_name}.py\n|-- pad.db\n\n```\n\n----------------------------------------\n\nTITLE: Package Installation Command\nDESCRIPTION: Installation command for required Python packages including Mirascope with OpenAI support and web scraping tools.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/blog_writing_agent.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\" requests beautifulsoup4 duckduckgo-search tenacity\n```\n\n----------------------------------------\n\nTITLE: LangChain Vector Store Retrieval and Chaining\nDESCRIPTION: Demonstrates creating a retrieval chain using LangChain components for context-based question answering with vector store retrieval\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-prompt-template.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nvectorstore = FAISS.from_texts(\n    [\"Julia is an expert in machine learning\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\n\nquestion = \"what is Julia's expertise?\"\ncontext = retriever.invoke(\"Julia is an expert in machine learning\")[\n    0\n]  # Retrieve context based on the input\n\nprompt = ChatPromptTemplate.from_template(\n    template.format(context=context, question=question)\n)\nmodel = ChatOpenAI()\n\nretrieval_chain = prompt | model | StrOutputParser()\n\nresult = retrieval_chain.invoke({\"question\": \"What is Julia's expertise?\"})\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Front Matter for Mirascope Documentation Home Page\nDESCRIPTION: This YAML configuration defines the template, title, and social card settings for the Mirascope documentation home page. It specifies the home.html template to be used and sets up social card layout options with a descriptive title.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/index.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntemplate: home.html\ntitle: Mirascope\nsocial:\n  cards_layout_options:\n    title: Documentation that simply works\n---\n```\n\n----------------------------------------\n\nTITLE: Testing Prompt Evaluation Workflow in Python\nDESCRIPTION: Demonstrates testing the prompt evaluation workflow by passing specific role requirements and candidate profiles to evaluate correspondence. Utilizes an asynchronous `run` function to call the evaluation with expected input types and prints the result for manual verification. The snippet affirms the functionality and use-case fit aligning with structured evaluation responses.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nasync def run(role_requirements: str, candidate_profile: str, llm_recommendation: str):\n    return await evaluate_candidate_suitability(\n        role_requirements,\n        candidate_profile,\n        llm_recommendation\n    )\n\n# Example usage\nrole_requirements = \"Minimum 5 years in Python backend development, strong communication, and teamwork skills.\"\ncandidate_profile = (\n    \"Name: Mary Jane\\n\"\n    \"Experience: 4 years in Python, 2 years in DevOps\\n\"\n    \"Key Strengths: Microservices architecture, containerization (Docker/Kubernetes)\\n\"\n    \"Soft Skills: Excellent written and verbal communication\\n\"\n)\nllm_recommendation = (\n    \"Based on the role requirements and Mary\\'s background, she appears to be a strong match. \"\n    \"She might need some support to bridge the experience gap, but her DevOps knowledge could be a plus.\"\n)\n\n# Run the evaluation\nresult = await run(role_requirements, candidate_profile, llm_recommendation)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI\nDESCRIPTION: This snippet installs the Mirascope library along with the OpenAI integration using pip.  This is a prerequisite step for using Mirascope with OpenAI models.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/pii_scrubbing.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"!pip install \\\"mirascope[openai]\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installation commands for setting up Mirascope and LangChain packages required for the RAG application.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-rag.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npip install \"mirascope[openai]\"\npip install -qU langchain-openai\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with Langfuse Support\nDESCRIPTION: Installation command for adding Langfuse integration to Mirascope using pip with the langfuse extras flag.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/integrations/langfuse.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install \"mirascope[langfuse]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope Dependencies - Python\nDESCRIPTION: This snippet installs Mirascope along with its OpenAI dependencies using pip. Ensure Python and pip are set up in your environment before executing this command.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/text_classification.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This snippet sets the OpenAI API key as an environment variable. This is necessary for authenticating with the OpenAI service. Replace `YOUR_API_KEY` with your actual API key.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/pii_scrubbing.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python\nDESCRIPTION: This snippet configures the OpenAI API key as an environment variable. This is necessary to allow access to the OpenAI API.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/knowledge_graph.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Git Feature Branch Commands\nDESCRIPTION: Commands for creating and committing to a feature branch.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout -b meaningful-branch-name\ngit add mirascope/modified_file.py tests/test_modified_file.py\ngit commit -m \"feat: specific description of changes contained in commit\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key Environment Variable\nDESCRIPTION: Sets the OpenAI API key as an environment variable for authentication and API access\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/generating_captions.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up Git Repository\nDESCRIPTION: Commands to clone the forked repository and add the upstream remote.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/<your-username>/mirascope.git\ngit remote add upstream https://github.com/Mirascope/mirascope.git\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Evaluation in Python\nDESCRIPTION: Uses regex pattern matching as a hardcoded evaluation method. This approach checks if the actual answer contains patterns defined in the expected answer, providing flexibility beyond exact matching.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/evals.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/evals/hardcoded/regex.py\"\n```\n\n----------------------------------------\n\nTITLE: Installing UV Package Manager on Windows\nDESCRIPTION: PowerShell command to install the UV package manager on Windows systems.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope Dependencies\nDESCRIPTION: Installs Mirascope with Anthropic and Tenacity support for LLM validation\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/llm_validation_with_retries.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[anthropic,tenacity]\"\n```\n\n----------------------------------------\n\nTITLE: Managing State in Mirascope v1 LLM Calls with Python\nDESCRIPTION: Illustrates how to manage state in v1 by using a class that wraps the LLM call. This approach provides a clear distinction between state and call arguments, offering a more flexible and intuitive way to handle stateful LLM interactions.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/MIGRATE.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel, computed_field\n\n\nclass Librarian(BaseModel):\n    genre: str\n\n    @openai.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian. You specialize in the {self.genre} genre\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def call(self, query: str): ...\n\n    @computed_field\n    @property\n    def history(self) -> list[BaseMessageParam | openai.OpenAIMessageParam]:\n        \"\"\"Returns dummy history for demonstration purposes\"\"\"\n        return [\n            {\"role\": \"user\", \"content\": \"What book should I read?\"},\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Do you like fantasy books?\",\n            },\n        ]\n\n\nfantasy_librarian = Librarian(genre=\"fantasy\")\nresponse = fantasy_librarian.call(\"I do like fantasy books!\")\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This snippet sets the OpenAI API key in the environment variables to allow Mirascope to authenticate requests to the OpenAI service.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/quickstart.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Setting Up Virtual Environment with UV\nDESCRIPTION: Command to create and sync a virtual environment with all extras and dev dependencies using UV.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nuv sync --all-extras --dev\n```\n\n----------------------------------------\n\nTITLE: Testing Conversation with LLM in Python\nDESCRIPTION: This snippet defines an asynchronous test case for evaluating LLM-generated conversations. It simulates user messages and checks if the LLM correctly understands the context to generate the expected SQL insert statement for updating the reading list.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_sql_agent.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai.types.chat import (\\\n    ChatCompletionAssistantMessageParam,\\\n    ChatCompletionUserMessageParam,\\\n)\\\n\\\n@pytest.mark.asyncio\\\nasync def test_conversation(mock_librarian: Librarian):\\\n    mock_librarian.messages = [\\\n        ChatCompletionUserMessageParam(\\\n            role=\"user\", content=\"Can you recommend me a fantasy book?\"\\\n        ),\\\n        ChatCompletionAssistantMessageParam(\\\n            role=\"assistant\",\\\n            content=\"I would recommend 'The Name of the Wind' by Patrick Rothfuss. It’s the first book in 'The Kingkiller Chronicle' series and features a beautifully written story about a gifted young man who grows up to be a legendary figure. It's filled with magic, adventure, and rich character development. I believe you'll enjoy it! Would you like to add it to your reading list?\",\\\n        ),\\\n    ]\\\n    response = await mock_librarian._stream(\"Can you add it to my reading list?\")\\\n    async for _, tool in response:\\\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\\\n        assert (\\\n            query\\\n            == \"INSERT INTO ReadingList (title, status) VALUES ('The Name of the Wind', 'Not Started');\"\\\n        )\\\n\\\nipytest.run()\n```\n\n----------------------------------------\n\nTITLE: Importing Call Parameters Module for Mirascope XAI\nDESCRIPTION: This code snippet imports the call_params module from the Mirascope core XAI package. It suggests that the file contains definitions and functions related to call parameters used in explainable AI operations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/xai/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n::: mirascope.core.xai.call_params\n```\n\n----------------------------------------\n\nTITLE: Git Sync Commands\nDESCRIPTION: Commands to sync with main repository for different types of changes.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# for anything that only requires a fix version bump (e.g. bug fixes)\ngit checkout main\ngit pull upstream main\n\n# for anything that is \"new\" and requires at least a minor version bump\ngit checkout release/vX.Y  # replace X with the current major version and Y with the next minor version\ngit pull upstream release/vX.Y\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for SQL Generation Evaluation\nDESCRIPTION: This snippet installs the necessary Python packages for the evaluation process, including Mirascope with OpenAI support, pytest, ipytest, and pytest-asyncio.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_sql_agent.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n!pip install pytest ipytest pytest-asyncio\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Mirascope\nDESCRIPTION: This code sets the OpenAI API key as an environment variable for use with Mirascope. It's a crucial step for authenticating API requests.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/self_ask.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Fast Documentation Development Server\nDESCRIPTION: Command to run a faster version of the documentation development server.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/CONTRIBUTING.md#2025-04-21_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n# Run the fast docs development server\nuv run python scripts/serve_docs_dev.py\n```\n\n----------------------------------------\n\nTITLE: Creating Data Functions for Support Scenarios in Python\nDESCRIPTION: The snippet defines helper functions for retrieving on-sale items, available rewards, billing, and account details. These functions imitate typical business operations that provide user-specific information.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/support_ticket_routing.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_sale_items():\n    return \"Sale items: we have a monitor at half off for $80!\"\n\ndef get_rewards(user: User):\n    if sum(user.past_charges) > 300:\n        return \"Rewards: for your loyalty, you get 10% off your next purchase!\"\n    else:\n        return \"Rewards: you have no rewards available right now.\"\n\ndef get_billing_details(user: User):\n    return {\n        \"user_email\": user.email,\n        \"user_name\": user.name,\n        \"past_purchases\": user.past_purchases,\n        \"past_charges\": user.past_charges,\n    }\n\ndef get_account_details(user: User):\n    return {\n        \"user_email\": user.email,\n        \"user_name\": user.name,\n        \"password\": user.password,\n        \"security_question\": user.security_question,\n        \"security_answer\": user.security_answer,\n    }\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Web Search Agent Evaluation in Python\nDESCRIPTION: This snippet installs the necessary Python packages for the Web Search Agent evaluation, including Mirascope with OpenAI support, and web search related libraries.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/evals/evaluating_web_search_agent.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n# for web search functionality\n!pip install requests beautifulsoup4 duckduckgo-search ipytest\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Documentation Agent\nDESCRIPTION: This snippet installs the necessary packages: Mirascope with OpenAI support and LlamaIndex for embedding and retrieving embeddings from a vectorstore.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/documentation_agent.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n# LLamaIndex for embedding and retrieving embeddings from a vectorstore\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Using Logfire with Example Code\nDESCRIPTION: Placeholder for example code showing Logfire integration with various LLM providers, using the with_logfire decorator to automatically log calls and capture spans\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/integrations/logfire.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/integrations/logfire/{{ provider | provider_dir }}/{{ method }}.py\"\n```\n\n----------------------------------------\n\nTITLE: Creating Google Search Tool with Nimble API\nDESCRIPTION: This snippet defines a function using the Nimble API to perform a Google search and retrieve content from the results. It handles HTTP requests and processes the returned HTML to extract relevant text.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/search_with_sources.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\n\nNIMBLE_TOKEN = \"YOUR_NIMBLE_API_KEY\"\n\ndef nimble_google_search(query: str):\n    \"\"\"\n    Use Nimble to get information about the query using Google Search.\n    \"\"\"\n    url = \"https://api.webit.live/api/v1/realtime/serp\"\n    headers = {\n        \"Authorization\": f\"Basic {NIMBLE_TOKEN}\",\n        \"Content-Type\": \"application/json\",\n    }\n    search_data = {\n        \"parse\": True,\n        \"query\": query,\n        \"search_engine\": \"google_search\",\n        \"format\": \"json\",\n        \"render\": True,\n        \"country\": \"US\",\n        \"locale\": \"en\",\n    }\n    response = requests.get(url, json=search_data, headers=headers)\n    data = response.json()\n    results = data[\"parsing\"][\"entities\"][\"OrganicResult\"]\n    urls = [result.get(\"url\", \"\") for result in results]\n    search_results = {}\n    for url in urls:\n        content = get_content(url)\n        search_results[url] = content\n    return search_results\n\n\ndef get_content(url: str):\n    data = []\n    response = requests.get(url)\n    content = response.content\n    soup = BeautifulSoup(content, \"html.parser\")\n    paragraphs = soup.find_all(\"p\")\n    for paragraph in paragraphs:\n        data.append(paragraph.text)\n    return \"\\n\".join(data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Response Models with Structured Outputs\nDESCRIPTION: Demonstrates setting up response models with strict structured outputs using ResponseModelConfigDict and json_mode.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/provider_specific_features/openai.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n--8<-- \"examples/learn/provider_specific_features/openai/structured_outputs/response_model.py\"\n```\n\n----------------------------------------\n\nTITLE: Implementing ClaudeStreamer for Anthropic Claude model responses\nDESCRIPTION: A concrete implementation of BedrockStreamer for Anthropic Claude models. It processes streaming responses from Claude models, extracting text content from each response chunk.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/bedrock/stream.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ClaudeStreamer(BedrockStreamer):\n    \"\"\"Streamer for Anthropic Claude models.\"\"\"\n\n    def __call__(self, response_stream: Iterator[Dict[str, Any]]) -> Iterator[str]:\n        \"\"\"Process a streaming response from Amazon Bedrock for Claude models.\n\n        Args:\n            response_stream: The streaming response from Amazon Bedrock.\n\n        Returns:\n            An iterator of strings representing the processed response.\n        \"\"\"\n        for response in response_stream:\n            chunk = json.loads(response.get(\"chunk\", {}).get(\"bytes\", b\"{}\").decode(\"utf-8\"))\n            content = chunk.get(\"completion\", \"\")\n            if content:\n                yield content\n```\n\n----------------------------------------\n\nTITLE: Rendering the Knowledge Graph using NetworkX and Matplotlib\nDESCRIPTION: This snippet visualizes the generated knowledge graph using NetworkX and Matplotlib. It renders nodes and edges, displays labels, and presents the structure in a graphical format.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/how-to-build-a-knowledge-graph.md#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n\ndef render_graph(kg: KnowledgeGraph):\n    G = nx.DiGraph()\n\n    for node in kg.nodes:\n        G.add_node(node.id, label=node.type, **(node.properties or {}))\n\n    for edge in kg.edges:\n        G.add_edge(edge.source, edge.target, label=edge.relationship)\n\n    plt.figure(figsize=(15, 10))\n    pos = nx.spring_layout(G)\n\n    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color=\"lightblue\")\n    nx.draw_networkx_edges(G, pos, arrowstyle=\"->\", arrowsize=20)\n    nx.draw_networkx_labels(G, pos, font_size=12, font_weight=\"bold\")\n\n    edge_labels = nx.get_edge_attributes(G, \"label\")\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color=\"red\")\n\n    plt.title(\"Knowledge Graph Visualization\", fontsize=15)\n    plt.show()\n\n\nrender_graph(kg)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stream Class in Python\nDESCRIPTION: Defines the Stream base class with standard iteration methods and stream management functionality. The class implements __iter__ and __next__ methods for iteration, with support for tracking stream state and yielding values with optional metadata.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/stream.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, Iterator, Optional, Tuple, TypeVar, cast\n\nfrom pydantic import BaseModel\n\nT = TypeVar(\"T\")\n\n\nclass Stream(BaseModel, Iterator[T]):\n    \"\"\"Base class for stream functionality.\n\n    This class provides the basic functionality for streaming data.\n    Subclasses should implement the `_yield_next` method to yield the next value in the stream.\n    \"\"\"\n\n    _stream_has_started: bool = False\n    _stream_has_completed: bool = False\n\n    def __iter__(self) -> Iterator[T]:\n        \"\"\"Initialize the stream and return this instance as an iterator.\"\"\"\n        self._stream_has_started = True\n        return self\n\n    def __next__(self) -> T:\n        \"\"\"Return the next value in the stream, or raise StopIteration if complete.\"\"\"\n        if self._stream_has_completed:\n            raise StopIteration\n\n        yielded = self._yield_next()\n        if yielded is None:\n            self._stream_has_completed = True\n            raise StopIteration\n\n        value, metadata = yielded\n        return value\n\n    def _yield_next(self) -> Optional[Tuple[T, Dict[str, Any]]]:\n        \"\"\"Yield the next value in the stream, along with any metadata.\n\n        Returns:\n            Optional[Tuple[T, Dict[str, Any]]]: A tuple of the next value and a dictionary of metadata,\n                                               or None if the stream is complete.\n        \"\"\"\n        return None\n```\n\n----------------------------------------\n\nTITLE: Using Messages Methods to Create Prompt Templates in Python\nDESCRIPTION: Shows how to create a prompt template using Mirascope's Messages.Role methods to generate BaseMessageParam instances.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/learn/prompts.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import Messages, prompt_template\n\n\n@prompt_template\ndef recommend_book_prompt(genre: str) -> Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book_prompt(genre=\"fantasy\"))\n\n```\n\n----------------------------------------\n\nTITLE: Binding Arguments to LLM Calls in LangChain\nDESCRIPTION: Demonstrates how to dynamically specify LLM call parameters in LangChain using the bind() method. The example shows binding tool definitions to a ChatOpenAI model instance for getting stock prices.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-flow-vs-langchain.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_stock_price\",\n            \"description\": \"Get the current stock price for a given company\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"company\": {\n                        \"type\": \"string\",\n                        \"description\": \"The company symbol, e.g., AAPL for Apple Inc.\",\n                    },\n                },\n                \"required\": [\"company\"],\n            },\n        },\n    }\n]\n\nmodel = ChatOpenAI(model=\"gpt-4\").bind(tools=tools)\nmodel.invoke(\"What's the stock price for AAPL, MSFT, and GOOGL?\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Tab-CoT with OpenAI GPT-4\nDESCRIPTION: Sets up a function using OpenAI's GPT-4 model to process queries with Tabular Chain of Thought formatting. Uses decorators for OpenAI call configuration and prompt templating.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/text_based/tabular_chain_of_thought.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\ntab_cot_augment = \"|step|subquestion|process|result|\"\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    {query}\n    {tab_cot_augment}\n    \"\"\"\n)\ndef call(query: str, tab_cot_prompt: bool = False) -> openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"tab_cot_augment\": tab_cot_augment if tab_cot_prompt else \"\",\n        }\n    }\n\nprompt = \"\"\"A pet store had 102 puppies. In one day they sold 21 of them and put\nthe rest into cages with 9 in each cage. How many cages did they use?\"\"\"\n\nprint(call(query=prompt, tab_cot_prompt=True))\n```\n\n----------------------------------------\n\nTITLE: Switching LLM Providers with Mirascope in Python\nDESCRIPTION: Demonstrates how to switch between different LLM providers using Mirascope. This example shows changing from OpenAI to Anthropic by simply modifying the decorator and model name.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Provider-Agnostic LLM Calls with Runtime Override - Python\nDESCRIPTION: This snippet illustrates how to make provider-agnostic LLM calls using Mirascope's @llm.call decorator, allowing runtime switching of LLM providers and models, along with overriding parameters like temperature.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-chaining.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope import llm\n\n@llm.call(provider=\"openai\", model=\"gpt-4o-mini\")\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n\noverride_response = llm.override(\n    recommend_book,\n    provider=\"anthropic\",\n    model=\"claude-3-5-sonnet-20240620\",\n    call_params={\"temperature\": 0.7},\n)(\"fantasy\")\n\nprint(override_response.content)\n```\n\n----------------------------------------\n\nTITLE: Using Custom OpenAI-Compatible Client with Mirascope in Python\nDESCRIPTION: Illustrates how to use a custom OpenAI-compatible client (in this case, Ollama) with Mirascope's decorator. This allows for flexibility in choosing LLM providers while maintaining a consistent interface.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai\nfrom openai import OpenAI\n\n\nclient = OpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\",  # required, but unused\n)\n\n\n@openai.call(\"gpt-4o-mini\", client=client)\ndef recommend_book(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Book Recommendation by Age - Python\nDESCRIPTION: This snippet demonstrates how to recommend books based on age. It dynamically adjusts the reading level output based on the user's age input, utilizing conditional logic in a callable.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/getting_started/dynamic_configuration_and_chaining.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book_by_age(genre: str, age: int) -> Messages.Type:\n    reading_level = \"adult\"\n    if age < 12:\n        reading_level = \"elementary\"\n    elif age < 18:\n        reading_level = \"young adult\"\n    return f\"Recommend a {genre} book with a reading level of {reading_level}\"\n\n\nresponse = recommend_book_by_age(\"fantasy\", 15)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Throttling and Retry Mechanism for API Calls\nDESCRIPTION: Creates a throttle decorator and a throttled_groq_call function to manage API call rates and implement retry logic for robustness.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef throttle(calls_per_minute: int) -> Callable:\n    min_interval = 60.0 / calls_per_minute\n    last_called: list[float] = [0.0]\n\n    def decorator(func: Callable) -> Callable:\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            elapsed = time.time() - last_called[0]\n            left_to_wait = min_interval - elapsed\n            if left_to_wait > 0:\n                time.sleep(left_to_wait)\n            ret = func(*args, **kwargs)\n            last_called[0] = time.time()\n            return ret\n\n        return wrapper\n\n    return decorator\n\n\n# Modify the groq_call decorator to include throttling and retrying\ndef throttled_groq_call(*args: Any, **kwargs: Any) -> Any:\n    @retry(\n        wait=wait_exponential(multiplier=1, min=4, max=10), stop=stop_after_attempt(5)\n    )\n    @throttle(calls_per_minute=6)  # Adjust this value based on your rate limit\n    def wrapped_call(*call_args, **call_kwargs):\n        return groq_call(*args, **kwargs)(*call_args, **call_kwargs)\n\n    return wrapped_call\n```\n\n----------------------------------------\n\nTITLE: Colocating Prompt and Parameters with OpenAI Call in Python\nDESCRIPTION: A complete example showing how to colocate the prompt template, model selection, and temperature parameters with the LLM call function. This promotes code cleanliness and makes the function self-contained.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-best-practices.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\", call_params={\"temperature\": 0.9})\n@prompt_template(\"Provide the current weather for {location}\")\ndef report_weather(location: str): ...\n\n\nresponse = report_weather(location=\"San Francisco\")\nprint(response.content)  # prints the string content of the call\nprint(response.call_params) # prints the call parameters of the call\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Prompt Templates in Mirascope with Python\nDESCRIPTION: Demonstrates how to create a simple prompt template function using Mirascope's prompt_template decorator. The function takes a genre parameter and returns a formatted prompt requesting a book recommendation.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-alternatives.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -> str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# > [BaseMessageParam(role='user', content='Recommend a fantasy book')]\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for RAG Implementation\nDESCRIPTION: Imports the necessary modules from LlamaIndex for document loading, indexing, and vectorizing, as well as from Mirascope for interfacing with the OpenAI API and creating prompt templates.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/rag-llm-example.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom mirascope.core import openai, prompt_template\n```\n\n----------------------------------------\n\nTITLE: Expected response for Text Classification Prompt\nDESCRIPTION: This is the expected response for the text classification prompt, correctly categorizing the email as spam. It demonstrates the LLM's ability to identify spam based on typical spam indicators.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-engineering-examples.md#2025-04-21_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Spam\"\n```\n\n----------------------------------------\n\nTITLE: LangChain Temperature Conversion Pipeline\nDESCRIPTION: Example of LangChain's chaining approach using RunnablePassthrough for temperature conversion. Demonstrates binding runtime parameters and pipeline construction.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llamaindex-vs-langchain.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Setup for temperature conversion using a query pipeline\nrunnable = (\n    {\"temperature_query\": RunnablePassthrough()}\n    | prompt\n    | model.bind(stop=\"CONVERSION_COMPLETED\")\n    | StrOutputParser()\n)\nprint(runnable.invoke(\"Convert 35 degrees Celsius to Fahrenheit\"))\n```\n\n----------------------------------------\n\nTITLE: Importing required modules for Chain of Verification\nDESCRIPTION: Imports necessary modules from mirascope and pydantic. These modules provide the foundation for defining LLM calls, prompt templates, and data models to structure the Chain of Verification process.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/prompt_engineering/chaining_based/chain_of_verification.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n```\n\n----------------------------------------\n\nTITLE: Storing Document Embeddings with LlamaIndex\nDESCRIPTION: Process for loading documents and storing their embeddings in a local vector store using LlamaIndex.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/local_chat_with_codebase.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.storage import StorageContext\nfrom llama_index.core.vector_stores import SimpleVectorStore\n\ndocuments = SimpleDirectoryReader(\"PATH/TO/YOUR/DOCS\").load_data()\nvector_store = SimpleVectorStore()\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\nindex.storage_context.persist()\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex with Ollama and HuggingFace Embeddings\nDESCRIPTION: Setup of LlamaIndex configuration using Ollama for the LLM and HuggingFace for embeddings.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/local_chat_with_codebase.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import (\n    Settings,\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n)\nfrom llama_index.legacy.embeddings import HuggingFaceEmbedding\nfrom llama_index.legacy.llms import Ollama\n\nSettings.llm = Ollama(model=\"llama3.1\")\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n```\n\n----------------------------------------\n\nTITLE: Basic Anthropic API Call\nDESCRIPTION: Demonstrates a basic call to Anthropic's Claude API for book recommendations.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nanthropic_recommend_book = anthropic.call(\"claude-3-5-sonnet-20240620\")(book_recommendation_prompt)\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n```\n\n----------------------------------------\n\nTITLE: Defining FunctionCallParams Class for LiteLLM in Python\nDESCRIPTION: This class specifies parameters for function calling in LiteLLM API requests. It includes options for function names and forced choices.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/litellm/call_params.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass FunctionCallParams(TypedDict, total=False):\n    name: str | None\n    functions: list[dict[str, Any]] | None\n    function_call: str | dict[str, Any] | None\n```\n\n----------------------------------------\n\nTITLE: Human Evaluation: Paired Comparisons Example\nDESCRIPTION: This example illustrates how paired comparisons can be used to evaluate prompts by comparing two or more outputs generated from different prompts.  Reviewers select the one that better meets the evaluation criteria, focusing on aspects like informativeness or engagement and provide justification for their choice.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/prompt-evaluation.md#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n\"Task 1: Which response do you believe provides a better answer? Justify your choice.\\n\\nPrompt = \\\"Compare these two responses to the question: 'What are the benefits of regular exercise?'\\\"\\n\\nResponse A = \\\"Regular exercise can help maintain a healthy weight, reduce the risk of chronic diseases, and improve mental health.\\\" \\nResponse B = \\\"Exercising regularly is good because it keeps you fit and makes you feel happy.\\\"\\n\\nTask 2: Explain what makes one response superior to the other. \\n\\nPrompt = \\\"Evaluate the following outputs for the question: 'How does exercise affect mental health?'\\\"\\n\\nResponse A = \\\"Exercise releases endorphins, which can reduce feelings of depression and anxiety.\\\" \\nResponse B = \\\"When you work out, you feel better mentally because it helps with stress.\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Streaming with Mirascope\nDESCRIPTION: Demonstrates asynchronous streaming capabilities in Mirascope which allow for concurrency when processing model responses. This improves efficiency by letting the application handle other tasks while waiting for data chunks.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-orchestration.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nfrom mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o\", stream=True)\nasync def recommend_book(genre: str) -> str:\n    return f\"Recomment a {genre} book\"\n\n\nasync def stream_book_recommendation(genre: str):\n    stream = await recommend_book(genre=genre)\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(stream_book_recommendation(genre=\"science fiction\"))\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema and Type Aliases in Python\nDESCRIPTION: This snippet defines type aliases for JSON structures and schemas used in Mirascope. It includes definitions for JSONSchema, JSONType, and utility types for working with JSON data.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/base/types.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Union\n\nJSONSchema = Dict[str, Any]\nJSONType = Union[Dict[str, Any], List[Any], str, int, float, bool, None]\nJSONObject = Dict[str, JSONType]\nJSONArray = List[JSONType]\n```\n\n----------------------------------------\n\nTITLE: Installing Mirascope with OpenAI support in Python\nDESCRIPTION: This snippet shows how to install Mirascope with optional OpenAI dependencies using pip. No additional dependencies are required other than pip.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/more_advanced/extraction_using_vision.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Prompting in Plaintext\nDESCRIPTION: A zero-shot prompting example in plaintext where the model is tasked with translating an English sentence to French. This prompt relies solely on the model's pre-trained data without additional context.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/llm-prompt.md#2025-04-21_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nTranslate \"Hello, how are you?\" from English to French.\n```\n\n----------------------------------------\n\nTITLE: Defining CallParams Class for Cohere API in Python\nDESCRIPTION: This class represents the parameters for a Cohere API call. It includes attributes for model, temperature, max_tokens, k, p, frequency_penalty, presence_penalty, and stop sequences. The class uses dataclasses for efficient attribute management.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/api/core/cohere/call_params.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@dataclass\nclass CallParams:\n    \"\"\"Parameters for a Cohere call.\"\"\"\n\n    model: Optional[str] = None\n    temperature: Optional[float] = None\n    max_tokens: Optional[int] = None\n    k: Optional[int] = None\n    p: Optional[float] = None\n    frequency_penalty: Optional[float] = None\n    presence_penalty: Optional[float] = None\n    stop_sequences: Optional[List[str]] = None\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert the CallParams to a dictionary.\"\"\"\n        return {k: v for k, v in asdict(self).items() if v is not None}\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"CallParams\":\n        \"\"\"Create a CallParams from a dictionary.\"\"\"\n        return cls(**{k: v for k, v in data.items() if k in cls.__dataclass_fields__})\n```\n\n----------------------------------------\n\nTITLE: Creating a prompt template using a string template\nDESCRIPTION: This snippet demonstrates an alternative way to create a prompt template in Mirascope by directly passing a string template as an argument to the `@prompt_template` decorator. The function `book_recommendation_prompt` now only takes the genre as input, as the template logic is handled by the decorator.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/engineers-should-handle-prompting-llms.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mirascope.core import prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book\")\ndef book_recommendation_prompt(genre: str): ...\n\n\nprompt = book_recommendation_prompt(\"fantasy\")\nprint(prompt)\n# > [BaseMessageParam(role='user', content='Recommend a fantasy book')]\n```\n\n----------------------------------------\n\nTITLE: Importing LangChain Dependencies\nDESCRIPTION: Import statements for LangChain components needed for document processing, embedding, and vector storage.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/langchain-rag.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport bs4\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Qwant Search Agent\nDESCRIPTION: Installs necessary Python packages including mirascope with Groq support, requests, BeautifulSoup, python-dotenv, and tenacity.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/qwant_search_agent_with_sources.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"mirascope[groq]\" requests beautifulsoup4 python-dotenv tenacity\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Configuration of OpenAI API key as an environment variable.\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/blog/posts/synthetic-data-generation.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Install necessary Python packages for web search and content extraction functionality\nSOURCE: https://github.com/mirascope/mirascope/blob/main/docs/tutorials/agents/web_search_agent.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install \"mirascope[openai]\" beautifulsoup4  duckduckgo-search requests\n```"
  }
]
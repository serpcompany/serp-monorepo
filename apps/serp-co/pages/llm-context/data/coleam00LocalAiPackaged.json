[
  {
    "owner": "coleam00",
    "repo": "local-ai-packaged",
    "content": "TITLE: Setting environment variables (Bash)\nDESCRIPTION: This code block shows the environment variables that need to be configured in the `.env` file. These variables are critical for the proper operation of the various services within the self-hosted AI package, including N8N, Supabase, and Langfuse. Each variable controls a specific aspect of the services, such as encryption keys, database passwords, and hostnames. Properly configuring these variables ensures that the services can communicate securely and function as expected. Note that secure random values must be generated for all secrets for production use.\nSOURCE: https://github.com/coleam00/local-ai-packaged/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n############\n# N8N Configuration\n############\nN8N_ENCRYPTION_KEY=\nN8N_USER_MANAGEMENT_JWT_SECRET=\n\n############\n# Supabase Secrets\n############\nPOSTGRES_PASSWORD=\nJWT_SECRET=\nANON_KEY=\nSERVICE_ROLE_KEY=\nDASHBOARD_USERNAME=\nDASHBOARD_PASSWORD=\nPOOLER_TENANT_ID=\n\n############\n# Langfuse credentials\n############\n\nCLICKHOUSE_PASSWORD=\nMINIO_ROOT_PASSWORD=\nLANGFUSE_SALT=\nNEXTAUTH_SECRET=\nENCRYPTION_KEY=\n```\n\n----------------------------------------\n\nTITLE: Cloning the repository and navigating to the project directory (Bash)\nDESCRIPTION: These commands clone the project repository from GitHub and then changes the current working directory to the cloned repository folder, preparing for subsequent setup steps.\nSOURCE: https://github.com/coleam00/local-ai-packaged/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/coleam00/local-ai-packaged.git\ncd local-ai-packaged\n```\n\n----------------------------------------\n\nTITLE: Stopping, Updating, and Restarting Dockerized AI Services - Docker Compose Bash\nDESCRIPTION: This sequence of bash commands stops all Docker Compose services for the stack, pulls the latest versions of all configured containers (including n8n, Open WebUI, Supabase, and related), and finally restarts them with the selected compute profile using a supporting Python script. Required dependencies include Docker Compose and Python installed on the system, as well as access to the relevant YAML files and the start_services.py script. The <your-profile> placeholder should be replaced by a supported profile name (cpu, gpu-nvidia, gpu-amd, or none). These commands expect the shell environment to be in the project root and may require elevated permissions depending on Docker/socket setup.\nSOURCE: https://github.com/coleam00/local-ai-packaged/blob/main/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Stop all services\ndocker compose -p localai --profile <your-profile> -f docker-compose.yml -f supabase/docker/docker-compose.yml down\n\n# Pull latest versions of all containers\ndocker compose -p localai --profile <your-profile> -f docker-compose.yml -f supabase/docker/docker-compose.yml pull\n\n# Start services again with your desired profile\npython start_services.py --profile <your-profile>\n\n```\n\n----------------------------------------\n\nTITLE: Setting Caddy Configuration Environment Variables (Bash)\nDESCRIPTION: These environment variables configure the Caddy web server for managing HTTPS/TLS for custom domains. They define the hostnames for various services like N8N, Open WebUI, Flowise, Supabase, Ollama, and SearXNG. LETSENCRYPT_EMAIL is used for Let's Encrypt certificate generation. Commenting indicates optional or non-production settings.\nSOURCE: https://github.com/coleam00/local-ai-packaged/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n############\n# Caddy Config\n############\n\nN8N_HOSTNAME=n8n.yourdomain.com\nWEBUI_HOSTNAME=:openwebui.yourdomain.com\nFLOWISE_HOSTNAME=:flowise.yourdomain.com\nSUPABASE_HOSTNAME=:supabase.yourdomain.com\nOLLAMA_HOSTNAME=:ollama.yourdomain.com\nSEARXNG_HOSTNAME=searxng.yourdomain.com\nLETSENCRYPT_EMAIL=your-email-address\n```\n\n----------------------------------------\n\nTITLE: Running the start_services.py script with Nvidia GPU (Bash)\nDESCRIPTION: This command executes the `start_services.py` script using Python, specifying the `gpu-nvidia` profile.  This indicates that the services should be started with configurations optimized for Nvidia GPUs, leveraging GPU acceleration where possible.\nSOURCE: https://github.com/coleam00/local-ai-packaged/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython start_services.py --profile gpu-nvidia\n```\n\n----------------------------------------\n\nTITLE: Running the start_services.py script with CPU (Bash)\nDESCRIPTION: This command executes the `start_services.py` script using Python, specifying the `cpu` profile. This indicates that the services should be started with configurations that use the CPU for processing, rather than a GPU.\nSOURCE: https://github.com/coleam00/local-ai-packaged/blob/main/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython start_services.py --profile cpu\n```\n\n----------------------------------------\n\nTITLE: Running the start_services.py script with AMD GPU (Bash)\nDESCRIPTION: This command executes the `start_services.py` script using Python, specifying the `gpu-amd` profile. This indicates that the services should be started with configurations optimized for AMD GPUs, leveraging GPU acceleration where possible.\nSOURCE: https://github.com/coleam00/local-ai-packaged/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython start_services.py --profile gpu-amd\n```\n\n----------------------------------------\n\nTITLE: Running the start_services.py script without GPU profile (Bash)\nDESCRIPTION: This command executes the `start_services.py` script using Python, specifying the `none` profile. This likely means to start most services but depend on an external Ollama installation. Useful for mac users.\nSOURCE: https://github.com/coleam00/local-ai-packaged/blob/main/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython start_services.py --profile none\n```\n\n----------------------------------------\n\nTITLE: OLLAMA_HOST Configuration for Mac (YAML)\nDESCRIPTION: This YAML snippet configures the `OLLAMA_HOST` environment variable for the n8n service when running Ollama locally on a Mac. It sets the host to `host.docker.internal:11434`, allowing the n8n Docker container to connect to the Ollama instance running on the host machine.\nSOURCE: https://github.com/coleam00/local-ai-packaged/blob/main/README.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nx-n8n: &service-n8n\n  # ... other configurations ...\n  environment:\n    # ... other environment variables ...\n    - OLLAMA_HOST=host.docker.internal:11434\n```"
  }
]
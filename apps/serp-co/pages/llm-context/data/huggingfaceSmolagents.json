[
  {
    "owner": "huggingface",
    "repo": "smolagents",
    "content": "TITLE: Initializing and Running a CodeAgent with Default Model and Tool (Python)\nDESCRIPTION: Shows how to initialize a CodeAgent with the default InferenceClientModel and a DuckDuckGoSearchTool, then run it on a sample prompt. Requires smolagents to be installed and that the necessary classes are imported from the package. The agent takes a natural language query and executes it using its available tools, returning the result.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import CodeAgent, DuckDuckGoSearchTool, InferenceClientModel\n\nmodel = InferenceClientModel()\nagent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)\n\nagent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Multi-step Agent Loop with Python\nDESCRIPTION: This snippet demonstrates the core structure of a multi-step agent where an LLM iteratively determines actions based on accumulated memory until a termination condition is met. Dependencies include an LLM interface providing should_continue and get_next_action functions and an execute_action function to perform the determined actions. The snippet uses a Python while loop to process agent actions and append observations, representing the memory. Inputs are an initial user-defined task and stateful memory, and output is the updated memory log capturing the history of actions and observations. The construct supports flexible workflows controlled by the LLM's output in a persistent state loop.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/conceptual_guides/intro_agents.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmemory = [user_defined_task]\nwhile llm_should_continue(memory): # यह लूप मल्टी-स्टेप भाग है\n    action = llm_get_next_action(memory) # यह टूल-कॉलिंग भाग है\n    observations = execute_action(action)\n    memory += [action, observations]\n```\n\n----------------------------------------\n\nTITLE: Multi-step Agent Loop in Python\nDESCRIPTION: Implements a multi-step agent loop that maintains a memory of actions and observations, executing actions until a satisfactory task completion criteria is met. It exemplifies how an LLM-driven agent can iteratively decide actions based on accumulated memory.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/conceptual_guides/intro_agents.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nmemory = [user_defined_task]\nwhile llm_should_continue(memory): # this loop is the multi-step part\n    action = llm_get_next_action(memory) # this is the tool-calling part\n    observations = execute_action(action)\n    memory += [action, observations]\n```\n\n----------------------------------------\n\nTITLE: Defining an SQL Query Tool Using Smolagents Decorator in Python\nDESCRIPTION: Defines a tool function 'sql_engine' decorated with '@tool' from the smolagents framework, enabling execution of arbitrary SQL queries against the defined 'receipts' table. The tool returns a string representation of the SQL query results. Requires smolagents installed and a live SQLAlchemy engine instance. Input is an SQL query string, output is a concatenated string of query result rows.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/text_to_sql.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import tool\n\n@tool\ndef sql_engine(query: str) -> str:\n    \"\"\"\n    Allows you to perform SQL queries on the table. Returns a string representation of the result.\n    The table is named 'receipts'. Its description is as follows:\n        Columns:\n        - receipt_id: INTEGER\n        - customer_name: VARCHAR(16)\n        - price: FLOAT\n        - tip: FLOAT\n\n    Args:\n        query: The query to perform. This should be correct SQL.\n    \"\"\"\n    output = \"\"\n    with engine.connect() as con:\n        rows = con.execute(text(query))\n        for row in rows:\n            output += \"\\n\" + str(row)\n    return output\n```\n\n----------------------------------------\n\nTITLE: Description of the Smolagents API and Agent Hierarchy\nDESCRIPTION: This section explains the overall purpose of the Smolagents API, detailing the roles of different agent classes, their inheritance structure, and the supported functionalities such as multi-step reasoning and tool calling in both Python and JSON formats. It also covers auxiliary components like UI integrations and prompt templates.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/agents.mdx#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Sharing and Loading Agents with Hugging Face Hub in Python\nDESCRIPTION: Code examples for pushing a configured agent to the Hugging Face Hub and loading it back with proper security considerations. Shows how to share and reuse custom agents.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Push agent to Hub\nagent.push_to_hub(\"m-ric/my_agent\")\n\n# Load agent from Hub\nagent.from_hub(\"m-ric/my_agent\", trust_remote_code=True)\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool Using Function Decorator in Python\nDESCRIPTION: Implementation of a custom tool using the @tool decorator pattern that retrieves the most downloaded model for a specified task from the Hugging Face Hub. Includes proper type hints and documentation.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import tool\n\n@tool\ndef model_download_tool(task: str) -> str:\n    \"\"\"\n    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n    It returns the name of the checkpoint.\n\n    Args:\n        task: The task for which to get the download count.\n    \"\"\"\n    most_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n    return most_downloaded_model.id\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables with python-dotenv in Python\nDESCRIPTION: This snippet imports the load_dotenv function from the dotenv package and calls it to load environment variables from a .env file into the runtime environment. It is critical for providing authentication tokens (such as HF_TOKEN) needed for calling inference providers. python-dotenv must be installed, and the .env file must be present and contain the desired variables.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with TransformersModel (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a CodeAgent with a local Transformers model. It requires the `smolagents[transformers]` extra to be installed. The agent is then used to calculate the 118th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[transformers]\nfrom smolagents import CodeAgent, TransformersModel\n\nmodel_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n\nmodel = TransformersModel(model_id=model_id)\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Sharing Custom Tool to Hugging Face Hub (Python)\nDESCRIPTION: Demonstrates how to share a custom `smolagents` tool instance to the Hugging Face Hub using the `push_to_hub` method. It requires a repository ID and an API token with write access to upload the tool's code and metadata to the specified location on the Hub.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_downloads_tool.push_to_hub(\"{your_username}/hf-model-downloads\", token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n```\n\n----------------------------------------\n\nTITLE: Building an SQL Execution Tool with smolagents (Python)\nDESCRIPTION: This snippet defines the 'sql_engine' function, decorated with smolagents.tool, to expose a tool for secure SQL query execution via an LLM agent. The function takes a raw SQL string, executes it using SQLAlchemy, and returns results as a formatted string. The schema information is included in the function docstring as required by the agent framework. This tool requires smolagents and SQLAlchemy dependencies, and is parameterized by the incoming SQL query.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import tool\n\n@tool\ndef sql_engine(query: str) -> str:\n    \"\"\"\n    Allows you to perform SQL queries on the table. Returns a string representation of the result.\n    The table is named 'receipts'. Its description is as follows:\n        Columns:\n        - receipt_id: INTEGER\n        - customer_name: VARCHAR(16)\n        - price: FLOAT\n        - tip: FLOAT\n\n    Args:\n        query: The query to perform. This should be correct SQL.\n    \"\"\"\n    output = \"\"\n    with engine.connect() as con:\n        rows = con.execute(text(query))\n        for row in rows:\n            output += \"\\n\" + str(row)\n    return output\n```\n\n----------------------------------------\n\nTITLE: Using LangChain Tool with smolagents Agent (Python)\nDESCRIPTION: Initializes a `CodeAgent` using an `InferenceClientModel` and the previously wrapped LangChain search tool. It executes the agent with a natural language query, demonstrating how `smolagents` can utilize integrated LangChain capabilities to perform tasks like web search.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nagent = CodeAgent(tools=[search_tool], model=model)\n\nagent.run(\"How many more blocks (also denoted as layers) are in BERT base encoder compared to the encoder from the architecture proposed in Attention is All You Need?\")\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Model for Smolagents in Python\nDESCRIPTION: This snippet demonstrates how to create a custom model for the Smolagents framework by subclassing the base `Model` class and implementing the `generate` method. The model utilizes the Hugging Face InferenceClient for interacting with an LLM and requires a Hugging Face API token. The `generate` method accepts a list of messages in a specific format, and a `stop_sequences` argument, and returns an object containing the model's output, adhering to the specified format.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom huggingface_hub import login, InferenceClient\n\nlogin(\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n\nclient = InferenceClient(model=model_id)\n\nclass CustomModel(Model):\n    def generate(messages, stop_sequences=[\"Task\"]):\n        response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1024)\n        answer = response.choices[0].message\n        return answer\n\ncustom_model = CustomModel()\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Local TransformersModel for Offline Inference (Python)\nDESCRIPTION: Demonstrates setting up a local TransformersModel by specifying model_id, token limit, and device mapping. Can be used with any local transformer model compatible with the transformers library. Useful when Internet access or privacy is a concern.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import TransformersModel\n\nmodel = TransformersModel(\n    model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n    max_new_tokens=4096,\n    device_map=\"auto\"\n)\n\n```\n\n----------------------------------------\n\nTITLE: Using Space Tool with smolagents Agent (Python)\nDESCRIPTION: Initializes a `smolagents.CodeAgent` using an `InferenceClientModel` and the previously imported Gradio Space tool. It shows how to run the agent with a prompt and pass additional arguments using the `additional_args` dictionary, illustrating the agent's ability to leverage the external tool.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nmodel = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\nagent = CodeAgent(tools=[image_generation_tool], model=model)\n\nagent.run(\n    \"Improve this prompt, then generate an image of it.\", additional_args={'user_prompt': 'A rabbit wearing a space suit'}\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Hugging Face InferenceClientModel Agent in Python\nDESCRIPTION: This snippet demonstrates creating a smolagents CodeAgent instance running on a remote Hugging Face Inference API model. It requires the 'smolagents' package and optionally an API token using the 'token' parameter or environment variable 'HF_TOKEN' for rate limit enhancements or access to restricted models. The 'model_id' specifies the LLM to use. The agent executes tasks via the run() method, receiving a string input and generating a response by leveraging the model and tools. No tools are included, but base tools are added via 'add_base_tools=True'.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n\nmodel = InferenceClientModel(model_id=model_id, token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Gradio Space as Tool in smolagents (Python)\nDESCRIPTION: Demonstrates importing a Gradio Space hosted on the Hugging Face Hub as a `smolagents` tool using `Tool.from_space`. It takes the Space ID, a desired tool name, and a description, creating a tool instance that internally uses `gradio-client` to interact with the specified Space.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimage_generation_tool = Tool.from_space(\n    \"black-forest-labs/FLUX.1-schnell\",\n    name=\"image_generator\",\n    description=\"Generate an image from a prompt\"\n)\n\nimage_generation_tool(\"A sunny beach\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Tool from Hugging Face Hub\nDESCRIPTION: This snippet shows how to load a tool from the Hugging Face Hub using `load_tool`. It requires specifying the repository ID of the tool and setting `trust_remote_code=True` to allow execution of custom code. The loaded tool can then be used within a Smolagents agent.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/tools.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import load_tool, CodeAgent\n\nmodel_download_tool = load_tool(\n    \"{your_username}/hf-model-downloads\",\n    trust_remote_code=True\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Tool from Hugging Face Hub (Python)\nDESCRIPTION: Shows how to load a previously shared `smolagents` tool directly from the Hugging Face Hub using the `load_tool` function. It requires the repository ID and setting `trust_remote_code=True` due to executing remote code, returning a usable tool instance for agent initialization.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import load_tool, CodeAgent\n\nmodel_download_tool = load_tool(\n    \"{your_username}/hf-model-downloads\",\n    trust_remote_code=True\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a SQL query execution tool with smolagents in Python\nDESCRIPTION: Creates a tool named 'sql_engine' using smolagents' @tool decorator, enabling execution of arbitrary SQL queries on the 'receipts' table. The function connects to the database, executes the input query, and returns the results as a concatenated string, providing a bridge for the LLM to perform SQL operations.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/text_to_sql.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import tool\n\n@tool\ndef sql_engine(query: str) -> str:\n    \"\"\"\n    Allows you to perform SQL queries on the table. Returns a string representation of the result.\n    The table is named 'receipts'. Its description is as follows:\n        Columns:\n        - receipt_id: INTEGER\n        - customer_name: VARCHAR(16)\n        - price: FLOAT\n        - tip: FLOAT\n\n    Args:\n        query: The query to perform. This should be correct SQL.\n    \"\"\"\n    output = \"\"\n    with engine.connect() as con:\n        rows = con.execute(text(query))\n        for row in rows:\n            output += \"\\n\" + str(row)\n    return output\n```\n\n----------------------------------------\n\nTITLE: Creating a CodeAgent with Smolagents and LLM Integration in Python\nDESCRIPTION: Constructs a smolagents 'CodeAgent' that uses the previously defined 'sql_engine' tool and an LLM model accessed via 'InferenceClientModel'. The agent runs a natural language prompt, internally translating it to SQL and querying the database. Requires smolagents and appropriate LLM credentials. The example uses 'meta-llama/Meta-Llama-3.1-8B-Instruct' as the model identifier for the InferenceClientModel.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/text_to_sql.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nagent = CodeAgent(\n    tools=[sql_engine],\n    model=InferenceClientModel(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n)\nagent.run(\"Can you give me the name of the client who got the most expensive receipt?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with AmazonBedrockServerModel (Basic) (Python)\nDESCRIPTION: This snippet demonstrates basic usage of AmazonBedrockServerModel, connecting directly to Amazon Bedrock. The `smolagents[aws_sdk]` extra must be installed. The agent is then used to calculate the 118th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[aws_sdk]\nfrom smolagents import CodeAgent, AmazonBedrockServerModel\n\nmodel = AmazonBedrockServerModel(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\")\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with InferenceClientModel (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a CodeAgent with InferenceClientModel, leveraging a Hugging Face Inference Endpoint. It requires a HF_TOKEN for authentication and shows how to either set it as an environment variable or pass it directly during initialization. The agent is then used to calculate the 118th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\" \n\nmodel = InferenceClientModel(model_id=model_id, token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\") # You can choose to not pass any model_id to InferenceClientModel to use a default model\n# you can also specify a particular provider e.g. provider=\"together\" or provider=\"sambanova\"\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Sharing a Custom Tool to Hugging Face Hub with push_to_hub in Python\nDESCRIPTION: Shows how to push a custom tool instance to Hugging Face Hub using `push_to_hub`, requiring a valid repository and API token. All methods and imports should be self-contained within the class. Class attributes are preferred over init parameters for static content.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/tools.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmodel_downloads_tool.push_to_hub(\"{your_username}/hf-model-downloads\", token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with LiteLLMModel (OpenAI/Anthropic) (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a CodeAgent with LiteLLMModel, using either OpenAI or Anthropic. It requires setting the environment variable `ANTHROPIC_API_KEY` or `OPENAI_API_KEY`, or passing the `api_key` during initialization. The agent is then used to calculate the 118th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[litellm]\nfrom smolagents import CodeAgent, LiteLLMModel\n\nmodel = LiteLLMModel(model_id=\"anthropic/claude-3-5-sonnet-latest\", api_key=\"YOUR_ANTHROPIC_API_KEY\") # Could use 'gpt-4o'\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running Multi-Agent System in E2B Sandbox Python\nDESCRIPTION: This Python code sets up and runs a multi-agent system within an E2B sandbox. It creates an E2B `Sandbox`, installs `smolagents`, and defines two agents: a coder agent and a manager agent. The coder agent is used to solve a difficult algorithmic problem (calculating the 20th Fibonacci number), which is managed by the manager agent.  The `run_code_raise_errors` function is used for running the code in the sandbox and handles potential errors.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/secure_code_execution.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom e2b_code_interpreter import Sandbox\nimport os\n\n# Create the sandbox\nsandbox = Sandbox()\n\n# Install required packages\nsandbox.commands.run(\"pip install smolagents\")\n\ndef run_code_raise_errors(sandbox, code: str, verbose: bool = False) -> str:\n    execution = sandbox.run_code(\n        code,\n        envs={'HF_TOKEN': os.getenv('HF_TOKEN')}\n    )\n    if execution.error:\n        execution_logs = \"\\n\".join([str(log) for log in execution.logs.stdout])\n        logs = execution_logs\n        logs += execution.error.traceback\n        raise ValueError(logs)\n    return \"\\n\".join([str(log) for log in execution.logs.stdout])\n\n# Define your agent application\nagent_code = \"\"\"\nimport os\nfrom smolagents import CodeAgent, InferenceClientModel\n\n# Initialize the agents\nagent = CodeAgent(\n    model=InferenceClientModel(token=os.getenv(\"HF_TOKEN\"), provider=\"together\"),\n    tools=[],\n    name=\"coder_agent\",\n    description=\"This agent takes care of your difficult algorithmic problems using code.\"\n)\n\nmanager_agent = CodeAgent(\n    model=InferenceClientModel(token=os.getenv(\"HF_TOKEN\"), provider=\"together\"),\n    tools=[],\n    managed_agents=[agent],\n)\n\n# Run the agent\nresponse = manager_agent.run(\"What's the 20th Fibonacci number?\")\nprint(response)\n\"\"\"\n\n# Run the agent code in the sandbox\nexecution_logs = run_code_raise_errors(sandbox, agent_code)\nprint(execution_logs)\n```\n\n----------------------------------------\n\nTITLE: Initializing SmolAgent for Web Automation in Python\nDESCRIPTION: This snippet initializes the web automation agent. It first sets up the language model using `InferenceClientModel` (specifying `meta-llama/Llama-3.3-70B-Instruct`). Then, it creates a `CodeAgent` instance, providing the previously defined tools (`go_back`, `close_popups`, `search_item_ctrl_f`), the model, authorizing `helium` imports, adding the `save_screenshot` callback, setting a maximum step limit, and configuring verbosity. Finally, it pre-imports `helium` functions into the agent's execution environment using `agent.python_executor`.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\n# Initialize the model\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"  # You can change this to your preferred model\nmodel = InferenceClientModel(model_id=model_id)\n\n# Create the agent\nagent = CodeAgent(\n    tools=[go_back, close_popups, search_item_ctrl_f],\n    model=model,\n    additional_authorized_imports=[\"helium\"],\n    step_callbacks=[save_screenshot],\n    max_steps=20,\n    verbosity_level=2,\n)\n\n# Import helium for the agent\nagent.python_executor(\"from helium import *\", agent.state)\n```\n\n----------------------------------------\n\nTITLE: Replaying an Agent Run with Smolagents in Python\nDESCRIPTION: This snippet demonstrates how to initialize a CodeAgent from the smolagents library, execute a task, and replay the agent's memory of the run. Required dependencies include the smolagents package. The user provides a natural language query to the agent's run method and can subsequently invoke replay() to review the sequence of steps executed, including tool calls and observations. This method is essential for debugging and visual analysis of agent runs; expected input is a string prompt, and the output is the agent's result and memory replay.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/memory.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent\n\nagent = CodeAgent(tools=[], model=InferenceClientModel(), verbosity_level=0)\n\nresult = agent.run(\"What's the 20th Fibonacci number?\")\n```\n\nLANGUAGE: python\nCODE:\n```\nagent.replay()\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with E2B Executor\nDESCRIPTION: This snippet demonstrates how to initialize a `CodeAgent` to use the E2B remote code execution service. This provides a sandboxed environment for code execution, enhancing security by isolating the code from the local environment. It involves specifying the `executor_type=\"e2b\"` flag and including the dependencies of the tools within `additional_authorized_imports`.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/secure_code_execution.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, VisitWebpageTool, InferenceClientModel\nagent = CodeAgent(\n    tools = [VisitWebpageTool()],\n    model=InferenceClientModel(),\n    additional_authorized_imports=[\"requests\", \"markdownify\"],\n    executor_type=\"e2b\"\n)\n\nagent.run(\"What was Abraham Lincoln's preferred pet?\")\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables with python-dotenv in Python\nDESCRIPTION: Loads environment variables from a `.env` file to access sensitive data like the HF_TOKEN for Hugging Face API calls. The snippet uses the 'python-dotenv' package to enable safe token loading and authentication needed for querying Hugging Face Inference API.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/rag.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with E2B Executor Python\nDESCRIPTION: This snippet demonstrates how to initialize a `CodeAgent` with the `executor_type` set to \"e2b\".  This allows the agent to execute code within an E2B sandbox.  It requires the `smolagents` library to be installed.  The agent is then used to get the 100th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/secure_code_execution.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent\n\nagent = CodeAgent(model=InferenceClientModel(), tools=[], executor_type=\"e2b\")\n\nagent.run(\"Can you give me the 100th Fibonacci number?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Inference Model and Web Search Agent using SmolAgents in Python\nDESCRIPTION: Sets up an inference model client using the previously specified model ID, then configures a 'ToolCallingAgent' to perform web searches. The web agent combines the DuckDuckGoSearchTool and the custom visit_webpage tool, is limited to a maximum of 10 steps, and includes descriptive name and description. Dependencies: smolagents, Hugging Face InferenceClientModel, DuckDuckGoSearchTool, visit_webpage tool. Inputs: model_id string. Output: constructed agent objects ready for managed orchestration.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/multiagents.mdx#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import (\n    CodeAgent,\n    ToolCallingAgent,\n    InferenceClientModel,\n    ManagedAgent,\n    DuckDuckGoSearchTool,\n    LiteLLMModel,\n)\n\nmodel = InferenceClientModel(model_id=model_id)\n\nweb_agent = ToolCallingAgent(\n    tools=[DuckDuckGoSearchTool(), visit_webpage],\n    model=model,\n    max_steps=10,\n    name=\"search\",\n    description=\"Runs web searches for you. Give it your query as an argument.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Smolagents Docker Setup\nDESCRIPTION: This Dockerfile sets up a Python environment for running Smolagents inside a Docker container. It starts with a Python 3.10-bullseye base image, installs build dependencies, upgrades pip, and installs the smolagents package.  It sets the working directory to /app, runs as the 'nobody' user, and defines the default command to print 'Container ready'.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/secure_code_execution.mdx#_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM python:3.10-bullseye\n\n# Install build dependencies\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n        build-essential \\\n        python3-dev && \\\n    pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir smolagents && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Run with limited privileges\nUSER nobody\n\n# Default command\nCMD [\"python\", \"-c\", \"print('Container ready')\"]\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running a CodeAgent with External Tools and Planning in Python\nDESCRIPTION: This snippet sets up environment variables and loads two external tools: a text-to-image generation tool and a DuckDuckGo search tool. It then initializes a CodeAgent with these tools and a large language model inference client, enabling a planning interval that allows the agent to periodically update its internal state and plan its actions without immediate tool calls. The agent runs a query asking how long a cheetah would take to run a specific distance, demonstrating the integrated tool usage and autonomous planning. Key dependencies include the smolagents package, dotenv for environment variable loading, and the availability of the specified model and tools. Inputs include the query string, and output is the agent's response to the query.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import load_tool, CodeAgent, InferenceClientModel, DuckDuckGoSearchTool\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Import tool from Hub\nimage_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True)\n\nsearch_tool = DuckDuckGoSearchTool()\n\nagent = CodeAgent(\n    tools=[search_tool, image_generation_tool],\n    model=InferenceClientModel(model_id=\"Qwen/Qwen2.5-72B-Instruct\"),\n    planning_interval=3 # This is where you activate planning!\n)\n\n# Run it!\nresult = agent.run(\n    \"How long would a cheetah at full speed take to run the length of Pont Alexandre III?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Testing VisitWebpageTool\nDESCRIPTION: Initializes and tests the `visit_webpage` tool by printing the first 500 characters of the Markdown content from the Hugging Face Wikipedia page.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(visit_webpage(\"https://en.wikipedia.org/wiki/Hugging_Face\")[:500])\n```\n\n----------------------------------------\n\nTITLE: MultiStepAgent Class Overview\nDESCRIPTION: The `MultiStepAgent` class serves as a base for agents capable of performing multi-step reasoning processes involving sequential thought, tool calls, and executions. It is essential for building complex, interactive agents that handle multiple interactions. It is typically used as a parent class for more specialized agents.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/agents.mdx#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Creating a Custom Tool Using Function Decoration\nDESCRIPTION: Code demonstrating how to create a custom tool by decorating a function using the @tool decorator in smolagents.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/guided_tour.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import tool\n\n@tool\ndef model_download_tool(task: str) -> str:\n    \"\"\"\n    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n    It returns the name of the checkpoint.\n\n    Args:\n        task: The task for which to get the download count.\n    \"\"\"\n    most_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n    return most_downloaded_model.id\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio using Hugging Face Wav2Vec2 in Python\nDESCRIPTION: This snippet demonstrates how to perform speech transcription using the Hugging Face `transformers` library with a pre-trained Wav2Vec2 model. It involves loading a dataset, initializing the processor and model, processing audio data, running inference, and decoding the output back into text.\n\nDependencies: `transformers`, `datasets`, `torch`.\nInputs: Raw audio array and its sampling rate.\nOutput: A list containing the transcribed text string.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n>>> predicted_ids = torch.argmax(logits, dim=-1)\n\n>>> # transcribe speech\n>>> transcription = processor.batch_decode(predicted_ids)\n>>> transcription[0]\n'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n```\n\n----------------------------------------\n\nTITLE: Running a Smolagent Using CLI with Multi-Step CodeAgent (Bash)\nDESCRIPTION: Provides a CLI example that runs a multi-step agent with specified model type, model ID, extra imports, and the web_search tool. User supplies a planning prompt as the first argument and customizes model/tool selection via command-line flags. This enables agent execution directly from the console without Python scripting.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nsmolagent \"Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7.\"  --model-type \"InferenceClientModel\" --model-id \"Qwen/Qwen2.5-Coder-32B-Instruct\" --imports \"pandas numpy\" --tools \"web_search\"\n\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Tool Class with smolagents (Python)\nDESCRIPTION: Defines a custom tool class `HFModelDownloadsTool` that inherits from `smolagents.Tool`. It provides metadata like name, description, input types, and output type, wrapping a `forward` method that uses `huggingface_hub` to find the most downloaded model for a specified task and returns its ID.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import Tool\n\nclass HFModelDownloadsTool(Tool):\n    name = \"model_download_counter\"\n    description = \"\"\"\n    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n    It returns the name of the checkpoint.\"\"\"\n    inputs = {\n        \"task\": {\n            \"type\": \"string\",\n            \"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n        }\n    }\n    output_type = \"string\"\n\n    def forward(self, task: str):\n        from huggingface_hub import list_models\n\n        model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n        return model.id\n\nmodel_downloads_tool = HFModelDownloadsTool()\n```\n\n----------------------------------------\n\nTITLE: Constructing an agent with smolagents and executing a question in Python\nDESCRIPTION: Initializes a CodeAgent from smolagents with the SQL query tool and a specified language model, then runs a simple natural language question, demonstrating how the agent interacts with the tool and language model via a reasoning framework.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/text_to_sql.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nagent = CodeAgent(\n    tools=[sql_engine],\n    model=InferenceClientModel(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n)\nagent.run(\"Can you give me the name of the client who got the most expensive receipt?\")\n```\n\n----------------------------------------\n\nTITLE: Loading MCP Tools from ToolCollection (stdio)\nDESCRIPTION: This snippet demonstrates how to load tools from a stdio-based MCP server using `ToolCollection.from_mcp`. It initializes `StdioServerParameters` with the server command, arguments, and environment variables.  The tool collection is then used to create a `CodeAgent` which utilizes the loaded tools. Requires `smolagents` and `mcp` packages.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n```py\nfrom smolagents import ToolCollection, CodeAgent\nfrom mcp import StdioServerParameters\nimport os\n\nserver_parameters = StdioServerParameters(\n    command=\"uvx\",\n    args=[\"--quiet\", \"pubmedmcp@0.1.3\"],\n    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n)\n\nwith ToolCollection.from_mcp(server_parameters, trust_remote_code=True) as tool_collection:\n    agent = CodeAgent(tools=[*tool_collection.tools], model=model, add_base_tools=True)\n    agent.run(\"Please find a remedy for hangover.\")\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring LiteLLMModel for Anthropic's Claude via API Key (Python)\nDESCRIPTION: Shows how to configure a LiteLLMModel for use with the Anthropic Claude 3.5 Sonnet model, setting model_id, temperature, and using an environment variable for the API key. Requires smolagents and prior setup of the ANTHROPIC_API_KEY environment variable. Outputs a model instance ready for inference.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import LiteLLMModel\n\nmodel = LiteLLMModel(\n    model_id=\"anthropic/claude-3-5-sonnet-latest\",\n    temperature=0.2,\n    api_key=os.environ[\"ANTHROPIC_API_KEY\"]\n)\n\n```\n\n----------------------------------------\n\nTITLE: Sharing a CodeAgent to Hugging Face Hub (Python)\nDESCRIPTION: Demonstrates how to push a previously configured agent to Hugging Face Hub for sharing and reuse, and how to load it back with from_hub. Requires an initialized agent and a valid hub repository identifier. push_to_hub uploads the agent, making it accessible via the specified path; from_hub retrieves it for further usage.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nagent.push_to_hub(\"m-ric/my_agent\")\n\n# agent.from_hub(\"m-ric/my_agent\") to load an agent from Hub\n\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running an Agent with Custom Tool in Python\nDESCRIPTION: Code to initialize a CodeAgent with a custom tool and run it to find the most downloaded model for a specific task. Demonstrates basic agent usage with custom tools.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\nagent = CodeAgent(tools=[model_download_tool], model=InferenceClientModel())\nagent.run(\n    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Example Task Execution Cycle in CodeAgent Using Python Code Snippets\nDESCRIPTION: Multiple Python code snippets illustrate the agent's stepwise solution methodology for various tasks. Each example follows a sequential pattern: a 'Thought:' explanation, a 'Code:' block ending with '<end_code>', and potentially 'Observation:' outputs. They demonstrate usage of notional tools such as `search`, `translator`, `image_generator`, and `final_answer`. The dependencies include the agent environment providing these tools and the requirement to properly chain Thought, Code, and Observation for task-solving. Snippets show how to query, compute, translate, and aggregate information, capturing inputs such as textual questions or URLs and returning final answers or intermediate outputs through printing.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/building_good_agents.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nanswer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\nprint(answer)\n```\n\nLANGUAGE: python\nCODE:\n```\nimage = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\nfinal_answer(image)\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = 5 + 3 + 1294.678\nfinal_answer(result)\n```\n\nLANGUAGE: python\nCODE:\n```\ntranslated_question = translator(question=question, src_lang=\"French\", tgt_lang=\"English\")\nprint(f\"The translated question is {translated_question}.\")\nanswer = image_qa(image=image, question=translated_question)\nfinal_answer(f\"The answer is {answer}\")\n```\n\nLANGUAGE: python\nCODE:\n```\npages = search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\")\nprint(pages)\n```\n\nLANGUAGE: python\nCODE:\n```\npages = search(query=\"1979 interview Stanislaus Ulam\")\nprint(pages)\n```\n\nLANGUAGE: python\nCODE:\n```\nfor url in [\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\", \"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\"]:\n    whole_page = visit_webpage(url)\n    print(whole_page)\n    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Print separator between pages\n```\n\nLANGUAGE: python\nCODE:\n```\nfinal_answer(\"diminished\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfor city in [\"Guangzhou\", \"Shanghai\"]:\n    print(f\"Population {city}:\", search(f\"{city} population\"))\n```\n\nLANGUAGE: python\nCODE:\n```\nfinal_answer(\"Shanghai\")\n```\n\nLANGUAGE: python\nCODE:\n```\npope_age_wiki = wiki(query=\"current pope age\")\nprint(\"Pope age as per wikipedia:\", pope_age_wiki)\npope_age_search = web_search(query=\"current pope age\")\nprint(\"Pope age as per google search:\", pope_age_search)\n```\n\nLANGUAGE: python\nCODE:\n```\npope_current_age = 88 ** 0.36\nfinal_answer(pope_current_age)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Chat Models with HuggingFace InferenceClient in Python\nDESCRIPTION: This snippet demonstrates how to define a custom callable model function using the HuggingFace InferenceClient for chat completions. It logs into HuggingFace Hub, initializes an InferenceClient with a specified model ID, and defines a custom_model function that sends chat messages to the model supporting stop sequences. The function returns the generated answer from the model. Dependencies include 'huggingface_hub' and a valid API token. Inputs include a list of message dictionaries and an optional list of stop sequences; it outputs a string response from the model.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/reference/agents.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import login, InferenceClient\n\nlogin(\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n\nclient = InferenceClient(model=model_id)\n\ndef custom_model(messages, stop_sequences=[\"Task\"]):\n    response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)\n    answer = response.choices[0].message\n    return answer\n```\n\n----------------------------------------\n\nTITLE: Configuring AzureOpenAIServerModel with Environment Variables (Python)\nDESCRIPTION: Shows how to instantiate AzureOpenAIServerModel using environment variables for all required keys and configuration. This allows integration with Azure-hosted OpenAI endpoints for compliance or enterprise scenarios. Required environment variables are AZURE_OPENAI_MODEL, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, and OPENAI_API_VERSION.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom smolagents import AzureOpenAIServerModel\n\nmodel = AzureOpenAIServerModel(\n    model_id = os.environ.get(\"AZURE_OPENAI_MODEL\"),\n    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n    api_version=os.environ.get(\"OPENAI_API_VERSION\")    \n)\n\n```\n\n----------------------------------------\n\nTITLE: Running the Multi-Agent System\nDESCRIPTION: Runs the multi-agent system by passing a question to the `manager_agent`. The question requires both calculation and research using the web.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nanswer = manager_agent.run(\"If LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What would that correspond to, compared to some countries? Please provide a source for any numbers used.\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Filtering Hugging Face Dataset for Knowledge Base in Python\nDESCRIPTION: Loads the 'm-ric/huggingface_doc' dataset, filters documents to retain only those originating from the 'huggingface/transformers' source. Converts filtered dataset entries into LangChain Document objects tagged by their source for further processing.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/rag.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\nfrom langchain.docstore.document import Document\n\nknowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\nknowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n\nsource_docs = [\n    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n    for doc in knowledge_base\n]\n```\n\n----------------------------------------\n\nTITLE: Generating and Printing Updated Table Descriptions for LLM Prompting (Python)\nDESCRIPTION: This snippet iterates through the current tables ('receipts' and 'waiters'), collecting column metadata with SQLAlchemy's inspection system, and merges their descriptions into a single prompt string. This updated description is designed to provide the large language model with an up-to-date schema for multi-table query reasoning by the agent. Dependencies: SQLAlchemy and a connected engine with updated metadata.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nupdated_description = \"\"\"Allows you to perform SQL queries on the table. Beware that this tool's output is a string representation of the execution output.\nIt can use the following tables:\"\"\"\n\ninspector = inspect(engine)\nfor table in [\"receipts\", \"waiters\"]:\n    columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(table)]\n\n    table_description = f\"Table '{table}':\\n\"\n\n    table_description += \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n    updated_description += \"\\n\\n\" + table_description\n\nprint(updated_description)\n```\n\n----------------------------------------\n\nTITLE: Using DuckDuckGoSearchTool Directly (Python)\nDESCRIPTION: This snippet demonstrates how to instantiate and directly use a `DuckDuckGoSearchTool` independently of an agent. The tool object is created and then called like a function, passing the search query as an argument. The output of the tool (the search result) is then printed to the console.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import DuckDuckGoSearchTool\n\nsearch_tool = DuckDuckGoSearchTool()\nprint(search_tool(\"Who's the current president of Russia?\"))\n```\n\n----------------------------------------\n\nTITLE: Step-by-Step Agent Execution and Memory Editing (Python)\nDESCRIPTION: This snippet provides a template for manually executing an agent step by step, updating memory at each iteration. Particularly useful for long-running tasks, it shows creating an agent, appending task steps, running ActionStep cycles, and modifying memory as required per step. Requires smolagents and its ActionStep/TaskStep types. Inputs include a task string and optionally prior agent memory; outputs are printed intermediate/final results and an agent with updated execution trace.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/memory.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent, ActionStep, TaskStep\n\nagent = CodeAgent(tools=[], model=InferenceClientModel(), verbosity_level=1)\nprint(agent.memory.system_prompt)\n\ntask = \"What is the 20th Fibonacci number?\"\n\n# You could modify the memory as needed here by inputting the memory of another agent.\n# agent.memory.steps = previous_agent.memory.steps\n\n# Let's start a new task!\nagent.memory.steps.append(TaskStep(task=task, task_images=[]))\n\nfinal_answer = None\nstep_number = 1\nwhile final_answer is None and step_number <= 10:\n    memory_step = ActionStep(\n        step_number=step_number,\n        observations_images=[],\n    )\n    # Run one step.\n    final_answer = agent.step(memory_step)\n    agent.memory.steps.append(memory_step)\n    step_number += 1\n\n    # Change the memory as you please!\n    # For instance to update the latest step:\n    # agent.memory.steps[-1] = ...\n\nprint(\"The final answer is:\", final_answer)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Webpage Visiting Tool with Markdown Output in Python\nDESCRIPTION: Implements the 'visit_webpage' function as an agent tool to fetch and convert web page content into Markdown format. Uses 'requests' for HTTP requests, 'markdownify' for HTML conversion, and handles exceptions to provide error feedback. Dependencies: requests, markdownify, smolagents (for @tool decorator). Input: URL string. Output: Markdown string of the page content or an error message if fetching fails.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/multiagents.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport re\nimport requests\nfrom markdownify import markdownify\nfrom requests.exceptions import RequestException\nfrom smolagents import tool\n\n\n@tool\ndef visit_webpage(url: str) -> str:\n    \"\"\"Visits a webpage at the given URL and returns its content as a markdown string.\n\n    Args:\n        url: The URL of the webpage to visit.\n\n    Returns:\n        The content of the webpage converted to Markdown, or an error message if the request fails.\n    \"\"\"\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for bad status codes\n\n        # Convert the HTML content to Markdown\n        markdown_content = markdownify(response.text).strip()\n\n        # Remove multiple line breaks\n        markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n\n        return markdown_content\n\n    except RequestException as e:\n        return f\"Error fetching the webpage: {str(e)}\"\n    except Exception as e:\n        return f\"An unexpected error occurred: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Tool Using Class Inheritance\nDESCRIPTION: Example of creating a custom tool by subclassing the Tool class from smolagents with explicit attribute definitions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/guided_tour.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import Tool\n\nclass ModelDownloadTool(Tool):\n    name = \"model_download_tool\"\n    description = \"This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub. It returns the name of the checkpoint.\"\n    inputs = {\"task\": {\"type\": \"string\", \"description\": \"The task for which to get the download count.\"}}\n    output_type = \"string\"\n\n    def forward(self, task: str) -> str:\n        most_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n        return most_downloaded_model.id\n```\n\n----------------------------------------\n\nTITLE: Using LiteLLMModel Wrapper to Interface with Various LLM Providers - Python\nDESCRIPTION: This example shows the initialization and usage of LiteLLMModel, which supports over 100 LLM providers via LiteLLM. It accepts a model_id and generation parameters like temperature and max_tokens at initialization, which are then used for subsequent inference calls. The snippet passes a message formatted list with user role and text content, and prints the model's response. Requires installation of LiteLLM and proper access to the specified model.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/reference/models.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import LiteLLMModel\n\nmessages = [\n  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]}\n]\n\nmodel = LiteLLMModel(model_id=\"anthropic/claude-3-5-sonnet-latest\", temperature=0.2, max_tokens=10)\nprint(model(messages))\n```\n\n----------------------------------------\n\nTITLE: Setting up an in-memory SQLite database and creating the 'receipts' table in Python\nDESCRIPTION: Initializes an SQLite database in memory, defines a 'receipts' table with columns for receipt ID, customer name, price, and tip, and populates it with sample data. This setup provides a test environment for executing SQL queries programmatically in Python.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/text_to_sql.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sqlalchemy import (\n    create_engine,\n    MetaData,\n    Table,\n    Column,\n    String,\n    Integer,\n    Float,\n    insert,\n    inspect,\n    text,\n)\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData()\n\n# create city SQL table\ntable_name = \"receipts\"\nreceipts = Table(\n    table_name,\n    metadata_obj,\n    Column(\"receipt_id\", Integer, primary_key=True),\n    Column(\"customer_name\", String(16), primary_key=True),\n    Column(\"price\", Float),\n    Column(\"tip\", Float),\n)\nmetadata_obj.create_all(engine)\n\nrows = [\n    {\"receipt_id\": 1, \"customer_name\": \"Alan Payne\", \"price\": 12.06, \"tip\": 1.20},\n    {\"receipt_id\": 2, \"customer_name\": \"Alex Mason\", \"price\": 23.86, \"tip\": 0.24},\n    {\"receipt_id\": 3, \"customer_name\": \"Woodrow Wilson\", \"price\": 53.43, \"tip\": 5.43},\n    {\"receipt_id\": 4, \"customer_name\": \"Margaret James\", \"price\": 21.11, \"tip\": 1.00},\n]\nfor row in rows:\n    stmt = insert(receipts).values(**row)\n    with engine.begin() as connection:\n        cursor = connection.execute(stmt)\n```\n\n----------------------------------------\n\nTITLE: Defining Browser Interaction Tools - Python\nDESCRIPTION: This code defines several tools for the agent to interact with the web browser. These include searching for text using Ctrl+F, going back to the previous page, and closing pop-ups. Each tool is decorated with @tool to enable its use by the agent. `search_item_ctrl_f` finds text and scrolls to it, `go_back` navigates back, and `close_popups` closes modal dialogs.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/web_browser.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@tool\ndef search_item_ctrl_f(text: str, nth_result: int = 1) -> str:\n    \"\"\"\n    Searches for text on the current page via Ctrl + F and jumps to the nth occurrence.\n    Args:\n        text: The text to search for\n        nth_result: Which occurrence to jump to (default: 1)\n    \"\"\"\n    elements = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{text}')]\")\n    if nth_result > len(elements):\n        raise Exception(f\"Match n°{nth_result} not found (only {len(elements)} matches found)\")\n    result = f\"Found {len(elements)} matches for '{text}'.\"\n    elem = elements[nth_result - 1]\n    driver.execute_script(\"arguments[0].scrollIntoView(true);\", elem)\n    result += f\"Focused on element {nth_result} of {len(elements)}\"\n    return result\n\n@tool\ndef go_back() -> None:\n    \"\"\"Goes back to previous page.\"\"\"\n    driver.back()\n\n@tool\ndef close_popups() -> str:\n    \"\"\"\n    Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows!\n    This does not work on cookie consent banners.\n    \"\"\"\n    webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n```\n\n----------------------------------------\n\nTITLE: Running Agent with Task - Python\nDESCRIPTION: This snippet defines a search request and runs the agent to find information on Wikipedia, demonstrating the agent's ability to navigate, interact with web elements, and retrieve specific content. The agent output is then printed.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/web_browser.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsearch_request = \"\"\"\nPlease navigate to https://en.wikipedia.org/wiki/Chicago and give me a sentence containing the word \\\"1992\\\" that mentions a construction accident.\n\"\"\"\n\nagent_output = agent.run(search_request + helium_instructions)\nprint(\"Final output:\")\nprint(agent_output)\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with Authorized Imports (Python)\nDESCRIPTION: This snippet demonstrates initializing a `CodeAgent` instance. It shows how to pass an `InferenceClientModel` as the language model and, crucially, how to specify a list of `additional_authorized_imports` that the agent's embedded Python interpreter is allowed to use during code execution. The agent is then instructed to perform a task using the `run` method.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nmodel = InferenceClientModel()\nagent = CodeAgent(tools=[], model=model, additional_authorized_imports=['requests', 'bs4'])\nagent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n```\n\n----------------------------------------\n\nTITLE: Creating Web Search Agent\nDESCRIPTION: Creates a `ToolCallingAgent` named `web_search_agent` with `DuckDuckGoSearchTool` and `visit_webpage` as tools. Sets the `max_steps` to 10 for allowing the agent to explore many pages before finding the correct answer.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import (\n    CodeAgent,\n    ToolCallingAgent,\n    InferenceClientModel,\n    DuckDuckGoSearchTool,\n    LiteLLMModel,\n)\n\nmodel = InferenceClientModel(model_id=model_id)\n\nweb_agent = ToolCallingAgent(\n    tools=[DuckDuckGoSearchTool(), visit_webpage],\n    model=model,\n    max_steps=10,\n    name=\"web_search_agent\",\n    description=\"Runs web searches for you.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Smolagents with LiteLLM Support (Bash)\nDESCRIPTION: Installs the `smolagents` library along with its optional `litellm` dependencies using pip in quiet mode (`-q`). This setup is necessary to utilize LiteLLM compatible models within the smolagents framework later in the script.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install \"smolagents[litellm]\" -q\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with LiteLLMModel (Amazon Bedrock) (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a CodeAgent with LiteLLMModel using Amazon Bedrock models. It illustrates using LiteLLMModel to interface with Bedrock models.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import LiteLLMModel, CodeAgent\n\nmodel = LiteLLMModel(model_name=\"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\")\nagent = CodeAgent(tools=[], model=model)\n\nagent.run(\"Explain the concept of quantum computing\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Second SQLite Table for Joins Using SQLAlchemy (Python)\nDESCRIPTION: This snippet extends the SQL schema with a new 'waiters' table, mapping receipt IDs to waiter names, and populates it using insert_rows_into_table. The primary key is composite on receipt_id and waiter_name, enabling many-to-one relationships for further query complexity. Requires SQLAlchemy, access to the previously defined insert_rows_into_table, and an initialized engine and metadata context.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntable_name = \"waiters\"\nwaiters = Table(\n    table_name,\n    metadata_obj,\n    Column(\"receipt_id\", Integer, primary_key=True),\n    Column(\"waiter_name\", String(16), primary_key=True),\n)\nmetadata_obj.create_all(engine)\n\nrows = [\n    {\"receipt_id\": 1, \"waiter_name\": \"Corey Johnson\"},\n    {\"receipt_id\": 2, \"waiter_name\": \"Michael Watts\"},\n    {\"receipt_id\": 3, \"waiter_name\": \"Michael Watts\"},\n    {\"receipt_id\": 4, \"waiter_name\": \"Margaret James\"},\n]\ninsert_rows_into_table(rows, waiters)\n```\n\n----------------------------------------\n\nTITLE: Initializing LiteLLMModel with Model Parameters for Multi-Provider LLM Support in Python\nDESCRIPTION: This snippet demonstrates the usage of LiteLLMModel from smolagents to access a variety of LLM providers. It shows passing initialization keyword arguments such as model_id, temperature, and max_tokens to customize generation behavior. The model is then called with a conversation message list and its output printed. This snippet requires smolagents and LiteLLM integration, and showcases flexible multi-provider model usage with runtime parameter configuration.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/reference/agents.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import LiteLLMModel\n\nmessages = [\n  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"No need to help, take it easy.\"},\n]\n\nmodel = LiteLLMModel(model_id=\"anthropic/claude-3-5-sonnet-latest\", temperature=0.2, max_tokens=10)\nprint(model(messages))\n```\n\n----------------------------------------\n\nTITLE: Providing Instructions to Agent - Python\nDESCRIPTION: This section provides the agent with specific instructions on how to interact with websites using Helium. It describes how to go to a page, click on elements, scroll, and close pop-ups. The instructions include code snippets to give the agent clear examples of usage.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/web_browser.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nhelium_instructions = \"\"\"\nYou can use helium to access websites. Don't bother about the helium driver, it's already managed.\nWe've already ran \\\"from helium import *\\\"\nThen you can go to pages!\nCode:\n```py\ngo_to('github.com/trending')\n```<end_code>\n\nYou can directly click clickable elements by inputting the text that appears on them.\nCode:\n```py\nclick(\\\"Top products\\\")\n```<end_code>\n\nIf it's a link:\nCode:\n```py\nclick(Link(\\\"Top products\\\"))\n```<end_code>\n\nIf you try to interact with an element and it's not found, you'll get a LookupError.\nIn general stop your action after each button click to see what happens on your screenshot.\nNever try to login in a page.\n\nTo scroll up or down, use scroll_down or scroll_up with as an argument the number of pixels to scroll from.\nCode:\n```py\nscroll_down(num_pixels=1200) # This will scroll one viewport down\n```<end_code>\n\nWhen you have pop-ups with a cross icon to close, don't try to click the close icon by finding its element or targeting an 'X' element (this most often fails).\nJust use your built-in tool `close_popups` to close them:\nCode:\n```py\nclose_popups()\n```<end_code>\n\nYou can use .exists() to check for the existence of an element. For example:\nCode:\n```py\nif Text('Accept cookies?').exists():\n    click('I accept')\n```<end_code>\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face\nDESCRIPTION: Logs into the Hugging Face Hub using the `huggingface_hub` library. This is required to access Inference Providers for running models.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import login\n\nlogin()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Loading Environment - Python\nDESCRIPTION: This code imports necessary libraries and loads environment variables. Key imports include modules from io, time, helium, PIL, selenium, and smolagents to enable browser interactions, image processing, and agent functionalities. dotenv is used for loading environment variables.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/web_browser.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\nfrom time import sleep\n\nimport helium\nfrom dotenv import load_dotenv\nfrom PIL import Image\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\n\nfrom smolagents import CodeAgent, tool\nfrom smolagents.agents import ActionStep\n\n# Load environment variables\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Using Tool Collection with smolagents Agent (Python)\nDESCRIPTION: Demonstrates initializing a `smolagents.CodeAgent` by providing the tools from a loaded `ToolCollection` instance. It shows how to unpack the tools from the collection's `tools` attribute into a list passed to the agent constructor, enabling the agent to utilize the collection's capabilities.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nagent = CodeAgent(tools=[*image_tool_collection.tools], model=model, add_base_tools=True)\n\nagent.run(\"Please draw me a picture of rivers and lakes.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring AmazonBedrockServerModel using Environment Variables (Python)\nDESCRIPTION: Creates an AmazonBedrockServerModel instance configured via the AMAZON_BEDROCK_MODEL_ID environment variable for deployment on Amazon Bedrock. Requires smolagents and correct AWS setup. The model object can then be passed to agent classes for inference via AWS infrastructure.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom smolagents import AmazonBedrockServerModel\n\nmodel = AmazonBedrockServerModel(\n    model_id = os.environ.get(\"AMAZON_BEDROCK_MODEL_ID\") \n)\n\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Knowledge Base (Python)\nDESCRIPTION: This Python code loads a dataset from the Hugging Face Hub (`m-ric/huggingface_doc`), filters it to include only the documentation for the `transformers` library, converts the relevant entries into LangChain `Document` objects, and then splits these documents into smaller, manageable chunks using `RecursiveCharacterTextSplitter`. These chunks are stored in the `docs_processed` variable and are ready to be indexed by a retriever.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/rag.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.retrievers import BM25Retriever\n\nknowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\nknowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n\nsource_docs = [\n    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n    for doc in knowledge_base\n]\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=50,\n    add_start_index=True,\n    strip_whitespace=True,\n    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n)\ndocs_processed = text_splitter.split_documents(source_docs)\n```\n\n----------------------------------------\n\nTITLE: Configuring Chrome and Setting Screenshot - Python\nDESCRIPTION: This code configures Chrome options for headless mode, window size, and disables the PDF viewer. It also sets up a callback function `save_screenshot` to take and save screenshots after each action, enhancing the agent's debugging and observation capabilities. The callback is used to capture screenshots of the browser at different steps and stores them within the agent's memory.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/web_browser.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Configure Chrome options\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument(\"--force-device-scale-factor=1\")\nchrome_options.add_argument(\"--window-size=1000,1350\")\nchrome_options.add_argument(\"--disable-pdf-viewer\")\nchrome_options.add_argument(\"--window-position=0,0\")\n\n# Initialize the browser\ndriver = helium.start_chrome(headless=False, options=chrome_options)\n\n# Set up screenshot callback\ndef save_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:\n    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot\n    driver = helium.get_driver()\n    current_step = memory_step.step_number\n    if driver is not None:\n        for previous_memory_step in agent.memory.steps:  # Remove previous screenshots for lean processing\n            if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= current_step - 2:\n                previous_memory_step.observations_images = None\n        png_bytes = driver.get_screenshot_as_png()\n        image = Image.open(BytesIO(png_bytes))\n        print(f\"Captured a browser screenshot: {image.size} pixels\")\n        memory_step.observations_images = [image.copy()]  # Create a copy to ensure it persists\n\n    # Update observations with current URL\n    url_info = f\"Current url: {driver.current_url}\"\n    memory_step.observations = (\n        url_info if memory_step.observations is None else memory_step.observations + \"\\n\" + url_info\n    )\n```\n\n----------------------------------------\n\nTITLE: Importing LangChain Tool in smolagents (Python)\nDESCRIPTION: Shows how to integrate tools from the LangChain ecosystem into `smolagents` using the `Tool.from_langchain` method. It demonstrates wrapping a LangChain web search tool (`serpapi`) into a `smolagents` compatible tool format for use within `CodeAgent`. Requires `langchain` and `google-search-results`.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.agents import load_tools\n\nsearch_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running a smolagents CodeAgent with LLM Inference (Python)\nDESCRIPTION: This snippet instantiates a CodeAgent object using smolagents, providing it with the sql_engine tool and an InferenceClientModel powered by the meta-llama model. The agent can be run with a natural language query, which gets converted to code/actions with dynamic reasoning. Requires smolagents and proper setup of tools and environment (including loaded model access tokens).\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nagent = CodeAgent(\n    tools=[sql_engine],\n    model=InferenceClientModel(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n)\nagent.run(\"Can you give me the name of the client who got the most expensive receipt?\")\n```\n\n----------------------------------------\n\nTITLE: Creating Additional SQL Table and Inserting Data for Joins in Python\nDESCRIPTION: Defines a second table 'waiters' to record waiter names associated with receipt IDs, inserts data into it, enabling join queries across 'receipts' and 'waiters' tables for more complex queries. Requires SQLAlchemy and the existing engine and metadata. This snippet supports advanced multi-table queries by the agent.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/text_to_sql.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntable_name = \"waiters\"\nreceipts = Table(\n    table_name,\n    metadata_obj,\n    Column(\"receipt_id\", Integer, primary_key=True),\n    Column(\"waiter_name\", String(16), primary_key=True),\n)\nmetadata_obj.create_all(engine)\n\nrows = [\n    {\"receipt_id\": 1, \"waiter_name\": \"Corey Johnson\"},\n    {\"receipt_id\": 2, \"waiter_name\": \"Michael Watts\"},\n    {\"receipt_id\": 3, \"waiter_name\": \"Michael Watts\"},\n    {\"receipt_id\": 4, \"waiter_name\": \"Margaret James\"},\n]\nfor row in rows:\n    stmt = insert(receipts).values(**row)\n    with engine.begin() as connection:\n        cursor = connection.execute(stmt)\n```\n\n----------------------------------------\n\nTITLE: Defining the model ID\nDESCRIPTION: Defines the model ID for the Qwen2.5-Coder-32B-Instruct model hosted on Hugging Face Hub. This ID is used to initialize the InferenceClientModel.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_id = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n```\n\n----------------------------------------\n\nTITLE: Connecting to Azure OpenAI Deployments using AzureOpenAIServerModel - Python\nDESCRIPTION: This snippet outlines the setup for AzureOpenAIServerModel allowing connections to Azure OpenAI service deployments. It includes passing model_id, azure_endpoint, api_key, and api_version parameters during initialization. These can be omitted if environment variables AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, and OPENAI_API_VERSION are defined respectively. Note the openai-python package uses OPENAI_API_VERSION without an AZURE prefix. This flexibility supports seamless Azure OpenAI integration in Smolagents.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/reference/models.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom smolagents import AzureOpenAIServerModel\n\nmodel = AzureOpenAIServerModel(\n    model_id = os.environ.get(\"AZURE_OPENAI_MODEL\"),\n    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n    api_version=os.environ.get(\"OPENAI_API_VERSION\")    \n)\n```\n\n----------------------------------------\n\nTITLE: Initializing LiteLLMModel Agent Using OpenAI/Anthropic API in Python\nDESCRIPTION: This snippet creates a CodeAgent backed by LiteLLMModel, interfacing with OpenAI or Anthropic APIs. It requires installing 'smolagents[litellm]' and setting API keys via environment variables ('ANTHROPIC_API_KEY' or 'OPENAI_API_KEY') or passing 'api_key' during model initialization. The model_id specifies the particular LLM (e.g., 'anthropic/claude-3-5-sonnet-latest'). The agent supports empty tool lists but adds base tools by default, and runs user prompts via the run() method to produce generated outputs.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[litellm]\nfrom smolagents import CodeAgent, LiteLLMModel\n\nmodel = LiteLLMModel(model_id=\"anthropic/claude-3-5-sonnet-latest\", api_key=\"YOUR_ANTHROPIC_API_KEY\") # 也可以使用 'gpt-4o'\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Querying for Most Downloaded Model in Python\nDESCRIPTION: Basic code to retrieve the most downloaded model for a specific task from the Hugging Face Hub using the huggingface_hub library.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import list_models\n\ntask = \"text-classification\"\n\nmost_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\nprint(most_downloaded_model.id)\n```\n\n----------------------------------------\n\nTITLE: Creating a RetrieverTool with Smolagents\nDESCRIPTION: This Python code defines a `RetrieverTool` class using Smolagents' `Tool` base class. It encapsulates the retrieval logic using BM25Retriever from LangChain. The tool takes a query as input, performs a semantic search on the provided documents, and returns the retrieved documents as a string.  The tool is initialized with a list of documents (`docs_processed`) to construct the BM25 index.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/rag.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import Tool\n\nclass RetrieverTool(Tool):\n    name = \"retriever\"\n    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n    inputs = {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, docs, **kwargs):\n        super().__init__(**kwargs)\n        self.retriever = BM25Retriever.from_documents(\n            docs, k=10\n        )\n\n    def forward(self, query: str) -> str:\n        assert isinstance(query, str), \"Your search query must be a string\"\n\n        docs = self.retriever.invoke(\n            query,\n        )\n        return \"\\nRetrieved documents:\\n\" + \"\".join(\n            [\n                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n                for i, doc in enumerate(docs)\n            ]\n        )\n\nretriever_tool = RetrieverTool(docs_processed)\n```\n\n----------------------------------------\n\nTITLE: Initializing MLXModel Agent for Local LLM Inference in Python\nDESCRIPTION: This snippet illustrates creating a CodeAgent with an MLXModel backend for local inference using an mlx-lm pipeline model. It requires 'smolagents[mlx-lm]' installed and the model specified by name (e.g., 'mlx-community/Qwen2.5-Coder-32B-Instruct-4bit'). The agent uses an empty tools list and includes base tools. The agent.run() method is invoked to process input prompts and produce outputs using the local pipeline.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[mlx-lm]\nfrom smolagents import CodeAgent, MLXModel\n\nmlx_model = MLXModel(\"mlx-community/Qwen2.5-Coder-32B-Instruct-4bit\")\nagent = CodeAgent(model=mlx_model, tools=[], add_base_tools=True)\n\nagent.run(\"Could you give me the 118th number in the Fibonacci sequence?\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Chrome Driver and Screenshot Callback in Python\nDESCRIPTION: This code configures and initializes the Chrome web driver using `helium.start_chrome`. It sets specific Chrome options like device scale factor, window size/position, and disabling the PDF viewer. A `save_screenshot` callback function is defined to capture the browser window state after each agent action step, storing it in the agent's memory (`memory_step.observations_images`) and adding the current URL to the observations. It includes a delay (`sleep`) for animations and cleans up older screenshots.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Configure Chrome options\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument(\"--force-device-scale-factor=1\")\nchrome_options.add_argument(\"--window-size=1000,1350\")\nchrome_options.add_argument(\"--disable-pdf-viewer\")\nchrome_options.add_argument(\"--window-position=0,0\")\n\n# Initialize the browser\ndriver = helium.start_chrome(headless=False, options=chrome_options)\n\n# Set up screenshot callback\ndef save_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:\n    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot\n    driver = helium.get_driver()\n    current_step = memory_step.step_number\n    if driver is not None:\n        for previous_memory_step in agent.memory.steps:  # Remove previous screenshots for lean processing\n            if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= current_step - 2:\n                previous_memory_step.observations_images = None\n        png_bytes = driver.get_screenshot_as_png()\n        image = Image.open(BytesIO(png_bytes))\n        print(f\"Captured a browser screenshot: {image.size} pixels\")\n        memory_step.observations_images = [image.copy()]  # Create a copy to ensure it persists\n\n    # Update observations with current URL\n    url_info = f\"Current url: {driver.current_url}\"\n    memory_step.observations = (\n        url_info if memory_step.observations is None else memory_step.observations + \"\\n\" + url_info\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Tool with Smolagents\nDESCRIPTION: This snippet defines a custom tool called `HFModelDownloadsTool` that inherits from the `Tool` class in Smolagents.  It retrieves the most downloaded model for a given task from the Hugging Face Hub.  It includes the tool's name, description, input type, output type, and a forward method for execution.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/tools.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import Tool\n\nclass HFModelDownloadsTool(Tool):\n    name = \"model_download_counter\"\n    description = \"\"\"\n    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n    It returns the name of the checkpoint.\"\"\"\n    inputs = {\n        \"task\": {\n            \"type\": \"string\",\n            \"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n        }\n    }\n    output_type = \"string\"\n\n    def forward(self, task: str):\n        from huggingface_hub import list_models\n\n        model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n        return model.id\n\nmodel_downloads_tool = HFModelDownloadsTool()\n```\n\n----------------------------------------\n\nTITLE: Creating an Interactive Gradio UI for Agent Interaction in Python\nDESCRIPTION: Setup for a Gradio-based interactive UI that allows users to submit tasks to an agent and visualize its thought process. Demonstrates how to load tools from Hub and create a web interface for agent interaction.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import (\n    load_tool,\n    CodeAgent,\n    InferenceClientModel,\n    GradioUI\n)\n\n# Import tool from Hub\nimage_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True)\n\nmodel = InferenceClientModel(model_id=model_id)\n\n# Initialize the agent with the image generation tool\nagent = CodeAgent(tools=[image_generation_tool], model=model)\n\nGradioUI(agent).launch()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Web Automation Agent in Python\nDESCRIPTION: This snippet imports required modules for the agent. It includes standard libraries (`io`, `time`), browser automation tools (`helium`, `selenium`), environment variable handling (`dotenv`), image processing (`PIL`), and components from the `smolagents` library (`CodeAgent`, `tool`, `ActionStep`). `load_dotenv()` is called to load environment variables, typically API keys, from a `.env` file.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\nfrom time import sleep\n\nimport helium\nfrom dotenv import load_dotenv\nfrom PIL import Image\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\n\nfrom smolagents import CodeAgent, tool\nfrom smolagents.agents import ActionStep\n\n# Load environment variables\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Viewing the Current System Prompt in CodeAgent\nDESCRIPTION: This snippet demonstrates how to access and display the default system prompt template used by the CodeAgent. This is useful when you want to understand the agent's instructions before customizing them.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprint(agent.prompt_templates[\"system_prompt\"])\n```\n\n----------------------------------------\n\nTITLE: Loading Tool Collection from Hub (Python)\nDESCRIPTION: Illustrates loading a group of tools packaged together as a `ToolCollection` from the Hugging Face Hub. It uses `ToolCollection.from_hub`, requiring the collection's slug and potentially an API token, providing a convenient way to access multiple related tools at once.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import ToolCollection, CodeAgent\n\nimage_tool_collection = ToolCollection.from_hub(\n    collection_slug=\"huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f\",\n    token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\"\n)\n```\n\n----------------------------------------\n\nTITLE: Using LiteLLMRouterModel with Smolagents in Python\nDESCRIPTION: The `LiteLLMRouterModel` provides a wrapper around the LiteLLM Router for advanced routing strategies.  It uses `model_id` and `model_list` for initialization, configuring multiple deployments. It allows the specification of routing strategies like 'simple-shuffle' via the `client_kwargs` parameter. This model allows for load-balancing, prioritization, and reliability measures supported by the LiteLLM Router.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import LiteLLMRouterModel\nimport os\n\nmessages = [\n  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]}\n]\n\nmodel = LiteLLMRouterModel(\n    model_id=\"llama-3.3-70b\",\n    model_list=[\n        {\n            \"model_name\": \"llama-3.3-70b\",\n            \"litellm_params\": {\"model\": \"groq/llama-3.3-70b\", \"api_key\": os.getenv(\"GROQ_API_KEY\")},\n        },\n        {\n            \"model_name\": \"llama-3.3-70b\",\n            \"litellm_params\": {\"model\": \"cerebras/llama-3.3-70b\", \"api_key\": os.getenv(\"CEREBRAS_API_KEY\")},\n        },\n    ],\n    client_kwargs={\n        \"routing_strategy\": \"simple-shuffle\",\n    },\n)\nprint(model(messages))\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Smolagents via pip (Bash)\nDESCRIPTION: Demonstrates the installation of the smolagents library using pip, required for all subsequent Python-based usage. This single-command snippet is the prerequisite before importing smolagents or any of its models and agent classes.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip install smolagents\n\n```\n\n----------------------------------------\n\nTITLE: Using LocalPythonExecutor for Secure Code Execution in Python\nDESCRIPTION: This code demonstrates how to use SmolAgents' LocalPythonExecutor to run Python code with security safeguards that prevent unauthorized imports and potentially harmful operations.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/secure_code_execution.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents.local_python_executor import LocalPythonExecutor\n\n# Set up custom executor, authorize package \"numpy\"\ncustom_executor = LocalPythonExecutor([\"numpy\"])\n\n# Utilisty for pretty printing errors\ndef run_capture_exception(command: str):\n    try:\n        custom_executor(harmful_command)\n    except Exception as e:\n        print(\"ERROR:\\n\", e)\n\n# Undefined command just do not work\nharmful_command=\"!echo Bad command\"\nrun_capture_exception(harmful_command)\n# >>> ERROR: invalid syntax (<unknown>, line 1)\n\n\n# Imports like os will not be performed unless explicitly added to `additional_authorized_imports`\nharmful_command=\"import os; exit_code = os.system(\"echo Bad command\")\"\nrun_capture_exception(harmful_command)\n# >>> ERROR: Code execution failed at line 'import os' due to: InterpreterError: Import of os is not allowed. Authorized imports are: ['statistics', 'numpy', 'itertools', 'time', 'queue', 'collections', 'math', 'random', 're', 'datetime', 'stat', 'unicodedata']\n\n# Even in authorized imports, potentially harmful packages will not be imported\nharmful_command=\"import random; random._os.system('echo Bad command')\"\nrun_capture_exception(harmful_command)\n# >>> ERROR: Code execution failed at line 'random._os.system('echo Bad command')' due to: InterpreterError: Forbidden access to module: os\n\n# Infinite loop are interrupted after N operations\nharmful_command=\"\"\"\nwhile True:\n    pass\n\"\"\"\nrun_capture_exception(harmful_command)\n# >>> ERROR: Code execution failed at line 'while True: pass' due to: InterpreterError: Maximum number of 1000000 iterations in While loop exceeded\n```\n\n----------------------------------------\n\nTITLE: Adding the 'waiters' table and inserting data in Python\nDESCRIPTION: Creates a new table 'waiters' in the database to record waiter names associated with receipt IDs, then populates the table with sample data, enabling cross-table queries for more complex questions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/text_to_sql.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntable_name = \"waiters\"\nreceipts = Table(\n    table_name,\n    metadata_obj,\n    Column(\"receipt_id\", Integer, primary_key=True),\n    Column(\"waiter_name\", String(16), primary_key=True),\n)\nmetadata_obj.create_all(engine)\n\nrows = [\n    {\"receipt_id\": 1, \"waiter_name\": \"Corey Johnson\"},\n    {\"receipt_id\": 2, \"waiter_name\": \"Michael Watts\"},\n    {\"receipt_id\": 3, \"waiter_name\": \"Michael Watts\"},\n    {\"receipt_id\": 4, \"waiter_name\": \"Margaret James\"},\n]\nfor row in rows:\n    stmt = insert(receipts).values(**row)\n    with engine.begin() as connection:\n        cursor = connection.execute(stmt)\n```\n\n----------------------------------------\n\nTITLE: Registering Step Callback on Agent Initialization with Smolagents in Python\nDESCRIPTION: This initialization snippet shows how to assign a step callback function, such as update_screenshot, to a CodeAgent. Additional authorized imports (e.g., helium) must be declared. The step_callbacks list accepts one or more functions that will be triggered on every agent step, allowing for fine-grained control over memory management and observations. Inputs are the callback function(s) and agent configuration; output is an agent instance ready for interactive or automated runs.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/memory.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCodeAgent(\n    tools=[DuckDuckGoSearchTool(), go_back, close_popups, search_item_ctrl_f],\n    model=model,\n    additional_authorized_imports=[\"helium\"],\n    step_callbacks=[update_screenshot],\n    max_steps=20,\n    verbosity_level=2,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating table description to include multiple tables in Python\nDESCRIPTION: Generates a comprehensive description of both 'receipts' and 'waiters' tables by inspecting each, and concatenates the schema information into an updated description string that guides the LLM in understanding the database schema for complex queries.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/text_to_sql.mdx#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nupdated_description = \"\"\"Allows you to perform SQL queries on the table. Beware that this tool's output is a string representation of the execution output.\nIt can use the following tables:\"\"\"\n\ninspector = inspect(engine)\nfor table in [\"receipts\", \"waiters\"]:\n    columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(table)]\n\n    table_description = f\"Table '{table}':\\n\"\n\n    table_description += \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n    updated_description += \"\\n\\n\" + table_description\n\nprint(updated_description)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with an Imported Space Tool in an Agent in Python\nDESCRIPTION: Shows how to initialize a `CodeAgent` with a Space-imported tool and generate an image based on an improved prompt. Utilizes `InferenceClientModel` for the model configuration.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/tools.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nmodel = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\nagent = CodeAgent(tools=[image_generation_tool], model=model)\n\nagent.run(\n    \"Improve this prompt, then generate an image of it.\", additional_args={'user_prompt': 'A rabbit wearing a space suit'}\n)\n```\n\n----------------------------------------\n\nTITLE: Using OpenAIServerModel with Smolagents in Python\nDESCRIPTION: This snippet demonstrates the use of `OpenAIServerModel` for connecting to an OpenAI-compatible server. It takes `model_id`, `api_base`, and `api_key` as parameters during initialization. The `api_base` URL is customizable to point to different server instances.  The `api_key` argument is used for authentication. It allows integration with any server compatible with the OpenAI API standard.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```py\nimport os\nfrom smolagents import OpenAIServerModel\n\nmodel = OpenAIServerModel(\n    model_id=\"gpt-4o\",\n    api_base=\"https://api.openai.com/v1\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n)\n```\n```\n\n----------------------------------------\n\nTITLE: Adapting Tool Description and Upgrading Model for Complex Reasoning (Python)\nDESCRIPTION: This snippet updates the existing sql_engine tool's description attribute to reflect the newly extended table schema, ensuring the agent's LLM has accurate context for multi-table queries. It instantiates a new CodeAgent using a more capable LLM (Qwen/Qwen2.5-Coder-32B-Instruct) to handle increased query complexity, then submits a prompt designed to invoke multi-table joins. Prerequisites: prior definition of sql_engine, setup of smolagents, InferenceClientModel, and all table/data structures.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsql_engine.description = updated_description\n\nagent = CodeAgent(\n    tools=[sql_engine],\n    model=InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n)\n\nagent.run(\"Which waiter got more total money from tips?\")\n```\n\n----------------------------------------\n\nTITLE: Switching to a more powerful LLM and running a cross-table query in Python\nDESCRIPTION: Replaces the existing model with a stronger LLM to improve reasoning over extended database schemas. Executes a complex query involving multiple tables, demonstrating the system's enhanced capability to handle sophisticated questions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/text_to_sql.mdx#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nsql_engine.description = updated_description\n\nagent = CodeAgent(\n    tools=[sql_engine],\n    model=InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n)\n\nagent.run(\"Which waiter got more total money from tips?\")\n```\n\n----------------------------------------\n\nTITLE: Creating Web Automation Agent - Python\nDESCRIPTION: This code initializes a CodeAgent using a specified language model, sets up available tools, defines authorized imports and assigns a step callback function. The model utilizes the \"meta-llama/Llama-3.3-70B-Instruct\" model. Additional parameters are configured to control step limits, and verbosity levels.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/web_browser.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\n# Initialize the model\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"  # You can change this to your preferred model\nmodel = InferenceClientModel(model_id=model_id)\n\n# Create the agent\nagent = CodeAgent(\n    tools=[go_back, close_popups, search_item_ctrl_f],\n    model=model,\n    additional_authorized_imports=[\"helium\"],\n    step_callbacks=[save_screenshot],\n    max_steps=20,\n    verbosity_level=2,\n)\n\n# Import helium for the agent\nagent.python_executor(\"from helium import *\", agent.state)\n```\n\n----------------------------------------\n\nTITLE: Reinitializing the CodeAgent with Updated SQL Tables and a More Powerful LLM in Python\nDESCRIPTION: Updates the 'sql_engine' tool's description to include schemas for multiple tables, then instantiates a new 'CodeAgent' with the updated tool and switches to a more powerful LLM model 'Qwen/Qwen2.5-Coder-32B-Instruct' for better reasoning with complex queries involving joins. Runs a prompt to find which waiter received the most total tips. Requires previous setup of tool, SQL tables, and smolagents framework.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/text_to_sql.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsql_engine.description = updated_description\n\nagent = CodeAgent(\n    tools=[sql_engine],\n    model=InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n)\n\nagent.run(\"Which waiter got more total money from tips?\")\n```\n\n----------------------------------------\n\nTITLE: Running Agent Query (Python)\nDESCRIPTION: This Python snippet executes the `run()` method of the initialized `CodeAgent`. It provides a specific query about the training process of transformers models. The agent will use its available tools (specifically the `retriever`) to find relevant information in the knowledge base to answer the query. The final output generated by the agent after completing its thought process and tool usage is captured in `agent_output` and then printed to the console.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/rag.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nagent_output = agent.run(\"For a transformers model training, which is slower, the forward or the backward pass?\")\n\nprint(\"Final output:\")\nprint(agent_output)\n```\n\n----------------------------------------\n\nTITLE: Using MLXModel with Smolagents in Python\nDESCRIPTION: This demonstrates the `MLXModel`, integrating with models compatible with MLX framework. The example uses `model_id` during initialization, specifying the model to use.  This model utilizes the MLX framework for inference, requiring `mlx-lm` to be installed. The example demonstrates initializing the model and calling it with a message list and stop sequences.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import MLXModel\n\nmodel = MLXModel(model_id=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n\nprint(model([{\"role\": \"user\", \"content\": \"Ok!\"}], stop_sequences=[\"great\"]))\n```\n```\n\n----------------------------------------\n\nTITLE: Using VLLMModel with Smolagents in Python\nDESCRIPTION: The `VLLMModel` class is used to integrate with the vLLM library for fast LLM inference. The example sets up a `VLLMModel` with a specified `model_id`. The provided code initializes the model and calls it with messages and stop sequences. The use of vLLM necessitates its installation on the machine to facilitate efficient LLM serving and inference.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import VLLMModel\n\nmodel = VLLMModel(model_id=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n\nprint(model([{\"role\": \"user\", \"content\": \"Ok!\"}], stop_sequences=[\"great\"]))\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with Tool (Python)\nDESCRIPTION: This Python code initializes a `CodeAgent` from the `smolagents` library. It is configured with the custom `retriever_tool` created previously, passed as a list to the `tools` argument. It uses `InferenceClientModel()` as the underlying Large Language Model engine to interact with the Hugging Face Inference API. `max_steps` and `verbosity_level` are set to control the agent's execution behavior.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/rag.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent\n\nagent = CodeAgent(\n    tools=[retriever_tool], model=InferenceClientModel(), max_steps=4, verbosity_level=2\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Utility Functions for Answer Processing and Scoring\nDESCRIPTION: Implements a collection of utility functions for normalizing strings and numbers, extracting numerical values, comparing answers, and scoring model responses against ground truth answers for different benchmark tasks.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/score.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport re\nimport string\nimport warnings\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom datetime import datetime\nfrom typing import List\n\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef normalize_number_str(number_str: str) -> float:\n    # we replace these common units and commas to allow\n    # conversion to float\n    for char in [\"$\", \"%\", \",\"]:\n        number_str = number_str.replace(char, \"\")\n    try:\n        return float(number_str)\n    except ValueError:\n        return float(\"inf\")\n\n\ndef split_string(\n    s: str,\n    char_list: list[str] = [\",\", \";\"],\n) -> list[str]:\n    pattern = f\"[{''.join(char_list)}]\"\n    return re.split(pattern, s)\n\n\ndef is_float(element: any) -> bool:\n    try:\n        float(element)\n        return True\n    except ValueError:\n        return False\n\n\ndef normalize_str(input_str, remove_punct=True) -> str:\n    \"\"\"\n    Normalize a string by:\n    - Removing all white spaces\n    - Optionally removing punctuation (if remove_punct is True)\n    - Converting to lowercase\n    Parameters:\n    - input_str: str, the string to normalize\n    - remove_punct: bool, whether to remove punctuation (default: True)\n    Returns:\n    - str, the normalized string\n    \"\"\"\n    # Remove all white spaces. Required e.g for seagull vs. sea gull\n    no_spaces = re.sub(r\"\\s\", \"\", input_str)\n\n    # Remove punctuation, if specified.\n    if remove_punct:\n        translator = str.maketrans(\"\", \"\", string.punctuation)\n        return no_spaces.lower().translate(translator)\n    else:\n        return no_spaces.lower()\n\n\ndef extract_numbers(text: str) -> List[str]:\n    \"\"\"This pattern matches:\n    - Optional negative sign\n    - Numbers with optional comma thousand separators\n    - Optional decimal points with decimal numbers\n    \"\"\"\n    pattern = r\"-?(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:\\.\\d+)?\"\n\n    return [el.replace(\",\", \"\") for el in re.findall(pattern, text)]\n\n\ndef get_question_score_gaia(\n    model_answer: str,\n    ground_truth: str,\n) -> bool:\n    \"\"\"Scoring function used to score functions from the GAIA benchmark\"\"\"\n    if is_float(ground_truth):\n        normalized_answer = normalize_number_str(str(model_answer))\n        return normalized_answer == float(ground_truth)\n\n    elif any(char in ground_truth for char in [\",\", \";\"]): # if gt is a list\n        # question with the fish: normalization removes punct\n        gt_elems = split_string(ground_truth)\n        ma_elems = split_string(model_answer)\n\n        if len(gt_elems) != len(ma_elems): # check length is the same\n            warnings.warn(\"Answer lists have different lengths, returning False.\", UserWarning)\n            return False\n\n        comparisons = []\n        for ma_elem, gt_elem in zip(ma_elems, gt_elems): # compare each element as float or str\n            if is_float(gt_elem):\n                normalized_ma_elem = normalize_number_str(ma_elem)\n                comparisons.append(normalized_ma_elem == float(gt_elem))\n            else:\n                # we do not remove punct since comparisons can include punct\n                comparisons.append(\n                    normalize_str(ma_elem, remove_punct=False) == normalize_str(gt_elem, remove_punct=False)\n                )\n        return all(comparisons)\n\n    else: # if gt is a str\n        return normalize_str(model_answer) == normalize_str(ground_truth)\n\n\ndef get_correct(row):\n    if row[\"source\"] == \"MATH\": # Checks the last number in answer\n        numbers_answer = extract_numbers(str(row[\"answer\"]))\n        if len(numbers_answer) == 0:\n            return False\n        return np.isclose(float(numbers_answer[-1]), float(row[\"true_answer\"]), rtol=1e-5, atol=1e-7)\n    else:\n        return get_question_score_gaia(str(row[\"answer\"]), str(row[\"true_answer\"]))\n\n\ndef score_answers_subset(answers_dataset, answers_subset):\n    try:\n        print(answers_dataset, answers_subset)\n        *model_id, action_type, task = answers_subset.split(\"__\")\n        model_id = \"/\".join(model_id)\n        ds = datasets.load_dataset(answers_dataset, answers_subset, split=\"test\")\n        df = ds.to_pandas()\n        df[\"correct\"] = df.apply(get_correct, axis=1)\n        assert df[\"correct\"].notnull().sum() > 30, \"Missing answers\"\n        acc = df[\"correct\"].mean().item()\n        result = df.loc[0, [\"model_id\", \"agent_action_type\", \"source\"]].to_dict()\n        result[\"acc\"] = acc\n        return result\n    except Exception as e:\n        print(f\"Error with {answers_subset}: {e}\")\n        return None\n\n\ndef score_answers(\n    answers_subsets,\n    answers_dataset=ANSWERS_DATASET,\n    date=DATE,\n    push_to_hub_dataset=RESULTS_DATASET if PUSH_RESULTS_DATASET_TO_HUB else None,\n    set_default=True,\n):\n    if not answers_dataset:\n        raise ValueError(\"Pass 'answers_dataset' to load the answers from it\")\n    date = date or datetime.date.today().isoformat()\n    results = []\n    with ThreadPoolExecutor(max_workers=16) as exe:\n        futures = [\n            exe.submit(score_answers_subset, answers_dataset, answers_subset) for answers_subset in answers_subsets\n        ]\n        for f in tqdm(as_completed(futures), total=len(answers_subsets), desc=\"Processing tasks\"):\n            result = f.result()\n            if result:\n                results.append(result)\n    df = pd.DataFrame(results)\n\n    if push_to_hub_dataset:\n        ds = datasets.Dataset.from_pandas(df)\n        config = date\n        set_default = set_default\n        ds.push_to_hub(\n            push_to_hub_dataset, config_name=config, set_default=set_default, commit_message=f\"Upload {config} results\"\n        )\n    return df\n```\n\n----------------------------------------\n\nTITLE: Using LiteLLMModel with Smolagents in Python\nDESCRIPTION: This snippet uses the `LiteLLMModel` to integrate with various LLMs supported by LiteLLM. The `model_id` parameter is required for model initialization and specifies the model to use.  The example passes `temperature` as a kwarg, demonstrating how to customize the model's behavior. The model leverages LiteLLM for communication with LLM providers, offering support for a wide range of models.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import LiteLLMModel\n\nmessages = [\n  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]}\n]\n\nmodel = LiteLLMModel(model_id=\"anthropic/claude-3-5-sonnet-latest\", temperature=0.2, max_tokens=10)\nprint(model(messages))\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Browser Interaction Tools for SmolAgent in Python\nDESCRIPTION: This snippet defines three functions decorated with `@tool` to be used by the SmolAgent. `search_item_ctrl_f` uses Selenium's `find_elements` with XPath to simulate a Ctrl+F search, finding text occurrences and scrolling to the nth result. `go_back` uses the Selenium driver's `back()` method for navigation. `close_popups` simulates pressing the Escape key to dismiss modals or popups, but notes it doesn't handle cookie banners. These tools encapsulate specific browser actions for the agent.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@tool\ndef search_item_ctrl_f(text: str, nth_result: int = 1) -> str:\n    \"\"\"\n    Searches for text on the current page via Ctrl + F and jumps to the nth occurrence.\n    Args:\n        text: The text to search for\n        nth_result: Which occurrence to jump to (default: 1)\n    \"\"\"\n    elements = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{text}')]\")\n    if nth_result > len(elements):\n        raise Exception(f\"Match n°{nth_result} not found (only {len(elements)} matches found)\")\n    result = f\"Found {len(elements)} matches for '{text}'.\"\n    elem = elements[nth_result - 1]\n    driver.execute_script(\"arguments[0].scrollIntoView(true);\", elem)\n    result += f\"Focused on element {nth_result} of {len(elements)}\"\n    return result\n\n@tool\ndef go_back() -> None:\n    \"\"\"Goes back to previous page.\"\"\"\n    driver.back()\n\n@tool\ndef close_popups() -> str:\n    \"\"\"\n    Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows!\n    This does not work on cookie consent banners.\n    \"\"\"\n    webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n```\n\n----------------------------------------\n\nTITLE: Importing Text Browser Tools and Initializing LLM (Python)\nDESCRIPTION: Imports necessary classes and tools for building a text-based web browsing agent. This includes agent runners (`answer_questions`), various text-based browser tools (`TextInspectorTool`, `ArchiveSearchTool`, `VisitTool`, etc.), a visual QA tool (`VisualQAGPT4Tool`), and the core agent/model classes (`CodeAgent`, `LiteLLMModel`) from `smolagents`. It then initializes a `LiteLLMModel` instance configured to use the 'gpt-4o' model.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom scripts.run_agents import answer_questions\nfrom scripts.text_inspector_tool import TextInspectorTool\nfrom scripts.text_web_browser import (\n    ArchiveSearchTool,\n    FinderTool,\n    FindNextTool,\n    NavigationalSearchTool,\n    PageDownTool,\n    PageUpTool,\n    SearchInformationTool,\n    VisitTool,\n)\nfrom scripts.visual_qa import VisualQAGPT4Tool\n\nfrom smolagents import CodeAgent, LiteLLMModel\n\n\nproprietary_model = LiteLLMModel(model_id=\"gpt-4o\")\n```\n\n----------------------------------------\n\nTITLE: Example: Web Search Workflow for Document Comprehension in Python\nDESCRIPTION: This is a multi-step workflow for information extraction: first, it performs a web search using the 'search' tool with a long query, prints results; if initial search fails, it retries with a broader query and prints new results. The agent then visits pages, reads content, and prints it for further reasoning. This pattern illustrates persistent state and separating unpredictable outputs into multiple code blocks. Dependencies: search and visit_webpage tools; variable 'url' is set within the snippet; valid web access is required. Output includes list of web results and full page content.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npages = search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\")\nprint(pages)\n```\n\nLANGUAGE: python\nCODE:\n```\npages = search(query=\"1979 interview Stanislaus Ulam\")\nprint(pages)\n```\n\nLANGUAGE: python\nCODE:\n```\nfor url in [\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\", \"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\"]:\n    whole_page = visit_webpage(url)\n    print(whole_page)\n    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Print separator between pages\n```\n\n----------------------------------------\n\nTITLE: Loading the GAIA Benchmark Dataset (Python)\nDESCRIPTION: Imports the `datasets` library from Hugging Face and loads the 'validation' split of the 'gaia-benchmark/GAIA' dataset (2023_all version). The loaded dataset is assigned to the `eval_ds` variable for subsequent filtering and evaluation.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\n\n\neval_ds = datasets.load_dataset(\"gaia-benchmark/GAIA\", \"2023_all\")[\"validation\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Helium Usage Instructions for SmolAgent in Python\nDESCRIPTION: This multi-line string variable `helium_instructions` contains natural language instructions and code examples for the agent on how to use the `helium` library for web automation. It covers navigation (`go_to`), clicking elements (`click`, `click(Link(...))`), handling errors (`LookupError`), scrolling (`scroll_down`, `scroll_up`), using the custom `close_popups` tool, and checking element existence (`Text(...).exists()`). These instructions guide the agent's code generation. Note the use of ````py` and `<end_code>` markers within the string, likely for the agent's parser.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nhelium_instructions = \"\"\"\nYou can use helium to access websites. Don't bother about the helium driver, it's already managed.\nWe've already ran \"from helium import *\"\nThen you can go to pages!\nCode:\n```py\ngo_to('github.com/trending')\n```<end_code>\n\nYou can directly click clickable elements by inputting the text that appears on them.\nCode:\n```py\nclick(\"Top products\")\n```<end_code>\n\nIf it's a link:\nCode:\n```py\nclick(Link(\"Top products\"))\n```<end_code>\n\nIf you try to interact with an element and it's not found, you'll get a LookupError.\nIn general stop your action after each button click to see what happens on your screenshot.\nNever try to login in a page.\n\nTo scroll up or down, use scroll_down or scroll_up with as an argument the number of pixels to scroll from.\nCode:\n```py\nscroll_down(num_pixels=1200) # This will scroll one viewport down\n```<end_code>\n\nWhen you have pop-ups with a cross icon to close, don't try to click the close icon by finding its element or targeting an 'X' element (this most often fails).\nJust use your built-in tool `close_popups` to close them:\nCode:\n```py\nclose_popups()\n```<end_code>\n\nYou can use .exists() to check for the existence of an element. For example:\nCode:\n```py\nif Text('Accept cookies?').exists():\n    click('I accept')\n```<end_code>\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initializing AzureOpenAIServerModel Agent for Azure OpenAI in Python\nDESCRIPTION: This snippet demonstrates initializing a CodeAgent with an AzureOpenAIServerModel backend to connect with Azure OpenAI Service. The model requires specifying the deployment (model_id) and optionally using environment variables 'AZURE_OPENAI_ENDPOINT', 'AZURE_OPENAI_API_KEY', and 'OPENAI_API_VERSION' or passing them as constructor parameters. The agent runs user prompts with empty tools and the default base tools enabled, invoking the Azure OpenAI deployment to generate responses.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[openai]\nfrom smolagents import CodeAgent, AzureOpenAIServerModel\n\nmodel = AzureOpenAIServerModel(model_id=\"gpt-4o-mini\")\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with LiteLLMModel (Ollama) (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a CodeAgent with LiteLLMModel, connecting to an Ollama instance. It specifies the model ID, API base, API key (if necessary), and context window size.  The agent is then used to calculate the 118th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[litellm]\nfrom smolagents import CodeAgent, LiteLLMModel\n\nmodel = LiteLLMModel(\n    model_id=\"ollama_chat/llama3.2\", # This model is a bit weak for agentic behaviours though\n    api_base=\"http://localhost:11434\", # replace with 127.0.0.1:11434 or remote open-ai compatible server if necessary\n    api_key=\"YOUR_API_KEY\", # replace with API key if necessary\n    num_ctx=8192, # ollama default is 2048 which will fail horribly. 8192 works for easy tasks, more is better. Check https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator to calculate how much VRAM this will need for the selected model.\n)\n\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Managing Agent Toolboxes\nDESCRIPTION: This snippet shows how to manage an agent's toolbox by adding or modifying tools within the `agent.tools` dictionary. It demonstrates adding a `model_download_tool` to an existing agent initialized with a default toolbox. \nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/tools.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\nmodel = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\nagent.tools[model_download_tool.name] = model_download_tool\n```\n\nLANGUAGE: python\nCODE:\n```\nagent.run(\n    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub but reverse the letters?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running SmolAgent for Wikipedia Search Task in Python\nDESCRIPTION: This code defines a specific task (`search_request`) for the agent: navigate to a Wikipedia page about Chicago and extract a sentence about a construction accident in 1992. It then runs the agent using `agent.run()`, passing the task concatenated with the `helium_instructions`. The final output from the agent is printed.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsearch_request = \"\"\"\nPlease navigate to https://en.wikipedia.org/wiki/Chicago and give me a sentence containing the word \"1992\" that mentions a construction accident.\n\"\"\"\n\nagent_output = agent.run(search_request + helium_instructions)\nprint(\"Final output:\")\nprint(agent_output)\n```\n\n----------------------------------------\n\nTITLE: Using MCP Tools with MCPClient (stdio)\nDESCRIPTION: This snippet demonstrates using `MCPClient` to directly manage a connection to a stdio-based MCP server.  It defines `StdioServerParameters`, then creates an `MCPClient` instance.  It subsequently initializes and runs a `CodeAgent` using the tools. Requires `smolagents` and `mcp` packages.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import MCPClient, CodeAgent\nfrom mcp import StdioServerParameters\nimport os\n\nserver_parameters = StdioServerParameters(\n    command=\"uvx\",  # Using uvx ensures dependencies are available\n    args=[\"--quiet\", \"pubmedmcp@0.1.3\"],\n    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n)\n\nwith MCPClient(server_parameters) as tools:\n    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)\n    agent.run(\"Please find the latest research on COVID-19 treatment.\")\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Helium for Vision Browser (Bash)\nDESCRIPTION: Installs the `helium` Python library using pip in quiet mode (`-q`). Helium is a high-level library for web automation and is used here to enable the vision-based browser agent's interactions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n!pip install helium -q\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAIServerModel to Call OpenAI-Server Compatible Models - Python\nDESCRIPTION: This code shows how to instantiate OpenAIServerModel from smolagents, which allows calling any OpenAI Server compatible LLM endpoint. It requires setting parameters such as model_id, api_base (URL to OpenAI-compatible server), and api_key (API token). This setup enables usage of alternative OpenAI-compatible servers and custom endpoints. Requires appropriate environment variables or manual parameter setup.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/reference/models.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom smolagents import OpenAIServerModel\n\nmodel = OpenAIServerModel(\n    model_id=\"gpt-4o\",\n    api_base=\"https://api.openai.com/v1\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Inference Model for the Agent System in Python\nDESCRIPTION: Defines the model identifier for the language model to be used in the inference API. This value is stored as a string variable 'model_id' for subsequent usage when initializing the inference client. The selected model is 'Qwen/Qwen2.5-Coder-32B-Instruct', hosted on Hugging Face Hub.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/multiagents.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmodel_id = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n```\n\n----------------------------------------\n\nTITLE: stream_to_gradio Component\nDESCRIPTION: The `stream_to_gradio` component handles real-time streaming of outputs to Gradio interfaces, enabling dynamic UI updates. It depends on Gradio being installed and is used to connect backend agent outputs with frontend display components.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/agents.mdx#_snippet_5\n\n\n\n----------------------------------------\n\nTITLE: Including Managed Agents Description in System Prompt\nDESCRIPTION: This snippet demonstrates the conditional template syntax for inserting managed agent descriptions. This section will only appear if managed agents are configured for the CodeAgent.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.mdx#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n{%- if managed_agents and managed_agents.values() | list %}\nYou can also give tasks to team members.\nCalling a team member works the same as for calling a tool: simply, the only argument you can give in the call is 'task', a long string explaining your task.\nGiven that this team member is a real human, you should be very verbose in your task.\n```\n\n----------------------------------------\n\nTITLE: Creating a CodeAgent with Smolagents\nDESCRIPTION: This Python code initializes a `CodeAgent` from Smolagents, configuring it with the `retriever_tool` and an `InferenceClientModel` pointing to the meta-llama/Llama-3.3-70B-Instruct model. The agent is set up to perform a maximum of 4 steps and operate in verbose mode, providing detailed output during execution.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/rag.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent\n\nagent = CodeAgent(\n    tools=[retriever_tool], model=InferenceClientModel(model_id=\"meta-llama/Llama-3.3-70B-Instruct\"), max_steps=4, verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing a CodeAgent with Hugging Face LLM and Retriever Tool in Python\nDESCRIPTION: Creates an instance of the CodeAgent class from smolagents, configuring it with the previously defined retriever_tool and an LLM powered by the Hugging Face Inference API using 'meta-llama/Llama-3.3-70B-Instruct' model. Sets maximum reasoning steps to 4 and verbosity to level 2. This agent executes iterative tool calls and generation to answer user queries with retrieval enhancement.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/rag.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent\n\nagent = CodeAgent(\n    tools=[retriever_tool], model=InferenceClientModel(model_id=\"meta-llama/Llama-3.3-70B-Instruct\"), max_steps=4, verbosity_level=2\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Local TransformersModel Agent in Python\nDESCRIPTION: This code snippet shows how to create a CodeAgent using a local Hugging Face transformers pipeline model. It requires the installation of 'smolagents[transformers]'. The model is specified with 'model_id'. Base tools are included via 'add_base_tools=True', while tools list remains empty. The agent can run prompts via the run() method and returns model-generated outputs. This setup offers local inference without needing API tokens.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[transformers]\nfrom smolagents import CodeAgent, TransformersModel\n\nmodel_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n\nmodel = TransformersModel(model_id=model_id)\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using InferenceClientModel with Smolagents in Python\nDESCRIPTION: The `InferenceClientModel` class wraps the Hugging Face Hub's `InferenceClient` allowing the use of various [Inference Providers](https://huggingface.co/docs/inference-providers/index).  The model initialization requires a `provider` argument, which specifies the inference provider to use. It demonstrates sending a message to the model. The output is the response from the specified inference provider. This model depends on the Hugging Face Hub's InferenceClient library.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import InferenceClientModel\n\nmessages = [\n  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]}\n]\n\nmodel = InferenceClientModel(provider=\"novita\")\nprint(model(messages))\n```\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies\nDESCRIPTION: Installs the necessary Python packages for the multi-agent system using pip. The packages include markdownify, duckduckgo-search, and smolagents.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install markdownify duckduckgo-search smolagents --upgrade -q\n```\n\n----------------------------------------\n\nTITLE: Example: Returning Final Short Answer Using final_answer in Python\nDESCRIPTION: This snippet shows the agent reporting a final short answer after multi-step reasoning using the 'final_answer' tool. No input except the answer string is required. Dependency: functional 'final_answer' tool. The output is a single-word string submitted as the conclusive response.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfinal_answer(\"diminished\")\n```\n\n----------------------------------------\n\nTITLE: DockerSandbox Class for Code Execution Python\nDESCRIPTION: This Python code defines a `DockerSandbox` class for running Python code within a Docker container. It uses the `docker` library to build an image (agent-sandbox) based on a Dockerfile located in the current directory, creates a container from that image, and executes the provided code. The `run_code` method executes the code using `python -c`, and it collects the output from the container. The `cleanup` method stops and removes the container to free resources. It includes proper error handling.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/secure_code_execution.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport docker\nimport os\nfrom typing import Optional\n\nclass DockerSandbox:\n    def __init__(self):\n        self.client = docker.from_env()\n        self.container = None\n\n    def create_container(self):\n        try:\n            image, build_logs = self.client.images.build(\n                path=\".\",\n                tag=\"agent-sandbox\",\n                rm=True,\n                forcerm=True,\n                buildargs={},\n                # decode=True\n            )\n        except docker.errors.BuildError as e:\n            print(\"Build error logs:\")\n            for log in e.build_log:\n                if 'stream' in log:\n                    print(log['stream'].strip())\n            raise\n\n        # Create container with security constraints and proper logging\n        self.container = self.client.containers.run(\n            \"agent-sandbox\",\n            command=\"tail -f /dev/null\",  # Keep container running\n            detach=True,\n            tty=True,\n            mem_limit=\"512m\",\n            cpu_quota=50000,\n            pids_limit=100,\n            security_opt=[\"no-new-privileges\"],\n            cap_drop=[\"ALL\"],\n            environment={\n                \"HF_TOKEN\": os.getenv(\"HF_TOKEN\")\n            },\n        )\n\n    def run_code(self, code: str) -> Optional[str]:\n        if not self.container:\n            self.create_container()\n\n        # Execute code in container\n        exec_result = self.container.exec_run(\n            cmd=[\"python\", \"-c\", code],\n            user=\"nobody\"\n        )\n\n        # Collect all output\n        return exec_result.output.decode() if exec_result.output else None\n\n\n    def cleanup(self):\n        if self.container:\n            try:\n                self.container.stop()\n            except docker.errors.NotFound:\n                # Container already removed, this is expected\n                pass\n            except Exception as e:\n                print(f\"Error during cleanup: {e}\")\n            finally:\n                self.container = None  # Clear the reference\n\n# Example usage:\nsandbox = DockerSandbox()\n\ntry:\n    # Define your agent code\n    agent_code = \"\"\"\nimport os\nfrom smolagents import CodeAgent, InferenceClientModel\n\n# Initialize the agent\nagent = CodeAgent(\n    model=InferenceClientModel(token=os.getenv(\"HF_TOKEN\"), provider=\"together\"),\n    tools=[]\n)\n\n# Run the agent\nresponse = agent.run(\"What's the 20th Fibonacci number?\")\nprint(response)\n\"\"\"\n\n    # Run the code in the sandbox\n    output = sandbox.run_code(agent_code)\n    print(output)\n\nfinally:\n    sandbox.cleanup()\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Retriever Tool (Python)\nDESCRIPTION: This Python code defines a custom tool `RetrieverTool` that inherits from `smolagents.Tool`. It wraps a LangChain `BM25Retriever` initialized with the processed documents. The tool is designed to be called by the agent with a `query` string. Its `forward` method performs the BM25 search using the retriever and formats the retrieved document chunks into a single string output for the agent to consume. An instance of this tool is created as `retriever_tool`.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/rag.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import Tool\n\nclass RetrieverTool(Tool):\n    name = \"retriever\"\n    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n    inputs = {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, docs, **kwargs):\n        super().__init__(**kwargs)\n        self.retriever = BM25Retriever.from_documents(\n            docs, k=10\n        )\n\n    def forward(self, query: str) -> str:\n        assert isinstance(query, str), \"Your search query must be a string\"\n\n        docs = self.retriever.invoke(\n            query,\n        )\n        return \"\\nRetrieved documents:\\n\" + \"\".join(\n            [\n                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n                for i, doc in enumerate(docs)\n            ]\n        )\n\nretriever_tool = RetrieverTool(docs_processed)\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with AzureOpenAIServerModel (Python)\nDESCRIPTION: This snippet shows how to initialize a CodeAgent with AzureOpenAIServerModel, connecting to Azure OpenAI. It requires the `smolagents[openai]` extra to be installed. The agent is then used to calculate the 118th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[openai]\nfrom smolagents import CodeAgent, AzureOpenAIServerModel\n\nmodel = AzureOpenAIServerModel(model_id=\"gpt-4o-mini\")\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables\nDESCRIPTION: This Python snippet uses the `dotenv` library to load environment variables from a `.env` file. This allows sensitive information such as API keys to be stored separately from the code.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/rag.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Using AzureOpenAIServerModel with Smolagents in Python\nDESCRIPTION: This demonstrates the `AzureOpenAIServerModel` class, enabling connection to Azure OpenAI deployments.  The example shows how to set up the model with `model_id`, `azure_endpoint`, `api_key`, and `api_version`. It supports environment variables for configuring authentication and endpoints, eliminating the need to specify arguments directly. This model allows integrating with Azure OpenAI services.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```py\nimport os\n\nfrom smolagents import AzureOpenAIServerModel\n\nmodel = AzureOpenAIServerModel(\n    model_id = os.environ.get(\"AZURE_OPENAI_MODEL\"),\n    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n    api_version=os.environ.get(\"OPENAI_API_VERSION\")    \n)\n```\n```\n\n----------------------------------------\n\nTITLE: Using a Space-based Tool with CodeAgent\nDESCRIPTION: This snippet shows how to integrate a tool imported from a Hugging Face Space with a `CodeAgent`.  It initializes the agent with the tool and then runs a task that leverages the Space's image generation capabilities, also showcasing passing additional arguments.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/tools.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\n\nmodel = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\nagent = CodeAgent(tools=[image_generation_tool], model=model)\n\nagent.run(\n    \"Improve this prompt, then generate an image of it.\", additional_args={'user_prompt': 'A rabbit wearing a space suit'}\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing TransformersModel Wrapper from Smolagents - Python\nDESCRIPTION: This snippet presents usage of the TransformersModel class from smolagents, which constructs a local Huggingface transformers pipeline using a specified model_id. It requires 'transformers' and 'torch' packages installed locally (installable via 'pip install smolagents[transformers]'). The model is invoked with a list of messages formatted with roles and content containing text entries, and supports stop_sequences to control generation termination. The output demonstrates partial response generation.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/reference/models.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import TransformersModel\n\nmodel = TransformersModel(model_id=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n\nprint(model([{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Ok!\"}]}], stop_sequences=[\"great\"]))\n```\n\nLANGUAGE: text\nCODE:\n```\n>>> What a\n```\n\n----------------------------------------\n\nTITLE: Using AmazonBedrockServerModel with Smolagents in Python\nDESCRIPTION: This example shows how to use the `AmazonBedrockServerModel` class to connect to Amazon Bedrock.  It requires the `model_id` parameter during initialization. The snippet demonstrates a basic setup for integrating Amazon Bedrock with Smolagents. This allows for utilizing Amazon Bedrock models within the agent framework.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n```py\nimport os\n\nfrom smolagents import AmazonBedrockServerModel\n\nmodel = AmazonBedrockServerModel(\n    model_id = os.environ.get(\"AMAZON_BEDROCK_MODEL_ID\"),\n)\n```\n```\n\n----------------------------------------\n\nTITLE: Importing a Space as a Tool\nDESCRIPTION: This snippet demonstrates how to import a Hugging Face Space directly as a tool using `Tool.from_space`. It requires the Space ID, a name for the tool, and a description. The `gradio-client` library is used internally to interact with the Space.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/tools.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimage_generation_tool = Tool.from_space(\n    \"black-forest-labs/FLUX.1-schnell\",\n    name=\"image_generator\",\n    description=\"Generate an image from a prompt\"\n)\n\nimage_generation_tool(\"A sunny beach\")\n```\n\n----------------------------------------\n\nTITLE: Registering Phoenix for tracing\nDESCRIPTION: This Python code snippet registers OpenTelemetry with Phoenix to trace agent runs. It imports `register` from `phoenix.otel` and instantiates and instruments the `SmolagentsInstrumentor`.  This enables sending traces to the Phoenix backend.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/inspect_runs.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom phoenix.otel import register\nfrom openinference.instrumentation.smolagents import SmolagentsInstrumentor\n\nregister()\nSmolagentsInstrumentor().instrument()\n```\n\n----------------------------------------\n\nTITLE: Importing a LangChain Tool\nDESCRIPTION: This snippet demonstrates importing a tool from LangChain using the `from_langchain` method. It imports the SerpAPI tool and integrates it into a `CodeAgent`. Requires installing `langchain` and `google-search-results`.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/tools.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.agents import load_tools\n\nsearch_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n\nagent = CodeAgent(tools=[search_tool], model=model)\n\nagent.run(\"How many more blocks (also denoted as layers) are in BERT base encoder compared to the encoder from the architecture proposed in Attention is All You Need?\")\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables\nDESCRIPTION: This Python snippet sets up environment variables required for Langfuse integration. It configures the OpenTelemetry endpoint and authorization headers, including API keys (which need to be replaced with the user's specific keys) and the Hugging Face token. The code uses base64 encoding to construct the authorization header.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/inspect_runs.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport base64\n\nLANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\nLANGFUSE_SECRET_KEY=\"sk-lf-...\"\nLANGFUSE_AUTH=base64.b64encode(f\"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}\".encode()).decode()\n\nos.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://cloud.langfuse.com/api/public/otel\" # EU data region\n# os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://us.cloud.langfuse.com/api/public/otel\" # US data region\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n\n# your Hugging Face token\nos.environ[\"HF_TOKEN\"] = \"hf_...\"\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with Docker Executor Python\nDESCRIPTION: This snippet demonstrates how to initialize a `CodeAgent` with the `executor_type` set to \"docker\". This configuration directs the agent to execute code within a Docker container. It requires `smolagents` and a working Docker setup. The agent is then used to calculate the 100th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/secure_code_execution.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent\n\nagent = CodeAgent(model=InferenceClientModel(), tools=[], executor_type=\"docker\")\n\nagent.run(\"Can you give me the 100th Fibonacci number?\")\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables (Python)\nDESCRIPTION: This Python snippet imports the `load_dotenv` function from the `dotenv` library. Calling this function loads environment variables from a `.env` file in the current directory (if one exists) into the system's environment. This is used here specifically to load the `HF_TOKEN` required for authenticating with the Hugging Face Inference API.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/rag.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with LiteLLMModel (Azure OpenAI) (Python)\nDESCRIPTION: This snippet demonstrates how to configure `LiteLLMModel` to connect to Azure OpenAI. It requires setting environment variables for the API key, API base, and API version, and prefixing the model ID with 'azure/'. The agent is then used to calculate the 118th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom smolagents import CodeAgent, LiteLLMModel\n\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"gpt-35-turbo-16k-deployment\" # example of deployment name\n\nos.environ[\"AZURE_API_KEY\"] = \"\" # api_key\nos.environ[\"AZURE_API_BASE\"] = \"\" # \"https://example-endpoint.openai.azure.com\"\nos.environ[\"AZURE_API_VERSION\"] = \"\" # \"2024-10-01-preview\"\n\nmodel = LiteLLMModel(model_id=\"azure/\" + AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n   \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using InferenceClientModel to Execute Chat Interactions via HF Inference API in Python\nDESCRIPTION: The snippet illustrates creating and using an InferenceClientModel instance from smolagents to handle a conversation with predefined messages. It prints the model's generated completion for the given dialogue. The InferenceClientModel wraps the HuggingFace Inference API, enabling seamless LLM interaction by accepting a list of message dicts and returning the assistant's response. Dependencies include smolagents and network connectivity to HuggingFace API.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/reference/agents.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\nmessages = [\n  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"No need to help, take it easy.\"},\n]\n\nmodel = InferenceClientModel()\nprint(model(messages))\n```\n\n----------------------------------------\n\nTITLE: Implementing a BM25 Retriever Tool Class for Semantic Search in Python\nDESCRIPTION: Defines a custom RetrieverTool class extending smolagents' Tool to perform semantic search using a BM25 retriever over the processed document chunks. Initializes with documents, accepts a query string input (expected in affirmative form), and returns a string concatenation of the top 10 retrieved documents with their contents. Ensures type checking for inputs and integrates BM25Retriever from langchain_community.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/rag.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import Tool\nfrom langchain_community.retrievers import BM25Retriever\n\nclass RetrieverTool(Tool):\n    name = \"retriever\"\n    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n    inputs = {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, docs, **kwargs):\n        super().__init__(**kwargs)\n        self.retriever = BM25Retriever.from_documents(\n            docs, k=10\n        )\n\n    def forward(self, query: str) -> str:\n        assert isinstance(query, str), \"Your search query must be a string\"\n\n        docs = self.retriever.invoke(\n            query,\n        )\n        return \"\\nRetrieved documents:\\n\" + \"\".join(\n            [\n                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n                for i, doc in enumerate(docs)\n            ]\n        )\n\nretriever_tool = RetrieverTool(docs_processed)\n```\n\n----------------------------------------\n\nTITLE: Using TransformersModel with Smolagents in Python\nDESCRIPTION: This example uses the `TransformersModel` class to integrate a local `transformers` pipeline within the Smolagents framework.  It takes a `model_id` during initialization, which specifies the model to load. The code demonstrates how to initialize the model and call it with a message list and stop sequences. This model utilizes the `transformers` and `torch` packages which need to be installed separately.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/models.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import TransformersModel\n\nmodel = TransformersModel(model_id=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n\nprint(model([{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Ok!\"}]}], stop_sequences=[\"great\"]))\n```\n```\n\n----------------------------------------\n\nTITLE: Installing SmolaAgents and Dependencies\nDESCRIPTION: Installs the development version of smolagents along with required packages including datasets, sympy, numpy, matplotlib, and seaborn for data processing and visualization.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/score.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -e .. datasets sympy numpy matplotlib seaborn -q  # Install dev version of smolagents + some packages\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running Vision Browser Agent Evaluation (Python)\nDESCRIPTION: Initializes the GPT-4o `LiteLLMModel` and the vision browser agent using `initialize_agent`. Defines a `CodeAgent` with vision-specific tools (search, navigation, pop-up closing), authorizes `helium` imports, sets `save_screenshot` as a step callback, and configures `max_steps` and `verbosity_level`. Finally, it runs the evaluation using `answer_questions`, saving results as 'code_gpt4o_27-01_vision', providing inspection tools, and adding specific instructions (`postprompt`) regarding Helium usage and PDF handling.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nproprietary_model = LiteLLMModel(model_id=\"gpt-4o\")\nvision_browser_agent = initialize_agent(proprietary_model)\n### BUILD AGENTS & TOOLS\n\nCodeAgent(\n    tools=[DuckDuckGoSearchTool(), go_back, close_popups, search_item_ctrl_f],\n    model=proprietary_model,\n    additional_authorized_imports=[\"helium\"],\n    step_callbacks=[save_screenshot],\n    max_steps=20,\n    verbosity_level=2,\n)\n\nresults_vision = answer_questions(\n    eval_ds,\n    vision_browser_agent,\n    \"code_gpt4o_27-01_vision\",\n    reformulation_model=proprietary_model,\n    output_folder=\"output_browsers\",\n    visual_inspection_tool=VisualQAGPT4Tool(),\n    text_inspector_tool=TextInspectorTool(proprietary_model, 40000),\n    postprompt=helium_instructions\n    + \"Any web browser controls won't work on .pdf urls, rather use the tool 'inspect_file_as_text' to read them\",\n)\n```\n\n----------------------------------------\n\nTITLE: Scoring Model Answers from the Dataset\nDESCRIPTION: Loads model answer datasets, processes them with the scoring functions, and returns a dataframe of results with accuracy metrics for each model configuration.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/score.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\nimport pandas as pd\n\n\n# Choose the answers subsets to score:\n# answers_subsets = [\"meta-llama__Llama-3.1-8B-Instruct__code__gaia\"]\n# or get all the answers subsets present in the ANSWERS_DATASET\nanswers_subsets = datasets.get_dataset_config_names(ANSWERS_DATASET)\nprint(\"Number of answers_subsets\", len(answers_subsets))\nprint(\"Example of answers_subset\", answers_subsets[0])\n\nresult_df = score_answers(answers_subsets)\nresult_df[\"acc\"] = (result_df[\"acc\"] * 100).round(2)\nresult_df.head()\n```\n\n----------------------------------------\n\nTITLE: Initializing TransformersModel Wrapper for Local Model Pipeline in Python\nDESCRIPTION: This snippet shows how to initialize the TransformersModel class from the smolagents package by providing a HuggingFace model ID. The example executes the model with a simple user message and a stop sequence to control output generation. It prints the partial generated output. The snippet demonstrates usage of a local transformer-based language model pipeline encapsulated by TransformersModel, which abstracts message formatting and stop sequence handling. It requires the smolagents package and access to the specified model.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/reference/agents.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import TransformersModel\n\nmodel = TransformersModel(model_id=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n\nprint(model([{\"role\": \"user\", \"content\": \"Ok!\"}], stop_sequences=[\"great\"]))\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Chat Completion Model Using huggingface_hub InferenceClient - Python\nDESCRIPTION: This snippet demonstrates how to create a custom callable model function using Huggingface's InferenceClient to perform chat completions. It requires the huggingface_hub package and a valid API token. The custom_model function accepts a list of message dictionaries and an optional stop_sequences list, then calls the InferenceClient's chat_completion method with these parameters and a max token limit. The function returns the first chat message response which includes a .content attribute. This model function adheres to Smolagents' expectation for message-formatted input and controlled output generation.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/reference/models.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import login, InferenceClient\n\nlogin(\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n\nclient = InferenceClient(model=model_id)\n\ndef custom_model(messages, stop_sequences=[\"Task\"]):\n    response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)\n    answer = response.choices[0].message\n    return answer\n```\n\n----------------------------------------\n\nTITLE: Modifying and Customizing Matplotlib Legends in Python\nDESCRIPTION: This snippet modifies the Matplotlib legend by combining agent and vanilla entries into grouped legend elements. It retrieves the existing legend handles and labels, processes them to merge related entries, and creates a custom legend using a specialized handler (HandlerTuple) to manage grouped handles. Additionally, the snippet sets grid lines with custom styles on the y-axis, enforces a bottom axis limit, hides the top and right spines, adjusts the layout to prevent clipping, and finally displays the plot using plt.show(). Required dependencies include Matplotlib and availability of the HandlerTuple from matplotlib.legend_handler.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/score.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nhandles, labels = ax.get_legend_handles_labels()\nunique_sources = sources\nlegend_elements = [\n    (handles[i * 2], handles[i * 2 + 1], labels[i * 2].replace(\" (Agent)\", \"\")) for i in range(len(unique_sources))\n]\ncustom_legend = ax.legend(\n    [(agent_handle, vanilla_handle) for agent_handle, vanilla_handle, _ in legend_elements],\n    [label for _, _, label in legend_elements],\n    handler_map={tuple: HandlerTuple(ndivide=None)},\n    bbox_to_anchor=(1.05, 1),\n    loc=\"upper left\",\n)\n\nax.yaxis.grid(True, linestyle=\"--\", alpha=0.3)\nax.set_ylim(bottom=0)\nplt.tight_layout()\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Understanding the System Prompt Structure with Tool Placeholders\nDESCRIPTION: This snippet shows the template syntax for inserting tool descriptions into a custom system prompt. These placeholders will be automatically populated with tool information when the agent is initialized.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/building_good_agents.mdx#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n{%- for tool in tools.values() %}\n- {{ tool.name }}: {{ tool.description }}\n    Takes inputs: {{tool.inputs}}\n    Returns an output of type: {{tool.output_type}}\n{%- endfor %}\n```\n\n----------------------------------------\n\nTITLE: Running the CodeAgent\nDESCRIPTION: This Python code runs the `CodeAgent` with a specific query, which is \"For a transformers model training, which is slower, the forward or the backward pass?\". The agent processes the query, utilizes the configured tools to retrieve relevant information, and generates an output based on its findings. The final output is then printed to the console.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/rag.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nagent_output = agent.run(\"For a transformers model training, which is slower, the forward or the backward pass?\")\n\nprint(\"Final output:\")\nprint(agent_output)\n```\n\n----------------------------------------\n\nTITLE: Running SmolAgent for GitHub Trending Analysis Task in Python\nDESCRIPTION: This code defines another task (`github_request`) for the agent: find the top trending repository on GitHub, navigate to the top author's profile, and retrieve their total commit count for the last year. Similar to the previous example, it runs the agent with the request and instructions using `agent.run()` and prints the result.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ngithub_request = \"\"\"\nI'm trying to find how hard I have to work to get a repo in github.com/trending.\nCan you navigate to the profile for the top author of the top trending repo, and give me their total number of commits over the last year?\n\"\"\"\n\nagent_output = agent.run(github_request + helium_instructions)\nprint(\"Final output:\")\nprint(agent_output)\n```\n\n----------------------------------------\n\nTITLE: Creating a Multi-Agent System with Managed Agents\nDESCRIPTION: Example of creating a hierarchical multi-agent system with a manager agent and a specialized web search agent.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/guided_tour.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel, DuckDuckGoSearchTool, ManagedAgent\n\nmodel = InferenceClientModel()\n\nweb_agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)\n\nmanaged_web_agent = ManagedAgent(\n    agent=web_agent,\n    name=\"web_search\",\n    description=\"Runs web searches for you. Give it your query as an argument.\"\n)\n\nmanager_agent = CodeAgent(\n    tools=[], model=model, managed_agents=[managed_web_agent]\n)\n\nmanager_agent.run(\"Who is the CEO of Hugging Face?\")\n```\n\n----------------------------------------\n\nTITLE: Using ToolCallingAgent for JSON-like Action Execution in Python\nDESCRIPTION: This snippet demonstrates initializing a ToolCallingAgent, which performs actions by generating JSON-like calls rather than executing code directly, enhancing safety by disallowing arbitrary imports. The agent takes an empty tools list and a specified LLM model and runs prompts to carry out tasks. This agent type is suitable when execution security is paramount and code generation is not allowed.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import ToolCallingAgent\n\nagent = ToolCallingAgent(tools=[], model=model)\nagent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\" )\n```\n\n----------------------------------------\n\nTITLE: Installing telemetry packages\nDESCRIPTION: This snippet installs the necessary Python packages for using smolagents with OpenTelemetry and Arize AI Phoenix.  It installs the smolagents package along with dependencies for telemetry integration.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/inspect_runs.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install 'smolagents[telemetry]'\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies for Langfuse\nDESCRIPTION: This code installs the necessary Python packages for integrating smolagents with Langfuse. It includes `smolagents` and various OpenTelemetry-related packages to export traces to Langfuse.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/inspect_runs.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%pip install smolagents\n%pip install opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-smolagents\n```\n\n----------------------------------------\n\nTITLE: Sample Output of SQL Table Columns in Text Format\nDESCRIPTION: Text output showing the column names and types of the 'receipts' SQL table, which serves as schema documentation for integration with the LLM agent tool. This static textual snippet helps inform the LLM about the structure of the database used in queries.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/text_to_sql.mdx#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nColumns:\n  - receipt_id: INTEGER\n  - customer_name: VARCHAR(16)\n  - price: FLOAT\n  - tip: FLOAT\n```\n\n----------------------------------------\n\nTITLE: Initializing LiteLLMModel Agent Connecting to Ollama LLM Server in Python\nDESCRIPTION: This example demonstrates constructing a LiteLLMModel-powered CodeAgent that connects to a local or remote Ollama LLM chat server on HTTP. It uses 'smolagents[litellm]' and supplies model_id, api_base, and api_key parameters to configure access. Parameters like context length ('num_ctx') can be set to customize the LLM context window. The agent runs tasks with an empty tool set but includes base tools and supports prompt inputs for generating language model output.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[litellm]\nfrom smolagents import CodeAgent, LiteLLMModel\n\nmodel = LiteLLMModel(\n    model_id=\"ollama_chat/llama3.2\", # 这个模型对于 agent 行为来说有点弱\n    api_base=\"http://localhost:11434\", # 如果需要可以替换为远程 open-ai 兼容服务器\n    api_key=\"YOUR_API_KEY\" # 如果需要可以替换为 API key\n    num_ctx=8192 # https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator\n)\n\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing ToolCallingAgent (Python)\nDESCRIPTION: This snippet shows the initialization of a `ToolCallingAgent`. Unlike `CodeAgent`, this agent type relies on JSON-like calls to interact with tools rather than executing Python code directly. It requires a list of tools and a language model (assuming 'model' is an `InferenceClientModel` instance), and the `run` method is used to provide the agent with a task.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import ToolCallingAgent\n\nagent = ToolCallingAgent(tools=[], model=model)\nagent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using an Agent with Custom Tool\nDESCRIPTION: Code example showing how to initialize and run a CodeAgent with a custom tool to perform tasks.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/guided_tour.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel\nagent = CodeAgent(tools=[model_download_tool], model=InferenceClientModel())\nagent.run(\n    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Loading MCP Tools from ToolCollection (SSE)\nDESCRIPTION: This code snippet demonstrates how to load tools from an SSE-based MCP server using `ToolCollection.from_mcp`. It directly passes a dictionary with the \"url\" parameter. A `CodeAgent` is created, using the tools retrieved from the MCP server. Requires the `smolagents` package.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n```py\nfrom smolagents import ToolCollection, CodeAgent\n\nwith ToolCollection.from_mcp({\"url\": \"http://127.0.0.1:8000/sse\"}, trust_remote_code=True) as tool_collection:\n    agent = CodeAgent(tools=[*tool_collection.tools], add_base_tools=True)\n    agent.run(\"Please find a remedy for hangover.\")\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Planning-Enabled CodeAgent with SmolAgents in Python\nDESCRIPTION: This snippet demonstrates how to import necessary modules and environment variables, load external tools, and configure a CodeAgent from the SmolAgents framework with a specified planning interval. The CodeAgent is initialized with a search tool and a remote text-to-image generation tool, and a large language inference model specified by its ID. The planning interval parameter activates a step in which the agent periodically updates its internal knowledge without tool calls. The snippet concludes by running the agent with a query to measure the time a cheetah would take to run a certain distance. Dependencies include the smolagents package, dotenv for environment loading, and access to the SmolAgents Hub for tool loading. Inputs are the question string, and output is the agent's response based on the model and tools.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import load_tool, CodeAgent, InferenceClientModel, DuckDuckGoSearchTool\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Import tool from Hub\nimage_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True)\n\nsearch_tool = DuckDuckGoSearchTool()\n\nagent = CodeAgent(\n    tools=[search_tool],\n    model=InferenceClientModel(model_id=\"Qwen/Qwen2.5-72B-Instruct\"),\n    planning_interval=3 # This is where you activate planning!\n)\n\n# Run it!\nresult = agent.run(\n    \"How long would a cheetah at full speed take to run the length of Pont Alexandre III?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Default System Prompt Template in CodeAgent - Python\nDESCRIPTION: This snippet shows how to access and print the default system prompt string from the CodeAgent's prompt_templates dictionary under the key 'system_prompt'. The output is a multi-line text string that defines the agent's expected reasoning and coding workflow. There are no dependencies besides having an instantiated agent object with a prompt_templates attribute containing the system_prompt key. The input is simply the agent instance, and the output is the printed string. This snippet helps verify or understand the default behavior before modification.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/building_good_agents.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprint(agent.prompt_templates[\"system_prompt\"])\n```\n\n----------------------------------------\n\nTITLE: Importing Vision Browser Tools and Components (Python)\nDESCRIPTION: Imports necessary components for the vision-based web browsing agent. This includes the `VisualQAGPT4Tool`, core `smolagents` classes (`CodeAgent`, `DuckDuckGoSearchTool`, `LiteLLMModel`), and specific functions from `smolagents.vision_web_browser` designed to work with Helium (`close_popups`, `go_back`, `helium_instructions`, `initialize_agent`, `save_screenshot`, `search_item_ctrl_f`).\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom scripts.visual_qa import VisualQAGPT4Tool\n\nfrom smolagents import CodeAgent, DuckDuckGoSearchTool, LiteLLMModel\nfrom smolagents.vision_web_browser import (\n    close_popups,\n    go_back,\n    helium_instructions,\n    initialize_agent,\n    save_screenshot,\n    search_item_ctrl_f,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceClientModel with Model ID and Provider (Python)\nDESCRIPTION: Initializes an InferenceClientModel with explicit model_id and provider parameters, supporting a wide range of inference backends on Hugging Face. Requires smolagents to be installed and appropriate credentials if provider access requires authentication. Returns a model object for downstream agent use.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\nmodel = InferenceClientModel(\n    model_id=\"deepseek-ai/DeepSeek-R1\",\n    provider=\"together\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: GradioUI Integration\nDESCRIPTION: The `GradioUI` class provides a user interface built with Gradio for interacting with agents. A TIP note indicates that `gradio` must be installed, which can be achieved via `pip install smolagents[gradio]`. It serves as the front-end component for agent interaction and visualization.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/agents.mdx#_snippet_6\n\n\n\n----------------------------------------\n\nTITLE: Configuring OpenAIServerModel for Custom OpenAI-Compatible Servers in Python\nDESCRIPTION: This example shows how to instantiate the OpenAIServerModel class from smolagents to interact with a model hosted on any OpenAI-compatible API server. It includes setting the model_id, a custom API base URL, and fetching the API key from environment variables. This approach allows flexibility to target various hosted or private OpenAI API services for chat model invocation. It requires smolagents and proper environment setup with the API key.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/reference/agents.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom smolagents import OpenAIServerModel\n\nmodel = OpenAIServerModel(\n    model_id=\"gpt-4o\",\n    api_base=\"https://api.openai.com/v1\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Running the Agentic RAG System to Answer a Query in Python\nDESCRIPTION: Calls the agent's .run() method with a specific user question to trigger the retrieval-augmented generation process. Prints the final answer returned by the CodeAgent after it completes all tool invocations and reasoning steps.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/rag.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nagent_output = agent.run(\"For a transformers model training, which is slower, the forward or the backward pass?\")\n\nprint(\"Final output:\")\nprint(agent_output)\n```\n\n----------------------------------------\n\nTITLE: PromptTemplates Classes Documentation\nDESCRIPTION: Multiple prompt template classes such as `PromptTemplates`, `PlanningPromptTemplate`, `ManagedAgentPromptTemplate`, and `FinalAnswerPromptTemplate` are documented. These classes define various prompt structures used in guiding agent reasoning, planning, management, and responses. Each is integral to customizing agent behavior and response formatting.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/agents.mdx#_snippet_7\n\n\n\n----------------------------------------\n\nTITLE: Configuring and Running Text Browser Agent Evaluation (Python)\nDESCRIPTION: Initializes a list (`WEB_TOOLS`) containing instances of various text-based web browsing tools. Configures a `CodeAgent` named `surfer_agent` using the previously defined GPT-4o model (`proprietary_model`), the `WEB_TOOLS`, a maximum step limit of 20, and verbosity level 2. Finally, it calls the `answer_questions` function to run this agent on the `eval_ds` dataset, saving the results under the name 'code_gpt4o_27-01_text' in the 'output_browsers' folder, and providing additional text/visual inspection tools.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n### BUILD AGENTS & TOOLS\n\nWEB_TOOLS = [\n    SearchInformationTool(),\n    NavigationalSearchTool(),\n    VisitTool(),\n    PageUpTool(),\n    PageDownTool(),\n    FinderTool(),\n    FindNextTool(),\n    ArchiveSearchTool(),\n]\n\n\nsurfer_agent = CodeAgent(\n    model=proprietary_model,\n    tools=WEB_TOOLS,\n    max_steps=20,\n    verbosity_level=2,\n)\n\nresults_text = answer_questions(\n    eval_ds,\n    surfer_agent,\n    \"code_gpt4o_27-01_text\",\n    reformulation_model=proprietary_model,\n    output_folder=\"output_browsers\",\n    visual_inspection_tool=VisualQAGPT4Tool(),\n    text_inspector_tool=TextInspectorTool(proprietary_model, 40000),\n)\n```\n\n----------------------------------------\n\nTITLE: Inspecting table schema and generating description in Python\nDESCRIPTION: Uses SQLAlchemy's inspect module to retrieve column information of the 'receipts' table and formats this information into a descriptive string. This description serves as metadata for the agent's tool to understand table schema.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/text_to_sql.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ninspector = inspect(engine)\ncolumns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(\"receipts\")]\n\ntable_description = \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\nprint(table_description)\n```\n\n----------------------------------------\n\nTITLE: Accessing and Modifying Agent Memory with Smolagents in Python\nDESCRIPTION: This snippet illustrates how to access and iterate through memory steps of a CodeAgent, using components such as system_prompt and steps. It leverages the ActionStep class to check for errors and observations in each step. The example requires the smolagents package and assumes an initialized agent. Users can examine system prompts, loop through history of steps, and selectively inspect or modify memory. Inputs must be valid agent and memory objects; outputs are printed logs of agent memory state. Limitations: ensure steps are populated appropriately to avoid index errors.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/memory.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import ActionStep\n\nsystem_prompt_step = agent.memory.system_prompt\nprint(\"The system prompt given to the agent was:\")\nprint(system_prompt_step.system_prompt)\n\ntask_step = agent.memory.steps[0]\nprint(\"\\n\\nThe first task step was:\")\nprint(task_step.task)\n\nfor step in agent.memory.steps:\n    if isinstance(step, ActionStep):\n        if step.error is not None:\n            print(f\"\\nStep {step.step_number} got this error:\\n{step.error}\\n\")\n        else:\n            print(f\"\\nStep {step.step_number} got these observations:\\n{step.observations}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Citing smolagents Library (BibTeX)\nDESCRIPTION: Provides the recommended BibTeX entry for citing the `smolagents` library in academic publications. It includes standard fields such as title, author names, howpublished URL, and the publication year.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_12\n\nLANGUAGE: bibtex\nCODE:\n```\n@Misc{smolagents,\n  title =        {`smolagents`: a smol library to build great agentic systems.},\n  author =       {Aymeric Roucher and Albert Villanova del Moral and Thomas Wolf and Leandro von Werra and Erik Kaunismäki},\n  howpublished = {\\url{https://github.com/huggingface/smolagents}},\n  year =         {2025}\n}\n```\n\n----------------------------------------\n\nTITLE: Previewing Documentation Locally - Bash\nDESCRIPTION: Executes the 'doc-builder preview' command, serving the built documentation from the given source directory. This enables local viewing via HTTP (typically at http://localhost:5173). The documentation must already exist in the specified directory. Use when you wish to preview changes prior to publishing.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndoc-builder preview smolagents docs/source/en/\n```\n\n----------------------------------------\n\nTITLE: Filtering and Renaming GAIA Dataset Columns (Python)\nDESCRIPTION: Filters the `eval_ds` dataset to retain only rows where the 'Question' field contains any of the strings specified in the `to_keep` list. Subsequently, it renames the 'Question', 'Final answer', and 'Level' columns to 'question', 'true_answer', and 'task' respectively, likely for compatibility with downstream functions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nto_keep = [\n    \"What's the last line of the rhyme under the flavor\",\n    'Of the authors (First M. Last) that worked on the paper \"Pie Menus or Linear Menus',\n    \"In Series 9, Episode 11 of Doctor Who, the Doctor is trapped inside an ever-shifting maze. What is this location called in the official script for the episode? Give the setting exactly as it appears in the first scene heading.\",\n    \"Which contributor to the version of OpenCV where support was added for the Mask-RCNN model has the same name as a former Chinese head of government when the names are transliterated to the Latin alphabet?\",\n    \"The photograph in the Whitney Museum of American Art's collection with accession number 2022.128 shows a person holding a book. Which military unit did the author of this book join in 1813? Answer without using articles.\",\n    \"I went to Virtue restaurant & bar in Chicago for my birthday on March 22, 2021 and the main course I had was delicious! Unfortunately, when I went back about a month later on April 21, it was no longer on the dinner menu.\",\n    \"In Emily Midkiff's June 2014 article in a journal named for the one of Hreidmar's \",\n    \"Under DDC 633 on Bielefeld University Library's BASE, as of 2020\",\n    \"In the 2018 VSCode blog post on replit.com, what was the command they clicked on in the last video to remove extra lines?\",\n    \"The Metropolitan Museum of Art has a portrait in its collection with an accession number of 29.100.5. Of the consecrators and co-consecrators\",\n    \"In Nature journal's Scientific Reports conference proceedings from 2012, in the article that did not mention plasmons or plasmonics, what nano-compound is studied?\",\n    'In the year 2022, and before December, what does \"R\" stand for in the three core policies of the type of content',\n    \"Who nominated the only Featured Article on English Wikipedia about a dinosaur that was promoted in November 2016?\",\n]\neval_ds = eval_ds.filter(lambda row: any([el in row[\"Question\"] for el in to_keep]))\neval_ds = eval_ds.rename_columns({\"Question\": \"question\", \"Final answer\": \"true_answer\", \"Level\": \"task\"})\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running a Smolagents CodeAgent with Periodic Planning in Python\nDESCRIPTION: This snippet initializes a CodeAgent from the Smolagents library in Python, loading necessary tools including a DuckDuckGo search tool and a text-to-image generation tool from the Huggingface Hub. It leverages an environment configuration loader and sets a planning interval to activate supplementary planning steps where the model refreshes its known facts before deciding the next action. The agent is then run with a question input, and the intended output is the model's inferred response. Dependencies include the smolagents package, dotenv for loading environment variables, and internet access for loading remote tools and models.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/building_good_agents.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import load_tool, CodeAgent, InferenceClientModel, DuckDuckGoSearchTool\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# 从 Hub 导入工具\nimage_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True)\n\nsearch_tool = DuckDuckGoSearchTool()\n\nagent = CodeAgent(\n    tools=[search_tool],\n    model=InferenceClientModel(model_id=\"Qwen/Qwen2.5-72B-Instruct\"),\n    planning_interval=3 # 这是你激活预划的处后！\n)\n\n# 运行它！\nresult = agent.run(\n    \"How long would a cheetah at full speed take to run the length of Pont Alexandre III?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Correctly Answered Questions by Vision Agent (Python)\nDESCRIPTION: Filters the `results_vision` DataFrame using boolean indexing (`.loc`) to select and display only the rows where the 'is_correct' column is True. This shows the specific questions from the evaluation set that were answered correctly by the vision-based browser agent.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncorrect_vision_results = results_vision.loc[results_vision[\"is_correct\"]]\ncorrect_vision_results\n```\n\n----------------------------------------\n\nTITLE: Manual MCPClient Connection Lifecycle (stdio)\nDESCRIPTION: This snippet demonstrates manual management of the `MCPClient` connection using a `try...finally` block. It initializes server parameters, connects to the server, retrieves the tools, uses them in a `CodeAgent`, and ensures disconnection in the `finally` block, regardless of errors. Requires `smolagents` and `mcp` packages.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import MCPClient, CodeAgent\nfrom mcp import StdioServerParameters\nimport os\n\n# Initialize server parameters\nserver_parameters = StdioServerParameters(\n    command=\"uvx\",\n    args=[\"--quiet\", \"pubmedmcp@0.1.3\"],\n    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n)\n\n# Manually manage the connection\ntry:\n    mcp_client = MCPClient(server_parameters)\n    tools = mcp_client.get_tools()\n\n    # Use the tools with your agent\n    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)\n    result = agent.run(\"What are the recent therapeutic approaches for Alzheimer's disease?\")\n\n    # Process the result as needed\n    print(f\"Agent response: {result}\")\nfinally:\n    # Always ensure the connection is properly closed\n    mcp_client.disconnect()\n```\n```\n\n----------------------------------------\n\nTITLE: Stepping Through Agent Tasks Iteratively with Smolagents in Python\nDESCRIPTION: This example demonstrates running an agent in single-step increments using CodeAgent, ActionStep, and TaskStep. It supports memory mutation between steps and conditional logic for termination. Required dependencies include smolagents. The input is a task string and the output is the agent's final answer after a loop of at most 10 steps. Users can modify memory on each step and inspect or replace steps as necessary. Be careful with step limits to prevent infinite loops.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/memory.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent, ActionStep, TaskStep\n\nagent = CodeAgent(tools=[], model=InferenceClientModel(), verbosity_level=1)\nagent.python_executor.send_tools({**agent.tools})\nprint(agent.memory.system_prompt)\n\ntask = \"What is the 20th Fibonacci number?\"\n\n# You could modify the memory as needed here by inputting the memory of another agent.\n# agent.memory.steps = previous_agent.memory.steps\n\n# Let's start a new task!\nagent.memory.steps.append(TaskStep(task=task, task_images=[]))\n\nfinal_answer = None\nstep_number = 1\nwhile final_answer is None and step_number <= 10:\n    memory_step = ActionStep(\n        step_number=step_number,\n        observations_images=[],\n    )\n    # Run one step.\n    final_answer = agent.step(memory_step)\n    agent.memory.steps.append(memory_step)\n    step_number += 1\n\n    # Change the memory as you please!\n    # For instance to update the latest step:\n    # agent.memory.steps[-1] = ...\n\nprint(\"The final answer is:\", final_answer)\n```\n\n----------------------------------------\n\nTITLE: Installing smolagents and Dependencies via pip (bash)\nDESCRIPTION: This snippet installs the required Python dependencies for the project using pip: smolagents, python-dotenv, and SQLAlchemy. smolagents is the core agent library, python-dotenv is used for loading environment variables, and SQLAlchemy provides ORM and SQL database interaction capabilities. The --upgrade and -q flags ensure dependencies are up-to-date and installation output is minimized. This should be run in a terminal or notebook cell before proceeding.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install smolagents python-dotenv sqlalchemy --upgrade -q\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies for smolagents (Shell)\nDESCRIPTION: Installs the `smolagents` package in editable mode along with its development dependencies using pip. This command is essential for setting up a local development environment to make code changes.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Manager Agent\nDESCRIPTION: Creates a `CodeAgent` named `manager_agent` with the `web_agent` passed in as a `managed_agent`. Also, the `additional_authorized_imports` argument is set to include modules `time`, `numpy` and `pandas`.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmanager_agent = CodeAgent(\n    tools=[],\n    model=model,\n    managed_agents=[web_agent],\n    additional_authorized_imports=[\"time\", \"numpy\", \"pandas\"],\n)\n```\n\n----------------------------------------\n\nTITLE: ManagedAgent Deprecated Class\nDESCRIPTION: The `ManagedAgent` class is deprecated since version 1.8.0. Instead of using this class, attributes `name` and `description` should be directly passed to a standard agent to enable management. It previously facilitated agent management and orchestration but is now obsolete.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/agents.mdx#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Fetching Most Downloaded Model for a Task\nDESCRIPTION: A simple Python code that fetches the most downloaded model for a specified task from the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/guided_tour.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import list_models\n\ntask = \"text-classification\"\n\nmost_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\nprint(most_downloaded_model.id)\n```\n\n----------------------------------------\n\nTITLE: Running Agent with Modified Toolbox (Python)\nDESCRIPTION: Executes the agent after adding a custom tool to its toolbox. It demonstrates that the agent can now recognize and utilize the newly added tool based on the provided prompt, showcasing the dynamic nature of the agent's tool management.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nagent.run(\n    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub but reverse the letters?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Tool to Agent Toolbox (Python)\nDESCRIPTION: Shows how to add a new tool to an existing `smolagents` agent's toolbox. It initializes an agent, accesses its `tools` attribute (a dictionary), and assigns a tool instance to the dictionary using the tool's name as the key, making the new tool available for the agent's use.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\nmodel = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\nagent.tools[model_download_tool.name] = model_download_tool\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Doc-Builder - Bash\nDESCRIPTION: Runs the 'doc-builder build' command to generate documentation from the Smolagents source files. The '--build_dir' argument determines the output directory for the rendered files. This command creates the build directory if not present and processes the markdown sources into MDX format for website rendering. All required packages must be installed prior to running.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndoc-builder build smolagents docs/source/en/ --build_dir ~/tmp/test-build\n```\n\n----------------------------------------\n\nTITLE: Using MCP Tools with MCPClient (SSE)\nDESCRIPTION: This snippet illustrates using `MCPClient` with an SSE-based MCP server. It passes a dictionary with the \"url\" parameter to `MCPClient`. It then creates and runs a `CodeAgent` with the tools obtained from the server. Requires the `smolagents` package.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import MCPClient, CodeAgent\n\nwith MCPClient({\"url\": \"http://127.0.0.1:8000/sse\"}) as tools:\n    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)\n    agent.run(\"Please find a remedy for hangover.\")\n```\n```\n\n----------------------------------------\n\nTITLE: Splitting Documents into Chunks Using LangChain's RecursiveCharacterTextSplitter in Python\nDESCRIPTION: Splits the loaded documents into smaller chunks of 500 characters with 50 characters overlap for better retrieval performance. Uses multiple separators to accurately split text segments. Produces a processed list of document chunks to be used by retrievers.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/rag.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=50,\n    add_start_index=True,\n    strip_whitespace=True,\n    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n)\ndocs_processed = text_splitter.split_documents(source_docs)\n```\n\n----------------------------------------\n\nTITLE: Aggregating and Calculating Mean Accuracy per Agent (Python)\nDESCRIPTION: Concatenates the three result DataFrames (`results_vision`, `results_text`, `results_browseruse`) into a single DataFrame named `results`. It then groups this combined DataFrame by the 'agent_name' column and calculates the mean of the 'is_correct' column for each agent group, effectively determining the average accuracy for each agent type.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresults = pd.concat([results_vision, results_text, results_browseruse])\nresults.groupby(\"agent_name\")[\"is_correct\"].mean()\n```\n\n----------------------------------------\n\nTITLE: Pivoting Results Data for Visualization\nDESCRIPTION: Transforms the results dataframe into a pivot table format, organizing data by model_id and source with columns for different agent action types, making it easier to compare performance across different configurations.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/score.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npivot_df = result_df.pivot_table(\n    index=[\"model_id\", \"source\"],\n    columns=[\"agent_action_type\"],\n    values=\"acc\",\n    fill_value=float(\"nan\"),\n).reset_index()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAIServerModel with Custom API Endpoint and Key (Python)\nDESCRIPTION: Initializes OpenAIServerModel to use a custom OpenAI-compatible API endpoint and authentication key, with a selected model_id. Requires appropriate values for TOGETHER_API_KEY and API base URL. This pattern supports inference on alternate (non-OpenAI) compatible providers.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom smolagents import OpenAIServerModel\n\nmodel = OpenAIServerModel(\n    model_id=\"deepseek-ai/DeepSeek-R1\",\n    api_base=\"https://api.together.xyz/v1/\", # Leave this blank to query OpenAI servers.\n    api_key=os.environ[\"TOGETHER_API_KEY\"], # Switch to the API key for the server you're targeting.\n)\n\n```\n\n----------------------------------------\n\nTITLE: Appending Custom Text to System Prompt Template in CodeAgent - Python\nDESCRIPTION: This snippet demonstrates how to customize the CodeAgent's default system prompt by appending additional instructions or content. It accesses the prompt_templates dictionary's 'system_prompt' key and concatenates a new string with a newline separator. This operation allows enhancement or extension of the agent's system prompt without overwriting the whole prompt. It requires an existing agent object with a prompt_templates attribute supporting such assignment. The snippet emphasizes the ease of tailoring the system prompt while maintaining the original structure and placeholders intact.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/building_good_agents.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nagent.prompt_templates[\"system_prompt\"] = agent.prompt_templates[\"system_prompt\"] + \"\\nHere you go!\"\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with MLXModel (Python)\nDESCRIPTION: This snippet shows how to initialize a CodeAgent with MLXModel, leveraging mlx-lm.  It requires the `smolagents[mlx-lm]` extra to be installed. The agent is then used to calculate the 118th Fibonacci number.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# !pip install smolagents[mlx-lm]\nfrom smolagents import CodeAgent, MLXModel\n\nmLx_model = MLXModel(\"mlx-community/Qwen2.5-Coder-32B-Instruct-4bit\")\nagent = CodeAgent(model=mlx_model, tools=[], add_base_tools=True)\n\nagent.run(\"Could you give me the 118th number in the Fibonacci sequence?\")\n```\n\n----------------------------------------\n\nTITLE: Creating Bar Chart Visualization of Model Performance\nDESCRIPTION: Generates a bar chart comparing agent and vanilla performance across different models and benchmark tasks, with customized styling and formatting to clearly show the differences in performance.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/score.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerTuple  # Added import\n\n\n# Assuming pivot_df is your original dataframe\nmodels = pivot_df[\"model_id\"].unique()\nsources = pivot_df[\"source\"].unique()\n\n# Create figure and axis\nplt.style.use(\"seaborn-v0_8-white\")\nfig, ax = plt.subplots(figsize=(15, 6))\n\n# Set the width of each bar group and positions of the bars\nwidth = 0.15  # width of each bar\nspacing = 0.02  # space between bars within a group\ngroup_spacing = 0.2  # space between model groups\n\n# Calculate positions for the bars\nnum_sources = len(sources)\ntotal_width_per_group = (width + spacing) * num_sources * 2  # *2 for agent and vanilla\nx = np.arange(len(models)) * (total_width_per_group + group_spacing)\n\n# Plot bars for each source\nfor i, source in enumerate(sources):\n    source_data = pivot_df[pivot_df[\"source\"] == source]\n    agent_scores = [\n        source_data[source_data[\"model_id\"] == model][\"code\"].values[0]\n        if len(source_data[source_data[\"model_id\"] == model]) > 0\n        else np.nan\n        for model in models\n    ]\n    vanilla_scores = [\n        source_data[source_data[\"model_id\"] == model][\"vanilla\"].values[0]\n        if len(source_data[source_data[\"model_id\"] == model]) > 0\n        else np.nan\n        for model in models\n    ]\n\n    # Position calculation for each pair of bars\n    pos = x + i * (width * 2 + spacing)\n\n    agent_bars = ax.bar(pos, agent_scores, width, label=f\"{source} (Agent)\", alpha=0.8)\n    vanilla_bars = ax.bar(\n        pos + width * 0.6,\n        vanilla_scores,\n        width,\n        hatch=\"////\",\n        alpha=0.5,\n        hatch_linewidth=2,\n        label=f\"{source} (Vanilla)\",\n        color=\"white\",\n        edgecolor=agent_bars[0].get_facecolor(),\n    )\n\n# Customize the plot\nax.set_ylabel(\"Score\")\nax.set_title(\"Model Performance Comparison\")\n\n# Set x-axis ticks in the middle of each group\ngroup_centers = x + (total_width_per_group - spacing) / 2\nax.set_xticks(group_centers)\n\n# Wrap long model names to prevent overlap\nwrapped_labels = [\"\\n\".join(model.split(\"/\")) for model in models]\nax.set_xticklabels(wrapped_labels, rotation=0, ha=\"center\")\n```\n\n----------------------------------------\n\nTITLE: Connecting to Multiple MCP Servers\nDESCRIPTION: This snippet shows how to connect to multiple MCP servers concurrently using `MCPClient`. It defines parameters for both stdio and SSE servers, then passes a list of these parameters to `MCPClient`.  A `CodeAgent` then runs using the combined tools. Requires `smolagents` and `mcp` packages.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom smolagents import MCPClient, CodeAgent\nfrom mcp import StdioServerParameters\nimport os\n\nserver_params1 = StdioServerParameters(\n    command=\"uvx\",\n    args=[\"--quiet\", \"pubmedmcp@0.1.3\"],\n    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n)\n\nserver_params2 = {\"url\": \"http://127.0.0.1:8000/sse\"}\n\nwith MCPClient([server_params1, server_params2]) as tools:\n    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)\n    agent.run(\"Please analyze the latest research and suggest remedies for headaches.\")\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Smolagents with Docker Dependencies Bash\nDESCRIPTION: This command uses pip to install the `smolagents` package along with docker dependencies. This installation prepares the environment for utilizing Docker sandboxes with Smolagents. It is crucial for running agents within Docker containers.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/secure_code_execution.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install 'smolagents[docker]'\n```\n\n----------------------------------------\n\nTITLE: Replaying Agent Memory with SmolAgents (Python)\nDESCRIPTION: This snippet demonstrates initializing a CodeAgent instance with an InferenceClientModel and running it with a simple Fibonacci question. After execution, the agent's replay method is used to review the memory and execution trace of the last run. Requires the smolagents package and its dependencies. Inputs include the natural language task prompt; output is the agent's response and optionally a visual step replay. The agent must be constructed with tool lists and model, and verbosity can be adjusted.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/memory.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import InferenceClientModel, CodeAgent\n\nagent = CodeAgent(tools=[], model=InferenceClientModel(), verbosity_level=0)\n\nresult = agent.run(\"What's the 20th Fibonacci number?\")\n```\n\nLANGUAGE: Python\nCODE:\n```\nagent.replay()\n```\n\n----------------------------------------\n\nTITLE: Creating a Multi-Agent System in Python\nDESCRIPTION: Implementation of a hierarchical multi-agent system with a specialized web search agent managed by a main agent. Shows how to structure agents with proper names and descriptions for effective collaboration.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent, InferenceClientModel, DuckDuckGoSearchTool\n\nmodel = InferenceClientModel()\n\nweb_agent = CodeAgent(\n    tools=[DuckDuckGoSearchTool()],\n    model=model,\n    name=\"web_search\",\n    description=\"Runs web searches for you. Give it your query as an argument.\"\n)\n\nmanager_agent = CodeAgent(\n    tools=[], model=model, managed_agents=[web_agent]\n)\n\nmanager_agent.run(\"Who is the CEO of Hugging Face?\")\n```\n\n----------------------------------------\n\nTITLE: Updating Agent Screenshot Memory via Step Callback (Python)\nDESCRIPTION: This callback function is designed to be hooked into a SmolAgents CodeAgent to update observation images for memory steps, keeping only the most recent screenshot. It uses external libraries helium and PIL, and is intended to reduce memory/token usage for agents that process webpage screenshots. Function parameters include the specific memory_step and the owning agent. Requires helium for browser automation and PIL for image processing. Input is the memory step and agent instance; output modifies agent memory in-place by updating or clearing images.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/memory.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport helium\nfrom PIL import Image\nfrom io import BytesIO\nfrom time import sleep\n\ndef update_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:\n    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot\n    driver = helium.get_driver()\n    latest_step = memory_step.step_number\n    for previous_memory_step in agent.memory.steps:  # Remove previous screenshots from logs for lean processing\n        if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= latest_step - 2:\n            previous_memory_step.observations_images = None\n    png_bytes = driver.get_screenshot_as_png()\n    image = Image.open(BytesIO(png_bytes))\n    memory_step.observations_images = [image.copy()]\n```\n\n----------------------------------------\n\nTITLE: Creating VisitWebpageTool\nDESCRIPTION: Creates a custom tool `visit_webpage` using `markdownify` to extract and convert webpage content to markdown. It handles potential request exceptions and returns a markdown string or an error message.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/multiagents.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport re\nimport requests\nfrom markdownify import markdownify\nfrom requests.exceptions import RequestException\nfrom smolagents import tool\n\n\n@tool\ndef visit_webpage(url: str) -> str:\n    \"\"\"Visits a webpage at the given URL and returns its content as a markdown string.\n\n    Args:\n        url: The URL of the webpage to visit.\n\n    Returns:\n        The content of the webpage converted to Markdown, or an error message if the request fails.\n    \"\"\"\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for bad status codes\n\n        # Convert the HTML content to Markdown\n        markdown_content = markdownify(response.text).strip()\n\n        # Remove multiple line breaks\n        markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n\n        return markdown_content\n\n    except RequestException as e:\n        return f\"Error fetching the webpage: {str(e)}\"\n    except Exception as e:\n        return f\"An unexpected error occurred: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Updating Screenshot Observations in Agent Memory with Helium and PIL in Python\nDESCRIPTION: This callback function enables dynamic management of memory steps by updating the most recent observation image and clearing outdated images, optimizing memory usage for web browser agents. Dependencies include helium, PIL (Pillow), and io. The function expects a memory_step (of type ActionStep) and agent (CodeAgent) as arguments, accesses the browser driver for a screenshot, and overwrites the observations_images attribute for current and previous steps. Limitation: code is incomplete; ensure required objects and imports are present in the actual use case.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/memory.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport helium\nfrom PIL import Image\nfrom io import BytesIO\nfrom time import sleep\n\ndef update_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:\n    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot\n    driver = helium.get_driver()\n    latest_step = memory_step.step_number\n    for previous_memory_step in agent.memory.steps:  # Remove previous screenshots from logs for lean processing\n        if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= latest_step - 2:\n            previous_memory_step.observations_images = None\n    png_bytes = driver.get_screenshot_as_png()\n    image = Image.open(BytesIO(png_bytes))\n    memory_step.observations_images = [image.copy()]\n```\n\n----------------------------------------\n\nTITLE: Configuring SmolaAgents Benchmark Parameters\nDESCRIPTION: Sets up configuration parameters for the benchmark, including evaluation date, dataset paths for evaluation, answers, and results, and flags for pushing datasets to the Hub.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/score.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Benchmark date\n# - set a concrete date:\nDATE = \"2024-12-26\"\n# - or use default: today\n# DATE = None\n\n# Evaluation dataset\n# - the dataset is gated, so you must first visit its page to request access: https://huggingface.co/datasets/smolagents-benchmark/benchmark-v1\nEVAL_DATASET = \"smolagents-benchmark/benchmark-v1\"\n\n# Answers dataset: it must be a gated dataset; required to score the answers\nANSWERS_DATASET = \"smolagents-benchmark/answers\"\n# Whether to push the answers dataset to the Hub\nPUSH_ANSWERS_DATASET_TO_HUB = True\n\n# Results dataset\nRESULTS_DATASET = \"smolagents-benchmark/results\"\n# Whether to push the results dataset to the Hub\nPUSH_RESULTS_DATASET_TO_HUB = True\n```\n\n----------------------------------------\n\nTITLE: Displaying Result Table\nDESCRIPTION: Displays the pivot table showing benchmark results for different models, sources, and agent configurations.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/score.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndisplay(pivot_df)\n```\n\n----------------------------------------\n\nTITLE: Running Agent with Another Task - Python\nDESCRIPTION: This snippet defines a different request, intended to test the agent's ability to find GitHub repository information by navigating to a trending page, and retrieves the number of commits. It shows an alternate use case of the web automation system. The output is printed.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/web_browser.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ngithub_request = \"\"\"\nI'm trying to find how hard I have to work to get a repo in github.com/trending.\nCan you navigate to the profile for the top author of the top trending repo, and give me their total number of commits over the last year?\n\"\"\"\n\nagent_output = agent.run(github_request + helium_instructions)\nprint(\"Final output:\")\nprint(agent_output)\n```\n\n----------------------------------------\n\nTITLE: Recreating a Search Tool using LangChain with Tool.from_langchain in Python\nDESCRIPTION: Illustrates integration with LangChain by loading web search tools via `load_tools` and wrapping them with `Tool.from_langchain`, enabling usage within the agency framework for web searches.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/tools.mdx#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom langchain.agents import load_tools\n\nsearch_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n\n# Usage in an agent\nagent = CodeAgent(tools=[search_tool], model=model)\n\nagent.run(\"How many more blocks (also denoted as layers) are in BERT base encoder compared to the encoder from the architecture proposed in Attention is All You Need?\")\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables and Hugging Face Login (Python)\nDESCRIPTION: Imports `os`, `load_dotenv` from `dotenv`, and `login` from `huggingface_hub`. It loads environment variables from a `.env` file (overriding existing ones) and then uses the value of the `HF_TOKEN` environment variable to authenticate with the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom dotenv import load_dotenv\nfrom huggingface_hub import login\n\n\nload_dotenv(override=True)\n\nlogin(os.getenv(\"HF_TOKEN\"))\n```\n\n----------------------------------------\n\nTITLE: Adding Section Relocation Anchors - Markdown\nDESCRIPTION: Demonstrates how to create relative links and anchors in Markdown to handle relocated or renamed documentation sections. Maintains navigability for legacy URLs by mapping old section anchors to new locations or headers. No external dependencies are required, but basic Markdown knowledge is needed.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/README.md#_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nSections that were moved:\n\n[ <a href=\"#section-b\">Section A</a><a id=\"section-a\"></a> ]\n```\n\nLANGUAGE: markdown\nCODE:\n```\nSections that were moved:\n\n[ <a href=\"../new-file#section-b\">Section A</a><a id=\"section-a\"></a> ]\n```\n\n----------------------------------------\n\nTITLE: Example: Extracting Oldest Person Using document_qa and image_generator in Python\nDESCRIPTION: This snippet demonstrates using notional tools within the agent's code execution context: first, it finds the oldest person in a provided document using the 'document_qa' tool, prints the intermediate answer, and then calls 'image_generator' to produce an image based on the identified person. The example shows the required print and tool invocation pattern, ending with 'final_answer'. Dependencies: access to document_qa, image_generator, and final_answer tools; a 'document' variable must be defined. Outputs include the identified person's description and the generated image object.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nanswer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\nprint(answer)\n```\n\nLANGUAGE: python\nCODE:\n```\nimage = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\nfinal_answer(image)\n```\n\n----------------------------------------\n\nTITLE: Initializing CodeAgent with AmazonBedrockServerModel (Advanced) (Python)\nDESCRIPTION: This snippet demonstrates advanced configuration of AmazonBedrockServerModel, including providing a custom Bedrock client and additional API configurations like inference and guardrail settings.  It shows how to initialize the agent using a custom boto3 client and specific API configurations.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\nfrom smolagents import AmazonBedrockServerModel\n\n# Create a custom Bedrock client\nbedrock_client = boto3.client(\n    'bedrock-runtime',\n    region_name='us-east-1',\n    aws_access_key_id='YOUR_ACCESS_KEY',\n    aws_secret_access_key='YOUR_SECRET_KEY'\n)\n\nadditional_api_config = {\n    \"inferenceConfig\": {\n        \"maxTokens\": 3000\n    },\n    \"guardrailConfig\": {\n        \"guardrailIdentifier\": \"identify1\",\n        \"guardrailVersion\": 'v1'\n    },\n}\n\n# Initialize with comprehensive configuration\nmodel = AmazonBedrockServerModel(\n    model_id=\"us.amazon.nova-pro-v1:0\",\n    client=bedrock_client,  # Use custom client\n    **additional_api_config\n)\n\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n    \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Managed CodeAgent for Multi-Agent Orchestration in Python\nDESCRIPTION: Initializes a 'CodeAgent' with no direct tools, utilizing the previously constructed model and managing the previously created web agent. Grants authority to import additional Python libraries (time, numpy, pandas) for more advanced reasoning tasks. This setup allows the manager agent to delegate tasks and perform high-level orchestration and computation. Dependencies: smolagents, defined web_agent. Inputs: model object, managed_agents list, authorized imports. Output: manager_agent object ready to run commands.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/multiagents.mdx#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nmanager_agent = CodeAgent(\n    tools=[],\n    model=model,\n    managed_agents=[web_agent],\n    additional_authorized_imports=[\"time\", \"numpy\", \"pandas\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring LiteLLMModel to Connect Azure OpenAI Deployment in Python\nDESCRIPTION: This example shows setting environment variables to configure LiteLLMModel for Azure OpenAI deployments by prefixing 'model_id' with 'azure/'. It sets 'AZURE_API_KEY', 'AZURE_API_BASE', and 'AZURE_API_VERSION' environment variables and initializes a LiteLLMModel wrapped in a CodeAgent. The agent runs requests with empty tools and default base tools enabled. This config enables communication with Azure OpenAI using LiteLLM abstractions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom smolagents import CodeAgent, LiteLLMModel\n\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"gpt-35-turbo-16k-deployment\" # example of deployment name\n\nos.environ[\"AZURE_API_KEY\"] = \"\" # api_key\nos.environ[\"AZURE_API_BASE\"] = \"\" # \"https://example-endpoint.openai.azure.com\"\nos.environ[\"AZURE_API_VERSION\"] = \"\" # \"2024-10-01-preview\"\n\nmodel = LiteLLMModel(model_id=\"azure/\" + AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\n\nagent.run(\n   \"Could you give me the 118th number in the Fibonacci sequence?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This bash script installs the necessary Python packages including smolagents, pandas, langchain, sentence-transformers, and rank_bm25. The --upgrade flag ensures that the latest versions are installed, and -q enables quiet mode to minimize output during installation.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/rag.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install smolagents pandas langchain langchain-community sentence-transformers rank_bm25 --upgrade -q\n```\n\n----------------------------------------\n\nTITLE: Example: Translating Image Question and Using image_qa in Python\nDESCRIPTION: This snippet demonstrates a two-step process: first, it uses the 'translator' tool to convert a French question about an image to English, prints the translated question, then queries the 'image_qa' tool using the translation. The result is passed to final_answer. Dependencies: translator, image_qa, final_answer tools; variables 'question' and 'image' must be defined. Inputs include the French question and an image path. Outputs: translation and image analysis results.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntranslated_question = translator(question=question, src_lang=\"French\", tgt_lang=\"English\")\nprint(f\"The translated question is {translated_question}.\")\nanswer = image_qa(image=image, question=translated_question)\nfinal_answer(f\"The answer is {answer}\")\n```\n\n----------------------------------------\n\nTITLE: Processing and Scoring Agent Results (Python)\nDESCRIPTION: Imports `pandas` for data manipulation and `question_scorer` from a custom script (`scripts.gaia_scorer`). Converts the raw results lists (`results_vision`, `results_text`, `results_browseruse`) into pandas DataFrames. It then applies the `question_scorer` function row-wise to each DataFrame to compare the 'prediction' with the 'true_answer', adding a new boolean column 'is_correct' indicating the accuracy of each answer.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom scripts.gaia_scorer import question_scorer\n\n\nresults_vision, results_text, results_browseruse = (\n    pd.DataFrame(results_vision),\n    pd.DataFrame(results_text),\n    pd.DataFrame(results_browseruse),\n)\n\nresults_vision[\"is_correct\"] = results_vision.apply(\n    lambda x: question_scorer(x[\"prediction\"], x[\"true_answer\"]), axis=1\n)\nresults_text[\"is_correct\"] = results_text.apply(lambda x: question_scorer(x[\"prediction\"], x[\"true_answer\"]), axis=1)\nresults_browseruse[\"is_correct\"] = results_browseruse.apply(\n    lambda x: question_scorer(x[\"prediction\"], x[\"true_answer\"]), axis=1\n)\n```\n\n----------------------------------------\n\nTITLE: Updating SQL Tool Description with Multiple Tables for Enhanced Agent Queries in Python\nDESCRIPTION: Recreates the tool's description string to include both 'receipts' and 'waiters' tables and their column schemas by inspecting the metadata, allowing the LLM agent to use updated schema information to perform more complex SQL queries including joins. It reads both tables' schemas from the SQLAlchemy engine and appends them into a multi-table description string used as the tool's docstring.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/text_to_sql.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nupdated_description = \"\"\"Allows you to perform SQL queries on the table. Beware that this tool's output is a string representation of the execution output.\nIt can use the following tables:\"\"\"\n\ninspector = inspect(engine)\nfor table in [\"receipts\", \"waiters\"]:\n    columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(table)]\n\n    table_description = f\"Table '{table}':\\n\"\n\n    table_description += \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n    updated_description += \"\\n\\n\" + table_description\n\nprint(updated_description)\n```\n\n----------------------------------------\n\nTITLE: Running Code Quality Checks for smolagents (Shell)\nDESCRIPTION: Executes the `quality` target defined in the project's Makefile to check the code quality of the source code. This command is used to ensure code adheres to project standards before committing changes.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nmake quality\n```\n\n----------------------------------------\n\nTITLE: Accessing and Modifying Agent Memory Steps (Python)\nDESCRIPTION: This snippet shows how to access and modify the detailed steps in a CodeAgent's memory. It retrieves the system prompt, inspects the first task step, and iterates through memory steps to check for errors or observations using the ActionStep class. Requires smolagents and its step-related classes. Useful for custom inspection, dynamic analysis, or debugging purposes. Input is an agent with populated memory; outputs are print statements detailing the internal state or errors at each step.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/memory.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import ActionStep\n\nsystem_prompt_step = agent.memory.system_prompt\nprint(\"The system prompt given to the agent was:\")\nprint(system_prompt_step.system_prompt)\n\ntask_step = agent.memory.steps[0]\nprint(\"\\n\\nThe first task step was:\")\nprint(task_step.task)\n\nfor step in agent.memory.steps:\n    if isinstance(step, ActionStep):\n        if step.error is not None:\n            print(f\"\\nStep {step.step_number} got this error:\\n{step.error}\\n\")\n        else:\n            print(f\"\\nStep {step.step_number} got these observations:\\n{step.observations}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Running a Multi-Agent Query Workflow for LLM Training Power Projections in Python\nDESCRIPTION: Executes the full multi-agent system by invoking the manager agent with a complex query combining future projections, research, and calculations. The input is a user query string requesting electric power projections for large LLM runs by 2030, with comparison to various countries and citation requirements. Output is the fully researched answer assembled by the collaborative agent system. Dependencies: previously configured manager_agent and its subordinate agents and tools.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/multiagents.mdx#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nanswer = manager_agent.run(\"If LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What would that correspond to, compared to some countries? Please provide a source for any numbers used.\")\n```\n\n----------------------------------------\n\nTITLE: Running a Webagent via CLI for Web Browsing Automation (Bash)\nDESCRIPTION: Shows how to use the webagent CLI command for automating web interaction with shopping and information extraction, specifying a language model. Expects agent infrastructure to be set up and an accessible web environment. Takes a web navigation and extraction prompt and runs the LLM-powered agent headlessly.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nwebagent \"go to xyz.com/men, get to sale section, click the first clothing item you see. Get the product details, and the price, return them. note that I'm shopping from France\" --model-type \"LiteLLMModel\" --model-id \"gpt-4o\"\n\n```\n\n----------------------------------------\n\nTITLE: Running Phoenix collector\nDESCRIPTION: This shell command starts the Phoenix collector in the background, which is needed to receive and process the OpenTelemetry traces. This allows for log collection and inspection using Arize AI Phoenix.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/inspect_runs.mdx#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m phoenix.server.main serve\n```\n\n----------------------------------------\n\nTITLE: Initializing SmolagentsInstrumentor with Langfuse\nDESCRIPTION: This Python code initializes the `SmolagentsInstrumentor` to trace agent runs and export them to Langfuse. It configures the tracer provider and adds a span processor using `OTLPSpanExporter`, which utilizes the endpoint and headers set in environment variables. This establishes the connection to send traces to Langfuse.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/inspect_runs.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom openinference.instrumentation.smolagents import SmolagentsInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\ntrace_provider = TracerProvider()\ntrace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\n\nSmolagentsInstrumentor().instrument(tracer_provider=trace_provider)\n```\n\n----------------------------------------\n\nTITLE: Creating InferenceClientModel to Call Huggingface Inference API - Python\nDESCRIPTION: This snippet illustrates how to instantiate and use the InferenceClientModel from smolagents, which internally wraps Huggingface's InferenceClient for LLM inference via the Huggingface Inference API or other supported providers. The model is called with a message list containing user role and text content, and prints out the generated response string. It requires network access and proper API configuration as per Huggingface's API.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/reference/models.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\nmessages = [\n  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]}\n]\n\nmodel = InferenceClientModel()\nprint(model(messages))\n```\n\nLANGUAGE: text\nCODE:\n```\n>>> Of course! If you change your mind, feel free to reach out. Take care!\n```\n\n----------------------------------------\n\nTITLE: Initializing a CodeAgent with Step Callbacks (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a CodeAgent instance with specific tools, a model, authorized imports, and a list of step callback functions (such as update_screenshot). This setup allows dynamic, per-step updates to the agent's memory during interaction. Dependencies include smolagents, tool classes, and any callback-related libraries. Inputs are the components for CodeAgent, including tool instances and callbacks; outputs are a configurable agent instance ready for advanced memory management and vision-based actions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/memory.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nCodeAgent(\n    tools=[DuckDuckGoSearchTool(), go_back, close_popups, search_item_ctrl_f],\n    model=model,\n    additional_authorized_imports=[\"helium\"],\n    step_callbacks=[update_screenshot],\n    max_steps=20,\n    verbosity_level=2,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Tool Class for Hugging Face Model Downloads in Python\nDESCRIPTION: This snippet demonstrates how to create a custom tool class inheriting from `Tool` to retrieve the most downloaded model from Hugging Face Hub based on a specified task. It shows defining properties like name, description, inputs, output type, and implementing the `forward` method for inference.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/tools.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import Tool\n\nclass HFModelDownloadsTool(Tool):\n    name = \"model_download_counter\"\n    description = \"\"\"\\n    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n    It returns the name of the checkpoint.\"\"\"\n    inputs = {\n        \"task\": {\n            \"type\": \"string\",\n            \"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n        }\n    }\n    output_type = \"string\"\n\n    def forward(self, task: str):\n        from huggingface_hub import list_models\n\n        model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n        return model.id\n\nmodel_downloads_tool = HFModelDownloadsTool()\n```\n\n----------------------------------------\n\nTITLE: Using MLXModel for Running Huggingface Models Locally - Python\nDESCRIPTION: This snippet demonstrates how to use MLXModel from smolagents to interact with Huggingface models locally or through MLX's infrastructure. It shows model initialization with model_id and performing inference by passing a list of messages formatted by Smolagents. Requires installation of the mlx-lm package, which can be done via 'pip install smolagents[mlx-lm]'. It supports stop_sequences to control output generation.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/reference/models.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import MLXModel\n\nmodel = MLXModel(model_id=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n\nprint(model([{\"role\": \"user\", \"content\": \"Ok!\"}], stop_sequences=[\"great\"]))\n```\n\nLANGUAGE: text\nCODE:\n```\n>>> What a\n```\n\n----------------------------------------\n\nTITLE: Importing a Space as a Tool with Tool.from_space in Python\nDESCRIPTION: Provides an example of importing a Hugging Face Space as a tool using `Tool.from_space`, giving it a name and description. The tool can then be used to generate outputs, such as images, based on prompts.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/tools.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimage_generation_tool = Tool.from_space(\n    \"black-forest-labs/FLUX.1-schnell\",\n    name=\"image_generator\",\n    description=\"Generate an image from a prompt\"\n)\n\n# Example usage\nimage_generation_tool(\"A sunny beach\")\n```\n\n----------------------------------------\n\nTITLE: Example: Getting and Using Pope's Age via wiki and web_search in Python\nDESCRIPTION: This sequence uses two information tools: 'wiki' and 'web_search' to retrieve the current pope's age, printing results from each to allow comparison. It demonstrates using multiple data sources and persisting variables for subsequent use. Dependencies: wiki, web_search tools; no external variables required. Outputs: printed ages from Wikipedia and Google.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npope_age_wiki = wiki(query=\"current pope age\")\nprint(\"Pope age as per wikipedia:\", pope_age_wiki)\npope_age_search = web_search(query=\"current pope age\")\nprint(\"Pope age as per google search:\", pope_age_search)\n```\n\n----------------------------------------\n\nTITLE: Preparing Knowledge Base with LangChain\nDESCRIPTION: This Python code prepares a knowledge base from a Hugging Face dataset using LangChain.  It loads a dataset, filters it for `transformers` documentation, creates Document objects, and splits the documents into smaller chunks using RecursiveCharacterTextSplitter for efficient retrieval.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/rag.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.retrievers import BM25Retriever\n\nknowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\nknowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n\nsource_docs = [\n    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n    for doc in knowledge_base\n]\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=50,\n    add_start_index=True,\n    strip_whitespace=True,\n    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n)\ndocs_processed = text_splitter.split_documents(source_docs)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Table Schema Using SQLAlchemy (Python)\nDESCRIPTION: This snippet utilizes SQLAlchemy's inspect utility to extract the column names and types from the 'receipts' table and builds a formatted table description string. This description is used to inform the language model and/or agent system about table structure, supporting automatic code or query generation. SQLAlchemy must be connected to the target schema before running this.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninspector = inspect(engine)\ncolumns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(\"receipts\")]\n\ntable_description = \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\nprint(table_description)\n```\n\n----------------------------------------\n\nTITLE: Loading a Shared Tool from Hub with load_tool in Python\nDESCRIPTION: Demonstrates loading a previously shared tool from Hugging Face Hub using `load_tool`, with `trust_remote_code=True` to execute custom code from the repository. Suitable for integrating shared tools into an agent.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/tools.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import load_tool, CodeAgent\n\nmodel_download_tool = load_tool(\n    \"{your_username}/hf-model-downloads\",\n    trust_remote_code=True\n)\n```\n\n----------------------------------------\n\nTITLE: Running sample agent with Langfuse\nDESCRIPTION: This Python code shows how to execute a sample `smolagent` and view the generated traces in Langfuse.  The agent uses `CodeAgent`, `ToolCallingAgent`, and several tools. The execution will be traced and logged to Langfuse using the configuration set up in previous steps.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/inspect_runs.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import (\n    CodeAgent,\n    ToolCallingAgent,\n    DuckDuckGoSearchTool,\n    VisitWebpageTool,\n    InferenceClientModel,\n)\n\nmodel = InferenceClientModel(\n    model_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n)\n\nsearch_agent = ToolCallingAgent(\n    tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],\n    model=model,\n    name=\"search_agent\",\n    description=\"This is an agent that can do web search.\",\n)\n\nmanager_agent = CodeAgent(\n    tools=[],\n    model=model,\n    managed_agents=[search_agent],\n)\nmanager_agent.run(\n    \"How can Langfuse be used to monitor and improve the reasoning and decision-making of smolagents when they execute multi-step tasks, like dynamically adjusting a recipe based on user feedback or available ingredients?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Inspecting SQL Table Columns Using SQLAlchemy in Python\nDESCRIPTION: Utilizes SQLAlchemy's inspect functionality to retrieve column names and types from the 'receipts' table. This information is formatted as a descriptive string intended to be embedded into a tool's docstring to inform an LLM about the database schema. Requires a previously initialized SQLAlchemy engine connected to the database containing the 'receipts' table.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/text_to_sql.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninspector = inspect(engine)\ncolumns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(\"receipts\")]\n\ntable_description = \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\nprint(table_description)\n```\n\n----------------------------------------\n\nTITLE: Using ToolCollection to Load Multiple Tools from Hub in Python\nDESCRIPTION: Demonstrates fetching a collection of tools from a hub repository with `ToolCollection.from_hub`, then passing them to a `CodeAgent` to enable a broader set of functionalities. Useful for initializing agents with pre-defined tool sets.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/tools.mdx#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import ToolCollection, CodeAgent\n\nimage_tool_collection = ToolCollection.from_hub(\n    collection_slug=\"huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f\",\n    token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\"\n)\nagent = CodeAgent(tools=[*image_tool_collection.tools], model=model, add_base_tools=True)\n\nagent.run(\"Please draw me a picture of rivers and lakes.\")\n```\n\n----------------------------------------\n\nTITLE: Running a sample agent\nDESCRIPTION: This Python code defines and runs a sample smolagent.  It utilizes `CodeAgent`, `ToolCallingAgent`, and several tools, and a model to perform a web search. The agent's execution will be traced and logged to Phoenix (or a configured OpenTelemetry backend).\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/inspect_runs.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import (\n    CodeAgent,\n    ToolCallingAgent,\n    DuckDuckGoSearchTool,\n    VisitWebpageTool,\n    InferenceClientModel,\n)\n\nmodel = InferenceClientModel()\n\nsearch_agent = ToolCallingAgent(\n    tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],\n    model=model,\n    name=\"search_agent\",\n    description=\"This is an agent that can do web search.\",\n)\n\nmanager_agent = CodeAgent(\n    tools=[],\n    model=model,\n    managed_agents=[search_agent],\n)\nmanager_agent.run(\n    \"If the US keeps its 2024 growth rate, how many years will it take for the GDP to double?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Using DuckDuckGoSearchTool for Web Search in Python\nDESCRIPTION: This code snippet shows how to manually use the DuckDuckGoSearchTool, part of the smolagents toolkit, to perform a web search and return immediate results. It requires importing the tool, instantiating it, and calling it with a string query. It demonstrates the standalone execution of tools outside an agent context.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import DuckDuckGoSearchTool\n\nsearch_tool = DuckDuckGoSearchTool()\nprint(search_tool(\"Who's the current president of Russia?\"))\n```\n\n----------------------------------------\n\nTITLE: Adding or Replacing Tools in an Existing Agent in Python\nDESCRIPTION: Shows how to add a custom tool to an existing agent's tools dictionary, allowing dynamic extension of the agent's capabilities after initialization.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/tutorials/tools.mdx#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom smolagents import InferenceClientModel\n\nmodel = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n\nagent = CodeAgent(tools=[], model=model, add_base_tools=True)\nagent.tools[model_download_tool.name] = model_download_tool\n```\n\n----------------------------------------\n\nTITLE: Testing the visit_webpage Tool by Printing Partial Markdown Output in Python\nDESCRIPTION: Calls the 'visit_webpage' tool with a Wikipedia URL and prints the first 500 characters of the resulting Markdown string. This serves to verify the tool's successful fetching and conversion behavior. Input: URL as a string to a real web page. Output: Printed (partial) Markdown content.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/multiagents.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nprint(visit_webpage(\"https://en.wikipedia.org/wiki/Hugging_Face\")[:500])\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for SmolAgents Web System\nDESCRIPTION: Installs required Python packages for building and running the multi-agent web search system using pip. Essential dependencies include markdownify for HTML-to-Markdown conversion, duckduckgo-search for performing web searches via DuckDuckGo, and smolagents for agent orchestration. This command should be run in a Unix-like shell environment and may require administrative permissions.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/multiagents.mdx#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n!pip install markdownify duckduckgo-search smolagents --upgrade -q\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Agentic RAG System in Bash\nDESCRIPTION: Installs required Python packages including smolagents for agent construction, pandas for data processing, langchain and langchain-community for vector database and retrieval utilities, sentence-transformers for text embeddings, and rank_bm25 for classic BM25 retrieval. This is a prerequisite step for setting up the RAG environment.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/rag.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install smolagents pandas langchain langchain-community sentence-transformers rank_bm25 --upgrade -q\n```\n\n----------------------------------------\n\nTITLE: Performing Web Search with Multiple Queries in CodeAgent Action (Python)\nDESCRIPTION: Provides an example Python code snippet, as might be generated and executed by a CodeAgent, which loops over several search queries and prints the results from a web_search tool. Demonstrates the agent's direct code-based action workflow. Assumes the existence of a web_search callable in the agent's environment.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/README.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nrequests_to_search = [\"gulf of mexico america\", \"greenland denmark\", \"tariffs\"]\nfor request in requests_to_search:\n    print(f\"Here are the search results for {request}:\", web_search(request))\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool by Subclassing Tool in Python\nDESCRIPTION: Implementation of a custom tool by subclassing the Tool class to retrieve the most downloaded model for a specified task. Includes explicit definition of name, description, input types, and output type.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/guided_tour.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import Tool\n\nclass ModelDownloadTool(Tool):\n    name = \"model_download_tool\"\n    description = \"This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub. It returns the name of the checkpoint.\"\n    inputs = {\"task\": {\"type\": \"string\", \"description\": \"The task for which to get the download count.\"}}\n    output_type = \"string\"\n\n    def forward(self, task: str) -> str:\n        most_downloaded_model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n        return most_downloaded_model.id\n```\n\n----------------------------------------\n\nTITLE: Running Open Deep Research Agent Script with Model and Query Arguments\nDESCRIPTION: This snippet runs the main Python script `run.py` with command-line arguments specifying the model identifier and a query string. It demonstrates how to interact with the agent by passing a question for it to process. The environment variables for API keys must be set beforehand to enable full functionality.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run.py --model-id \"o1\" \"Your question here!\"\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face Hub via Python\nDESCRIPTION: Authenticates the current Python session with the Hugging Face Hub to enable API access for fetching models via the Inference API. The 'login' function prompts for user credentials or uses an authentication token. This is necessary before model inference or downloading gated models.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/multiagents.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import login\n\nlogin()\n```\n\n----------------------------------------\n\nTITLE: Installing Web Automation Dependencies using Bash\nDESCRIPTION: This command installs the necessary Python libraries for the web automation project: `smolagents` for the agent framework, `selenium` for browser automation, `helium` for a higher-level browser automation API, and `pillow` for image handling (screenshots). The `-q` flag ensures a quiet installation with less output.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/web_browser.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install smolagents selenium helium pillow -q\n```\n\n----------------------------------------\n\nTITLE: Agent Executing Image Generation Code (Text)\nDESCRIPTION: Shows the output of the agent's thought process and the code it executes when using the imported image generation tool. The agent first improves the user's prompt and then calls the `image_generator` tool with the refined prompt to produce an image.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/tutorials/tools.mdx#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n=== Agent thoughts:\nimproved_prompt could be \"A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background\"\n\nNow that I have improved the prompt, I can use the image generator tool to generate an image based on this prompt.\n>>> Agent is executing the code below:\nimage = image_generator(prompt=\"A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background\")\nfinal_answer(image)\n```\n\n----------------------------------------\n\nTITLE: Pushing a Tool to Hugging Face Hub\nDESCRIPTION: This snippet demonstrates how to push a custom tool to the Hugging Face Hub using the `push_to_hub` method. It requires a Hugging Face Hub repository and a token with write access. Ensure that all imports are within the tool's functions for successful saving and pushing.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/tools.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_downloads_tool.push_to_hub(\"{your_username}/hf-model-downloads\", token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n```\n\n----------------------------------------\n\nTITLE: Cloning Open Deep Research Repository Using Bash\nDESCRIPTION: This snippet demonstrates cloning the smolagents GitHub repository and changing into the example directory for the Open Deep Research agent. It uses the standard git command line tool and basic shell commands. This is a prerequisite for accessing the project files locally.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/smolagents.git\ncd smolagents/examples/open_deep_research\n```\n\n----------------------------------------\n\nTITLE: Adding Additional Authorized Imports to CodeAgent Python Execution Environment\nDESCRIPTION: This example shows how to initialize a CodeAgent in Python that allows importing additional safe external modules during code execution generated by the LLM. The 'additional_authorized_imports' parameter is specified as a list of module names (e.g., 'requests', 'bs4'), enabling their use safely within the executed code. This enhances agent capability while maintaining controlled execution. The base model used here is an InferenceClientModel without explicit parameters.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/guided_tour.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import CodeAgent\n\nmodel = InferenceClientModel()\nagent = CodeAgent(tools=[], model=model, additional_authorized_imports=['requests', 'bs4'])\nagent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\" )\n```\n\n----------------------------------------\n\nTITLE: CodeAgent Class Overview\nDESCRIPTION: The `CodeAgent` class inherits from `MultiStepAgent` and is designed to generate tool call scripts in Python code. It is useful for automating code-based interactions with tools within the agent framework, requiring dependencies like Python and relevant libraries.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/agents.mdx#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with pip\nDESCRIPTION: This snippet installs all required Python dependencies listed in the requirements.txt file using pip. It ensures that the environment contains necessary packages for running the Open Deep Research agent. This command requires Python and pip to be already installed on the system.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Development Version of smolagents Package Using pip\nDESCRIPTION: This snippet installs the smolagents package in editable mode along with its development dependencies. It allows for local changes to the package code to be immediately reflected without reinstalling, facilitating development and testing.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e ../../.[dev]\n```\n\n----------------------------------------\n\nTITLE: Running Local Tests for smolagents (Shell)\nDESCRIPTION: Executes the `test` target defined in the project's Makefile to run the project's test suite locally. This command verifies that code changes do not break existing functionality and pass all tests.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Applying Code Formatting for smolagents (Shell)\nDESCRIPTION: Executes the `style` target defined in the project's Makefile to automatically format the codebase according to project style guidelines. This command is typically run if the `make quality` checks fail due to formatting issues.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nmake style\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies - Bash\nDESCRIPTION: This snippet installs the required Python packages using pip. It specifies smolagents, selenium, helium, and pillow as dependencies, which are necessary for browser automation and image handling.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/zh/examples/web_browser.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install smolagents selenium helium pillow -q\n```\n\n----------------------------------------\n\nTITLE: Example: Comparing City Populations via search Tool in Python\nDESCRIPTION: This snippet queries the populations of specified cities with the 'search' tool in a for-loop, printing the results for each city. The pattern shows correct use of tool-call separation and print for capturing intermediate data. Dependency: working 'search' tool. Inputs: list of city names. Outputs: printed results for each city's population, then a final answer based on the comparison.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor city in [\"Guangzhou\", \"Shanghai\"]:\n    print(f\"Population {city}:\", search(f\"{city} population\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfinal_answer(\"Shanghai\")\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running Browser-Use Agent Evaluation (Python)\nDESCRIPTION: Imports necessary modules, including `asyncio`, `nest_asyncio` (applied to handle nested event loops often found in notebooks), `Agent` from `browser_use`, `load_dotenv`, and `ChatOpenAI` from `langchain_openai`. Defines a wrapper class `BrowserUseAgent` that uses `browser_use.Agent` internally (configured with GPT-4o via Langchain) and adapts its execution (`run` method) and result retrieval for the `answer_questions` function. Initializes this wrapper agent and runs the evaluation on `eval_ds`, saving results as 'gpt-4o_27-01_browseruse' and setting `run_simple=True` for the evaluation function.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nimport nest_asyncio\n\n\nnest_asyncio.apply()\n\nfrom browser_use import Agent\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\n\n\nload_dotenv()\n\n\nclass BrowserUseAgent:\n    logs = []\n\n    def write_inner_memory_from_logs(self, summary_mode):\n        return self.results\n\n    def run(self, task, **kwargs):\n        agent = Agent(\n            task=task,\n            llm=ChatOpenAI(model=\"gpt-4o\"),\n        )\n        self.results = asyncio.get_event_loop().run_until_complete(agent.run())\n        return self.results.history[-1].result[0].extracted_content\n\n\nbrowser_use_agent = BrowserUseAgent()\n\nresults_browseruse = answer_questions(\n    eval_ds,\n    browser_use_agent,\n    \"gpt-4o_27-01_browseruse\",\n    reformulation_model=proprietary_model,\n    output_folder=\"output_browsers\",\n    visual_inspection_tool=VisualQAGPT4Tool(),\n    text_inspector_tool=TextInspectorTool(proprietary_model, 40000),\n    postprompt=\"\",\n    run_simple=True,\n)\n```\n\n----------------------------------------\n\nTITLE: ToolCallingAgent Class Overview\nDESCRIPTION: The `ToolCallingAgent` class, also derived from `MultiStepAgent`, outputs tool calls in JSON format. This class is suitable for applications requiring structured data exchange and integration with external systems that process JSON commands, with dependencies on JSON libraries.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/reference/agents.mdx#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Initializing SQLite In-Memory Database and Defining Tables in Python\nDESCRIPTION: Sets up an in-memory SQLite database using SQLAlchemy, defines a 'receipts' table with columns for receipt ID, customer name, price, and tip, and inserts sample data into this table. Requires SQLAlchemy installed. The snippet is foundational for performing SQL queries later using the agent system.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/examples/text_to_sql.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy import (create_engine, MetaData, Table, Column, String, Integer, Float, insert, inspect, text)\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData()\n\n# create city SQL table\ntable_name = \"receipts\"\nreceipts = Table(\n    table_name,\n    metadata_obj,\n    Column(\"receipt_id\", Integer, primary_key=True),\n    Column(\"customer_name\", String(16), primary_key=True),\n    Column(\"price\", Float),\n    Column(\"tip\", Float),\n)\nmetadata_obj.create_all(engine)\n\nrows = [\n    {\"receipt_id\": 1, \"customer_name\": \"Alan Payne\", \"price\": 12.06, \"tip\": 1.20},\n    {\"receipt_id\": 2, \"customer_name\": \"Alex Mason\", \"price\": 23.86, \"tip\": 0.24},\n    {\"receipt_id\": 3, \"customer_name\": \"Woodrow Wilson\", \"price\": 53.43, \"tip\": 5.43},\n    {\"receipt_id\": 4, \"customer_name\": \"Margaret James\", \"price\": 21.11, \"tip\": 1.00},\n]\nfor row in rows:\n    stmt = insert(receipts).values(**row)\n    with engine.begin() as connection:\n        cursor = connection.execute(stmt)\n```\n\n----------------------------------------\n\nTITLE: Setting up In-Memory SQLite Tables and Data with SQLAlchemy in Python\nDESCRIPTION: This snippet creates an in-memory SQLite database and defines a 'receipts' table using SQLAlchemy's ORM features. It includes helper function insert_rows_into_table for populating tables with dictionaries of row data using the insert construct. The schema includes receipt_id, customer_name, price, and tip fields. The metadata_obj.create_all(engine) call ensures the schema is created. The initialized schema and helper facilitate further interaction, queries, and extensions (such as joins). SQLAlchemy is required for this setup.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/text_to_sql.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy import (\n    create_engine,\n    MetaData,\n    Table,\n    Column,\n    String,\n    Integer,\n    Float,\n    insert,\n    inspect,\n    text,\n)\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData()\n\ndef insert_rows_into_table(rows, table, engine=engine):\n    for row in rows:\n        stmt = insert(table).values(**row)\n        with engine.begin() as connection:\n            connection.execute(stmt)\n\ntable_name = \"receipts\"\nreceipts = Table(\n    table_name,\n    metadata_obj,\n    Column(\"receipt_id\", Integer, primary_key=True),\n    Column(\"customer_name\", String(16), primary_key=True),\n    Column(\"price\", Float),\n    Column(\"tip\", Float),\n)\nmetadata_obj.create_all(engine)\n\nrows = [\n    {\"receipt_id\": 1, \"customer_name\": \"Alan Payne\", \"price\": 12.06, \"tip\": 1.20},\n    {\"receipt_id\": 2, \"customer_name\": \"Alex Mason\", \"price\": 23.86, \"tip\": 0.24},\n    {\"receipt_id\": 3, \"customer_name\": \"Woodrow Wilson\", \"price\": 53.43, \"tip\": 5.43},\n    {\"receipt_id\": 4, \"customer_name\": \"Margaret James\", \"price\": 21.11, \"tip\": 1.00},\n]\ninsert_rows_into_table(rows, receipts)\n```\n\n----------------------------------------\n\nTITLE: Example: Computing Arithmetic Result Using Python in CodeAgent\nDESCRIPTION: This code example solves a given arithmetic question by computing the result with basic Python, then reporting it via final_answer. The snippet highlights that the agent can use core Python operations for calculation. Dependency: functional 'final_answer' tool. Inputs: no external inputs required. Output: numerical result of the calculation passed to final_answer.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresult = 5 + 3 + 1294.678\nfinal_answer(result)\n```\n\n----------------------------------------\n\nTITLE: Printing Default System Prompt for CodeAgent in Python\nDESCRIPTION: This snippet retrieves and prints the default system prompt template from the agent's prompt_templates dictionary for the 'system_prompt' key. It requires a pre-instantiated agent object (presumably a CodeAgent). The output helps users inspect or debug the current prompt template for customization or verification. No external dependencies are needed except for the agent instance setup. Expected input: an agent object. Output: contents of the prompt template as a string.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprint(agent.prompt_templates[\"system_prompt\"])\n```\n\n----------------------------------------\n\nTITLE: Creating an Interactive Gradio UI for Agent Visualization\nDESCRIPTION: Code for creating an interactive Gradio interface to visualize agent's thinking and execution process.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/guided_tour.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom smolagents import (\n    load_tool,\n    CodeAgent,\n    InferenceClientModel,\n    GradioUI\n)\n\n# Import tool from Hub\nimage_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True)\n\nmodel = InferenceClientModel(model_id=model_id)\n\n# Initialize the agent with the image generation tool\nagent = CodeAgent(tools=[image_generation_tool], model=model)\n\nGradioUI(agent).launch()\n```\n\n----------------------------------------\n\nTITLE: Example: Exponentiating Pope's Age Calculation in Python\nDESCRIPTION: After retrieving the pope's age, this code snippet raises it to the power of 0.36 and reports the result via 'final_answer'. It highlights basic Python math operations and correct agent output reporting. Dependency: 'final_answer' tool. Inputs: numeric pope age (in this case, 88). Output: exponentiation result.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/hi/tutorials/building_good_agents.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npope_current_age = 88 ** 0.36\nfinal_answer(pope_current_age)\n```\n\n----------------------------------------\n\nTITLE: Displaying Incorrectly Answered Questions by Text Agent (Python)\nDESCRIPTION: Filters the `results_text` DataFrame using boolean indexing (`.loc`) with the negation operator (`~`) to select and display only the rows where the 'is_correct' column is False. This highlights the specific questions from the evaluation set that the text-based browser agent failed to answer correctly.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfalse_text_results = results_text.loc[~results_text[\"is_correct\"]]\nfalse_text_results\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Agentic RAG (Bash)\nDESCRIPTION: This bash command installs the necessary Python libraries required to build and run the agentic RAG system. It includes libraries like `smolagents`, `pandas`, `langchain`, `langchain-community`, `sentence-transformers`, `datasets`, `python-dotenv`, and `rank_bm25`. The `--upgrade` flag ensures the latest versions are installed, and `-q` keeps the output quiet.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/source/en/examples/rag.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install smolagents pandas langchain langchain-community sentence-transformers datasets python-dotenv rank_bm25 --upgrade -q\n```\n\n----------------------------------------\n\nTITLE: Installing Browser-Use and Playwright Dependencies (Bash)\nDESCRIPTION: Installs the `browser-use` and `lxml_html_clean` Python libraries using pip in quiet mode (`-q`). It then executes the `playwright install` command to download and set up the necessary browser binaries required by Playwright, which is used by the `browser-use` library for browser automation.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n!pip install browser-use lxml_html_clean -q\n!playwright install\n```\n\n----------------------------------------\n\nTITLE: Installing Smolagents and Documentation Dependencies - Bash\nDESCRIPTION: Installs the HuggingFace Smolagents package in editable mode along with two additional required dependencies: 'doc-builder' (from the HuggingFace Doc-Builder repository) for building the documentation and 'watchdog' for enabling live reload during documentation editing. The commands should be run at the root of the repository. Internet access and pip must be available.\nSOURCE: https://github.com/huggingface/smolagents/blob/main/docs/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/doc-builder@main\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install watchdog\n```"
  }
]
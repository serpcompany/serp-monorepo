[
  {
    "owner": "autoscrape-labs",
    "repo": "pydoll",
    "content": "TITLE: Basic Browser Automation with Pydoll using Context Manager\nDESCRIPTION: A simple example demonstrating how to use Pydoll to open a webpage, locate an element, and interact with it using the context manager pattern for automatic resource cleanup.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/index.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport time\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://github.com/autoscrape-labs/pydoll')\n        star_button = await page.wait_element(\n            By.XPATH, '//form[@action=\"/autoscrape-labs/pydoll/star\"]//button',\n            timeout=5,\n            raise_exc=False\n        )\n        if not star_button:\n            print(\"Ops! The button was not found.\")\n            return\n\n        await star_button.click()\n        await asyncio.sleep(3)\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Basic Pydoll Usage with Chrome Browser\nDESCRIPTION: Demonstrates how to use Pydoll to automate Chrome browser interactions asynchronously. This example shows browser initialization, navigation to a website with Cloudflare protection, and basic element interaction.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Works with captcha-protected sites\n        await page.go_to('https://example-with-cloudflare.com')\n        button = await page.find_element(By.CSS_SELECTOR, 'button')\n        await button.click()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Bypassing Cloudflare Captcha Using Context Manager in Python\nDESCRIPTION: This example demonstrates how to bypass Cloudflare Turnstile captchas using Pydoll's context manager approach. The code creates a Chrome browser instance, navigates to a protected site, and uses the expect_and_bypass_cloudflare_captcha context manager to handle captcha challenges automatically.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/features.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser import Chrome\n\nasync def bypass_cloudflare_example():\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n    \n    # The context manager will wait for the captcha to be processed\n    # before continuing execution\n    async with page.expect_and_bypass_cloudflare_captcha():\n        await page.go_to('https://site-with-cloudflare.com')\n        print(\"Waiting for captcha to be handled...\")\n    \n    # This code runs only after the captcha is successfully bypassed\n    print(\"Captcha bypassed! Continuing with automation...\")\n    await page.find_element_by_id('protected-content').get_text()\n    \n    await browser.stop()\n\nasyncio.run(bypass_cloudflare_example())\n```\n\n----------------------------------------\n\nTITLE: Implementing Request Interception with Pydoll\nDESCRIPTION: This example demonstrates how to intercept and modify network requests before they're sent to the server. It shows how to enable fetch events, register an interception handler that modifies API requests by adding custom headers, and how to continue the requests with the modified data.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/features.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.fetch import FetchEvents\nfrom pydoll.commands.fetch import FetchCommands\nfrom functools import partial\n\nasync def request_interception_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Define the request interceptor\n        async def intercept_request(page, event):\n            request_id = event['params']['requestId']\n            url = event['params']['request']['url']\n            \n            if '/api/' in url:\n                # Get original headers\n                original_headers = event['params']['request'].get('headers', {})\n                \n                # Add custom headers\n                custom_headers = {\n                    **original_headers,\n                    'Authorization': 'Bearer my-token-123',\n                    'X-Custom-Header': 'CustomValue'\n                }\n                \n                print(f\"ðŸ”„ Modifying request to: {url}\")\n                await page._execute_command(\n                    FetchCommands.continue_request(\n                        request_id=request_id,\n                        headers=custom_headers\n                    )\n                )\n            else:\n                # Continue normally for non-API requests\n                await page._execute_command(\n                    FetchCommands.continue_request(\n                        request_id=request_id\n                    )\n                )\n        \n        # Enable interception and register handler\n        await page.enable_fetch_events()\n        await page.on(FetchEvents.REQUEST_PAUSED, \n                      partial(intercept_request, page))\n        \n        # Navigate to trigger requests\n        await page.go_to('https://example.com')\n        await asyncio.sleep(5)  # Allow time for requests to process\n\nasyncio.run(request_interception_example())\n```\n\n----------------------------------------\n\nTITLE: Concurrent Web Scraping with Pydoll in Python\nDESCRIPTION: This example demonstrates Pydoll's concurrent scraping capabilities using asyncio. The code creates multiple browser instances to process several URLs simultaneously, extracting page titles and content elements. This approach significantly reduces execution time compared to sequential scraping.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/features.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def scrape_page(url):\n    \"\"\"Process a single page and extract data\"\"\"\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to(url)\n        \n        # Extract data\n        title = await page.execute_script('return document.title')\n        \n        # Find elements and extract content\n        elements = await page.find_elements(By.CSS_SELECTOR, '.article-content')\n        content = []\n        for element in elements:\n            text = await element.get_element_text()\n            content.append(text)\n            \n        return {\n            \"url\": url,\n            \"title\": title,\n            \"content\": content\n        }\n\nasync def main():\n    # List of URLs to scrape in parallel\n    urls = [\n        'https://example.com/page1',\n        'https://example.com/page2',\n        'https://example.com/page3',\n        'https://example.com/page4',\n        'https://example.com/page5',\n    ]\n    \n    # Process all URLs concurrently\n    results = await asyncio.gather(*(scrape_page(url) for url in urls))\n    \n    # Print results\n    for result in results:\n        print(f\"Scraped {result['url']}: {result['title']}\")\n        print(f\"Found {len(result['content'])} content blocks\")\n    \n    return results\n\n# Run the concurrent scraping\nall_data = asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Interception with PyDoll in Python\nDESCRIPTION: This example demonstrates how to enable request interception using PyDoll's Fetch domain. It sets up a Chrome browser instance, registers an interceptor function, and shows the basic pattern of intercepting a request and then continuing it. This pattern is essential for inspecting network traffic without breaking page functionality.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.fetch import FetchEvents\nfrom pydoll.commands.fetch import FetchCommands\nfrom functools import partial\n\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Define a request interceptor\n        async def intercept_request(page, event):\n            request_id = event['params']['requestId']\n            request = event['params']['request']\n            url = request['url']\n            \n            print(f\"Intercepted request to: {url}\")\n            \n            # You must continue the request to proceed\n            await page._execute_command(\n                FetchCommands.continue_request(request_id)\n            )\n        \n        # Enable fetch events and register the interceptor\n        await page.enable_fetch_events()\n        await page.on(\n            FetchEvents.REQUEST_PAUSED, \n            partial(intercept_request, page)\n        )\n        \n        # Navigate to a page\n        await page.go_to('https://example.com')\n        \nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Event-Driven Scraping Implementation\nDESCRIPTION: Comprehensive example of implementing an event-driven scraper that responds to page changes in real-time, including AJAX response handling and infinite scroll management.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom functools import partial\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.events import NetworkEvents, PageEvents\n\nasync def scrape_dynamic_content():\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n    \n    # Create a data storage container\n    scraped_data = []\n    data_complete = asyncio.Event()\n    \n    # Set up a callback to extract data when AJAX responses are received\n    async def extract_data_from_response(page, event):\n        if 'api/products' in event['params']['response']['url']:\n            # Extract the response body\n            request_id = event['params']['requestId']\n            body, is_base64 = await page.get_network_response_body(request_id)\n            \n            # Process the data\n            products = json.loads(body)\n            for product in products:\n                scraped_data.append({\n                    'id': product['id'],\n                    'name': product['name'],\n                    'price': product['price']\n                })\n            \n            print(f\"Extracted {len(products)} products\")\n            \n            # If we've collected enough data, signal completion\n            if len(scraped_data) >= 100:\n                data_complete.set()\n    \n    # Set up navigation monitoring\n    async def handle_page_load(page, event):\n        print(f\"Page loaded: {await page.current_url}\")\n        \n        # Now that the page is loaded, trigger the infinite scroll\n        await page.execute_script(\"\"\"\n            function scrollDown() {\n                window.scrollTo(0, document.body.scrollHeight);\n                setTimeout(scrollDown, 1000);\n            }\n            scrollDown();\n        \"\"\")\n    \n    # Enable events and register callbacks\n    await page.enable_network_events()\n    await page.enable_page_events()\n    await page.on(NetworkEvents.RESPONSE_RECEIVED, partial(extract_data_from_response, page))\n    await page.on(PageEvents.PAGE_LOADED, partial(handle_page_load, page))\n    \n    # Navigate to the page with dynamic content\n    await page.go_to(\"https://example.com/products\")\n    \n    # Wait for data collection to complete or timeout after 60 seconds\n    try:\n        await asyncio.wait_for(data_complete.wait(), timeout=60)\n    except asyncio.TimeoutError:\n        print(\"Timeout reached, continuing with data collected so far\")\n    \n    # Process the collected data\n    print(f\"Total products collected: {len(scraped_data)}\")\n    \n    # Clean up\n    await browser.stop()\n    \n    return scraped_data\n```\n\n----------------------------------------\n\nTITLE: Element Interaction Chain Pattern in Python\nDESCRIPTION: A condensed example illustrating the typical flow pattern for element interaction in Pydoll: finding an element, interacting with it, and then reacting to the result through waiting for another element.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Find an element\nbutton = await page.find_element(By.ID, 'submit-button')\n\n# Interact with the element\nawait button.click()\n\n# React to the result\nconfirmation = await page.wait_element(By.CLASS_NAME, 'success-message')\n```\n\n----------------------------------------\n\nTITLE: Element Interaction Workflow with Google Search Example\nDESCRIPTION: A comprehensive example showing how to find elements, type into input fields, click buttons, wait for elements to appear, and extract text from search results using the Page domain's element interaction methods.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Navigate to a search engine\nawait page.go_to(\"https://www.google.com\")\n\n# Find the search input field\nsearch_box = await page.find_element(By.NAME, \"q\")\n\n# Type into the search box\nawait search_box.type_keys(\"Pydoll browser automation\")\n\n# Find and click the search button\nsearch_button = await page.find_element(By.NAME, \"btnK\")\nawait search_button.click()\n\n# Wait for results to load\nresults = await page.wait_element(By.CSS_SELECTOR, \"#search\")\n\n# Find all result links\nlinks = await page.find_elements(By.CSS_SELECTOR, \"a h3\")\n\n# Print the first 3 result titles\nfor i, link in enumerate(links[:3]):\n    text = await link.text\n    print(f\"Result {i+1}: {text}\")\n```\n\n----------------------------------------\n\nTITLE: Coordinating Network Idle State with PyDoll Events\nDESCRIPTION: This code snippet demonstrates how to use PyDoll events to coordinate actions based on network activity. It tracks in-flight requests and responses to determine when the network is idle. This is useful for ensuring all data has been loaded before proceeding with scraping or other actions.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nasync def wait_for_network_idle():\n    network_idle = asyncio.Event()\n    in_flight_requests = 0\n    \n    async def track_request(event):\n        nonlocal in_flight_requests\n        in_flight_requests += 1\n    \n    async def track_response(event):\n        nonlocal in_flight_requests\n        in_flight_requests -= 1\n        if in_flight_requests == 0:\n            network_idle.set()\n    \n    await page.enable_network_events()\n    await page.on(NetworkEvents.REQUEST_WILL_BE_SENT, track_request)\n    await page.on(NetworkEvents.LOADING_FINISHED, track_response)\n    await page.on(NetworkEvents.LOADING_FAILED, track_response)\n    \n    await network_idle.wait()\n    \n    # Clean up\n    await page.disable_network_events()\n```\n\n----------------------------------------\n\nTITLE: Basic Browser Usage Example with Async/Await\nDESCRIPTION: Demonstrates basic browser initialization, page navigation, and cleanup using async/await patterns.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\n\nasync def simple_browser_example():\n    # Create and start a browser instance\n    browser = Chrome()\n    await browser.start()\n    \n    try:\n        # Get a page and navigate to a URL\n        page = await browser.get_page()\n        await page.go_to(\"https://example.com\")\n        \n        # Perform operations with the page\n        title = await page.execute_script(\"return document.title\")\n        print(f\"Page title: {title}\")\n        \n    finally:\n        # Always ensure the browser is properly closed\n        await browser.stop()\n\n# Run the async example\nasyncio.run(simple_browser_example())\n```\n\n----------------------------------------\n\nTITLE: Creating Real-time Network Activity Dashboard with Pydoll\nDESCRIPTION: Implements a real-time network monitoring dashboard that tracks request statistics, response types, and domain analytics. Uses Chrome DevTools Protocol through Pydoll to monitor network events and display live statistics.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport time\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def main():\n    # Statistics counters\n    stats = {\n        'total_requests': 0,\n        'completed_requests': 0,\n        'failed_requests': 0,\n        'bytes_received': 0,\n        'request_types': {},\n        'status_codes': {},\n        'domains': {},\n        'start_time': time.time()\n    }\n    \n    async def update_dashboard():\n        while True:\n            # Calculate elapsed time\n            elapsed = time.time() - stats['start_time']\n            \n            # Clear console and print stats\n            print(\"\\033c\", end=\"\")  # Clear console\n            print(f\"Network Activity Dashboard - Running for {elapsed:.1f}s\")\n            print(f\"Total Requests: {stats['total_requests']}\")\n            print(f\"Completed: {stats['completed_requests']} | Failed: {stats['failed_requests']}\")\n            print(f\"Data Received: {stats['bytes_received'] / 1024:.1f} KB\")\n            \n            print(\"\\nRequest Types:\")\n            for rtype, count in sorted(stats['request_types'].items(), key=lambda x: x[1], reverse=True):\n                print(f\"  {rtype}: {count}\")\n            \n            print(\"\\nStatus Codes:\")\n            for code, count in sorted(stats['status_codes'].items()):\n                print(f\"  {code}: {count}\")\n            \n            print(\"\\nTop Domains:\")\n            top_domains = sorted(stats['domains'].items(), key=lambda x: x[1], reverse=True)[:5]\n            for domain, count in top_domains:\n                print(f\"  {domain}: {count}\")\n            \n            await asyncio.sleep(1)\n    \n    # Start the dashboard updater task\n    dashboard_task = asyncio.create_task(update_dashboard())\n    \n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Track request starts\n        async def on_request_sent(page, event):\n            stats['total_requests'] += 1\n            \n            # Track request type\n            resource_type = event['params'].get('type', 'Other')\n            stats['request_types'][resource_type] = stats['request_types'].get(resource_type, 0) + 1\n            \n            # Track domain\n            url = event['params']['request']['url']\n            try:\n                from urllib.parse import urlparse\n                domain = urlparse(url).netloc\n                stats['domains'][domain] = stats['domains'].get(domain, 0) + 1\n            except:\n                pass\n        \n        # Track responses\n        async def on_response(page, event):\n            status = event['params']['response']['status']\n            stats['status_codes'][status] = stats['status_codes'].get(status, 0) + 1\n        \n        # Track request completions\n        async def on_loading_finished(page, event):\n            stats['completed_requests'] += 1\n            if 'encodedDataLength' in event['params']:\n                stats['bytes_received'] += event['params']['encodedDataLength']\n        \n        # Track failures\n        async def on_loading_failed(page, event):\n            stats['failed_requests'] += 1\n        \n        # Register callbacks\n        await page.enable_network_events()\n        await page.on(NetworkEvents.REQUEST_WILL_BE_SENT, partial(on_request_sent, page))\n        await page.on(NetworkEvents.RESPONSE_RECEIVED, partial(on_response, page))\n        await page.on(NetworkEvents.LOADING_FINISHED, partial(on_loading_finished, page))\n        await page.on(NetworkEvents.LOADING_FAILED, partial(on_loading_failed, page))\n        \n        # Navigate to a page with lots of requests\n        await page.go_to('https://news.ycombinator.com')\n        \n        # Wait for user to press Enter to exit\n        await asyncio.sleep(60)\n    \n    # Clean up\n    dashboard_task.cancel()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Concurrent Page Data Fetching with Async/Await in Python\nDESCRIPTION: Demonstrates how to fetch data from multiple web pages concurrently using Python's asyncio framework and Pydoll's Chrome automation capabilities. Shows the implementation of async functions for browser control and parallel execution.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\n\nasync def fetch_page_data(url):\n    print(f\"Starting fetch for {url}\")\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n    \n    # Navigation takes time - this is where we yield control\n    await page.go_to(url)\n    \n    # Get page title\n    title = await page.execute_script(\"return document.title\")\n    \n    # Extract some data\n    description = await page.execute_script(\n        \"return document.querySelector('meta[name=\\\"description\\\"]')?.content || ''\"\n    )\n    \n    await browser.stop()\n    print(f\"Completed fetch for {url}\")\n    return {\"url\": url, \"title\": title, \"description\": description}\n\nasync def main():\n    # Start two page operations concurrently\n    task1 = asyncio.create_task(fetch_page_data(\"https://example.com\"))\n    task2 = asyncio.create_task(fetch_page_data(\"https://github.com\"))\n    \n    # Wait for both to complete and get results\n    result1 = await task1\n    result2 = await task2\n    \n    return [result1, result2]\n\n# Run the async function\nresults = asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Advanced Browser Automation with Custom Chrome Options\nDESCRIPTION: Extended example showing how to customize Chrome browser behavior using the Options class, configure headless mode, set binary location, take screenshots, and extract text from elements.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/index.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport os\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.browser.options import Options\nfrom pydoll.constants import By\n\nasync def main():\n    options = Options()\n    options.binary_location = '/usr/bin/google-chrome-stable'\n    options.add_argument('--headless=new')\n    options.add_argument('--start-maximized')\n    options.add_argument('--disable-notifications')\n    \n    async with Chrome(options=options) as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://github.com/autoscrape-labs/pydoll')\n        star_button = await page.wait_element(\n            By.XPATH, '//form[@action=\"/autoscrape-labs/pydoll/star\"]//button',\n            timeout=5,\n            raise_exc=False\n        )\n        if not star_button:\n            print(\"Ops! The button was not found.\")\n            return\n\n        await star_button.click()\n        await asyncio.sleep(3)\n\n        screenshot_path = os.path.join(os.getcwd(), 'pydoll_repo.png')\n        await page.get_screenshot(screenshot_path)\n        print(f\"Screenshot saved to: {screenshot_path}\")\n\n        repo_description_element = await page.find_element(\n            By.CLASS_NAME, 'f4.my-3'\n        )\n        repo_description = await repo_description_element.text\n        print(f\"Repository description: {repo_description}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: API Traffic Analysis with Pydoll\nDESCRIPTION: Implements detailed API traffic analysis including request tracking, response monitoring, and performance metrics calculation. Provides endpoint-specific analytics and success rate calculations.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def analyze_api_traffic():\n    api_calls = []\n    \n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Track API requests\n        async def track_api(page, event, request_type='request'):\n            if '/api/' in event['params']['request']['url']:\n                # Create a new tracking entry for requests\n                if request_type == 'request':\n                    request_id = event['params']['requestId']\n                    request = event['params']['request']\n                    \n                    api_calls.append({\n                        'id': request_id,\n                        'url': request['url'],\n                        'method': request['method'],\n                        'timestamp': event['params'].get('timestamp', 0),\n                        'headers': request.get('headers', {}),\n                        'postData': request.get('postData'),\n                        'status': None,\n                        'responseHeaders': None,\n                        'responseBody': None,\n                        'duration': None\n                    })\n                \n                # Update existing entries with response data\n                elif request_type == 'response':\n                    request_id = event['params']['requestId']\n                    response = event['params']['response']\n                    \n                    # Find the matching request\n                    for call in api_calls:\n                        if call['id'] == request_id:\n                            call['status'] = response['status']\n                            call['responseHeaders'] = response.get('headers', {})\n                            \n                            # Calculate request duration if timing info is available\n                            if 'timing' in response:\n                                start = response['timing'].get('requestTime', 0) * 1000\n                                end = response['timing'].get('receiveHeadersEnd', 0)\n                                call['duration'] = end - start\n        \n        # Register event handlers\n        await page.enable_network_events()\n        await page.on(\n            NetworkEvents.REQUEST_WILL_BE_SENT, \n            partial(track_api, page, request_type='request')\n        )\n        await page.on(\n            NetworkEvents.RESPONSE_RECEIVED, \n            partial(track_api, page, request_type='response')\n        )\n        \n        # Navigate to the page and wait for activity\n        await page.go_to('https://example.com/app')\n        await asyncio.sleep(10)  # Wait for API activity\n        \n        # Fetch response bodies for the API calls\n        for call in api_calls:\n            try:\n                body, is_base64 = await page.get_network_response_body(call['id'])\n                if not is_base64:\n                    try:\n                        call['responseBody'] = json.loads(body)\n                    except:\n                        call['responseBody'] = body\n            except Exception as e:\n                print(f\"Error retrieving response body: {e}\")\n        \n        # Analyze the API calls\n        print(f\"Found {len(api_calls)} API calls\")\n        \n        # Group by endpoint\n        endpoints = {}\n        for call in api_calls:\n            url = call['url']\n            endpoint = url.split('/api/')[1].split('?')[0]  # Extract endpoint path\n            \n            if endpoint not in endpoints:\n                endpoints[endpoint] = []\n            endpoints[endpoint].append(call)\n        \n        # Print summary of endpoints\n        for endpoint, calls in endpoints.items():\n            avg_time = sum(c['duration'] for c in calls if c['duration']) / len(calls) if calls else 0\n            success_rate = sum(1 for c in calls if 200 <= c.get('status', 0) < 300) / len(calls) if calls else 0\n            \n            print(f\"\\nEndpoint: /api/{endpoint}\")\n            print(f\"  Calls: {len(calls)}\")\n            print(f\"  Methods: {set(c['method'] for c in calls)}\")\n            print(f\"  Avg Response Time: {avg_time:.2f}ms\")\n            print(f\"  Success Rate: {success_rate:.1%}\")\n        \n        return api_calls\n\n# Run the analysis\nasyncio.run(analyze_api_traffic())\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Monitoring with Pydoll's Event System\nDESCRIPTION: This snippet demonstrates how to use Pydoll's event system to monitor page loads and network requests in real-time. It shows how to enable both page and network events, register event handlers, and process events during navigation.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/features.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom pydoll.events.page import PageEvents\nfrom functools import partial\n\nasync def event_monitoring_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Monitor page load events\n        async def on_page_loaded(event):\n            print(f\"ðŸŒ Page loaded: {event['params'].get('url')}\")\n            \n        await page.enable_page_events()\n        await page.on(PageEvents.PAGE_LOADED, on_page_loaded)\n        \n        # Monitor network requests\n        async def on_request(page, event):\n            url = event['params']['request']['url']\n            print(f\"ðŸ”„ Request to: {url}\")\n            \n        await page.enable_network_events()\n        await page.on(NetworkEvents.REQUEST_WILL_BE_SENT, \n                      partial(on_request, page))\n        \n        # Navigate and see events in action\n        await page.go_to('https://example.com')\n        await asyncio.sleep(5)  # Allow time to see events\n        \nasyncio.run(event_monitoring_example())\n```\n\n----------------------------------------\n\nTITLE: Running Cross-Browser Tests with Pydoll\nDESCRIPTION: This code demonstrates how to use the same automation script across different browsers (Chrome and Edge) with Pydoll's consistent API. The example navigates to a website and retrieves the page title using JavaScript execution in both browsers.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/features.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.browser.edge import Edge\n\nasync def multi_browser_example():\n    # Run the same automation in Chrome\n    async with Chrome() as chrome:\n        await chrome.start()\n        chrome_page = await chrome.get_page()\n        await chrome_page.go_to('https://example.com')\n        chrome_title = await chrome_page.execute_script('return document.title')\n        print(f\"Chrome title: {chrome_title}\")\n    \n    # Run the same automation in Edge\n    async with Edge() as edge:\n        await edge.start()\n        edge_page = await edge.get_page()\n        await edge_page.go_to('https://example.com')\n        edge_title = await edge_page.execute_script('return document.title')\n        print(f\"Edge title: {edge_title}\")\n\nasyncio.run(multi_browser_example())\n```\n\n----------------------------------------\n\nTITLE: Dynamic Tab Creation with Event Handling in PyDoll\nDESCRIPTION: This code snippet demonstrates dynamic tab creation in response to network events using PyDoll. It processes category links, creates new tabs for each category, and collects product data. The script uses asyncio for asynchronous operations and manages tab lifecycle efficiently.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom functools import partial\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.events import PageEvents, NetworkEvents\n\nasync def dynamic_tab_creation():\n    browser = Chrome()\n    await browser.start()\n    main_page = await browser.get_page()\n    \n    # Store results from all product pages\n    all_results = []\n    # Count active tabs to know when we're done\n    active_tabs = 1  # Start with 1 (main page)\n    # Event that signals all work is complete\n    all_done = asyncio.Event()\n    \n    # This callback processes product links and creates a new tab for each\n    async def process_category_links(main_page, event):\n        # Check if this is the categories response\n        if 'api/categories' not in event['params'].get('response', {}).get('url', ''):\n            return\n            \n        # Extract categories from the response\n        request_id = event['params']['requestId']\n        body, _ = await main_page.get_network_response_body(request_id)\n        categories = json.loads(body)\n        \n        print(f\"Found {len(categories)} categories to process\")\n        nonlocal active_tabs\n        active_tabs += len(categories)  # Update tab counter\n        \n        # Create a new tab for each category\n        for category in categories:\n            # Create a new tab\n            page_id = await browser.new_page()\n            page = await browser.get_page_by_id(page_id)\n            \n            # Setup a callback for this tab\n            async def process_product_data(page, category_name, event):\n                if 'api/products' not in event['params'].get('response', {}).get('url', ''):\n                    return\n                    \n                # Process the product data\n                request_id = event['params']['requestId']\n                body, _ = await page.get_network_response_body(request_id)\n                products = json.loads(body)\n                \n                # Add to results\n                nonlocal all_results\n                all_results.extend(products)\n                print(f\"Added {len(products)} products from {category_name}\")\n                \n                # Close this tab when done\n                nonlocal active_tabs\n                await page.close()\n                active_tabs -= 1\n                \n                # If this was the last tab, signal completion\n                if active_tabs == 0:\n                    all_done.set()\n            \n            # Enable network events on the new tab\n            await page.enable_network_events()\n            await page.on(\n                NetworkEvents.RESPONSE_RECEIVED,\n                partial(process_product_data, page, category['name'])\n            )\n            \n            # Navigate to the category page\n            asyncio.create_task(page.go_to(f\"https://example.com/products/{category['id']}\"))\n    \n    # Set up the main page to find categories\n    await main_page.enable_network_events()\n    await main_page.on(\n        NetworkEvents.RESPONSE_RECEIVED,\n        partial(process_category_links, main_page)\n    )\n    \n    # Navigate to the main categories page\n    await main_page.go_to(\"https://example.com/categories\")\n    \n    # Wait for all tabs to complete their work\n    try:\n        await asyncio.wait_for(all_done.wait(), timeout=60)\n    except asyncio.TimeoutError:\n        print(\"Timeout reached, continuing with data collected so far\")\n    \n    # Process results\n    print(f\"Total products collected: {len(all_results)}\")\n    \n    # Clean up\n    await browser.stop()\n    return all_results\n```\n\n----------------------------------------\n\nTITLE: Basic Pydoll Usage Pattern Example in Python\nDESCRIPTION: A complete example demonstrating the core usage pattern for Pydoll, including browser initialization, page creation, navigation, and proper cleanup. This pattern is foundational for all Pydoll automation scripts.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def pydoll_example():\n    # Create a browser instance\n    browser = Chrome()\n    await browser.start()\n    \n    # Get a page\n    page = await browser.get_page()\n    \n    # Work with the page...\n    await page.go_to(\"https://example.com\")\n    \n    # Clean up when done\n    await browser.stop()\n\n# Run your example with asyncio\nasyncio.run(pydoll_example())\n```\n\n----------------------------------------\n\nTITLE: Registering Network Event Callbacks in Pydoll\nDESCRIPTION: Code snippet showing how to register callbacks for network events in Pydoll, specifically handling request and response events to monitor and log network activity in real-time.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\n# Define a callback to handle request events\nasync def on_request(page, event):\n    url = event['params']['request']['url']\n    method = event['params']['request']['method']\n    \n    print(f\"{method} request to: {url}\")\n    \n    # You can access request headers\n    headers = event['params']['request'].get('headers', {})\n    if 'content-type' in headers:\n        print(f\"Content-Type: {headers['content-type']}\")\n\n# Define a callback to handle response events\nasync def on_response(page, event):\n    url = event['params']['response']['url']\n    status = event['params']['response']['status']\n    \n    print(f\"Response from {url}: Status {status}\")\n    \n    # Extract response timing information\n    timing = event['params']['response'].get('timing')\n    if timing:\n        total_time = timing['receiveHeadersEnd'] - timing['requestTime']\n        print(f\"Request completed in {total_time:.2f}s\")\n\n# Register the callbacks\nawait page.enable_network_events()\nawait page.on(NetworkEvents.REQUEST_WILL_BE_SENT, partial(on_request, page))\nawait page.on(NetworkEvents.RESPONSE_RECEIVED, partial(on_response, page))\n```\n\n----------------------------------------\n\nTITLE: Command Execution Implementation for CDP Communication\nDESCRIPTION: Implements the command execution flow for sending commands to the browser via Chrome DevTools Protocol. Handles command validation, connection management, timeout handling, and error processing.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def execute_command(self, command: dict, timeout: int = 10) -> dict:\n    # Validate command\n    if not isinstance(command, dict):\n        logger.error('Command must be a dictionary.')\n        raise exceptions.InvalidCommand('Command must be a dictionary')\n\n    # Ensure connection is active\n    await self._ensure_active_connection()\n    \n    # Create future for this command\n    future = self._command_manager.create_command_future(command)\n    command_str = json.dumps(command)\n\n    # Send command and await response\n    try:\n        await self._ws_connection.send(command_str)\n        response: str = await asyncio.wait_for(future, timeout)\n        return json.loads(response)\n    except asyncio.TimeoutError as exc:\n        self._command_manager.remove_pending_command(command['id'])\n        raise exc\n    except websockets.ConnectionClosed as exc:\n        await self._handle_connection_loss()\n        raise exc\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Tab Scraping with PyDoll in Python\nDESCRIPTION: This code demonstrates a multi-tab scraping approach using a single Chrome browser instance. It scrapes multiple product categories concurrently, processes network responses, and collects data efficiently. The script uses asyncio for asynchronous operations and PyDoll for browser control.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom functools import partial\nimport json\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.events import NetworkEvents\n\nasync def multi_tab_scraping():\n    # Create a single browser instance for all tabs\n    browser = Chrome()\n    await browser.start()\n    \n    # Categories to scrape\n    categories = ['electronics', 'clothing', 'books', 'home']\n    base_url = 'https://example.com/products'\n    \n    # Track results for each category\n    results = {category: [] for category in categories}\n    completion_events = {category: asyncio.Event() for category in categories}\n    \n    # Create a callback for processing category data\n    async def process_category_data(page, category, event):\n        if f'api/{category}' in event['params'].get('response', {}).get('url', ''):\n            request_id = event['params']['requestId']\n            body, _ = await page.get_network_response_body(request_id)\n            data = json.loads(body)\n            \n            # Add results to the appropriate category\n            results[category].extend(data['items'])\n            print(f\"Added {len(data['items'])} items to {category}, total: {len(results[category])}\")\n            \n            # Signal completion if we have enough data\n            if len(results[category]) >= 20 or data.get('isLastPage', False):\n                completion_events[category].set()\n    \n    # Prepare pages, one for each category\n    pages = {}\n    for category in categories:\n        # Create a new tab\n        page_id = await browser.new_page()\n        page = await browser.get_page_by_id(page_id)\n        pages[category] = page\n        \n        # Setup event monitoring for this tab\n        await page.enable_network_events()\n        await page.on(\n            NetworkEvents.RESPONSE_RECEIVED,\n            partial(process_category_data, page, category)\n        )\n        \n        # Start navigation (don't await here to allow parallel loading)\n        asyncio.create_task(page.go_to(f\"{base_url}/{category}\"))\n    \n    # Wait for all categories to complete or timeout\n    try:\n        await asyncio.wait_for(\n            asyncio.gather(*(event.wait() for event in completion_events.values())),\n            timeout=45\n        )\n    except asyncio.TimeoutError:\n        print(\"Some categories timed out, proceeding with collected data\")\n    \n    # Display results\n    total_items = 0\n    for category, items in results.items():\n        count = len(items)\n        total_items += count\n        print(f\"{category}: collected {count} items\")\n        \n        # Show sample items\n        for item in items[:2]:\n            print(f\"  - {item['name']}: ${item['price']}\")\n    \n    print(f\"Total items across all categories: {total_items}\")\n    \n    # Clean up\n    await browser.stop()\n    return results\n\n# Run the multi-tab scraper\nasyncio.run(multi_tab_scraping())\n```\n\n----------------------------------------\n\nTITLE: Retrieving Response Bodies in Pydoll\nDESCRIPTION: Code snippet demonstrating how to retrieve and analyze response bodies from network requests in Pydoll, including tracking API requests, fetching response content, and parsing JSON responses.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Enable network events\n        await page.enable_network_events()\n        \n        # Track API requests\n        api_request_ids = []\n        \n        async def track_api_request(page, event):\n            if '/api/' in event['params']['request']['url']:\n                request_id = event['params']['requestId']\n                api_request_ids.append(request_id)\n        \n        await page.on(\n            NetworkEvents.REQUEST_WILL_BE_SENT, \n            partial(track_api_request, page)\n        )\n        \n        # Navigate to a page with API calls\n        await page.go_to('https://example.com/data-heavy-page')\n        \n        # Wait a moment for requests to complete\n        await asyncio.sleep(3)\n        \n        # Fetch response bodies for API requests\n        for request_id in api_request_ids:\n            try:\n                body, is_base64 = await page.get_network_response_body(request_id)\n                \n                # Parse JSON responses\n                if not is_base64 and body:\n                    try:\n                        data = json.loads(body)\n                        print(f\"API Response Data: {json.dumps(data, indent=2)[:200]}...\")\n                    except json.JSONDecodeError:\n                        print(f\"Non-JSON response: {body[:100]}...\")\n            except Exception as e:\n                print(f\"Error retrieving response for request {request_id}: {e}\")\n        \n        # Alternative: use the built-in method to get responses matching a pattern\n        api_responses = await page.get_network_response_bodies(matches=['api'])\n        print(f\"Found {len(api_responses)} API responses\")\n        \nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Handling File Uploads with Pydoll\nDESCRIPTION: This example shows two methods for handling file uploads in web automation. The first uses direct file input for simple upload fields, while the second demonstrates using the file chooser API for more complex upload interactions that open system dialogs.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/features.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport os\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def file_upload_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://example.com/upload')\n        \n        # Method 1: Direct file input\n        file_input = await page.find_element(By.XPATH, '//input[@type=\"file\"]')\n        await file_input.set_input_files('path/to/document.pdf')\n        \n        # Method 2: Using file chooser with an upload button\n        sample_file = os.path.join(os.getcwd(), 'sample.jpg')\n        async with page.expect_file_chooser(files=sample_file):\n            upload_button = await page.find_element(By.ID, 'upload-button')\n            await upload_button.click()\n            \n        # Submit the form\n        submit = await page.find_element(By.ID, 'submit-button')\n        await submit.click()\n        \n        print(\"Files uploaded successfully!\")\n\nasyncio.run(file_upload_example())\n```\n\n----------------------------------------\n\nTITLE: Modifying Request Body Data in PyDoll\nDESCRIPTION: Demonstrates how to modify the POST data in intercepted requests. This example shows parsing JSON body content, adding or modifying fields, and then sending the modified request. It only targets specific endpoints and request methods.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nasync def modify_post_data(page, event):\n    request_id = event['params']['requestId']\n    request = event['params']['request']\n    url = request['url']\n    method = request['method']\n    \n    # Only process POST requests to specific endpoints\n    if method == 'POST' and '/api/submit' in url:\n        # Get the original post data, if any\n        original_post_data = request.get('postData', '{}')\n        \n        try:\n            # Parse the original data\n            data = json.loads(original_post_data)\n            \n            # Modify the data\n            data['additionalField'] = 'injected-value'\n            data['timestamp'] = int(time.time())\n            \n            # Convert back to string\n            modified_post_data = json.dumps(data)\n            \n            print(f\"Modified POST data for {url}\")\n            \n            await page._execute_command(\n                FetchCommands.continue_request(\n                    request_id=request_id,\n                    post_data=modified_post_data\n                )\n            )\n        except json.JSONDecodeError:\n            # If not JSON, continue normally\n            await page._execute_command(\n                FetchCommands.continue_request(request_id)\n            )\n    else:\n        # Continue normally for non-POST requests\n        await page._execute_command(\n            FetchCommands.continue_request(request_id)\n        )\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Support in Pydoll\nDESCRIPTION: This example shows how to configure proxy settings in Pydoll for web automation. It demonstrates setting up both authenticated and non-authenticated proxies, configuring bypass rules, and verifying the proxy connection by checking the public IP address.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/features.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.browser.options import Options\n\nasync def proxy_example():\n    # Create browser options\n    options = Options()\n    \n    # Simple proxy without authentication\n    options.add_argument('--proxy-server=192.168.1.100:8080')\n    # Or proxy with authentication\n    # options.add_argument('--proxy-server=username:password@192.168.1.100:8080')\n    \n    # Bypass proxy for specific domains\n    options.add_argument('--proxy-bypass-list=*.internal.company.com,localhost')\n\n    # Start browser with proxy configuration\n    async with Chrome(options=options) as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Test the proxy by visiting an IP echo service\n        await page.go_to('https://api.ipify.org')\n        ip_address = await page.page_source\n        print(f\"Current IP address: {ip_address}\")\n        \n        # Continue with your automation\n        await page.go_to('https://example.com')\n        title = await page.execute_script('document.title')\n        print(f\"Page title: {title}\")\n\nasyncio.run(proxy_example())\n```\n\n----------------------------------------\n\nTITLE: Finding DOM Elements with Multiple Selector Strategies in Python\nDESCRIPTION: This function locates elements on a web page using various selector strategies (CSS, XPath, ID, etc.). It translates high-level locator types to specific browser commands and creates WebElement instances to represent the found elements.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nasync def find_element(\n    self, by: By, value: str, raise_exc: bool = True\n) -> Optional[WebElement]:\n    \"\"\"\n    Finds an element on the page using the specified locator strategy.\n    \n    Args:\n        by: The method to locate the element\n        value: The selector value\n        raise_exc: Whether to raise exception if element not found\n        \n    Returns:\n        WebElement if found, None otherwise (if raise_exc is False)\n        \n    Raises:\n        ElementNotFound: If the element is not found and raise_exc is True\n    \"\"\"\n    selector = self._from_by_to_selector(by, value)\n    selector_function = self._get_selector_function(by)\n    \n    result = await selector_function(selector)\n    if not result:\n        if raise_exc:\n            raise ElementNotFound(f\"Element not found with {by}: {value}\")\n        return None\n        \n    # Create WebElement from result data\n    return WebElement(self._connection_handler, result['nodeId'], result['objectId'])\n```\n\n----------------------------------------\n\nTITLE: Modifying HTTP Headers in Intercepted Requests with PyDoll\nDESCRIPTION: Shows how to add or modify HTTP headers in intercepted requests. This includes preserving existing headers while adding custom headers such as authorization tokens, custom user agents, and other application-specific headers.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nasync def inject_headers(page, event):\n    request_id = event['params']['requestId']\n    request = event['params']['request']\n    url = request['url']\n    \n    # Get existing headers\n    headers = request.get('headers', {})\n    \n    # Add or modify headers\n    custom_headers = {\n        **headers,  # Keep existing headers\n        'X-Custom-Header': 'CustomValue',\n        'Authorization': 'Bearer your-token-here',\n        'User-Agent': 'Custom User Agent String',\n    }\n    \n    await page._execute_command(\n        FetchCommands.continue_request(\n            request_id=request_id,\n            headers=custom_headers\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Element Waiting in Python for Web Automation\nDESCRIPTION: This function waits for an element to appear on a web page with a configurable timeout. It uses polling to periodically check for the element and provides options for timeout management and exception handling when elements aren't found.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nasync def wait_element(\n    self, by: By, value: str, timeout: int = 10, raise_exc: bool = True\n) -> Optional[WebElement]:\n    \"\"\"\n    Waits for an element to be present on the page.\n    \n    Args:\n        by: The method to locate the element\n        value: The selector value\n        timeout: Maximum time to wait in seconds\n        raise_exc: Whether to raise exception if element not found\n        \n    Returns:\n        WebElement if found, None otherwise (if raise_exc is False)\n        \n    Raises:\n        ElementNotFound: If the element is not found within timeout\n    \"\"\"\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        element = await self.find_element(by, value, raise_exc=False)\n        if element:\n            return element\n        await asyncio.sleep(0.1)\n        \n    if raise_exc:\n        raise ElementNotFound(f\"Element not found with {by}: {value}\")\n    return None\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots and PDFs in Pydoll\nDESCRIPTION: Shows how to capture screenshots and export pages as PDFs using the Page domain. The methods include saving files directly or getting base64-encoded content.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Take a screenshot and save it to a file\nscreenshot_path = \"homepage.png\"\nawait page.get_screenshot(screenshot_path)\n\n# Get a screenshot as base64 (useful for embedding in reports)\nscreenshot_base64 = await page.get_screenshot_base64()\n\n# Export page as PDF\npdf_path = \"homepage.pdf\"\nawait page.print_to_pdf(pdf_path)\n```\n\n----------------------------------------\n\nTITLE: Handling Cloudflare Captchas in Pydoll\nDESCRIPTION: Shows two approaches for handling Cloudflare captchas: using a context manager that blocks until the captcha is solved, or enabling auto-solve functionality that works in the background.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Context manager approach (blocks until captcha is solved)\nasync with page.expect_and_bypass_cloudflare_captcha():\n    await page.go_to(\"https://site-with-cloudflare.com\")\n    # Continue only after captcha is solved\n\n# Background processing approach\nawait page.enable_auto_solve_cloudflare_captcha()\nawait page.go_to(\"https://another-protected-site.com\")\n# Code continues immediately, captcha solved in background\n\n# When finished with auto-solving\nawait page.disable_auto_solve_cloudflare_captcha()\n```\n\n----------------------------------------\n\nTITLE: Network Monitoring in Pydoll Page Domain\nDESCRIPTION: Demonstrates how to use Pydoll's network monitoring capabilities in the Page domain. Shows enabling network events, navigating to a page, filtering network logs, and retrieving response bodies from specific requests.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Enable network events\nawait page.enable_network_events()\n\n# Perform some action that triggers network requests\nawait page.go_to(\"https://example.com\")\n\n# Get network logs matching specific patterns\napi_logs = await page.get_network_logs(matches=[\"api\", \"graphql\"])\n\n# Get response bodies for specific requests\njson_responses = await page.get_network_response_bodies(matches=[\"api/data\"])\n\n# Get a specific response body by request ID\nrequest_id = api_logs[0]['params']['requestId']\nbody, is_base64 = await page.get_network_response_body(request_id)\n```\n\n----------------------------------------\n\nTITLE: Handling HTTP Authentication Challenges with PyDoll\nDESCRIPTION: Shows how to automatically respond to HTTP authentication challenges using PyDoll. This example enables authentication handling and registers a handler that provides username and password credentials when an authentication challenge is detected.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Define authentication handler\n        async def handle_auth(page, event):\n            request_id = event['params']['requestId']\n            auth_challenge = event['params']['authChallenge']\n            \n            print(f\"Authentication required: {auth_challenge['origin']}\")\n            \n            # Provide credentials\n            await page._execute_command(\n                FetchCommands.continue_request_with_auth(\n                    request_id=request_id,\n                    proxy_username=\"username\",\n                    proxy_password=\"password\"\n                )\n            )\n        \n        # Enable fetch events with auth handling\n        await page.enable_fetch_events(handle_auth=True)\n        await page.on(\n            FetchEvents.AUTH_REQUIRED, \n            partial(handle_auth, page)\n        )\n        \n        # Navigate to a page requiring authentication\n        await page.go_to('https://protected-site.com')\n```\n\n----------------------------------------\n\nTITLE: Multi-Page Automation Example\nDESCRIPTION: Shows how to work with multiple browser pages simultaneously\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nasync def multi_page_example():\n    browser = Chrome()\n    await browser.start()\n    \n    # Create and work with multiple pages\n    page1 = await browser.get_page()\n    await page1.go_to(\"https://example.com\")\n    \n    page2 = await browser.get_page()\n    await page2.go_to(\"https://github.com\")\n    \n    # Get information from both pages\n    title1 = await page1.execute_script(\"return document.title\")\n    title2 = await page2.execute_script(\"return document.title\")\n    \n    print(f\"Page 1: {title1}\")\n    print(f\"Page 2: {title2}\")\n    \n    await browser.stop()\n```\n\n----------------------------------------\n\nTITLE: Tracking DOM Events in Pydoll\nDESCRIPTION: Illustrates how to monitor DOM structure changes using Pydoll's DOM events. Shows how to react to attribute modifications and perform conditional actions based on DOM changes.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n# Enable DOM events\nawait page.enable_dom_events()\n\n# Track attribute changes\nasync def track_attribute_change(page, event):\n    node_id = event['params']['nodeId']\n    name = event['params']['name']\n    value = event['params']['value']\n    print(f\"Attribute changed on node {node_id}: {name}={value}\")\n    \n    # You can react to specific attribute changes\n    if name == 'data-status' and value == 'loaded':\n        element = await page.find_element(By.CSS_SELECTOR, f\"[data-id='{node_id}']\")\n        await element.click()\n\nawait page.on(DOMEvents.ATTRIBUTE_MODIFIED, partial(track_attribute_change, page))\n```\n\n----------------------------------------\n\nTITLE: Modifying URL and Method in Intercepted Requests with PyDoll\nDESCRIPTION: Demonstrates how to modify the URL and HTTP method of intercepted requests before they're sent to the server. This includes redirecting requests to different domains and converting GET requests to POST requests for specific endpoints.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def redirect_request(page, event):\n    request_id = event['params']['requestId']\n    request = event['params']['request']\n    url = request['url']\n    \n    # Redirect requests for one domain to another\n    if 'old-domain.com' in url:\n        new_url = url.replace('old-domain.com', 'new-domain.com')\n        print(f\"Redirecting {url} to {new_url}\")\n        \n        await page._execute_command(\n            FetchCommands.continue_request(\n                request_id=request_id,\n                url=new_url\n            )\n        )\n    # Change GET to POST for specific endpoints\n    elif '/api/data' in url and request['method'] == 'GET':\n        print(f\"Converting GET to POST for {url}\")\n        \n        await page._execute_command(\n            FetchCommands.continue_request(\n                request_id=request_id,\n                method='POST'\n            )\n        )\n    else:\n        # Continue normally\n        await page._execute_command(\n            FetchCommands.continue_request(request_id)\n        )\n```\n\n----------------------------------------\n\nTITLE: Listing Open Browser Pages\nDESCRIPTION: Shows how to get information about all open pages in the browser\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Get all targets (pages, service workers, etc.)\ntargets = await browser.get_targets()\n\n# Filter for page targets only\npages = [t for t in targets if t.get('type') == 'page']\n\nfor page in pages:\n    print(f\"Page ID: {page['targetId']}\")\n    print(f\"URL: {page['url']}\")\n```\n\n----------------------------------------\n\nTITLE: Parallel Scraping with Multiple Browser Instances\nDESCRIPTION: Advanced implementation of parallel scraping using multiple browser instances and event handling for concurrent data collection across different product categories.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom functools import partial\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.events import PageEvents, NetworkEvents\n\nasync def scrape_with_events(url, product_type):\n    # Create an individual scraping task\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n    \n    # Data container and completion signal\n    results = []\n    scraping_done = asyncio.Event()\n    \n    # Define callbacks for data extraction\n    async def product_data_handler(page, product_type, event):\n        if f'api/{product_type}' in event['params'].get('response', {}).get('url', ''):\n            request_id = event['params']['requestId']\n            body, _ = await page.get_network_response_body(request_id)\n            data = json.loads(body)\n            results.extend(data['items'])\n            \n            # Check if we have all the data we need\n            if len(results) >= 20 or data.get('isLastPage', False):\n                scraping_done.set()\n    \n    # Setup monitoring\n    await page.enable_network_events()\n    await page.on(\n        NetworkEvents.RESPONSE_RECEIVED, \n        partial(product_data_handler, page, product_type)\n    )\n    \n    # Navigate and wait for data\n    await page.go_to(f\"{url}/{product_type}\")\n    \n    try:\n        # Wait up to 30 seconds for data collection\n        await asyncio.wait_for(scraping_done.wait(), 30)\n    except asyncio.TimeoutError:\n        print(f\"Timeout for {product_type}, collected {len(results)} items\")\n    \n    await browser.stop()\n    return results\n\nasync def main():\n    # Define different product categories to scrape in parallel\n    product_types = ['electronics', 'clothing', 'books', 'home']\n    base_url = 'https://example.com/products'\n    \n    # Launch concurrent scraping tasks\n    tasks = [scrape_with_events(base_url, product_type) for product_type in product_types]\n    all_results = await asyncio.gather(*tasks)\n    \n    # Process combined results\n    for product_type, results in zip(product_types, all_results):\n        print(f\"{product_type}: {len(results)} products found\")\n        \n        # Process specific category data\n        for item in results[:3]:  # Show first 3 items\n            print(f\"  - {item['name']}: ${item['price']}\")\n    \n    # Calculate overall statistics\n    total_products = sum(len(category) for category in all_results)\n    print(f\"Total products across all categories: {total_products}\")\n\n# Run the concurrent scraper\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Using CSS Selectors with Pydoll's FindElementsMixin\nDESCRIPTION: Examples showing how to use CSS selectors with the FindElementsMixin to locate elements in the DOM. CSS selectors provide a concise and familiar syntax for finding elements based on their attributes, position, and state.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# CSS selector examples\nawait page.find_element(By.CSS_SELECTOR, \"div.content > p.intro\")\nawait page.find_element(By.CSS_SELECTOR, \"#login-form input[type='password']\")\n```\n\n----------------------------------------\n\nTITLE: Advanced Keyboard Control with Human-Like Typing in Python\nDESCRIPTION: This snippet demonstrates Pydoll's advanced keyboard control capabilities with human-like typing. The code shows how to use realistic timing between keystrokes, special key combinations, and key modifiers to mimic human behavior and avoid detection by anti-bot systems.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/features.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.common.keys import Keys\n\nasync def realistic_typing_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://example.com/login')\n        \n        # Find login form elements\n        username = await page.find_element(By.ID, 'username')\n        password = await page.find_element(By.ID, 'password')\n        \n        # Type with realistic timing (interval between keystrokes)\n        await username.type_keys(\"user@example.com\", interval=0.15)\n        \n        # Use special key combinations\n        await password.click()\n        await password.key_down(Keys.SHIFT)\n        await password.send_keys(\"PASSWORD\")\n        await password.key_up(Keys.SHIFT)\n        \n        # Press Enter to submit\n        await password.send_keys(Keys.ENTER)\n        \n        # Wait for navigation\n        await asyncio.sleep(2)\n        print(\"Logged in successfully!\")\n\nasyncio.run(realistic_typing_example())\n```\n\n----------------------------------------\n\nTITLE: Page Creation and Management\nDESCRIPTION: Demonstrates various methods for creating and managing browser pages\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Create a new empty page\nnew_page_id = await browser.new_page()\n\n# Create a new page and navigate to a URL\nnew_page_id = await browser.new_page(\"https://example.com\")\n\n# Get a page object from an existing or new page\npage = await browser.get_page()\n\n# Get a page object for a specific page ID\npage = await browser.get_page_by_id(page_id)\n```\n\n----------------------------------------\n\nTITLE: Using XPath Selectors with Pydoll's FindElementsMixin\nDESCRIPTION: Examples demonstrating XPath expressions with FindElementsMixin to navigate and select elements in the DOM. XPath offers powerful features like text content selection and traversing up the DOM tree.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# XPath examples\nawait page.find_element(By.XPATH, \"//div[@id='content']/p[contains(text(), 'Welcome')]\")\nawait page.find_element(By.XPATH, \"//button[text()='Submit']\")\n```\n\n----------------------------------------\n\nTITLE: Bypassing Cloudflare Captcha with Background Processing in Python\nDESCRIPTION: This snippet shows an alternative approach to bypass Cloudflare captchas using Pydoll's background processing method. The code enables automatic captcha solving before navigation, allowing the process to happen in the background while the script continues execution.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/features.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser import Chrome\n\nasync def background_bypass_example():\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n    \n    # Enable automatic captcha solving before navigating\n    await page.enable_auto_solve_cloudflare_captcha()\n    \n    # Navigate to the protected site - captcha handled automatically in background\n    await page.go_to('https://site-with-cloudflare.com')\n    print(\"Page loaded, captcha will be handled in the background...\")\n    \n    # Add a small delay to allow captcha solving to complete\n    await asyncio.sleep(3)\n    \n    # Continue with automation\n    await page.find_element_by_id('protected-content').get_text()\n    \n    # Disable auto-solving when no longer needed\n    await page.disable_auto_solve_cloudflare_captcha()\n    \n    await browser.stop()\n\nasyncio.run(background_bypass_example())\n```\n\n----------------------------------------\n\nTITLE: Managing Page-Specific Cookie State\nDESCRIPTION: Demonstrates how to manage cookies at the individual Page level, allowing multiple browser tabs to have independent cookie states. This enables testing different user scenarios simultaneously.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# First page with one set of cookies (user A)\npage1 = await browser.get_page()\nawait page1.go_to(\"https://example.com\")\nawait page1.set_cookies([{\"name\": \"user\", \"value\": \"user_a\", \"domain\": \"example.com\"}])\n\n# Second page with different cookies (user B)\npage2 = await browser.get_page()\nawait page2.go_to(\"https://example.com\") \nawait page2.set_cookies([{\"name\": \"user\", \"value\": \"user_b\", \"domain\": \"example.com\"}])\n```\n\n----------------------------------------\n\nTITLE: Mouse Click Implementation in WebElement\nDESCRIPTION: The click method implementation which handles different element types specially (like option tags), checks visibility, scrolls elements into view, and properly simulates mouse press and release events with configurable hold time.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/webelement-domain.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def click(\n    self,\n    x_offset: int = 0,\n    y_offset: int = 0,\n    hold_time: float = 0.1,\n):\n    \"\"\"\n    Clicks on the element using mouse events.\n    \"\"\"\n    if self._is_option_tag():\n        return await self.click_option_tag()\n\n    if not await self._is_element_visible():\n        raise exceptions.ElementNotVisible(\n            'Element is not visible on the page.'\n        )\n\n    await self.scroll_into_view()\n    \n    # Get element position and calculate click point\n    # ... (position calculation code)\n    \n    # Send mouse press and release events\n    press_command = InputCommands.mouse_press(*position_to_click)\n    release_command = InputCommands.mouse_release(*position_to_click)\n    await self._connection_handler.execute_command(press_command)\n    await asyncio.sleep(hold_time)\n    await self._connection_handler.execute_command(release_command)\n```\n\n----------------------------------------\n\nTITLE: JavaScript Execution Methods in Page Context\nDESCRIPTION: Demonstrates the dual-mode JavaScript execution capabilities of the Page domain, including global script execution to retrieve window dimensions and element-contextualized JavaScript execution.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Execute JavaScript in page context\ndimensions = await page.execute_script(\"\"\"\n    return {\n        width: window.innerWidth,\n        height: window.innerHeight,\n        devicePixelRatio: window.devicePixelRatio\n    }\n\"\"\")\nprint(f\"Window dimensions: {dimensions}\")\n\n# Find an element and manipulate it with JavaScript\nheading = await page.find_element(By.TAG_NAME, \"h1\")\n```\n\n----------------------------------------\n\nTITLE: Using Async Context Managers for File Uploads in Pydoll\nDESCRIPTION: Demonstrates how to use Pydoll's built-in async context manager for handling file uploads. The context manager waits for the file chooser dialog and automatically handles the file selection event.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nasync with page.expect_file_chooser(files=\"path/to/file.pdf\"):\n    # Trigger the file chooser dialog\n    upload_button = await page.find_element(By.ID, \"upload-button\")\n    await upload_button.click()\n    # Context manager handles waiting for and responding to the file chooser event\n```\n\n----------------------------------------\n\nTITLE: Error-Handled Network Request Interception in Pydoll\nDESCRIPTION: Implements a safe network request handler with proper error handling and fallback continuation to ensure requests are not blocked even when errors occur.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nasync def safe_network_handler(page, event):\n    try:\n        # Your interception logic here\n        request_id = event['params']['requestId']\n        # ...\n        await page._execute_command(FetchCommands.continue_request(request_id))\n    except Exception as e:\n        print(f\"Error in request handler: {e}\")\n        # Try to continue the request even if there was an error\n        try:\n            request_id = event['params']['requestId']\n            await page._execute_command(FetchCommands.continue_request(request_id))\n        except:\n            pass\n```\n\n----------------------------------------\n\nTITLE: Implementing Browser Base Class and Chrome Extension\nDESCRIPTION: Abstract base class definition for Browser with Chrome implementation example. Shows core initialization and required abstract methods.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Abstract base class (simplified)\nclass Browser(ABC):\n    def __init__(self, options=None, connection_port=None, browser_type=None):\n        # Initialize components\n        # ...\n    \n    @abstractmethod\n    def _get_default_binary_location(self) -> str:\n        \"\"\"Must be implemented by subclasses\"\"\"\n        pass\n\n    async def start(self):\n        # Start browser process\n        # Establish CDP connection\n        # Configure initial state\n        # ...\n\n# Implementation for Chrome\nclass Chrome(Browser):\n    def _get_default_binary_location(self) -> str:\n        # Return path to Chrome binary\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Monitoring Network Events and Logging in Pydoll\nDESCRIPTION: Shows how to enable network events to monitor HTTP requests and responses. Includes tracking specific requests, logging network activity, and retrieving response bodies for analysis.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n# Enable network events\nawait page.enable_network_events()\n\n# Monitor request activity\nasync def log_request(page, event):\n    url = event['params']['request']['url']\n    method = event['params']['request']['method']\n    print(f\"{method} request to: {url}\")\n    \n    # You can trigger actions based on specific requests\n    if 'api/login' in url and method == 'POST':\n        print(\"Login request detected, waiting for response...\")\n\nawait page.on(NetworkEvents.REQUEST_WILL_BE_SENT, partial(log_request, page))\n\n# After performing actions, retrieve logs\napi_logs = await page.get_network_logs(matches=[\"api\", \"graphql\"])\n\n# Get response bodies for specific requests\njson_responses = await page.get_network_response_bodies(matches=[\"api/data\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring Chrome Browser with Custom Options in Pydoll\nDESCRIPTION: Shows how to customize browser behavior in Pydoll by using the Options class. This example demonstrates setting up a proxy connection and specifying a custom browser binary location.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/README.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.browser.options import Options\n\noptions = Options()\n# Add a proxy\noptions.add_argument('--proxy-server=username:password@ip:port')\n# Custom browser location\noptions.binary_location = '/path/to/your/browser'\n\nasync with Chrome(options=options) as browser:\n    await browser.start()\n    # Your code here\n```\n\n----------------------------------------\n\nTITLE: WebElement Class Initialization in Python\nDESCRIPTION: The constructor for the WebElement class that initializes key properties including object_id, connection handler, search method, selector, and attributes. This shows how element references are managed in Pydoll.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/webelement-domain.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass WebElement(FindElementsMixin):\n    def __init__(\n        self,\n        object_id: str,\n        connection_handler: ConnectionHandler,\n        method: str = None,\n        selector: str = None,\n        attributes_list: list = [],\n    ):\n        self._object_id = object_id\n        self._search_method = method\n        self._selector = selector\n        self._connection_handler = connection_handler\n        self._attributes = {}\n        self._last_input = ''\n        self._command_id = 0\n        self._def_attributes(attributes_list)\n```\n\n----------------------------------------\n\nTITLE: Managing Dialog Interactions in Pydoll\nDESCRIPTION: Demonstrates how to handle JavaScript dialogs (alerts, confirms, prompts) by registering event handlers and using dialog interaction methods. Includes checking for dialog presence and retrieving dialog messages.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Set up a dialog handler\nasync def handle_dialog(event):\n    if await page.has_dialog():\n        message = await page.get_dialog_message()\n        print(f\"Dialog detected: {message}\")\n        await page.accept_dialog()\n\n# Enable page events to detect dialogs\nawait page.enable_page_events()\nawait page.on('Page.javascriptDialogOpening', handle_dialog)\n\n# Trigger an alert dialog\nawait page.execute_script(\"alert('This is a test alert')\")\n```\n\n----------------------------------------\n\nTITLE: Proxy Configuration Setup\nDESCRIPTION: Demonstrates how to configure and use proxies for browser automation\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.browser.options import Options\n\noptions = Options()\n\n# Configure a proxy\noptions.add_argument('--proxy-server=http://proxy.example.com:8080')\n\n# For proxies requiring authentication\nbrowser = Chrome(options=options)\nawait browser.start()\n\n# Pydoll automatically handles proxy authentication challenges\npage = await browser.get_page()\nawait page.go_to(\"https://example.com\")\n```\n\n----------------------------------------\n\nTITLE: Efficient Callback Patterns in Pydoll\nDESCRIPTION: Demonstrates how to write efficient event callbacks in Pydoll by implementing early filtering to minimize processing overhead for high-frequency events.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# LESS EFFICIENT: Processes every request\nasync def log_all_requests(event):\n    print(f\"Request: {event['params']['request']['url']}\")\n\n# MORE EFFICIENT: Early filtering\nasync def log_api_requests(event):\n    url = event['params']['request']['url']\n    if '/api/' not in url:\n        return  # Early exit for non-API requests\n    print(f\"API Request: {url}\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Selective Resource Type Interception in Pydoll\nDESCRIPTION: Shows how to enable fetch event interception for specific resource types like XHR and Document to optimize performance by limiting interception scope.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nawait page.enable_fetch_events(resource_type='XHR')  # For API calls\nawait page.enable_fetch_events(resource_type='Document')  # For main documents\n```\n\n----------------------------------------\n\nTITLE: Event Callback Registration Example\nDESCRIPTION: Shows how to register and remove event callbacks for handling browser events through the Connection Handler. Demonstrates the basic pattern for event-driven automation.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Register a callback for a specific event\ncallback_id = await connection.register_callback(\n    'Page.loadEventFired', \n    handle_page_load\n)\n\n# Remove a specific callback\nawait connection.remove_callback(callback_id)\n```\n\n----------------------------------------\n\nTITLE: Enabling Event Domains in Pydoll\nDESCRIPTION: Shows how to enable different event domains (page, network, DOM, fetch) which must be explicitly activated before they will emit events. Each domain corresponds to a specific area of browser functionality.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Enable page events to monitor page load, navigation, dialogs, etc.\nawait page.enable_page_events()\n\n# Enable network events to monitor requests, responses, etc.\nawait page.enable_network_events()\n\n# Enable DOM events to monitor DOM changes\nawait page.enable_dom_events()\n\n# Enable fetch events to intercept and modify requests\nawait page.enable_fetch_events()\n```\n\n----------------------------------------\n\nTITLE: Browser Options Configuration Example\nDESCRIPTION: Shows how to configure browser options including binary location and command-line arguments.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.browser.options import Options\n\noptions = Options()\noptions.binary_location = '/usr/bin/google-chrome-stable'\noptions.add_argument('--headless=new')\noptions.add_argument('--disable-gpu')\noptions.add_argument('--window-size=1920,1080')\n\nbrowser = Chrome(options=options)\n```\n\n----------------------------------------\n\nTITLE: Finding Elements by XPath Using JavaScript Execution in Python\nDESCRIPTION: This method finds DOM elements using XPath selectors through JavaScript execution. It's necessary because XPath queries aren't directly supported by CDP and require using the browser's document.evaluate() function.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nasync def _find_by_xpath(self, xpath: str):\n    \"\"\"Finds an element using XPath via JavaScript execution.\"\"\"\n    result = await self._execute_command(\n        RuntimeCommands.evaluate_script(\n            \"\"\"\n            function getElementByXpath(path) {\n              return document.evaluate(\n                path, document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null\n              ).singleNodeValue;\n            }\n            return getElementByXpath(arguments[0]);\n            \"\"\",\n            arguments=[xpath]\n        )\n    )\n    # Process result...\n```\n\n----------------------------------------\n\nTITLE: Registering Event Callbacks in Pydoll\nDESCRIPTION: Demonstrates how to enable page events and register event callbacks for browser events. This is the foundation of event-driven browser automation in Pydoll.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Basic example of enabling events and registering a callback\nawait page.enable_page_events()\nawait page.on('Page.loadEventFired', handle_load_event)\n```\n\n----------------------------------------\n\nTITLE: Cookie Management Operations\nDESCRIPTION: Browser-wide cookie management including setting, getting, and deleting cookies\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Set cookies at the browser level\ncookies_to_set = [\n    {\n        \"name\": \"session_id\",\n        \"value\": \"global_session_123\",\n        \"domain\": \"example.com\",\n        \"path\": \"/\",\n        \"secure\": True,\n        \"httpOnly\": True\n    }\n]\nawait browser.set_cookies(cookies_to_set)\n\n# Get all cookies from the browser\nall_cookies = await browser.get_cookies()\nprint(f\"Number of cookies: {len(all_cookies)}\")\n\n# Delete all cookies from the browser\nawait browser.delete_all_cookies()\n```\n\n----------------------------------------\n\nTITLE: WebElement Technical Architecture with Mermaid Class Diagram\nDESCRIPTION: A class diagram showing the relationships between WebElement, FindElementsMixin, and ConnectionHandler classes. It illustrates the inheritance and composition patterns used in the architecture.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/webelement-domain.md#2025-04-19_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nclassDiagram\n    class WebElement {\n        -_object_id: str\n        -_search_method: str\n        -_selector: str\n        -_connection_handler: ConnectionHandler\n        -_attributes: dict\n        -_last_input: str\n        -_command_id: int\n        +click()\n        +click_using_js()\n        +type_keys(text: str)\n        +get_attribute(name: str)\n        +text\n        +inner_html\n        +value\n        +id\n        +class_name\n        +is_enabled\n    }\n    \n    class FindElementsMixin {\n        +find_element(by: By, value: str, raise_exc: bool = True)\n        +find_elements(by: By, value: str, raise_exc: bool = True)\n        +wait_element(by: By, value: str, timeout: int, raise_exc: bool = True)\n        +wait_elements(by: By, value: str, timeout: int, raise_exc: bool = True)\n    }\n    \n    class ConnectionHandler {\n        +execute_command(command: dict)\n    }\n    \n    WebElement --|> FindElementsMixin : inherits\n    WebElement *-- ConnectionHandler : uses\n```\n\n----------------------------------------\n\nTITLE: Using Various Selector Types with FindElementsMixin\nDESCRIPTION: Examples of using different selector types supported by FindElementsMixin including ID, class, tag name, link text, and name attribute selectors.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# ID selector (shorthand for CSS #id)\nawait page.find_element(By.ID, \"username\")\n\n# Class name (shorthand for CSS .classname)\nawait page.find_element(By.CLASS_NAME, \"submit-button\")\n\n# Tag name (shorthand for CSS tagname)\nawait page.find_element(By.TAG_NAME, \"button\")\n\n# Link text (finds <a> elements by their text)\nawait page.find_element(By.LINK_TEXT, \"Click here\")\n\n# Partial link text\nawait page.find_element(By.PARTIAL_LINK_TEXT, \"Click\")\n\n# Name attribute (shorthand for CSS [name='value'])\nawait page.find_element(By.NAME, \"username\")\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Async Context Managers in Pydoll\nDESCRIPTION: Shows how to create a custom async context manager for waiting for page navigation. It registers a temporary callback for the navigation event and provides cleanup logic when the operation completes.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@asynccontextmanager\nasync def wait_for_navigation():\n    navigation_complete = asyncio.Event()\n    \n    async def on_navigation(event):\n        navigation_complete.set()\n    \n    # Enable events if not already enabled\n    was_enabled = page.page_events_enabled\n    if not was_enabled:\n        await page.enable_page_events()\n        \n    # Register temporary callback\n    await page.on(PageEvents.FRAME_NAVIGATED, on_navigation, temporary=True)\n    \n    try:\n        yield\n        # Wait for navigation to complete\n        await navigation_complete.wait()\n    finally:\n        # Clean up if we enabled events\n        if not was_enabled:\n            await page.disable_page_events()\n```\n\n----------------------------------------\n\nTITLE: Browser Window Management\nDESCRIPTION: Methods for controlling browser window position and size\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Get the current window ID\nwindow_id = await browser.get_window_id()\n\n# Set window bounds (position and size)\nawait browser.set_window_bounds({\n    'left': 100,\n    'top': 100,\n    'width': 1024,\n    'height': 768\n})\n\n# Maximize the window\nawait browser.set_window_maximized()\n\n# Minimize the window\nawait browser.set_window_minimized()\n```\n\n----------------------------------------\n\nTITLE: Setting and Managing Cookies in Pydoll\nDESCRIPTION: Shows how to set, retrieve, and delete cookies using the Page domain. The example demonstrates creating cookies with various attributes, getting all cookies, and deleting them.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Set a cookie\ncookies_to_set = [\n    {\n        \"name\": \"session_id\",\n        \"value\": \"test_session_123\",\n        \"domain\": \"example.com\",\n        \"path\": \"/\",\n        \"secure\": True,\n        \"httpOnly\": True\n    }\n]\nawait page.set_cookies(cookies_to_set)\n\n# Get all cookies\nall_cookies = await page.get_cookies()\nprint(f\"Number of cookies: {len(all_cookies)}\")\n\n# Delete all cookies\nawait page.delete_all_cookies()\n```\n\n----------------------------------------\n\nTITLE: Handling File Uploads with Context Manager\nDESCRIPTION: Shows how to handle file uploads using a context manager pattern that manages the file chooser dialog interaction. This simplifies the upload process by automatically handling the file selection.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Path to a file to upload\nfile_path = \"document.pdf\"\n\n# Use the context manager to handle file chooser dialog\nasync with page.expect_file_chooser(files=file_path):\n    # Find and click the upload button\n    upload_button = await page.find_element(By.ID, \"upload-button\")\n    await upload_button.click()\n```\n\n----------------------------------------\n\nTITLE: Registering Synchronous and Asynchronous Callbacks in Python for ConnectionHandler\nDESCRIPTION: Illustrates how to register both synchronous and asynchronous callbacks for different Chrome DevTools Protocol events.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Synchronous callback\ndef synchronous_callback(event):\n    print(f\"Event received: {event['method']}\")\n\n# Asynchronous callback\nasync def asynchronous_callback(event):\n    await asyncio.sleep(0.1)  # Perform some async operation\n    print(f\"Event processed asynchronously: {event['method']}\")\n\n# Both can be registered the same way\nawait connection.register_callback('Network.requestWillBeSent', synchronous_callback)\nawait connection.register_callback('Network.responseReceived', asynchronous_callback)\n```\n\n----------------------------------------\n\nTITLE: Executing JavaScript on WebElement in Python\nDESCRIPTION: This snippet shows how to execute JavaScript in the context of a specific element. It uses the CDP Runtime domain to call a function on the element's object ID.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/webelement-domain.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def _execute_script(\n    self, script: str, return_by_value: bool = False\n):\n    \"\"\"\n    Executes a JavaScript script in the context of this element.\n    \"\"\"\n    return await self._execute_command(\n        RuntimeCommands.call_function_on(\n            self._object_id, script, return_by_value\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Parallel Command Execution in Python using ConnectionHandler\nDESCRIPTION: Demonstrates how to execute multiple Chrome DevTools Protocol commands concurrently and wait for all results using asyncio.gather().\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nasync def get_page_metrics(connection):\n    commands = [\n        {\"id\": 1, \"method\": \"Performance.getMetrics\"},\n        {\"id\": 2, \"method\": \"Network.getResponseBody\", \"params\": {\"requestId\": \"...\"}},\n        {\"id\": 3, \"method\": \"DOM.getDocument\"}\n    ]\n    \n    results = await asyncio.gather(\n        *(connection.execute_command(cmd) for cmd in commands)\n    )\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Element Property Access Implementation in WebElement\nDESCRIPTION: Examples of synchronous and asynchronous property implementations in WebElement. The text property is asynchronous and retrieves live data from the DOM using BeautifulSoup, while the id property is synchronous and returns from the cached attributes dictionary.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/webelement-domain.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@property\nasync def text(self) -> str:\n    \"\"\"\n    Retrieves the text of the element.\n    \"\"\"\n    outer_html = await self.inner_html\n    soup = BeautifulSoup(outer_html, 'html.parser')\n    return soup.get_text(strip=True)\n\n@property\ndef id(self) -> str:\n    \"\"\"\n    Retrieves the id of the element.\n    \"\"\"\n    return self._attributes.get('id')\n```\n\n----------------------------------------\n\nTITLE: Capturing Element Screenshot in Python\nDESCRIPTION: This snippet demonstrates how to take a screenshot of a specific element using CDP commands. It involves getting the element's bounds, creating a clip region, and saving the image to a specified path.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/webelement-domain.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def get_screenshot(self, path: str):\n    \"\"\"\n    Takes a screenshot of the element.\n    \"\"\"\n    bounds = await self.get_bounds_using_js()\n    clip = {\n        'x': bounds['x'],\n        'y': bounds['y'],\n        'width': bounds['width'],\n        'height': bounds['height'],\n        'scale': 1,\n    }\n    screenshot = await self._connection_handler.execute_command(\n        PageCommands.screenshot(fmt='jpeg', clip=clip)\n    )\n    async with aiofiles.open(path, 'wb') as file:\n        image_bytes = decode_image_to_bytes(screenshot['result']['data'])\n        await file.write(image_bytes)\n```\n\n----------------------------------------\n\nTITLE: Navigating and Monitoring URLs in Python\nDESCRIPTION: Examples of Page domain navigation methods including going to a URL with custom timeout, getting the current URL, and refreshing the page. These methods abstract the complexities of browser navigation.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Navigate to a page with custom timeout\nawait page.go_to(\"https://example.com\", timeout=60)\n\n# Get the current URL\ncurrent_url = await page.current_url\nprint(f\"Current URL: {current_url}\")\n\n# Refresh the page\nawait page.refresh()\n```\n\n----------------------------------------\n\nTITLE: Backspace Keyboard Interaction in WebElement\nDESCRIPTION: The backspace method implementation which tracks input state to enable deleting previously entered text. It iteratively sends the Backspace key and updates the internal _last_input tracking variable.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/webelement-domain.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync def backspace(self, interval: float = 0.1):\n    \"\"\"\n    Backspaces the key at the element.\n    \"\"\"\n    for _ in range(len(self._last_input)):\n        await self.send_keys(('Backspace', 8))\n        await asyncio.sleep(interval)\n        self._last_input = self._last_input[:-1]\n```\n\n----------------------------------------\n\nTITLE: Browser vs Page Event Structure Diagram in Pydoll\nDESCRIPTION: A diagram showing the relationship between browser-level and page-level events in Pydoll. Illustrates how events are organized hierarchically between the browser and individual page instances.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_20\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    Browser[Browser Instance] -->|\"Global Events (e.g., Target events)\"| BrowserCallbacks[Browser-Level Callbacks]\n    Browser -->|\"Creates\"| Page1[Page Instance 1]\n    Browser -->|\"Creates\"| Page2[Page Instance 2]\n    Page1 -->|\"Page-Specific Events\"| Page1Callbacks[Page 1 Callbacks]\n    Page2 -->|\"Page-Specific Events\"| Page2Callbacks[Page 2 Callbacks]\n```\n\n----------------------------------------\n\nTITLE: Handling Page Domain Events in Pydoll\nDESCRIPTION: Demonstrates how to enable and handle page domain events like page loading and JavaScript dialogs. Uses partial functions to maintain reference to the page instance in event callbacks.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n# Enable page events\nawait page.enable_page_events()\n\n# Handle page load\nasync def handle_page_load(page, event):\n    print(f\"Page loaded: {await page.current_url}\")\n    # Perform actions after page load\n    await page.find_element(By.ID, \"search\").type_keys(\"pydoll\")\n\nawait page.on(PageEvents.PAGE_LOADED, partial(handle_page_load, page))\n\n# Handle JavaScript dialogs\nasync def handle_dialog(page, event):\n    if await page.has_dialog():\n        message = await page.get_dialog_message()\n        print(f\"Dialog message: {message}\")\n        await page.accept_dialog()\n\nawait page.on(PageEvents.JS_DIALOG_OPENING, partial(handle_dialog, page))\n```\n\n----------------------------------------\n\nTITLE: Event Callback Examples in Pydoll\nDESCRIPTION: Examples of synchronous and asynchronous callbacks for handling page and network events. Shows how to define callbacks and register them with the on() method.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Synchronous callback example\ndef handle_page_load(event):\n    print(f\"Page loaded at: {time.time()}\")\n\n# Asynchronous callback example\nasync def handle_network_request(event):\n    request_url = event['params']['request']['url']\n    print(f\"Request sent to: {request_url}\")\n    # Can perform async operations here\n    await save_request_details(request_url)\n\n# Register the callbacks\nawait page.on('Page.loadEventFired', handle_page_load)\nawait page.on('Network.requestWillBeSent', handle_network_request)\n```\n\n----------------------------------------\n\nTITLE: Network Event Cleanup Example in Pydoll\nDESCRIPTION: Demonstrates proper cleanup of network events to prevent memory leaks and improve performance. Shows selective event enabling and proper disable sequence.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Enable events only when needed\nawait page.enable_network_events()\nawait page.enable_fetch_events(resource_type='XHR')  # Only intercept XHR requests\n\n# Do your automation work...\n\n# Clean up when done\nawait page.disable_network_events()\nawait page.disable_fetch_events()\n```\n\n----------------------------------------\n\nTITLE: Advanced Navigation with Event Listeners in Python\nDESCRIPTION: An example of combining navigation with event listeners to monitor network activity during page load. This allows for more sophisticated automation patterns with callback-based event handling.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Listen for network requests during navigation\nawait page.enable_network_events()\nawait page.on('Network.responseReceived', handle_response)\n\n# Navigate to the page\nawait page.go_to('https://example.com')\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Types for Request Interception in PyDoll\nDESCRIPTION: Shows different ways to filter request interception by resource type. This allows for more targeted interception based on the type of resource being requested (documents, XHR, images, etc.). This helps reduce overhead by only intercepting specific types of requests.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Intercept all requests (could be resource-intensive)\nawait page.enable_fetch_events(resource_type='')\n\n# Intercept only document (HTML) requests\nawait page.enable_fetch_events(resource_type='Document')\n\n# Intercept only XHR/fetch API requests\nawait page.enable_fetch_events(resource_type='XHR')\n\n# Intercept only image requests\nawait page.enable_fetch_events(resource_type='Image')\n```\n\n----------------------------------------\n\nTITLE: Context Manager Implementation Example\nDESCRIPTION: Shows how to use the browser with Python's context manager pattern for automatic resource cleanup.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def context_manager_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to(\"https://example.com\")\n        # The browser is automatically closed when exiting the context\n\nasyncio.run(context_manager_example())\n```\n\n----------------------------------------\n\nTITLE: Using Partial for Page Access in Callbacks\nDESCRIPTION: Demonstrates using functools.partial to provide the Page instance to callbacks, allowing them to interact with the page. This technique is useful when callbacks need to find elements or execute scripts on the page.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n# Define a callback that needs access to the page\nasync def handle_navigation(page, event):\n    # The callback can now use the page object\n    print(f\"Navigation occurred to: {await page.current_url}\")\n    \n    # Access page methods directly\n    elements = await page.find_elements(By.TAG_NAME, \"a\")\n    print(f\"Found {len(elements)} links on the new page\")\n\n# Register with partial to bind the page parameter\nawait page.enable_page_events()\nawait page.on(PageEvents.FRAME_NAVIGATED, partial(handle_navigation, page))\n```\n\n----------------------------------------\n\nTITLE: Executing JavaScript with Element Context\nDESCRIPTION: Demonstrates how to execute JavaScript code with an element as the context using the page.execute_script method. The JavaScript code modifies the element's style and content.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.execute_script(\"\"\"\n    // 'argument' refers to the element\n    argument.style.color = 'red';\n    argument.style.fontSize = '32px';\n    argument.textContent = 'Modified by JavaScript';\n\"\"\", heading)\n```\n\n----------------------------------------\n\nTITLE: Browser Lifecycle Management Example\nDESCRIPTION: Demonstrates context manager usage for automatic browser resource cleanup.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def scrape_data():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://example.com')\n        # Work with page...\n        # Browser automatically closes when exiting the context\n```\n\n----------------------------------------\n\nTITLE: Implementing By to Selector Conversion in FindElementsMixin\nDESCRIPTION: Method implementation showing how the FindElementsMixin converts By enum values to appropriate selector strings for use with CDP commands.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef _from_by_to_selector(self, by: By, value: str) -> str:\n    \"\"\"Converts a By enum and value to the appropriate selector string.\"\"\"\n    if by == By.ID:\n        return f'#{value}'\n    elif by == By.CLASS_NAME:\n        return f'.{value}'\n    elif by == By.TAG_NAME:\n        return value\n    # ... other conversions\n    else:\n        return value  # CSS_SELECTOR and XPATH use the value directly\n```\n\n----------------------------------------\n\nTITLE: Connection Handler Class Initialization in Python\nDESCRIPTION: Defines the initialization method for the ConnectionHandler class that manages WebSocket connections to the Chrome DevTools Protocol. Includes parameter definitions for connection port, page ID, and custom resolvers.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n    self,\n    connection_port: int,\n    page_id: str = 'browser',\n    ws_address_resolver: Callable[[int], str] = get_browser_ws_address,\n    ws_connector: Callable = websockets.connect,\n):\n    # Initialize components...\n```\n\n----------------------------------------\n\nTITLE: Common Chrome Command Line Arguments for Pydoll\nDESCRIPTION: Collection of frequently used Chrome command line arguments that can be added to Options for customizing browser behavior, including performance, appearance, network, and privacy settings.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/index.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Performance & Behavior Options\noptions.add_argument('--headless=new')         # Run Chrome in headless mode\noptions.add_argument('--disable-gpu')          # Disable GPU hardware acceleration\noptions.add_argument('--no-sandbox')           # Disable sandbox (use with caution)\noptions.add_argument('--disable-dev-shm-usage') # Overcome limited resource issues\n\n# Appearance Options\noptions.add_argument('--start-maximized')      # Start with maximized window\noptions.add_argument('--window-size=1920,1080') # Set specific window size\noptions.add_argument('--hide-scrollbars')      # Hide scrollbars\n\n# Network Options\noptions.add_argument('--proxy-server=socks5://127.0.0.1:9050') # Use proxy\noptions.add_argument('--disable-extensions')   # Disable extensions\noptions.add_argument('--disable-notifications') # Disable notifications\n\n# Privacy & Security\noptions.add_argument('--incognito')            # Run in incognito mode\noptions.add_argument('--disable-infobars')     # Disable infobars\n```\n\n----------------------------------------\n\nTITLE: Implementing a Mixin in Python for Logging Functionality\nDESCRIPTION: An example demonstrating how mixins work in Python by creating a LoggerMixin that provides logging capabilities to other classes through composition. This illustrates the 'has-a' relationship rather than an 'is-a' relationship.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example of a mixin in Python\nclass LoggerMixin:\n    def log(self, message):\n        print(f\"LOG: {message}\")\n        \n    def log_error(self, error):\n        print(f\"ERROR: {error}\")\n\nclass DataProcessor(LoggerMixin):\n    def process_data(self, data):\n        self.log(\"Processing data...\")\n        # Process the data\n        self.log(\"Data processing complete\")\n```\n\n----------------------------------------\n\nTITLE: Download Path Configuration\nDESCRIPTION: Shows how to configure and handle browser downloads\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Set a custom download path\ndownload_path = \"/path/to/downloads\"\nawait browser.set_download_path(download_path)\n\n# Navigate to a page with downloadable content\npage = await browser.get_page()\nawait page.go_to(\"https://example.com/download\")\n\n# Click a download link\ndownload_link = await page.find_element(By.ID, \"download-button\")\nawait download_link.click()\n```\n\n----------------------------------------\n\nTITLE: Temporary Event Callbacks in Pydoll\nDESCRIPTION: Shows how to register a temporary callback that is automatically removed after firing once. This is useful for one-time setup operations or waiting for specific events.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# This callback will automatically be removed after the first time it fires\nawait page.on('Page.loadEventFired', handle_first_load, temporary=True)\n```\n\n----------------------------------------\n\nTITLE: Launching Chrome with Remote Debugging for CDP Connection\nDESCRIPTION: Command to start Chrome with the remote debugging flag enabled, which opens a WebSocket server on port 9222 that Pydoll can connect to for establishing a bidirectional communication channel.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/cdp.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nchrome --remote-debugging-port=9222\n```\n\n----------------------------------------\n\nTITLE: Process Manager Implementation\nDESCRIPTION: Shows the structure of the BrowserProcessManager class for handling browser process lifecycle.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BrowserProcessManager:\n    def start_browser_process(self, binary, port, arguments):\n        # Launch browser executable with proper arguments\n        # Monitor process startup\n        # ...\n        \n    def stop_process(self):\n        # Terminate browser process\n        # Cleanup resources\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Request Continuation Handler in Pydoll\nDESCRIPTION: Demonstrates how to implement a request interception handler that continues requests after making modifications like adding custom headers.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nasync def intercept_handler(page, event):\n    request_id = event['params']['requestId']\n    \n    # Make any modifications needed\n    custom_headers = { ... }\n    \n    # Continue the request\n    await page._execute_command(\n        FetchCommands.continue_request(\n            request_id=request_id,\n            headers=custom_headers\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Request Interception Flow using Mermaid Diagram\nDESCRIPTION: A sequence diagram visualizing the flow of request interception in Pydoll, showing the interactions between Application Code, Pydoll Library, Browser, and Web Server during the interception and modification process.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_5\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant App as Application Code\n    participant Pydoll as Pydoll Library\n    participant Browser as Browser\n    participant Server as Web Server\n    \n    App->>Pydoll: Enable fetch events\n    Pydoll->>Browser: FetchCommands.enable_fetch_events()\n    Browser-->>Pydoll: Enabled\n    \n    App->>Pydoll: Register callback for REQUEST_PAUSED\n    \n    App->>Pydoll: Navigate to URL\n    Pydoll->>Browser: Navigate command\n    Browser->>Browser: Initiates request\n    Browser->>Pydoll: Fetch.requestPaused event\n    Pydoll->>App: Execute callback\n    \n    App->>Pydoll: Modify and continue request\n    Pydoll->>Browser: FetchCommands.continue_request() with modifications\n    Browser->>Server: Modified request\n    \n    Server-->>Browser: Response\n    Browser-->>Pydoll: Complete\n    Pydoll-->>App: Continue execution\n```\n\n----------------------------------------\n\nTITLE: Using Partial Functions for Request Handler Configuration in Pydoll\nDESCRIPTION: Shows how to use functools.partial to create pre-configured request handlers with additional parameters and clean callback management.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\n# Define your handler with page object as first parameter\nasync def handle_request(page, config, event):\n    # Now you have access to both page and custom config\n    \n# Register with partial to pre-bind parameters\nconfig = {\"headers_to_add\": {\"X-Custom\": \"Value\"}}\nawait page.on(\n    FetchEvents.REQUEST_PAUSED, \n    partial(handle_request, page, config)\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering Network Logs in Pydoll\nDESCRIPTION: Code snippet showing how to filter network logs in Pydoll based on URL patterns, enabling focused analysis of specific types of requests such as API calls or image resources.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Get all network logs\nall_logs = await page.get_network_logs()\n\n# Filter logs by URL pattern\napi_logs = await page.get_network_logs(matches=['api'])\nimage_logs = await page.get_network_logs(matches=['.jpg', '.png', '.gif'])\n```\n\n----------------------------------------\n\nTITLE: Visualizing Pydoll's Asynchronous Architecture with Mermaid\nDESCRIPTION: A complex diagram illustrating Pydoll's asynchronous architecture using Python's asyncio framework, showing how multiple operations can be executed concurrently through a single WebSocket connection to the browser.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/cdp.md#2025-04-19_snippet_5\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    subgraph \"Pydoll Async Architecture\"\n        EL[Event Loop]\n        \n        subgraph \"Concurrent Tasks\"\n            T1[Task 1: Navigate]\n            T2[Task 2: Wait for Element]\n            T3[Task 3: Handle Network Events]\n        end\n        \n        EL --> T1\n        EL --> T2\n        EL --> T3\n        \n        T1 --> WS[WebSocket Connection]\n        T2 --> WS\n        T3 --> WS\n        \n        WS --> B[Browser]\n    end\n```\n\n----------------------------------------\n\nTITLE: Finding High-Scoring Results in Specific Category Using XPath\nDESCRIPTION: XPath query to locate search results with numerical scores >= 90 and a specific technology category, demonstrating numerical evaluation with multiple conditions.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_elements(\n    By.XPATH,\n    \"//div[@class='result'][number(div[@class='score']) >= 90 and .//span[@class='category']='Technology']\"\n)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Event Handling Performance\nDESCRIPTION: Demonstrates best practices for optimizing performance by enabling only the necessary event domains. Selective event enabling reduces processing overhead and improves automation script efficiency.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# GOOD: Enable only what you need\nawait page.enable_network_events()  # Only enable network events\n\n# BAD: Enabling unnecessary events creates overhead\nawait page.enable_page_events()\nawait page.enable_network_events()\nawait page.enable_dom_events()\nawait page.enable_fetch_events()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Domain Relationships with Mermaid\nDESCRIPTION: A Mermaid diagram showing the hierarchical relationship between the Browser, Page, and WebElement domains in Pydoll. It illustrates how Browser manages Pages, which in turn locate and create WebElements.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_19\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    Browser[\"Browser Domain<br>(Browser management)\"]\n    Page[\"Page Domain<br>(Page interaction)\"]\n    Element[\"WebElement Domain<br>(Element interaction)\"]\n    \n    Browser -->|\"creates and manages\"| Page\n    Page -->|\"locates and creates\"| Element\n```\n\n----------------------------------------\n\nTITLE: Finding Most Recent Element with XPath Comparison\nDESCRIPTION: XPath query that locates the newest employee by using a negation predicate with date comparison to find rows where no other row has a later date.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_element(\n    By.XPATH,\n    \"//table[@id='employees']//tr[not(//tr/td[5] > td[5])]\"\n)\n```\n\n----------------------------------------\n\nTITLE: Using Predefined Event Constants in Pydoll\nDESCRIPTION: Demonstrates how to use predefined event constants from the events package to handle common browser events like page loads, network requests, DOM updates, and fetch intercepts.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydoll.events import PageEvents, NetworkEvents, DOMEvents, FetchEvents\n\n# Using predefined events\nawait page.on(PageEvents.PAGE_LOADED, handle_page_load)\nawait page.on(NetworkEvents.REQUEST_WILL_BE_SENT, handle_request)\nawait page.on(DOMEvents.DOCUMENT_UPDATED, handle_dom_update)\nawait page.on(FetchEvents.REQUEST_PAUSED, handle_fetch_intercept)\n```\n\n----------------------------------------\n\nTITLE: Message Processing Pipeline Diagram for ConnectionHandler\nDESCRIPTION: Mermaid sequence diagram illustrating the sophisticated message processing pipeline that handles the continuous stream of messages from the WebSocket connection.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_9\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant WS as WebSocket\n    participant RCV as _receive_events\n    participant MSG as _process_single_message\n    participant PARSE as _parse_message\n    participant CMD as _handle_command_message\n    participant EVT as _handle_event_message\n    \n    loop While connected\n        WS->>RCV: message\n        RCV->>MSG: raw_message\n        MSG->>PARSE: raw_message\n        PARSE-->>MSG: parsed JSON or None\n        \n        alt Is command response\n            MSG->>CMD: message\n            CMD->>CMD: resolve command future\n        else Is event notification\n            MSG->>EVT: message\n            EVT->>EVT: process event & trigger callbacks\n        end\n    end\n```\n\n----------------------------------------\n\nTITLE: Custom Command Execution in Python using ConnectionHandler\nDESCRIPTION: Shows how to execute arbitrary Chrome DevTools Protocol commands directly using the ConnectionHandler, including an example of getting document HTML without using the Page class.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nasync def custom_cdp_command(connection, method, params=None):\n    command = {\n        \"id\": random.randint(1, 10000),\n        \"method\": method,\n        \"params\": params or {}\n    }\n    return await connection.execute_command(command)\n\n# Example: Get document HTML without using Page class\nasync def get_html(connection):\n    result = await custom_cdp_command(\n        connection,\n        \"Runtime.evaluate\",\n        {\"expression\": \"document.documentElement.outerHTML\"}\n    )\n    return result['result']['result']['value']\n```\n\n----------------------------------------\n\nTITLE: Registering Event Callbacks with on() Method\nDESCRIPTION: Shows the signature of the on() method used to register event callbacks. This method is available on both Page and Browser instances and returns a callback ID that can be used for later removal.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def on(\n    self, event_name: str, callback: callable, temporary: bool = False\n) -> int:\n    \"\"\"\n    Registers an event listener for the page.\n\n    Args:\n        event_name (str): The event name to listen for.\n        callback (callable): The callback function to execute when the\n            event is triggered.\n        temporary (bool): If True, the callback will be removed after it's\n            triggered once. Defaults to False.\n\n    Returns:\n        int: The ID of the registered callback.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Finding Elements with Multiple Conditions Using Chained XPath Predicates\nDESCRIPTION: XPath query that uses multiple predicates to find table rows matching specific criteria (engineering department with salary over 80000), demonstrating numerical comparison.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_elements(\n    By.XPATH,\n    \"//table[@id='employees']//tr[td[3]='Engineering' and number(translate(td[4], '$,', '')) > 80000]\"\n)\n```\n\n----------------------------------------\n\nTITLE: Direct Event Monitoring in Python using ConnectionHandler\nDESCRIPTION: Demonstrates how to directly monitor specific Chrome DevTools Protocol events using the ConnectionHandler for advanced scenarios.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pydoll.connection.connection import ConnectionHandler\n\nasync def monitor_network():\n    connection = ConnectionHandler(9222)\n    \n    async def log_request(event):\n        url = event['params']['request']['url']\n        print(f\"Request: {url}\")\n    \n    await connection.register_callback(\n        'Network.requestWillBeSent', \n        log_request\n    )\n    \n    # Enable network events via CDP command\n    await connection.execute_command({\n        \"id\": 1,\n        \"method\": \"Network.enable\"\n    })\n    \n    # Keep running until interrupted\n    try:\n        while True:\n            await asyncio.sleep(1)\n    finally:\n        await connection.close()\n```\n\n----------------------------------------\n\nTITLE: Ensuring Active Connection in Python for ConnectionHandler\nDESCRIPTION: Internal method to guarantee an active WebSocket connection exists before proceeding with operations, implementing automatic reconnection.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def _ensure_active_connection(self):\n    \"\"\"\n    Guarantees that an active connection exists before proceeding.\n    \"\"\"\n    if self._ws_connection is None or self._ws_connection.closed:\n        await self._establish_new_connection()\n```\n\n----------------------------------------\n\nTITLE: Finding Most Recent Result Using Comparative XPath\nDESCRIPTION: XPath query to find the most recent search result by comparing date fields across all results, demonstrating advanced comparison logic.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_element(\n    By.XPATH,\n    \"//div[@class='result'][not(//div[@class='result']/div[@class='metadata']/span[@class='date'] > ./div[@class='metadata']/span[@class='date'])]\"\n)\n```\n\n----------------------------------------\n\nTITLE: Closing Connection in Python for ConnectionHandler\nDESCRIPTION: Method to close the WebSocket connection and clear all callbacks, ensuring proper resource cleanup.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def close(self):\n    \"\"\"\n    Closes the WebSocket connection and clears all callbacks.\n    \"\"\"\n    await self.clear_callbacks()\n    if self._ws_connection is not None:\n        try:\n            await self._ws_connection.close()\n        except websockets.ConnectionClosed as e:\n            logger.info(f'WebSocket connection has closed: {e}')\n        logger.info('WebSocket connection closed.')\n```\n\n----------------------------------------\n\nTITLE: Registering Page-Level Events in Pydoll\nDESCRIPTION: Shows how to register page-specific events in Pydoll and how to handle events for multiple pages independently.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Get a specific page\npage = await browser.get_page()\n\n# Register a page-level event\nawait page.enable_page_events()\nawait page.on(PageEvents.PAGE_LOADED, handle_page_load)\n\n# Each page has its own event context\npage2 = await browser.get_page()\nawait page2.enable_page_events()\nawait page2.on(PageEvents.PAGE_LOADED, handle_different_page_load)\n```\n\n----------------------------------------\n\nTITLE: Options Manager Implementation\nDESCRIPTION: Shows the BrowserOptionsManager class for handling option validation and defaults.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass BrowserOptionsManager:\n    @staticmethod\n    def initialize_options(options, browser_type):\n        # Create options instance if None\n        # Set appropriate defaults\n        # ...\n        \n    @staticmethod\n    def add_default_arguments(options):\n        # Add required CDP arguments\n        # Configure automation settings\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Development Workflow with Auto-Reconnect\nDESCRIPTION: Shows a development pattern with automatic reconnection on errors\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nbrowser = Chrome(connection_port=9222)\n\nwhile True:\n    try:\n        page = await browser.connect()\n        # Test your automation code\n        # No need to restart browser between runs\n        break\n    except Exception as e:\n        print(f\"Error: {e}, retrying...\")\n```\n\n----------------------------------------\n\nTITLE: Custom CDP Event Handling\nDESCRIPTION: Shows how to use direct CDP event strings for handling less common events that don't have predefined constants.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Using a direct CDP event string\nawait page.on('Security.certificateError', handle_cert_error)\n```\n\n----------------------------------------\n\nTITLE: Proxy Manager Implementation\nDESCRIPTION: Shows the ProxyManager class for handling browser proxy configuration.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass ProxyManager:\n    def __init__(self, options):\n        # Parse proxy settings from options\n        # ...\n        \n    def get_proxy_credentials(self):\n        # Extract authentication details\n        # Format proxy configuration\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Connecting to Remote Chrome Browser\nDESCRIPTION: Demonstrates how to connect to a Chrome instance running with remote debugging enabled\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/browser-domain.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbrowser = Chrome(connection_port=9222)\npage = await browser.connect()\n```\n\n----------------------------------------\n\nTITLE: Handling Browser-Level Event Exceptions in Pydoll\nDESCRIPTION: Shows an example of code that would raise an exception when attempting to use page-specific events at the browser level in Pydoll.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# This would raise an EventNotSupported exception\nawait browser.on(PageEvents.PAGE_LOADED, handle_page_load)  # Error!\n```\n\n----------------------------------------\n\nTITLE: Installing Pydoll via pip\nDESCRIPTION: Command to install the Pydoll Python package using pip package manager. This is the primary installation method for the library.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pydoll-python\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for XPath Predicate Examples\nDESCRIPTION: HTML table structure containing employee data used to demonstrate XPath's capabilities with multiple predicates and functions.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_9\n\nLANGUAGE: html\nCODE:\n```\n<table id=\"employees\">\n  <thead>\n    <tr>\n      <th>ID</th>\n      <th>Name</th>\n      <th>Department</th>\n      <th>Salary</th>\n      <th>Start Date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1001</td>\n      <td>John Smith</td>\n      <td>Engineering</td>\n      <td>75000</td>\n      <td>2019-04-15</td>\n    </tr>\n    <tr>\n      <td>1002</td>\n      <td>Maria Garcia</td>\n      <td>Marketing</td>\n      <td>70000</td>\n      <td>2020-08-01</td>\n    </tr>\n    <tr>\n      <td>1003</td>\n      <td>Ahmed Khan</td>\n      <td>Engineering</td>\n      <td>85000</td>\n      <td>2018-02-12</td>\n    </tr>\n  </tbody>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Listing Core Dependencies for Pydoll in Python\nDESCRIPTION: Specifies the minimal set of dependencies required for Pydoll, including Python version and key packages. This approach ensures faster installation, fewer conflicts, and better security.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/index.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython = \"^3.10\"\nwebsockets = \"^13.1\"\naiohttp = \"^3.9.5\"\naiofiles = \"^23.2.1\"\nbs4 = \"^0.0.2\"\n```\n\n----------------------------------------\n\nTITLE: Cloning the PyDoll Repository\nDESCRIPTION: Instructions for cloning the PyDoll repository and navigating to the project directory. This is the first step in setting up the development environment.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/CONTRIBUTING.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone [REPOSITORY_URL]\ncd pydoll\n```\n\n----------------------------------------\n\nTITLE: Finding Elements After Reference Using XPath Following Axis\nDESCRIPTION: XPath query to locate all price elements that appear after a specific product in the document using the following axis.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_elements(By.XPATH, \"//div[@class='product' and contains(.,'Laptop')]/following::p[@class='price']\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Page Domain Core Structure with Mermaid\nDESCRIPTION: A graph diagram showing how the Page domain acts as a bridge between user code and core browser capabilities including navigation, element operations, JavaScript execution, event systems, and session management.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TB\n    User[User Code] --> Page[Page Domain]\n    \n    subgraph \"Core Capabilities\"\n        Page --> Nav[Navigation]\n        Page --> Elements[Element Operations]\n        Page --> JS[JavaScript Execution]\n        Page --> Events[Event System]\n        Page --> State[Session Management]\n    end\n    \n    Nav & Elements & JS --> Website[Website]\n    Events <--> Website\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry\nDESCRIPTION: Command to install project dependencies using Poetry, which is the required dependency management tool for the project.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/CONTRIBUTING.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Registering Browser-Level Events in Pydoll\nDESCRIPTION: Demonstrates how to register browser-level events in Pydoll and explains the limitations of using page-specific events at the browser level.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Register a browser-level event\nawait browser.on('Target.targetCreated', handle_new_target)\n```\n\n----------------------------------------\n\nTITLE: Defining Page Domain Technical Architecture with Class Diagram\nDESCRIPTION: A class diagram illustrating the Page domain's implementation, showing relationships between Page, FindElementsMixin, ConnectionHandler, and WebElement classes. It visualizes the inheritance and composition patterns used in the architecture.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nclassDiagram\n    class Page {\n        -_connection_handler: ConnectionHandler\n        -_page_events_enabled: bool\n        -_network_events_enabled: bool\n        -_fetch_events_enabled: bool\n        -_dom_events_enabled: bool\n        -_intercept_file_chooser_dialog_enabled: bool\n        -_cloudflare_captcha_callback_id: Any\n        +go_to(url: str, timeout: int)\n        +refresh()\n        +execute_script(script: str, element: WebElement)\n        +find_element(by: By, value: str)\n        +find_elements(by: By, value: str)\n        +wait_element(by: By, value: str, timeout: int)\n        +get_screenshot(path: str)\n        +print_to_pdf(path: str)\n        +enable_page_events()\n        +enable_network_events()\n        +on(event_name: str, callback: callable)\n    }\n    \n    class FindElementsMixin {\n        +find_element(by: By, value: str)\n        +find_elements(by: By, value: str)\n        +wait_element(by: By, value: str, timeout: int)\n    }\n    \n    class ConnectionHandler {\n        +execute_command(command: dict)\n        +register_callback(event_name: str, callback: callable)\n    }\n    \n    class WebElement {\n        -_connection_handler: ConnectionHandler\n        -_element_id: str\n        -_object_id: str\n        +click()\n        +type_keys(text: str)\n        +get_attribute(name: str)\n    }\n    \n    Page --|> FindElementsMixin : inherits\n    Page *-- ConnectionHandler : uses\n    Page ..> WebElement : creates\n    WebElement *-- ConnectionHandler : uses\n```\n\n----------------------------------------\n\nTITLE: Activating the Poetry Virtual Environment\nDESCRIPTION: Command to activate the Poetry-managed virtual environment for isolated development.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/CONTRIBUTING.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npoetry shell\n```\n\n----------------------------------------\n\nTITLE: Pydoll's Streamlined CDP Architecture Flowchart\nDESCRIPTION: A diagram showing Pydoll's streamlined approach to browser automation using direct CDP communication, eliminating the intermediate WebDriver server for improved performance.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/cdp.md#2025-04-19_snippet_4\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    AS[Automation Script] --> P[Pydoll]\n    P --> B[Browser via CDP]\n```\n\n----------------------------------------\n\nTITLE: Visualizing WebElement Domain Structure with Mermaid\nDESCRIPTION: A graph visualization showing how the WebElement domain connects user code to the Browser DOM, highlighting its key functional areas including properties, interactions, state, and text operations.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/webelement-domain.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TB\n    Client[User Code] --> Page[Page Domain]\n    Page --> FindElement[FindElementsMixin]\n    FindElement --> WebElement[WebElement Domain]\n    WebElement --> DOM[Browser DOM]\n    \n    WebElement --> Properties[Properties & Attributes]\n    WebElement --> Interactions[User Interactions]\n    WebElement --> State[Element State]\n    WebElement --> TextOperations[Text Operations]\n    \n    class WebElement stroke:#4CAF50,stroke-width:3px\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Command to execute all project tests using pytest, which also generates an HTML coverage report.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/CONTRIBUTING.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npoetry run task test\n```\n\n----------------------------------------\n\nTITLE: Traditional Webdriver Architecture Flowchart with Mermaid\nDESCRIPTION: A diagram showing the multi-layered approach of traditional webdriver-based tools like Selenium, highlighting the additional overhead of the WebDriver server acting as a translation layer.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/cdp.md#2025-04-19_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    AS[Automation Script] --> WC[WebDriver Client]\n    WC --> WS[WebDriver Server]\n    WS --> B[Browser]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Chrome DevTools Protocol Integration with Sequence Diagram\nDESCRIPTION: A sequence diagram showing the interaction flow between user code, the Page domain, Chrome DevTools Protocol, and the browser when navigating to a URL and finding an element. It illustrates the asynchronous message passing between components.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant Client as User Code\n    participant Page as Page Domain\n    participant CDP as Chrome DevTools Protocol\n    participant Browser as Browser\n    \n    Client->>Page: await page.go_to(\"https://example.com\")\n    Page->>CDP: Navigation.navigate\n    CDP->>Browser: Execute navigation\n    \n    Browser-->>CDP: Page.loadEventFired\n    CDP-->>Page: Event notification\n    Page-->>Client: Navigation completed\n    \n    Client->>Page: await page.find_element(By.ID, \"login\")\n    Page->>CDP: DOM.querySelector\n    CDP->>Browser: Execute DOM query\n    Browser-->>CDP: Return element\n    CDP-->>Page: Element response\n    Page->>Page: Create WebElement\n    Page-->>Client: Return WebElement\n```\n\n----------------------------------------\n\nTITLE: Running Code Linting\nDESCRIPTION: Command to run the linter (Ruff) on the codebase to check for code style and potential issues.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/CONTRIBUTING.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry run task lint\n```\n\n----------------------------------------\n\nTITLE: Subscribing to CDP Network Events in Pydoll\nDESCRIPTION: Python code example demonstrating how to enable network event monitoring in Pydoll and register a callback function to handle network request events. This showcases Pydoll's event-driven approach to browser automation.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/cdp.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def on_request(page, event):\n    url = event['params']['request']['url']\n    print(f\"Request to: {url}\")\n\n# Subscribe to network request events\nawait page.enable_network_events()\nawait page.on(NetworkEvents.REQUEST_WILL_BE_SENT, partial(on_request, page))\n```\n\n----------------------------------------\n\nTITLE: WebSocket Communication and CDP Flow Diagram\nDESCRIPTION: A sequence diagram showing the communication flow between Pydoll code, ConnectionHandler, WebSocket, and Browser. Illustrates how callbacks are registered, events are enabled, and how event messages are processed.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant Client as Pydoll Code\n    participant Connection as ConnectionHandler\n    participant WebSocket\n    participant Browser\n    \n    Client->>Connection: Register callback for event\n    Connection->>Connection: Store callback in registry\n    \n    Client->>Connection: Enable event domain\n    Connection->>WebSocket: Send CDP command to enable domain\n    WebSocket->>Browser: Forward command\n    Browser-->>WebSocket: Acknowledge domain enabled\n    WebSocket-->>Connection: Forward response\n    Connection-->>Client: Domain enabled\n    \n    Browser->>WebSocket: Event occurs, sends CDP event message\n    WebSocket->>Connection: Forward event message\n    Connection->>Connection: Look up callbacks for this event\n    Connection->>Client: Execute registered callback\n```\n\n----------------------------------------\n\nTITLE: Formatting Code\nDESCRIPTION: Command to automatically format the code according to project standards using Ruff.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/CONTRIBUTING.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run task format\n```\n\n----------------------------------------\n\nTITLE: Enabling Network Monitoring in Pydoll\nDESCRIPTION: Code snippet demonstrating how to enable network monitoring in Pydoll, including starting a Chrome browser, enabling network events, navigating to a page, and retrieving network logs.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        \n        # Enable network monitoring\n        await page.enable_network_events()\n        \n        # Navigate to a page\n        await page.go_to('https://example.com')\n        \n        # Retrieve network logs\n        logs = await page.get_network_logs()\n        print(f\"Captured {len(logs)} network requests\")\n        \nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Event Flow and Lifecycle Diagram\nDESCRIPTION: Flowchart showing the complete event processing flow from browser activity to callback execution. Illustrates how events are generated, filtered, and processed through the system.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_7\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    A[Browser Activity] -->|Generates| B[CDP Event]\n    B -->|Sent via WebSocket| C[ConnectionHandler]\n    C -->|Filters by Event Name| D{Registered Callbacks?}\n    D -->|Yes| E[Process Event]\n    D -->|No| F[Discard Event]\n    E -->|For Each Callback| G[Execute Callback]\n    G -->|If Temporary| H[Remove Callback]\n    G -->|If Permanent| I[Retain for Future Events]\n```\n\n----------------------------------------\n\nTITLE: Clearing Callbacks in Python for ConnectionHandler\nDESCRIPTION: Demonstrates how to remove all registered callbacks from a ConnectionHandler instance.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait connection.clear_callbacks()\n```\n\n----------------------------------------\n\nTITLE: Network Architecture Overview using Mermaid Diagram\nDESCRIPTION: A flowchart visualization of Pydoll's network architecture showing the relationships between Chrome/Edge Browser, Chrome DevTools Protocol, Pydoll Library, and User Automation Code components.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TB\n    subgraph Browser[\"Chrome/Edge Browser\"]\n        Net[\"Network Stack\"] --> CDP[\"Chrome DevTools Protocol\"]\n    end\n    \n    subgraph Pydoll[\"Pydoll Library\"]\n        CDP --> NetMon[\"Network Monitoring\"]\n        CDP --> Interception[\"Request Interception\"]\n        CDP --> Headers[\"Headers Manipulation\"]\n        CDP --> Body[\"Body Modification\"]\n        CDP --> Emulation[\"Network Condition Emulation\"]\n    end\n    \n    subgraph UserCode[\"User Automation Code\"]\n        NetMon --> Analysis[\"Traffic Analysis\"]\n        Interception --> Auth[\"Authentication Handling\"]\n        Headers --> CustomHeaders[\"Custom Headers Injection\"]\n        Body --> DataModification[\"Request/Response Data Modification\"] \n        Emulation --> Testing[\"Network Condition Testing\"]\n    end\n    \n    class Browser,Pydoll,UserCode rounded\n\n    \n    class Browser blue\n    class Pydoll green\n    class UserCode orange\n```\n\n----------------------------------------\n\nTITLE: Advanced XPath Selection with Conditional Expressions in Pydoll\nDESCRIPTION: Examples of complex XPath queries using logical operators to select elements based on multiple conditions. Demonstrates combining type checks, required attributes, and class selectors in various ways.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_elements(By.XPATH, \"//input[(@type='text' or @type='email') and @required]\")\n```\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_elements(By.XPATH, \"//button[@type='reset' or contains(@class, 'secondary')]\")\n```\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_element(By.XPATH, \"//button[@type='submit'] | //input[@type='submit']\")\n```\n\n----------------------------------------\n\nTITLE: Registering Temporary Callback in Python for ConnectionHandler\nDESCRIPTION: Shows how to register a temporary callback that will be automatically removed after being triggered once, useful for handling one-time events like dialogs.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/connection-layer.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nawait connection.register_callback(\n    'Page.javascriptDialogOpening',\n    handle_dialog,\n    temporary=True\n)\n```\n\n----------------------------------------\n\nTITLE: HTML Search Results Structure for Dynamic Content Examples\nDESCRIPTION: HTML structure showing search results with scores, metadata, and categories used to demonstrate XPath for dynamic content processing.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_12\n\nLANGUAGE: html\nCODE:\n```\n<div id=\"search-results\">\n  <div class=\"result\">\n    <div class=\"score\">98</div>\n    <h3>Result Title 1</h3>\n    <div class=\"metadata\">\n      <span class=\"author\">User123</span>\n      <span class=\"date\">2023-06-15</span>\n      <span class=\"category\">Technology</span>\n    </div>\n  </div>\n  <div class=\"result\">\n    <div class=\"score\">75</div>\n    <h3>Result Title 2</h3>\n    <div class=\"metadata\">\n      <span class=\"author\">Expert99</span>\n      <span class=\"date\">2023-04-22</span>\n      <span class=\"category\">Science</span>\n    </div>\n  </div>\n  <!-- More results... -->\n</div>\n```\n\n----------------------------------------\n\nTITLE: HTML Form Structure for Selector Examples\nDESCRIPTION: An HTML form example used to demonstrate various selector strategies. The form includes text inputs, email inputs, password field, and buttons with different attributes and classes.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<form id=\"registration\">\n  <div class=\"form-group\">\n    <label for=\"username\">Username</label>\n    <input type=\"text\" id=\"username\" name=\"username\" required>\n  </div>\n  <div class=\"form-group\">\n    <label for=\"email\">Email</label>\n    <input type=\"email\" id=\"email\" name=\"email\" required>\n  </div>\n  <div class=\"form-group\">\n    <label for=\"password\">Password</label>\n    <input type=\"password\" id=\"password\" name=\"password\" required>\n  </div>\n  <div class=\"actions\">\n    <button type=\"reset\" class=\"btn-secondary\">Reset</button>\n    <button type=\"submit\" class=\"btn-primary\">Register</button>\n  </div>\n</form>\n```\n\n----------------------------------------\n\nTITLE: Creating a Feature Branch\nDESCRIPTION: Git command for creating a new branch for feature development, following the naming convention 'feature/feature-name'.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/CONTRIBUTING.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b feature/your-feature-name\n```\n\n----------------------------------------\n\nTITLE: Attribute Management in WebElement\nDESCRIPTION: The _def_attributes method processes element attributes during creation, handling special cases like 'class' which is renamed to 'class_name' to avoid conflict with Python keywords. This enables fast synchronous access to common attributes.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/webelement-domain.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _def_attributes(self, attributes_list: list):\n    \"\"\"\n    Defines element attributes from a flat list of key-value pairs.\n    \"\"\"\n    for i in range(0, len(attributes_list), 2):\n        key = attributes_list[i]\n        key = key if key != 'class' else 'class_name'\n        value = attributes_list[i + 1]\n        self._attributes[key] = value\n```\n\n----------------------------------------\n\nTITLE: HTML Structure Example for XPath Navigation\nDESCRIPTION: HTML structure example showing a product catalog with nested categories and products, used to demonstrate XPath navigation capabilities.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<div id=\"products\">\n  <div class=\"category\">\n    <h2>Electronics</h2>\n    <div class=\"product\" data-id=\"e1\">\n      <h3>Smartphone</h3>\n      <p class=\"price\">$599</p>\n      <p class=\"description\">Latest model with high-resolution camera</p>\n      <button class=\"add-to-cart\">Add to Cart</button>\n    </div>\n    <div class=\"product\" data-id=\"e2\">\n      <h3>Laptop</h3>\n      <p class=\"price\">$999</p>\n      <p class=\"description\">Powerful processor with SSD storage</p>\n      <button class=\"add-to-cart\">Add to Cart</button>\n    </div>\n  </div>\n  <div class=\"category\">\n    <h2>Books</h2>\n    <div class=\"product\" data-id=\"b1\">\n      <h3>Python Programming</h3>\n      <p class=\"price\">$39</p>\n      <p class=\"description\">Learn Python from basics to advanced</p>\n      <button class=\"add-to-cart\">Add to Cart</button>\n    </div>\n  </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Creating a Fix Branch\nDESCRIPTION: Git command for creating a new branch for bug fixes, following the naming convention 'fix/fix-name'.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/CONTRIBUTING.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b fix/your-fix-name\n```\n\n----------------------------------------\n\nTITLE: Browser Automation with Pydoll without Context Manager\nDESCRIPTION: An alternative approach to using Pydoll for browser automation without the context manager pattern, requiring manual resource cleanup with browser.stop().\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/index.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def main():\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n    await page.go_to('https://github.com/autoscrape-labs/pydoll')\n    star_button = await page.wait_element(\n        By.XPATH, '//form[@action=\"/autoscrape-labs/pydoll/star\"]//button',\n        timeout=5,\n        raise_exc=False\n    )\n    if not star_button:\n        print(\"Ops! The button was not found.\")\n        return\n\n    await star_button.click()\n    await asyncio.sleep(3)\n    await browser.stop()\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Finding Category by Product Using XPath Ancestor Axis\nDESCRIPTION: XPath query to find the parent category element that contains a specific product by utilizing the ancestor axis.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_element(By.XPATH, \"//div[@class='product' and contains(.,'Python Programming')]/ancestor::div[@class='category']\")\n```\n\n----------------------------------------\n\nTITLE: Installing Pydoll via pip\nDESCRIPTION: Instructions for installing the Pydoll library using pip, either the stable version from PyPI or the development version directly from GitHub.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/index.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install pydoll-python\n\n---> 100%\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install git+https://github.com/autoscrape-labs/pydoll.git\n```\n\n----------------------------------------\n\nTITLE: Finding Products in Same Category Using XPath Ancestor and Descendant\nDESCRIPTION: XPath query to find all products within the same category as a reference product by traversing up with ancestor and then down with implicit descendant axis.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_elements(By.XPATH, \"//div[@class='product' and contains(.,'Python Programming')]/ancestor::div[@class='category']/div[@class='product']\")\n```\n\n----------------------------------------\n\nTITLE: Event Domain State Diagram\nDESCRIPTION: State diagram showing the lifecycle of event domains in Pydoll, from disabled to enabled states. Shows how domains are enabled, disabled, and what happens when a page is closed.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/event-system.md#2025-04-19_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nstateDiagram-v2\n    [*] --> Disabled: Initial State\n    Disabled --> Enabled: enable_xxx_events()\n    Enabled --> Disabled: disable_xxx_events()\n    Enabled --> [*]: Page Closed\n    Disabled --> [*]: Page Closed\n```\n\n----------------------------------------\n\nTITLE: HTML Article Structure for Text Manipulation Examples\nDESCRIPTION: HTML structure containing an article with sections and paragraphs, used to demonstrate XPath text manipulation functions.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_15\n\nLANGUAGE: html\nCODE:\n```\n<div id=\"article\">\n  <h1>Data Analysis with Python</h1>\n  <p>This article discusses data analysis techniques using <span class=\"code\">pandas</span> and <span class=\"code\">numpy</span>.</p>\n  <div class=\"section\">\n    <h2>1. Data Preparation</h2>\n    <p>Before analysis, data must be cleaned...</p>\n  </div>\n  <div class=\"section\">\n    <h2>2. Exploratory Analysis</h2>\n    <p>Explore patterns using visualization...</p>\n  </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Finding Elements with Numeric Content Using XPath Translate\nDESCRIPTION: XPath query that uses the translate function to detect if a heading contains numbers, demonstrating character replacement for pattern detection.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_elements(\n    By.XPATH,\n    \"//div[@class='section'][contains(translate(h2, '0123456789', '##########'), '#')]\"\n)\n```\n\n----------------------------------------\n\nTITLE: Case-Insensitive Text Search Using XPath Translate\nDESCRIPTION: XPath query that performs case-insensitive text search for Python library names within paragraphs using the translate function to normalize case.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/find-elements-mixin.md#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nawait page.find_elements(\n    By.XPATH,\n    \"//p[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'pandas') or contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'numpy')]\"\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Page Instance in Python\nDESCRIPTION: The Page class constructor that initializes the connection handler and state flags for various event domains. It requires a connection port and page ID to establish communication with the browser tab.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/page-domain.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, connection_port: int, page_id: str):\n    \"\"\"\n    Initializes the Page instance.\n\n    Args:\n        connection_port (int): The port number for the connection to the browser.\n        page_id (str): The ID of the page, obtained via the DevTools Protocol.\n    \"\"\"\n    self._connection_handler = ConnectionHandler(connection_port, page_id)\n    self._page_events_enabled = False\n    self._network_events_enabled = False\n    self._fetch_events_enabled = False\n    self._dom_events_enabled = False\n    self._intercept_file_chooser_dialog_enabled = False\n    self._cloudflare_captcha_callback_id = None\n```\n\n----------------------------------------\n\nTITLE: Displaying MIT License Text for Pydoll Project\nDESCRIPTION: Provides the full text of the MIT License under which Pydoll is released. This permissive license allows for use, modification, and distribution of the code with minimal restrictions.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/index.md#2025-04-19_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nMIT License\n\nCopyright (c) 2023 Pydoll Contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n----------------------------------------\n\nTITLE: Visualizing CDP Communication Flow with Mermaid Diagram\nDESCRIPTION: A sequence diagram illustrating how Pydoll communicates with the Chrome browser through a WebSocket connection. It shows the flow of commands from Pydoll to the browser and events from the browser back to Pydoll.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/cdp.md#2025-04-19_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant App as Pydoll Application\n    participant WS as WebSocket Connection\n    participant Browser as Chrome Browser\n\n    App ->> WS: Command: navigate to URL\n    WS ->> Browser: Execute navigation\n\n    Browser -->> WS: Send page load event\n    WS -->> App: Receive page load event\n```\n\n----------------------------------------\n\nTITLE: Resource Type Filtering Example in Pydoll\nDESCRIPTION: Shows an anti-pattern example of inefficient resource type filtering that can impact performance by intercepting all requests.\nSOURCE: https://github.com/autoscrape-labs/pydoll/blob/main/docs/deep-dive/network-capabilities.md#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Bad: Intercept all requests (performance impact)\nawait page.enable_fetch_events(resource_type='')\n```"
  }
]